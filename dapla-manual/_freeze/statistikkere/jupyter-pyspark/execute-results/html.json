{
  "hash": "716d8da4d1dd8d189a0fbbbed1e790bd",
  "result": {
    "markdown": "---\ntitle: Jupyter-pyspark\nfreeze: true\nexecute:\n    echo: false\n    include: true\n\n---\n\n::: {.cell execution_count=1}\n\n::: {.cell-output .cell-output-stdout}\n```\nSist endret: 2024-11-11 av Johnny Niklasson\n```\n:::\n:::\n\n\n![](../images/dapla-lab-jupyter-pyspark-service.png){style=\"max-width: 50%; float: right;\" fig-alt=\"Jupyter Service\"}\n\n**Jupyter-pyspark** er en tjeneste på [Dapla Lab](./dapla-lab.html) som lar brukerne kode i [Jupyterlab](https://docs.jupyter.org/en/latest/). Tjenesten kommer med Python, Pyspark og noen vanlige Jupyterlab-extensions ferdig installert. Målgruppen for tjenesten er brukere som skal skrive produksjonskode med Pyspark i Jupyterlab.\n\nSiden tjenesten er ment for produksjonskode så er det veldig få Python-pakker som er forhåndsinstallert. Antagelsen er at brukeren/teamet heller bør installere de pakkene de selv trenger, framfor at det ligger ferdiginstallerte pakker som skal dekke behovet til alle brukere/team i SSB. Det reduserer kompleksitet i tjenesten og dermed sannsynligheten for feilsituasjoner. \n\n## Forberedelser\n\nFør man starter **Jupyter-pyspark** bør man ha lest [kapitlet om Dapla Lab](./dapla-lab.html) og satt opp Git- og GitHub-konfigurasjonen under **Min konto**. Deretter gjør du følgende:\n\n1. Logg deg inn på [Dapla Lab](https://lab.dapla.ssb.no/)\n2. Under **Tjenestekatalog** trykker du på **Start**-knappen for **Jupyter-pyspark**\n3. Gi tjenesten et navn\n4. Åpne **Jupyter-pyspark konfigurasjoner**\n\n## Konfigurasjon\n\nFør man åpner en tjeneste kan man konfigurere hvor mye ressurser man ønsker, hvilket team man skal representere, om et GitHub-repo skal klones ved oppstart, og mange andre ting. Valgene man gjør kan også lagres slik at man slipper å gjøre samme jobb senere. @fig-dapla-lab-tjenestekonf-detail viser Tjeneste-fanen i konfigurasjonen for Jupyter-pyspark.\n\n![Detaljert tjenestekonfigurasjon i Dapla Lab](../images/dapla-lab-jupyter-pyspark-tjenestekonf-detail.png){fig-alt=\"Viser tjenestekonfigurasjonen i Dapla Lab.\" #fig-dapla-lab-tjenestekonf-detail}\n\n### Tjeneste\n\nUnder fanen **Tjeneste** kan man velge hvilke **Versjon** av Jupyter-pyspark som skal startes^[Med versjon menes her hvilken Docker-container som skal bygges. Men i konfigurasjonen har vi forenklet valget ved å kun vise hvilke Python- og Spark-versjoner som skal gjelde]. Siden Jupyter-pyspark kommer installert Python og Spark, er det her man velger hvilke versjoner av disse man ønsker. Man kan velge mellom alle tidligere tilbudte kombinasjoner av Python og Spark. \n\nI @fig-dapla-lab-tjenestekonf-detail ser vi av navnet `py311-spark3.3.1-v1-2024.11.07` at tjenesten som default vil startes med versjon 3.11 av Python og og 3.3.1 av Spark. Etter hvert som nye versjoner av Python og Spark kommer, kan disse tilgjengeliggjøres i tjenesten, men brukeren kan velge å starte en eldre versjon av tjenesten.\n\n### Dapla\n\nUnder **Dapla**-fanen kan man velge **Aktiver** for å få tilgang til data fra bøtter i tjenesten. I tillegg må man da velge hvilket team og tilgangsgruppe man skal representere.\n\nAktiveres **Bøtter som filsystem** så blir bøttene til teamet tilgjengeliggjort som et vanlig filsystem under filstien `/buckets/`.\n\nUnder **Team og tilgangsgruppe** kan man velge hvilket team og tilgangsgruppe man skal representere i tjenesten. Man gjør dette ved å velge navnet på tilgangsgruppen, og denne er alltid på formen `<teamnavn>-<tilgangsgruppe>`. @fig-dapla-lab-konf-buckets viser at brukeren har valgt tilgangsgruppen *dapla-felles-developers*, dvs. at de representerer tilgangsgruppen *developers* for teamet *dapla-felles*. \n\n![Detaljert tjenestekonfigurasjon for bøttetilgang i Dapla Lab](../images/dapla-lab-konf-buckets.png){fig-alt=\"Viser tjenestekonfigurasjonen i Dapla Lab.\" #fig-dapla-lab-konf-buckets}\n\nUnder **Team og tilgangsgruppe** kan brukeren også velge å representere tilgangsgruppen *data-admins* for et team. I de tilfellene er det et krav om brukeren oppgir en skriftlig begrunnelse for hvorfor tilgangen er nødvendig. I tillegg må kan de maksimalt aktivere tilgangen i 8 timer. \n\n@fig-dapla-lab-konf-buckets-data-admins viser en bruker som aktiverer sin *data-admins* tilgang for team *dapla-felles*. Hvis brukeren ikke oppgir en begrunnelse vil de få en feilmelding ved oppstart av tjenesten.  \n\n![Aktivere tilgang til kildedata for data-admins.](../images/dapla-lab-konf-buckets-data-admin.png){fig-alt=\"Viser tjenestekonfigurasjonen i Dapla Lab.\" #fig-dapla-lab-konf-buckets-data-admins}\n\n### Ressurser\n\nUnder fanen **Ressurser** kan man velge hvor mye CPU og RAM man ønsker i tjenesten, slik som vist i @fig-dapla-lab-resources. Velg så lite som trengs for å gjøre jobben du skal gjøre. \n\n![Konfigurasjon av ressurser for Jupyter-pyspark i Dapla Lab](../images/dapla-lab-resources.png){fig-alt=\"Viser Resources-fanen i Jupyter-pyspark-konfigurasjonen i Dapla Lab.\" #fig-dapla-lab-resources}\n\n\n### Diskplass\n\nSom default får alle som starter en instans av Jupyter-pyspark en lokal disk på **10GB** inne i tjenesten. Under *Diskplass*-fanen kan man velge å øke størrelsen på disken eller ikke ha noe disk i det hele tatt. Siden lokal disk i tjenesten hovedsakelig skal benyttes til å lagre en lokal kopi av koden som lagres på GitHub mens man gjør endringer, bør ikke størrelsen på disken være større enn nødvendig. @fig-dapla-lab-persistence viser valgene som kan gjøres under Diskplass-fanen. \n\n![Konfigurasjon av lokal disk for Jupyter-pyspark i Dapla Lab](../images/dapla-lab-persistence.png){fig-alt=\"Viser Persistence-fanen i Jupyter-pyspark-konfigurasjonen i Dapla Lab.\" #fig-dapla-lab-persistence}\n\n### Git\n\nUnder fanen **Git** kan man konfigurere Git og GitHub slik at det blir lettere å jobbe med inne i tjenesten. Som standard arves informasjonen som er lagret under [*Min konto*-*Git* i Dapla Lab](./dapla-lab.html#git). Informasjonen under tjenestekonfigurasjonen blir tilgjengeliggjort som miljøvariabler i tjenesten. Informasjonen blir også lagt i `$HOME/.netrc` slik at man kan benytte det uten å gjøre noe mer for å jobbe mot GitHub fra tjenesten.  \n\n![Konfigurasjon av Git for Jupyter-pyspark i Dapla Lab](../images/dapla-lab-konf-git.png){fig-alt=\"Viser Git-fanen i Jupyter-pyspark-konfigurasjonen i Dapla Lab.\" #fig-dapla-lab-konf-git}\n\n@fig-dapla-lab-konf-git viser at brukeren som standard får aktivert **Aktiver Git**. Dette innebærer at Git-brukernavn, Git e-post og GitHub-token arves fra brukerkonfigurasjonen. I tillegg opprettes SSBs standard Git-konfigurasjon i `~/.gitconfig`. \n\n## Konfigurering av PySpark-ressurser i Notebook\n\nFor å konfigurere PySpark-ressurser etter at Jupyter-pyspark har startet, kan du sette opp spesifikke PySpark-innstillinger direkte i en Jupyter Notebook. Dette gir fleksibilitet til å justere ressursene basert på behovet for hver sesjon. Merk her at begrensningen er hva du har konfigurert i tjenestekonfigurasjonen, [se Ressurser](#ressurser)\n\n1. **Start Jupyter-pyspark**:\n   - Logg inn på **Dapla Lab** og start **Jupyter-pyspark**-tjenesten.\n   - Når tjenesten er oppe, åpner du en ny eller eksisterende Spark (local) Notebook for å konfigurere PySpark.\n\n2. **Angi spesifikke PySpark-konfigurasjoner i Notebook**:\n   - I din Notebook, inkluder følgende Python-kode for å konfigurere CPU, minne og andre PySpark-ressurser:\n     ```python\n     from pyspark.sql import SparkSession\n\n     # Start SparkSession med ønskede konfigurasjoner\n     spark = SparkSession.builder \\\n         .appName(\"Jupyter-pyspark-konfig\") \\\n         .master(\"local[*]\") \\\n         .config(\"spark.driver.memory\", \"10g\") \\\n         .config(\"spark.executor.memory\", \"4g\") \\\n         .getOrCreate()\n     ```\n\n   - Juster verdiene etter behov for å sikre optimal ytelse for oppgavene du kjører i Notebook.\n\n## Tilgjengelige JAR-filer og bruk i PySpark\n\nI `/jupyter/lib`-mappen i Jupyter-pyspark-miljøet er flere nyttige JAR-filer tilgjengelige for bruk med PySpark, inkludert støtte for Google Cloud Storage, BigQuery, Avro og Delta Lake. Disse JAR-filene kan inkluderes i PySpark-konfigurasjonen for å få tilgang til og arbeide med data fra disse kildene.\n\n1. **Tilgjengelige JAR-filer**:\n   - `gcs-connector-hadoop.jar`: Kobler PySpark til Google Cloud Storage.\n   - `spark-bigquery-with-dependencies_2.12.jar`: Kobler PySpark til Google BigQuery.\n   - `spark-avro_2.12.jar`: Støtte for å lese og skrive Avro-data.\n   - `delta-storage.jar` og `delta-core_2.12.jar`: Støtte for Delta Lake, som muliggjør ACID-transaksjoner og data versjonering.\n\n2. **Legge til JAR-filer i PySpark**:\n   - For å bruke disse JAR-filene, konfigurer PySpark med stien til hver fil:\n     ```python\n     spark = SparkSession.builder \\\n         .appName(\"Jupyter-pyspark-konfig\") \\\n         .config(\"spark.jars\", \"/jupyter/lib/gcs-connector-hadoop.jar,\"\n                               \"/jupyter/lib/spark-bigquery-with-dependencies_2.12.jar,\"\n                               \"/jupyter/lib/spark-avro_2.12.jar,\"\n                               \"/jupyter/lib/delta-storage.jar,\"\n                               \"/jupyter/lib/delta-core_2.12.jar\") \\\n         .getOrCreate()\n     ```\n\n3. **Eksempler på bruk av tilkoblingene**:\n   - **Google Cloud Storage (GCS)**:\n     ```python\n     df = spark.read.format(\"parquet\").load(\"gs://ditt-bucket-navn/path/to/data\")\n     ```\n\n   - **Google BigQuery**:\n     ```python\n     df = spark.read.format(\"bigquery\") \\\n         .option(\"table\", \"prosjekt_id.dataset_id.tabell_id\") \\\n         .load()\n     ```\n\n   - **Avro**:\n     ```python\n     df = spark.read.format(\"avro\").load(\"/path/to/avro/files\")\n     ```\n\n   - **Delta Lake**:\n     - For å skrive til en Delta-tabell:\n       ```python\n       df.write.format(\"delta\").save(\"/path/to/delta-table\")\n       ```\n     - For å lese fra en Delta-tabell:\n       ```python\n       delta_df = spark.read.format(\"delta\").load(\"/path/to/delta-table\")\n       ```\n\nMed disse instruksjonene kan brukerne effektivt konfigurere Jupyter-PySpark til å jobbe med eksterne datakilder og forskjellige dataformater i sitt PySpark-miljø.\n\n## Hvordan Spark Lokalt Fungerer og Arbeidsfordeling på Kjerner\n\nNår Spark kjøres lokalt, starter det en **SparkSession** som kjører på en enkelt node (tjenesten din) uten å involvere en distribuert klynge. Lokalt i Spark kan du kontrollere ressursbruken og fordele arbeidsmengden på tilgjengelige CPU-kjerner for å optimalisere ytelsen.\n\n### Kjøring i Lokal Modus\n\nNår Spark konfigureres til å kjøre i lokal modus, spesifiseres dette med `\"local[N]\"`, der `N` representerer antall kjerner som Spark skal bruke. For eksempel:\n- `\"local[*]\"`: Bruk alle tilgjengelige kjerner på maskinen.\n- `\"local[2]\"`: Bruk 2 kjerner, uavhengig av hvor mange som er tilgjengelige.\n\nEksempel på konfigurasjon:\n```python\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"Lokal Spark\") \\\n    .master(\"local[*]\") \\  # Bruker alle tilgjengelige kjerner\n    .config(\"spark.driver.memory\", \"10g\") \\\n    .config(\"spark.executor.memory\", \"4g\") \\\n    .getOrCreate()\n```\n\nHvis du ikke overstyrer konfigurasjonen vil innhold i miljøvariabelen **PYSPARK_LOCAL_SUBMIT_ARGS** gjelde.\n```sh\n--master local[*] --driver-cores 1 --driver-memory 2g --executor-memory 2g  --conf spark.driver.maxResultSize=2g --conf spark.executor.pyspark.memory=1g pyspark-shell\n```\n## Datatilgang\n\nHvis man har valgt å tilgjengeliggjøre data fra et team sitt bøtter i tjenesten, så kan man inspisere dataene fra en terminal inne i tjenesten: \n\n1. Åpne en instans av Jupyter-pyspark med data fra bøtter \n2. Åpne en terminal inne i Jupyter\n3. Gå til mappen med bøttene ved å kjøre dette fra terminalen `cd /buckets`\n3. Kjør `ls -ahl` i terminalen for å se på hvilke bøtter som er montert.\n\n## Installere pakker\n\nSiden det nesten ikke er installert noen pakker i tjenesten så kan brukeren opprette et [ssb-project](jobbe-med-kode.qmd#ssb-project) og installere pakker som vanlig. \n\nFor å [bygge et eksisterende ssb-project](./jobbe-med-kode.qmd#bygg-eksisterende-ssb-project) kan brukeren også bruke ssb-project.\n\n## Slette tjenesten\n\nFor å slette tjenesten kan man trykke på Slette-knappen i Dapla Lab under **Mine tjenester**. Når man sletter en tjeneste så slettes hele disken inne i tjenesten, og alle ressurser frigjøres. Vi anbefaler at man avslutter heller enn pauser tjenester som ikke benyttes. \n\n## Pause tjenesten\n\nMan kan pause tjenesten ved å trykke på Pause-knappen i Dapla Lab under **Mine tjenester**. Når man pauser, slettes alt på den lokale disken som ikke er lagret under `$HOME/work`. Vi anbefaler at man avslutter heller enn pauser tjenester som ikke benyttes.\n\n## Monitorering\n\n:::: {.columns}\n\n::: {.column width=\"70%\"}\nMan kan moniterere en instans av Jupyter-pyspark ved å trykke på Jupyter-pyspark-teksten under Mine tjenester i Dapla Lab, slik som vist i @fig-dapla-lab-monitoring.\n\nDenne funksjonaliteten er under arbeid og mer informasjon kommer snart. \n:::\n\n::: {.column width=\"5%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"25%\"}\n![Monitorering av Jupyter-pyspark i Dapla Lab](../images/dapla-lab-monitoring.png){fig-alt=\"Viser Persistence-fanen i Jupyter-pyspark-konfigurasjonen i Dapla Lab.\" #fig-dapla-lab-monitoring}\n:::\n\n::::\n\n",
    "supporting": [
      "jupyter-pyspark_files"
    ],
    "filters": [],
    "includes": {}
  }
}