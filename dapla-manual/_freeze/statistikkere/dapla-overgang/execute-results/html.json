{
  "hash": "517fb3b27b6453491e3c0779153b25b5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Hordan komme i gang med overgangen til Dapla?\njupyter:\n  jupytext:\n    text_representation:\n      extension: .qmd\n      format_name: quarto\n      format_version: '1.0'\n      jupytext_version: 1.17.0\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n---\n\n\n\n\nJobber du med å få over en statistikk? \nI denne artikkelen beskriver vi hvordan man bør jobbe for å få en statistikk over på Dapla, både overordnede prinsipper og en steg-for-steg veiledning.\n\n### Steg 1: Opprette Dapla team\n\nDet første man må gjøre når man skal få en statistikk over på Dapla er å opprette et Dapla team. Et Dapla team gir en tilgang til Google Cloud. Det er anbefalt praksis å ha ett team per seksjon. Seksjon for Prisstatistikk (s240) har eksempelvis team-navnet `prisstat`.\n\n::: {.callout-tip}\n# Hvordan oppretter du et team?\nI [applikasjonen Dapla-Ctrl](https://dapla-ctrl.intern.ssb.no/)! Dette må gjøres av seksjonsleder. Les brukerveiledning i [artikkelen vår om Dapla-Ctrl](https://manual.dapla.ssb.no/statistikkere/dapla-ctrl.html#opprette-team) om du trenger hjelp.\n:::\n\n### Steg 2: Flytt data til Dapla!\n\nEn av de viktigste delene av å få en statistikk over på Dapla er å jobbe på Dapla. Derfor må man flytte noe data som man kan bruke til å bygge et produksjonsløp.\n\nFlere i SSB bruker produksjonssonen for å skrive produksjonssløp i Python og R. Det er ikke anbefalt. Det er tre hovedgrunner til dette:\n\n* Jobber man på bakken isolerer man seg fra ny og viktig Dapla-funksjonalitet\n* Det tar tid å flytte et produksjonsløp fra bakken til sky!\n* Jupyter på bakken er ikke skalert for å håndtere produksjonssløp\n\n#### Hvordan flytter man data fra produksjonssonen til Google Cloud Storage?\n\n##### 1. Finn dataene du skal flytte\n\nBestem deg for hvilke data du skal begynne å jobbe med. Vi anbefaler å begynne med inndata. Kildedata kan overføres når man setter i gang med [kildomaten](kildomaten.qmd).\n\n##### 2. Klargjør dataene for flytting (filformat og filnavn)\n\nLes filene inn i Python i jupyter i produksjonssonen og lagre de som .parquet-filer med filnavn som følger [navnestandard for datalagring](navnestandard.qmd).\n\n##### 3. Flytt dataene til ssb/cloud_sync/<dapla-team>/standard/tilsky/\n\nFlytt så dataene til *ssb/cloud_sync/<dapla-team>/standard/tilsky/* på linux.\nHvordan dette gjøres spørs på hvor dataene ligger. Ligger dataene på linux kan man enkelt kopiere og lime ved å bruke `cp`\nLigger de på X-disken må man bruke FileZilla. Utvid boksen nedenfor for å lese hvordan man overfører data fra X-disken til linux.\n\n:::{.callout-tip collapse=true}\n# Flytte data fra X-disken til Linux\n\nLigger dataene du vil flytte på X-disken må du bruke FileZilla. Dette er fordi data kun kan synkroniseres mellom linux og google cloud.\n\nFor å flytte data fra X-disken følger du [denne filsluse-veiledningen skrevet av IT kundeservice](https://ssbno.sharepoint.com/sites/IT-service/SitePages/Bruk-av-filsluse.aspx?web=1).\n\nDet eneste unntaket er at du skal skrive *sftp://sl-sas-compute-2* som vert, ikke filsluse.ssb.no\n\n:::\n\n##### 4: Finn frem til Transfer service-området på Google Cloud Platform\n\n1. Gå inn på [Google Cloud Console](https://console.cloud.google.com) i en nettleser.\n2. Sjekk, øverst i høyre hjørne, at du er logget inn med din SSB-konto (xxx@ssb.no).\n3. Velg prosjektet^[Du kan velge prosjekt øverst på siden, til høyre for teksten **Google Cloud**. I bildet under ser du at hvordan det ser ut når prosjektet `dapla-felles-p` er valgt.![](../images/gcc-project-selector.png){fig-alt=\"Diagram av mapper i prodsonen og bøtter på Dapla, og hvordan overføringene kan skje mellom de.\"}] som overføringen skal settes opp under).\n\n* Eks. Skal vi overføre *inndata* for teamet *prisstat* vi velge prosjektet *prisstat-p*\n\n4. Etter at du har valgt prosjekt kan du søke etter **Transfer jobs** eller **Storage transfer** i søkefeltet øverst på siden, og gå inn på siden *Transfer jobs*\n\n\n##### 5: Lag en transfer job for flytting fra bakke til sky\n\n* Trykk så på `CREATE TRANSFER JOB`-knappen.\n* Velg *POSIX filesystem* under source type, og *Google Cloud Storage* under Destination type\n* Velg mellom *Batch* og *Event-driven* under Scheduling mode\n* Velg *transfer_service_default* under Agent pool\n* Velg skriv  *tilsky/* under Source directory path\n* Velg bøtte du vil overføre til. Enten tilsky (*ssb-<team-navn>-data-tilsky-prod*) eller produkt (*ssb-<team-navn>-data-produkt-prod*)\n* Velg overførignshyppighet. Velger du *Run once* kan du uansett kjøre tjenesten manuelt i fremtiden\n* Skriv en kort beskrivelse (eks. 'Bakke til sky - engangskjøring')\n* Voilà! \n\n##### 6: Kjør transfer job!\n\nGå tilbake til transfer jobs-siden og kjør din nye transfer-job vewd å trykke på jobb-navnet og ~Start a run*\n\n### Steg 3: Lag repo med SSB-project\n\nNeste steg er å lage et repository for statistikken du skal jobbe med! Ifølge navnestandarden skal reponavnet begynne med *stat-*.\n\nKort fortalt gjør man det ved å kjøre `ssb-project create <repo-navn> --github --github-token='<din token>'`\n\nLes mer i [artikkelen vår om SSB-project](ssb-project.qmd)!\n\n::: {.callout-tip}\nNysgjerrig på hvordan et SSB-project repo ser ut? Sjekk ut [`stat-eksempel`](https://github.com/statisticsnorway/stat-eksempel) på GitHub - et fiktivt produksjonsløp utviklet av A200 støtteteam.\n:::\n\n### Steg 4: Tegn dagens produksjonssystem\n\nSkal vi lage bedre produksjonssystemer på Dapla må vi kunne analysere dagens systemer for å unngå å kopiere systemene vi har i dag.\n\nDerfor er det lurt å lage en oversikt over hvordan data flyter gjennom ulike systemer i dagens produksjonsopplegg. Dette kan gjelde alle stegene fra datainnsamling og klargjøring til analyse og publisering. \n \nFor å forenkle denne prosessen og ha en felles forståelse av både nåværende («as-is») og framtidige («to-be») produksjonsløp, er det laget et helhetlig opplegg som kombinerer et Excel-skjema med automatisk generering av visualiseringer i Tableau.\n \nKort fortalt innebærer dette at statistikkprodusenter selv fyller inn informasjon om hvilke systemer og datasett som er i bruk, hvordan dataene transformeres underveis, og i hvilken rekkefølge de ulike stegene finner sted. Thomas Bjørnskau kan bistå med dette og konverterer skjemaet til dataflytdiagrammer ved hjelp av Tableau.\n\n### Steg 5: Jobb deg gjennom datatilstandene \n\nNår alt annet er på plass kan man begynne å lage det nye produksjonssystemet - en datatilstand om gangen. Det er slik vi har gjort det i [stat-eksempel](https://github.com/statisticsnorway/stat-eksempel).\n\n### Og så må du finne din egen vei.... Men her har du noen tips!! \n\n#### 1. Ta finpussen til slutt!\n\n* Ref. [First Rule Of Optimization](https://wiki.c2.com/?FirstRuleOfOptimization)\n* Ikke heng deg opp i finpuss som detaljert dokumentasjon, visualiseringer og nice-to-haves\n    -> God dokumentasjon er viktig, men vi anbefaler å ta detaljene til slutt\n\n#### 2. Bruk [ssb-fagfunksjoner](https://github.com/statisticsnorway/ssb-fagfunksjoner) - en python-pakke med masse fellesfunksjonalitet\n\n* ... som for eksempel funksjoner for versjonering vist frem i [navnestandard-artikkelen vår](navnestandard.qmd).\n\n#### 3. Spør om hjelp!\n\n* Bruk støtteteamene, statistikktjenester, viva engage og andre kanaler\n\n",
    "supporting": [
      "dapla-overgang_files"
    ],
    "filters": [],
    "includes": {}
  }
}