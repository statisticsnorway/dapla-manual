[
  {
    "objectID": "blog/posts/2024-07-11-pseudo-testdata/index.html",
    "href": "blog/posts/2024-07-11-pseudo-testdata/index.html",
    "title": "Pseudonymisering med testdata",
    "section": "",
    "text": "Den strenge tilgangsstyringen til pseudonymiseringsfunksjonaliteten på Dapla gjør at det er vanskelig for brukere å bli kjent med funksjonaliteten ved bruk av produksjonsdata. Derfor bør alle som jobber med dette starte med å bruke testdata og jobbe i test-miljøet på Dapla."
  },
  {
    "objectID": "blog/posts/2024-07-11-pseudo-testdata/index.html#importer-data",
    "href": "blog/posts/2024-07-11-pseudo-testdata/index.html#importer-data",
    "title": "Pseudonymisering med testdata",
    "section": "Importer data",
    "text": "Importer data\nFørst så importerer vi noen biblioteker som vi skal benytte. Versjonen\n\nimport json\n\nimport dapla as dp\nimport pandas as pd\nimport polars as pl\nfrom dapla_pseudo import Depseudonymize, Pseudonymize\nfrom dapla_pseudo.constants import MapFailureStrategy\nfrom dapla_pseudo.utils import convert_to_date\nfrom IPython.display import JSON\n\nVersjonen av dapla-toolbelt-pseudo er 2.1.2.\nDataene vi skal bruke syntetiske fødselsnummer fra testversjonen SNR-katalogen. På den måten får vi også testet pseudonymiseringen via SNR-katalogen som er veldig vanlig i SSB. Denne SNR-katalogen ligger som en fil i en bøtte som alle i SSB har tilgang til.\n\npath = \"gs://ssb-staging-dapla-felles-data-delt/test/data/freg/kilde/snr_kat_latest/snr_kat.csv\"\n\ndf = dp.read_pandas(path, file_format=\"csv\", dtype={\"fnr\": str, \"fnr_date\": str})\n\ndf.head(n=5).style.set_table_styles([\n    {'selector': 'th', 'props': [('text-align', 'left')]}\n])\n\n\n\nTabell 1: Syntetisk versjon av SNR-katalogen\n\n\n\n\n\n\n\n\n \nfnr\ncurrent_fnr\nsnr\ncurrent_snr\nfnr_date\ncurrent_fnr_date\n\n\n\n\n0\n16890249063\n16890249063\n026mxd3\n026mxd3\n20201222\n20201222\n\n\n1\n15854996565\n15854996565\n34qm7pt\n34qm7pt\n20201222\n20201222\n\n\n2\n27871547810\n27871547810\n53uxelp\n53uxelp\n20201222\n20201222\n\n\n3\n50889200399\n50889200399\nf35lbnf\nf35lbnf\n20201222\n20201222\n\n\n4\n22919199052\n22919199052\nc2hxvdv\nc2hxvdv\n20201222\n20201222\n\n\n\n\n\n\n\n\nFra Tabell 1 ser vi at datasettet inkluderer en del kolonner. For utforsking av pseudonymiseringsfunksjonalitet så trenger vi kun fnr-kolonnen."
  },
  {
    "objectID": "blog/posts/2024-07-11-pseudo-testdata/index.html#forberedelse-av-datasettet",
    "href": "blog/posts/2024-07-11-pseudo-testdata/index.html#forberedelse-av-datasettet",
    "title": "Pseudonymisering med testdata",
    "section": "Forberedelse av datasettet",
    "text": "Forberedelse av datasettet\nLa oss kun beholde fnr-kolonnen og kopiere den en ny kolonne slik at vi enklere kan sammenligne før og etter pseudonymisering. I tillegg kutter jeg antall rader til 10, siden vi ikke trenger noe mer for formålet her.\n\ndf2 = df.head(n=10)\ndf3 = df2[[\"fnr\"]]\ndf4 = df3.copy()\ndf4['fnr_original'] = df4['fnr']\n\ndf4.head(n=5).style.set_table_styles([\n    {'selector': 'th', 'props': [('text-align', 'left')]}\n])\n\n\n\nTabell 2: Nedstrippet versjon av SNR-katalogen\n\n\n\n\n\n\n\n\n \nfnr\nfnr_original\n\n\n\n\n0\n16890249063\n16890249063\n\n\n1\n15854996565\n15854996565\n\n\n2\n27871547810\n27871547810\n\n\n3\n50889200399\n50889200399\n\n\n4\n22919199052\n22919199052\n\n\n\n\n\n\n\n\nHvis du ønsker å teste hvordan krypteringsalgoritmene fungerer med kolonner som inneholder navn, så kan vi generere noe data med et også.\n\nfornavn = [\n    \"Jo\",\n    \"Hans-August\",\n    \"Nils\",\n    \"Eva\",\n    \"Lars\",\n    \"Øyvind\",\n    \"Kenneth\",\n    \"Johnny\",\n    \"Rupinder\",\n    \"Nicolas\",\n]\netternavn = [\n    \"Nordman\",\n    \"Karlsen\",\n    \"Nordmann\",\n    \"Karlsen\",\n    \"Carlsen\",\n    \"Nordmann\",\n    \"Carlsen\",\n    \"Nordmann\",\n    \"Karlsen\",\n    \"Normann\",\n]\n\ndf4['fornavn'] = fornavn\ndf4['etternavn'] = etternavn\ndf5 = df4.copy()\n\nTil slutt legger vi på noen ugyldige fødselsnummer slik at vi får testet hvordan algoritmene håndterer dette.\n\nnew_row1 = pd.DataFrame(\n    [\n        {\n            \"fnr\": \"99999999999\",\n            \"fnr_original\": \"99999999999\",\n            \"fornavn\": \"Michael\",\n            \"etternavn\": \"Norman\",\n        }\n    ]\n)\ndf6 = pd.concat([df5, new_row1], ignore_index=True)\n\nnew_row2 = pd.DataFrame(\n    [{\"fnr\": \"XX\", \"fnr_original\": \"XX\", \"fornavn\": \"Ola Glenn\", \"etternavn\": \"Gåås\"}]\n)\ndf7 = pd.concat([df6, new_row2], ignore_index=True)\n\nnew_row3 = pd.DataFrame(\n    [\n        {\n            \"fnr\": \"X8b7k28\",\n            \"fnr_original\": \"X8b7k28\",\n            \"fornavn\": \"Lars\",\n            \"etternavn\": \"Gaas\",\n        }\n    ]\n)\ndf8 = pd.concat([df7, new_row3], ignore_index=True)\n\ndf8.head(n=5).style.set_table_styles([\n    {'selector': 'th', 'props': [('text-align', 'left')]}\n])\n\n\n\nTabell 3: Datasett for å teste pseudonymiseringsfunksjonalitet\n\n\n\n\n\n\n\n\n \nfnr\nfnr_original\nfornavn\netternavn\n\n\n\n\n0\n16890249063\n16890249063\nJo\nNordman\n\n\n1\n15854996565\n15854996565\nHans-August\nKarlsen\n\n\n2\n27871547810\n27871547810\nNils\nNordmann\n\n\n3\n50889200399\n50889200399\nEva\nKarlsen\n\n\n4\n22919199052\n22919199052\nLars\nCarlsen\n\n\n\n\n\n\n\n\nTabell 3 viser datasettet vi skal bruke til å teste med."
  },
  {
    "objectID": "blog/posts/2024-07-11-pseudo-testdata/index.html#pseudonymisering",
    "href": "blog/posts/2024-07-11-pseudo-testdata/index.html#pseudonymisering",
    "title": "Pseudonymisering med testdata",
    "section": "Pseudonymisering",
    "text": "Pseudonymisering\nNå kan vi begynne å leke med dataene. Det første vi kan gjøre er å pseudonymisere med den mest vanlige algoritmen som benyttes i produksjon: Papis-nøkkelen.\n\nresult = (\n    Pseudonymize.from_pandas(df8)\n    .on_fields(\"fnr\")\n    .with_stable_id()\n    .run()\n)\nresult.to_pandas()\n\nUnexpected length of metadata: 2\n\n\n\n\nTabell 4: Pseudonymiserer med Papis-algoritmen\n\n\n\n\n\n\n\n\n\n\nfnr\nfnr_original\nfornavn\netternavn\n\n\n\n\n0\nBnQe23u\n16890249063\nJo\nNordman\n\n\n1\nI1mQmBP\n15854996565\nHans-August\nKarlsen\n\n\n2\neVbGYLy\n27871547810\nNils\nNordmann\n\n\n3\nO4jegSM\n50889200399\nEva\nKarlsen\n\n\n4\n6JhhRKi\n22919199052\nLars\nCarlsen\n\n\n5\nvygqpLq\n66821775168\nØyvind\nNordmann\n\n\n6\nJWzitL8\n13824498614\nKenneth\nCarlsen\n\n\n7\noIhVngD\n46927100797\nJohnny\nNordmann\n\n\n8\n0y8qqUZ\n16907699157\nRupinder\nKarlsen\n\n\n9\nFNnp5AL\n10920998203\nNicolas\nNormann\n\n\n10\n4yI2BlkviaI\n99999999999\nMichael\nNorman\n\n\n11\nXX\nXX\nOla Glenn\nGåås\n\n\n12\ntKHXmUl\nX8b7k28\nLars\nGaas\n\n\n\n\n\n\n\n\n\n\nI Tabell 4 ser vi at kolonnen fnr har blitt pseudonymisert. Det er også verdt å legge merke til at kolonnen ikke endrer navn. Grunnen til at lengden på verdiene som er pseudonymiserte er på 7 tegn for de opprinnelige fødselsnummerne, er at det først skjer en oversetting fra fnr til snr før det pseudonymiseres, og snr-nummerserien er på 7 tegn. Med andre ord så preserverer algoritmen lengden på snr-nummeret siden det er dette som pseudonymiseres.\nDet er også verdt å merke seg at verdier som er kortere enn 4 i lengde, f.eks. XX i rad 11, ikke blir pseudonymisert i det hele tatt. Verdier som er 4 eller lengre, vil bli pseudonymisert selv om de ikke fikk treff i SNR-katalogen.\n\nMetadata\nDet genereres også 2 metadata-objekter ved pseudonymisering. Disse er:\n\nresult.datadoc\nresult.metadata\n\nLa oss se nærmere på de:\n\ndata = json.loads(result.datadoc)\ndisplay(data)\n\n{'document_version': '0.0.1',\n 'pseudonymization': {'document_version': '0.1.0',\n  'pseudo_variables': [{'short_name': 'fnr',\n    'data_element_path': 'fnr',\n    'data_element_pattern': '/fnr',\n    'stable_identifier_type': 'FREG_SNR',\n    'stable_identifier_version': '2023-08-31',\n    'encryption_algorithm': 'TINK-FPE',\n    'encryption_key_reference': 'papis-common-key-1',\n    'encryption_algorithm_parameters': [{'keyId': 'papis-common-key-1'},\n     {'strategy': 'skip'}]}]}}\n\n\nDette er metadata som skal integreres i Datadoc etter hvert.\nLa oss se på den andre typen metadata:\n\ndisplay(result.metadata)\n\n{'logs': ['No SID-mapping found for fnr 999********',\n  'No SID-mapping found for fnr X8b****'],\n 'metrics': {'MAPPED_SID': 10, 'FPE_LIMITATION': 1, 'MISSING_SID': 2}}\n\n\nHer ser vo at 10 felt fikk treff i SNR-katalogen, 1 felt var for kort for algoritmen, og 2 felt fikk ikke treff SNR-katalogen. Vi får også se litt fødselsnummeret til de 2 som ikke fikk treff."
  },
  {
    "objectID": "blog/posts/2024-07-11-pseudo-testdata/index.html#depseudonymisering",
    "href": "blog/posts/2024-07-11-pseudo-testdata/index.html#depseudonymisering",
    "title": "Pseudonymisering med testdata",
    "section": "Depseudonymisering",
    "text": "Depseudonymisering\nLa oss ta vare på den pseudonymiserte kolonnen og så depseudonymisere og se om resultatet blir riktig:\n\nresult2 = result.to_pandas()\nresult2['pseudo_fnr'] = result2['fnr']\n\nresult_df = (\n    Depseudonymize.from_pandas(result2)         \n    .on_fields(\"fnr\")                              \n    .with_stable_id()                              \n    .run()                                         \n    .to_pandas() \n)\n\nresult_df.style.set_table_styles([\n    {'selector': 'th', 'props': [('text-align', 'left')]}\n])\n\n\n\nTabell 5: Depseudonymisering av fødselsnummer\n\n\n\n\n\n\n\n\n \nfnr\nfnr_original\nfornavn\netternavn\npseudo_fnr\n\n\n\n\n0\n16890249063\n16890249063\nJo\nNordman\nBnQe23u\n\n\n1\n15854996565\n15854996565\nHans-August\nKarlsen\nI1mQmBP\n\n\n2\n27871547810\n27871547810\nNils\nNordmann\neVbGYLy\n\n\n3\n50889200399\n50889200399\nEva\nKarlsen\nO4jegSM\n\n\n4\n22919199052\n22919199052\nLars\nCarlsen\n6JhhRKi\n\n\n5\n66821775168\n66821775168\nØyvind\nNordmann\nvygqpLq\n\n\n6\n13824498614\n13824498614\nKenneth\nCarlsen\nJWzitL8\n\n\n7\n46927100797\n46927100797\nJohnny\nNordmann\noIhVngD\n\n\n8\n16907699157\n16907699157\nRupinder\nKarlsen\n0y8qqUZ\n\n\n9\n10920998203\n10920998203\nNicolas\nNormann\nFNnp5AL\n\n\n10\n99999999999\n99999999999\nMichael\nNorman\n4yI2BlkviaI\n\n\n11\nXX\nXX\nOla Glenn\nGåås\nXX\n\n\n12\nX8b7k28\nX8b7k28\nLars\nGaas\ntKHXmUl\n\n\n\n\n\n\n\n\nTabell 5 viser at depseudonymiseringen returnerer de opprinnelige fødselsnummerne.\nVidere kan man utforske å pseudonymisere navn ved bruk av ulike algoritmer."
  },
  {
    "objectID": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html",
    "href": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html",
    "title": "Ideen bak bloggen",
    "section": "",
    "text": "SSB-ere løser hele tiden problemer på nye måter som andre gjerne skulle nyttiggjort seg av. Spesielt når vi gjør en så stor overgang i arbeidsform som overgangen til en ny plattform (Dapla), og vi samtidig skifter mange verktøyene vi har i verktøykassen vår. Av den grunn har vi opprettet denne bloggen. Her vil vi skrive om hvordan vi løser problemer, hvilke verktøy vi bruker og hvordan vi bruker dem. Vi vil også skrive om hvordan vi jobber med å utvikle nye verktøy og hvordan vi jobber med å utvikle Dapla.\nMålsetningen med denne bloggen er at alle i SSB som ønsker å dele noe med andre kan skrive en artikkel og dele i bloggen. Mens Byrånettet er kanal for å dele informasjon med alle i SSB, og Viva Engage en kanal for å si det du tenker uten særlig noen formell struktur, er denne bloggen en kanal for å dele informasjon med andre som jobber med data og teknologi i SSB.\nFordelen med bloggen er at den er tilpasset hvordan statistikkere, forskere og IT-utviklere jobber til daglig. Artiklene kan skrives samme sted som man utvikler kode, og man inkludere output fra kodekjøringer i artikler."
  },
  {
    "objectID": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html#hvordan-skrive-en-artikkel",
    "href": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html#hvordan-skrive-en-artikkel",
    "title": "Ideen bak bloggen",
    "section": "Hvordan skrive en artikkel?",
    "text": "Hvordan skrive en artikkel?\nBloggen er generert med Quarto. Quarto er et rammeverk for å skrive artikler i markdown. Det er enkelt å komme i gang med Quarto, og det er enkelt å skrive artikler i Quarto.\nFor å skrive en artikkel gjør du følgende:\n\nSkriv artikkelen som en markdown-fil (.qmd-fil) eller en notebook (.ipynb-fil).\nKlon dapla-manual-internal repoet:\ngit clone https://github.com/statisticsnorway/dapla-manual-internal.git\nOpprett en mappe for artikkelen din i mappen ./dapla-manual-internal/blog/posts/. Gi mappen et navn som beskriver artikkelen din.\nInne mappen legger du din .qmd- eller .ipynb-fil. Eventuelle bilder i artikkelen legges også i samme mappe.\nOpprette en pull request på repoet og noen vil se over artikkelen din og publisere den."
  },
  {
    "objectID": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html#metadata-om-artikkelen",
    "href": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html#metadata-om-artikkelen",
    "title": "Ideen bak bloggen",
    "section": "Metadata om artikkelen",
    "text": "Metadata om artikkelen\nNår du skriver artikkelen så må du starte dokumentet med følgende metadata:\n\n\nindex.qmd\n\n---\ntitle: Ideen bak bloggen\nsubtitle: Hvorfor vi har opprettet denne bloggen? \ncategories:\n  - Quarto\nauthor:\n  - name: Øyvind Bruer-Skarsbø\n    affiliation: \n      - name: Seksjon for dataplattform (724)\n        email: obr@ssb.no\ndate: \"01/11/2024\"\ndate-modified: \"01/11/2024\"\nimage: ../../../images/dapla-long.png\nimage-alt: \"Bilde av Fame-logoen\"\ndraft: false\n---\n\nHusk å fylle ut alle feltene slik at det blir riktig informasjon for din artikkel. Skriver du en ipynb-fil så må metadataene ligge i en celle av typen raw.\nØnsker du å komme fort igang så kan se hvordan denne artikkelen ble skrevet."
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Eksempler",
    "section": "",
    "text": "Eksempler\nSe menyen til venstre for eksempler.",
    "crumbs": [
      "Eksempler"
    ]
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.out.html",
    "href": "notebooks/spark/sparkr-intro.out.html",
    "title": "Introduksjon til SparkR",
    "section": "",
    "text": "Akkurat som PySpark så gir SparkR oss et grensesnitt mot Apache Spark fra R. I denne notebooken viser vi noen eksempler hvordan du gjøre vanlige operasjoner med SparkR."
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.out.html#oppsett",
    "href": "notebooks/spark/sparkr-intro.out.html#oppsett",
    "title": "Introduksjon til SparkR",
    "section": "Oppsett",
    "text": "Oppsett\nEksemplene i notebooken bruker SparkR (k8s cluster) på https://jupyter.dapla.ssb.no/. Det vil si at den kan distribuere kjøringene på flere maskiner i Kubernetes.\n\nspark\n\nJava ref type org.apache.spark.sql.SparkSession id 1"
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.out.html#lese-inn-fil",
    "href": "notebooks/spark/sparkr-intro.out.html#lese-inn-fil",
    "title": "Introduksjon til SparkR",
    "section": "Lese inn fil",
    "text": "Lese inn fil\n\nfile = read.parquet(\"gs://ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet\")\n\n\nselectedColumns &lt;- select(file, \"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\")\nshowDF(selectedColumns, numRows = 5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html",
    "href": "notebooks/spark/pyspark-intro.out.html",
    "title": "Introduksjon til PySpark",
    "section": "",
    "text": "Apache Spark er et sterkt verktøy som utvider mulighetsrommet for databehandling med R og Python. Kjernefunksjonaliteten ligger i at den lar deg kjøre en jobb på flere maskiner samtidig, noe som ikke er mulig med klassiske rammeverk som Pandas og Tidyverse. Følgelig er det et rammeverk som blant annet er veldig egnet for å prosessere store datamengder eller gjøre store beregninger. Les om mer om Apache Spark i Dapla-manualen\nI denne notebooken vises noen enkle eksempler på hvordan du kan jobbe med data med PySpark, et Python-grensesnitt mot Spark."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html#oppsett",
    "href": "notebooks/spark/pyspark-intro.out.html#oppsett",
    "title": "Introduksjon til PySpark",
    "section": "Oppsett",
    "text": "Oppsett\nNår du logger deg inn på Dapla kan du velge mellom 2 ferdigoppsatte kernels for å jobbe med PySpark:\n\nPyspark (local)\nPyspark (k8s cluster)\n\nDen første lar deg bruke Spark på en enkeltmaskin, mens den andre lar deg distribuere kjøringen på mange maskiner avhengig av hvor store jobbene er. I eksemplene under brukes Pyspark (local).\n\n# Importer biblioteker\nimport numpy as np\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import date_format, explode, expr, sequence, sum, avg\nfrom pyspark.sql.types import DateType, DoubleType, StructField, StructType\n\n# Initialierer en SparkSession\nspark = (\n    SparkSession.builder.master(\"local[2]\")\n    .appName(\"Dapla-manual-example\")\n    .getOrCreate()\n)\n\nI koden over importerer vi de bibliotekene vi skal bruke under. Grunnen til at vi importerer pyspark.sql er at dette er at Spark SQL er Apache Spark sin modul for å jobbe med strukturerte data. Og som navnet tilsier vil det si at vi kan blande Python og SQL i koden vår. Vi skal nærmere på hvordan man bruke SQL fra PySpark-notebook senere.\nSpark tilbyr også et eget grensesnitt, Spark UI, for å monitorere hva som skjer under en SparkSession. Vi kan bruke følgende kommando for å få opp en lenke til Spark UI i notebooken vår:\n\nspark.sparkContext\n\n\n        \n            SparkContext\n\n            Spark UI\n\n            \n              Version\n                v3.3.1\n              Master\n                local[*]\n              AppName\n                pyspark-shell\n            \n        \n        \n\n\nKlikker du på Spark UI-lenken så tar den deg til dashboard som lar deg monitorere, debugge, optimalisere og forstå kjøringene dine. Det kan være et svært nyttig verktøy i mange tilfeller."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html#generere-data",
    "href": "notebooks/spark/pyspark-intro.out.html#generere-data",
    "title": "Introduksjon til PySpark",
    "section": "Generere data",
    "text": "Generere data\nVi kan begynne med å generere en Spark DataFrame med en kolonne som inneholder månedlige datoer for perioden 2000M1-2023M8.\n\n# Genererer månedlige data\ndates_df = spark.range(1).select(\n    explode(\n        sequence(\n            start=expr(\"date '2000-01-01'\"),\n            stop=expr(\"date '2023-08-01'\"),\n            step=expr(\"interval 1 month\"),\n        )\n    ).alias(\"Date\")\n)\ndates_df.show(5)\n\n+----------+\n|      Date|\n+----------+\n|2000-01-01|\n|2000-02-01|\n|2000-03-01|\n|2000-04-01|\n|2000-05-01|\n+----------+\nonly showing top 5 rows\n\n\nEn Spark DataFrame er en distribuert samling av data som er organisert inn i kolonner. Siden Spark lar deg distribuere kjøringer på flere maskiner, er DataFrames optimalisert for å kunne splittes opp slik at de kan brukes på flere maskiner. Med andre er dette ikke det samme som en Pandas dataframe mange kjenner fra før.\nOver genererte vi en datokolonne. For å få litt mer data kan vi også generere 100 kolonner med tidsseriedata og så printer vi de 2 første av disse:\n\n# Genererer random walk data\nschema = StructType(\n    [StructField(f\"serie{i:02d}\", DoubleType(), True) for i in range(100)]\n)\n\ndata = [\n    tuple((10 + np.random.normal(0, 1, 100)).cumsum().tolist())\n    for _ in range(284)  # 284 months from 2000-01 to 2023-08\n]\n\ndata_df = spark.createDataFrame(data, schema=schema)\n\ndata_df.select(\"serie00\", \"serie01\").show(5)\n\n+------------------+------------------+\n|           serie00|           serie01|\n+------------------+------------------+\n|10.410703377184355| 21.06318801110079|\n|10.509249410154466|  19.5674295298024|\n| 9.618310122060274|17.635805093465642|\n| 9.691112692298294|18.593842915949082|\n| 9.903675228685067|20.012215769058564|\n+------------------+------------------+\nonly showing top 5 rows\n\n\nTil slutt kan vi joine de to datasettene sammen og lage noen kolonner som viser år, kvartal og måned. Deretter printer vi ut noen av kolonnene med kommandoen show().\n\n# Legger til row index til DataFrame før join med dates_df\ndata_df = data_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n\n# Joiner de to datasettene\ndf = (\n    dates_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n    .join(data_df, \"row_index\")\n    .drop(\"row_index\")\n)\n\n# Legger til år, kvartal og mnd\ndf = df.withColumn(\"Year\", date_format(df.Date, \"yyyy\"))\ndf = df.withColumn(\"Quarter\", expr(\"quarter(Date)\"))\ndf = df.withColumn(\"Month\", date_format(df.Date, \"MM\"))\n\ndf.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows\n\n\nOg med det har vi noe data vi kan jobbe med i resten av notebooken."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html#skrive-til-parquet",
    "href": "notebooks/spark/pyspark-intro.out.html#skrive-til-parquet",
    "title": "Introduksjon til PySpark",
    "section": "Skrive til Parquet",
    "text": "Skrive til Parquet\nPySpark tilbyr mange opsjoner ved skriving til parquet-filer som vi kanskje ikke er vant til å forholde oss til med enklere rammeverk som Pandas. Den enkleste måten å skrive ut en fil er som følger:\ndf.write.parquet(\n    \"gs://ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet\"\n)\nDette vil fungere hvis filen ikke finnes fra før. Hvis den finnes fra før så vil den feile. Grunnen er at vi ikke har spesifisert hva vi ønsker at den skal gjøre. Vi kan velge mellom overwrite, append, ignore eller errorifexists. Sistnevnte er også default-oppførsel hvis du ikke ber den gjøre noe annet.\nUnder bruker vi opsjonen overwrite, det vil si at den skriver over en evt eksisterende fil med samme navn.\n\ndf.write.mode(\"overwrite\").parquet(\n    \"gs://ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet\"\n)\n\nVi kan inspisere hva som ble skrevet ved å liste ut innholder i bøtta.\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/temp/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp/',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/_SUCCESS',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/part-00000-b32e7299-0590-4b31-bcc2-dc3d58725529-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/part-00001-b32e7299-0590-4b31-bcc2-dc3d58725529-c000.snappy.parquet']\n\n\nHvis denne parquet-filen hadde vært partisjonert etter en kolonne, så ville det vært egne undermapper med navnestruktur column_name=value som indikerte hva filen er partisjonert på. Siden vi her bruker en maskin og har et lite datasett, valgte Spark å ikke partisjonere."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html#lese-inn-parquet",
    "href": "notebooks/spark/pyspark-intro.out.html#lese-inn-parquet",
    "title": "Introduksjon til PySpark",
    "section": "Lese inn Parquet",
    "text": "Lese inn Parquet\nApache Spark kan lese inn flere parquet-filer samtidig. Syntaxen er like enkel som den for å skrive ut.\n\ndf_ts = spark.read.parquet(\n    \"gs://ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet\"\n)\ndf_ts.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie66\", \"serie55\").show(5)\n\n+----------+----+-------+-----+-----------------+-----------------+\n|      Date|Year|Quarter|Month|          serie66|          serie55|\n+----------+----+-------+-----+-----------------+-----------------+\n|2000-01-01|2000|      1|   01|670.2679830025959|562.4312808525777|\n|2000-02-01|2000|      1|   02|675.4233411662802|562.5168447360121|\n|2000-03-01|2000|      1|   03|687.3412458214908|568.6203957584232|\n|2000-04-01|2000|      2|   04|673.1128047244955|557.4633871253379|\n|2000-05-01|2000|      2|   05| 667.513406101114|561.7766450346327|\n+----------+----+-------+-----+-----------------+-----------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html#pyspark-og-sql",
    "href": "notebooks/spark/pyspark-intro.out.html#pyspark-og-sql",
    "title": "Introduksjon til PySpark",
    "section": "PySpark og SQL",
    "text": "PySpark og SQL\nDu kan også skrive SQL med Spark. For å skrive SQL må vi først lage et temporary view. Under kaller vi viewt for tidsserie.\n\ndf.createOrReplaceTempView(\"tidsserie\")\n\nVi kan deretter skrive en SQL-statement som vi ønsker å kjøre på viewet:\n\nquery = \"SELECT * FROM tidsserie WHERE Year = 2010\"\n\nDeretter kan vi bruke det til å filtrere datasettet:\n\nresult_df = spark.sql(query)\nresult_df.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2010-01-01|2010|      1|   01| 11.26910423907778|21.730128215168268|\n|2010-02-01|2010|      1|   02| 8.722783282690738| 17.46851086792347|\n|2010-03-01|2010|      1|   03|10.376873608348605|20.109386343182802|\n|2010-04-01|2010|      2|   04|11.459959305590926|21.995141825523866|\n|2010-05-01|2010|      2|   05|10.441456792180572| 21.25096473981906|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html#aggregering",
    "href": "notebooks/spark/pyspark-intro.out.html#aggregering",
    "title": "Introduksjon til PySpark",
    "section": "Aggregering",
    "text": "Aggregering\nvi kan aggregere opp enten med PySpark-syntax eller SQL-syntax. Under viser vi begge:\n\nfrom pyspark.sql import functions as F\n\n# Assuming df_ts is your DataFrame\naggregated_df = df_ts.groupBy(\"Quarter\").agg(F.sum(\"serie00\").alias(\"Sum\"),\n                                             F.avg(\"serie00\").alias(\"Average\"),\n                                             F.max(\"serie00\").alias(\"Maximum\"))\n\n# Show the result\naggregated_df.show()\n\n+-------+------------------+------------------+------------------+\n|Quarter|               Sum|           Average|           Maximum|\n+-------+------------------+------------------+------------------+\n|      1|363.95869885234185|10.109963857009497|11.829453550532005|\n|      3|365.68324879453405| 10.15786802207039|12.233378837422391|\n|      4| 342.2334082209804|10.065688477087658|12.210138970053695|\n|      2|  361.991445506568|10.055317930738001|12.276030776082463|\n+-------+------------------+------------------+------------------+\n\n\nLa oss gjøre det samme med SQL, men grupperer etter to variabler og sorterer output etterpå.\n\n# Assuming df_ts is your DataFrame\ndf_ts.createOrReplaceTempView(\"temp_table\")\n\n# Now you can run a SQL query\nquery = \"\"\"\n    SELECT\n        Year,\n        Quarter,\n        SUM(serie00) AS Sum,\n        AVG(serie00) AS Average,\n        MAX(serie00) AS Maximum\n    FROM \n        temp_table\n    GROUP BY \n        Year, Quarter\n    ORDER BY\n        YEAR, QUARTER\n\"\"\"\n\naggregated_df_sql = spark.sql(query)\n\n# Show the result\naggregated_df_sql.show(10)\n\n+----+-------+------------------+------------------+------------------+\n|Year|Quarter|               Sum|           Average|           Maximum|\n+----+-------+------------------+------------------+------------------+\n|2000|      1|31.323050432219453|10.441016810739818|11.118293927071951|\n|2000|      2|28.911192473027377| 9.637064157675793| 9.900977410685329|\n|2000|      3|33.670797229797415|11.223599076599138|12.233378837422391|\n|2000|      4|28.094793356286914| 9.364931118762305| 10.32000478359274|\n|2001|      1|31.636678535169537|10.545559511723178|11.367822302191831|\n|2001|      2|29.629770128521507| 9.876590042840503|11.135215930381191|\n|2001|      3| 30.75408440118315| 10.25136146706105|10.723803326978505|\n|2001|      4|30.361048932627902|  10.1203496442093|10.368365984482093|\n|2002|      1|31.184163218551227|10.394721072850409|10.550579652234951|\n|2002|      2|29.128978392451202|   9.7096594641504|10.186365745367246|\n+----+-------+------------------+------------------+------------------+\nonly showing top 10 rows"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html",
    "href": "notebooks/spark/pyspark-intro.html",
    "title": "Introduksjon til PySpark",
    "section": "",
    "text": "Apache Spark er et sterkt verktøy som utvider mulighetsrommet for databehandling med R og Python. Kjernefunksjonaliteten ligger i at den lar deg kjøre en jobb på flere maskiner samtidig, noe som ikke er mulig med klassiske rammeverk som Pandas og Tidyverse. Følgelig er det et rammeverk som blant annet er veldig egnet for å prosessere store datamengder eller gjøre store beregninger. Les om mer om Apache Spark i Dapla-manualen\nI denne notebooken vises noen enkle eksempler på hvordan du kan jobbe med data med PySpark, et Python-grensesnitt mot Spark."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#oppsett",
    "href": "notebooks/spark/pyspark-intro.html#oppsett",
    "title": "Introduksjon til PySpark",
    "section": "Oppsett",
    "text": "Oppsett\nNår du logger deg inn på Dapla kan du velge mellom 2 ferdigoppsatte kernels for å jobbe med PySpark:\n\nPyspark (local)\nPyspark (k8s cluster)\n\nDen første lar deg bruke Spark på en enkeltmaskin, mens den andre lar deg distribuere kjøringen på mange maskiner avhengig av hvor store jobbene er. I eksemplene under brukes Pyspark (local).\n\n# Importer biblioteker\nimport numpy as np\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import date_format, explode, expr, sequence, sum, avg\nfrom pyspark.sql.types import DateType, DoubleType, StructField, StructType\n\n# Initialierer en SparkSession\nspark = (\n    SparkSession.builder.master(\"local[2]\")\n    .appName(\"Dapla-manual-example\")\n    .getOrCreate()\n)\n\nI koden over importerer vi de bibliotekene vi skal bruke under. Grunnen til at vi importerer pyspark.sql er at dette er at Spark SQL er Apache Spark sin modul for å jobbe med strukturerte data. Og som navnet tilsier vil det si at vi kan blande Python og SQL i koden vår. Vi skal nærmere på hvordan man bruke SQL fra PySpark-notebook senere.\nSpark tilbyr også et eget grensesnitt, Spark UI, for å monitorere hva som skjer under en SparkSession. Vi kan bruke følgende kommando for å få opp en lenke til Spark UI i notebooken vår:\n\nspark.sparkContext\n\n\n        \n            SparkContext\n\n            Spark UI\n\n            \n              Version\n                v3.3.1\n              Master\n                local[*]\n              AppName\n                pyspark-shell\n            \n        \n        \n\n\nKlikker du på Spark UI-lenken så tar den deg til dashboard som lar deg monitorere, debugge, optimalisere og forstå kjøringene dine. Det kan være et svært nyttig verktøy i mange tilfeller."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#generere-data",
    "href": "notebooks/spark/pyspark-intro.html#generere-data",
    "title": "Introduksjon til PySpark",
    "section": "Generere data",
    "text": "Generere data\nVi kan begynne med å generere en Spark DataFrame med en kolonne som inneholder månedlige datoer for perioden 2000M1-2023M8.\n\n# Genererer månedlige data\ndates_df = spark.range(1).select(\n    explode(\n        sequence(\n            start=expr(\"date '2000-01-01'\"),\n            stop=expr(\"date '2023-08-01'\"),\n            step=expr(\"interval 1 month\"),\n        )\n    ).alias(\"Date\")\n)\ndates_df.show(5)\n\n+----------+\n|      Date|\n+----------+\n|2000-01-01|\n|2000-02-01|\n|2000-03-01|\n|2000-04-01|\n|2000-05-01|\n+----------+\nonly showing top 5 rows\n\n\n\nEn Spark DataFrame er en distribuert samling av data som er organisert inn i kolonner. Siden Spark lar deg distribuere kjøringer på flere maskiner, er DataFrames optimalisert for å kunne splittes opp slik at de kan brukes på flere maskiner. Med andre er dette ikke det samme som en Pandas dataframe mange kjenner fra før.\nOver genererte vi en datokolonne. For å få litt mer data kan vi også generere 100 kolonner med tidsseriedata og så printer vi de 2 første av disse:\n\n# Genererer random walk data\nschema = StructType(\n    [StructField(f\"serie{i:02d}\", DoubleType(), True) for i in range(100)]\n)\n\ndata = [\n    tuple((10 + np.random.normal(0, 1, 100)).cumsum().tolist())\n    for _ in range(284)  # 284 months from 2000-01 to 2023-08\n]\n\ndata_df = spark.createDataFrame(data, schema=schema)\n\ndata_df.select(\"serie00\", \"serie01\").show(5)\n\n+------------------+------------------+\n|           serie00|           serie01|\n+------------------+------------------+\n|10.410703377184355| 21.06318801110079|\n|10.509249410154466|  19.5674295298024|\n| 9.618310122060274|17.635805093465642|\n| 9.691112692298294|18.593842915949082|\n| 9.903675228685067|20.012215769058564|\n+------------------+------------------+\nonly showing top 5 rows\n\n\n\nTil slutt kan vi joine de to datasettene sammen og lage noen kolonner som viser år, kvartal og måned. Deretter printer vi ut noen av kolonnene med kommandoen show().\n\n\nCode\n# Legger til row index til DataFrame før join med dates_df\ndata_df = data_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n\n# Joiner de to datasettene\ndf = (\n    dates_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n    .join(data_df, \"row_index\")\n    .drop(\"row_index\")\n)\n\n# Legger til år, kvartal og mnd\ndf = df.withColumn(\"Year\", date_format(df.Date, \"yyyy\"))\ndf = df.withColumn(\"Quarter\", expr(\"quarter(Date)\"))\ndf = df.withColumn(\"Month\", date_format(df.Date, \"MM\"))\n\ndf.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows\n\n\n\nOg med det har vi noe data vi kan jobbe med i resten av notebooken."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#skrive-til-parquet",
    "href": "notebooks/spark/pyspark-intro.html#skrive-til-parquet",
    "title": "Introduksjon til PySpark",
    "section": "Skrive til Parquet",
    "text": "Skrive til Parquet\nPySpark tilbyr mange opsjoner ved skriving til parquet-filer som vi kanskje ikke er vant til å forholde oss til med enklere rammeverk som Pandas. Den enkleste måten å skrive ut en fil er som følger:\ndf.write.parquet(\n    \"gs://ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet\"\n)\nDette vil fungere hvis filen ikke finnes fra før. Hvis den finnes fra før så vil den feile. Grunnen er at vi ikke har spesifisert hva vi ønsker at den skal gjøre. Vi kan velge mellom overwrite, append, ignore eller errorifexists. Sistnevnte er også default-oppførsel hvis du ikke ber den gjøre noe annet.\nUnder bruker vi opsjonen overwrite, det vil si at den skriver over en evt eksisterende fil med samme navn.\n\ndf.write.mode(\"overwrite\").parquet(\n    \"gs://ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet\"\n)\n\nVi kan inspisere hva som ble skrevet ved å liste ut innholder i bøtta.\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/temp/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp/',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/_SUCCESS',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/part-00000-b32e7299-0590-4b31-bcc2-dc3d58725529-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/part-00001-b32e7299-0590-4b31-bcc2-dc3d58725529-c000.snappy.parquet']\n\n\nHvis denne parquet-filen hadde vært partisjonert etter en kolonne, så ville det vært egne undermapper med navnestruktur column_name=value som indikerte hva filen er partisjonert på. Siden vi her bruker en maskin og har et lite datasett, valgte Spark å ikke partisjonere."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#lese-inn-parquet",
    "href": "notebooks/spark/pyspark-intro.html#lese-inn-parquet",
    "title": "Introduksjon til PySpark",
    "section": "Lese inn Parquet",
    "text": "Lese inn Parquet\nApache Spark kan lese inn flere parquet-filer samtidig. Syntaxen er like enkel som den for å skrive ut.\n\ndf_ts = spark.read.parquet(\n    \"gs://ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet\"\n)\ndf_ts.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie66\", \"serie55\").show(5)\n\n+----------+----+-------+-----+-----------------+-----------------+\n|      Date|Year|Quarter|Month|          serie66|          serie55|\n+----------+----+-------+-----+-----------------+-----------------+\n|2000-01-01|2000|      1|   01|670.2679830025959|562.4312808525777|\n|2000-02-01|2000|      1|   02|675.4233411662802|562.5168447360121|\n|2000-03-01|2000|      1|   03|687.3412458214908|568.6203957584232|\n|2000-04-01|2000|      2|   04|673.1128047244955|557.4633871253379|\n|2000-05-01|2000|      2|   05| 667.513406101114|561.7766450346327|\n+----------+----+-------+-----+-----------------+-----------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#pyspark-og-sql",
    "href": "notebooks/spark/pyspark-intro.html#pyspark-og-sql",
    "title": "Introduksjon til PySpark",
    "section": "PySpark og SQL",
    "text": "PySpark og SQL\nDu kan også skrive SQL med Spark. For å skrive SQL må vi først lage et temporary view. Under kaller vi viewt for tidsserie.\n\ndf.createOrReplaceTempView(\"tidsserie\")\n\nVi kan deretter skrive en SQL-statement som vi ønsker å kjøre på viewet:\n\nquery = \"SELECT * FROM tidsserie WHERE Year = 2010\"\n\nDeretter kan vi bruke det til å filtrere datasettet:\n\nresult_df = spark.sql(query)\nresult_df.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2010-01-01|2010|      1|   01| 11.26910423907778|21.730128215168268|\n|2010-02-01|2010|      1|   02| 8.722783282690738| 17.46851086792347|\n|2010-03-01|2010|      1|   03|10.376873608348605|20.109386343182802|\n|2010-04-01|2010|      2|   04|11.459959305590926|21.995141825523866|\n|2010-05-01|2010|      2|   05|10.441456792180572| 21.25096473981906|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#aggregering",
    "href": "notebooks/spark/pyspark-intro.html#aggregering",
    "title": "Introduksjon til PySpark",
    "section": "Aggregering",
    "text": "Aggregering\nvi kan aggregere opp enten med PySpark-syntax eller SQL-syntax. Under viser vi begge:\n\nfrom pyspark.sql import functions as F\n\n# Assuming df_ts is your DataFrame\naggregated_df = df_ts.groupBy(\"Quarter\").agg(F.sum(\"serie00\").alias(\"Sum\"),\n                                             F.avg(\"serie00\").alias(\"Average\"),\n                                             F.max(\"serie00\").alias(\"Maximum\"))\n\n# Show the result\naggregated_df.show()\n\n+-------+------------------+------------------+------------------+\n|Quarter|               Sum|           Average|           Maximum|\n+-------+------------------+------------------+------------------+\n|      1|363.95869885234185|10.109963857009497|11.829453550532005|\n|      3|365.68324879453405| 10.15786802207039|12.233378837422391|\n|      4| 342.2334082209804|10.065688477087658|12.210138970053695|\n|      2|  361.991445506568|10.055317930738001|12.276030776082463|\n+-------+------------------+------------------+------------------+\n\n\n\nLa oss gjøre det samme med SQL, men grupperer etter to variabler og sorterer output etterpå.\n\n# Assuming df_ts is your DataFrame\ndf_ts.createOrReplaceTempView(\"temp_table\")\n\n# Now you can run a SQL query\nquery = \"\"\"\n    SELECT\n        Year,\n        Quarter,\n        SUM(serie00) AS Sum,\n        AVG(serie00) AS Average,\n        MAX(serie00) AS Maximum\n    FROM \n        temp_table\n    GROUP BY \n        Year, Quarter\n    ORDER BY\n        YEAR, QUARTER\n\"\"\"\n\naggregated_df_sql = spark.sql(query)\n\n# Show the result\naggregated_df_sql.show(10)\n\n+----+-------+------------------+------------------+------------------+\n|Year|Quarter|               Sum|           Average|           Maximum|\n+----+-------+------------------+------------------+------------------+\n|2000|      1|31.323050432219453|10.441016810739818|11.118293927071951|\n|2000|      2|28.911192473027377| 9.637064157675793| 9.900977410685329|\n|2000|      3|33.670797229797415|11.223599076599138|12.233378837422391|\n|2000|      4|28.094793356286914| 9.364931118762305| 10.32000478359274|\n|2001|      1|31.636678535169537|10.545559511723178|11.367822302191831|\n|2001|      2|29.629770128521507| 9.876590042840503|11.135215930381191|\n|2001|      3| 30.75408440118315| 10.25136146706105|10.723803326978505|\n|2001|      4|30.361048932627902|  10.1203496442093|10.368365984482093|\n|2002|      1|31.184163218551227|10.394721072850409|10.550579652234951|\n|2002|      2|29.128978392451202|   9.7096594641504|10.186365745367246|\n+----+-------+------------------+------------------+------------------+\nonly showing top 10 rows"
  },
  {
    "objectID": "statistikkere/kildedata-prosessering.html",
    "href": "statistikkere/kildedata-prosessering.html",
    "title": "Kildedata prosessering",
    "section": "",
    "text": "Denne tjenesten er under utvikling og kan ikke anses som klar for produksjon.\n\n\n\nFor å minske aksessering av PII1, oppfordres alle team på Dapla å benytte seg av automatisering av kildedata prosessering. Automatisering av kildedata er en tjeneste som er tilgjengelig for team å ta i bruk 100% selv-betjent. Kildedata (Standardutvalget 2021, 5) prosesseres til inndata gjennom et bestemt utvalg av operasjoner. Kildedata prosesseres som individuelle filer for å holde oppsettet enkelt og målrettet mot de definerte operasjoner. Mer kompleks operasjoner som går på tvers av flere filer burde utføres på inndata eller senere datatilstander.\n\n\n\n\n\n\nDet er kun teamets kildedataansvarlige som skal aksessere kildedata.\n\n\n\n\n\n\n\n\n\nTeamets kildedataansvarlige tar ansvar for å prosessere kildedata til inndata på en forsvarlig måte.\n\n\n\n\n\n\n\n\n\n\n\nFigur 1: Operasjoner som inngår i kildedata prosessering\n\n\n\n\nInndata er kildedata som er transformert til SSBs standard lagringsformat. Variabelnavn og -innhold er uendret bortsett fra at\n\ndirekte identifiserende variabler (f.eks. fødselsnummer) er pseudonymisert\ntegnsett, datoformat, adresse mm er endret til SSBs standardformat\ndet benyttes standard kodeverk (Klass) der det er mulig (f.eks. kjønn)\ndataene er minimert slik at kun variablene som er nødvendige i den videre produksjonsprosessen, inngår.\n\n\n(Standardutvalget 2021, 8)\nDet er ikke anbefalt å gjennomføre operasjoner som:\n\nGår på tvers av flere filer\nLegge til nye felt\nEndre navn på felt\nAggregerer data\nosv.\n\n\n\n\n\n\nFølg instruksjonene her for å koble prosjektet ditt til Github.\n\n\n\n\n\nKilder konfigureres i et teams Infrastructure as Code (IaC) repo på Github. Det kan finnes basert på følgende formulering: github.com/statisticsnorway/&lt;teamnavn&gt;-iac. Kilder konfigureres under stien automation/source_data på repoet.\n\n\n\nHver kilde konfigureres ved hjelp av to filer:\n\nconfig.yaml som blant annet konfigurer hvilke stier i teamets kildedatabøtte prosesseres.\nprocess_source_data.py som kjøres når en kildedatafil prosesseres. Her må man skrive en python funksjon på en viss format.\n\nDisse filene er lagt til en mappe per kilde, under automation/source_data i IaC repoet. Se eksemplet under for en detaljert forklaring.\n\n\n\n\n\n\nMappenavnet for kilder i IaC repoene er brukt som navn for ressurser. Dette i praksis betyr at det enesete tillatte tegnene i mappenavnet er bokstaver, tall og bindestrek. Det er ikke tillatt med mellomrom eller andre spesialtegn. Mappenavnet kan bestå av opptil 20 tegn.\n\n\n\n\n\n\nDette går ut på om prosesseringsscriptet kan enkelt håndtere variasjonen i filene som samles inn.\nGrunn til å opprette en ny kilde kan være: - Kildedatafilen har en annen format (f.eks xml eller json) - Kildedataen har ulike felter - Kildedataen inneholder PII2 eller ikke\n\n\n\n\n\n\n\n\n\nDisse instruksjoner forutsetter at prosjektet ditt er koblet til Github allerede.\n\n\n\n\nSkrive skriptet process_source_data.py som prosesserer kildedatafilen til inndata. Dette kan testes av kildedataansvarlige manuelt på Jupyter for å verifisere at dataene blir prosessert som ønsket.\nI en branch i teamets IaC repo, legge til config.yaml og process_source_data.py i en mappe under automation/source_data. Se eksemplet under for en detaljert forklaring av formatet.\nLag en PR på grenen og få den godkjent av kildedataansvarlige.\nVent til alle tester er ferdige. Det skal stå “All checks have passed” før man går videre, hvis testene feiler følg stegene her. \nSkrive atlantis apply i en kommentar på PRen for å opprette det nødvendige infrastruktur for å prosessere kilden.\nMerge PRen.\nSjekk resultatet av det automatiske bygget.\nVerifisere at nye filer lagt i kildedatabøtten blir prosessert til inndata som forventet.\n\n\n\n\n\nLa oss si at et team (småbakst) har to datakilder levert av ulik dataeiere på ulik formater. Den ene er om boller og er på csv format og den andre er om rundstykker og er på json format. Kildedataansvarlige i teamet bestemmer seg for at filene i boller/ er like nok å prosesseres som en kilde, og at filene i rundstykker/ kan prosesseres som en annen kilde.\n\n\nssb-prod-smaabakst-data-kilde\n├── boller\n│   ├── hvetebolle\n│   │   ├── 2018-salg.csv\n│   │   ├── 2019-salg.csv\n│   │   ├── ...\n│   ├── kanelbolle\n│   │   ├── 2018-salg.csv\n│   │   ├── 2019-salg.csv\n│   │   ├── ...\n│   └── skolebolle\n│       ├── 2018-salg.csv\n│       ├── 2019-salg.csv\n│       ├── ...\n└── rundstykker\n    ├── haandverker\n    │   ├── apr-2022-resultater.json\n    │   ├── aug-2022-resultater.json\n    │   ├── ...\n    └── havre\n        ├── apr-2022-resultater.json\n        ├── aug-2022-resultater.json\n        ├── ...\n\n\n\nsmaabakst-iac\n└── automation\n    └── source_data\n        ├── boller\n        │   ├── config.yaml\n        │   └── process_source_data.py\n        └── rundstykker\n            ├── config.yaml\n            └── process_source_data.py\n\n\n\n\n\nconfig.yaml\n\nfolder_prefix: boller\n\nVerdien for folder_prefix tilsvarer en “fil sti” i kildedatabøtte. I dette tilfellet vil nye filer lagt til under boller/ trigge en prosessering.\n\n\n\n\n\nconfig.yaml\n\nfolder_prefix: rundstykker\n\n\n\n\n\n\nMed prosesseringsscriptet mener vi filen process_source_data.py som ligger i en mappe per kilde under automation/source_data. Dette scriptet vil bli kalt hver gang det blir lagt til en ny fil i kildedatabøtten. Metodesignaturen ser slik ut:\n\n\nprocess_source_data.py\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n\nDette betyr at hvis f.eks. 10 filer blir lagt til i kildedatabøtten samtidig så vil det startes opp 10 individuelle Python-prosesser som kaller denne main-metoden med forskjellig file_path. Parameteren file_path vil inneholde hele filstien inkl. filnavn. Så en enkel flytteoperasjon fra kildedatabøtten til inndatebøtten (uten noen form for konvertering) vil kunne uttrykkes slik:\n\n\nprocess_source_data.py\n\nimport dapla as dp\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n    source_bucket_name = \"ssb-prod-my-project-data-kilde\"\n    destination_bucket_name = \"ssb-prod-my-project-data-produkt\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    df = dp.read_pandas(file_path)\n    dp.write_pandas(df, destination_bucket_path)\n\nAlternativt…\n\n\nprocess_source_data.py\n\nfrom dapla import FileClient\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n    source_bucket_name = \"ssb-prod-my-project-data-kilde\"\n    destination_bucket_name = \"ssb-prod-my-project-data-produkt\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    fs = FileClient.get_gcs_file_system()\n    fs.copy(file_path, destination_path)\n\n\n\nDet anbefales å bruke Pythons logging modul for å logge, og ikke bruke print eller skrive til stdout/stderr. Det er satt opp en standard logger-konfigurasjon som skriver informasjonsmeldinger (log level info) til stdout og feilmeldinger (log level warning eller error) til stderr. Feil som ikke blir håndtert blir automatisk fanget opp og logget av automatiseringsløsningen. Eksemplet nedenfor logger en informasjonsmelding, en advarsel og en feilmelding:\n\n\nprocess_source_data.py\n\nimport logging\nlogging.info('Til info')\nlogging.warning('Advarsel!')\nlogging.error('En feil oppstod!')"
  },
  {
    "objectID": "statistikkere/kildedata-prosessering.html#operasjoner-som-inngår-i-kildedata-prosessering",
    "href": "statistikkere/kildedata-prosessering.html#operasjoner-som-inngår-i-kildedata-prosessering",
    "title": "Kildedata prosessering",
    "section": "",
    "text": "Figur 1: Operasjoner som inngår i kildedata prosessering\n\n\n\n\nInndata er kildedata som er transformert til SSBs standard lagringsformat. Variabelnavn og -innhold er uendret bortsett fra at\n\ndirekte identifiserende variabler (f.eks. fødselsnummer) er pseudonymisert\ntegnsett, datoformat, adresse mm er endret til SSBs standardformat\ndet benyttes standard kodeverk (Klass) der det er mulig (f.eks. kjønn)\ndataene er minimert slik at kun variablene som er nødvendige i den videre produksjonsprosessen, inngår.\n\n\n(Standardutvalget 2021, 8)\nDet er ikke anbefalt å gjennomføre operasjoner som:\n\nGår på tvers av flere filer\nLegge til nye felt\nEndre navn på felt\nAggregerer data\nosv."
  },
  {
    "objectID": "statistikkere/kildedata-prosessering.html#ta-tjenesten-i-bruk",
    "href": "statistikkere/kildedata-prosessering.html#ta-tjenesten-i-bruk",
    "title": "Kildedata prosessering",
    "section": "",
    "text": "Følg instruksjonene her for å koble prosjektet ditt til Github.\n\n\n\n\n\nKilder konfigureres i et teams Infrastructure as Code (IaC) repo på Github. Det kan finnes basert på følgende formulering: github.com/statisticsnorway/&lt;teamnavn&gt;-iac. Kilder konfigureres under stien automation/source_data på repoet.\n\n\n\nHver kilde konfigureres ved hjelp av to filer:\n\nconfig.yaml som blant annet konfigurer hvilke stier i teamets kildedatabøtte prosesseres.\nprocess_source_data.py som kjøres når en kildedatafil prosesseres. Her må man skrive en python funksjon på en viss format.\n\nDisse filene er lagt til en mappe per kilde, under automation/source_data i IaC repoet. Se eksemplet under for en detaljert forklaring.\n\n\n\n\n\n\nMappenavnet for kilder i IaC repoene er brukt som navn for ressurser. Dette i praksis betyr at det enesete tillatte tegnene i mappenavnet er bokstaver, tall og bindestrek. Det er ikke tillatt med mellomrom eller andre spesialtegn. Mappenavnet kan bestå av opptil 20 tegn.\n\n\n\n\n\n\nDette går ut på om prosesseringsscriptet kan enkelt håndtere variasjonen i filene som samles inn.\nGrunn til å opprette en ny kilde kan være: - Kildedatafilen har en annen format (f.eks xml eller json) - Kildedataen har ulike felter - Kildedataen inneholder PII2 eller ikke\n\n\n\n\n\n\n\n\n\nDisse instruksjoner forutsetter at prosjektet ditt er koblet til Github allerede.\n\n\n\n\nSkrive skriptet process_source_data.py som prosesserer kildedatafilen til inndata. Dette kan testes av kildedataansvarlige manuelt på Jupyter for å verifisere at dataene blir prosessert som ønsket.\nI en branch i teamets IaC repo, legge til config.yaml og process_source_data.py i en mappe under automation/source_data. Se eksemplet under for en detaljert forklaring av formatet.\nLag en PR på grenen og få den godkjent av kildedataansvarlige.\nVent til alle tester er ferdige. Det skal stå “All checks have passed” før man går videre, hvis testene feiler følg stegene her. \nSkrive atlantis apply i en kommentar på PRen for å opprette det nødvendige infrastruktur for å prosessere kilden.\nMerge PRen.\nSjekk resultatet av det automatiske bygget.\nVerifisere at nye filer lagt i kildedatabøtten blir prosessert til inndata som forventet.\n\n\n\n\n\nLa oss si at et team (småbakst) har to datakilder levert av ulik dataeiere på ulik formater. Den ene er om boller og er på csv format og den andre er om rundstykker og er på json format. Kildedataansvarlige i teamet bestemmer seg for at filene i boller/ er like nok å prosesseres som en kilde, og at filene i rundstykker/ kan prosesseres som en annen kilde.\n\n\nssb-prod-smaabakst-data-kilde\n├── boller\n│   ├── hvetebolle\n│   │   ├── 2018-salg.csv\n│   │   ├── 2019-salg.csv\n│   │   ├── ...\n│   ├── kanelbolle\n│   │   ├── 2018-salg.csv\n│   │   ├── 2019-salg.csv\n│   │   ├── ...\n│   └── skolebolle\n│       ├── 2018-salg.csv\n│       ├── 2019-salg.csv\n│       ├── ...\n└── rundstykker\n    ├── haandverker\n    │   ├── apr-2022-resultater.json\n    │   ├── aug-2022-resultater.json\n    │   ├── ...\n    └── havre\n        ├── apr-2022-resultater.json\n        ├── aug-2022-resultater.json\n        ├── ...\n\n\n\nsmaabakst-iac\n└── automation\n    └── source_data\n        ├── boller\n        │   ├── config.yaml\n        │   └── process_source_data.py\n        └── rundstykker\n            ├── config.yaml\n            └── process_source_data.py\n\n\n\n\n\nconfig.yaml\n\nfolder_prefix: boller\n\nVerdien for folder_prefix tilsvarer en “fil sti” i kildedatabøtte. I dette tilfellet vil nye filer lagt til under boller/ trigge en prosessering.\n\n\n\n\n\nconfig.yaml\n\nfolder_prefix: rundstykker"
  },
  {
    "objectID": "statistikkere/kildedata-prosessering.html#skrive-prosesseringsscriptet",
    "href": "statistikkere/kildedata-prosessering.html#skrive-prosesseringsscriptet",
    "title": "Kildedata prosessering",
    "section": "",
    "text": "Med prosesseringsscriptet mener vi filen process_source_data.py som ligger i en mappe per kilde under automation/source_data. Dette scriptet vil bli kalt hver gang det blir lagt til en ny fil i kildedatabøtten. Metodesignaturen ser slik ut:\n\n\nprocess_source_data.py\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n\nDette betyr at hvis f.eks. 10 filer blir lagt til i kildedatabøtten samtidig så vil det startes opp 10 individuelle Python-prosesser som kaller denne main-metoden med forskjellig file_path. Parameteren file_path vil inneholde hele filstien inkl. filnavn. Så en enkel flytteoperasjon fra kildedatabøtten til inndatebøtten (uten noen form for konvertering) vil kunne uttrykkes slik:\n\n\nprocess_source_data.py\n\nimport dapla as dp\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n    source_bucket_name = \"ssb-prod-my-project-data-kilde\"\n    destination_bucket_name = \"ssb-prod-my-project-data-produkt\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    df = dp.read_pandas(file_path)\n    dp.write_pandas(df, destination_bucket_path)\n\nAlternativt…\n\n\nprocess_source_data.py\n\nfrom dapla import FileClient\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n    source_bucket_name = \"ssb-prod-my-project-data-kilde\"\n    destination_bucket_name = \"ssb-prod-my-project-data-produkt\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    fs = FileClient.get_gcs_file_system()\n    fs.copy(file_path, destination_path)\n\n\n\nDet anbefales å bruke Pythons logging modul for å logge, og ikke bruke print eller skrive til stdout/stderr. Det er satt opp en standard logger-konfigurasjon som skriver informasjonsmeldinger (log level info) til stdout og feilmeldinger (log level warning eller error) til stderr. Feil som ikke blir håndtert blir automatisk fanget opp og logget av automatiseringsløsningen. Eksemplet nedenfor logger en informasjonsmelding, en advarsel og en feilmelding:\n\n\nprocess_source_data.py\n\nimport logging\nlogging.info('Til info')\nlogging.warning('Advarsel!')\nlogging.error('En feil oppstod!')"
  },
  {
    "objectID": "statistikkere/kildedata-prosessering.html#footnotes",
    "href": "statistikkere/kildedata-prosessering.html#footnotes",
    "title": "Kildedata prosessering",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPersonidentifiserende Informasjon↩︎\nPersonidentifiserende Informasjon↩︎"
  },
  {
    "objectID": "statistikkere/produksjonsløp.html",
    "href": "statistikkere/produksjonsløp.html",
    "title": "Produksjonsløp",
    "section": "",
    "text": "Produksjonsløp\nI forrige del beskrev vi noen grunnleggende ting rundt Dapla. I denne delen tar vi for oss hvordan du kan begynne å jobbe med skarpe data på plattformen.\nKapittelet som beskriver hvordan man logger seg inn på Dapla vil fungere uten at du må gjøre noen forberedelser. Er man koblet på SSB sitt nettverk så vil alle SSB-ansatte kunne gå inn på plattformen og kode i Python og R. Men du får ikke tilgang til SSBs område for datalagring på plattformen. I praksis vil det si at man kan generere data med kode, men man kan ikke jobbe med skarpe data.\nFor å få muligheten til å jobbe med skarpe data MÅ du først opprette et dapla-team. Dette er det første naturlige steget å ta når man skal begynne å jobbe med statistikkproduksjon på Dapla. I dette kapittelet vil vi forklare det du trenger å vite om det å opprette og jobbe innenfor et team."
  },
  {
    "objectID": "statistikkere/index.html",
    "href": "statistikkere/index.html",
    "title": "Velkommen",
    "section": "",
    "text": "Denne manualen tar sikte på å gi SSB-ansatte mulighet til å ta i bruk Dapla uten hjelp fra eksperter. Boken er delt i syv kapitler som er kort skildret i denne innledningen, i tillegg til en appendiks, en blogg og eksempel-notebooks. Under hjelp-fanen øverst på nettsiden finner du blant annet siden FAQ hvor ofte stilte spørsmål besvares. I denne boken omtaler vi den gamle produksjonssonen, ofte kalt prodsonen, som bakke, og det nye skymiljøet Google Cloud som sky. Det er ikke helt presist men duger for formålene i denne boken.\n\n\n\n\n\n\nSett i gang på bakken!\n\n\n\nDet er tilrettelagt for en treningsarena i bakkemiljøet (jupyter lab). Dette miljøet er nesten identisk med det som møter deg på Dapla, med unntak av at du her har tilgang til mange av de gamle systemene og mye mindre ‘hestekrefter’ i maskinene. Ideen er at SSB-ere ofte vil ønske å lære seg de nye verktøyene1 i kjente og kjære omgivelser først, og deretter flytte et ferdig skrevet produksjonsløp til Dapla.\n\n\nManualen er delt opp i syv kapitler: Første kapittel handler om Dapla-team. Å ha et team er en forutsetningen for å drive databehandling på platformen. Dermed er det et naturlig første steg å opprette et team.\nI andre kapittel, data, forklares blant annet hvordan data leses, lagres og deles. Her tar vi for oss bruk av vår interne Python-pakke dapla-toolbelt, og forklarer hva det vil si at data lagres i bøtter.\nTredje kapittel, kode, tar utgangspunkt i at man skal starte å kode opp sin statistikkproduksjon eller kjøre eksisterende kode. Her dekkes temaer som versjonshåndtering på github og virtuellle miljøer i både Python og R. Verktøyet ssb-project er en viktig del av dette kapitlet.\nFjerde kapittel handler om standarder og beskriver de standardene vi må forholde oss til når vi driver statistikkproduksjon på dapla. Dette inkluderer versjonering av filer, mappestrukturer og filnavn.\nI kapittel fem, metadata, kan du blant annet lære om Datadoc - et verktøy for å dokumentere datasett og variablene som utgjør datasettet.\nI sjette kapittel kan du lære om Dapla Lab: vår utforsker og platform for å kjøre og skrive kode, og kommunisere med Google Cloud Platform. I skrivende stund er Dapla Lab i beta, og artikkelen dermed uferdig.\nI syvende kapittel viser vi til datatjenestene som tilbys på Dapla. I dette kapitlet er det artikler om automatisk kildedataprosessering med kildomaten, pseudonymisering, dapla-statbank-client og tilgangsstyring med Dapla Ctrl.\nTil slutt har vi en appendiks med litt forskjellig innhold. Her finnes det artikler om alt fra Altinn 3 til hvordan man kan bidra til denne manualen.\nForhåpentligvis senker denne boken terskelen for å ta i bruk Dapla. Kommentarer og ønsker vedrørende boken tas imot med åpne armer. Dette kan gjøres ved å lage en issue i GitHub-repoet.\nGod fornøyelse😁\n\n\n\n\n\n\nVi trenger bidragsytere!\n\n\n\nDapla er i konstant utvikling og det er manualen og! Derfor trenger vi flere bidragsytere til å fjerne utdatert informasjon, forbedre eksisterende artikler og skrive nye.\nKunne du tenkt deg å bidra? Les om hvordan du kan bidra i denne artikkelen i appendiksen. Har du lyst til å bidra, men er ikke helt sikker på hva du kan bidra med? Ta en titt på issues i GitHub-repoet.",
    "crumbs": [
      "Manual",
      "Velkommen"
    ]
  },
  {
    "objectID": "statistikkere/index.html#footnotes",
    "href": "statistikkere/index.html#footnotes",
    "title": "Velkommen",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nDet som omtales som nye verktøy vil som regel bety R, Python, Git, GitHub og Jupyterlab.↩︎",
    "crumbs": [
      "Manual",
      "Velkommen"
    ]
  },
  {
    "objectID": "statistikkere/kartdata.html",
    "href": "statistikkere/kartdata.html",
    "title": "Kartdata",
    "section": "",
    "text": "Det er tilrettelagt kartdata i Dapla til analyse, statistikkproduksjon og visualisering. Det er de samme dataene som er i GEODB i vanlig produksjonssone. Kartdataene er lagret på ssb-kart-data-delt-prod. De er lagret som parquetfiler i standardprojeksjonen vi bruker i SSB (UTM sone 33N) hvis ikke annet er angitt. Det er også SSBs standard-rutenett i ulike størrelser samt Eurostats rutenett over Norge.\nI tillegg ligger det noe testdata i fellesbøtta her: ssb-prod-dapla-felles-data-delt/GIS/testdata\n\n\n\n\nGeopandas er et Python-bibliotek som integrerer Pandas med GIS-analysering og kartlegging. Geopandas er bygd opp rundt GeoDataFrames, som er en pandas DataFrame med geografisk informasjon. Disse kan lastes inn direkte hvis man har geografiske filer, eller man kan konvertere fra DataFrames med koordinat-kolonner (se eksempel under).\nGeopandas kan blant annet brukes til å kartlegge dataene, beregne avstander og labe variabler for nærmiljø ved å koble datasett sammen basert på geografisk overlapp.\nFor lesing og skriving av geodata i Dapla, kan man bruke SSB-pakken sgis. Med denne kan man blant annet også beregne reisetider langs veg og lage publiseringsverdige kart.\nPakkene kan installeres i Dapla sånn her:\n\n\nterminal\n\npoetry add geopandas\npoetry add ssb-sgis\n\nOg så importeres det i Python på vanlig vis.\n\n\nnotebook\n\nimport geopandas as gpd\nimport sgis as sg\n\nEksempel på lesing/skriving av kartdata\nMan kan lese/skrive geodata i Dapla med sgis-funksjonene read_geopandas og write_geopandas. Støttede formater er blant annet (geo)parquet, geojson og gml. En rekke kartdata for hele landet til analyse- og visualiseringsformål ligger i bøtta “kart”. For eksempel kommuneflater til analyse eller statistikk:\n\n\nnotebook\n\nfilsti = \"ssb-kart-data-delt-prod/analyse_data/klargjorte-data/2024/ABAS_kommune_flate_p2024_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\n\nEksempel på lesing av kommuneflater for visualisering. Disse dataene er generalisert slik at de egner seg for å lage kart, men blir unøyaktige til statistikk:\n\n\nnotebook\n\nfilsti = \"ssb-kart-data-delt-prod/visualisering_data/klargjorte-data/2024/parquet/N5000_kommune_flate_p2024_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\n\nTilsvarende for skriving til parquet eller annet geodataformat:\n\n\nnotebook\n\nsg.write_geopandas(kommuner, filsti_ny) \n\nMan kan også gjøre vanlige tabell-filer geografiske hvis man har koordinat-kolonner. For eksempel situasjonsuttak fra Virksomhets- og foretaksregisteret (VoF):\n\n\nnotebook\n\nimport dapla as dp\n\nVOFSTI = \"ssb-vof-data-delt-stedfesting-prod/klargjorte-data/parquet\"\nvof_df = dp.read_pandas(\n    f\"{VOFSTI}/stedfesting-situasjonsuttak_p2023-01_v1.parquet\"\n)\nvof_gdf = gpd.GeoDataFrame(\n    vof_df, \n    geometry=gpd.points_from_xy(\n        vof_df[\"y_koordinat\"],\n        vof_df[\"x_koordinat\"],\n    ),\n    crs=25833,\n)\nvof_gdf\n\n\n\n\nI tillegg til kartdataene, ligger det testdata i fellesbøtta som man kan leke seg med. Testdataene finnes for Oslo og Kongsvinger, og kan leses sånn her:\n\n\nnotebook\n\ntestdatasti = \"ssb-prod-dapla-felles-data-delt/GIS/testdata\"\nveger = sg.read_geopandas(f\"{testdatasti}/veger_oslo.parquet\")\n\nUnder følger noen eksempler på GIS-prosessering med testdataene.\nEksempel på avstandsberegning\nAvstand i luftlinje finner man enkelt med sjoin_nearest. Sånn her finner man avstand fra boligbygg (1000 tilfeldige) i Oslo til nærmeste butikkbygg.\n\n\nnotebook\n\nboligbygg = sg.read_geopandas(f\"{testdatasti}/noen_boligbygg_oslo.parquet\")\nbutikkbygg = sg.read_geopandas(f\"{testdatasti}/butikkbygg_oslo.parquet\")\n\nboligbygg_med_butikkavstand = boligbygg.sjoin_nearest(butikkbygg, distance_col=\"meter_til_butikk\")\n\nFor å finne avstand eller reisetid langs veier, kan man gjøre nettverksanalyse med sgis. Man må først klargjøre vegnettet og bestemme regler for beregningen(e):\n\n\nnotebook\n\nveger_klargjort = (\n    sg.get_connected_components(veger)\n    .loc[lambda x: x[\"connected\"] == 1]\n    .pipe(sg.make_directed_network_norway, dropnegative=True)\n)\n\nregler = sg.NetworkAnalysisRules(directed=True, weight=\"minutes\")\n\nnwa = sg.NetworkAnalysis(network=veger_klargjort, rules=regler)\n\nSå kan man beregne reisetider fra boligbygg til butikkbygg:\n\n\nnotebook\n\navstander = nwa.od_cost_matrix(boligbygg, butikkbygg)\n\nKorteste reisetid per bolig kan kobles på som kolonne i boligdataene sånn her:\n\n\nnotebook\n\nboligbygg[\"minutter_til_butikk\"] = avstander.groupby(\"origin\")[\"minutes\"].min()\n\nUndersøk resultatene i interaktivt kart:\n\n\nnotebook\n\nsg.explore(boligbygg, butikkbygg, \"minutter_til_butikk\")\n\nSe mer informasjon og eksempler i dokumentasjonen.\nOg flere nettverksanalyse-eksempler her.\nEksempel på geografisk kobling\nDatasett kan kobles basert på geografisk overlapp med geopandas.sjoin. Denne fungerer akkurat som pandas.merge, bare at det kobles basert på geometrien.\nKodesnutten under returnerer én kommuneflate for hvert bygg som overlapper, med kolonner fra begge datasettene. Inner-join er default, men med left-join beholder man også geometriene som ikke overlapper.\n\n\nnotebook\n\nkommuner = sg.read_geopandas(f\"{testdatasti}/enkle_kommuner.parquet\")\nkommuner_med_boliginfo = kommuner.sjoin(boligbygg, how=\"left\")\n\nMed motsatt rekkefølge, får man ett boligbygg per kommune den overlapper med. Siden byggene er punkter, vil hvert bygg havne i maksimalt én kommune.\n\n\nnotebook\n\nboligbygg_med_kommuneinfo = boligbygg.sjoin(kommuner, how=\"left\")\n\nEksempel på å lage kart\nMan kan lage tematiske kart med sgis-klassen ThematicMap. Her er et enkelt eksempel på et kart over arealet i kommuner.\n\n\nnotebook\n\nkommuner[\"Kvadratmeter\"] = kommuner.area\n\nm = sg.ThematicMap(kommuner, column=\"Kvadratmeter\", size=15)\nm.title = \"Areal i kommunene\"\nm.plot()\n\nSe flere kart-eksempler her.\nKonvertere DataFrame til GeoDataFrame\nEn pandas.DataFrame med koordinat-kolonner, kan konverteres til GeoDataFrame sånn her:\n\n\nnotebook\n\nimport pandas as pd\ndf = pd.DataFrame({\"x\": [10.8, 10.7, 10.9], \"y\": [59.9, 60, 59.85]})\n\ngdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.x, df.y), crs=4326)\n\nFor å beregne avtand i meter og kunne koble med annen geodata i Dapla, må man ha UTM-koordinater (hvis man ikke hadde det fra før):\n\n\nnotebook\n\ngdf = gdf.to_crs(25833)\n\nSe også geopandas’ dokumentasjon for mer utfyllende informasjon.\n\n\n\n\nDen viktigste GIS-pakken i R er sf. Pakken er bygd opp rundt sf-objekter, som er data.frames med geografisk informasjon. Man kan gjøre standard tidyverse-opersjoner på sf-objektene, pluss GIS-operasjoner.\nSindre Mikael Haugen har lagd funksjoner for å lese og skrive blant annet geodata i Dapla. For å få geodata, setter man parametret ‘sf’ til TRUE:\n\n\nnotebook\n\nlibrary(fellesr)\n\nveger = read_SSB(\"ssb-prod-dapla-felles-data-delt/GIS/testdata/veger_oslo.parquet\", sf = TRUE)\n\nHaugen har også lagd en pakke for å gjøre nettverksanalyse, som også lar deg geokode adresser, altså å finne adressenes koordinater.\n\n\nnotebook\n\nlibrary(GISSB)\n\nLite eksempel på kobling basert geografisk sammenfall (enkle kommuner er egentlig til visualisering). Her får man ett bygg per kommune som overlapper (som maksimalt er én kommune siden dette er bygningspunkter):\n\n\nnotebook\n\nlibrary(sf)\n\nboligbygg = read_SSB(\"ssb-prod-dapla-felles-data-delt/GIS/testdata/noen_boligbygg_oslo.parquet\", sf = TRUE)\nenkle_kommuner = read_SSB(\"ssb-prod-dapla-felles-data-delt/GIS/testdata/enkle_kommuner.parquet\", sf = TRUE)\n\nboligbygg_med_kommuneinfo = st_join(boligbygg, enkle_kommuner)\n\nMed motsatt rekkefølge, får man én kommuneflate per bolig som overlapper:\n\n\nnotebook\n\nkommuner_med_boliginfo = st_join(enkle_kommuner, boligbygg)",
    "crumbs": [
      "Manual",
      "Appendix",
      "Kartdata"
    ]
  },
  {
    "objectID": "statistikkere/kartdata.html#python",
    "href": "statistikkere/kartdata.html#python",
    "title": "Kartdata",
    "section": "",
    "text": "Geopandas er et Python-bibliotek som integrerer Pandas med GIS-analysering og kartlegging. Geopandas er bygd opp rundt GeoDataFrames, som er en pandas DataFrame med geografisk informasjon. Disse kan lastes inn direkte hvis man har geografiske filer, eller man kan konvertere fra DataFrames med koordinat-kolonner (se eksempel under).\nGeopandas kan blant annet brukes til å kartlegge dataene, beregne avstander og labe variabler for nærmiljø ved å koble datasett sammen basert på geografisk overlapp.\nFor lesing og skriving av geodata i Dapla, kan man bruke SSB-pakken sgis. Med denne kan man blant annet også beregne reisetider langs veg og lage publiseringsverdige kart.\nPakkene kan installeres i Dapla sånn her:\n\n\nterminal\n\npoetry add geopandas\npoetry add ssb-sgis\n\nOg så importeres det i Python på vanlig vis.\n\n\nnotebook\n\nimport geopandas as gpd\nimport sgis as sg\n\nEksempel på lesing/skriving av kartdata\nMan kan lese/skrive geodata i Dapla med sgis-funksjonene read_geopandas og write_geopandas. Støttede formater er blant annet (geo)parquet, geojson og gml. En rekke kartdata for hele landet til analyse- og visualiseringsformål ligger i bøtta “kart”. For eksempel kommuneflater til analyse eller statistikk:\n\n\nnotebook\n\nfilsti = \"ssb-kart-data-delt-prod/analyse_data/klargjorte-data/2024/ABAS_kommune_flate_p2024_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\n\nEksempel på lesing av kommuneflater for visualisering. Disse dataene er generalisert slik at de egner seg for å lage kart, men blir unøyaktige til statistikk:\n\n\nnotebook\n\nfilsti = \"ssb-kart-data-delt-prod/visualisering_data/klargjorte-data/2024/parquet/N5000_kommune_flate_p2024_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\n\nTilsvarende for skriving til parquet eller annet geodataformat:\n\n\nnotebook\n\nsg.write_geopandas(kommuner, filsti_ny) \n\nMan kan også gjøre vanlige tabell-filer geografiske hvis man har koordinat-kolonner. For eksempel situasjonsuttak fra Virksomhets- og foretaksregisteret (VoF):\n\n\nnotebook\n\nimport dapla as dp\n\nVOFSTI = \"ssb-vof-data-delt-stedfesting-prod/klargjorte-data/parquet\"\nvof_df = dp.read_pandas(\n    f\"{VOFSTI}/stedfesting-situasjonsuttak_p2023-01_v1.parquet\"\n)\nvof_gdf = gpd.GeoDataFrame(\n    vof_df, \n    geometry=gpd.points_from_xy(\n        vof_df[\"y_koordinat\"],\n        vof_df[\"x_koordinat\"],\n    ),\n    crs=25833,\n)\nvof_gdf\n\n\n\n\nI tillegg til kartdataene, ligger det testdata i fellesbøtta som man kan leke seg med. Testdataene finnes for Oslo og Kongsvinger, og kan leses sånn her:\n\n\nnotebook\n\ntestdatasti = \"ssb-prod-dapla-felles-data-delt/GIS/testdata\"\nveger = sg.read_geopandas(f\"{testdatasti}/veger_oslo.parquet\")\n\nUnder følger noen eksempler på GIS-prosessering med testdataene.\nEksempel på avstandsberegning\nAvstand i luftlinje finner man enkelt med sjoin_nearest. Sånn her finner man avstand fra boligbygg (1000 tilfeldige) i Oslo til nærmeste butikkbygg.\n\n\nnotebook\n\nboligbygg = sg.read_geopandas(f\"{testdatasti}/noen_boligbygg_oslo.parquet\")\nbutikkbygg = sg.read_geopandas(f\"{testdatasti}/butikkbygg_oslo.parquet\")\n\nboligbygg_med_butikkavstand = boligbygg.sjoin_nearest(butikkbygg, distance_col=\"meter_til_butikk\")\n\nFor å finne avstand eller reisetid langs veier, kan man gjøre nettverksanalyse med sgis. Man må først klargjøre vegnettet og bestemme regler for beregningen(e):\n\n\nnotebook\n\nveger_klargjort = (\n    sg.get_connected_components(veger)\n    .loc[lambda x: x[\"connected\"] == 1]\n    .pipe(sg.make_directed_network_norway, dropnegative=True)\n)\n\nregler = sg.NetworkAnalysisRules(directed=True, weight=\"minutes\")\n\nnwa = sg.NetworkAnalysis(network=veger_klargjort, rules=regler)\n\nSå kan man beregne reisetider fra boligbygg til butikkbygg:\n\n\nnotebook\n\navstander = nwa.od_cost_matrix(boligbygg, butikkbygg)\n\nKorteste reisetid per bolig kan kobles på som kolonne i boligdataene sånn her:\n\n\nnotebook\n\nboligbygg[\"minutter_til_butikk\"] = avstander.groupby(\"origin\")[\"minutes\"].min()\n\nUndersøk resultatene i interaktivt kart:\n\n\nnotebook\n\nsg.explore(boligbygg, butikkbygg, \"minutter_til_butikk\")\n\nSe mer informasjon og eksempler i dokumentasjonen.\nOg flere nettverksanalyse-eksempler her.\nEksempel på geografisk kobling\nDatasett kan kobles basert på geografisk overlapp med geopandas.sjoin. Denne fungerer akkurat som pandas.merge, bare at det kobles basert på geometrien.\nKodesnutten under returnerer én kommuneflate for hvert bygg som overlapper, med kolonner fra begge datasettene. Inner-join er default, men med left-join beholder man også geometriene som ikke overlapper.\n\n\nnotebook\n\nkommuner = sg.read_geopandas(f\"{testdatasti}/enkle_kommuner.parquet\")\nkommuner_med_boliginfo = kommuner.sjoin(boligbygg, how=\"left\")\n\nMed motsatt rekkefølge, får man ett boligbygg per kommune den overlapper med. Siden byggene er punkter, vil hvert bygg havne i maksimalt én kommune.\n\n\nnotebook\n\nboligbygg_med_kommuneinfo = boligbygg.sjoin(kommuner, how=\"left\")\n\nEksempel på å lage kart\nMan kan lage tematiske kart med sgis-klassen ThematicMap. Her er et enkelt eksempel på et kart over arealet i kommuner.\n\n\nnotebook\n\nkommuner[\"Kvadratmeter\"] = kommuner.area\n\nm = sg.ThematicMap(kommuner, column=\"Kvadratmeter\", size=15)\nm.title = \"Areal i kommunene\"\nm.plot()\n\nSe flere kart-eksempler her.\nKonvertere DataFrame til GeoDataFrame\nEn pandas.DataFrame med koordinat-kolonner, kan konverteres til GeoDataFrame sånn her:\n\n\nnotebook\n\nimport pandas as pd\ndf = pd.DataFrame({\"x\": [10.8, 10.7, 10.9], \"y\": [59.9, 60, 59.85]})\n\ngdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.x, df.y), crs=4326)\n\nFor å beregne avtand i meter og kunne koble med annen geodata i Dapla, må man ha UTM-koordinater (hvis man ikke hadde det fra før):\n\n\nnotebook\n\ngdf = gdf.to_crs(25833)\n\nSe også geopandas’ dokumentasjon for mer utfyllende informasjon.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Kartdata"
    ]
  },
  {
    "objectID": "statistikkere/kartdata.html#r",
    "href": "statistikkere/kartdata.html#r",
    "title": "Kartdata",
    "section": "",
    "text": "Den viktigste GIS-pakken i R er sf. Pakken er bygd opp rundt sf-objekter, som er data.frames med geografisk informasjon. Man kan gjøre standard tidyverse-opersjoner på sf-objektene, pluss GIS-operasjoner.\nSindre Mikael Haugen har lagd funksjoner for å lese og skrive blant annet geodata i Dapla. For å få geodata, setter man parametret ‘sf’ til TRUE:\n\n\nnotebook\n\nlibrary(fellesr)\n\nveger = read_SSB(\"ssb-prod-dapla-felles-data-delt/GIS/testdata/veger_oslo.parquet\", sf = TRUE)\n\nHaugen har også lagd en pakke for å gjøre nettverksanalyse, som også lar deg geokode adresser, altså å finne adressenes koordinater.\n\n\nnotebook\n\nlibrary(GISSB)\n\nLite eksempel på kobling basert geografisk sammenfall (enkle kommuner er egentlig til visualisering). Her får man ett bygg per kommune som overlapper (som maksimalt er én kommune siden dette er bygningspunkter):\n\n\nnotebook\n\nlibrary(sf)\n\nboligbygg = read_SSB(\"ssb-prod-dapla-felles-data-delt/GIS/testdata/noen_boligbygg_oslo.parquet\", sf = TRUE)\nenkle_kommuner = read_SSB(\"ssb-prod-dapla-felles-data-delt/GIS/testdata/enkle_kommuner.parquet\", sf = TRUE)\n\nboligbygg_med_kommuneinfo = st_join(boligbygg, enkle_kommuner)\n\nMed motsatt rekkefølge, får man én kommuneflate per bolig som overlapper:\n\n\nnotebook\n\nkommuner_med_boliginfo = st_join(enkle_kommuner, boligbygg)",
    "crumbs": [
      "Manual",
      "Appendix",
      "Kartdata"
    ]
  },
  {
    "objectID": "statistikkere/transfer-service.html",
    "href": "statistikkere/transfer-service.html",
    "title": "Transfer Service",
    "section": "",
    "text": "Storage Transfer Service1 er en Google-tjeneste for å flytte data mellom lagringsområder. I SSB bruker vi hovedsakelig tjenesten til å:\nTjenesten støtter både automatiserte og ad-hoc overføringer, og den inkluderer et brukergrensesnitt for å sette opp og administrere overføringene i Google Cloud Console (GCC).",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Transfer Service"
    ]
  },
  {
    "objectID": "statistikkere/transfer-service.html#tilgangsstyring",
    "href": "statistikkere/transfer-service.html#tilgangsstyring",
    "title": "Transfer Service",
    "section": "Tilgangsstyring",
    "text": "Tilgangsstyring\nTilgangsstyringen til data gjelder også for overføringer av data med Transfer Service. Det betyr at du må ha tilgang til dataene du skal sette opp overføringsjobber for. Ved bruk av Transfer Service for overføring av data mellom bakke og sky så er det satt opp en dedikerte mapper for dette i prodsonen. Også her følges tilgangsstyringen til dataene, med unntak av at data-admins har permanent tilgang til kildedata som er synkronisert ned til bakken, mens man på Dapla må de gi seg selv korte, begrunnede tilganger ved behov.\n\n\nPå Dapla så er det opprettet dedikerte bøtter for overføring av data mellom bakke og sky. Disse heter tilsky og frasky. Tanken med disse “mellomstasjonene” for overføring av data er at de skal beskytte Dapla-team fra å overskrive data ved en feil. Ved å ha egne bøtter som data blir synkronisert gjennom, så legges det opp til at man deretter manuelt3 flytter dataene til riktig bøtte.\nMen det er ikke lagt noen sperrer for synkronisere direkte til en annen bøtte man har tilgang til. Systembrukeren (se forklaringsboks) som kjører Transfer Service har tilgang til alle bøttene i prosjektet. Det betyr at en data-admin kan velge å synkronisere data direkte inn i kildebøtta hvis man mener at det er hensiktsmessig. Det samme gjelder for developers som setter opp dataoverføringer i standardprosjektet. Men da er det som sagt viktig å være bevisst på hvordan man setter opp reglene for overskriving av data hvis filene har like navn. Disse opsjonene forklares nærmere senere i kapitlet.\n\n\n\n\n\n\n\n\n\nPersonlig bruker vs systembruker\n\n\n\nNår du setter opp en overføringsjobb med Transfer Service så setter du opp en jobb som kjøres av en systembruker4 og ikke din egen personlige bruker. Dette er spesielt viktig å være klar over når man setter opp automatiserte overføringsjobber. En konsekves av dette er at automatiske overføringsjobber vil fortsette å kjøre selv om din tilgang til dataene er midlertidig, siden det er en systembruker som faktisk kjører jobben.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Transfer Service"
    ]
  },
  {
    "objectID": "statistikkere/transfer-service.html#forberedelser",
    "href": "statistikkere/transfer-service.html#forberedelser",
    "title": "Transfer Service",
    "section": "Forberedelser",
    "text": "Forberedelser\nFørste gang du bruker Transfer Service må du sjekke at tjenesten er aktivert for teamet. Transfer Service er en såkalt feature som teamet kan skru av og på selv. For å sjekke om den er skrudd på går du inn i teamets IaC-repo5 og sjekker filen ./infra/projects.yaml.\n\n\ndapla-example-iac/infra/projects.yaml\n\nteam_uniform_name: dapla-example\n\nprojects:\n  - project_name: dapla-example\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service\n\nI filen over ser du at teamet har skrudd på tjenesten i prod-miljøet, siden den transfer-service er listet under features. Hvis tjenesten ikke er skrudd på kan du lese om hvordan du skrur den på i feature-dokumentasjonen.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Transfer Service"
    ]
  },
  {
    "objectID": "statistikkere/transfer-service.html#overføring-av-data",
    "href": "statistikkere/transfer-service.html#overføring-av-data",
    "title": "Transfer Service",
    "section": "Overføring av data",
    "text": "Overføring av data\n\n\n\n\n\n\nOverføring av kildedata\n\n\n\nOverføring av kildedata må gjøres av en data-admin i teamet som har aktivert sin forhåndsgodkjente tilgang til kildedata. Tilgangen aktiveres ved å gå inn i JIT-applikasjonen og velge prosjekt-id. Deretter velger du rollene ssb.bucket.write, ssb.buckets.list og storagetransfer.admin, og hvor lenge du ønsker tilgangen. Til slutt oppgir du en begrunnelse for hvorfor du trenger tilgangentilgangen og trykker Request access. Når du har gjort dette vil du få en bekreftelse på at tilgangen er aktivert, og det tar ca 1 minutt før den aktiverte tilgangen er synlig i GCC.\n\n\nGrensesnittet for å sette opp overføringsjobber i Transfer Service er tilgjengelig i Google Cloud Console (GCC).\n\n\n\nGå inn på Google Cloud Console i en nettleser.\nSjekk, øverst i høyre hjørne, at du er logget inn med din SSB-konto (xxx@ssb.no).\nVelg prosjektet6 som overføringen skal settes opp under.\nEtter at du har valgt prosjekt kan du søke etter Storage Transfer i søkefeltet øverst på siden, og gå inn på siden.\n\n\n\n\n\n\n\n\n\n\nHva er mitt prosjektnavn?\n\n\n\nNår det opprettes et Dapla-team, så opprettes det flere Google-prosjekter for teamet. Når du skal velge hvilket prosjekt du skal jobbe på i GCC, så følger de en fast navnestruktur. For eksempel så vil et team med navnet dapla-example få et standardprosjekt som heter dapla-example-p. Det blir også opprettet et kildeprosjekt som heter dapla-example-kilde-p.\n\n\n\n\nFørste gang du bruker Storage Transfer må man gjøre en engangsjobb for å bruke tjenesten. Dette gjøres kun første gang din bruker setter opp en jobb, og deretter trenger du ikke å gjøre det flere ganger.\nNår du kommer inn på siden til Storage Transfer så trykker du på Set Up Connection. Når du trykker på denne vil det dukke opp et nytt felt hvor du får valget Create Pub-Sub Resources. Trykk på den blå Create-knappen, og deretter trykk på Close lenger nede. Da er engangsjobben gjort, og du kan begynne å sette opp overføringsjobber.\n\nI navigasjonsmenyen til venstre trykk Transfer jobs, og deretter trykk på + Create transfer job øverst på siden for å opprette en ny overføringsjobb. Da får du opp bildet som vist i Figur 1.\n\n\n\n\n\n\n\nFigur 1: Opprett overføringsjobb i Google Cloud Console.\n\n\n\nVidere vil det variere om man skal overføre data mellom bøtter eller mellom Dapla og prodsonen. Under forklarer vi begge fremgangsmåtene.\n\nProdsonen og Dapla\nOverføring mellom bakke og sky er en overføring av data mellom en bøtte på Dapla og en mappe i prodsonen. Siden tilgangsstyring til kildedata er strengere enn tilgangsstyring til annen data, så er det det to litt fremgangsmåter for å sette opp overføringsjobber for disse.\nSiden stegene er litt forskjellig avhengig av om man skal flytte kildedata eller annen data, så deler vi denne delen i to. Figur 2 viser hvordan dette er satt opp. Kildeprosjektet på Dapla har en tilsky-bøtte for å flytting av data fra prodsonen til Dapla, og den har en frasky-bøtte for å flytte data fra Dapla til prodsonen. Standardprosjektet på Dapla har også en tilsky-bøtte for å flytte data fra prodsonen til Dapla, og den har en frasky-bøtte for å flytte data fra Dapla til prodsonen.\n\n\n\n\n\n\nFigur 2: Overføring av data mellom prodsonen og Dapla.\n\n\n\nVidere viser vi hvordan man overfører fra Dapla til prodsonen. Overføring motsatt vei innebærer bare at man bytter om på Source type og Destination type.\n\nI fanen Get started velger du:\n\nSource type: Google Cloud Storage\nDestination type: POSIX filesystem\n\nI fanen Choose a source trykker du på Browse, velger hvilken bøtte eller “undermappe” i en bøtte du skal overføre fra, og trykker Select7.\nI fanen Choose a destination velger du transfer_service_default under Agent pool. Under Destination directory path velger du hvilken undermappe av som filen skal overføres til. Tjenesten vet allerede om du er i kilde- eller standardprosjektet, så du trenger kun å skrive inn frasky/ eller tilsky/ her, og evt. undermappenavn hvis det er aktuelt (f.eks. frasky/data/8). Trykk Next step.\nI fanen Choose when to run job velger du hvor ofte og hvordan jobber skal kjøre. Tabell 1 viser hvilke valg du kan ta. Trykk Next step.\n\n\n\n\nTabell 1: Valg under Choose when to run job\n\n\n\n\n\n\n\n\n\nValg\nFrekvens\n\n\n\n\nRun once\nEngangoverføringer\n\n\nRun every day\nSynkroniser hver dag\n\n\nRun every week\nSynkroniser hver uke\n\n\nRun with custom frequency\nSynkroniser inntill hver time\n\n\nRun on demand\nSynkroniserer når du manuelt trigger jobben\n\n\n\n\n\n\n\nI fanen Choose settings kan du velge hvordan detaljer knyttet til overføringen skal håndteres. Tabell 2 viser hvilke valg du kan ta.\n\n\n\n\nTabell 2: Valg under Choose settings\n\n\n\n\n\n\n\n\n\n\nValg\nUndervalg\nHandling\n\n\n\n\nIdentify your job\n\nBeskriv jobben kort.\n\n\nManifest file\n\nIkke relevant. Bruk default valg.\n\n\nChoose how to handle your data\nMetadata options\nIkke relevant. Bruk default valg.\n\n\n\nWhen to overwrite\nTenk nøye gjennom hva du velger her.\n\n\n\nWhen to delete\nTenk nøye gjennom hva du velger her.\n\n\nChoose how to keep track of transfer progress\nLogging options\nSkru på logging.\n\n\n\n\n\n\nValgene When to overwrite og When to delete er det viktig at tenkes nøye gjennom, spesielt ved automatiske synkroniseringer. When to overwrite er spesielt siden det kan føre til data blir overskrevet eller tapt.\n\nTrykk på den blå Create-knappen for å opprette overføringsjobben. Du vil kunne se kjørende jobber under menyen Transfer jobs.\n\n\nMappestrukturen i prodsonen\nMappestrukturen for overføringer med Transfer Service mellom bakke og sky har en fast struktur som er likt for alle team. Hvis du logger deg inn i terminalen på en av Linux-serverne i prodsonen, åpner du mappen ved å skrive cd /ssb/cloud_sync. Under denne mappen finner du en mappe for hvert team som har aktivert Transfer Service. Hvis et team for eksempel heter dapla-example så vil det være en mappe som heter dapla-example. Her kan teamet hente og levere data som skal synkroniseres mellom bakke og sky. Videre er det undermapper for kilde- og standardprosjektet til teamet. Det er kun data-admins som har tilgang til kildeprosjektet, og det er kun developers som har tilgang til standardprosjektet. Under finner du en oversikt over hvordan mappene ser ut for et team som heter dapla-example.\n\n\n/ssb/cloud_sync/dapla-example/\n\ndapla-example\n│\n├── kilde\n│   │\n│   │── tilsky\n│   │\n│   └── frasky\n│\n└── standard\n    │\n    │── tilsky\n    │\n    └── frasky\n\n\n\n\nBøtte til bøtte\nOverføring mellom bøtter er en overføring av data mellom to bøtter på Dapla. Fremgangsmåten er helt likt som beskrevet tidligere, men at du nå velger Google Cloud Storage som både kilde og destinasjon. Igjen er vi avhengig av at systembrukeren som utfører jobben har tilgang til begge bøttene som er involvert i overføringen. Default er at et team kan overføre mellom bøtter i kildeprosjektet, og at de kan overføre mellom bøtter i standardprosjektet, men aldri mellom de to. Hvis du ønsker å overføre mellom bøtter i ditt prosjekt og et annet teams prosjekt, så må du be det andre teamet om å gi din systembruker tilgang til dette.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Transfer Service"
    ]
  },
  {
    "objectID": "statistikkere/transfer-service.html#footnotes",
    "href": "statistikkere/transfer-service.html#footnotes",
    "title": "Transfer Service",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI SSB kaller vi tjenesten for Transfer Service, men du kan oppleve at Google kaller den litt forskjellige ting. Den blir omtalt som Storage Transfer Service noen steder, mens i Google Cloud Console blir den omtalt som Data Transfer eller Storage Transfer↩︎\nFlytting av data mellom bøtter krever at prosjektets Transfer Service har tilgang til begge bøttene.↩︎\nMed manuelt menes her at man går inn og flytter filer fra en bøtte til en annen. Men det kan også bety at man flytter data til riktig bøtte som en del produksjonskoden sin, som igjen kan kjøres automatisk.↩︎\nSystembrukere heter Service Accounts på engelsk og blir ofte referert til som SA-er i dagligtale.↩︎\nDu finner teamets IaC-repo ved å gå inn på https://github.com/orgs/statisticsnorway/repositories og søke etter ditt teamnavn og åpne den som har navnestrukturen teamnavn-iac. For eksempel vil et team som heter dapla-example har et IaC-repo som heter dapla-example-iac.↩︎\nDu kan velge prosjekt øverst på siden, til høyre for teksten Google Cloud. I bildet under ser du at hvordan det ser ut når prosjektet dapla-felles-p er valgt.↩︎\nNår du skal velge en undermappe i en bøtte så er grensesnittet litt lite intuitivt. Du kan ikke trykke på navnet, men du på trykke på -tegnet for å se undermappene.↩︎\nNår du skal synkronisere fra Dapla til en undermappe i prodsonen, så må mappen i prodsonen allerede eksisterere. Hvis den ikke gjør det vil jobben feile. Ved synkronsiering fra prodsonen til Dapla trenger ikke undermappen eksistere, siden bøtter egentlig ikke har undermapper og filstien fra prodsonen bare blir til filnavnet i bøtta.↩︎",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Transfer Service"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html",
    "href": "statistikkere/maskinporten-guardian.html",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "Maskinporten er en tjeneste fra Digdir som gir sikker autentisering og tilgangskontroll for datautveksling mellom virksomheter.\nTilgangsstyring for data administeres via et webgrensesnitt (Samarbeidsportalen) En virksomhet som deler data (API-tilbyder) definerer såkalte API scopes, og velger hvilke andre virksomheter (API-konsumenter) som skal ha tilgang til disse. API-konsumenter kan på sin side selv opprette en eller flere Maskinporten-klienter og gir de tilgang til API scopes som er delt med virksomheten.\n\n\n\n\n\n\nFigur 1: Datauveksling mellom virksomheter\n\n\n\nLes mer om Maskinporten her.\n\n\n\nUtveksling av data fra en API-tilbyder gjøres ved å inkludere et sikkerhetstoken som hentes fra Maskinporten på vegne av virksomheten man representerer (f. eks SSB). Hvilke data (API scopes) et sikkerhetstoken har tilgang til er knyttet til Maskinporten-klienten. For å hente et sikkerhetstoken for en klient, kreves det at man autentiserer seg som virksomhet. Dette gjøres ved å signere forespørsler til Maskinporten ved bruk av et virksomhetssertifikat. Det er her Maskinporten Guardian kommer inn i bildet.\nMaskinporten Guardian har tilgang til SSBs virksomhetssertifikat og kan dermed signere forespørsler mot Maskinporten for å hente ut sikkerhetstokens.\n\n\n\nHvis en skal ta i bruk et API som er beskyttet av Maskinporten er det noen steg som må gjøres i forkant:\n\n\n\n\n\nflowchart TD\n    A[API-tilbyder gir SSB tilgang til API via Samarbeidsportalen]\n    A --&gt; B[M2M-teamet hos SSB oppretter Maskinporten-klienter]\n    B --&gt; C[Keycloak-klienter for Maskinporten Guardian opprettes]\n\n\n\n\n\n\n\nAPI-tilbyderen må gi SSB tilgang til et API scope. API-tilbydere kan gjøre dette via Samarbeidsportalen hos Digdir. Fra API-tilbyderen får du også annen informasjon om API-ene, som:\n\n\nURL-er som skal benyttes for å snakke med API-et\nNavn på API scopes\nOm det finnes testadata\nDokumentasjon for API-endepunktene\n\n\nNår data er delt av en API-tilbyder, og en har navnet på API scopes, kan M2M-teamet hos SSB kontaktes for å få opprettet Maskinporten-klienter, én pr miljø (f. eks prod og test). De må vite hvilke API scopes og miljøer (test/prod) som skal benyttes. M2M-teamet vil gi deg ID-er (f. eks 12345678-9abc-def0-1234-567890abcdef) for klientene som er blitt opprettet.\n\n\n\n\n\n\n\nDu kan anse ID for en Maskinporten-klient som et slags brukernavn, og behandle dette deretter. Slike ID-er er ikke sensitive i seg selv, men de bør allikevel ikke ligge i åpne git-repoer (SSB-private repoer er OK). Det er heller ingen grunn til å behandle disse som hemmeligheter. Det anbefales at man opererer med Maskinporten-klienter som ekstern konfigurasjon til koden, f. eks på samme måte som man behandler URL-er til API-ene.\n\n\n\n\nNår du har ID for Maskinporten-klienten(e), er neste steg å få opprettet en Keycloak-bruker for Maskinporten Guardian. Se Opprette en Maskinporten Guardian M2M-bruker. Ta kontakt med oss på Dapla via Pureservice dersom du trenger hjelp til dette. Client ID og client secret for Keycloak-brukeren kan hentes fra Secret Manager (mer om det lenger ned). Medlemmer av Dapla-teamet har tilgang til å hente disse.\n\nOm noen på teamet trenger personlig tilgang til API-ene så må det konfigureres i Maskinporten Guardian. Da må vi vite hvilke personer som skal ha denne tilgangen. Les mer om forskjellen på M2M og personlig tilgang lenger ned.\n\n\n\nDet er M2M-teamet i SSB som administrerer det formelle i forbindelse med integrasjoner mot eksterne API-er. For å få opprettet nye Maskinporten-klienter er det dette teamet man kontakter. De må vite hvilke miljøer (test og/eller prod) og hvilke API scopes som skal benyttes. Hver Maskinporten-klient som opprettes identifiseres av en ID (f. eks 12345678-9abc-def0-1234-567890abcdef), som du vil få tilsendt.\nLegg merke til at det er SSB selv som oppretter og administrerer Maskinporten-klienter. Det gjøres i Samarbeidsportalen hos Difi. Klientene knyttes til API scope(s) (som er delt med SSB av API-tilbyderen). Du kan lese mer om hvordan M2M-teamet administrerer Maskinporten-klienter her.\n\n\n\n\n\n\nAlle med en SSB-epostadresse kan registerere en personlig bruker i Digdir Samarbeidsportalen for å få innsyn i hvilke API-integrasjoner som finnes.\n\n\n\n\n\n\nMaskinporten Guardian er tilgjengelig fra alle SSB og NAIS sine IP-adresser, og kan nås på:\n\nProd: [https://guardian.dapla.ssb.no]\nTest: [https://guardian.dapla-staging.ssb.no]\n\nMaskinporten Guardian sine endepunkter er selv beskyttet av Keycloak. To typer brukere støttes:\n\nMaskin-til-maskin (M2M) - Systembruker knyttet til en gitt Maskinporten-klient. For å opptre på vegne av en M2M-bruker autentiserer man seg med en Keycloak client secret. Denne hemmeligheten er lagret i Google Secret Manager og kun Service Accounts eller Dapla-grupper med tilgang kan hente den ut.\nPersonlig - Din egen SSB-bruker. Man kan f. eks bruke dapla-toolbelt for å hente ut sitt personlige Keycloak-token. I tillegg til å autentisere deg må din bruker være autorisert til å gjøre oppslag på vegne av en Maskinporten-klient. Dette styres i konfigurasjonen til Maskinporten Guardian.\n\n\n\n\n\n\n\nMan skal i hovedsak kun anvende M2M-brukere for datautveksling mot API-er som er beskyttet av Maskinporten. Personlige brukere skal kun brukes unntaksvis for enkeltoppslag (f. eks ved feilsøking) mot API-er eller for utvikling og test.\n\n\n\n\n\n\nFølgende gir en oversikt over hvordan systemer henger sammen. En API-konsument kan f. eks være et Dapla-team som ønsker å hente data, mens en API-tilbyder er en ekstern virksomhet som tilbyr data via Maskinporten. Det er noen forskjeller i flyt avhengig av om Maskinporten Guardian aksesseres med systembruker (M2M) eller personlig bruker.\n\n\n\n\n\n\nFigur 2\n\n\n\n\n\n\nAPI-konsumenten henter sin Keycloak client secret for en gitt Maskinporten-klient fra Secret Manager.\nAPI-konsumenten henter et Keycloak sikkerhetstoken ved å bruke client secret fra steg 1.\nAPI-konsumenten henter et Maskinporten sikkerhetstoken fra Maskinporten Guardian ved å bruke Keycloak sikkerhetstoken fra steg 2. En kan alternativt angi andre API scopes enn det som er standard for Maskinporten-klienten, men dette er vanligvis ikke nødvendig.\nMaskinporten Guardian bruker SSB sitt virksomhetssertifikat til å signere en forespørsel om å hente et sikkerhetstoken fra Maskinporten (ref steg 3)\nAPI-konsumenten henter data fra den eksterne API-tilbyderen ved å bruke sikkerhetstoken fra Maskinporten.\n\n\n\n\n\n\n\n\n\n\nSteg 1 og 2 gjelder kun for M2M-brukere. Dersom man aksesserer Maskinporten Guardian med personlig bruker så hentes Keycloak-tokenet f. eks ved hjelp av AuthClient i dapla-toolbelt.\n\n\n\n\nAPI-konsumenten henter et Maskinporten sikkerhetstoken fra Maskinporten Guardian ved å bruke sitt personlige Keycloak sikkerhetstoken. Det må angis hvilken Maskinporten-klient og hvilke API scopes Maskinporten sikkerhetstokenet skal gjelde for. Den personlige brukeren må på forhånd være autorisert (ref Maskinporten Guardian sin tilgangskonfigurasjon) til å kunne hente sikkerthetstokens for Maskinporten-klienten.\nMaskinporten Guardian bruker SSB sitt virksomhetssertifikat til å signere en forespørsel om å hente et sikkerhetstoken fra Maskinporten (ref steg 3)\nAPI-konsumenten henter data fra den eksterne API-tilbyderen ved å bruke sikkerhetstoken fra Maskinporten.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDette avsnittet inneholder tekniske instrukser ment for deg som er kjent med Git og som f. eks jobber i et Self-managed Dapla-team. Ta kontakt med oss via Pureservice så hjelper vi deg gjerne med dette.\n\n\nFor å kunne opptre på vegne av Maskinporten-klienten uavhengig av din personlige bruker, må man opprette en Keycloak systembruker. Det gjøres ved å åpne en Pull Request (konfigurasjon som gjennomgås av en tekniker) til keycloak-iac der du angir informasjon som ID for Maskinporten-klient, API scopes og hvem som skal ha tilgang. Legg merke til at du må opprette en klient pr miljø (test og prod). Du kan se bort fra play-miljøet.\n\n\namends \".../pkl/MaskinportenGuardianClient.pkl\"\n\napi_shortname = \"Kort API-beskrivelse (maks 32 tegn)\"\nmaskinporten_client_id = \"12345678-9abc-def0-1234-567890abcdef\"\nmaskinporten_audience = \"https://maskinporten.no/\"\nmaskinporten_default_scopes {\n  \"foo:data1\"\n  \"foo:data2\"\n}\ncredentials_access {\n  \"group:play-foeniks-data-admins@groups.ssb.no\"\n  \"serviceaccount:foo-sa@play-foeniks-p-ab.iam.gserviceaccount.com\"\n}\n\n\n\n\n\n\nNote\n\n\n\nI test skal maskinporten_audience ha verdien https://test.maskinporten.no/. I prod skal det være https://maskinporten.no/ (merk: skråstrek på slutten er viktig)\n\n\nPull Requesten må godkjennes og behandles av en Dapla platformutvikler. Når dette er gjort blir det opprettet en Keycloak-klient, og hemmeligheten som kan brukes for å hente ut sikkerhetstokens for denne klienten er tilgjengelig i Secret Manager.\nSe følgende dokumentasjon for mer informasjon:\n\nDetaljert beskrivelse av konfigurasjonsmuligeter.\nGenerell beskrivelse av hvordan man oppretter en Keycloak-klient\nKeycloak client credentials\n\nTa kontakt med Kundeservice hvis du har spørsmål eller trenger ei hand å halde i.\n\n\n\n\nFølgende viser Python kodeeksempler for hvordan man kan hente ut et Maskinporten sikkerhetstoken.\n\n\n\"\"\"\nRetrieve maskinporten M2M access token for querying an external API.\n\nThe following code example expects a toml config file to exist named\n&lt;team_uniform_name&gt;-&lt;env&gt;-maskinporten-m2m.toml (e.g. play-foeniks-prod-maskinporten-m2m.toml)\n\nwith contents such as:\n\nkeycloak_url = \"https://auth.test.ssb.no\"\nkeycloak_clients_gcp_project_id = \"keycloak-clients-&lt;p|t&gt;-??\"\nguardian_url = \"https://guardian.dapla-staging.ssb.no\"\n\n[my-api]\n    maskinporten_client_id = \"12345678-9abc-def0-1234-567890abcdef\"\n\"\"\"\nimport os\nimport re\nimport requests\nimport toml\n\n\nteam_uniform_name = \"play-foeniks\"\napi_name = \"my-api\" # identifies the API in the config.toml file\n\n# Check which environment we're running and load corresponding config\ndapla_env = os.getenv(\"DAPLA_ENVIRONMENT\").lower()\nconfig = toml.load(f\"{team_uniform_name}-{dapla_env}-maskinporten-m2m.toml\")\n\n# Get Maskinporten Guardian M2M Keycloak client credentials from Google Secret Manager\n# The secret's name is deduced from the team name and maskinporten client id\nname = f\"{team_uniform_name}-ssb-maskinporten-{config[api_name]['maskinporten_client_id']}-credentials\"\nsecret = get_secret_version(project_id=config['keycloak_clients_gcp_project_id'],\n                            shortname=name)\n\n# The credentials are stored as yaml. Here we simply use a regex to parse. \nkeycloak_client_id = re.search(r'\"client_id\": \"(.*)\"', secret).group(1)\nkeycloak_client_secret = re.search(r'\"client_secret\": \"(.*)\"', secret).group(1)\n\n# Get Keycloak access token\n# This token includes custom claims with values such as maskinporten_default_scopes\nresponse = requests.post(f\"{config['keycloak_url']}/realms/ssb/protocol/openid-connect/token\",\n    headers={\n        \"Content-type\": \"application/x-www-form-urlencoded\",\n    },\n    auth=(keycloak_client_id, keycloak_client_secret),\n    data={\"grant_type\": \"client_credentials\"}\n)\nkeycloak_access_token = response.json()['access_token']\n\n# Get Maskinporten access token from Maskinporten Guardian (using the Keycloak token from above)\n# Note that you can specify custom scopes in the request body if you need to. Using defaults defined in the client config if not specified.\nrequest_body={}\nresponse = requests.post(f\"{config['guardian_url']}/maskinporten/access-token\",\n            headers={\n                \"Authorization\": f\"Bearer {keycloak_access_token}\",\n                \"Content-type\": \"application/json\",\n            },\n            json=request_body,\n        )\nmaskinporten_access_token = response.json()['accessToken']\n\n# Then use the maskinporten access token to query the external API...\nStøttefunksjon for å hente ut secrets fra Secret Manager\nfrom dapla import AuthClient\nfrom google.cloud import secretmanager\n\ndef get_secret_version(project_id, shortname, version_id='latest'):\n    \"\"\"\n    Access the payload for a given secret version.\n    The user's google credentials are used to authorize that the user have permission\n    to access the secret_id.\n    \n    Args:\n    - project_id (str): ID of the Google Cloud project where the secret is stored.\n    - shortname (str): Name (not full path) of the secret in Secret Manager.\n    - version_id (str, optional): The version of the secret to access. Defaults to 'latest'.\n\n    Returns:\n    - str: The payload of the secret version as a UTF-8 decoded string.\n    \"\"\"\n    client = secretmanager.SecretManagerServiceClient(credentials=AuthClient.fetch_google_credentials())\n    secret_name = f\"projects/{project_id}/secrets/{shortname}/versions/{version_id}\"\n    response = client.access_secret_version(name=secret_name)\n    return response.payload.data.decode(\"UTF-8\")\n\n\n\n\"\"\"\nRetrieve maskinporten personal access token for querying an external API.\n\nThe following code example expects a toml config file to exist named\n&lt;team_uniform_name&gt;-&lt;env&gt;-maskinporten-pers.toml (e.g. play-foeniks-prod-maskinporten-pers.toml)\n\nwith contents such as:\n\nguardian_url = \"https://guardian.dapla-staging.ssb.no\"\n\n[my-api]\n    maskinporten_client_id = \"12345678-9abc-def0-1234-567890abcdef\"\n    scopes = [\"some:scope1\", \"some:scope2\"]\n\"\"\"\nimport os\nimport requests\nimport toml\nfrom dapla import AuthClient\n\nteam_uniform_name = \"play-foeniks\"\napi_name = \"my-api\" # identifies the API in the config.toml file\n\n# Check which environment we're running and load corresponding config\ndapla_env = os.getenv(\"DAPLA_ENVIRONMENT\").lower()\nconfig = toml.load(f\"{team_uniform_name}-{dapla_env}-maskinporten-pers.toml\")\n\n# Get Keycloak access token\nkeycloak_access_token = AuthClient.fetch_personal_token()\n\n# Get Maskinporten access token from Maskinporten Guardian (using the personal Keycloak token from above)\nrequest_body = {\n  \"maskinportenClientId\": config[api_name]['maskinporten_client_id'],\n  \"scopes\": config[api_name]['scopes']\n}\nresponse = requests.post(f\"{config['guardian_url']}/maskinporten/access-token\",\n            headers={\n                \"Authorization\": f\"Bearer {keycloak_access_token}\",\n                \"Content-type\": \"application/json\",\n            },\n            json=request_body\n        )\n\nmaskinporten_access_token = response.json()['accessToken']\n\n# Finally, use the maskinporten access token to query the external API\nprint(maskinporten_access_token)",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#maskinporten",
    "href": "statistikkere/maskinporten-guardian.html#maskinporten",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "Maskinporten er en tjeneste fra Digdir som gir sikker autentisering og tilgangskontroll for datautveksling mellom virksomheter.\nTilgangsstyring for data administeres via et webgrensesnitt (Samarbeidsportalen) En virksomhet som deler data (API-tilbyder) definerer såkalte API scopes, og velger hvilke andre virksomheter (API-konsumenter) som skal ha tilgang til disse. API-konsumenter kan på sin side selv opprette en eller flere Maskinporten-klienter og gir de tilgang til API scopes som er delt med virksomheten.\n\n\n\n\n\n\nFigur 1: Datauveksling mellom virksomheter\n\n\n\nLes mer om Maskinporten her.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#hva-gjør-maskinporten-guardian",
    "href": "statistikkere/maskinporten-guardian.html#hva-gjør-maskinporten-guardian",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "Utveksling av data fra en API-tilbyder gjøres ved å inkludere et sikkerhetstoken som hentes fra Maskinporten på vegne av virksomheten man representerer (f. eks SSB). Hvilke data (API scopes) et sikkerhetstoken har tilgang til er knyttet til Maskinporten-klienten. For å hente et sikkerhetstoken for en klient, kreves det at man autentiserer seg som virksomhet. Dette gjøres ved å signere forespørsler til Maskinporten ved bruk av et virksomhetssertifikat. Det er her Maskinporten Guardian kommer inn i bildet.\nMaskinporten Guardian har tilgang til SSBs virksomhetssertifikat og kan dermed signere forespørsler mot Maskinporten for å hente ut sikkerhetstokens.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#komme-igang",
    "href": "statistikkere/maskinporten-guardian.html#komme-igang",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "Hvis en skal ta i bruk et API som er beskyttet av Maskinporten er det noen steg som må gjøres i forkant:\n\n\n\n\n\nflowchart TD\n    A[API-tilbyder gir SSB tilgang til API via Samarbeidsportalen]\n    A --&gt; B[M2M-teamet hos SSB oppretter Maskinporten-klienter]\n    B --&gt; C[Keycloak-klienter for Maskinporten Guardian opprettes]\n\n\n\n\n\n\n\nAPI-tilbyderen må gi SSB tilgang til et API scope. API-tilbydere kan gjøre dette via Samarbeidsportalen hos Digdir. Fra API-tilbyderen får du også annen informasjon om API-ene, som:\n\n\nURL-er som skal benyttes for å snakke med API-et\nNavn på API scopes\nOm det finnes testadata\nDokumentasjon for API-endepunktene\n\n\nNår data er delt av en API-tilbyder, og en har navnet på API scopes, kan M2M-teamet hos SSB kontaktes for å få opprettet Maskinporten-klienter, én pr miljø (f. eks prod og test). De må vite hvilke API scopes og miljøer (test/prod) som skal benyttes. M2M-teamet vil gi deg ID-er (f. eks 12345678-9abc-def0-1234-567890abcdef) for klientene som er blitt opprettet.\n\n\n\n\n\n\n\nDu kan anse ID for en Maskinporten-klient som et slags brukernavn, og behandle dette deretter. Slike ID-er er ikke sensitive i seg selv, men de bør allikevel ikke ligge i åpne git-repoer (SSB-private repoer er OK). Det er heller ingen grunn til å behandle disse som hemmeligheter. Det anbefales at man opererer med Maskinporten-klienter som ekstern konfigurasjon til koden, f. eks på samme måte som man behandler URL-er til API-ene.\n\n\n\n\nNår du har ID for Maskinporten-klienten(e), er neste steg å få opprettet en Keycloak-bruker for Maskinporten Guardian. Se Opprette en Maskinporten Guardian M2M-bruker. Ta kontakt med oss på Dapla via Pureservice dersom du trenger hjelp til dette. Client ID og client secret for Keycloak-brukeren kan hentes fra Secret Manager (mer om det lenger ned). Medlemmer av Dapla-teamet har tilgang til å hente disse.\n\nOm noen på teamet trenger personlig tilgang til API-ene så må det konfigureres i Maskinporten Guardian. Da må vi vite hvilke personer som skal ha denne tilgangen. Les mer om forskjellen på M2M og personlig tilgang lenger ned.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#administrasjon-av-api-integrasjoner-i-ssb",
    "href": "statistikkere/maskinporten-guardian.html#administrasjon-av-api-integrasjoner-i-ssb",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "Det er M2M-teamet i SSB som administrerer det formelle i forbindelse med integrasjoner mot eksterne API-er. For å få opprettet nye Maskinporten-klienter er det dette teamet man kontakter. De må vite hvilke miljøer (test og/eller prod) og hvilke API scopes som skal benyttes. Hver Maskinporten-klient som opprettes identifiseres av en ID (f. eks 12345678-9abc-def0-1234-567890abcdef), som du vil få tilsendt.\nLegg merke til at det er SSB selv som oppretter og administrerer Maskinporten-klienter. Det gjøres i Samarbeidsportalen hos Difi. Klientene knyttes til API scope(s) (som er delt med SSB av API-tilbyderen). Du kan lese mer om hvordan M2M-teamet administrerer Maskinporten-klienter her.\n\n\n\n\n\n\nAlle med en SSB-epostadresse kan registerere en personlig bruker i Digdir Samarbeidsportalen for å få innsyn i hvilke API-integrasjoner som finnes.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#hvordan-maskinporten-guardian",
    "href": "statistikkere/maskinporten-guardian.html#hvordan-maskinporten-guardian",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "Maskinporten Guardian er tilgjengelig fra alle SSB og NAIS sine IP-adresser, og kan nås på:\n\nProd: [https://guardian.dapla.ssb.no]\nTest: [https://guardian.dapla-staging.ssb.no]\n\nMaskinporten Guardian sine endepunkter er selv beskyttet av Keycloak. To typer brukere støttes:\n\nMaskin-til-maskin (M2M) - Systembruker knyttet til en gitt Maskinporten-klient. For å opptre på vegne av en M2M-bruker autentiserer man seg med en Keycloak client secret. Denne hemmeligheten er lagret i Google Secret Manager og kun Service Accounts eller Dapla-grupper med tilgang kan hente den ut.\nPersonlig - Din egen SSB-bruker. Man kan f. eks bruke dapla-toolbelt for å hente ut sitt personlige Keycloak-token. I tillegg til å autentisere deg må din bruker være autorisert til å gjøre oppslag på vegne av en Maskinporten-klient. Dette styres i konfigurasjonen til Maskinporten Guardian.\n\n\n\n\n\n\n\nMan skal i hovedsak kun anvende M2M-brukere for datautveksling mot API-er som er beskyttet av Maskinporten. Personlige brukere skal kun brukes unntaksvis for enkeltoppslag (f. eks ved feilsøking) mot API-er eller for utvikling og test.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#systemskisse",
    "href": "statistikkere/maskinporten-guardian.html#systemskisse",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "Følgende gir en oversikt over hvordan systemer henger sammen. En API-konsument kan f. eks være et Dapla-team som ønsker å hente data, mens en API-tilbyder er en ekstern virksomhet som tilbyr data via Maskinporten. Det er noen forskjeller i flyt avhengig av om Maskinporten Guardian aksesseres med systembruker (M2M) eller personlig bruker.\n\n\n\n\n\n\nFigur 2\n\n\n\n\n\n\nAPI-konsumenten henter sin Keycloak client secret for en gitt Maskinporten-klient fra Secret Manager.\nAPI-konsumenten henter et Keycloak sikkerhetstoken ved å bruke client secret fra steg 1.\nAPI-konsumenten henter et Maskinporten sikkerhetstoken fra Maskinporten Guardian ved å bruke Keycloak sikkerhetstoken fra steg 2. En kan alternativt angi andre API scopes enn det som er standard for Maskinporten-klienten, men dette er vanligvis ikke nødvendig.\nMaskinporten Guardian bruker SSB sitt virksomhetssertifikat til å signere en forespørsel om å hente et sikkerhetstoken fra Maskinporten (ref steg 3)\nAPI-konsumenten henter data fra den eksterne API-tilbyderen ved å bruke sikkerhetstoken fra Maskinporten.\n\n\n\n\n\n\n\n\n\n\nSteg 1 og 2 gjelder kun for M2M-brukere. Dersom man aksesserer Maskinporten Guardian med personlig bruker så hentes Keycloak-tokenet f. eks ved hjelp av AuthClient i dapla-toolbelt.\n\n\n\n\nAPI-konsumenten henter et Maskinporten sikkerhetstoken fra Maskinporten Guardian ved å bruke sitt personlige Keycloak sikkerhetstoken. Det må angis hvilken Maskinporten-klient og hvilke API scopes Maskinporten sikkerhetstokenet skal gjelde for. Den personlige brukeren må på forhånd være autorisert (ref Maskinporten Guardian sin tilgangskonfigurasjon) til å kunne hente sikkerthetstokens for Maskinporten-klienten.\nMaskinporten Guardian bruker SSB sitt virksomhetssertifikat til å signere en forespørsel om å hente et sikkerhetstoken fra Maskinporten (ref steg 3)\nAPI-konsumenten henter data fra den eksterne API-tilbyderen ved å bruke sikkerhetstoken fra Maskinporten.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#opprette-en-maskinporten-guardian-m2m-bruker",
    "href": "statistikkere/maskinporten-guardian.html#opprette-en-maskinporten-guardian-m2m-bruker",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "Note\n\n\n\nDette avsnittet inneholder tekniske instrukser ment for deg som er kjent med Git og som f. eks jobber i et Self-managed Dapla-team. Ta kontakt med oss via Pureservice så hjelper vi deg gjerne med dette.\n\n\nFor å kunne opptre på vegne av Maskinporten-klienten uavhengig av din personlige bruker, må man opprette en Keycloak systembruker. Det gjøres ved å åpne en Pull Request (konfigurasjon som gjennomgås av en tekniker) til keycloak-iac der du angir informasjon som ID for Maskinporten-klient, API scopes og hvem som skal ha tilgang. Legg merke til at du må opprette en klient pr miljø (test og prod). Du kan se bort fra play-miljøet.\n\n\namends \".../pkl/MaskinportenGuardianClient.pkl\"\n\napi_shortname = \"Kort API-beskrivelse (maks 32 tegn)\"\nmaskinporten_client_id = \"12345678-9abc-def0-1234-567890abcdef\"\nmaskinporten_audience = \"https://maskinporten.no/\"\nmaskinporten_default_scopes {\n  \"foo:data1\"\n  \"foo:data2\"\n}\ncredentials_access {\n  \"group:play-foeniks-data-admins@groups.ssb.no\"\n  \"serviceaccount:foo-sa@play-foeniks-p-ab.iam.gserviceaccount.com\"\n}\n\n\n\n\n\n\nNote\n\n\n\nI test skal maskinporten_audience ha verdien https://test.maskinporten.no/. I prod skal det være https://maskinporten.no/ (merk: skråstrek på slutten er viktig)\n\n\nPull Requesten må godkjennes og behandles av en Dapla platformutvikler. Når dette er gjort blir det opprettet en Keycloak-klient, og hemmeligheten som kan brukes for å hente ut sikkerhetstokens for denne klienten er tilgjengelig i Secret Manager.\nSe følgende dokumentasjon for mer informasjon:\n\nDetaljert beskrivelse av konfigurasjonsmuligeter.\nGenerell beskrivelse av hvordan man oppretter en Keycloak-klient\nKeycloak client credentials\n\nTa kontakt med Kundeservice hvis du har spørsmål eller trenger ei hand å halde i.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#kodeeksempler",
    "href": "statistikkere/maskinporten-guardian.html#kodeeksempler",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "Følgende viser Python kodeeksempler for hvordan man kan hente ut et Maskinporten sikkerhetstoken.\n\n\n\"\"\"\nRetrieve maskinporten M2M access token for querying an external API.\n\nThe following code example expects a toml config file to exist named\n&lt;team_uniform_name&gt;-&lt;env&gt;-maskinporten-m2m.toml (e.g. play-foeniks-prod-maskinporten-m2m.toml)\n\nwith contents such as:\n\nkeycloak_url = \"https://auth.test.ssb.no\"\nkeycloak_clients_gcp_project_id = \"keycloak-clients-&lt;p|t&gt;-??\"\nguardian_url = \"https://guardian.dapla-staging.ssb.no\"\n\n[my-api]\n    maskinporten_client_id = \"12345678-9abc-def0-1234-567890abcdef\"\n\"\"\"\nimport os\nimport re\nimport requests\nimport toml\n\n\nteam_uniform_name = \"play-foeniks\"\napi_name = \"my-api\" # identifies the API in the config.toml file\n\n# Check which environment we're running and load corresponding config\ndapla_env = os.getenv(\"DAPLA_ENVIRONMENT\").lower()\nconfig = toml.load(f\"{team_uniform_name}-{dapla_env}-maskinporten-m2m.toml\")\n\n# Get Maskinporten Guardian M2M Keycloak client credentials from Google Secret Manager\n# The secret's name is deduced from the team name and maskinporten client id\nname = f\"{team_uniform_name}-ssb-maskinporten-{config[api_name]['maskinporten_client_id']}-credentials\"\nsecret = get_secret_version(project_id=config['keycloak_clients_gcp_project_id'],\n                            shortname=name)\n\n# The credentials are stored as yaml. Here we simply use a regex to parse. \nkeycloak_client_id = re.search(r'\"client_id\": \"(.*)\"', secret).group(1)\nkeycloak_client_secret = re.search(r'\"client_secret\": \"(.*)\"', secret).group(1)\n\n# Get Keycloak access token\n# This token includes custom claims with values such as maskinporten_default_scopes\nresponse = requests.post(f\"{config['keycloak_url']}/realms/ssb/protocol/openid-connect/token\",\n    headers={\n        \"Content-type\": \"application/x-www-form-urlencoded\",\n    },\n    auth=(keycloak_client_id, keycloak_client_secret),\n    data={\"grant_type\": \"client_credentials\"}\n)\nkeycloak_access_token = response.json()['access_token']\n\n# Get Maskinporten access token from Maskinporten Guardian (using the Keycloak token from above)\n# Note that you can specify custom scopes in the request body if you need to. Using defaults defined in the client config if not specified.\nrequest_body={}\nresponse = requests.post(f\"{config['guardian_url']}/maskinporten/access-token\",\n            headers={\n                \"Authorization\": f\"Bearer {keycloak_access_token}\",\n                \"Content-type\": \"application/json\",\n            },\n            json=request_body,\n        )\nmaskinporten_access_token = response.json()['accessToken']\n\n# Then use the maskinporten access token to query the external API...\nStøttefunksjon for å hente ut secrets fra Secret Manager\nfrom dapla import AuthClient\nfrom google.cloud import secretmanager\n\ndef get_secret_version(project_id, shortname, version_id='latest'):\n    \"\"\"\n    Access the payload for a given secret version.\n    The user's google credentials are used to authorize that the user have permission\n    to access the secret_id.\n    \n    Args:\n    - project_id (str): ID of the Google Cloud project where the secret is stored.\n    - shortname (str): Name (not full path) of the secret in Secret Manager.\n    - version_id (str, optional): The version of the secret to access. Defaults to 'latest'.\n\n    Returns:\n    - str: The payload of the secret version as a UTF-8 decoded string.\n    \"\"\"\n    client = secretmanager.SecretManagerServiceClient(credentials=AuthClient.fetch_google_credentials())\n    secret_name = f\"projects/{project_id}/secrets/{shortname}/versions/{version_id}\"\n    response = client.access_secret_version(name=secret_name)\n    return response.payload.data.decode(\"UTF-8\")\n\n\n\n\"\"\"\nRetrieve maskinporten personal access token for querying an external API.\n\nThe following code example expects a toml config file to exist named\n&lt;team_uniform_name&gt;-&lt;env&gt;-maskinporten-pers.toml (e.g. play-foeniks-prod-maskinporten-pers.toml)\n\nwith contents such as:\n\nguardian_url = \"https://guardian.dapla-staging.ssb.no\"\n\n[my-api]\n    maskinporten_client_id = \"12345678-9abc-def0-1234-567890abcdef\"\n    scopes = [\"some:scope1\", \"some:scope2\"]\n\"\"\"\nimport os\nimport requests\nimport toml\nfrom dapla import AuthClient\n\nteam_uniform_name = \"play-foeniks\"\napi_name = \"my-api\" # identifies the API in the config.toml file\n\n# Check which environment we're running and load corresponding config\ndapla_env = os.getenv(\"DAPLA_ENVIRONMENT\").lower()\nconfig = toml.load(f\"{team_uniform_name}-{dapla_env}-maskinporten-pers.toml\")\n\n# Get Keycloak access token\nkeycloak_access_token = AuthClient.fetch_personal_token()\n\n# Get Maskinporten access token from Maskinporten Guardian (using the personal Keycloak token from above)\nrequest_body = {\n  \"maskinportenClientId\": config[api_name]['maskinporten_client_id'],\n  \"scopes\": config[api_name]['scopes']\n}\nresponse = requests.post(f\"{config['guardian_url']}/maskinporten/access-token\",\n            headers={\n                \"Authorization\": f\"Bearer {keycloak_access_token}\",\n                \"Content-type\": \"application/json\",\n            },\n            json=request_body\n        )\n\nmaskinporten_access_token = response.json()['accessToken']\n\n# Finally, use the maskinporten access token to query the external API\nprint(maskinporten_access_token)",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/pakke-install-bakken.html",
    "href": "statistikkere/pakke-install-bakken.html",
    "title": "Installere pakker",
    "section": "",
    "text": "Installering av pakker i Jupyter miljøer på bakken (f.eks https://sl-jupyter-p.ssb.no) foregår stort sett helt lik som på Dapla. Det er én viktig forskjell, og det er at installasjon skjer via en proxy som heter Nexus.\n\n\nPip er ferdig konfigurert for bruk av Nexus og kan kjøres som beskrevet for Dapla\n\n\n\nHvis man bruker Poetry for håndtering av pakker i et prosjekt, så må man kjøre følgende kommando i prosjekt-mappe etter prosjektet er opprettet.\n\n\nterminal\n\npoetry source add --default nexus `echo $PIP_INDEX_URL`\n\nDa får man installere pakker som vanlig f.eks\n\n\nterminal\n\npoetry add matplotlib\n\n\n\n\n\n\n\nHvis man forsøker å installere prosjektet i et annet miljø (f.eks Dapla), så må man først fjerne nexus som kilde ved å kjøre:\n\n\nterminal\n\npoetry source remove nexus\n\n\n\n\n\n\n\n\nProsessen med å installere pakker for R på bakken er det samme som på Dapla. Noen pakker (for eksempel devtools) kan foreløpig ikke installeres på bakken på egenhånd pga 3. parti avhengigheter. Vi jobber med å finne en løsning til dette.\nFor å installere arrow, kopier og kjør følgende kommando i R:\n\n\nnotebook\n\ninstall.packages(\"/ssb/bruker/felles/R_pakker/arrow_11.0.0.2.tar.gz\",\n                repos = NULL,\n                type = \"source\")"
  },
  {
    "objectID": "statistikkere/pakke-install-bakken.html#python",
    "href": "statistikkere/pakke-install-bakken.html#python",
    "title": "Installere pakker",
    "section": "",
    "text": "Installering av pakker i Jupyter miljøer på bakken (f.eks https://sl-jupyter-p.ssb.no) foregår stort sett helt lik som på Dapla. Det er én viktig forskjell, og det er at installasjon skjer via en proxy som heter Nexus.\n\n\nPip er ferdig konfigurert for bruk av Nexus og kan kjøres som beskrevet for Dapla\n\n\n\nHvis man bruker Poetry for håndtering av pakker i et prosjekt, så må man kjøre følgende kommando i prosjekt-mappe etter prosjektet er opprettet.\n\n\nterminal\n\npoetry source add --default nexus `echo $PIP_INDEX_URL`\n\nDa får man installere pakker som vanlig f.eks\n\n\nterminal\n\npoetry add matplotlib\n\n\n\n\n\n\n\nHvis man forsøker å installere prosjektet i et annet miljø (f.eks Dapla), så må man først fjerne nexus som kilde ved å kjøre:\n\n\nterminal\n\npoetry source remove nexus"
  },
  {
    "objectID": "statistikkere/pakke-install-bakken.html#r",
    "href": "statistikkere/pakke-install-bakken.html#r",
    "title": "Installere pakker",
    "section": "",
    "text": "Prosessen med å installere pakker for R på bakken er det samme som på Dapla. Noen pakker (for eksempel devtools) kan foreløpig ikke installeres på bakken på egenhånd pga 3. parti avhengigheter. Vi jobber med å finne en løsning til dette.\nFor å installere arrow, kopier og kjør følgende kommando i R:\n\n\nnotebook\n\ninstall.packages(\"/ssb/bruker/felles/R_pakker/arrow_11.0.0.2.tar.gz\",\n                repos = NULL,\n                type = \"source\")"
  },
  {
    "objectID": "statistikkere/jobbe-med-data.html",
    "href": "statistikkere/jobbe-med-data.html",
    "title": "Jobbe med data",
    "section": "",
    "text": "Når vi oppretter et dapla-team så får vi tildelt et eget området for lagring av data. For å kunne lese og skrive data fra Jupyter til disse områdene må vi autentisere oss, siden Jupyter og lagringsområdet er to separate sikkerhetsoner.\nFigur 1 viser dette klarer skillet mellom hvor vi koder og hvor dataene ligger på Dapla1. I dette kapitlet beskriver vi nærmere hvordan du kan jobbe med dataene dine på Dapla.\n\n\n\n\n\n\nFigur 1: Tydelig skille mellom kodemiljø og datalagring på Dapla.\n\n\n\n\n\nFor å gjøre det enklere å jobbe data på tvers av Jupyter og lagringsområdet er det laget noen egne SSB-utviklede biblioteker for å gjøre vanlige operasjoner mot lagringsområdet. Siden både R og Python skal brukes på Dapla, så er det laget to biblioteker, en for hver av disse språkene. fellesr er biblioteket for R, og dapla-toolbelt er biblioteket for Python.\n\n\ndapla-toolbelt er en en pakke som lar deg enkelt lese og skrive til lagringsområdet uten å måtte autentifisere deg manuelt. Den har en Pandas-aktig syntaks som forhåpentlig er gjenkjennbar for de fleste. Pakken er installert i alle Python-kernels på Dapla, så du trenger ikke å installere den selv hvis du åpner en notebook med Python3 for eksempel. For å importere hele biblioteket i en notebook skriver du bare\n\n\nnotebook\n\nimport dapla as dp\n\ndapla-toolbelt bruker en pakke som heter gcsfs for å kommunisere med lagringsområdet. gcsfs er en pakke som lar deg bruke Google Cloud Storage (GCS) som om det var en filsystem. Det betyr at du kan bruke samme syntaks som du bruker for å lese og skrive til filer på din egen maskin. Du kan lese mulighetene i gcsfs her. Et eksempel på hvordan de to pakkene kan brukes sammen ser du her:\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Example of how you can use gcsfs and dapla-toolbelt together\nfs.touch(\"gs://my-bucket/my-folder/\")\n\nI koden over brukte jeg kommandoen touch fra gcsfs og FileClient fra dapla-toolbelt for å opprette en mappe i lagringsområdet.\nI kapitlene under finner du konkrete eksempler på hvordan du kan bruke dapla-toolbelt til å jobbe med data i SSBs lagringsområdet.\n\n\n\nR-pakken fellesr er under utvikling og gir mye av den samme funksjonaliteten som dapla-toolbelt gir for Python. I tillegg til å kunne lese og skrive til lagringsområdet på Dapla, så har fellesr også funksjoner for å jobbe med metadata på Dapla.\nfellesr er installert på Dapla og funksjoner kan benyttes ved:\n\n\nnotebook\n\nlibrary(fellesr)\n\nHvis du benytte en renv miljø, må pakken installeres en gang. Dette kan gjøres ved:\n\n\nnotebook\n\nrenv::install(\"statisticsnorway/fellesr\")\n\n\n\n\n\nI denne delen viser vi hvordan man gjør veldig vanlige operasjoner når man koder et produksonsløp for en statistikk. Flere eksempler på nyttige systemkommandoer finner du her.\n\n\n\n\n\n\n\n\nEksempeldata\n\n\n\nDet finnes et område som alle SSB-ansatte har lese- og skrivetilgang til. Det er\ngs://ssb-dapla-felles-data-delt-prod/ i prod-miljøet på Dapla, og\ngs://ssb-dapla-felles-data-delt-test/ i staging-miljøet. Eksemplene under bruker førstnevnte i koden, slik at alle kan kjøre koden selv.\nKode-eksemplene finnes for både R og Python, og du kan velge hvilken du skal se ved å trykke på den arkfanen du er interessert i.\n\n\nÅ liste ut innhold i et gitt mappe på Dapla er ganske enkelt. Under ser du hvordan du kan liste ut innholdet i følgende mappe:\ngs://ssb-dapla-felles-data-delt-prod/felles/veiledning/python/eksempler/purchases\n\nPython \n\n\nVi bruker modulen FileClient fra dapla-toolbelt for å liste ut innholdet i en mappe.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Set path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\nFileClient.ls(f\"{bucket}/{folder}\")\n\nMed kommandoen over får du listet ut alle filnavn i mappen. Hvis du vil ha mer informasjon om filene så kan du bruke ls-kommandoen med detail = True, som under:\n\n\nnotebook\n\nFileClient.ls(f\"{bucket}/{folder}\", detail = True)\n\nSyntaksen med ls er veldig lik det man kjenner fra en Linux-terminal. Men når vi bruker detail = True blir metadata om filene returnert som en Python-liste med dictionaries. Det kan være svært nyttig når du f.eks. trenger å vite dato og tidspunkt for når en fil ble opprettet, eller når den sist ble oppdatert.\n\n\n\n\nnotebook\n\n# Loading functions into notebook\nlibrary(fellesr)\n\n# Path to folder\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod/\"\nfolder &lt;- \"felles/veiledning/python/eksempler/purchases\"\n\n# List files in folder \nlist.files(paste0(bucket, folder))\n\nMerknad: Når du spesifisere bøtter i R, trenger du ikke “gs://” foran.\n\n\n\n\n\n\nÅ skrive filer til et lagringsområde på Dapla er også ganske enkelt. Det ligner mye på den syntaksen vi er kjent med fra vanlige R- og Python-pakker, med noen små unntak.\n\n\nUnder lager vi en dataframe i en notebook og skriver den ut til en parquet-fil.\n\nPython \n\n\nNår vi leser en Parquet-fil med dapla-toolbelt så bruker den pyarrow i bakgrunnen. Dette er en av de raskeste måtene å lese og skrive Parquet-filer på.\n\n\nnotebook\n\nimport dapla as dp\nimport pandas as pd\nimport numpy as np\n\n# Set path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Create pandas dataframe\npurchases = pd.DataFrame(np.random.randn(10, 5),\n                        columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\n# Write pandas dataframe as parquet to GCS bucket\ndp.write_pandas(df = purchases,\n                gcs_path = f\"{bucket}/{folder}/data.parquet\",\n                file_format = \"parquet\",)\n\nNår vi kalte write_pandas over så spesifiserte vi at filformatet skulle være parquet. Dette er default, så vi kunne også ha skrevet det slik:\n\n\nnotebook\n\ndp.write_pandas(df = purchases,\n                gcs_path = f\"{bucket}/{folder}/data.parquet\")\n\nMen for de andre filformatene må vi altså spesifisere dette.\n\n\nNår vi jobber med Parquet-fil i R, bruker vi pakken arrow. Dette er en del av fellesr pakken så du trenger kun å kalle inn dette. Pakken inneholder funksjonen write_SSB som kan brukes til å skrive data til bøtte på Dapla.\n\n\nnotebook\n\nlibrary(fellesr)\n\n# Set stien til hvor data skal lagres\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"felles/veiledning/r/eksempler/purchases\"\n\n# Lage en eksempel dataframe\npurchases = data.frame(A = runif(10), B= runif(10), C=runif(10))\n\n# Skrive data til bøttet som en parquet\nwrite_SSB(purchases, file.path(bucket, folder, \"purchases.parquet\"))\n\nMerknad: Når du spesifisere bøtter i R, trenger du ikke “gs://” foran.\n\n\n\n\n\n\nNoen ganger ønsker vi å lagre data i andre formatter slik som CSV, JSON og XML.\n\nPython \n\n\ndapla-toolbelt kan skrive ut json, csv og posisjonsfiler (fixed-width-files/fwf) til lagringsområdet. Måten den gjør det på er å bruke Pandas sine funksjoner read_json, read_csv, read_fwf under panseret. Dette kan være nyttig å vite for skjønne hvordan dapla-toolbelt håndterer ulike strukturer i (spesielt hierarkiske) tekstfiler. Under ser du hvordan du kan skrive ut en dataframe til en json-fil.\n\n\nnotebook\n\nimport numpy as np\nimport pandas as pd\nfrom dapla import FileClient\n\n# Set path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Create a dataframe with Pandas\ndf = pd.DataFrame(np.random.randn(10, 5), columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\n# Save dataframe as json with dapla-toolbelt\ndp.write_pandas(df = df,\n                gcs_path = f\"{bucket}/{folder}/test.json\",\n                file_format = \"json\")\n\nSom vi ser at syntaksen over så kunne vi skrevet ut til noe annet enn json ved å endre verdien i argumentet file_format.\n\n\nPakken fellesr kan også brukes til å skrive andre type filer, for eksempel csv, til bøtter. Dette gjøres med funksjonen write_SSB og spesifisere ønsket filtype i filnavn.\nFørst kaller vi biblioteket og lage noe test data ved:\n\n\nnotebook\n\nlibrary(fellesr)\n\n# Set stien til hvor data skal lagres\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"felles/veiledning/r/eksempler/purchases\"\n\n# Lage en eksempel dataframe\npurchases = data.frame(A = runif(10), B= runif(10), C=runif(10))\n\n# Skrive til csv\nwrite_SSB(purchases, file.path(bucket, folder, \"purchases.csv\")\n\n\n\n\n\n\n\nDet er ikke anbefalt å bruke xlsx-formatet, men her ser du hvordan det kan skrives ut. Mer kommer.\n\nPython \n\n\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import AuthClient\n\n# Henter token for å kunne lese fra Dapla\ntoken = AuthClient.fetch_google_credentials()\n\n# Filsti\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\ndf.to_excel(f\"{bucket}/{folder}/test.xlsx\",\n           storage_options={\"token\": token})\n\n\n\nKommer snart\n\n\n\n\n\n\n\nUnder finner du eksempler på hvordan du kan lese inn data til en Jupyter Notebooks på Dapla.\n\n\n\nPython \n\n\n\n\nnotebook\n\nimport dapla as dp\n\n# Set path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Read path into pandas dataframe \ndp.read_pandas(gcs_path= f\"{bucket}/{folder}/data.parquet\",\n               file_format = \"parquet\",\n               columns = None,)\n\nSom vi så med write_pandas så er file_format default satt til parquet, og default for columns = None, så vi kunne også ha skrevet det slik:\ndp.read_pandas(gcs_path= f\"{bucket}/{folder}/data.parquet\")\ncolumns-argumentet er en liste med kolonnenavn som vi ønsker å lese inn. Hvis vi ikke spesifiserer noen kolonner så vil alle kolonnene leses inn.\n\n\nPakken fellesr kan brukes til å lese inn data. Funksjonen read_SSB() kan lese inn filer i flere format inkluderende parquet.\nHer er et eksempel av å lese inn parquet fil “1987”.\n\n\nnotebook\n\nlibrary(fellesr)\n\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"R_smoke_test\"\n\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.parquet\"))\n\nVi kan også filtrere hvilke variabel vi ønsker å lese inn ved å spesifisere parameter col_select. For eksempel:\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.parquet\"),\n                    col_select = c(\"Year\", \"Month\"))\nInnlesning av parquet som er kartdata finner du her: Lese kartdata\n\n\n\n\n\n\nKommer mer snart. Python-koden under bygger på eksempelet over.\n\nPython \n\n\n\n\nnotebook\n\nimport dapla as dp\n\n# Path to write to\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Read in json-file from dapla-storage\ndf = dp.read_pandas(gcs_path = f\"{bucket}/{folder}/test3.json\",\n               file_format = \"json\")\n\n\n\nFunksjonen read_SSB() kan lese inn flere type av fil-format, slik som csv og json. Du trenger ikke å endre koden, kun spesifisere hele filnavn.\nFørst kaller vi inn biblioteket fellesr og spesifisere bøtte/mappen:\n\n\nnotebook\n\nlibrary(fellesr)\n\n# Filsti\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"R_smoke_test\"\n\n# Lese inn CSV-fil\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.csv\"))\n\nFor å lese inn en json-fil kan skrive følgende:\n\n\nnotebook\n\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.json\"))\n\n\n\n\n\n\n\n\nPython \n\n\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import AuthClient\n\n# Hent token\ntoken = AuthClient.fetch_google_credentials()\n\n# Les inn fil\ndf = pd.read_excel(\"gs://ssb-prod-arbmark-skjema-data-produkt/test_gcp.xlsx\",\n    storage_options={\"token\": token})\n\n\n\nKommer snart\n\n\n\n\n\n\nHer er et eksempel på hvordan man leser inn en sas7bdat-fil på Dapla som har blitt generert i prodsonen.\n\nPython \n\n\n\n\nnotebook\n\nimport dapla as dp\n\nsti = \"gs://ssb-dapla-felles-data-delt-prod/felles/veiledning/sas/statbank_ledstill.sas7bdat\"\n\ndp.read_pandas(sti, file_format=\"sas7bdat\", encoding=\"latin1\")\n\n\n\nSiden innlesing av sas7bdat-filer ikke er støttet i fellesr, så kan vi bruke R-pakken reticulate for å benytte oss av funksjonaliteten i Python-pakken dapla-toolbelt.\n\n\nnotebook\n\nlibrary(reticulate)\ndp &lt;- import(\"dapla\")\nsti  &lt;- gs://ssb-dapla-felles-data-delt-prod/felles/veiledning/sas/statbank_ledstill.sas7bdat\"\ndp$read_pandas(sti, file_format=\"sas7bdat\", encoding=\"latin1\")\n\n\n\n\n\n\n\n\nÅ slette filer fra lagringsområdet kan gjøres på flere måter. I kapitlet om sletting av data viste vi hvordan man gjør det med pek-og-klikk i Google Cloud Console. Under ser du hvordan du kan slette filer med dapla-toolbelt og gcsfs eller fellesr.\n\nPython \n\n\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\nfs.rm(f\"{bucket}/{from_folder}/df.json\")\n\n\n\nFunksjonen gc_delete_object kan brukes til å slette data på lagringsområdet.\n\n\nnotebook\n\nlibrary(fellesr)\n\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"felles/veiledning/r/eksempler/purchases\"\n\ngcs_delete_object(file.path(bucket, folder, \"purchases.parquet\"))\n\n\n\n\n\n\n\nÅ kopiere filer mellom mapper på et Linux-filsystem innebærer som regel bruke cp-kommandoen. På Dapla er det ikke så mye forskjell. Vi bruker en ligende tilnærming nå vi skal kopiere mellom bøtter eller mapper på lagringsområdet til SSB. Under ser du hvordan du kan kopiere en fil fra en mappe til en annen.\n\nPython \n\n\nLa oss begynne med et eksempel der vi kopierer en fil fra en mappe til en annen i samme bøtte.\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Path to folders\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfrom_folder = \"felles/veiledning/python/eksempler/purchases\"\nto_folder = \"felles/veiledning/python/eksempler\"\n\n# Copy file\nfs.cp(f\"{bucket}/{from_folder}/data.parquet\",\n      f\"{bucket}/{to_folder}/data_copy.parquet\")\n\nDet også fungere for å kopiere filer mellom bøtter.\nEt annet scenario vi ofte vil støte på er at vi ønsker å kopiere en fil fra vårt Jupyter-filsystem til en mappe på lagringsområdet. Her kan vi bruke fs.put-metoden.\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Create a new file in your home directory called test.txt\nwith open('/home/jovyan/test.txt', 'w') as f:\n    f.write('Create a new text file!')\n\n#Path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler\"\n\n# Copy file from local to remote file system\nfs.put(lpath=f\"/home/jovyan/test.txt\", rpath=f\"{bucket}/{folder}/test.txt\")\n\nØnsker vi å kopiere en hel mappe fra lagringsområdet til Jupyter-filsystemet, kan vi bruke fs.get-metoden, med opsjonen recursive=True.\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Copy file\nfs.get(&lt;from_bucket&gt;,\n      \"/home/jovyan/sesongjustering/\",\n      recursive=True)\n\n\n\nKommer snart\n\n\n\n\n\n\n\nPython \n\n\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfrom_folder = \"felles/veiledning/python/eksempler/purchases\"\nto_folder = \"felles/veiledning/python/eksempler\"\n\nfs.mv(f\"{bucket}/{from_folder}/data.parquet\", f\"{bucket}/{to_folder}/data.parquet\")\n\n\n\nKommer snart\n\n\n\n\n\n\nSelv om bøtter ikke har mapper med en hierarkisk struktur slik man er kjent med fra klassike filsystemer, så kan man opprette det som ser ut som mapper i objektnavnet. I realiteten blir bare / oppfattet som en del av navnet på objektet. Skulle du likevel ønske å opprette dette så kan du gjøre det følgende måte:\n\nPython \n\n\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n#Path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler\"\n\n# Create folder\nfs.touch(f\"{bucket}/{folder}/testmappe/\")\n\n\n\nKommer snart",
    "crumbs": [
      "Manual",
      "Data"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-data.html#ssb-biblioteker",
    "href": "statistikkere/jobbe-med-data.html#ssb-biblioteker",
    "title": "Jobbe med data",
    "section": "",
    "text": "For å gjøre det enklere å jobbe data på tvers av Jupyter og lagringsområdet er det laget noen egne SSB-utviklede biblioteker for å gjøre vanlige operasjoner mot lagringsområdet. Siden både R og Python skal brukes på Dapla, så er det laget to biblioteker, en for hver av disse språkene. fellesr er biblioteket for R, og dapla-toolbelt er biblioteket for Python.\n\n\ndapla-toolbelt er en en pakke som lar deg enkelt lese og skrive til lagringsområdet uten å måtte autentifisere deg manuelt. Den har en Pandas-aktig syntaks som forhåpentlig er gjenkjennbar for de fleste. Pakken er installert i alle Python-kernels på Dapla, så du trenger ikke å installere den selv hvis du åpner en notebook med Python3 for eksempel. For å importere hele biblioteket i en notebook skriver du bare\n\n\nnotebook\n\nimport dapla as dp\n\ndapla-toolbelt bruker en pakke som heter gcsfs for å kommunisere med lagringsområdet. gcsfs er en pakke som lar deg bruke Google Cloud Storage (GCS) som om det var en filsystem. Det betyr at du kan bruke samme syntaks som du bruker for å lese og skrive til filer på din egen maskin. Du kan lese mulighetene i gcsfs her. Et eksempel på hvordan de to pakkene kan brukes sammen ser du her:\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Example of how you can use gcsfs and dapla-toolbelt together\nfs.touch(\"gs://my-bucket/my-folder/\")\n\nI koden over brukte jeg kommandoen touch fra gcsfs og FileClient fra dapla-toolbelt for å opprette en mappe i lagringsområdet.\nI kapitlene under finner du konkrete eksempler på hvordan du kan bruke dapla-toolbelt til å jobbe med data i SSBs lagringsområdet.\n\n\n\nR-pakken fellesr er under utvikling og gir mye av den samme funksjonaliteten som dapla-toolbelt gir for Python. I tillegg til å kunne lese og skrive til lagringsområdet på Dapla, så har fellesr også funksjoner for å jobbe med metadata på Dapla.\nfellesr er installert på Dapla og funksjoner kan benyttes ved:\n\n\nnotebook\n\nlibrary(fellesr)\n\nHvis du benytte en renv miljø, må pakken installeres en gang. Dette kan gjøres ved:\n\n\nnotebook\n\nrenv::install(\"statisticsnorway/fellesr\")",
    "crumbs": [
      "Manual",
      "Data"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-data.html#vanlige-operasjoner",
    "href": "statistikkere/jobbe-med-data.html#vanlige-operasjoner",
    "title": "Jobbe med data",
    "section": "",
    "text": "I denne delen viser vi hvordan man gjør veldig vanlige operasjoner når man koder et produksonsløp for en statistikk. Flere eksempler på nyttige systemkommandoer finner du her.\n\n\n\n\n\n\n\n\nEksempeldata\n\n\n\nDet finnes et område som alle SSB-ansatte har lese- og skrivetilgang til. Det er\ngs://ssb-dapla-felles-data-delt-prod/ i prod-miljøet på Dapla, og\ngs://ssb-dapla-felles-data-delt-test/ i staging-miljøet. Eksemplene under bruker førstnevnte i koden, slik at alle kan kjøre koden selv.\nKode-eksemplene finnes for både R og Python, og du kan velge hvilken du skal se ved å trykke på den arkfanen du er interessert i.\n\n\nÅ liste ut innhold i et gitt mappe på Dapla er ganske enkelt. Under ser du hvordan du kan liste ut innholdet i følgende mappe:\ngs://ssb-dapla-felles-data-delt-prod/felles/veiledning/python/eksempler/purchases\n\nPython \n\n\nVi bruker modulen FileClient fra dapla-toolbelt for å liste ut innholdet i en mappe.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Set path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\nFileClient.ls(f\"{bucket}/{folder}\")\n\nMed kommandoen over får du listet ut alle filnavn i mappen. Hvis du vil ha mer informasjon om filene så kan du bruke ls-kommandoen med detail = True, som under:\n\n\nnotebook\n\nFileClient.ls(f\"{bucket}/{folder}\", detail = True)\n\nSyntaksen med ls er veldig lik det man kjenner fra en Linux-terminal. Men når vi bruker detail = True blir metadata om filene returnert som en Python-liste med dictionaries. Det kan være svært nyttig når du f.eks. trenger å vite dato og tidspunkt for når en fil ble opprettet, eller når den sist ble oppdatert.\n\n\n\n\nnotebook\n\n# Loading functions into notebook\nlibrary(fellesr)\n\n# Path to folder\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod/\"\nfolder &lt;- \"felles/veiledning/python/eksempler/purchases\"\n\n# List files in folder \nlist.files(paste0(bucket, folder))\n\nMerknad: Når du spesifisere bøtter i R, trenger du ikke “gs://” foran.\n\n\n\n\n\n\nÅ skrive filer til et lagringsområde på Dapla er også ganske enkelt. Det ligner mye på den syntaksen vi er kjent med fra vanlige R- og Python-pakker, med noen små unntak.\n\n\nUnder lager vi en dataframe i en notebook og skriver den ut til en parquet-fil.\n\nPython \n\n\nNår vi leser en Parquet-fil med dapla-toolbelt så bruker den pyarrow i bakgrunnen. Dette er en av de raskeste måtene å lese og skrive Parquet-filer på.\n\n\nnotebook\n\nimport dapla as dp\nimport pandas as pd\nimport numpy as np\n\n# Set path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Create pandas dataframe\npurchases = pd.DataFrame(np.random.randn(10, 5),\n                        columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\n# Write pandas dataframe as parquet to GCS bucket\ndp.write_pandas(df = purchases,\n                gcs_path = f\"{bucket}/{folder}/data.parquet\",\n                file_format = \"parquet\",)\n\nNår vi kalte write_pandas over så spesifiserte vi at filformatet skulle være parquet. Dette er default, så vi kunne også ha skrevet det slik:\n\n\nnotebook\n\ndp.write_pandas(df = purchases,\n                gcs_path = f\"{bucket}/{folder}/data.parquet\")\n\nMen for de andre filformatene må vi altså spesifisere dette.\n\n\nNår vi jobber med Parquet-fil i R, bruker vi pakken arrow. Dette er en del av fellesr pakken så du trenger kun å kalle inn dette. Pakken inneholder funksjonen write_SSB som kan brukes til å skrive data til bøtte på Dapla.\n\n\nnotebook\n\nlibrary(fellesr)\n\n# Set stien til hvor data skal lagres\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"felles/veiledning/r/eksempler/purchases\"\n\n# Lage en eksempel dataframe\npurchases = data.frame(A = runif(10), B= runif(10), C=runif(10))\n\n# Skrive data til bøttet som en parquet\nwrite_SSB(purchases, file.path(bucket, folder, \"purchases.parquet\"))\n\nMerknad: Når du spesifisere bøtter i R, trenger du ikke “gs://” foran.\n\n\n\n\n\n\nNoen ganger ønsker vi å lagre data i andre formatter slik som CSV, JSON og XML.\n\nPython \n\n\ndapla-toolbelt kan skrive ut json, csv og posisjonsfiler (fixed-width-files/fwf) til lagringsområdet. Måten den gjør det på er å bruke Pandas sine funksjoner read_json, read_csv, read_fwf under panseret. Dette kan være nyttig å vite for skjønne hvordan dapla-toolbelt håndterer ulike strukturer i (spesielt hierarkiske) tekstfiler. Under ser du hvordan du kan skrive ut en dataframe til en json-fil.\n\n\nnotebook\n\nimport numpy as np\nimport pandas as pd\nfrom dapla import FileClient\n\n# Set path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Create a dataframe with Pandas\ndf = pd.DataFrame(np.random.randn(10, 5), columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\n# Save dataframe as json with dapla-toolbelt\ndp.write_pandas(df = df,\n                gcs_path = f\"{bucket}/{folder}/test.json\",\n                file_format = \"json\")\n\nSom vi ser at syntaksen over så kunne vi skrevet ut til noe annet enn json ved å endre verdien i argumentet file_format.\n\n\nPakken fellesr kan også brukes til å skrive andre type filer, for eksempel csv, til bøtter. Dette gjøres med funksjonen write_SSB og spesifisere ønsket filtype i filnavn.\nFørst kaller vi biblioteket og lage noe test data ved:\n\n\nnotebook\n\nlibrary(fellesr)\n\n# Set stien til hvor data skal lagres\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"felles/veiledning/r/eksempler/purchases\"\n\n# Lage en eksempel dataframe\npurchases = data.frame(A = runif(10), B= runif(10), C=runif(10))\n\n# Skrive til csv\nwrite_SSB(purchases, file.path(bucket, folder, \"purchases.csv\")\n\n\n\n\n\n\n\nDet er ikke anbefalt å bruke xlsx-formatet, men her ser du hvordan det kan skrives ut. Mer kommer.\n\nPython \n\n\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import AuthClient\n\n# Henter token for å kunne lese fra Dapla\ntoken = AuthClient.fetch_google_credentials()\n\n# Filsti\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\ndf.to_excel(f\"{bucket}/{folder}/test.xlsx\",\n           storage_options={\"token\": token})\n\n\n\nKommer snart\n\n\n\n\n\n\n\nUnder finner du eksempler på hvordan du kan lese inn data til en Jupyter Notebooks på Dapla.\n\n\n\nPython \n\n\n\n\nnotebook\n\nimport dapla as dp\n\n# Set path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Read path into pandas dataframe \ndp.read_pandas(gcs_path= f\"{bucket}/{folder}/data.parquet\",\n               file_format = \"parquet\",\n               columns = None,)\n\nSom vi så med write_pandas så er file_format default satt til parquet, og default for columns = None, så vi kunne også ha skrevet det slik:\ndp.read_pandas(gcs_path= f\"{bucket}/{folder}/data.parquet\")\ncolumns-argumentet er en liste med kolonnenavn som vi ønsker å lese inn. Hvis vi ikke spesifiserer noen kolonner så vil alle kolonnene leses inn.\n\n\nPakken fellesr kan brukes til å lese inn data. Funksjonen read_SSB() kan lese inn filer i flere format inkluderende parquet.\nHer er et eksempel av å lese inn parquet fil “1987”.\n\n\nnotebook\n\nlibrary(fellesr)\n\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"R_smoke_test\"\n\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.parquet\"))\n\nVi kan også filtrere hvilke variabel vi ønsker å lese inn ved å spesifisere parameter col_select. For eksempel:\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.parquet\"),\n                    col_select = c(\"Year\", \"Month\"))\nInnlesning av parquet som er kartdata finner du her: Lese kartdata\n\n\n\n\n\n\nKommer mer snart. Python-koden under bygger på eksempelet over.\n\nPython \n\n\n\n\nnotebook\n\nimport dapla as dp\n\n# Path to write to\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Read in json-file from dapla-storage\ndf = dp.read_pandas(gcs_path = f\"{bucket}/{folder}/test3.json\",\n               file_format = \"json\")\n\n\n\nFunksjonen read_SSB() kan lese inn flere type av fil-format, slik som csv og json. Du trenger ikke å endre koden, kun spesifisere hele filnavn.\nFørst kaller vi inn biblioteket fellesr og spesifisere bøtte/mappen:\n\n\nnotebook\n\nlibrary(fellesr)\n\n# Filsti\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"R_smoke_test\"\n\n# Lese inn CSV-fil\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.csv\"))\n\nFor å lese inn en json-fil kan skrive følgende:\n\n\nnotebook\n\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.json\"))\n\n\n\n\n\n\n\n\nPython \n\n\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import AuthClient\n\n# Hent token\ntoken = AuthClient.fetch_google_credentials()\n\n# Les inn fil\ndf = pd.read_excel(\"gs://ssb-prod-arbmark-skjema-data-produkt/test_gcp.xlsx\",\n    storage_options={\"token\": token})\n\n\n\nKommer snart\n\n\n\n\n\n\nHer er et eksempel på hvordan man leser inn en sas7bdat-fil på Dapla som har blitt generert i prodsonen.\n\nPython \n\n\n\n\nnotebook\n\nimport dapla as dp\n\nsti = \"gs://ssb-dapla-felles-data-delt-prod/felles/veiledning/sas/statbank_ledstill.sas7bdat\"\n\ndp.read_pandas(sti, file_format=\"sas7bdat\", encoding=\"latin1\")\n\n\n\nSiden innlesing av sas7bdat-filer ikke er støttet i fellesr, så kan vi bruke R-pakken reticulate for å benytte oss av funksjonaliteten i Python-pakken dapla-toolbelt.\n\n\nnotebook\n\nlibrary(reticulate)\ndp &lt;- import(\"dapla\")\nsti  &lt;- gs://ssb-dapla-felles-data-delt-prod/felles/veiledning/sas/statbank_ledstill.sas7bdat\"\ndp$read_pandas(sti, file_format=\"sas7bdat\", encoding=\"latin1\")\n\n\n\n\n\n\n\n\nÅ slette filer fra lagringsområdet kan gjøres på flere måter. I kapitlet om sletting av data viste vi hvordan man gjør det med pek-og-klikk i Google Cloud Console. Under ser du hvordan du kan slette filer med dapla-toolbelt og gcsfs eller fellesr.\n\nPython \n\n\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\nfs.rm(f\"{bucket}/{from_folder}/df.json\")\n\n\n\nFunksjonen gc_delete_object kan brukes til å slette data på lagringsområdet.\n\n\nnotebook\n\nlibrary(fellesr)\n\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"felles/veiledning/r/eksempler/purchases\"\n\ngcs_delete_object(file.path(bucket, folder, \"purchases.parquet\"))\n\n\n\n\n\n\n\nÅ kopiere filer mellom mapper på et Linux-filsystem innebærer som regel bruke cp-kommandoen. På Dapla er det ikke så mye forskjell. Vi bruker en ligende tilnærming nå vi skal kopiere mellom bøtter eller mapper på lagringsområdet til SSB. Under ser du hvordan du kan kopiere en fil fra en mappe til en annen.\n\nPython \n\n\nLa oss begynne med et eksempel der vi kopierer en fil fra en mappe til en annen i samme bøtte.\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Path to folders\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfrom_folder = \"felles/veiledning/python/eksempler/purchases\"\nto_folder = \"felles/veiledning/python/eksempler\"\n\n# Copy file\nfs.cp(f\"{bucket}/{from_folder}/data.parquet\",\n      f\"{bucket}/{to_folder}/data_copy.parquet\")\n\nDet også fungere for å kopiere filer mellom bøtter.\nEt annet scenario vi ofte vil støte på er at vi ønsker å kopiere en fil fra vårt Jupyter-filsystem til en mappe på lagringsområdet. Her kan vi bruke fs.put-metoden.\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Create a new file in your home directory called test.txt\nwith open('/home/jovyan/test.txt', 'w') as f:\n    f.write('Create a new text file!')\n\n#Path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler\"\n\n# Copy file from local to remote file system\nfs.put(lpath=f\"/home/jovyan/test.txt\", rpath=f\"{bucket}/{folder}/test.txt\")\n\nØnsker vi å kopiere en hel mappe fra lagringsområdet til Jupyter-filsystemet, kan vi bruke fs.get-metoden, med opsjonen recursive=True.\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Copy file\nfs.get(&lt;from_bucket&gt;,\n      \"/home/jovyan/sesongjustering/\",\n      recursive=True)\n\n\n\nKommer snart\n\n\n\n\n\n\n\nPython \n\n\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfrom_folder = \"felles/veiledning/python/eksempler/purchases\"\nto_folder = \"felles/veiledning/python/eksempler\"\n\nfs.mv(f\"{bucket}/{from_folder}/data.parquet\", f\"{bucket}/{to_folder}/data.parquet\")\n\n\n\nKommer snart\n\n\n\n\n\n\nSelv om bøtter ikke har mapper med en hierarkisk struktur slik man er kjent med fra klassike filsystemer, så kan man opprette det som ser ut som mapper i objektnavnet. I realiteten blir bare / oppfattet som en del av navnet på objektet. Skulle du likevel ønske å opprette dette så kan du gjøre det følgende måte:\n\nPython \n\n\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n#Path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler\"\n\n# Create folder\nfs.touch(f\"{bucket}/{folder}/testmappe/\")\n\n\n\nKommer snart",
    "crumbs": [
      "Manual",
      "Data"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-data.html#footnotes",
    "href": "statistikkere/jobbe-med-data.html#footnotes",
    "title": "Jobbe med data",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI de tidligere systemene på bakken så var det ikke nødvendig med autentisering mellom kodemiljø og datalagringen↩︎",
    "crumbs": [
      "Manual",
      "Data"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html",
    "href": "statistikkere/jupyter-playground.html",
    "title": "Jupyter-playground",
    "section": "",
    "text": "Jupyter-playground er en tjeneste på Dapla Lab som er ment for nybegynnere og andre som vil komme raskt i gang med koding i Jupyterlab. Den har mange likheter med Jupyter-tjenesten på Dapla Lab med den forskjellen at mange flere pakker og extensions er ferdig installert i Jupyter-playground. Tjenesten har både R og Python installert.\nSiden tjenesten er ment for opplæring og utforskning så er det ikke anbefalt å bygge produksjonskode fra denne tjenesten. Grunnen til det er at det er flere avhengigheter mellom programvare enn nødvendig, noe som skaper mer komplisert kode enn nødvendig. Derimot er det et ideelt sted for å lære seg R eller Python siden man slipper kompleksiteten med å installere sine egne pakker og forholde seg til ssb-project. For de som skal utvikle produksjonskode anbefales det at koden heller utvikles fra Jupyter-tjenesten.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#forberedelser",
    "href": "statistikkere/jupyter-playground.html#forberedelser",
    "title": "Jupyter-playground",
    "section": "Forberedelser",
    "text": "Forberedelser\nFør man starter Jupyter-playground-tjenesten bør man ha lest kapitlet om Dapla Lab og satt opp Git- og GitHub-konfigurasjonen under Min konto. Deretter gjør du følgende:\n\nLogg deg inn på Dapla Lab\nUnder Tjenestekatalog trykker du på Start-knappen for Jupyter\nGi tjenesten et navn\nÅpne Jupyter konfigurasjoner",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#konfigurasjon",
    "href": "statistikkere/jupyter-playground.html#konfigurasjon",
    "title": "Jupyter-playground",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nKonfigurasjonen av Jupyter-playground er identisk som for Jupyter-tjenesten. Se dokumentasjonen for konfigurasjon av Jupyter.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#datatilgang",
    "href": "statistikkere/jupyter-playground.html#datatilgang",
    "title": "Jupyter-playground",
    "section": "Datatilgang",
    "text": "Datatilgang\nHvis man har valgt å tilgjengeliggjøre data fra et team sitt bøtter i tjenesten, så kan man inspisere dataene fra en terminal inne i tjenesten:\n\nÅpne en instans av Jupyter med data fra bøtter\nÅpne en terminal inne i Jupyter\nGå til mappen med bøttene ved å kjøre dette fra terminalen cd /buckets\nKjør ls -ahl i teminalen for å se på hvilke bøtter som er montert.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#installere-pakker",
    "href": "statistikkere/jupyter-playground.html#installere-pakker",
    "title": "Jupyter-playground",
    "section": "Installere pakker",
    "text": "Installere pakker\nSiden det nesten ikke er installert noen pakker i tjenesten så kan brukeren opprette et ssb-project og installere pakker som vanlig.\nFor å bygge et eksisterende ssb-project så kan brukeren også bruke ssb-project.\nFor å installere R-pakker følger man beskrivelsen for renv.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#slette-tjenesten",
    "href": "statistikkere/jupyter-playground.html#slette-tjenesten",
    "title": "Jupyter-playground",
    "section": "Slette tjenesten",
    "text": "Slette tjenesten\nFor å slette tjenesten kan man trykke på Slette-knappen i Dapla Lab under Mine tjenester. Når man sletter en tjeneste så sletter man hele disken inne i tjenesten og frigjør alle ressurser som er reservert. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#pause-tjenesten",
    "href": "statistikkere/jupyter-playground.html#pause-tjenesten",
    "title": "Jupyter-playground",
    "section": "Pause tjenesten",
    "text": "Pause tjenesten\nMan kan pause tjenesten ved å trykke på Pause-knappen i Dapla Lab under Mine tjenester. Når man pauser så slettes alt påden lokale disken som ikke er lagret under $HOME/work. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#monitorering",
    "href": "statistikkere/jupyter-playground.html#monitorering",
    "title": "Jupyter-playground",
    "section": "Monitorering",
    "text": "Monitorering\n\n\nMan kan moniterere en instans av Jupyter ved å trykke på Jupyter-teksten under Mine tjenester i Dapla Lab, slik som vist i Figur 1.\nDenne funksjonaliteten er under arbeid og mer informasjon kommer snart.\n\n\n\n\n\n\n\n\n\nFigur 1: Monitorering av Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/test.html",
    "href": "statistikkere/test.html",
    "title": "Notebooks",
    "section": "",
    "text": "Notebooks\nsdløkfjlksdjfkl"
  },
  {
    "objectID": "statistikkere/opprette-dapla-team.html",
    "href": "statistikkere/opprette-dapla-team.html",
    "title": "Opprette Dapla-team",
    "section": "",
    "text": "Opprette Dapla-team\nFor å komme i gang med å opprette et Dapla-team trengs det en oversikt over teamets medlemmer og hvilke tilgangsgrupper medlemmene skal være med i. Det trengs også informasjon om hvilke Dapla-tjenester som er aktuelle for teamet å ta i bruk. Derfor har det blitt opprettet en egen veileder for dette kalt Dapla Start.\n\n\n\n\n\n\nGå til Dapla Start for starte bestilling av et nytt Dapla-team.\n\n\n\nNår teamet er opprettet får alle medlemmene tilgang til sitt eget prosjekt i Google Cloud Platform (GCP), som er SSBs leverandør av skytjenester. Videre får hvert prosjekt et sett med tjenester og tilganger som knyttes til teamet. Det opprettes også datalagringsområder (kalt bøtter) som bare kan aksesseres av brukere som er med i teamets tilgangsgrupper.\nDapla-teamet vil også få sin egen gruppe i SSBs Active Directory slik at medlemskapet i gruppen kan administreres av Kundeservice."
  },
  {
    "objectID": "statistikkere/deling-av-data.html",
    "href": "statistikkere/deling-av-data.html",
    "title": "Deling av data",
    "section": "",
    "text": "Dapla-team kan dele data mellom team via såkalte delt-bøtter. Hvert team kan opprette de delt-bøttene de har behov for, og deretter gi tilgang til grupper i andre team. Opprettelse av bøtter skal følge retningslinjene som er definert her.\n\n\nFor å opprette delt-bøtter må man først skru på featuren shared-buckets for teamet. Det gjøres ved at en på teamet gjør følgende:\n\n\n\nGå til IaC-repoet til teamet på Github.\nÅpne filen ./infra/projects.yaml.\nLegg til en linje med shared-buckets under features i miljøet du ønsker, slik som vist til høyre.\nOpprette en PR med endringen.\nBe en i gruppa data-admins se over endringen og godkjenne.\nKjør atlantis plan og atlantis apply slik som beskrevet her.\n\n\n\n\n\n\n\nprojects.yaml\n\nteam_uniform_name: dapla-example\n\nprojects:\n  - project_name: dapla-example\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service\n      - shared-buckets\n\n\n\n\n\n\nOpprettelse av delt-bøtter gjøres i teamets IaC-repo. For å opprette en delt-bøtte må man legge til en linje i fila ./infra/projects/&lt;teamnavn&gt;-prod/buckets-shared.yaml. Du legger bare til kortnavnet for bøtta, altså den delen av bøtte-navnet som teamet kan styre selv. For å opprette en bøtte med kortnavn freg, så fyller man bare inn følgende:\n\n\nbuckets-shared.yaml\n\n# Use this file to create new buckets for sharing data easily.\n# Simply add a line with a dash and your new shared bucket's name:\n# - my-shared-data\n# This will create a bucket with the name ssb-arbmark-register-data-delt-my-shared-data-prod\n# The following line creates the standard shared bucket:\n- freg\n\nDe 5 første linjene er bare kommentarer som forteller hvordan man oppretter bøttene. På linje 6 ser vi hva som måtte fylles ut for å opprette en bøtte med kortnavn freg, som igjen får bøttenavnet:\nssb-&lt;teamnavn&gt;-data-delt-freg-prod\nEtter at endringen er gjort i buckets-shared.yaml, så gjør du følgende:\n\nOpprette en PR på repoet med endringen.\nFå en i gruppen data-admins til å gå gjennom og godkjenne.\nKjør atlantis plan og atlantis apply som beskrevet her.\n\n\n\n\nTilgangsstyringen til delt-bøtta kan gjøres av teamet som eier bøtta. Hvem som har tilgang blir definert i fila ./infra/projects/&lt;teamnavn&gt;-prod/iam.yaml i IaC-repoet til teamet. Under ser man strukturen på iam.yaml for å gi tilgang til delt-bøtter:\n\n\niam.yaml\n\nbuckets:\n  ssb-dapla-example-data-delt-freg-prod:\n    team-alpha-data-admins:\n    - ssb.bucket.read\n    team-alpha-developers:\n    - ssb.bucket.read\n\n  ssb-dapla-example-data-delt-ameld-prod:\n    team-beta-developers:\n    - ssb.bucket.read\n\nOver ser vi at teamet dapla-example har to delt-bøtter:\n\nssb-dapla-example-data-delt-freg-prod\nssb-dapla-example-data-delt-ameld-prod\n\nDen første bøtta har de gitt lesetilgang til data-admins og developers i team-alpha. Fra linje 8 og nedover ser vi at de har gitt developers i team-beta lesetilgang til ssb-dapla-example-data-delt-ameld-prod.\nNår man har gjort endringer i iam.yaml så gjør man følgende:\n\nOpprette en PR med endringen.\nFå en i gruppen data-admins til å gå gjennom endringen og godkjenne.\nKjør atlantis plan og atlantis apply som beskrevet her.",
    "crumbs": [
      "Manual",
      "Data",
      "Deling av data"
    ]
  },
  {
    "objectID": "statistikkere/deling-av-data.html#forberedelser",
    "href": "statistikkere/deling-av-data.html#forberedelser",
    "title": "Deling av data",
    "section": "",
    "text": "For å opprette delt-bøtter må man først skru på featuren shared-buckets for teamet. Det gjøres ved at en på teamet gjør følgende:\n\n\n\nGå til IaC-repoet til teamet på Github.\nÅpne filen ./infra/projects.yaml.\nLegg til en linje med shared-buckets under features i miljøet du ønsker, slik som vist til høyre.\nOpprette en PR med endringen.\nBe en i gruppa data-admins se over endringen og godkjenne.\nKjør atlantis plan og atlantis apply slik som beskrevet her.\n\n\n\n\n\n\n\nprojects.yaml\n\nteam_uniform_name: dapla-example\n\nprojects:\n  - project_name: dapla-example\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service\n      - shared-buckets",
    "crumbs": [
      "Manual",
      "Data",
      "Deling av data"
    ]
  },
  {
    "objectID": "statistikkere/deling-av-data.html#opprettelse-av-bøtte",
    "href": "statistikkere/deling-av-data.html#opprettelse-av-bøtte",
    "title": "Deling av data",
    "section": "",
    "text": "Opprettelse av delt-bøtter gjøres i teamets IaC-repo. For å opprette en delt-bøtte må man legge til en linje i fila ./infra/projects/&lt;teamnavn&gt;-prod/buckets-shared.yaml. Du legger bare til kortnavnet for bøtta, altså den delen av bøtte-navnet som teamet kan styre selv. For å opprette en bøtte med kortnavn freg, så fyller man bare inn følgende:\n\n\nbuckets-shared.yaml\n\n# Use this file to create new buckets for sharing data easily.\n# Simply add a line with a dash and your new shared bucket's name:\n# - my-shared-data\n# This will create a bucket with the name ssb-arbmark-register-data-delt-my-shared-data-prod\n# The following line creates the standard shared bucket:\n- freg\n\nDe 5 første linjene er bare kommentarer som forteller hvordan man oppretter bøttene. På linje 6 ser vi hva som måtte fylles ut for å opprette en bøtte med kortnavn freg, som igjen får bøttenavnet:\nssb-&lt;teamnavn&gt;-data-delt-freg-prod\nEtter at endringen er gjort i buckets-shared.yaml, så gjør du følgende:\n\nOpprette en PR på repoet med endringen.\nFå en i gruppen data-admins til å gå gjennom og godkjenne.\nKjør atlantis plan og atlantis apply som beskrevet her.",
    "crumbs": [
      "Manual",
      "Data",
      "Deling av data"
    ]
  },
  {
    "objectID": "statistikkere/deling-av-data.html#tilgangsstyring",
    "href": "statistikkere/deling-av-data.html#tilgangsstyring",
    "title": "Deling av data",
    "section": "",
    "text": "Tilgangsstyringen til delt-bøtta kan gjøres av teamet som eier bøtta. Hvem som har tilgang blir definert i fila ./infra/projects/&lt;teamnavn&gt;-prod/iam.yaml i IaC-repoet til teamet. Under ser man strukturen på iam.yaml for å gi tilgang til delt-bøtter:\n\n\niam.yaml\n\nbuckets:\n  ssb-dapla-example-data-delt-freg-prod:\n    team-alpha-data-admins:\n    - ssb.bucket.read\n    team-alpha-developers:\n    - ssb.bucket.read\n\n  ssb-dapla-example-data-delt-ameld-prod:\n    team-beta-developers:\n    - ssb.bucket.read\n\nOver ser vi at teamet dapla-example har to delt-bøtter:\n\nssb-dapla-example-data-delt-freg-prod\nssb-dapla-example-data-delt-ameld-prod\n\nDen første bøtta har de gitt lesetilgang til data-admins og developers i team-alpha. Fra linje 8 og nedover ser vi at de har gitt developers i team-beta lesetilgang til ssb-dapla-example-data-delt-ameld-prod.\nNår man har gjort endringer i iam.yaml så gjør man følgende:\n\nOpprette en PR med endringen.\nFå en i gruppen data-admins til å gå gjennom endringen og godkjenne.\nKjør atlantis plan og atlantis apply som beskrevet her.",
    "crumbs": [
      "Manual",
      "Data",
      "Deling av data"
    ]
  },
  {
    "objectID": "statistikkere/tilgangsstyring.html",
    "href": "statistikkere/tilgangsstyring.html",
    "title": "Tilgangsstyring",
    "section": "",
    "text": "Hvert Dapla-team har sine egne lagringsområder for data som ingen andre har tilgang til, med mindre teamet eksplisitt velger å dele data med andre team. I tillegg har teamet tilgang til egne ressurser for å behandle dataene.\nDet er tilgangsgruppen managers som bestemmer hvilke personer som skal ha hvilke roller i et team, og dermed hvilke data de ulike team-medlemmene får tilgang til. Den som jobber med data kan bli plassert i tilgangsgruppene data-admins eller developers. Sistnevnte får tilgang til alle datatilstander utenom kildedata, mens data-admins er forhåndsgodkjent til å også å aksessere kildedata ved behov. Dermed er data-admins en priveligert rolle på teamet som er forbeholdt noen få personer.\n\n\n\n\n\n\nFigur 1: Datatilstander som et team sitt medlemmer har ilgang til.\n\n\n\nFigur 1 viser hvem som har tilgang til hvilke datatilstander. Som nevnt er data-admins ansett som forhåndsgodkjent til å aksessere kildedata ved behov. Måten dette er implementert på er at data-admins må aktivere denne tilgangen selv, ved å bruke et JIT-grensesnitt (Just-In-Time Access). Tilgangen krever en begrunnelse og bruken kan løpende monitoreres av managers for teamet.",
    "crumbs": [
      "Manual",
      "Dapla-team",
      "Tilgangsstyring"
    ]
  },
  {
    "objectID": "statistikkere/metadata.html",
    "href": "statistikkere/metadata.html",
    "title": "Metadata",
    "section": "",
    "text": "Metadata\nDet jobbes med å bygge et helt nytt metadata system på Dapla.\nSidene herunder dokumenterer oversikt, arkitektur og hvordan man bruker de ulike produkter som utgjør systemet.",
    "crumbs": [
      "Manual",
      "Metadata"
    ]
  },
  {
    "objectID": "statistikkere/features.html",
    "href": "statistikkere/features.html",
    "title": "Features",
    "section": "",
    "text": "Under arbeid\n\n\n\nFeatures må foreløpig skrus på av plattformteamene. Ta kontakt med Kundeservice hvis du ønsker å få en feature skrudd på.\n\n\nEn feature er en GCP-tjeneste som som er satt opp og konfigurert slik at Dapla-team kan ta det i bruk på en enkel og selvbetjent måte. Når man tar i bruk en feature kan man være sikker på at sikkerhet og beste-praksis i SSB er ivaretatt. Et viktig poeng med features er at teamene selv skal kunne skru av og på features etter behov.\nForeløpig er det tilgjengeliggjort følgende features på Dapla:\n\ndapla-buckets\ndapla-buckets er en feature som gir deg Google Cloud Storage bøttene som statistikkteam skal bruke for å lagre data i Dapla. Dvs. en bøtte for kildedata, en bøtte for produkt-data, og en bøtte for delt data.\nkildomaten kildomaten er en feature som gir deg tilgang til Kildomaten. Den lar deg automatisere prosessering av data fra kildedata til inndata ved hjelp av Cloud Run.\ntransfer-service\ntransfer-service er en feature som gir deg tilgang til å overføre data mellom lagringstjenester i Dapla. Den lar deg overføre data mellom bøtter, og mellom bakke- og skyplattformen i SSB. Den er bygget på GCP-tjenesten Google Transfer Service.\nshared-buckets\nshared-buckets er en feature som lar teamet selv opprette delt-bøtter og styre tilganger til disse.\n\n\n\n\n\n\n\n\n\nSkru på en feature om gangen.\n\n\n\nHvis du ønsker å skru på flere features samtidig, så må du gjøre det i flere PR-er. Atlantis vil ikke klare å håndtere flere features i samme PR. Følg oppskriften under for hver feature du ønsker å skru på.\n\n\nAktivering av en feature for Dapla-teamet ditt er veldig enkelt. Endringen er så liten at vi anbefaler å gjøre endringen direkte i GitHubs grensesnitt, uten å klone repoet først. Slik går du frem:\n\nSøk opp ditt teams sitt IaC-repoet under https://github.com/statisticsnorway/.\n\n\n\n\nI repoet åpner du fila ./infra/projects.yaml. Se eksempel for teamet play-obr til høyre.\nFinn ut om du ønsker å skru på en feature i test eller prod. Hvis du ønsker å gjøre det i prod, så skal du legge til en linje under features der env: prod. Hvis du ønsker å gjøre det i test, så skal du legge til en linje under features der env: test.\nDu kan legge til linjen ved å trykke på -ikonet øverst til høyre i fila, endre teksten, og trykke på Commit changes. Velg deretter hvilket navn du ønsker på branchen, og trykk Create pull request.\nI PR-en venter du og ser at atlantis/plan1 har blitt kjørt og får en  til venstre for hver kjøring, slik som vist i Figur 1.\n\n\n\n\n./infra/projects.yaml\n\nteam_uniform_name: play-obr\n\nprojects:\n  - project_name: play-obr\n    env: test\n    features:\n      - dapla-buckets\n      - kildomaten\n\n  - project_name: play-obr\n    env: prod\n    features:\n      - dapla-buckets\n\n\n\n\n\n\n\n\n\nFigur 1: Atlantis plan planlegger endringen og sjekker om den er klar til gjennomføring.\n\n\n\n\nHvis alt er i orden så ber du en kollega om å se over endringen og godkjenne hvis alt ser riktig ut. Når den er godkjent vil du se et bilde som ligner det du ser i Figur 2.\n\n\n\n\n\n\n\nFigur 2: Sjekker av PR-er i teamets IaC-repo.\n\n\n\n\nNår PR er godkjent av en kollega, og atlantis plan ikke viser noen feilmeldinger, så kan du effektuere endringene ved å atlantis apply i kommentarfeltet til PR-en. Kommentareren vil trigge en kjøring som effektuerer alle endringer på plattformen.\nEtter at atlantis apply er kjørt, så kan du merge inn branchen til main.\nTil slutt kan du slette branchen.\n\nNår dette er gjort så endringen effektuert på Dapla og klar til bruk. For eksempel, hvis vi hadde aktivert transfer-service i prod for projects.yaml over, så ville den etter endringen sett slik ut:\n\n\n./infra/projects.yaml\n\nteam_uniform_name: play-obr\n\nprojects:\n  - project_name: play-obr\n    env: test\n    features:\n      - dapla-buckets\n      - kildomaten\n\n  - project_name: play-obr\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service\n\n\n\n\nFor å deaktivere en feature som ikke lenger i bruk, så følger du bare beskrivelsen for Aktivere tjeneste over, med det unntak at du nå fjerner en linje istedenfor å legge til.",
    "crumbs": [
      "Manual",
      "Dapla-team",
      "Features"
    ]
  },
  {
    "objectID": "statistikkere/features.html#aktivere-feature",
    "href": "statistikkere/features.html#aktivere-feature",
    "title": "Features",
    "section": "",
    "text": "Skru på en feature om gangen.\n\n\n\nHvis du ønsker å skru på flere features samtidig, så må du gjøre det i flere PR-er. Atlantis vil ikke klare å håndtere flere features i samme PR. Følg oppskriften under for hver feature du ønsker å skru på.\n\n\nAktivering av en feature for Dapla-teamet ditt er veldig enkelt. Endringen er så liten at vi anbefaler å gjøre endringen direkte i GitHubs grensesnitt, uten å klone repoet først. Slik går du frem:\n\nSøk opp ditt teams sitt IaC-repoet under https://github.com/statisticsnorway/.\n\n\n\n\nI repoet åpner du fila ./infra/projects.yaml. Se eksempel for teamet play-obr til høyre.\nFinn ut om du ønsker å skru på en feature i test eller prod. Hvis du ønsker å gjøre det i prod, så skal du legge til en linje under features der env: prod. Hvis du ønsker å gjøre det i test, så skal du legge til en linje under features der env: test.\nDu kan legge til linjen ved å trykke på -ikonet øverst til høyre i fila, endre teksten, og trykke på Commit changes. Velg deretter hvilket navn du ønsker på branchen, og trykk Create pull request.\nI PR-en venter du og ser at atlantis/plan1 har blitt kjørt og får en  til venstre for hver kjøring, slik som vist i Figur 1.\n\n\n\n\n./infra/projects.yaml\n\nteam_uniform_name: play-obr\n\nprojects:\n  - project_name: play-obr\n    env: test\n    features:\n      - dapla-buckets\n      - kildomaten\n\n  - project_name: play-obr\n    env: prod\n    features:\n      - dapla-buckets\n\n\n\n\n\n\n\n\n\nFigur 1: Atlantis plan planlegger endringen og sjekker om den er klar til gjennomføring.\n\n\n\n\nHvis alt er i orden så ber du en kollega om å se over endringen og godkjenne hvis alt ser riktig ut. Når den er godkjent vil du se et bilde som ligner det du ser i Figur 2.\n\n\n\n\n\n\n\nFigur 2: Sjekker av PR-er i teamets IaC-repo.\n\n\n\n\nNår PR er godkjent av en kollega, og atlantis plan ikke viser noen feilmeldinger, så kan du effektuere endringene ved å atlantis apply i kommentarfeltet til PR-en. Kommentareren vil trigge en kjøring som effektuerer alle endringer på plattformen.\nEtter at atlantis apply er kjørt, så kan du merge inn branchen til main.\nTil slutt kan du slette branchen.\n\nNår dette er gjort så endringen effektuert på Dapla og klar til bruk. For eksempel, hvis vi hadde aktivert transfer-service i prod for projects.yaml over, så ville den etter endringen sett slik ut:\n\n\n./infra/projects.yaml\n\nteam_uniform_name: play-obr\n\nprojects:\n  - project_name: play-obr\n    env: test\n    features:\n      - dapla-buckets\n      - kildomaten\n\n  - project_name: play-obr\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service",
    "crumbs": [
      "Manual",
      "Dapla-team",
      "Features"
    ]
  },
  {
    "objectID": "statistikkere/features.html#deaktivere-en-feature",
    "href": "statistikkere/features.html#deaktivere-en-feature",
    "title": "Features",
    "section": "",
    "text": "For å deaktivere en feature som ikke lenger i bruk, så følger du bare beskrivelsen for Aktivere tjeneste over, med det unntak at du nå fjerner en linje istedenfor å legge til.",
    "crumbs": [
      "Manual",
      "Dapla-team",
      "Features"
    ]
  },
  {
    "objectID": "statistikkere/features.html#footnotes",
    "href": "statistikkere/features.html#footnotes",
    "title": "Features",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nDet som skjer når atlantis plan kjøres er at det genereres en detaljert beskrivelse av hvilke endringer som må skje på plattformen for at teamets feature skal aktiveres. Derfor må eventuelle feilmeldinger fra atlantis plan fikses før man faktiske kan effektuere endringene med atlantis apply. ↩︎",
    "crumbs": [
      "Manual",
      "Dapla-team",
      "Features"
    ]
  },
  {
    "objectID": "statistikkere/dapla-ctrl.html",
    "href": "statistikkere/dapla-ctrl.html",
    "title": "Dapla Ctrl",
    "section": "",
    "text": "Dapla Ctrl1 er tjeneste for tilgangsstyring på Dapla. Formålet med appen er at det skal være lett å få oversikt og administrere tilganger knyttet til Dapla-team.\n\n\nAlle som jobber i SSB kan logge seg inn på https://ctrl.dapla.ssb.no/ for å bruke tjenesten.\n\n\n\nAlle SSB-ansatte som logger seg inn i Dapla Ctrl får tilgang til å se informasjon om Dapla-team og tilganger. I tillegg kan de som er i tilgangsgruppen managers legge til, fjerne og endre medlemmer i teamet de har denne rollen. Seksjonsledere har tilgang til å opprette nye team.\n\n\n\n\n\n\n\n\n\nFigur 1: Bilde av forsiden med oversikt over Mine team i Dapla Ctrl.\n\n\n\nFigur 1 viser landingssiden/forsiden som først møter den som logger seg inn i Dapla Ctrl. den. Her får den som er innlogget oversikt over hvilke Dapla-team man er medlem av, og følgende informasjon om teamene:\n\nTeknisk teamnavn\nTeamets eierseksjon\nAntall teammedlemmer\nManagers for teamet\n\nMan kan også bytte fane fra Mine team til Alle team for å se samme informasjon om alle team som finnes på Dapla.\n\n\n\n\n\n\n\n\n\n\nFigur 2: Bilde av oversikten over et enkelt-team i Dapla Ctrl.\n\n\n\nFra Teamoversikten kan man trykke seg inn på et spesifikt team og få en oversikt slik som vist i Figur 2.\nPå toppen av siden får man se følgende informasjon:\n\nteamets visningsnavn\nteamets tekniske kortnavn\neierseksjonens seksjonsleder\neierseksjonens seksjonsnavn\nautonomitetsnivået til teamet\n\nVidere ser vi at det er en fane for Teammedlemmer og en for Delte Data. Under fanen for Teammedlemmer ser man følgende informasjon om alle medlemmene av teamet:\n\nnavn på medlem\nhvilken seksjons de jobber på\nhvilken tilgangsgruppe de tilhører på teamet\ne-postadresse med brukerens kortnavn\n\nUnder fanen Delte data får man en oversikt over hvilke bøtter teamet har opprettet for å dele data med andre team.\n\n\n\n\n\n\nFigur 3: Bilde av oversikten over hvilke bøtter teamet har opprettet for å dele data med andre team.\n\n\n\nFigur 3 viser hvilken informasjon man får over teamets delte data. Følgende informasjon vises:\n\nkortnavnet på bøttene\ntekniske navnet til bøttene\nhvor mange team som har tilgang\nhvor mange personer som har tilgang2\n\n\n\n\n\n\n\n\n\n\n\nFigur 4: Bilde av oversikten over hvilke personer som har tilgang til en delt-bøtte.\n\n\n\nFra Teamvisningen kan man velge fanen Delte data og trykke seg på en av teamets delte-bøtter. Figur 4 viser informasjon man får se i denne visningen. På toppen av siden får man se kortnavnet til bøtta, det tekniske navnet på bøtta, hvilket team som eier bøtta og hvilken eierseksjon teamet har. I tabellen som vises kan man undersøke hvilke personer som har tilgang til bøtta og få følgende informasjon om de:\n\nnavn\nhvilken seksjon de jobber på\nhvilket team-medelemskap de har tilgang i kraft av\nhvilken tilgangsgruppe de er i på teamet de har tilgang i kraft av\n\nFra tabellen kan man velge å se nærmere på personen som har tilgang, f.eks. se hvilke andre tilganger denne personen har, eller man kan se nærmere på teamet som personen har tilgang i kraft av. Ved å undersøke teamet nærmere kommer man inn på Teamvisningen som er beskrevet over, mens visning av Teammedlemmer forklares under.\n\n\n\n\n\n\n\n\n\n\nFigur 5: Bilde av oversikten over hvilke personer som med i dine team.\n\n\n\nFigur 5 viser oversikt over teammedlemmer. Øverst på siden kan man velge mellom en fane for Mine teammedlemmer og Alle teammedlemmer. Førstnevnte viser hvilke andre medlemmer som er i de teamene den innloggede er med i, mens sistnevnte viser alle teammedlemmer i SSB3. I tabellen under får man følgende informasjon om teammedlemmene:\n\nnavn\nhvilken seksjons de jobber på\nhvor mange team de er medlem av\nhvor mange team de har tilgangsrollen data-admins\nnavn på personens seksjonsleder\n\n\n\n\n\n\n\n\n\n\n\nFigur 6: Bilde av oversikten over hvilke team en person er medlem av.\n\n\n\nFigur 6 viser hva man ser når går inn på en enkeltperson, enten via Teamoversikten eller Teammedlemmer. Øverst på siden står navnet til personen, hvorvidt de har arbeidssted i Oslo eller Kongsvinger4, hvilken seksjon de jobber på og e-postadressen deres.\nTabellen i Figur 6 får man en oversikt over hvilke team personen er medlem av, samt følgende detaljer:\n\nteamets tekniske kortnavn\nseksjonseier av teamet\nhvilke tilgangsgrupper personen er med i\nhvem som er managers for teamet\n\nVidere kan man gå videre inn på et av teamene og se nærmere på hvem som er medlemmer og hvilke data de deler.\n\n\n\n\n\nDet er kun seksjonsledere i SSB som kan opprette et Dapla-team. Hvis en seksjonsleder logger seg inn i Dapla Ctrl så vil knappen i Figur 7 vises på Teamoversikt-siden. Eierseksjonen til et team vil bli definert av hvilken seksjonsleder som oppretter teamet.\n\n\n\n\n\n\n\n\n\nFigur 7: Bilde av knappen som vises for seksjonsledere.\n\n\n\n\n\nNår man oppretter et team må man fylle ut skjemaet i Figur 8. Under finner du en oversikt hva som er viktig å vurdere når man fyller ut de ulike feltene.\n\n\n\n\nVisningsnavn er teamets navn i et lesevennlig format. Navnet bør bestå av et hoveddomenet og et subdomenet. Det er tillatt med små/store bokstaver, mellomrom, Æ, Ø og Å.\nEksempel på et hoveddomenet i SSB er Skatt, og under det finnes det subdomener som Person og Næring. Visningsnavnet til teamene er da Skatt Person og Skatt Næring.\nI noen tilfeller gir det ikke mening med et subdomenet og da er det greit å kun ha et hoveddomenet. Et eksempel på et visningsnavn som kun har hoveddomenet er Nasjonalregnskap.\n\n\n\n\n\n\n\n\n\n\nFigur 8: Bilde av siden for opprettelse av team.\n\n\n\n\n\n\n\nVelg dette for å overstyre det automatisk genererte tekniske teamnavnet. Hvis man ikke krysser av denne boksen vil det genereres et teknisk teamnavn basert på visningsnavnet. Det kan være nyttig å velge dette hvis man har et langt visningsnavn og ønsker å forkorte det genererte tekniske teamnavnet.\n\n\n\nTeamets navn i et maskinvennlig format som bl.a. benyttes i filstier til lagringsbøtter. Det er ikke tillatt med mellomrom og norske tegn (Æ, Ø og Å). Navnet kan ikke overskride 17 tegn.\nDet tekniske teamnavnet blir automatisk generert basert på visningsnavn hvis man ikke velger Overstyr teknisk navn. Tabell 1 viser eksempler på visningsnavn der man både har overtyrt det tekniske navnet og ikke.\n\n\n\nTabell 1: Eksempler på teamnavn\n\n\n\n\n\nVisningsnavn\nTeknisk navn\nOverstyrt\n\n\n\n\nSkatt Person\nskatt-person\nnei\n\n\nSkatt Næring\nskatt-naering\nnei\n\n\nNasjonalregnskap\nnr\nja\n\n\nFinansmarkedsstatistikk\nfinmark\nja\n\n\n\n\n\n\n\n\n\nAlle team tilhører en seksjon og denne informasjonen ligger lagret i metadataene til teamet. Standard er at seksjonslederen som søker fyller ut sitt seksjonsnummer her, men det er mulig å velge andre seksjoner.\n\n\n\nNivå av frihet et team har til å definere sin egen infrastruktur. Statistikkproduserende team er vanligvis i kategorien Managed, dvs. at de kun bruker tjenester som tilbys av plattformen. IT-team vil ofte defineres som Self-Managed fordi dette gir større kontroll til teamet. Les mer her.\n\n\n\n\n\n\nHvis man som medlem av tilgangsgruppen managers logger seg inn Dapla Ctrl, vil man få tilgang til å legge til medlemmer i teamet man er i managers-gruppa for. Går man inn på Teamvisning vil man se knappen i Figur 9.\n\n\n\n\n\n\n\n\n\nFigur 9: Bilde av knappen som vises for managers.\n\n\n\n\n\nTrykker man på knappen så får man opp en side for å legge til nye medlemmer i teamet, slik som vist Figur 10. Man kan søke opp alle ansatte i SSB, og man kan velge å legge de til i en eller flere tilgangsgrupper. Når man har valgt person, og hvilke tilgangsgrupper de skal legges i, så avslutter man med å trykke på Legg til medlem for å effektuere endringen.\n\n\n\n\n\n\nFigur 10: Bilde av siden for å legge til medlemmer.\n\n\n\nDet kan ta mellom 1-2 minutter før tilgangen er aktivert og klar til bruk.\n\n\n\nHvis man som medlem av tilgangsgruppen managers logger seg inn Dapla Ctrl, vil man få tilgang til å fjerne og endre medlemmer i teamet man er i managers-gruppa for. Går man inn på Teamvisning så vil man se Endre-knapp for hver person i teamet.\n\n\n\n\n\n\nFigur 11: Bilde av Teamvisning som vises for personer i tilgangsgruppen managers.\n\n\n\n\n\nAv Figur 11 ser vi at hvert medlem i teamet har en Endre-knapp. Trykker man på den så får man opp bilde som vises i Figur 12.\nØnsker man å fjerne et medlem fra teamet, så kan man bare trykke på Fjern fra teamet. Da vil man bli spurt om å bekrefte at personen skal gjernes, og velger man ok så effektureres endringen ila et par minutter.\nØnsker man endre hvilken tilgangsgruppe en person er med i, så gjør man det ved å enten fjerne eller legge til tilganger som listet under dropdown-menyen for Tilgangsgruppe(r). For eksempel hvis en person ligger som både data-admins og developers, slik som eksempelet i Figur 12, så trykker man bare på X-ikonet for den tilgangen, og til slutt effektuerer man endringen ved å velge Oppdater tilgang.\n\n\n\n\n\n\n\n\n\nFigur 12: Bilde av siden for å endre eller fjerne medlemmer.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Dapla Ctrl"
    ]
  },
  {
    "objectID": "statistikkere/dapla-ctrl.html#innlogging",
    "href": "statistikkere/dapla-ctrl.html#innlogging",
    "title": "Dapla Ctrl",
    "section": "",
    "text": "Alle som jobber i SSB kan logge seg inn på https://ctrl.dapla.ssb.no/ for å bruke tjenesten.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Dapla Ctrl"
    ]
  },
  {
    "objectID": "statistikkere/dapla-ctrl.html#funksjonalitet",
    "href": "statistikkere/dapla-ctrl.html#funksjonalitet",
    "title": "Dapla Ctrl",
    "section": "",
    "text": "Alle SSB-ansatte som logger seg inn i Dapla Ctrl får tilgang til å se informasjon om Dapla-team og tilganger. I tillegg kan de som er i tilgangsgruppen managers legge til, fjerne og endre medlemmer i teamet de har denne rollen. Seksjonsledere har tilgang til å opprette nye team.\n\n\n\n\n\n\n\n\n\nFigur 1: Bilde av forsiden med oversikt over Mine team i Dapla Ctrl.\n\n\n\nFigur 1 viser landingssiden/forsiden som først møter den som logger seg inn i Dapla Ctrl. den. Her får den som er innlogget oversikt over hvilke Dapla-team man er medlem av, og følgende informasjon om teamene:\n\nTeknisk teamnavn\nTeamets eierseksjon\nAntall teammedlemmer\nManagers for teamet\n\nMan kan også bytte fane fra Mine team til Alle team for å se samme informasjon om alle team som finnes på Dapla.\n\n\n\n\n\n\n\n\n\n\nFigur 2: Bilde av oversikten over et enkelt-team i Dapla Ctrl.\n\n\n\nFra Teamoversikten kan man trykke seg inn på et spesifikt team og få en oversikt slik som vist i Figur 2.\nPå toppen av siden får man se følgende informasjon:\n\nteamets visningsnavn\nteamets tekniske kortnavn\neierseksjonens seksjonsleder\neierseksjonens seksjonsnavn\nautonomitetsnivået til teamet\n\nVidere ser vi at det er en fane for Teammedlemmer og en for Delte Data. Under fanen for Teammedlemmer ser man følgende informasjon om alle medlemmene av teamet:\n\nnavn på medlem\nhvilken seksjons de jobber på\nhvilken tilgangsgruppe de tilhører på teamet\ne-postadresse med brukerens kortnavn\n\nUnder fanen Delte data får man en oversikt over hvilke bøtter teamet har opprettet for å dele data med andre team.\n\n\n\n\n\n\nFigur 3: Bilde av oversikten over hvilke bøtter teamet har opprettet for å dele data med andre team.\n\n\n\nFigur 3 viser hvilken informasjon man får over teamets delte data. Følgende informasjon vises:\n\nkortnavnet på bøttene\ntekniske navnet til bøttene\nhvor mange team som har tilgang\nhvor mange personer som har tilgang2\n\n\n\n\n\n\n\n\n\n\n\nFigur 4: Bilde av oversikten over hvilke personer som har tilgang til en delt-bøtte.\n\n\n\nFra Teamvisningen kan man velge fanen Delte data og trykke seg på en av teamets delte-bøtter. Figur 4 viser informasjon man får se i denne visningen. På toppen av siden får man se kortnavnet til bøtta, det tekniske navnet på bøtta, hvilket team som eier bøtta og hvilken eierseksjon teamet har. I tabellen som vises kan man undersøke hvilke personer som har tilgang til bøtta og få følgende informasjon om de:\n\nnavn\nhvilken seksjon de jobber på\nhvilket team-medelemskap de har tilgang i kraft av\nhvilken tilgangsgruppe de er i på teamet de har tilgang i kraft av\n\nFra tabellen kan man velge å se nærmere på personen som har tilgang, f.eks. se hvilke andre tilganger denne personen har, eller man kan se nærmere på teamet som personen har tilgang i kraft av. Ved å undersøke teamet nærmere kommer man inn på Teamvisningen som er beskrevet over, mens visning av Teammedlemmer forklares under.\n\n\n\n\n\n\n\n\n\n\nFigur 5: Bilde av oversikten over hvilke personer som med i dine team.\n\n\n\nFigur 5 viser oversikt over teammedlemmer. Øverst på siden kan man velge mellom en fane for Mine teammedlemmer og Alle teammedlemmer. Førstnevnte viser hvilke andre medlemmer som er i de teamene den innloggede er med i, mens sistnevnte viser alle teammedlemmer i SSB3. I tabellen under får man følgende informasjon om teammedlemmene:\n\nnavn\nhvilken seksjons de jobber på\nhvor mange team de er medlem av\nhvor mange team de har tilgangsrollen data-admins\nnavn på personens seksjonsleder\n\n\n\n\n\n\n\n\n\n\n\nFigur 6: Bilde av oversikten over hvilke team en person er medlem av.\n\n\n\nFigur 6 viser hva man ser når går inn på en enkeltperson, enten via Teamoversikten eller Teammedlemmer. Øverst på siden står navnet til personen, hvorvidt de har arbeidssted i Oslo eller Kongsvinger4, hvilken seksjon de jobber på og e-postadressen deres.\nTabellen i Figur 6 får man en oversikt over hvilke team personen er medlem av, samt følgende detaljer:\n\nteamets tekniske kortnavn\nseksjonseier av teamet\nhvilke tilgangsgrupper personen er med i\nhvem som er managers for teamet\n\nVidere kan man gå videre inn på et av teamene og se nærmere på hvem som er medlemmer og hvilke data de deler.\n\n\n\n\n\nDet er kun seksjonsledere i SSB som kan opprette et Dapla-team. Hvis en seksjonsleder logger seg inn i Dapla Ctrl så vil knappen i Figur 7 vises på Teamoversikt-siden. Eierseksjonen til et team vil bli definert av hvilken seksjonsleder som oppretter teamet.\n\n\n\n\n\n\n\n\n\nFigur 7: Bilde av knappen som vises for seksjonsledere.\n\n\n\n\n\nNår man oppretter et team må man fylle ut skjemaet i Figur 8. Under finner du en oversikt hva som er viktig å vurdere når man fyller ut de ulike feltene.\n\n\n\n\nVisningsnavn er teamets navn i et lesevennlig format. Navnet bør bestå av et hoveddomenet og et subdomenet. Det er tillatt med små/store bokstaver, mellomrom, Æ, Ø og Å.\nEksempel på et hoveddomenet i SSB er Skatt, og under det finnes det subdomener som Person og Næring. Visningsnavnet til teamene er da Skatt Person og Skatt Næring.\nI noen tilfeller gir det ikke mening med et subdomenet og da er det greit å kun ha et hoveddomenet. Et eksempel på et visningsnavn som kun har hoveddomenet er Nasjonalregnskap.\n\n\n\n\n\n\n\n\n\n\nFigur 8: Bilde av siden for opprettelse av team.\n\n\n\n\n\n\n\nVelg dette for å overstyre det automatisk genererte tekniske teamnavnet. Hvis man ikke krysser av denne boksen vil det genereres et teknisk teamnavn basert på visningsnavnet. Det kan være nyttig å velge dette hvis man har et langt visningsnavn og ønsker å forkorte det genererte tekniske teamnavnet.\n\n\n\nTeamets navn i et maskinvennlig format som bl.a. benyttes i filstier til lagringsbøtter. Det er ikke tillatt med mellomrom og norske tegn (Æ, Ø og Å). Navnet kan ikke overskride 17 tegn.\nDet tekniske teamnavnet blir automatisk generert basert på visningsnavn hvis man ikke velger Overstyr teknisk navn. Tabell 1 viser eksempler på visningsnavn der man både har overtyrt det tekniske navnet og ikke.\n\n\n\nTabell 1: Eksempler på teamnavn\n\n\n\n\n\nVisningsnavn\nTeknisk navn\nOverstyrt\n\n\n\n\nSkatt Person\nskatt-person\nnei\n\n\nSkatt Næring\nskatt-naering\nnei\n\n\nNasjonalregnskap\nnr\nja\n\n\nFinansmarkedsstatistikk\nfinmark\nja\n\n\n\n\n\n\n\n\n\nAlle team tilhører en seksjon og denne informasjonen ligger lagret i metadataene til teamet. Standard er at seksjonslederen som søker fyller ut sitt seksjonsnummer her, men det er mulig å velge andre seksjoner.\n\n\n\nNivå av frihet et team har til å definere sin egen infrastruktur. Statistikkproduserende team er vanligvis i kategorien Managed, dvs. at de kun bruker tjenester som tilbys av plattformen. IT-team vil ofte defineres som Self-Managed fordi dette gir større kontroll til teamet. Les mer her.\n\n\n\n\n\n\nHvis man som medlem av tilgangsgruppen managers logger seg inn Dapla Ctrl, vil man få tilgang til å legge til medlemmer i teamet man er i managers-gruppa for. Går man inn på Teamvisning vil man se knappen i Figur 9.\n\n\n\n\n\n\n\n\n\nFigur 9: Bilde av knappen som vises for managers.\n\n\n\n\n\nTrykker man på knappen så får man opp en side for å legge til nye medlemmer i teamet, slik som vist Figur 10. Man kan søke opp alle ansatte i SSB, og man kan velge å legge de til i en eller flere tilgangsgrupper. Når man har valgt person, og hvilke tilgangsgrupper de skal legges i, så avslutter man med å trykke på Legg til medlem for å effektuere endringen.\n\n\n\n\n\n\nFigur 10: Bilde av siden for å legge til medlemmer.\n\n\n\nDet kan ta mellom 1-2 minutter før tilgangen er aktivert og klar til bruk.\n\n\n\nHvis man som medlem av tilgangsgruppen managers logger seg inn Dapla Ctrl, vil man få tilgang til å fjerne og endre medlemmer i teamet man er i managers-gruppa for. Går man inn på Teamvisning så vil man se Endre-knapp for hver person i teamet.\n\n\n\n\n\n\nFigur 11: Bilde av Teamvisning som vises for personer i tilgangsgruppen managers.\n\n\n\n\n\nAv Figur 11 ser vi at hvert medlem i teamet har en Endre-knapp. Trykker man på den så får man opp bilde som vises i Figur 12.\nØnsker man å fjerne et medlem fra teamet, så kan man bare trykke på Fjern fra teamet. Da vil man bli spurt om å bekrefte at personen skal gjernes, og velger man ok så effektureres endringen ila et par minutter.\nØnsker man endre hvilken tilgangsgruppe en person er med i, så gjør man det ved å enten fjerne eller legge til tilganger som listet under dropdown-menyen for Tilgangsgruppe(r). For eksempel hvis en person ligger som både data-admins og developers, slik som eksempelet i Figur 12, så trykker man bare på X-ikonet for den tilgangen, og til slutt effektuerer man endringen ved å velge Oppdater tilgang.\n\n\n\n\n\n\n\n\n\nFigur 12: Bilde av siden for å endre eller fjerne medlemmer.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Dapla Ctrl"
    ]
  },
  {
    "objectID": "statistikkere/dapla-ctrl.html#footnotes",
    "href": "statistikkere/dapla-ctrl.html#footnotes",
    "title": "Dapla Ctrl",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nNavnet Dapla Ctrl er valgt for å kommunisere at målsetning med appen er å gi SSB-ere kontroll over tilgangsstyring på Dapla på en effektiv måte.↩︎\nAntall personer som har tilgang til en delt-bøtte viser hvor mange personer det er som har tilgang fra de teamene som har tilgang. Som regel vil det være slik at kun noen tilgangsgrupper i et team får tilgang til andre sine delte data, og ikke hele teamet.↩︎\nAlle teammedlemmer vil i praksis si alle ansatte i SSB, siden teamet Dapla Felles alltid legger til alle ansatte i SSB. Formålet med dette er å la alle ansatte få tilgang til testdata i en bøtte.↩︎\nHvorvidt arbeidssted er Oslo eller Kongsvinger indikeres med henholdsvis O eller K før seksjonsnummeret.↩︎",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Dapla Ctrl"
    ]
  },
  {
    "objectID": "statistikkere/github-app-integrasjon.html",
    "href": "statistikkere/github-app-integrasjon.html",
    "title": "Koble prosjektet til Github",
    "section": "",
    "text": "For at automatiseringsløsningen på Dapla skal kunne settes opp automatisk må denne ha tilgang til å lese fra prosjektets IAC-repo1. Dette avsnittet vil beskrive denne prosessen. Merk at dette er en engangsjobb som må gjøres av prosjektets kildedataansvarlige.\n\n\n\n\n\n\nViktig: Prosjektets kildedataansvarlige også må ha administrator-rettigheter til IAC-repoet i Github.\n\n\n\n\nLogg inn på Google Cloud Console og velg det prosjektet som skal konfigureres øverst venstre hjørte. Søk opp Cloud Build i søkefeltet og trykk på det valget som kommer opp.\nDet skal nå være en venstremeny tilgjengelig med tittel Cloud Build. Trykk på menyvalget som heter Triggers (Figur 1)\n\n\n\n\n\n\n\nFigur 1: Bilde av venstremeny\n\n\n\n\nI nedtrekkslisten Region sørg for at europe-north1 er valgt (Figur 2)\n\n\n\n\n\n\n\nFigur 2: Velg korrekt region\n\n\n\n\nTrykk deretter på en link som heter CONNECT REPOSITORY ca. midt på siden.\n\n\n\n\n\n\n\nFigur 3: Oversikt over triggers\n\n\n\n\nNå vil det dukke opp et vindu på høyre side med overskrift Connect repository (Figur 4). Velg GitHub (Cloud Build GitHub App) og trykk på CONTINUE\n\n\n\n\n\n\n\nFigur 4: Vindu for å velge Cloud Build Github App\n\n\n\n\nEt pop-up vindu tilsvarende Figur 5 vil komme opp. Trykk på Authorize. Vinduet vil etter hvert lukke seg og man kommer videre til et steg som heter Select repository (Figur 6)\n\n\n\n\n\n\n\nFigur 5: Pop-up vindu for Github\n\n\n\n\n\n\n\n\n\nFigur 6: Valg av Github repository\n\n\n\n\nTrykk på nedtrekkslisten Repository og skriv inn teamets navn. Huk av boksen ved teamets IAC-repo og trykk OK.\n\n\n\n\n\n\n\nFigur 7: Gi Google Build tilgang til Github repository\n\n\n\n\nKryss så av i sjekkboksen som i (Figur 8) og trykk CONNECT.\n\n\n\n\n\n\n\nFigur 8: Bekreft nytt Github repository\n\n\n\n\nTil slutt vil skjermbildet se ut som vist i Figur 9. Det siste steget Create a trigger kan du hoppe over. Dette vil bli satt opp av automatiseringsløsningen senere. Trykk på knappen DONE\n\n\n\n\n\n\n\nFigur 9: Siste steg - Create a trigger"
  },
  {
    "objectID": "statistikkere/github-app-integrasjon.html#footnotes",
    "href": "statistikkere/github-app-integrasjon.html#footnotes",
    "title": "Koble prosjektet til Github",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nIAC-repo er et en kodebase i Github på formen https://github.com/statisticsnorway/team-navn-iac.↩︎"
  },
  {
    "objectID": "statistikkere/pseudonymisering.html",
    "href": "statistikkere/pseudonymisering.html",
    "title": "Pseudonymisering",
    "section": "",
    "text": "På Dapla er det flere tjenester som til sammen gir team muligheten til å pseudonymisere, de-pseudonymisere eller re-pseudonymisere data1. Disse tjenestene er satt opp på en slik måte at statistikkteam kan være selvbetjent i bruken av funksjonaliteten, samtidig som de sikrer at direkte identifiserende opplysninger håndteres i henhold til lovverk og SSBs tolkninger av disse.\nI dette kapitlet forteller vi hvordan man går frem for å pseudonymisere data i et statistikkteam på Dapla. Før man leser videre er det viktig at leseren har en klar forståelse av hvordan Dapla-team og tilgangsstyring fungerer på Dapla, samt er kjent med tjenesten Kildomaten. Kildomaten er tjenesten som automatiserer overgangen fra kildedata til inndata, som blant annet inkluderer pseudonymisering.\n\n\nFor å pseudonymisere på Dapla må man være en del av Dapla-team og Kildomaten må være tilgjengeliggjort for teamet. Hvis du er usikker på om ditt team har tilgang til Kildomaten, så kan du sjekke dette selv i teamets IaC-repo. For hjelp til å aktivere Kildomaten kan man kontakte Kundeservice.\n\n\n\nHvert team er selv ansvarlig for at deres sensitiv data håndteres i henhold til lover og regler. I kontekst av pseudonymisering vil dette bety at teamet må finne ut av hvilken informasjon som skal pseudomymiseres i overgangen fra kildedata til inndata. All kildedata i SSB er klassifisert som sensitivt, og hvis man har kildedata som skal pseudonymiseres, så skal det skje i overgangen fra kildedata til inndata. Er man usikker på om man skal pseudonymisere eller ikke, eller hvilke lover og regler som gjelder for teamets kildedata, så kan man diskutere med personvernombudet og/eller ta kontakt med juristene i SSB.\n\n\n\n\n\n\nIkke glem dataminimering!\n\n\n\nEt av de viktigste tiltakene for å verne om personvern i data er å fjerne informasjon som ikke er strengt nødvendig for formålet. Dataminimering bør gjøres når data samles inn til SSB. I de tilfellene der det ikke er mulig, så skal det dataminimeres i overgangen fra kildedata til inndata. Etter at man har dataminimert så kan man vurdere hvilken gjenværende informasjon som skal pseudonymiseres.\n\n\n\n\nPersonidentifiserende informasjon (PII) er alle opplysninger som kan knyttes til en enkeltperson. Det følger av både personopplysningsloven og statistikkloven at personidentifiserende informasjon som samles inn for statistikkformål skal pseudonymiseres. I SSB så er det gjennomført personvernkonsekvensutredninger2 (PVK) for to områder som benytter seg av PII:\n\nPersonstatistikk\nNæringsstatistikk\n\nUnder går vi gjennom hvordan PII skal behandels i de to tilfellene, og hvilke konsekvenser det har for samarbeid mellom team på Dapla.\n\n\n\n\nI SSB er det gjennomført en PVK for personstatistikk som presiserer at all PII innen personstatistikk skal pseudonymiseres. Hvert team som håndterer personopplysninger må derfor vurdere sine kildedata og identifisere alt av PII. All data som innholder PII, og som ikke kan dataminimeres bort, skal pseudonymiseres.\nTabell 1 viser en ikke-uttømmende liste over PII som har vært identifisert i SSB tidligere. Merk at dette ikke er en fullstendig liste over PII, men hvis et team har denne informasjonen i sine data så skal de pseudonymiseres i overgangen fra kildedata til inndata.\nFor mange er fødselsnummer en viktig variabel i kraft at den fungerer som en koblingsnøkkel for ulike typer persondata. Men som vist i Tabell 1 så er det mye annen informasjon som også er å regne som PII. For eksempel er også adresse og bankkontonummer å regne som PII.\n\n\nI Tabell 1 ser vi at organisasjonsnummeret til et enkeltpersonforetak (ENK) er å regne som PII. Grunnen til det er at en ENK eies av en fysisk person og all informasjon knyttet til ENK er å regne som personopplysninger. PVK for personstatistikk begrenser sine vurderinger til å ikke inkludere informasjon om ENK.\nSiden personstatistikk skal pseudonymisere PII så følger det at også PII knyttet til ENK også bør pseudonymiseres. Grunnen til det er at det finnes mye åpen informasjon om ENK, og derfor kan man lett knytte pseudonym til en person basert lett tilgjengelig informasjon. Kan man knytte et pseudonym til en person så kan pseudonymet anses å være avslørt. Selv om det samme ikke gjelder for andre organisasjonsformer (AS, ansvarlig selskap, etc.) så vil det være naturlig at disse også pseudonymiseres, blant annet for å sikre ENK’er ikke lett kan skilles ut.\n\n\n\nI SSB er det gjennomført en egen PVK for næringsstatistikk. Grunnen til det er at det også her behandles personopplysninger, men at behovene i næringstatistikken er vurdert som såpass annerledes sammenlignet med personstatistikk, at de gjennomførte en egen PVK. Et viktig bakteppe for denne vurderingen var at organisasjonsidentifiserende informasjon (OII) tidligere hadde blitt vurdert som nødvendig å se klarttekst. Mer om OII i neste avsnitt.\n\n\n\n\n\n\n\nTabell 1: Ikke-uttømmende liste over PII som har vært identifisert i SSB tidligere.\n\n\n\n\n\nPII\n\n\n\n\nFødselsnummer\n\n\nD-nummer\n\n\nS-nummer\n\n\nEktefellenummer\n\n\nFamilienummer\n\n\nHusholdningsnummer\n\n\nDufnummer\n\n\nBankkontonummer\n\n\nAdresse (tekstlig)\n\n\nEiendomsidentifikator\n\n\nVeiadresse (numerisk)\n\n\nMatrikkeladresse (numerisk)\n\n\nRegistreringsnummer (kjøretøy)\n\n\nNavn\n\n\nKontaktinformasjon\n\n\nHelsepersonellnummer\n\n\nOrganisasjonsnummer enkeltpersonsforetak\n\n\nForetakets navn\n\n\n\n\n\n\n\n\nSiden informasjon om ENK, en viktig del av næringstatistikken, er regnet som personopplysninger, var dette et viktig område som måtte avklares i PVK for næringsstatistikk. I tillegg kobles det andre personopplysninger, blant annet fra team som er definert under PVK for personstatistikk, som ytterligere kompliserer behandlingen av personopplysninger i statistikkproduksjon.\nKonklusjonen i PVK for næringsstatistikk er et klart skille mellom personopplysninger knyttet til populasjonsforvaltning og statistikkproduksjon. Det blir vurdert som nødvendig at ansatte som jobber med populasjonsforvaltning alltid har tilgang til all informasjon i klartekst, dvs. at ingen personopplysninger skal pseudonymiseres. Når det gjelder personopplysninger i statistikkproduksjons så er bildet noe mer sammensatt. Følgende PII blir identifisert som tilstede i produksjon av næringsstatistikk:\n\nEnkeltpersonforetak (ENK).\nFysiske personer som innehar roller i foretak eller bedrifter.\nFysiske personer som er kontaktpersoner for virksomheter i rapporteringer til SSB.\nPersonopplysninger fra administrative registre og skjemaundersøkelser om fysiske personer som har roller i foretak eller virksomheter.\n\nDe tre første punktene over blir vurdert som ikke nødvendig å pseudonymisere. Det siste punktet, som ofte innebærer å koble på informasjon fra team som er definert under PVK for personstatistikk, skal som hovedregel pseudonymiseres. I disse tilfellene skal man da som hovedregel pseudonymisere både PII og OII. Men det åpnes for at unntak kan gjøres i følgende scenarioer:\n\nDersom opplysninger om foretak/virksomhet (OII) må behandles upseudonymisert, så må både PII og OII behandles upseudonymisert3.\nHvis det er opplysninger om foretak/virksomhet som gjør det enkelt å identifisere enheten ut fra f.eks. geografisk tilhørighet, næringskode eller dominans innen næringen som utøves, bør både PII og OII behandles upseudonymisert.\n\nI sum betyr dette at PII i næringstatistikk kun skal pseudonymiseres der det kobles på personoppplysninger fra registre eller skjemaundersøkelser, og det ikke er nødvendig med OII for å produsere statistikken.\n\n\n\n\nOrganisasjonsidentifiserende informasjon (OII) er alle opplysninger som kan knyttes til en konkret virksomhet, foretak, selskap eller annen organisasjonsenhet. I SSB har det vært en lang tradisjon for at OII må kunne ses i klartekst, og at kravene er mindre strenge enn for personopplysninger. Men i statistikkloven av 2019 ble kravene til informasjonssikkerhet skjerpet, og § 9 (2) inneholder en mer generell bestemmelse om at direkte identifiserende opplysninger skal behandles og lagres adskilt fra øvrige opplysninger. Statisikklovutvalget presiserer også følgende om intensjonen med bak bestemmelsen:\n\nByrået behandler imidlertid også opplysninger som faller utenfor personopplysningsloven, inkludert markedssensitiv informasjon om foretak. Det bør derfor tas inn en generell bestemmelse om informasjonssikkerhet i statistikkloven, som omfatter alle opplysninger. Det kan være grunn til å behandle ulike opplysninger ulikt, og derfor bør bestemmelsen være av nokså generell karakter.\n\nI 2022 ble satt ned en gruppe som skulle evaluere hvordan SSB skulle behandle organisasjonsidentifiserende informasjon (OII) etter den nye statistikkloven fra 2019. Gruppa leverte en rapport med konklusjonen om at pseudonymisering av OII er uforenlig med formålet med behandlingen.\nKonklusjonen er at OII ikke skal pseudonymiseres.\n\n\n\nPå Dapla jobber man i team som organisasjonen selv utformer. Team som definerer seg under PVK for næringsstatistikk må derfor ta hensyn til hvilke data ulike medarbeidere får tilgang til når de oppretter et team. F.eks. kan ikke de samme ansatte være med i team arbeider med PII i klartekst, samtidig som de er med i team der PII behandles i pseudonymisert form. I disse tilfellene må managers (som er ansvarlig for teamet) sikre at tilgang til data ikke avslører pseudonymer eller bryter regelverket.\n\n\n\nSiden team som definerer seg under PVK for personstatistikk pseudonymiserer både PII og OII, skaper dette hindringer for deling av data med team som definerer seg under PVK for næringstatistikk. F.eks. hvis førstnevnte deler et datasett med pseudonymer med sistnevnte, vil det være stor risiko for at pseudonymet blir avslørt. I tillegg vil det være praktisk vanskelig å koble data på tvers når koblingsnøkkelen er representert forskjellig. På nåværende tidspunkt finnes det ingen tjenester på Dapla som lar et team koble data der koblingsnøkkel er behandlet forskjellig.\n\n\n\n\nPseudonymisering skjer med python-biblioteket dapla-toolbelt-pseudo, og skal skje i overgangen fra kildedata til inndata. Brukeren kan sende inn skriptet som skal kjøres på dataene, men selve kjøringen skjer automatisk i Kildomaten.\n\n\n\n\n\n\nIkke mulig å pseudonymisere “manuelt”\n\n\n\nDet er ikke mulig å pseudonymisere “manuelt” fra en Jupyter Notebook eller lignende med ekte data i prod-miljøet. Grunnen til det er at det ville gitt brukeren/data-admins mulighet til å printe ut både data i klartekst og pseudonymisert form, og på den måten avsløre pseudonymet. Av den grunn er det bare systembrukeren i Kildomaten som kan prosessere skarpe data. Hvis man ønsker å teste koden sin manuelt før man produksjonssetter det i Kildomaten, så kan man benytte testdata i teamets test-miljø.\n\n\n\n\nI dette avsnittet viser vi et en eksempel på en typisk arbeidsflyt for et team skal pseudonymisere data. La oss anta at det finnes et team som heter dapla-example, som ønsker å dataminimere og pseudonymisere sine kildedata i overgangen til inndata. Kort fortalt må teamet gjøre følgende:\n\nSørg for at Kildomaten er aktivert i prod-miljøet.\nSkriv et script som skal kjøres på alle nye filer som kommer inn på en gitt filsti i kildebøtta.\nAutomatiser scriptet som en kilde i Kildomaten.\n\nPunkt 1 og 3 er nøye forklart i kapitlet oom Kildomaten. Derfor fokuserer vi på skriving av selve sciptet under, deriblant bruken av **dapla-toolbelt-pseudo.\n\n\nAnta at team dapla-example fortløpende får inn nye kildedatafiler4 i sin kildebøtte som vist i Tabell 2.\n\n\n\nTabell 2: Kildedata for team dapla-example\n\n\n\n\n\nfødselsnummer\nfornavn\netternavn\nadresse\ninntekt\n\n\n\n\n11111122222\nDonald\nDuck\nAndeby 15\n500000\n\n\n\n\n\n\nTeamet er definert under PVK for personstatistikk og ønsker dataminimere bort kolonnene fornavn, etternavn og adresse, mens de ønsker å pseudonymisere fødselsnummer og beholde inntekt som den er. Teamet skriver derfor følgende script:\n\n\nprocess_source_data.py\n\nimport dapla as dp\nfrom dapla_pseudo import Pseudonymize\n\n# Filsti til en kildedatafil\nsource_file = \"gs://ssb-dapla-example-data-kilde-prod/donald/andeby_inntekt_p2024_v1.parquet\"\n\n# Leser inn parquet-filen til en Pandas dataframe\ndf = dp.read_pandas(source_file)\n\n# Dataminimerer ved å beholde to kolonner\ndf2 = df[['fødselsnummer', 'inntekt']]\n\n# Pseudonymiserer fødselsnummer\ndf3 = (\n    Pseudonymize.from_pandas(df2)                  \n    .on_fields(\"fødselsnummer\")                    \n    .with_stable_id()                              \n    .run()                                     \n    .to_pandas()\n)\n\n# Skriv inndata til produktbøtte ved å bytte ut et ord i bøttenavnet\ninndata_file = source_file.replace(\"kilde\", \"produkt\")\ndp.write_pandas(inndata_file)\n\nI process_source_data.py-eksempelet over så leses det inn en konkret fil fra kildebøtta, deretter dataminimeres det ved å kun beholde de to kolonne av interesse. Så pseudonymiseres fødselsnummer, og til slutt skrives en parquet-fil til produktbøtta.\nPseudonymize()-metoden følger et såkalt builder-pattern der vi kan spesifisere hva som skal gjøres i hvilken rekkefølge. I from_pandas() sier vi at dataene som skal brukes er en Pandas dataframe, i on_fields() spesifiseres hvilke kolonner som skal pseudonymiseres, og with_stable_id spesifiserer at vi ønsker å oversette fnr til stabil ID og bruke samme krypteringsalgoritme som i Papis-prosjektet5. Til slutt ber vi om at alt blir kjørt med run() og output skal være en Pandas dataframe med to_pandas().\nResultatet blir en dataframe som vist i Tabell 3.\n\n\n\nTabell 3: Dataminimert og pseudonymisert inndata fra Tabell 2\n\n\n\n\n\nfødselsnummer\ninntekt\n\n\n\n\n1a45x88\n500000\n\n\n\n\n\n\nSiden with_stable_id() bytter ut fødselsnummer med stabil ID før den pseudonymiseres med en formatbevarende algoritme, og stabil ID alltid er syv karakterer lang, så får vi tilbake et pseudonym som er syv karakterer lang. Legg også merke til at pseudonymiseringen aldri endrer navn på kolonner eller datatyper.\nFor å teste denne koden på noe data fra et verktøy som Jupyterlab, så må vi gjøre det i test-miljøet til teamet. Til det trenger vi noe testdata. I tilfellet med stabil ID, så må man da generere testdata med fødselsnummer som finnes i test-versjonen av stabilID-katalogen (mer informasjon rundt dette kommer snart).\nFør vi kan sende inn dette scriptet for automatisk prosessering i Kildomaten, så må vi tilpasse koden litt. Helt konkret må vi gjøre følgende:\n\nPakke koden inn i en main()-funksjon\nKvitte oss med hardkoding av stier i koden.\n\nI eksempelet over kan gjøre følgende for å tilpasse koden til Kildomaten:\n\n\nprocess_source_data.py\n\ndef main(source_file):\n    import dapla as dp\n    from dapla_pseudo import Pseudonymize\n\n    # Leser inn parquet-filen til en Pandas dataframe\n    df = dp.read_pandas(source_file)\n\n    # Dataminimerer ved å beholde to kolonner\n    df2 = df[['fødselsnummer', 'inntekt']]\n\n    # Pseudonymiserer fødselsnummer\n    df3 = (\n        Pseudonymize.from_pandas(df2)                  \n        .on_fields(\"fødselsnummer\")                    \n        .with_stable_id()                              \n        .run()                                     \n        .to_pandas()\n    )\n\n    # Skriv inndata til produktbøtte ved å bytte ut et ord i bøttenavnet\n    inndata_file = source_file.replace(\"kilde\", \"produkt\")\n    dp.write_pandas(inndata_file)\n\nSiden Kildomaten trigges hver gang en ny fil dukker opp, så får main()-funksjonen filstien injisert hver gang en fil prosesseres. Du trenger derfor ikke å definere denne selv, og derfor er denne delen av koden slettet. Og med de tilpassingene kan koden produksjonssettes i IaC-repoet til dapla-example, slik som vist her.\n\n\n\n\ndapla-toolbelt-pseudo er ferdig installert i Kildomaten. Men ønsker du å bruke det i test-miljøet til teamet så kan du installere det i et ssb-project fra PyPI med denne kommandoen:\n\n\nTerminal\n\npoetry add dapla-toolbelt-pseudo\n\n\n\n\nPseudonymisering tilbys via Pseudonymize-metoden i dapla-toolbelt-pseudo. Den følger et builder-pattern der man spesifiserer hva og i hvilken rekkefølge operasjonene skal gjøres. Anta at det finnes i en dataframe i minnet som heter df hvor kolonnen fnr skal pseudonymiseres. Da vil koden se slik ut:\nfrom dapla_pseudo import Pseudonymize\n\nresult_df = (\n    Pseudonymize.from_polars(df) \n    .on_fields(\"fnr\")\n    .with_default_encryption()                     \n    .run()                                \n    .to_polars()                                   \n)\nI koden over så angir from_polars(df) at kolonnen vi ønsker å pseudonymisere ligger i en Polars dataframe i minnet og heter df. Deretter spesifiserer vi at kolonnen fnr er den som skal behandles med funksjonen on_fields(\"fnr\"). Videre angir with_default_encryption() at fnr skal pseudonymiseres med dapla-toolbelt-speudo sin standard-algoritme6. Til slutt ber vi om at det ovennevnte blir kjørt med funksjonen run(), og at dataene skal returneres som en Polars dataframe med funksjonen to_polars().\nSe flere eksempler i dokumentasjonen.\n\n\n\nPseudonymisering tilbys via Depseudonymize-metoden i dapla-toolbelt-pseudo. Den følger et builder-pattern der man spesifiserer hva og i hvilken rekkefølge operasjonene skal gjøres. Anta at det finnes i en Polars dataframe i minnet som heter df hvor kolonnen fnr skal de-pseudonymiseres. Da vil koden se slik ut:\nresult_df = (\n    Depseudonymize.from_polars(df)            \n    .on_fields(\"fnr\")                          \n    .with_default_encryption()                     \n    .run()                                         \n    .to_polars()                                   \n)\nOppbygningen av koden med Depseudonymize() er helt lik som for Pseudonomize(). Les beskrivelsen der for å se hva de ulike funksjonskallene gjør.\nDepseudonymize() fungerer ikke for data som er pseudponymisert med with_stable_id() enda. Kommer snart.\nSe flere eksempler i dokumentasjonen.\n\n\n\nIkke tilgjengelig enda.\n\n\n\nI statistikkproduksjon og forskning er det viktig å kunne følge de samme personene over tid. Derfor har fødselsnummer ofte blitt oversatt til en mer stabilt identifikator, ofte kalt SNR eller stabil ID7. Funksjonene with_stable_id() og validator() bruker stabilID-katalogen til å henholdsvis bytte ut fødselsnummer med stabil ID, og for å validere om fødselsnummer er gyldige(se under).\nDu kan selv spesifisere hvilken versjon av SNR-katalogen du ønsker å bruke. Det gjør du ved å oppgi en gyldighetsdato og så finner dapla-toolbelt-pseudo hvilken versjon av katalogen som ligger nærmest i tid.\n\n\n\nValidator-metoden kan benyttes til å sjekke om fødselsnummer finnes i SNR-katalogen (se over). Her kan man også spesifisere hvilken versjon av SNR-katalogen man ønsker å bruke. Standard, hvis ingenting velges, er at siste tilgjengelige versjon velges. Under er et eksempel på hvordan man validerer fødselsnummer for en gitt gyldighetsdato:\nfrom dapla_pseudo import Validator\nfrom dapla_pseudo.utils import convert_to_date\n\nresult = (\n    Validator.from_polars(df)\n    .on_field(\"fnr\")\n    .validate_map_to_stable_id(\n        sid_snapshot_date=convert_to_date(\"2023-08-29\")\n    )\n)\n# Vis hvilken versjon av SNR-katalogen som er benyttet\nresult.metadata\n# Vis fødselsnummer som ikke fikk treff i SNR-katalogen som en Polars dataframe\nresult.to_polars()\n\n\n\ndapla-toolbelt-pseudo støtter følgende dataformater:\n\ncsv (filformat)\njson (filformat)\nPolars dataframe (minnet)\nPandas dataframe (minnet)\n\nOver har vi vist hvordan vi leser data fra minnet, men det støttes også å lese direkte fra filformatene csv og json. Under er et eksempel med en csv-fil:\nfrom dapla_pseudo import Pseudonymize\nfrom dapla import AuthClient\n\n\nfile_path=\"data/personer.csv\"\n\noptions = {\n    \"dtypes\": {\"fnr\": pl.Utf8, \"fornavn\": pl.Utf8, \"etternavn\": pl.Utf8, \"kjonn\": pl.Categorical, \"fodselsdato\": pl.Utf8}\n}\n\nresult_df = (\n    Pseudonymize.from_file(file_path)\n    .on_fields(\"fnr\")\n    .with_default_encryption()\n    .run()\n    .to_polars(**options)\n)\nSe flere eksempler i dokumentasjonen.\n\n\n\ndapla-toolbelt-pseudo støtter mange forskjellige krypteringsalgoritmer. Les mer i dokumentasjonen.\n\n\n\nKommer snart.\n\n\n\n\nKommer snart."
  },
  {
    "objectID": "statistikkere/pseudonymisering.html#forberedelser",
    "href": "statistikkere/pseudonymisering.html#forberedelser",
    "title": "Pseudonymisering",
    "section": "",
    "text": "For å pseudonymisere på Dapla må man være en del av Dapla-team og Kildomaten må være tilgjengeliggjort for teamet. Hvis du er usikker på om ditt team har tilgang til Kildomaten, så kan du sjekke dette selv i teamets IaC-repo. For hjelp til å aktivere Kildomaten kan man kontakte Kundeservice."
  },
  {
    "objectID": "statistikkere/pseudonymisering.html#hva-skal-pseudonymiseres",
    "href": "statistikkere/pseudonymisering.html#hva-skal-pseudonymiseres",
    "title": "Pseudonymisering",
    "section": "",
    "text": "Hvert team er selv ansvarlig for at deres sensitiv data håndteres i henhold til lover og regler. I kontekst av pseudonymisering vil dette bety at teamet må finne ut av hvilken informasjon som skal pseudomymiseres i overgangen fra kildedata til inndata. All kildedata i SSB er klassifisert som sensitivt, og hvis man har kildedata som skal pseudonymiseres, så skal det skje i overgangen fra kildedata til inndata. Er man usikker på om man skal pseudonymisere eller ikke, eller hvilke lover og regler som gjelder for teamets kildedata, så kan man diskutere med personvernombudet og/eller ta kontakt med juristene i SSB.\n\n\n\n\n\n\nIkke glem dataminimering!\n\n\n\nEt av de viktigste tiltakene for å verne om personvern i data er å fjerne informasjon som ikke er strengt nødvendig for formålet. Dataminimering bør gjøres når data samles inn til SSB. I de tilfellene der det ikke er mulig, så skal det dataminimeres i overgangen fra kildedata til inndata. Etter at man har dataminimert så kan man vurdere hvilken gjenværende informasjon som skal pseudonymiseres.\n\n\n\n\nPersonidentifiserende informasjon (PII) er alle opplysninger som kan knyttes til en enkeltperson. Det følger av både personopplysningsloven og statistikkloven at personidentifiserende informasjon som samles inn for statistikkformål skal pseudonymiseres. I SSB så er det gjennomført personvernkonsekvensutredninger2 (PVK) for to områder som benytter seg av PII:\n\nPersonstatistikk\nNæringsstatistikk\n\nUnder går vi gjennom hvordan PII skal behandels i de to tilfellene, og hvilke konsekvenser det har for samarbeid mellom team på Dapla.\n\n\n\n\nI SSB er det gjennomført en PVK for personstatistikk som presiserer at all PII innen personstatistikk skal pseudonymiseres. Hvert team som håndterer personopplysninger må derfor vurdere sine kildedata og identifisere alt av PII. All data som innholder PII, og som ikke kan dataminimeres bort, skal pseudonymiseres.\nTabell 1 viser en ikke-uttømmende liste over PII som har vært identifisert i SSB tidligere. Merk at dette ikke er en fullstendig liste over PII, men hvis et team har denne informasjonen i sine data så skal de pseudonymiseres i overgangen fra kildedata til inndata.\nFor mange er fødselsnummer en viktig variabel i kraft at den fungerer som en koblingsnøkkel for ulike typer persondata. Men som vist i Tabell 1 så er det mye annen informasjon som også er å regne som PII. For eksempel er også adresse og bankkontonummer å regne som PII.\n\n\nI Tabell 1 ser vi at organisasjonsnummeret til et enkeltpersonforetak (ENK) er å regne som PII. Grunnen til det er at en ENK eies av en fysisk person og all informasjon knyttet til ENK er å regne som personopplysninger. PVK for personstatistikk begrenser sine vurderinger til å ikke inkludere informasjon om ENK.\nSiden personstatistikk skal pseudonymisere PII så følger det at også PII knyttet til ENK også bør pseudonymiseres. Grunnen til det er at det finnes mye åpen informasjon om ENK, og derfor kan man lett knytte pseudonym til en person basert lett tilgjengelig informasjon. Kan man knytte et pseudonym til en person så kan pseudonymet anses å være avslørt. Selv om det samme ikke gjelder for andre organisasjonsformer (AS, ansvarlig selskap, etc.) så vil det være naturlig at disse også pseudonymiseres, blant annet for å sikre ENK’er ikke lett kan skilles ut.\n\n\n\nI SSB er det gjennomført en egen PVK for næringsstatistikk. Grunnen til det er at det også her behandles personopplysninger, men at behovene i næringstatistikken er vurdert som såpass annerledes sammenlignet med personstatistikk, at de gjennomførte en egen PVK. Et viktig bakteppe for denne vurderingen var at organisasjonsidentifiserende informasjon (OII) tidligere hadde blitt vurdert som nødvendig å se klarttekst. Mer om OII i neste avsnitt.\n\n\n\n\n\n\n\nTabell 1: Ikke-uttømmende liste over PII som har vært identifisert i SSB tidligere.\n\n\n\n\n\nPII\n\n\n\n\nFødselsnummer\n\n\nD-nummer\n\n\nS-nummer\n\n\nEktefellenummer\n\n\nFamilienummer\n\n\nHusholdningsnummer\n\n\nDufnummer\n\n\nBankkontonummer\n\n\nAdresse (tekstlig)\n\n\nEiendomsidentifikator\n\n\nVeiadresse (numerisk)\n\n\nMatrikkeladresse (numerisk)\n\n\nRegistreringsnummer (kjøretøy)\n\n\nNavn\n\n\nKontaktinformasjon\n\n\nHelsepersonellnummer\n\n\nOrganisasjonsnummer enkeltpersonsforetak\n\n\nForetakets navn\n\n\n\n\n\n\n\n\nSiden informasjon om ENK, en viktig del av næringstatistikken, er regnet som personopplysninger, var dette et viktig område som måtte avklares i PVK for næringsstatistikk. I tillegg kobles det andre personopplysninger, blant annet fra team som er definert under PVK for personstatistikk, som ytterligere kompliserer behandlingen av personopplysninger i statistikkproduksjon.\nKonklusjonen i PVK for næringsstatistikk er et klart skille mellom personopplysninger knyttet til populasjonsforvaltning og statistikkproduksjon. Det blir vurdert som nødvendig at ansatte som jobber med populasjonsforvaltning alltid har tilgang til all informasjon i klartekst, dvs. at ingen personopplysninger skal pseudonymiseres. Når det gjelder personopplysninger i statistikkproduksjons så er bildet noe mer sammensatt. Følgende PII blir identifisert som tilstede i produksjon av næringsstatistikk:\n\nEnkeltpersonforetak (ENK).\nFysiske personer som innehar roller i foretak eller bedrifter.\nFysiske personer som er kontaktpersoner for virksomheter i rapporteringer til SSB.\nPersonopplysninger fra administrative registre og skjemaundersøkelser om fysiske personer som har roller i foretak eller virksomheter.\n\nDe tre første punktene over blir vurdert som ikke nødvendig å pseudonymisere. Det siste punktet, som ofte innebærer å koble på informasjon fra team som er definert under PVK for personstatistikk, skal som hovedregel pseudonymiseres. I disse tilfellene skal man da som hovedregel pseudonymisere både PII og OII. Men det åpnes for at unntak kan gjøres i følgende scenarioer:\n\nDersom opplysninger om foretak/virksomhet (OII) må behandles upseudonymisert, så må både PII og OII behandles upseudonymisert3.\nHvis det er opplysninger om foretak/virksomhet som gjør det enkelt å identifisere enheten ut fra f.eks. geografisk tilhørighet, næringskode eller dominans innen næringen som utøves, bør både PII og OII behandles upseudonymisert.\n\nI sum betyr dette at PII i næringstatistikk kun skal pseudonymiseres der det kobles på personoppplysninger fra registre eller skjemaundersøkelser, og det ikke er nødvendig med OII for å produsere statistikken.\n\n\n\n\nOrganisasjonsidentifiserende informasjon (OII) er alle opplysninger som kan knyttes til en konkret virksomhet, foretak, selskap eller annen organisasjonsenhet. I SSB har det vært en lang tradisjon for at OII må kunne ses i klartekst, og at kravene er mindre strenge enn for personopplysninger. Men i statistikkloven av 2019 ble kravene til informasjonssikkerhet skjerpet, og § 9 (2) inneholder en mer generell bestemmelse om at direkte identifiserende opplysninger skal behandles og lagres adskilt fra øvrige opplysninger. Statisikklovutvalget presiserer også følgende om intensjonen med bak bestemmelsen:\n\nByrået behandler imidlertid også opplysninger som faller utenfor personopplysningsloven, inkludert markedssensitiv informasjon om foretak. Det bør derfor tas inn en generell bestemmelse om informasjonssikkerhet i statistikkloven, som omfatter alle opplysninger. Det kan være grunn til å behandle ulike opplysninger ulikt, og derfor bør bestemmelsen være av nokså generell karakter.\n\nI 2022 ble satt ned en gruppe som skulle evaluere hvordan SSB skulle behandle organisasjonsidentifiserende informasjon (OII) etter den nye statistikkloven fra 2019. Gruppa leverte en rapport med konklusjonen om at pseudonymisering av OII er uforenlig med formålet med behandlingen.\nKonklusjonen er at OII ikke skal pseudonymiseres.\n\n\n\nPå Dapla jobber man i team som organisasjonen selv utformer. Team som definerer seg under PVK for næringsstatistikk må derfor ta hensyn til hvilke data ulike medarbeidere får tilgang til når de oppretter et team. F.eks. kan ikke de samme ansatte være med i team arbeider med PII i klartekst, samtidig som de er med i team der PII behandles i pseudonymisert form. I disse tilfellene må managers (som er ansvarlig for teamet) sikre at tilgang til data ikke avslører pseudonymer eller bryter regelverket.\n\n\n\nSiden team som definerer seg under PVK for personstatistikk pseudonymiserer både PII og OII, skaper dette hindringer for deling av data med team som definerer seg under PVK for næringstatistikk. F.eks. hvis førstnevnte deler et datasett med pseudonymer med sistnevnte, vil det være stor risiko for at pseudonymet blir avslørt. I tillegg vil det være praktisk vanskelig å koble data på tvers når koblingsnøkkelen er representert forskjellig. På nåværende tidspunkt finnes det ingen tjenester på Dapla som lar et team koble data der koblingsnøkkel er behandlet forskjellig."
  },
  {
    "objectID": "statistikkere/pseudonymisering.html#funksjonalitet",
    "href": "statistikkere/pseudonymisering.html#funksjonalitet",
    "title": "Pseudonymisering",
    "section": "",
    "text": "Pseudonymisering skjer med python-biblioteket dapla-toolbelt-pseudo, og skal skje i overgangen fra kildedata til inndata. Brukeren kan sende inn skriptet som skal kjøres på dataene, men selve kjøringen skjer automatisk i Kildomaten.\n\n\n\n\n\n\nIkke mulig å pseudonymisere “manuelt”\n\n\n\nDet er ikke mulig å pseudonymisere “manuelt” fra en Jupyter Notebook eller lignende med ekte data i prod-miljøet. Grunnen til det er at det ville gitt brukeren/data-admins mulighet til å printe ut både data i klartekst og pseudonymisert form, og på den måten avsløre pseudonymet. Av den grunn er det bare systembrukeren i Kildomaten som kan prosessere skarpe data. Hvis man ønsker å teste koden sin manuelt før man produksjonssetter det i Kildomaten, så kan man benytte testdata i teamets test-miljø.\n\n\n\n\nI dette avsnittet viser vi et en eksempel på en typisk arbeidsflyt for et team skal pseudonymisere data. La oss anta at det finnes et team som heter dapla-example, som ønsker å dataminimere og pseudonymisere sine kildedata i overgangen til inndata. Kort fortalt må teamet gjøre følgende:\n\nSørg for at Kildomaten er aktivert i prod-miljøet.\nSkriv et script som skal kjøres på alle nye filer som kommer inn på en gitt filsti i kildebøtta.\nAutomatiser scriptet som en kilde i Kildomaten.\n\nPunkt 1 og 3 er nøye forklart i kapitlet oom Kildomaten. Derfor fokuserer vi på skriving av selve sciptet under, deriblant bruken av **dapla-toolbelt-pseudo.\n\n\nAnta at team dapla-example fortløpende får inn nye kildedatafiler4 i sin kildebøtte som vist i Tabell 2.\n\n\n\nTabell 2: Kildedata for team dapla-example\n\n\n\n\n\nfødselsnummer\nfornavn\netternavn\nadresse\ninntekt\n\n\n\n\n11111122222\nDonald\nDuck\nAndeby 15\n500000\n\n\n\n\n\n\nTeamet er definert under PVK for personstatistikk og ønsker dataminimere bort kolonnene fornavn, etternavn og adresse, mens de ønsker å pseudonymisere fødselsnummer og beholde inntekt som den er. Teamet skriver derfor følgende script:\n\n\nprocess_source_data.py\n\nimport dapla as dp\nfrom dapla_pseudo import Pseudonymize\n\n# Filsti til en kildedatafil\nsource_file = \"gs://ssb-dapla-example-data-kilde-prod/donald/andeby_inntekt_p2024_v1.parquet\"\n\n# Leser inn parquet-filen til en Pandas dataframe\ndf = dp.read_pandas(source_file)\n\n# Dataminimerer ved å beholde to kolonner\ndf2 = df[['fødselsnummer', 'inntekt']]\n\n# Pseudonymiserer fødselsnummer\ndf3 = (\n    Pseudonymize.from_pandas(df2)                  \n    .on_fields(\"fødselsnummer\")                    \n    .with_stable_id()                              \n    .run()                                     \n    .to_pandas()\n)\n\n# Skriv inndata til produktbøtte ved å bytte ut et ord i bøttenavnet\ninndata_file = source_file.replace(\"kilde\", \"produkt\")\ndp.write_pandas(inndata_file)\n\nI process_source_data.py-eksempelet over så leses det inn en konkret fil fra kildebøtta, deretter dataminimeres det ved å kun beholde de to kolonne av interesse. Så pseudonymiseres fødselsnummer, og til slutt skrives en parquet-fil til produktbøtta.\nPseudonymize()-metoden følger et såkalt builder-pattern der vi kan spesifisere hva som skal gjøres i hvilken rekkefølge. I from_pandas() sier vi at dataene som skal brukes er en Pandas dataframe, i on_fields() spesifiseres hvilke kolonner som skal pseudonymiseres, og with_stable_id spesifiserer at vi ønsker å oversette fnr til stabil ID og bruke samme krypteringsalgoritme som i Papis-prosjektet5. Til slutt ber vi om at alt blir kjørt med run() og output skal være en Pandas dataframe med to_pandas().\nResultatet blir en dataframe som vist i Tabell 3.\n\n\n\nTabell 3: Dataminimert og pseudonymisert inndata fra Tabell 2\n\n\n\n\n\nfødselsnummer\ninntekt\n\n\n\n\n1a45x88\n500000\n\n\n\n\n\n\nSiden with_stable_id() bytter ut fødselsnummer med stabil ID før den pseudonymiseres med en formatbevarende algoritme, og stabil ID alltid er syv karakterer lang, så får vi tilbake et pseudonym som er syv karakterer lang. Legg også merke til at pseudonymiseringen aldri endrer navn på kolonner eller datatyper.\nFor å teste denne koden på noe data fra et verktøy som Jupyterlab, så må vi gjøre det i test-miljøet til teamet. Til det trenger vi noe testdata. I tilfellet med stabil ID, så må man da generere testdata med fødselsnummer som finnes i test-versjonen av stabilID-katalogen (mer informasjon rundt dette kommer snart).\nFør vi kan sende inn dette scriptet for automatisk prosessering i Kildomaten, så må vi tilpasse koden litt. Helt konkret må vi gjøre følgende:\n\nPakke koden inn i en main()-funksjon\nKvitte oss med hardkoding av stier i koden.\n\nI eksempelet over kan gjøre følgende for å tilpasse koden til Kildomaten:\n\n\nprocess_source_data.py\n\ndef main(source_file):\n    import dapla as dp\n    from dapla_pseudo import Pseudonymize\n\n    # Leser inn parquet-filen til en Pandas dataframe\n    df = dp.read_pandas(source_file)\n\n    # Dataminimerer ved å beholde to kolonner\n    df2 = df[['fødselsnummer', 'inntekt']]\n\n    # Pseudonymiserer fødselsnummer\n    df3 = (\n        Pseudonymize.from_pandas(df2)                  \n        .on_fields(\"fødselsnummer\")                    \n        .with_stable_id()                              \n        .run()                                     \n        .to_pandas()\n    )\n\n    # Skriv inndata til produktbøtte ved å bytte ut et ord i bøttenavnet\n    inndata_file = source_file.replace(\"kilde\", \"produkt\")\n    dp.write_pandas(inndata_file)\n\nSiden Kildomaten trigges hver gang en ny fil dukker opp, så får main()-funksjonen filstien injisert hver gang en fil prosesseres. Du trenger derfor ikke å definere denne selv, og derfor er denne delen av koden slettet. Og med de tilpassingene kan koden produksjonssettes i IaC-repoet til dapla-example, slik som vist her.\n\n\n\n\ndapla-toolbelt-pseudo er ferdig installert i Kildomaten. Men ønsker du å bruke det i test-miljøet til teamet så kan du installere det i et ssb-project fra PyPI med denne kommandoen:\n\n\nTerminal\n\npoetry add dapla-toolbelt-pseudo\n\n\n\n\nPseudonymisering tilbys via Pseudonymize-metoden i dapla-toolbelt-pseudo. Den følger et builder-pattern der man spesifiserer hva og i hvilken rekkefølge operasjonene skal gjøres. Anta at det finnes i en dataframe i minnet som heter df hvor kolonnen fnr skal pseudonymiseres. Da vil koden se slik ut:\nfrom dapla_pseudo import Pseudonymize\n\nresult_df = (\n    Pseudonymize.from_polars(df) \n    .on_fields(\"fnr\")\n    .with_default_encryption()                     \n    .run()                                \n    .to_polars()                                   \n)\nI koden over så angir from_polars(df) at kolonnen vi ønsker å pseudonymisere ligger i en Polars dataframe i minnet og heter df. Deretter spesifiserer vi at kolonnen fnr er den som skal behandles med funksjonen on_fields(\"fnr\"). Videre angir with_default_encryption() at fnr skal pseudonymiseres med dapla-toolbelt-speudo sin standard-algoritme6. Til slutt ber vi om at det ovennevnte blir kjørt med funksjonen run(), og at dataene skal returneres som en Polars dataframe med funksjonen to_polars().\nSe flere eksempler i dokumentasjonen.\n\n\n\nPseudonymisering tilbys via Depseudonymize-metoden i dapla-toolbelt-pseudo. Den følger et builder-pattern der man spesifiserer hva og i hvilken rekkefølge operasjonene skal gjøres. Anta at det finnes i en Polars dataframe i minnet som heter df hvor kolonnen fnr skal de-pseudonymiseres. Da vil koden se slik ut:\nresult_df = (\n    Depseudonymize.from_polars(df)            \n    .on_fields(\"fnr\")                          \n    .with_default_encryption()                     \n    .run()                                         \n    .to_polars()                                   \n)\nOppbygningen av koden med Depseudonymize() er helt lik som for Pseudonomize(). Les beskrivelsen der for å se hva de ulike funksjonskallene gjør.\nDepseudonymize() fungerer ikke for data som er pseudponymisert med with_stable_id() enda. Kommer snart.\nSe flere eksempler i dokumentasjonen.\n\n\n\nIkke tilgjengelig enda.\n\n\n\nI statistikkproduksjon og forskning er det viktig å kunne følge de samme personene over tid. Derfor har fødselsnummer ofte blitt oversatt til en mer stabilt identifikator, ofte kalt SNR eller stabil ID7. Funksjonene with_stable_id() og validator() bruker stabilID-katalogen til å henholdsvis bytte ut fødselsnummer med stabil ID, og for å validere om fødselsnummer er gyldige(se under).\nDu kan selv spesifisere hvilken versjon av SNR-katalogen du ønsker å bruke. Det gjør du ved å oppgi en gyldighetsdato og så finner dapla-toolbelt-pseudo hvilken versjon av katalogen som ligger nærmest i tid.\n\n\n\nValidator-metoden kan benyttes til å sjekke om fødselsnummer finnes i SNR-katalogen (se over). Her kan man også spesifisere hvilken versjon av SNR-katalogen man ønsker å bruke. Standard, hvis ingenting velges, er at siste tilgjengelige versjon velges. Under er et eksempel på hvordan man validerer fødselsnummer for en gitt gyldighetsdato:\nfrom dapla_pseudo import Validator\nfrom dapla_pseudo.utils import convert_to_date\n\nresult = (\n    Validator.from_polars(df)\n    .on_field(\"fnr\")\n    .validate_map_to_stable_id(\n        sid_snapshot_date=convert_to_date(\"2023-08-29\")\n    )\n)\n# Vis hvilken versjon av SNR-katalogen som er benyttet\nresult.metadata\n# Vis fødselsnummer som ikke fikk treff i SNR-katalogen som en Polars dataframe\nresult.to_polars()\n\n\n\ndapla-toolbelt-pseudo støtter følgende dataformater:\n\ncsv (filformat)\njson (filformat)\nPolars dataframe (minnet)\nPandas dataframe (minnet)\n\nOver har vi vist hvordan vi leser data fra minnet, men det støttes også å lese direkte fra filformatene csv og json. Under er et eksempel med en csv-fil:\nfrom dapla_pseudo import Pseudonymize\nfrom dapla import AuthClient\n\n\nfile_path=\"data/personer.csv\"\n\noptions = {\n    \"dtypes\": {\"fnr\": pl.Utf8, \"fornavn\": pl.Utf8, \"etternavn\": pl.Utf8, \"kjonn\": pl.Categorical, \"fodselsdato\": pl.Utf8}\n}\n\nresult_df = (\n    Pseudonymize.from_file(file_path)\n    .on_fields(\"fnr\")\n    .with_default_encryption()\n    .run()\n    .to_polars(**options)\n)\nSe flere eksempler i dokumentasjonen.\n\n\n\ndapla-toolbelt-pseudo støtter mange forskjellige krypteringsalgoritmer. Les mer i dokumentasjonen.\n\n\n\nKommer snart."
  },
  {
    "objectID": "statistikkere/pseudonymisering.html#ytelse",
    "href": "statistikkere/pseudonymisering.html#ytelse",
    "title": "Pseudonymisering",
    "section": "",
    "text": "Kommer snart."
  },
  {
    "objectID": "statistikkere/pseudonymisering.html#footnotes",
    "href": "statistikkere/pseudonymisering.html#footnotes",
    "title": "Pseudonymisering",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPseudonymisering defineres her som å erstatte spesifikke data med kunstige data (pseudonymer), mens de-pseudonymisering gjør det samme bare motsatt vei. Re-pseudonymisering defineres som å bytte ut et pseudonym med et annet, uten at brukeren nødvendigvis får tilgang til den opprinnelige verdien. ↩︎\nPersonvernkonsekvensutredningen (PVK) er det norske ordet for den engelske betegnelsen Data Protection Impact Assessment (DPIA). Datatilsynet definerer en DPIA som en systematisk prosess, som identifiserer og evaluerer potensielle personvernkonsekvenser fra alle interessenters synsvinkel i et prosjekt, initiativ, foreslått system eller prosess. Les mer om DPIA i SSB↩︎\nGrunnen til dette er at hvis hvis PII er pseudonymisert og OII ikke, så vil det være lett å avsløre pseudonymet.↩︎\nFor å holde eksempelet enkelt så kan vi anta at filene kommer i parquet-formatet↩︎\nPapis-prosjektet pseudonymiserte alle PII på bakken med et spesifikt formatbevarende krypteringsalgoritme. Dette kunne gjøres direkte på fødselsnummer eller på stabil ID, også kjent som snr-nummer. For å gjøre det lett å migrere data til Dapla så støtter også pseudonymisering på Dapla denne tilnærmingen.↩︎\nStandardalgoritmen i dapla-toolbelt-pseudo er den deterministiske krypteringsalgoritmen Deterministic Authenticated Encryption with Associated Data, eller DAEAD-algoritmen.↩︎\nSNR-katalogen eies og tilbys av Team Register på Dapla.↩︎"
  },
  {
    "objectID": "statistikkere/dapla-toolbelt-metadata.html",
    "href": "statistikkere/dapla-toolbelt-metadata.html",
    "title": "dapla-toolbelt-metadata",
    "section": "",
    "text": "Verktøy og klienter for å jobbe med Metadata på Dapla.\n\n\n\n\nPyPI pakke: https://pypi.org/project/dapla-toolbelt-metadata/\nAPI dokumentasjon: https://statisticsnorway.github.io/dapla-toolbelt-metadata/\nGitHub repo: https://github.com/statisticsnorway/dapla-toolbelt-metadata\n\n\n\n\n\n\n\n\nTerminal\n\npip install dapla-toolbelt-metadata\n\n\n\n\n\n\nTerminal\n\npoetry add dapla-toolbelt-metadata\n\n\n\n\n\nDen anbefalte måten å skape metadata for et nytt datasett er med Datadoc brukergrensesnittet. Men det finnes tilfeller hvor det kan være tungvint å bruke og man ønsker å jobbe med metadata ved hjelp av kodespråk. Det kan man gjøre med Datadoc klassen i dapla-toolbelt-metadata pakken.\n\n\n\n\nDet primære brukstilfelle er å skape metadata for datasett skapt for nye tidsperioder som en del av produksjonsløp.\nI dette tilfelle er Datadoc brukergrensesnittet brukt for å skape det første metadatadokumentet. Man kan deretter bruke metadatadokumentet som en utgangspunkt for nye tidsperioder. Datadoc klassen utleder oppdatert informasjon fra filstien til datasettene for nye tidsperioder og oppdaterer metadataen til å stemme.\n\n\n\n\n\n\nNote\n\n\n\nDette forutsetter at strukturen av datasettet og betydningen av variablene er lik mellom tidsperiodene. Om det finnes uforventede avvik så kastes en InconsistentDatasetsError med detaljer om avviket.\n\n\n\n\n\n\nNotebook\n\nfrom dapla_metadata.datasets import Datadoc\n\nmeta = Datadoc(\n1    dataset_path=\"gs://ssb-staging-dapla-felles-data-delt/datadoc/sykefratot/klargjorte_data/person_testdata_p2022_v1.parquet\",\n2    metadata_document_path=\"gs://ssb-staging-dapla-felles-data-delt/datadoc/sykefratot/klargjorte_data/person_testdata_p2021_v1__DOC.json\",\n)\n3meta.write_metadata_document()\n\n\n1\n\nEn ny tidsperiode av et datasett\n\n2\n\nEksisterende metadatadokument\n\n3\n\nSkaper et nytt metadatadokument med filsti som stemmer med datasettet, i dette tilfellet gs://ssb-staging-dapla-felles-data-delt/datadoc/sykefratot/klargjorte_data/person_testdata_p2022_v1__DOC.json",
    "crumbs": [
      "Manual",
      "Metadata",
      "dapla-toolbelt-metadata"
    ]
  },
  {
    "objectID": "statistikkere/dapla-toolbelt-metadata.html#lenker",
    "href": "statistikkere/dapla-toolbelt-metadata.html#lenker",
    "title": "dapla-toolbelt-metadata",
    "section": "",
    "text": "PyPI pakke: https://pypi.org/project/dapla-toolbelt-metadata/\nAPI dokumentasjon: https://statisticsnorway.github.io/dapla-toolbelt-metadata/\nGitHub repo: https://github.com/statisticsnorway/dapla-toolbelt-metadata",
    "crumbs": [
      "Manual",
      "Metadata",
      "dapla-toolbelt-metadata"
    ]
  },
  {
    "objectID": "statistikkere/dapla-toolbelt-metadata.html#installasjon",
    "href": "statistikkere/dapla-toolbelt-metadata.html#installasjon",
    "title": "dapla-toolbelt-metadata",
    "section": "",
    "text": "Terminal\n\npip install dapla-toolbelt-metadata\n\n\n\n\n\n\nTerminal\n\npoetry add dapla-toolbelt-metadata",
    "crumbs": [
      "Manual",
      "Metadata",
      "dapla-toolbelt-metadata"
    ]
  },
  {
    "objectID": "statistikkere/dapla-toolbelt-metadata.html#datasett",
    "href": "statistikkere/dapla-toolbelt-metadata.html#datasett",
    "title": "dapla-toolbelt-metadata",
    "section": "",
    "text": "Den anbefalte måten å skape metadata for et nytt datasett er med Datadoc brukergrensesnittet. Men det finnes tilfeller hvor det kan være tungvint å bruke og man ønsker å jobbe med metadata ved hjelp av kodespråk. Det kan man gjøre med Datadoc klassen i dapla-toolbelt-metadata pakken.\n\n\n\n\nDet primære brukstilfelle er å skape metadata for datasett skapt for nye tidsperioder som en del av produksjonsløp.\nI dette tilfelle er Datadoc brukergrensesnittet brukt for å skape det første metadatadokumentet. Man kan deretter bruke metadatadokumentet som en utgangspunkt for nye tidsperioder. Datadoc klassen utleder oppdatert informasjon fra filstien til datasettene for nye tidsperioder og oppdaterer metadataen til å stemme.\n\n\n\n\n\n\nNote\n\n\n\nDette forutsetter at strukturen av datasettet og betydningen av variablene er lik mellom tidsperiodene. Om det finnes uforventede avvik så kastes en InconsistentDatasetsError med detaljer om avviket.\n\n\n\n\n\n\nNotebook\n\nfrom dapla_metadata.datasets import Datadoc\n\nmeta = Datadoc(\n1    dataset_path=\"gs://ssb-staging-dapla-felles-data-delt/datadoc/sykefratot/klargjorte_data/person_testdata_p2022_v1.parquet\",\n2    metadata_document_path=\"gs://ssb-staging-dapla-felles-data-delt/datadoc/sykefratot/klargjorte_data/person_testdata_p2021_v1__DOC.json\",\n)\n3meta.write_metadata_document()\n\n\n1\n\nEn ny tidsperiode av et datasett\n\n2\n\nEksisterende metadatadokument\n\n3\n\nSkaper et nytt metadatadokument med filsti som stemmer med datasettet, i dette tilfellet gs://ssb-staging-dapla-felles-data-delt/datadoc/sykefratot/klargjorte_data/person_testdata_p2022_v1__DOC.json",
    "crumbs": [
      "Manual",
      "Metadata",
      "dapla-toolbelt-metadata"
    ]
  },
  {
    "objectID": "statistikkere/ordforklaringer.html",
    "href": "statistikkere/ordforklaringer.html",
    "title": "Ordforklaringer",
    "section": "",
    "text": "Ordforklaringer\n\nbip\nbip er det tidligere navnet på den underliggende plattformen som SSB bygger i GCP, hovedsakelig ment for utviklere som bygger tjenester på Dapla. Plattformen skulle være selvbetjent for utviklere og basert på DevOps-prinsipper. bip eksisterer fortsatt, men er nå blitt en del av det større begrepet dapla.\n\n\nbucket\nbucket (eller bøtte på norsk) er en lagringsenhet på Dapla. Det ligner litt på en klassisk diskstasjon, for eksempel X-disken eller C-disken på en lokal maskin. I en bøtte kan det ligge undermapper slik som i et klassisk filsystem.\n\n\nconsumer\nconsumer er en AD-gruppe som gir tilgang til et Dapla-team sin delt-bøtte. En SSB-ansatt som skal bruke data fra et Dapla-team må være medlem av consumer-gruppen til det aktuelle Dapla-teamet.\n\n\ndapla\nDapla er et akronym for den nye dataplattformen til SSB, der Da står for Data og pla står for Plattform. Dapla er en plattform for lagring, prosessering og deling av SSB sine data. Den består både av Jupyter-miljøet, som er et verktøy for å utføre beregninger og analysere data, og et eget område for lagre data. I tillegg inkluderer begrepet Dapla også en rekke andre verktøy som er nødvendige for å kunne bruke plattformen.\n\n\ndapla-team\nKommer snart.\n\n\ndapla-toolbelt\nKommer snart.\n\n\ndata-admin\ndata-admin er en AD-gruppe som gir de videste tilgangene i et dapla-team. En SSB-ansatt som har data-admin-rollen i et Dapla-team har tilgang til alle bøtter for det teamet, inkludert kilde-bøtta som kan inneha sensitive data.\nKommer snart.\n\n\ndapla-start\n*dapla-start** er et brukergrensesnitt der SSB-ansatte kan søke om å få opprettet et nytt dapla-team.\n\n\ndelt-bøtte\nKommer snart.\n\n\ndeveloper\nKommer snart.\n\n\nPersonidentifiserende Informasjon (PII)\nPII er variabler som kan identifisere en person i et datasett.\nMer informasjon finnes hos Datatilsynet.\n\n\ngoogle cloud platform (gcp)\nAllmenn skyplattform utviklet og levert av Google. Konkurrent med Amazon Web Services (AWS) og Microsoft Azure. Dapla primært benytter seg av tjenester på GCP.\nVideo som forklarer hva GCP er.\n\n\ngcp\nForkortelse for Google Cloud Platform. Se forklaring under google cloud platform (GCP).\n\n\nInfrastructure as Code (IaC)\nInfrastuktur som kode på norsk. Kode som defineres ressurser, typisk på en allmenn skyplatform som GCP. Eksempler av ressurser er bøtter, databaser, virtuelle maskiner, nettverk og sikkerhetsregler.\n\n\nkilde-bøtte\nKommer snart.\n\n\nprodukt-bøtte\nKommer snart.\n\n\nPull Request (PR)\nEn PR er en Github konsept, som gir et forum for kodegjennomgang, diskusjon og ikke minst dokumentasjon av kodeendringer.\nDette er anbefalt av KVAKK som måten å endre kode på i SSB.\n\n\nssb-project\nKommer snart.\n\n\ntransfer service\nKommer snart.\n\n\nPyflakes\nPyflakes er et enkelt kodeanalyseverktøy som finner feil i Python kode. Les mer om Pyflakes på deres PyPi side"
  },
  {
    "objectID": "statistikkere/altinn3.html",
    "href": "statistikkere/altinn3.html",
    "title": "Altinn 3",
    "section": "",
    "text": "Frem mot sommeren 2025 skal alle skjema-undersøkelser i SSB som gjennomføres på Altinn 2 flyttes over til Altinn 3. Skjemaer som flyttes til Altinn 3 vil motta sine data på Dapla, og ikke på bakken som tidligere. Datafangsten håndteres av Team SUV, mens statistikkseksjonene henter sine data fra Team SUV sitt lagringsområde på Dapla. I dette kapitlet beskriver vi nærmere hvordan statistikkseksjonene kan jobbe med Altinn3-data på Dapla. Kort oppsummert består det av disse stegene:\n\nStatistikkprodusenten avtaler overføring av skjema fra Altinn 2 til Altinn 3 med planleggere på S821, som koordinerer denne jobben.\nNår statistikkprodusentene får beskjed om at Altinn3-skjemaet skal sendes ut til oppgavegiverne, så må de opprette et Dapla-team.\nNår Dapla-teamet er opprettet, og første skjema er sendt inn, så ber de Team SUV om å gi statistikkteamet tilgang til dataene som har kommet inn fra Altinn 3. I tillegg ber de om at Team SUV gir tilgang til teamets Transfer Service instans. 1 Merk at det må gis separate tilganger for data i staging- og produksjonsmiljø.\nStatistikkprodusenten setter opp en automatisk overføring av skjemadata med Transfer Service, fra Team SUV sitt lagringsområde over til Dapla-teamet sin kildebøtte.\nStatistikkprodusentene kan begynne å jobbe med dataene i Dapla. Blant annet tilbyr Dapla en automatiseringstjeneste man kan bruke for å prosessere dataene fra kildedata til inndata2.\n\nUnder forklarer vi mer med mer detaljer hvordan man går frem for gjennomføre steg 4-5 over.\n\n\n\n\n\n\nAnsvar for kildedata\n\n\n\nSelv om Team SUV tar ansvaret for datafangst fra Altinn3, så er det statistikkteamet som har ansvaret for langtidslagring av dataene i sin kildebøtte. Det vil si at at statistikkteamet må sørge for at data overføres til sin kildebøtte, og at de kan ikke regne med at Team SUV tar vare på en backup av dataene.\n\n\n\n\nNår skjema-dataene kommer inn fra Altinn 3 til Team SUV sitt lagringsområde, så er det en del ting som er verdt å tenke på:\n\nHvordan er filene organisert hos Team SUV?\nFilstrukturen der Team SUV lagrer dataene som kommer inn fra Altinn 3, har en klar struktur. Du kan selv gå inn å kikke på dataene, enten fra Jupyter eller Google Cloud Console, og bli bedre kjent med strukturen3. Figur 1 viser en hvordan en typisk filsti ser ut på lagringsområdet til Team SUV. Det starter med navnet til bøtta som Team SUV har opprettet for skjemaet. Deretter viser den innvitteringsdato, deretter et teknisk navn, og til slutt selve skjemanavnet.\n\n\n\n\n\n\n\nFigur 1: Typisk filsti for et Altinn3-skjema.\n\n\n\n\nHvordan organisere dataene i din kildebøtte?\nNår vi bruker Transfer Service til å synkronisere innholdet i Team SUV sitt lagringsområde til Dapla-teamet sitt lagringsområde, så er det mest hensiktmessig å fortsette å bruke mapppe-strukturen som Team SUV har. Grunnen er at vi ikke kan legge på noe logikk som lager en ny struktur. Tjenesten bare kopierer over data. Men du kan kan lage et nytt toppnivå-mappe som du ønsker at dataene skal synkroniseres til. F.eks. at alle Altinn-dataene legger seg inn i en mappe som heter altinn. Det er spesielt nyttig hvis du har flere datakilder som skal ligge i samme kildebøtte.\nUnike skjemanavn\nSkjemanavnet du ser i Figur 1 er unike. Dvs. at hvis en oppgavegiver sender inn flere skjemaer på samme dag, så er fortsatt skjemanavnet unikt. Det er viktig å være klar over når man bruker automatiseringstjenesten for kildedata senere. Siden tjenesten trigges per fil, og man er avhengig av unike navn for å ikke skrive over filer, så er det nyttig å vite at man kan videreføre skjemanavnet i overgangen fra kildedata til inndata.\n\n\n\n\nNår vi skal overføre filer fra Team SUV sin bøtte til vår kildebøtte, så kan vi gjøre det manuelt fra Jupyter som forklart her.. Men det er en bedre løsning å bruke en tjeneste som gjør dette for deg. Transfer Service er en tjeneste som kan brukes til å synkronisere innholdet mellom bøtter på Dapla, samt mellom bakke og sky. Når du skal ta i bruk tjenesten for å overføre data mellom en bøtte fra Team SUV sitt prosjekt altinn-data-prod, til en kildedata-bøtte i Dapla-teamet ditt, så gjør du følgende:\n\nFølg denne beskrivelsen hvordan man setter opp overføringsjobber.\nEtter at du har trykket på Create Transfer Job velger du Google Cloud Storage på begge alternativene under Get Started. Deretter går du videre ved å klikke på Next Step.\nUnder Choose a source så skal du velge hvor du skal kopiere data fra. Trykk på Browse. I vinduet som dukker opp trykker du på 🔻-ikonet ved siden av Project ID. I søkevinduet som dukker opp søker du opp altinn-data-prod og trykker på navnet. Da får du listet opp alle bøttene i altinn-data-prod prosjektet. Til slutt trykker du på bøtta som Team SUV har opprettet for undersøkelsen4 og klikker Select til nederst på siden. Trykk deretter Next step for å gå videre.\nUnder Choose a destination så skal du velge hvor du skal kopiere data til. Dette steget er ganske likt som forrige, men du skal nå velge ditt eget projekt og kildebøtta der. Trykk på Browse. I vinduet som dukker opp trykker du på 🔻-ikonet ved siden av Project ID. I søkevinduet som dukker opp søker du opp prod-&lt;ditt teamnavn&gt; og trykker på navnet. Da får du listet opp alle bøttene i ditt team sitt prosjekt. Velg kildebøtta som har navnet ssb-prod-&lt;teamnavn&gt;-data-kilde. Hvis du ønsker å kopiere data til en undermappe i bøtta, så trykker du på &gt;-ikonet ved bøttenavnet og velger ønsket undermappe5. Til slutt trykker du på Select til nederst på siden. Trykk deretter Next step for å gå videre.\nUnder Choose how and when to run job velger du Run with custom frequency og Starting now i Batch-modus. Hvis du ønsker å overføre så ofte som mulig, så velger du Custom frequency og Repeat every 1 Hours. Til slutt trykker du Next Step nederst på siden.\nUnder Choose Settings så legger du til en kort beskrivelse av jobben du har opprettet. Under de andre valgene kan du gjøre følgende:\n\nUnder Advanced transfer Options trenger du ikke gjøre noen endringer.\nUnder When to overwrite kan du velge If different.\nUnder When to delete kan du velge Never.\nUnder Manifest trenger du ikke huke av.\nUnder Logging options velger du samme som vist i Figur 2.\n\n\n\n\n\n\n\n\nFigur 2: Valg av opsjoner for logging i Transfer Service\n\n\n\nTil slutt trykker du Create for å aktivere tjenesten. Den vil da sjekke Team SUV sin bøtte hver time og kopiere over alle filer som ikke allerede eksisterer statistikkteamets kildebøtte.\n\n\n\nNår du har satt opp Transfer Service til å kopiere over filer fra Team SUV sin bøtte til statistikkteamets kildebøtte, så vil det potensielt komme inn nye skjemaer hver time. Disse kan du lese inn i Jupyter og jobbe med hvis du er kildedata-ansvarlig i teamet. Hvis du ikke er det så må du vente til dataene er tilgjengeliggjort i produkt-bøtta til teamet.\nSiden få personer innehar rollen som kildedata-ansvarlig så er det laget en automatiseringstjeneste som kan bearbeide alle filer som kommer inn i kildebøtta. Den lar deg kjøre et python-script på alle filer som kommer inn i kildebøtta.\nLes mer om hvordan du kan bruker tjenesten her.\n\n\n\nI denne delen deles noen tips og triks for å jobbe med Altinn3-dataene på Dapla. Fokuset vil være på hvordan du kan lese inn og transformere xml-filer fra skjema-dataene.\n\n\nFor å se innholdet i en mappe gir det mest mening å bruke Google Cloud Console. Her kan du se både filer og mapper i et pek-og-klikk grensesnitt. Men du kan ikke se på innholdet i filene der. Til det må du bruke Jupyter.\nAnta at vi ønsker å liste ut alle som leverte skjema den 10. mars 2023. Da kan vi bruke gcsfs til å gjøre det6:\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/**.xml\")\n\n# Print the list of parquet files\nxml_files\nHer bruker vi fs.glob()-funksjonen fra gcsfs til å loope gjennom alle undermapper av gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/, og legge alle filer som slutter med .xml til listen xml_files. Dermed har vi et Python-objekt som kun brukes til å hente inn de filene vi ønsker. Vi bruker da det vi vet om mappestrukturen i Team SUV sin bøtte som vi så tidligere i Figur 1.\n\n\n\nNoen ganger kan det være nyttig å se en xml-fil direkte. Da kan vi lese den inn i Jupyter og printe den ut. Her er et eksempel på hvordan vi kan gjøre det, samtidig som vi formatterer den med xml.dom.minidom:\nfrom xml.dom.minidom import parseString\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\n# Sett inn filstien din her\nfile = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/form_dc551844cd74.xml\"\n\ndom = parseString(fs.cat_file(file))\npretty_xml = dom.toprettyxml(indent=\"  \")\nprint(pretty_xml)\nVi brukte da fs.cat_file() til å hente inn innholdet i mappen, gjorde det om til xml-object, og formatterer outputen med xml.dom.minidom. Resultatet vil da se ut noe sånt som dette:\n&lt;?xml version=\"1.0\" ?&gt;\n&lt;melding xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" dataFormatProvider=\"SERES\" dataFormatId=\"7079\" dataFormatVersion=\"47315\"&gt;\n  &lt;InternInfo&gt;\n    &lt;raNummer&gt;RA-XXXX&lt;/raNummer&gt;\n    &lt;skjemaVersjon&gt;1.0&lt;/skjemaVersjon&gt;\n    &lt;undersoekelsesNr&gt;XXXXX&lt;/undersoekelsesNr&gt;\n    &lt;visOppgaveByrde&gt;0&lt;/visOppgaveByrde&gt;\n    &lt;visBrukeropplevelse&gt;0&lt;/visBrukeropplevelse&gt;\n    &lt;delregNr&gt;2XXXXX&lt;/delregNr&gt;\n    &lt;periodeFritekst&gt;3. kvartal 2022&lt;/periodeFritekst&gt;\n    &lt;periodeFomDato&gt;2022-07-01&lt;/periodeFomDato&gt;\n    &lt;periodeTomDato&gt;2022-09-30&lt;/periodeTomDato&gt;\n    &lt;periodeType&gt;KVRT&lt;/periodeType&gt;\n    &lt;periodeNummer&gt;3&lt;/periodeNummer&gt;\n    &lt;periodeAAr&gt;2022&lt;/periodeAAr&gt;\n    &lt;periodeDato&gt; &lt;/periodeDato&gt;\n    &lt;enhetsIdent&gt;XXXXXX&lt;/enhetsIdent&gt;\n    &lt;enhetsType&gt;BEDR&lt;/enhetsType&gt;\n    &lt;enhetsOrgNr&gt;XXXXXXXXX&lt;/enhetsOrgNr&gt;\n    &lt;enhetsNavn&gt;STATISTISK SENTRALBYRÅ &lt;/enhetsNavn&gt;\n    &lt;enhetsGateadresse&gt;Akersveien 26&lt;/enhetsGateadresse&gt;\n    &lt;enhetsPostnr&gt;0177&lt;/enhetsPostnr&gt;\n    &lt;enhetsPoststed&gt;OSLO&lt;/enhetsPoststed&gt;\n    &lt;enhetsAvdeling&gt;AVD XXXX&lt;/enhetsAvdeling&gt;\n    &lt;reporteeOrgNr&gt;XXXXXXXXX&lt;/reporteeOrgNr&gt;\n  &lt;/InternInfo&gt;\n  &lt;Kontakt&gt;\n    &lt;kontaktPersonNavn&gt;OLA NORDMANN&lt;/kontaktPersonNavn&gt;\n    &lt;kontaktPersonEpost&gt;ola@tull.no&lt;/kontaktPersonEpost&gt;\n    &lt;kontaktPersonTelefon&gt; &lt;/kontaktPersonTelefon&gt;\n    &lt;kontaktInfoBekreftet&gt;1&lt;/kontaktInfoBekreftet&gt;\n    &lt;kontaktInfoKommentar&gt;Dette er en et eksempel for Dapla-manualen&lt;/kontaktInfoKommentar&gt;\n  &lt;/Kontakt&gt;\n  &lt;Skjemadata&gt;\n    &lt;ledigeStillinger&gt;75&lt;/ledigeStillinger&gt;\n    &lt;datoPrefill&gt; &lt;/datoPrefill&gt;\n  &lt;/Skjemadata&gt;\n&lt;/melding&gt;\n\n\n\nDe færreste ønsker å jobbe direkte med XML-filer. Derfor er det nyttig å kunne transformere XML-filene til et mer brukervennlig format, f.eks. en Pandas Dataframe. Her er et eksempel på hvordan vi kan gjøre det med Pandas:\n\nimport xml.etree.ElementTree as ET\nimport pandas as pd\nfrom dapla import FileClient\n\ndef single_xml_to_dataframe(file_path: str) -&gt; pd.DataFrame:\n    fs = FileClient.get_gcs_file_system()\n\n    with fs.open(file_path, mode=\"r\") as f:\n        single_xml = f.read()\n\n    root = ET.fromstring(single_xml)\n    intern_info = root.find(\"InternInfo\")\n    kontakt = root.find(\"Kontakt\")\n    skjemadata = root.find(\"Skjemadata\")\n\n    data = []\n    all_tags = set()\n\n    for element in intern_info:\n        all_tags.add(element.tag)\n\n    for element in kontakt:\n        all_tags.add(element.tag)\n\n    for element in skjemadata:\n        all_tags.add(element.tag)\n\n    for tag in all_tags:\n        element = intern_info.find(tag)\n        if element is None:\n            element = kontakt.find(tag)\n        if element is None:\n            element = skjemadata.find(tag)\n        if element is not None:\n            value = element.text\n            data.append(value)\n        else:\n            data.append(None)\n\n    # Include the full path to the XML file in the data list\n    data.append(file_path)\n\n    # Create the DataFrame inside the function\n    df = pd.DataFrame([data], columns=list(all_tags) + [\"pathToXmlFile\"])\n    path_to_xml_file_split = df[\"pathToXmlFile\"].str.split(\"/\")\n    return df\n\n# Run the function\nfile_path = \"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/24/28c34dce4232_e7bed8a4-def5-42dc-b1e2-b9fc66beb769/form_28c34dce4232.xml\"\ndf = single_xml_to_dataframe(file_path=file_path)\nI funksjonen så søker vi etter alle elementer under taggene intern_info, kontakt og skjemadata. Dvs. at vi lager en dataframe med en rad, der vi tar med all data fra xml-filen. Til slutt legger vi til en kolonne med filstien til filen. Det kan være nyttig senere hvis man gå tilbake til xml-filen for å sjekke om konverteringen til en dataframe er riktig.\nFunksjonen single_xml_to_dataframe() kan brukes til å loope over en liste med filstier (vi viste hvordan man lager en slik liste tidligere i kapitlet). Men et typisk problem som vil oppstå da er at xml-filene har forskjellig antall elementer i seg, som gir dataframes med ulikt antall kolonner, og derfor vil ikke Pandas vite hvordan man konkatinerer disse. For å fikse dette må du modifisere funksjonen til å ta høyde for dette.\n\n\n\nHvis vi ønsker å kopiere filer manuelt (dvs. ikke med Transfer Service) fra Team SUV sine bøtter til egen kildebøtte, kan vi gjøre det fra Jupyter. Vi har tidligere sett hvordan vi kan kopiere enkeltfiler mellom to bøtter. Under viser vi hvordan man kan kopiere alle filer under en viss filsti med kommandoen fs.copy():\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfrom_path = \"gs://ssb-prod-arbmark-skjema-data-kilde/ledstill/altinn/2022/11/21/\"\nto_path = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/\"\nfs.copy(from_path, to_path, recursive=True)\nI koden over så kopierer vi alle filer under from_path til to_path. Vi bruker recursive=True for å sørge for at vi kopierer alle filer under from_path.\nI eksempelet over så kopierer vi over alt som er from_path og dets undermapper. I tilfellet med Altinn-data så ligger det også pdf-filer av skjemaet som kanskje ikke ønsker å kopiere. I de tilfellene kan vi først søke etter de filene vi ønsker å kopiere og legge de i en liste. Deretter kan vi kopiere over en og en fil fra listen. Denne tilnærmingen er veldig nyttig hvis vi ønsker å filtrere ut filer som ikke er XML-filer, eller vi ønsker en annen mappestruktur en den som ligger i from_path. Her er en måte vi kan gjøre det på:\nfrom dapla import FileClient\n\n# Lager en filsystem-instans av GCS\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/**.xml\")\n\n# Stien du ønsker å kopiere til.\n# Koden under foutsetter at du har med gs:// først\nto_folder = \"gs://ssb-dapla-felles-data-delt-prod/\"\n\n# Kopierer over filene\nfor file in xml_files:\n    from_bucket = file.split(\"/\")[0]\n    to_bucket = to_folder.split(\"/\")[2]\n    to_path = file.replace(\n        from_bucket, to_bucket\n    )\n    fs.cp(file, to_path)\nI koden over så bruker vi fs.glob() og ** til å søke rekursivt etter alle xml-filer under filstien gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10. Deretter kopierer vi over filene til egen kildebøtte med fs.cp(). Når vi skal kopiere over til en ny bøtte må vi bestemme oss for hvor filene skal ligge og hvor mye av den gamle filstien vi skal beholde. Anta at vi skal beholde hele mappestrukturen i Team SUV sin bøtte. Da kan vi egentlig bare erstatte ra0678-01-altinn-data-prod-e17d-ssb-altinn/ med den nye bøtte-navnet, og vi vil få den samme strukturen som i Team SUV sin bøtte.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn3.html#forberedelse",
    "href": "statistikkere/altinn3.html#forberedelse",
    "title": "Altinn 3",
    "section": "",
    "text": "Når skjema-dataene kommer inn fra Altinn 3 til Team SUV sitt lagringsområde, så er det en del ting som er verdt å tenke på:\n\nHvordan er filene organisert hos Team SUV?\nFilstrukturen der Team SUV lagrer dataene som kommer inn fra Altinn 3, har en klar struktur. Du kan selv gå inn å kikke på dataene, enten fra Jupyter eller Google Cloud Console, og bli bedre kjent med strukturen3. Figur 1 viser en hvordan en typisk filsti ser ut på lagringsområdet til Team SUV. Det starter med navnet til bøtta som Team SUV har opprettet for skjemaet. Deretter viser den innvitteringsdato, deretter et teknisk navn, og til slutt selve skjemanavnet.\n\n\n\n\n\n\n\nFigur 1: Typisk filsti for et Altinn3-skjema.\n\n\n\n\nHvordan organisere dataene i din kildebøtte?\nNår vi bruker Transfer Service til å synkronisere innholdet i Team SUV sitt lagringsområde til Dapla-teamet sitt lagringsområde, så er det mest hensiktmessig å fortsette å bruke mapppe-strukturen som Team SUV har. Grunnen er at vi ikke kan legge på noe logikk som lager en ny struktur. Tjenesten bare kopierer over data. Men du kan kan lage et nytt toppnivå-mappe som du ønsker at dataene skal synkroniseres til. F.eks. at alle Altinn-dataene legger seg inn i en mappe som heter altinn. Det er spesielt nyttig hvis du har flere datakilder som skal ligge i samme kildebøtte.\nUnike skjemanavn\nSkjemanavnet du ser i Figur 1 er unike. Dvs. at hvis en oppgavegiver sender inn flere skjemaer på samme dag, så er fortsatt skjemanavnet unikt. Det er viktig å være klar over når man bruker automatiseringstjenesten for kildedata senere. Siden tjenesten trigges per fil, og man er avhengig av unike navn for å ikke skrive over filer, så er det nyttig å vite at man kan videreføre skjemanavnet i overgangen fra kildedata til inndata.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn3.html#transfer-service",
    "href": "statistikkere/altinn3.html#transfer-service",
    "title": "Altinn 3",
    "section": "",
    "text": "Når vi skal overføre filer fra Team SUV sin bøtte til vår kildebøtte, så kan vi gjøre det manuelt fra Jupyter som forklart her.. Men det er en bedre løsning å bruke en tjeneste som gjør dette for deg. Transfer Service er en tjeneste som kan brukes til å synkronisere innholdet mellom bøtter på Dapla, samt mellom bakke og sky. Når du skal ta i bruk tjenesten for å overføre data mellom en bøtte fra Team SUV sitt prosjekt altinn-data-prod, til en kildedata-bøtte i Dapla-teamet ditt, så gjør du følgende:\n\nFølg denne beskrivelsen hvordan man setter opp overføringsjobber.\nEtter at du har trykket på Create Transfer Job velger du Google Cloud Storage på begge alternativene under Get Started. Deretter går du videre ved å klikke på Next Step.\nUnder Choose a source så skal du velge hvor du skal kopiere data fra. Trykk på Browse. I vinduet som dukker opp trykker du på 🔻-ikonet ved siden av Project ID. I søkevinduet som dukker opp søker du opp altinn-data-prod og trykker på navnet. Da får du listet opp alle bøttene i altinn-data-prod prosjektet. Til slutt trykker du på bøtta som Team SUV har opprettet for undersøkelsen4 og klikker Select til nederst på siden. Trykk deretter Next step for å gå videre.\nUnder Choose a destination så skal du velge hvor du skal kopiere data til. Dette steget er ganske likt som forrige, men du skal nå velge ditt eget projekt og kildebøtta der. Trykk på Browse. I vinduet som dukker opp trykker du på 🔻-ikonet ved siden av Project ID. I søkevinduet som dukker opp søker du opp prod-&lt;ditt teamnavn&gt; og trykker på navnet. Da får du listet opp alle bøttene i ditt team sitt prosjekt. Velg kildebøtta som har navnet ssb-prod-&lt;teamnavn&gt;-data-kilde. Hvis du ønsker å kopiere data til en undermappe i bøtta, så trykker du på &gt;-ikonet ved bøttenavnet og velger ønsket undermappe5. Til slutt trykker du på Select til nederst på siden. Trykk deretter Next step for å gå videre.\nUnder Choose how and when to run job velger du Run with custom frequency og Starting now i Batch-modus. Hvis du ønsker å overføre så ofte som mulig, så velger du Custom frequency og Repeat every 1 Hours. Til slutt trykker du Next Step nederst på siden.\nUnder Choose Settings så legger du til en kort beskrivelse av jobben du har opprettet. Under de andre valgene kan du gjøre følgende:\n\nUnder Advanced transfer Options trenger du ikke gjøre noen endringer.\nUnder When to overwrite kan du velge If different.\nUnder When to delete kan du velge Never.\nUnder Manifest trenger du ikke huke av.\nUnder Logging options velger du samme som vist i Figur 2.\n\n\n\n\n\n\n\n\nFigur 2: Valg av opsjoner for logging i Transfer Service\n\n\n\nTil slutt trykker du Create for å aktivere tjenesten. Den vil da sjekke Team SUV sin bøtte hver time og kopiere over alle filer som ikke allerede eksisterer statistikkteamets kildebøtte.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn3.html#automatiseringstjeneste-for-kildedata",
    "href": "statistikkere/altinn3.html#automatiseringstjeneste-for-kildedata",
    "title": "Altinn 3",
    "section": "",
    "text": "Når du har satt opp Transfer Service til å kopiere over filer fra Team SUV sin bøtte til statistikkteamets kildebøtte, så vil det potensielt komme inn nye skjemaer hver time. Disse kan du lese inn i Jupyter og jobbe med hvis du er kildedata-ansvarlig i teamet. Hvis du ikke er det så må du vente til dataene er tilgjengeliggjort i produkt-bøtta til teamet.\nSiden få personer innehar rollen som kildedata-ansvarlig så er det laget en automatiseringstjeneste som kan bearbeide alle filer som kommer inn i kildebøtta. Den lar deg kjøre et python-script på alle filer som kommer inn i kildebøtta.\nLes mer om hvordan du kan bruker tjenesten her.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn3.html#tips-og-triks",
    "href": "statistikkere/altinn3.html#tips-og-triks",
    "title": "Altinn 3",
    "section": "",
    "text": "I denne delen deles noen tips og triks for å jobbe med Altinn3-dataene på Dapla. Fokuset vil være på hvordan du kan lese inn og transformere xml-filer fra skjema-dataene.\n\n\nFor å se innholdet i en mappe gir det mest mening å bruke Google Cloud Console. Her kan du se både filer og mapper i et pek-og-klikk grensesnitt. Men du kan ikke se på innholdet i filene der. Til det må du bruke Jupyter.\nAnta at vi ønsker å liste ut alle som leverte skjema den 10. mars 2023. Da kan vi bruke gcsfs til å gjøre det6:\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/**.xml\")\n\n# Print the list of parquet files\nxml_files\nHer bruker vi fs.glob()-funksjonen fra gcsfs til å loope gjennom alle undermapper av gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/, og legge alle filer som slutter med .xml til listen xml_files. Dermed har vi et Python-objekt som kun brukes til å hente inn de filene vi ønsker. Vi bruker da det vi vet om mappestrukturen i Team SUV sin bøtte som vi så tidligere i Figur 1.\n\n\n\nNoen ganger kan det være nyttig å se en xml-fil direkte. Da kan vi lese den inn i Jupyter og printe den ut. Her er et eksempel på hvordan vi kan gjøre det, samtidig som vi formatterer den med xml.dom.minidom:\nfrom xml.dom.minidom import parseString\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\n# Sett inn filstien din her\nfile = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/form_dc551844cd74.xml\"\n\ndom = parseString(fs.cat_file(file))\npretty_xml = dom.toprettyxml(indent=\"  \")\nprint(pretty_xml)\nVi brukte da fs.cat_file() til å hente inn innholdet i mappen, gjorde det om til xml-object, og formatterer outputen med xml.dom.minidom. Resultatet vil da se ut noe sånt som dette:\n&lt;?xml version=\"1.0\" ?&gt;\n&lt;melding xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" dataFormatProvider=\"SERES\" dataFormatId=\"7079\" dataFormatVersion=\"47315\"&gt;\n  &lt;InternInfo&gt;\n    &lt;raNummer&gt;RA-XXXX&lt;/raNummer&gt;\n    &lt;skjemaVersjon&gt;1.0&lt;/skjemaVersjon&gt;\n    &lt;undersoekelsesNr&gt;XXXXX&lt;/undersoekelsesNr&gt;\n    &lt;visOppgaveByrde&gt;0&lt;/visOppgaveByrde&gt;\n    &lt;visBrukeropplevelse&gt;0&lt;/visBrukeropplevelse&gt;\n    &lt;delregNr&gt;2XXXXX&lt;/delregNr&gt;\n    &lt;periodeFritekst&gt;3. kvartal 2022&lt;/periodeFritekst&gt;\n    &lt;periodeFomDato&gt;2022-07-01&lt;/periodeFomDato&gt;\n    &lt;periodeTomDato&gt;2022-09-30&lt;/periodeTomDato&gt;\n    &lt;periodeType&gt;KVRT&lt;/periodeType&gt;\n    &lt;periodeNummer&gt;3&lt;/periodeNummer&gt;\n    &lt;periodeAAr&gt;2022&lt;/periodeAAr&gt;\n    &lt;periodeDato&gt; &lt;/periodeDato&gt;\n    &lt;enhetsIdent&gt;XXXXXX&lt;/enhetsIdent&gt;\n    &lt;enhetsType&gt;BEDR&lt;/enhetsType&gt;\n    &lt;enhetsOrgNr&gt;XXXXXXXXX&lt;/enhetsOrgNr&gt;\n    &lt;enhetsNavn&gt;STATISTISK SENTRALBYRÅ &lt;/enhetsNavn&gt;\n    &lt;enhetsGateadresse&gt;Akersveien 26&lt;/enhetsGateadresse&gt;\n    &lt;enhetsPostnr&gt;0177&lt;/enhetsPostnr&gt;\n    &lt;enhetsPoststed&gt;OSLO&lt;/enhetsPoststed&gt;\n    &lt;enhetsAvdeling&gt;AVD XXXX&lt;/enhetsAvdeling&gt;\n    &lt;reporteeOrgNr&gt;XXXXXXXXX&lt;/reporteeOrgNr&gt;\n  &lt;/InternInfo&gt;\n  &lt;Kontakt&gt;\n    &lt;kontaktPersonNavn&gt;OLA NORDMANN&lt;/kontaktPersonNavn&gt;\n    &lt;kontaktPersonEpost&gt;ola@tull.no&lt;/kontaktPersonEpost&gt;\n    &lt;kontaktPersonTelefon&gt; &lt;/kontaktPersonTelefon&gt;\n    &lt;kontaktInfoBekreftet&gt;1&lt;/kontaktInfoBekreftet&gt;\n    &lt;kontaktInfoKommentar&gt;Dette er en et eksempel for Dapla-manualen&lt;/kontaktInfoKommentar&gt;\n  &lt;/Kontakt&gt;\n  &lt;Skjemadata&gt;\n    &lt;ledigeStillinger&gt;75&lt;/ledigeStillinger&gt;\n    &lt;datoPrefill&gt; &lt;/datoPrefill&gt;\n  &lt;/Skjemadata&gt;\n&lt;/melding&gt;\n\n\n\nDe færreste ønsker å jobbe direkte med XML-filer. Derfor er det nyttig å kunne transformere XML-filene til et mer brukervennlig format, f.eks. en Pandas Dataframe. Her er et eksempel på hvordan vi kan gjøre det med Pandas:\n\nimport xml.etree.ElementTree as ET\nimport pandas as pd\nfrom dapla import FileClient\n\ndef single_xml_to_dataframe(file_path: str) -&gt; pd.DataFrame:\n    fs = FileClient.get_gcs_file_system()\n\n    with fs.open(file_path, mode=\"r\") as f:\n        single_xml = f.read()\n\n    root = ET.fromstring(single_xml)\n    intern_info = root.find(\"InternInfo\")\n    kontakt = root.find(\"Kontakt\")\n    skjemadata = root.find(\"Skjemadata\")\n\n    data = []\n    all_tags = set()\n\n    for element in intern_info:\n        all_tags.add(element.tag)\n\n    for element in kontakt:\n        all_tags.add(element.tag)\n\n    for element in skjemadata:\n        all_tags.add(element.tag)\n\n    for tag in all_tags:\n        element = intern_info.find(tag)\n        if element is None:\n            element = kontakt.find(tag)\n        if element is None:\n            element = skjemadata.find(tag)\n        if element is not None:\n            value = element.text\n            data.append(value)\n        else:\n            data.append(None)\n\n    # Include the full path to the XML file in the data list\n    data.append(file_path)\n\n    # Create the DataFrame inside the function\n    df = pd.DataFrame([data], columns=list(all_tags) + [\"pathToXmlFile\"])\n    path_to_xml_file_split = df[\"pathToXmlFile\"].str.split(\"/\")\n    return df\n\n# Run the function\nfile_path = \"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/24/28c34dce4232_e7bed8a4-def5-42dc-b1e2-b9fc66beb769/form_28c34dce4232.xml\"\ndf = single_xml_to_dataframe(file_path=file_path)\nI funksjonen så søker vi etter alle elementer under taggene intern_info, kontakt og skjemadata. Dvs. at vi lager en dataframe med en rad, der vi tar med all data fra xml-filen. Til slutt legger vi til en kolonne med filstien til filen. Det kan være nyttig senere hvis man gå tilbake til xml-filen for å sjekke om konverteringen til en dataframe er riktig.\nFunksjonen single_xml_to_dataframe() kan brukes til å loope over en liste med filstier (vi viste hvordan man lager en slik liste tidligere i kapitlet). Men et typisk problem som vil oppstå da er at xml-filene har forskjellig antall elementer i seg, som gir dataframes med ulikt antall kolonner, og derfor vil ikke Pandas vite hvordan man konkatinerer disse. For å fikse dette må du modifisere funksjonen til å ta høyde for dette.\n\n\n\nHvis vi ønsker å kopiere filer manuelt (dvs. ikke med Transfer Service) fra Team SUV sine bøtter til egen kildebøtte, kan vi gjøre det fra Jupyter. Vi har tidligere sett hvordan vi kan kopiere enkeltfiler mellom to bøtter. Under viser vi hvordan man kan kopiere alle filer under en viss filsti med kommandoen fs.copy():\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfrom_path = \"gs://ssb-prod-arbmark-skjema-data-kilde/ledstill/altinn/2022/11/21/\"\nto_path = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/\"\nfs.copy(from_path, to_path, recursive=True)\nI koden over så kopierer vi alle filer under from_path til to_path. Vi bruker recursive=True for å sørge for at vi kopierer alle filer under from_path.\nI eksempelet over så kopierer vi over alt som er from_path og dets undermapper. I tilfellet med Altinn-data så ligger det også pdf-filer av skjemaet som kanskje ikke ønsker å kopiere. I de tilfellene kan vi først søke etter de filene vi ønsker å kopiere og legge de i en liste. Deretter kan vi kopiere over en og en fil fra listen. Denne tilnærmingen er veldig nyttig hvis vi ønsker å filtrere ut filer som ikke er XML-filer, eller vi ønsker en annen mappestruktur en den som ligger i from_path. Her er en måte vi kan gjøre det på:\nfrom dapla import FileClient\n\n# Lager en filsystem-instans av GCS\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/**.xml\")\n\n# Stien du ønsker å kopiere til.\n# Koden under foutsetter at du har med gs:// først\nto_folder = \"gs://ssb-dapla-felles-data-delt-prod/\"\n\n# Kopierer over filene\nfor file in xml_files:\n    from_bucket = file.split(\"/\")[0]\n    to_bucket = to_folder.split(\"/\")[2]\n    to_path = file.replace(\n        from_bucket, to_bucket\n    )\n    fs.cp(file, to_path)\nI koden over så bruker vi fs.glob() og ** til å søke rekursivt etter alle xml-filer under filstien gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10. Deretter kopierer vi over filene til egen kildebøtte med fs.cp(). Når vi skal kopiere over til en ny bøtte må vi bestemme oss for hvor filene skal ligge og hvor mye av den gamle filstien vi skal beholde. Anta at vi skal beholde hele mappestrukturen i Team SUV sin bøtte. Da kan vi egentlig bare erstatte ra0678-01-altinn-data-prod-e17d-ssb-altinn/ med den nye bøtte-navnet, og vi vil få den samme strukturen som i Team SUV sin bøtte.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn3.html#footnotes",
    "href": "statistikkere/altinn3.html#footnotes",
    "title": "Altinn 3",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nForslag til e-post til Team SUV etter at teamet er opprettet:\nVi har opprettet et Dapla-tema som heter &lt;ditt teamnavn&gt; for å jobbe med skjema &lt;RA-XXXX&gt;. Kan dere gi oss tilgang til riktig lagringsområde og også gi vår Transfer Service lesetilgang.↩︎\nEn typisk prosessering som de fleste vil ønske å gjøre er å konvertere fra xml-formatet det kom på, og over til parquet-formatet.↩︎\nDu kan gå inn i Google Cloud Console og søke opp prosjektet til Team SUV som de bruker for å dele data. Det heter altinn-data-prod, og du finner bøttene ved å klikke deg inn på Cloud Storage↩︎\nBøttenavnet starter alltid med RA-nummeret til undersøkelsen.↩︎\nAlternativt oppretter du en mappe direkte vinduet ved å trykke på mappe-ikonet med en +-tegn i seg.↩︎\nFor å jobbe mot datat i GCS som i et “vanlig” filsysten kan vi bruke FileClient.get_gcs_file_system() fra dapla-toolbelt.↩︎",
    "crumbs": [
      "Manual",
      "Appendix",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/arkivering.html",
    "href": "statistikkere/arkivering.html",
    "title": "Arkivering",
    "section": "",
    "text": "Alle som flytter produksjon til Dapla må fortsatt arkivere dataene i bakkemiljøet. Grunnen til dette er at det enda ikke er bestemt hvordan arkivering skal foregå på Dapla. Inntill videre må derfor statistikkteam arkivere i de gamle systemene.\n\n\nFør man kan arkivere data må det skrives ut slik det er definert i Datadok. Mens man tidligere gjorde dette fra SAS, skal man på Dapla gjøre det fra R eller Python.\n\n\n\nEtter at filen er skrevet må den flyttes fra Dapla til bakkemiljøet, og til slutt inn i riktig arkiv-mappe. Overføring av filer mellom bakke og sky gjøres med Transfer Service. Når filen er flyttet til bakkemiljøet, må brukeren selv flytte filen til arkiv-mappen. Ønsker man å automatisere flyttingen, så kan man sende en forespørsel til Kundeservice.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Arkivering"
    ]
  },
  {
    "objectID": "statistikkere/arkivering.html#skrive-fil",
    "href": "statistikkere/arkivering.html#skrive-fil",
    "title": "Arkivering",
    "section": "",
    "text": "Før man kan arkivere data må det skrives ut slik det er definert i Datadok. Mens man tidligere gjorde dette fra SAS, skal man på Dapla gjøre det fra R eller Python.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Arkivering"
    ]
  },
  {
    "objectID": "statistikkere/arkivering.html#overføre-fil",
    "href": "statistikkere/arkivering.html#overføre-fil",
    "title": "Arkivering",
    "section": "",
    "text": "Etter at filen er skrevet må den flyttes fra Dapla til bakkemiljøet, og til slutt inn i riktig arkiv-mappe. Overføring av filer mellom bakke og sky gjøres med Transfer Service. Når filen er flyttet til bakkemiljøet, må brukeren selv flytte filen til arkiv-mappen. Ønsker man å automatisere flyttingen, så kan man sende en forespørsel til Kundeservice.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Arkivering"
    ]
  },
  {
    "objectID": "statistikkere/innlogging.html",
    "href": "statistikkere/innlogging.html",
    "title": "Innlogging",
    "section": "",
    "text": "Innlogging på Dapla er veldig enkelt. Dapla er en nettadresse som alle SSB-ere kan gå inn på hvis de er logget på SSB sitt nettverk. Å være logget på SSB sitt nettverk betyr i denne sammenhengen at man er logget på med VPN, enten man er på kontoret eller på hjemmekontor. For å gjøre det enda enklere har vi laget en fast snarvei til denne nettadressen på vårt intranett/Byrånettet(se Figur 1).\n\n\n\n\n\n\nFigur 1: Snarvei til Dapla fra intranett\n\n\n\nMen samtidig som det er lett å logge seg på, så er det noen kompliserende ting som fortjener en forklaring. Noe skyldes at vi mangler et klart språk for å definere bakkemiljøet og skymiljøet slik at alle skjønner hva man snakker om. I denne boken definerer bakkemiljøet som stedet der man har drevet med statistikkproduksjon de siste tiårene. Skymiljøet er den nye dataplattformen Dapla på Google Cloud.\nDet som gjør ting litt komplisert er at vi har 2 Jupyter-miljøer på både bakke og sky. Årsaken er at vi har ett test- og ett prod-område for hver, og det blir i alt 4 Jupyter-miljøer. Figur 2 viser dette.\n\n\n\n\n\n\nFigur 2: De 4 Jupyter-miljøene i SSB. Et test-miljø og et prod-miljø på bakke og sky/Dapla\n\n\n\nHver av disse miljøene har sin egen nettadresse og sitt eget bruksområde.\n\n\nI de fleste tilfeller vil en statistikker eller forsker ønske å logge seg inn i prod-miljøet. Det er her man skal kjøre koden sin i et produksjonsløp som skal publiseres eller utvikles. I noen tilfeller hvor man ber om å få tilgjengliggjort en ny tjeneste så vil denne først rulles ut i testområdet som vi kaller staging-området. Årsaken er at vi ønsker å beskytte prod-miljøet fra software som potensielt ødelegger for eksisterende funksjonalitet. Derfor ruller vi ut nye ting i staging først. Av den grunn vil de fleste oppleve å bli bedt om å logge seg inn der for testing en eller annen gang. Under forklarer vi hvordan man går frem for å logge seg på de to ulike miljøene på Dapla.\n\n\nFor å logge seg inn inn i prod-miljøet på Dapla kan man gjøre følgende:\n\nGå inn på lenken https://jupyter.dapla.ssb.no/ i en Chrome-nettleser eller klikk på lenken på Byrånettet som vist i Figur 1.\nAlle i SSB har en Google Cloud-konto som må brukes når man logger seg på Dapla. Brukernavnet i Google er det samme som din korte epost-adresse (f.eks. cth@ssb.no). Hvis du ikke allerede er logget inn i Google vil du få spørsmål om å velge hvilken Google-konto som skal brukes (Figur 3). Logg inn med din Google-konto (ssb.no) og ditt AD-passord.\n\n\n\n\n\n\n\nFigur 3: Velg en Google-konto\n\n\n\n\nDeretter blir man spurt om man godtar at ssb.no (altså Dapla) kan bruke din Google Cloud-konto (Figur 4). Trykk Allow.\n\n\n\n\n\n\n\nFigur 4: Tillat at ssb.no får bruke din Google Cloud-konto\n\n\n\n\nDeretter lander man på en side som lar deg avgjøre hvor mye maskinkraft som skal holdes av til deg (Figur 5). Det øverste alternativet er valgt som standard, og er tilstrekkelig for de fleste.\n\n\n\n\n\n\n\nFigur 5: Velg hvor mye maskinkraft du trenger\n\n\n\n\nVent til maskinen din starter opp (Figur 6). Oppstartstiden kan variere.\n\n\n\n\n\n\n\nFigur 6: Starter opp Jupyter\n\n\n\nEtter dette er man logget inn i et Jupyter-miljø som kjører på en minimal Ubuntu-maskin. Hvis man er del av et Dapla-team får man også tilgang til alt teamet har tilgang til.\n\n\n\nInnlogging til staging-miljøet er identisk med innloggingen til prod-miljøet, med ett viktig unntak: nettadressen er nå https://jupyter.dapla-staging.ssb.no/.\nLitt mer om hva som er tilgjenglig her kommer.\n\n\n\n\n\nNoen ganger kan man komme i en situasjon hvor løsningen for Single Sign-On (pålogging på tvers av flere systemer) gir en feilmelding a la Figur 7:\n\n\n\n\n\n\nFigur 7: Feil som kan oppstå ved pålogging\n\n\n\nI denne situasjonen må man trykke på knappen “Add to existing account”. Da vil skjermbildet Figur 8 dukke opp:\n\n\n\n\n\n\nFigur 8: Klikk på Google-knappen for å logge på igjen\n\n\n\nHer må man tykke på Google-knappen (se pil), og deretter logge inn som vist i Figur 3 tidligere i dette avsnittet.\n\n\n\n\n\nJupyter-miljøet på bakken bruker samme base-image1 for å installere Jupyterlab, og er derfor identisk på mange måter. Men innloggingen er ganske forskjellig.\n\n\n\n\n\n\nNote\n\n\n\nFom. 5. desember 2022 har vi byttet ut Jupyter-miljøet på bakken. Beskrivelsene under gjelder derfor det nye miljøet. Fram til 15. januar vil du kunne bruke det gamle miljøet ved å gå inn på lenken https://jupyter-prod.ssb.no/ manuelt i Google Chrome. Etter 15. januar blir det gamle Jupyter-miljøet avviklet.\n\n\n\n\nDu logger deg inn på prod i bakkemiljøet på følgende måte:\n\nLogg deg inn på Citrix-Windows i bakkemiljøet. Det kan gjøres ved å bruke lenken Citrix på Byrånettet, som også vises i Figur 1.\nTrykk på Jupyterlab-ikonet, som vist på Figur 9, og logg deg inn med vanlig brukernavn og passord.\n\n\n\n\n\n\n\nFigur 9: Jupyterlab-ikon på Skrivebordet i Citrix-Windows.\n\n\n\nNår du trykker på ikonet blir du tatt til nettadressen https://sl-jupyter-p.ssb.no/. Du kunne også åpnet Jupyterlab ved åpne Chrome-nettleseren og skrive inn adressen manuelt.\n\n\n\nInnlogging til staging-miljøet har ingen snarvei på Skrivebordet, og du må gjøre følgende for å åpne miljøet:\n\nÅpne Chrome-nettleseren i Citrix-Windows.\nSkriv inn url-en https://sl-jupyter-t.ssb.no/",
    "crumbs": [
      "Manual",
      "Appendix",
      "Innlogging"
    ]
  },
  {
    "objectID": "statistikkere/innlogging.html#dapla",
    "href": "statistikkere/innlogging.html#dapla",
    "title": "Innlogging",
    "section": "",
    "text": "I de fleste tilfeller vil en statistikker eller forsker ønske å logge seg inn i prod-miljøet. Det er her man skal kjøre koden sin i et produksjonsløp som skal publiseres eller utvikles. I noen tilfeller hvor man ber om å få tilgjengliggjort en ny tjeneste så vil denne først rulles ut i testområdet som vi kaller staging-området. Årsaken er at vi ønsker å beskytte prod-miljøet fra software som potensielt ødelegger for eksisterende funksjonalitet. Derfor ruller vi ut nye ting i staging først. Av den grunn vil de fleste oppleve å bli bedt om å logge seg inn der for testing en eller annen gang. Under forklarer vi hvordan man går frem for å logge seg på de to ulike miljøene på Dapla.\n\n\nFor å logge seg inn inn i prod-miljøet på Dapla kan man gjøre følgende:\n\nGå inn på lenken https://jupyter.dapla.ssb.no/ i en Chrome-nettleser eller klikk på lenken på Byrånettet som vist i Figur 1.\nAlle i SSB har en Google Cloud-konto som må brukes når man logger seg på Dapla. Brukernavnet i Google er det samme som din korte epost-adresse (f.eks. cth@ssb.no). Hvis du ikke allerede er logget inn i Google vil du få spørsmål om å velge hvilken Google-konto som skal brukes (Figur 3). Logg inn med din Google-konto (ssb.no) og ditt AD-passord.\n\n\n\n\n\n\n\nFigur 3: Velg en Google-konto\n\n\n\n\nDeretter blir man spurt om man godtar at ssb.no (altså Dapla) kan bruke din Google Cloud-konto (Figur 4). Trykk Allow.\n\n\n\n\n\n\n\nFigur 4: Tillat at ssb.no får bruke din Google Cloud-konto\n\n\n\n\nDeretter lander man på en side som lar deg avgjøre hvor mye maskinkraft som skal holdes av til deg (Figur 5). Det øverste alternativet er valgt som standard, og er tilstrekkelig for de fleste.\n\n\n\n\n\n\n\nFigur 5: Velg hvor mye maskinkraft du trenger\n\n\n\n\nVent til maskinen din starter opp (Figur 6). Oppstartstiden kan variere.\n\n\n\n\n\n\n\nFigur 6: Starter opp Jupyter\n\n\n\nEtter dette er man logget inn i et Jupyter-miljø som kjører på en minimal Ubuntu-maskin. Hvis man er del av et Dapla-team får man også tilgang til alt teamet har tilgang til.\n\n\n\nInnlogging til staging-miljøet er identisk med innloggingen til prod-miljøet, med ett viktig unntak: nettadressen er nå https://jupyter.dapla-staging.ssb.no/.\nLitt mer om hva som er tilgjenglig her kommer.\n\n\n\n\n\nNoen ganger kan man komme i en situasjon hvor løsningen for Single Sign-On (pålogging på tvers av flere systemer) gir en feilmelding a la Figur 7:\n\n\n\n\n\n\nFigur 7: Feil som kan oppstå ved pålogging\n\n\n\nI denne situasjonen må man trykke på knappen “Add to existing account”. Da vil skjermbildet Figur 8 dukke opp:\n\n\n\n\n\n\nFigur 8: Klikk på Google-knappen for å logge på igjen\n\n\n\nHer må man tykke på Google-knappen (se pil), og deretter logge inn som vist i Figur 3 tidligere i dette avsnittet.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Innlogging"
    ]
  },
  {
    "objectID": "statistikkere/innlogging.html#bakkemiljøet",
    "href": "statistikkere/innlogging.html#bakkemiljøet",
    "title": "Innlogging",
    "section": "",
    "text": "Jupyter-miljøet på bakken bruker samme base-image1 for å installere Jupyterlab, og er derfor identisk på mange måter. Men innloggingen er ganske forskjellig.\n\n\n\n\n\n\nNote\n\n\n\nFom. 5. desember 2022 har vi byttet ut Jupyter-miljøet på bakken. Beskrivelsene under gjelder derfor det nye miljøet. Fram til 15. januar vil du kunne bruke det gamle miljøet ved å gå inn på lenken https://jupyter-prod.ssb.no/ manuelt i Google Chrome. Etter 15. januar blir det gamle Jupyter-miljøet avviklet.\n\n\n\n\nDu logger deg inn på prod i bakkemiljøet på følgende måte:\n\nLogg deg inn på Citrix-Windows i bakkemiljøet. Det kan gjøres ved å bruke lenken Citrix på Byrånettet, som også vises i Figur 1.\nTrykk på Jupyterlab-ikonet, som vist på Figur 9, og logg deg inn med vanlig brukernavn og passord.\n\n\n\n\n\n\n\nFigur 9: Jupyterlab-ikon på Skrivebordet i Citrix-Windows.\n\n\n\nNår du trykker på ikonet blir du tatt til nettadressen https://sl-jupyter-p.ssb.no/. Du kunne også åpnet Jupyterlab ved åpne Chrome-nettleseren og skrive inn adressen manuelt.\n\n\n\nInnlogging til staging-miljøet har ingen snarvei på Skrivebordet, og du må gjøre følgende for å åpne miljøet:\n\nÅpne Chrome-nettleseren i Citrix-Windows.\nSkriv inn url-en https://sl-jupyter-t.ssb.no/",
    "crumbs": [
      "Manual",
      "Appendix",
      "Innlogging"
    ]
  },
  {
    "objectID": "statistikkere/innlogging.html#footnotes",
    "href": "statistikkere/innlogging.html#footnotes",
    "title": "Innlogging",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nHva er base-image?↩︎",
    "crumbs": [
      "Manual",
      "Appendix",
      "Innlogging"
    ]
  },
  {
    "objectID": "statistikkere/overforing-av-data.html",
    "href": "statistikkere/overforing-av-data.html",
    "title": "Overføring av data",
    "section": "",
    "text": "For å overføre data mellom bakke og sky brukes Data Transfer, som er en tjeneste i Google Cloud Console. Denne tjenesten kan brukes til å flytte data både til og fra Linuxstammen og Dapla, og er tilgjengelig for teamets kildedataansvarlige.\nFor å få tilgang til å overføre filer må man be om dette ved opprettelsen av teamet. Ber man om det skjer følgende:\n\nEn mappe blir opprettet på Linux i prodsonen under /ssb/cloud_sync/\nEt Google Project blir opprettet med navn &lt;team navn&gt;-ts.\n\nDette Google-prosjektet er ikke det samme som der du lagrer annen data. Det har navnet &lt;team navn&gt;-ts, og filstiene på bakken og sky vises i Figur 1.\n\n\n\n\n\n\nFigur 1: Hvordan Transfer Service kan flytte filer mellom bakke og sky.\n\n\n\nTeamets kildedataansvarlige vil være spesifisert som en del av å opprette et Dapla-team.\n\n\nEnten man skal overføre filer opp til sky eller ned til bakken så bruker man den samme Data Transfer tjenesten. For å få tilgang til denne må man først logge seg inn i Google Cloud Console. Sjekk at du er logget inn med din SSB-konto (xxx@ssb.no).\nØverst på siden, til høyre for teksten Google Cloud finnes det en prosjektvelger, og her er det viktig å velge korrekt Google prosjekt. Hvis du trykker på prosjektvelgeren vil det åpnes opp et nytt vindu. Sjekk at det står SSB.NO øverst i dette vinduet. Trykk deretter på fanen ALL for å få opp alle tilgjengelige Google-prosjekter under organisasjonen ssb.no (Figur 2)\n\n\n\n\n\n\nFigur 2: Prosjektvelgeren i Google Cloud Console\n\n\n\nUnder ssb.no vil det ligge flere mapper. Åpne mappen som heter production og let frem en undermappe som har navnet på ditt Dapla-team. Strukturen skal se slik ut:\n    ssb.no\n    ├── production\n        └── &lt;teamnavn&gt;\n            ├── prod-&lt;teamnavn&gt;\n            └── &lt;teamnavn&gt;-ts\nDet underste nivået (prod-&lt;teamnavn&gt; og &lt;teamnavn&gt;-ts) viser prosjektene, nivået i mellom er mapper, og toppnivået er organisasjonen (ssb.no). Prosjektet &lt;teamnavn&gt;-ts er et separat prosjekt som bare teamets kildedataansvarlige har tilgang til, og det er her tjenesten Data Transfer skal settes opp.\n\nVelg derfor prosjektet &lt;teamnavn&gt;-ts.\nI søkefeltet til Google Cloud Console, skriv Data transfer og trykk på det valget som kommer opp.\nFørste gang man kommer inn på siden til Transfer Services vil man bli vist en blå knapp med teksten Set Up Connection. Når du trykker på denne vil det dukke opp et nytt felt hvor du får valget Create Pub-Sub Resources. Dette er noe som bare trengs å gjøre én gang. Trykk på den blå CREATE knappen, og deretter trykk på Close lenger nede.\nI navigasjonsmenyen til venstre trykk Transfer jobs, og deretter trykk på + Create transfer job øverst på siden for å opprette en ny overføringsjobb.\n\n\n\nFølgende oppskrift tar utgangspunkt i siden Create a transfer job (Figur 3):\n\n\n\n\n\n\nFigur 3: Opprett overføringsjobb i Google Cloud Console\n\n\n\n\nVelg POSIX filesystem under “Source type” og Google cloud storage under “Destination type” (eller motsatt hvis overføringsjobben skal gå fra Dapla til Linuxstammen). Trykk Next step\nNedtrekkslisten “Agent pool” skal normalt bare ha ett valg: transfer_service_default. Velg denne.\nI feltet “Source directory path” skal man kun skrive data/tilsky siden overføringsagenten kun har tilgang til mapper som ligger relativt plassert under /ssb/cloud_sync/&lt;teamnavn&gt;/. Trykk Next step\nVelg en destinasjon for overføringsjobben. Trykk på Browse og velg bøtten med navn som passer til ssb-prod-&lt;teamnavn&gt;-data-synk-opp. Vi anbefaler at du også oppretter en mappe inne i denne bøtten. Det gjøres ved å trykke på mappeikonet med et +-tegn foran. Skriv inn et passende mappenavn og trykk Select i bunnen av siden. Trykk deretter Next step\nNeste steg “Choose how and when to run this job” er opp til brukeren å bestemme. Hvis man f.eks. velger at Data Transfer skal overføre data en gang i uken, vil den kun starte en overføring hvis det finnes nye data. Trykk Next step\nBeskriv overføringsjobben, f.eks: “Flytter data for  til sky.”. Resten av feltene er opp til brukeren å bestemme. Standardverdiene er OK.\n\nTrykk til slutt på den blå Create-knappen. Du vil kunne se kjørende jobber under menyen Transfer jobs.\nFor å sjekke om data har blitt overført, skriv inn cloud storage i søkefeltet øverst på siden og trykk på det første valget som kommer opp. Her vil du finne en oversikt over alle teamets bøtter, deriblant en med navn ssb-prod-&lt;team-name&gt;-data-synk-opp. Når overføringsjobben er ferdig vil du kunne finne igjen dataene i den mappen som ble definert i stegene overnfor.\n\n\n\nOverføringsjobben settes opp nesten identisk med Overføring fra Linuxstammen til Dapla med unntak av følgende:\n\nSteg 1: Velg Google cloud storage under “Source type” og POSIX filesystem under “Destination type”\nSteg 2: Velg bøtten ssb-prod-&lt;team-name&gt;-data-synk-ned\nStep 3: Velg transfer_service_default som “Agent pool” og skriv data/frasky inn i feltet for “Destination directory path”.\n\nFor å se om data har blitt overført til Linuxstammen må du nå gå til mappen /ssb/cloud_sync/&lt;team-name&gt;/data/frasky fra FileZilla.\nHusk: Du kan alltids gå tilbake og se på tidligere fullførte jobber, og starte en overføringsjobb manuelt fra menyen Transfer jobs.\n\n\n\n\nNår du har satt opp en, enten for å overføre fra sky eller til sky, kan du skrive ut data til mappen eller bøtten som du har bedt Transfer Service om å overføre data fra.\nHvis du skal overføre data fra bakken/prodsonen til sky, så må teamets kildedataansvarlige skrive ut data til Linux-mappen /ssb/cloud_sync/&lt;team navn&gt;/data/tilsky, og det vil ende opp i Dapla-bøtta gs://ssb-prod-&lt;team navn&gt;-data-synk-opp Dette kan du gjøre med alle programmeringsverktøy som har en kobling til Linux-stammene der dataene ligger. For eksempel:\n\nSAS EG\nSAS-installasjon på Linux\nJupyterlab i prodsonen\nRstudio på sl-stata-03\n\nSkal du flytte data fra Dapla til bakken/prodsonen, så må teamets kildedataansvarlige skrive ut data til gs://ssb-prod-&lt;team navn&gt;-data-synk-opp-bøtta på Dapla. Det er noe man typisk gjør fra Jupyterlab på Dapla."
  },
  {
    "objectID": "statistikkere/overforing-av-data.html#sette-opp-overføringsjobber",
    "href": "statistikkere/overforing-av-data.html#sette-opp-overføringsjobber",
    "title": "Overføring av data",
    "section": "",
    "text": "Enten man skal overføre filer opp til sky eller ned til bakken så bruker man den samme Data Transfer tjenesten. For å få tilgang til denne må man først logge seg inn i Google Cloud Console. Sjekk at du er logget inn med din SSB-konto (xxx@ssb.no).\nØverst på siden, til høyre for teksten Google Cloud finnes det en prosjektvelger, og her er det viktig å velge korrekt Google prosjekt. Hvis du trykker på prosjektvelgeren vil det åpnes opp et nytt vindu. Sjekk at det står SSB.NO øverst i dette vinduet. Trykk deretter på fanen ALL for å få opp alle tilgjengelige Google-prosjekter under organisasjonen ssb.no (Figur 2)\n\n\n\n\n\n\nFigur 2: Prosjektvelgeren i Google Cloud Console\n\n\n\nUnder ssb.no vil det ligge flere mapper. Åpne mappen som heter production og let frem en undermappe som har navnet på ditt Dapla-team. Strukturen skal se slik ut:\n    ssb.no\n    ├── production\n        └── &lt;teamnavn&gt;\n            ├── prod-&lt;teamnavn&gt;\n            └── &lt;teamnavn&gt;-ts\nDet underste nivået (prod-&lt;teamnavn&gt; og &lt;teamnavn&gt;-ts) viser prosjektene, nivået i mellom er mapper, og toppnivået er organisasjonen (ssb.no). Prosjektet &lt;teamnavn&gt;-ts er et separat prosjekt som bare teamets kildedataansvarlige har tilgang til, og det er her tjenesten Data Transfer skal settes opp.\n\nVelg derfor prosjektet &lt;teamnavn&gt;-ts.\nI søkefeltet til Google Cloud Console, skriv Data transfer og trykk på det valget som kommer opp.\nFørste gang man kommer inn på siden til Transfer Services vil man bli vist en blå knapp med teksten Set Up Connection. Når du trykker på denne vil det dukke opp et nytt felt hvor du får valget Create Pub-Sub Resources. Dette er noe som bare trengs å gjøre én gang. Trykk på den blå CREATE knappen, og deretter trykk på Close lenger nede.\nI navigasjonsmenyen til venstre trykk Transfer jobs, og deretter trykk på + Create transfer job øverst på siden for å opprette en ny overføringsjobb.\n\n\n\nFølgende oppskrift tar utgangspunkt i siden Create a transfer job (Figur 3):\n\n\n\n\n\n\nFigur 3: Opprett overføringsjobb i Google Cloud Console\n\n\n\n\nVelg POSIX filesystem under “Source type” og Google cloud storage under “Destination type” (eller motsatt hvis overføringsjobben skal gå fra Dapla til Linuxstammen). Trykk Next step\nNedtrekkslisten “Agent pool” skal normalt bare ha ett valg: transfer_service_default. Velg denne.\nI feltet “Source directory path” skal man kun skrive data/tilsky siden overføringsagenten kun har tilgang til mapper som ligger relativt plassert under /ssb/cloud_sync/&lt;teamnavn&gt;/. Trykk Next step\nVelg en destinasjon for overføringsjobben. Trykk på Browse og velg bøtten med navn som passer til ssb-prod-&lt;teamnavn&gt;-data-synk-opp. Vi anbefaler at du også oppretter en mappe inne i denne bøtten. Det gjøres ved å trykke på mappeikonet med et +-tegn foran. Skriv inn et passende mappenavn og trykk Select i bunnen av siden. Trykk deretter Next step\nNeste steg “Choose how and when to run this job” er opp til brukeren å bestemme. Hvis man f.eks. velger at Data Transfer skal overføre data en gang i uken, vil den kun starte en overføring hvis det finnes nye data. Trykk Next step\nBeskriv overføringsjobben, f.eks: “Flytter data for  til sky.”. Resten av feltene er opp til brukeren å bestemme. Standardverdiene er OK.\n\nTrykk til slutt på den blå Create-knappen. Du vil kunne se kjørende jobber under menyen Transfer jobs.\nFor å sjekke om data har blitt overført, skriv inn cloud storage i søkefeltet øverst på siden og trykk på det første valget som kommer opp. Her vil du finne en oversikt over alle teamets bøtter, deriblant en med navn ssb-prod-&lt;team-name&gt;-data-synk-opp. Når overføringsjobben er ferdig vil du kunne finne igjen dataene i den mappen som ble definert i stegene overnfor.\n\n\n\nOverføringsjobben settes opp nesten identisk med Overføring fra Linuxstammen til Dapla med unntak av følgende:\n\nSteg 1: Velg Google cloud storage under “Source type” og POSIX filesystem under “Destination type”\nSteg 2: Velg bøtten ssb-prod-&lt;team-name&gt;-data-synk-ned\nStep 3: Velg transfer_service_default som “Agent pool” og skriv data/frasky inn i feltet for “Destination directory path”.\n\nFor å se om data har blitt overført til Linuxstammen må du nå gå til mappen /ssb/cloud_sync/&lt;team-name&gt;/data/frasky fra FileZilla.\nHusk: Du kan alltids gå tilbake og se på tidligere fullførte jobber, og starte en overføringsjobb manuelt fra menyen Transfer jobs."
  },
  {
    "objectID": "statistikkere/overforing-av-data.html#skrive-ut-data",
    "href": "statistikkere/overforing-av-data.html#skrive-ut-data",
    "title": "Overføring av data",
    "section": "",
    "text": "Når du har satt opp en, enten for å overføre fra sky eller til sky, kan du skrive ut data til mappen eller bøtten som du har bedt Transfer Service om å overføre data fra.\nHvis du skal overføre data fra bakken/prodsonen til sky, så må teamets kildedataansvarlige skrive ut data til Linux-mappen /ssb/cloud_sync/&lt;team navn&gt;/data/tilsky, og det vil ende opp i Dapla-bøtta gs://ssb-prod-&lt;team navn&gt;-data-synk-opp Dette kan du gjøre med alle programmeringsverktøy som har en kobling til Linux-stammene der dataene ligger. For eksempel:\n\nSAS EG\nSAS-installasjon på Linux\nJupyterlab i prodsonen\nRstudio på sl-stata-03\n\nSkal du flytte data fra Dapla til bakken/prodsonen, så må teamets kildedataansvarlige skrive ut data til gs://ssb-prod-&lt;team navn&gt;-data-synk-opp-bøtta på Dapla. Det er noe man typisk gjør fra Jupyterlab på Dapla."
  },
  {
    "objectID": "statistikkere/contribution.html",
    "href": "statistikkere/contribution.html",
    "title": "Bidra til Dapla-manualen",
    "section": "",
    "text": "Alle i SSB kan bidra til denne manualen. Endringer må godkjennes av noen i Team Statistikktjenester, si gjerne i fra at det ligger en PR å se på. Vi trenger bidrag med alt fra språkvask, nye artikler og andre gode initiativer! Har du lyst til å bidra, men er ikke helt sikker på hva du kan bidra med? Ta en titt på issues i GitHub-repoet.\n\n\n\n\n\n\nWarning\n\n\n\nDenne nettsiden er åpen og hvem som helst kan lese det som er skrevet her. Hold det i tankene når du skriver.\n\n\n\n\n\nMan trenger basis git kompetanse, det ligger en fin beskrivelse av det på Beste Praksis siden fra KVAKK.\nMan trenger en konto på Github, det kan man opprette ved å følge instruksjonene her.\nMan kan lære seg Markdown (enkel tekst formatering), en fin oversikt finnes her.\nVerktøyet Quarto burde installeres for å kunne se endringene slik som de ser ut på nettsiden. Installasjon instruksjoner finnes her.\n\n\n\n\n\nKlone repoet\n\n\n\nterminal\n\ngit clone https://github.com/statisticsnorway/dapla-manual.git\n\n\nLage en ny gren\nGjøre endringen\nKjør følgende og følge lenken for å sjekke at alt ser bra ut på nettsiden:\n\n\n\nterminal\n\nquarto preview dapla-manual\n\n\nÅpne en PR\nBe noen å gjennomgå endringen og eventuelt godkjenne den\nMerge PRen\nEtter et par minutter skal din endring være synlig!\n\n\n\nQuarto tilbyr å legge ved (embed) notebooks inn i nettsiden. Dette er en fin måte å dele kode og output på. Men det krever at vi tenker gjennom hvor output`en genereres. Siden Dapla-manualen renderes med GitHub-action, så ønsker vi ikke å introdusere kompleksiteten det innebærer å generere output fra kode her. I tillegg er det mange miljø-spesifikke konfigurasjoner som bestemmer hvordan output kan genereres. Derfor anbefaler vi følgende tilnærming når man skal legge ved en notebook til et kapittel:\n\nLogg deg inn i miljøet du ønsker å bruke.\nKlone ned repoet til Dapla-manualen.\nOpprett en notebook i mappen ./dapla-manual/notebooks/notebook-navn.ipynb\nSkriv kode og generer output som du ønsker. Husk å bare bruke data som alle har tilgang til eller de kan generere selv.\nEmbed output i kapitlet du ønsker iht til denne beskrivelsen\nPå toppen av notebooken lager du en raw-celle der du skriver:\n\n\n\nnotebook\n\n---\nfreeze: true\n---\n\n\nKjør denne notebooken fra terminalen med kommandoen:\n\n\n\nterminal\n\nquarto render ./notebooks/&lt;notebook-navn.ipynb&gt; --execute\n\nDette sikrer at vi kan se output etter at boken bygges i GitHub Actions. Husk at notebooks ikke blir strippet for output på vanlig måte, slik at kun åpne data skal benyttes.\nSpør Team Statistikktjenester om du lurer på noe.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Bidra til Dapla-manualen"
    ]
  },
  {
    "objectID": "statistikkere/contribution.html#forutsetninger",
    "href": "statistikkere/contribution.html#forutsetninger",
    "title": "Bidra til Dapla-manualen",
    "section": "",
    "text": "Man trenger basis git kompetanse, det ligger en fin beskrivelse av det på Beste Praksis siden fra KVAKK.\nMan trenger en konto på Github, det kan man opprette ved å følge instruksjonene her.\nMan kan lære seg Markdown (enkel tekst formatering), en fin oversikt finnes her.\nVerktøyet Quarto burde installeres for å kunne se endringene slik som de ser ut på nettsiden. Installasjon instruksjoner finnes her.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Bidra til Dapla-manualen"
    ]
  },
  {
    "objectID": "statistikkere/contribution.html#fremgangsmåten",
    "href": "statistikkere/contribution.html#fremgangsmåten",
    "title": "Bidra til Dapla-manualen",
    "section": "",
    "text": "Klone repoet\n\n\n\nterminal\n\ngit clone https://github.com/statisticsnorway/dapla-manual.git\n\n\nLage en ny gren\nGjøre endringen\nKjør følgende og følge lenken for å sjekke at alt ser bra ut på nettsiden:\n\n\n\nterminal\n\nquarto preview dapla-manual\n\n\nÅpne en PR\nBe noen å gjennomgå endringen og eventuelt godkjenne den\nMerge PRen\nEtter et par minutter skal din endring være synlig!\n\n\n\nQuarto tilbyr å legge ved (embed) notebooks inn i nettsiden. Dette er en fin måte å dele kode og output på. Men det krever at vi tenker gjennom hvor output`en genereres. Siden Dapla-manualen renderes med GitHub-action, så ønsker vi ikke å introdusere kompleksiteten det innebærer å generere output fra kode her. I tillegg er det mange miljø-spesifikke konfigurasjoner som bestemmer hvordan output kan genereres. Derfor anbefaler vi følgende tilnærming når man skal legge ved en notebook til et kapittel:\n\nLogg deg inn i miljøet du ønsker å bruke.\nKlone ned repoet til Dapla-manualen.\nOpprett en notebook i mappen ./dapla-manual/notebooks/notebook-navn.ipynb\nSkriv kode og generer output som du ønsker. Husk å bare bruke data som alle har tilgang til eller de kan generere selv.\nEmbed output i kapitlet du ønsker iht til denne beskrivelsen\nPå toppen av notebooken lager du en raw-celle der du skriver:\n\n\n\nnotebook\n\n---\nfreeze: true\n---\n\n\nKjør denne notebooken fra terminalen med kommandoen:\n\n\n\nterminal\n\nquarto render ./notebooks/&lt;notebook-navn.ipynb&gt; --execute\n\nDette sikrer at vi kan se output etter at boken bygges i GitHub Actions. Husk at notebooks ikke blir strippet for output på vanlig måte, slik at kun åpne data skal benyttes.\nSpør Team Statistikktjenester om du lurer på noe.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Bidra til Dapla-manualen"
    ]
  },
  {
    "objectID": "statistikkere/administrasjon-av-team.html",
    "href": "statistikkere/administrasjon-av-team.html",
    "title": "Administrasjon av team",
    "section": "",
    "text": "I dette kapitlet viser vi hvordan du kan opprette et nytt team eller gjøre endringer i et eksisterende team. Typiske endringer er å:\n\nLegge til, fjerne eller endre medlemmer i et team\nListe ut medlemmer og tilgangsgrupper i et team\n\n\n\nFor å opprette et Dapla-team så må en seksjonsleder gå inn i Teamoversikten i Dapla Ctrl og trykke på ikonet Opprett team. Her blir man bedt om å fylle inn relevant informasjon.\n\n\n\n\n\n\nNote\n\n\n\nLes mer om Dapla Ctrl her.\n\n\n\n\n\nFor å legge til, fjerne eller endre medlemmer i et team må kan gjøres av medlemmer i managers-gruppen i teamet. Dette gjøres i Dapla Ctrl. Les mer om hvordan dette gjøres her.\n\n\n\n\n\n\nManagers i semi- eller self-managed team\n\n\n\nManagers i semi- eller self-managed teams kan ikke legge til, fjerne eller endre medlemmer fra Dapla Ctrl enda. Disse må foreløpig kontakte Kundeservice for å gjøre endringer.\n\n\n\n\n\nDapla Ctrl lar alle i SSB se hvilke team som finnes, hvem som er medlemmer og hvilke tilgangsgrupper de ligger i. Man kan også få oversikt over hvilke data alle team deler. Les mer her.",
    "crumbs": [
      "Manual",
      "Dapla-team",
      "Administrasjon av team"
    ]
  },
  {
    "objectID": "statistikkere/administrasjon-av-team.html#opprette-dapla-team",
    "href": "statistikkere/administrasjon-av-team.html#opprette-dapla-team",
    "title": "Administrasjon av team",
    "section": "",
    "text": "For å opprette et Dapla-team så må en seksjonsleder gå inn i Teamoversikten i Dapla Ctrl og trykke på ikonet Opprett team. Her blir man bedt om å fylle inn relevant informasjon.\n\n\n\n\n\n\nNote\n\n\n\nLes mer om Dapla Ctrl her.",
    "crumbs": [
      "Manual",
      "Dapla-team",
      "Administrasjon av team"
    ]
  },
  {
    "objectID": "statistikkere/administrasjon-av-team.html#legge-til-eller-fjerne-medlemmer-i-et-team",
    "href": "statistikkere/administrasjon-av-team.html#legge-til-eller-fjerne-medlemmer-i-et-team",
    "title": "Administrasjon av team",
    "section": "",
    "text": "For å legge til, fjerne eller endre medlemmer i et team må kan gjøres av medlemmer i managers-gruppen i teamet. Dette gjøres i Dapla Ctrl. Les mer om hvordan dette gjøres her.\n\n\n\n\n\n\nManagers i semi- eller self-managed team\n\n\n\nManagers i semi- eller self-managed teams kan ikke legge til, fjerne eller endre medlemmer fra Dapla Ctrl enda. Disse må foreløpig kontakte Kundeservice for å gjøre endringer.",
    "crumbs": [
      "Manual",
      "Dapla-team",
      "Administrasjon av team"
    ]
  },
  {
    "objectID": "statistikkere/administrasjon-av-team.html#se-medlemmer-og-roller-i-et-team",
    "href": "statistikkere/administrasjon-av-team.html#se-medlemmer-og-roller-i-et-team",
    "title": "Administrasjon av team",
    "section": "",
    "text": "Dapla Ctrl lar alle i SSB se hvilke team som finnes, hvem som er medlemmer og hvilke tilgangsgrupper de ligger i. Man kan også få oversikt over hvilke data alle team deler. Les mer her.",
    "crumbs": [
      "Manual",
      "Dapla-team",
      "Administrasjon av team"
    ]
  },
  {
    "objectID": "statistikkere/virtual-env.html",
    "href": "statistikkere/virtual-env.html",
    "title": "Virtuelle miljøer",
    "section": "",
    "text": "Et python virtuelt miljø inneholder en spesifikk versjon av python og et sett med pakker. Pakkene er kun tilgjengelige når det virtuelle miljøet er aktivert. Dette gjør at man ungår avhengighetskonflikter på tvers av prosjekter.\nSe her for mer informasjon om virtuelle miljøer.\n\n\nDet er anbefalt å benytte verktøyet poetry for å administrere prosjekter og deres virtuelle miljø.\nPoetry setter opp virtuelt miljø, gjør det enkelt å oppdatere avhengigheter, sette versjonsbegrensninger og reprodusere prosjektet.\nPoetry gjør dette ved å lagre avhengigheters eksakte versjon i prosjektets “poetry.lock”. Og eventuelle begrensninger i “pyproject.toml”. Dette gjør det enkelt for andre å bygge prosjektet med akkurat de samme pakkene og begrensningene."
  },
  {
    "objectID": "statistikkere/virtual-env.html#python",
    "href": "statistikkere/virtual-env.html#python",
    "title": "Virtuelle miljøer",
    "section": "",
    "text": "Et python virtuelt miljø inneholder en spesifikk versjon av python og et sett med pakker. Pakkene er kun tilgjengelige når det virtuelle miljøet er aktivert. Dette gjør at man ungår avhengighetskonflikter på tvers av prosjekter.\nSe her for mer informasjon om virtuelle miljøer.\n\n\nDet er anbefalt å benytte verktøyet poetry for å administrere prosjekter og deres virtuelle miljø.\nPoetry setter opp virtuelt miljø, gjør det enkelt å oppdatere avhengigheter, sette versjonsbegrensninger og reprodusere prosjektet.\nPoetry gjør dette ved å lagre avhengigheters eksakte versjon i prosjektets “poetry.lock”. Og eventuelle begrensninger i “pyproject.toml”. Dette gjør det enkelt for andre å bygge prosjektet med akkurat de samme pakkene og begrensningene."
  },
  {
    "objectID": "statistikkere/statistikkbanken.html",
    "href": "statistikkere/statistikkbanken.html",
    "title": "dapla-statbank-client",
    "section": "",
    "text": "Pakken “dapla-statbank-client” kan brukes til å overføre tabeller til Statistikkbanken fra Jupyterlab i prodsonen og på Dapla. Den henter også “filbeskrivelsen” som beskriver formen dataene skal ha når de sendes inn til Statistikkbanken. Og den kan også hente publiserte data fra Statistikkbanken. Pakken er en python-pakke som baserer seg på at dataene (deltabellene) lastes inn i en eller flere pandas DataFrames før overføring. Ved å hente ned “filbeskrivelsen” kan man validere dataene sine (dataframene) mot denne lokalt, uten å sende dataene til Statistikkbanken. Dette kan være til hjelp under setting av formen på dataene. Å hente publiserte data fra Statistikkbanken kan gjøres gjennom løse funksjoner, eller via “klienten”.\nLenker: - Pakken ligger her på Pypi. Og kan installeres via poetry med: poetry add dapla-statbank-client - Kodebasen for pakken ligger her, readme-en gir en teknisk innføring som du kan følge og kopiere kode fra, og om du finner noe du vil rapportere om bruken av pakken så gjør det gjerne under “issues” på github-sidene. - Noe demokode ligger i repoet, og kan være ett godt utgangspunkt å kopiere og endre fra.\n\n\nStatistikkbanken opererer med ulike databaser for testing og produksjonssetting. Om du vil at tallene faktisk skal oppdateres på nettsidene så må du sende til Statistikkbankens “PROD”-database. Om du kun vil teste innsending skal du sende til databasen “TEST”. Disse databasene har de samme lastebrukerne, men passordene er ulike. Om du vil teste innsending må du derfor skaffe deg “test-passordet” til den lastebrukeren som du har tilgjengelig. For å gjøre tester via pakken må du være i staging på dapla: https://jupyter.dapla-staging.ssb.no/ eller staging i prodosonen: https://sl-jupyter-t.ssb.no/ Om du faktisk vil sende inn data til publisering i Statistikkbanken må du være i dapla-prod (den vanlige): https://jupyter.dapla.ssb.no/ eller i prodsonen på: https://sl-jupyter-p.ssb.no/ For å teste er det fint å skaffe seg noe data fra fjorårets publisering på et produksjonsløp man kjenner fra før. Evt. kan man hente data fra Statistikkbanken og sende disse tilbake snudd rett, og med riktig antall prikke-kolonner lagt til.\n\n\n\nSe mer detaljer i readme-en på prosjektets kodebase.\n\n\nFor å kunne bruke pakken må du importere klienten:\n\n\nnotebook\n\nfrom statbank import StatbankClient\n\nSå initialiserer du klienten med de innstillingene som oftest er faste på tvers av alle innsendingene fra ett produksjonsløp:\n\n\nnotebook\n\nstatcli = StatbankClient(date=\"2050-01-01\", overwrite=True, approve=2)\n\nHer vil du bli bedt om å skrive inn lastebruker, og passordet til lastebrukeren.\n\n\n\nOm du har dataene klare er det bare å overføre, men du må vite navnet på deltabell-dat-filene. (Statistikkbanken lagrer disse ned igjen som dat-filer.)\n\n\nnotebook\n\ndata_07495 = {\"kargrs01fylker1.dat\" : df_07495_fylker,\n              \"kargrs01landet1.dat\" : df_07495_landet,}\nstatcli.transfer(data_07495, tableid=\"07495\")\n\nEtter innsending kommer det en link til Statistikkbankens GUI for å følge med på om innsendingen gikk bra hos dem. Om det var det du ønsket, så er du nå ferdig… Men det finnes mer funksjonalitet her…\n\n\n\nFor å hente filbeskrivelsen til en hovedtabell bruker du denne metoden under klienten:\n\n\nnotebook\n\nfilbeskrivelse = statcli.get_description(tableid=\"07495\")\nprint(filbeskrivelse)\n\nMed filbeskrivelsen kan du lett få en mal på dictionaryet du må plassere dataene i:\n\n\nnotebook\n\nfilbeskrivelse.transferdata_template()\n\nDu kan også validere dataene dine mot filbeskrivelsen:\n\n\nnotebook\n\nfilbeskrivelse.validate(data_07495, tableid=\"07495\")\n\n\n\n\n\nDet tas noe ekstra ansvar i pakken rundt avrunding av desimaltall, da filbeskrivelsen inneholder informasjon om hvor mange desimaler som blir lagret per kolonne, kan vi runde av til rett antall samtidig som vi tar ett bevisst valg rundt “hvilken vei vi skal runde av”. På barneskolen lærte vi at ved 2,5 avrundet til 0 desimaler, så runder vi opp til 3. Det samme skjer i utgangspunktet i SAS og Excel. Python og R runder derimot “mot nærmeste partall”, så fra 2,5 blir det rundet til 2, men fra 1,5 blir det også rundet til 2. Dette er for å forhindre bias i en retning. Dvs. om alle tall rundes opp, vil en hel kolonne med tall “dras oppover”, ved å gjøre annenhver opp og ned, vil ikke helheten bli “dratt en spesifikk vei”. Siden “round to even” ikke er det folk er vandte til, gjør vi derfor noe annet i denne pakken, enn det som er vanlig oppførsel i Python. Vi runder opp. Om du bruker følgende metoden under filbeskrivelsen på dataene, så vil denne runde oppover, samtidig som den konverterer til en streng for å bevare formateringen.\n\n\nnotebook\n\ndata_07495 = filbeskrivelse.round_data(data_07495)  # For å ta vare på endringene, så må du skrive tilbake over variabelen\n\n\n\n\n\nEn date-widget for å visuelt endre til en valid dato.\nLagring av overføring og filbeskrivelses-objekter til json-filer\nUthenting av logg fra klienten\nHenting av publisert data fra statbanken\nRotering av tabeller fra statbanken",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-statbank-client"
    ]
  },
  {
    "objectID": "statistikkere/statistikkbanken.html#testoverføring-fra-staging---faktisk-oppdatering-fra-prod",
    "href": "statistikkere/statistikkbanken.html#testoverføring-fra-staging---faktisk-oppdatering-fra-prod",
    "title": "dapla-statbank-client",
    "section": "",
    "text": "Statistikkbanken opererer med ulike databaser for testing og produksjonssetting. Om du vil at tallene faktisk skal oppdateres på nettsidene så må du sende til Statistikkbankens “PROD”-database. Om du kun vil teste innsending skal du sende til databasen “TEST”. Disse databasene har de samme lastebrukerne, men passordene er ulike. Om du vil teste innsending må du derfor skaffe deg “test-passordet” til den lastebrukeren som du har tilgjengelig. For å gjøre tester via pakken må du være i staging på dapla: https://jupyter.dapla-staging.ssb.no/ eller staging i prodosonen: https://sl-jupyter-t.ssb.no/ Om du faktisk vil sende inn data til publisering i Statistikkbanken må du være i dapla-prod (den vanlige): https://jupyter.dapla.ssb.no/ eller i prodsonen på: https://sl-jupyter-p.ssb.no/ For å teste er det fint å skaffe seg noe data fra fjorårets publisering på et produksjonsløp man kjenner fra før. Evt. kan man hente data fra Statistikkbanken og sende disse tilbake snudd rett, og med riktig antall prikke-kolonner lagt til.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-statbank-client"
    ]
  },
  {
    "objectID": "statistikkere/statistikkbanken.html#kode-eksempler",
    "href": "statistikkere/statistikkbanken.html#kode-eksempler",
    "title": "dapla-statbank-client",
    "section": "",
    "text": "Se mer detaljer i readme-en på prosjektets kodebase.\n\n\nFor å kunne bruke pakken må du importere klienten:\n\n\nnotebook\n\nfrom statbank import StatbankClient\n\nSå initialiserer du klienten med de innstillingene som oftest er faste på tvers av alle innsendingene fra ett produksjonsløp:\n\n\nnotebook\n\nstatcli = StatbankClient(date=\"2050-01-01\", overwrite=True, approve=2)\n\nHer vil du bli bedt om å skrive inn lastebruker, og passordet til lastebrukeren.\n\n\n\nOm du har dataene klare er det bare å overføre, men du må vite navnet på deltabell-dat-filene. (Statistikkbanken lagrer disse ned igjen som dat-filer.)\n\n\nnotebook\n\ndata_07495 = {\"kargrs01fylker1.dat\" : df_07495_fylker,\n              \"kargrs01landet1.dat\" : df_07495_landet,}\nstatcli.transfer(data_07495, tableid=\"07495\")\n\nEtter innsending kommer det en link til Statistikkbankens GUI for å følge med på om innsendingen gikk bra hos dem. Om det var det du ønsket, så er du nå ferdig… Men det finnes mer funksjonalitet her…\n\n\n\nFor å hente filbeskrivelsen til en hovedtabell bruker du denne metoden under klienten:\n\n\nnotebook\n\nfilbeskrivelse = statcli.get_description(tableid=\"07495\")\nprint(filbeskrivelse)\n\nMed filbeskrivelsen kan du lett få en mal på dictionaryet du må plassere dataene i:\n\n\nnotebook\n\nfilbeskrivelse.transferdata_template()\n\nDu kan også validere dataene dine mot filbeskrivelsen:\n\n\nnotebook\n\nfilbeskrivelse.validate(data_07495, tableid=\"07495\")",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-statbank-client"
    ]
  },
  {
    "objectID": "statistikkere/statistikkbanken.html#problematikk-knyttet-til-avrunding",
    "href": "statistikkere/statistikkbanken.html#problematikk-knyttet-til-avrunding",
    "title": "dapla-statbank-client",
    "section": "",
    "text": "Det tas noe ekstra ansvar i pakken rundt avrunding av desimaltall, da filbeskrivelsen inneholder informasjon om hvor mange desimaler som blir lagret per kolonne, kan vi runde av til rett antall samtidig som vi tar ett bevisst valg rundt “hvilken vei vi skal runde av”. På barneskolen lærte vi at ved 2,5 avrundet til 0 desimaler, så runder vi opp til 3. Det samme skjer i utgangspunktet i SAS og Excel. Python og R runder derimot “mot nærmeste partall”, så fra 2,5 blir det rundet til 2, men fra 1,5 blir det også rundet til 2. Dette er for å forhindre bias i en retning. Dvs. om alle tall rundes opp, vil en hel kolonne med tall “dras oppover”, ved å gjøre annenhver opp og ned, vil ikke helheten bli “dratt en spesifikk vei”. Siden “round to even” ikke er det folk er vandte til, gjør vi derfor noe annet i denne pakken, enn det som er vanlig oppførsel i Python. Vi runder opp. Om du bruker følgende metoden under filbeskrivelsen på dataene, så vil denne runde oppover, samtidig som den konverterer til en streng for å bevare formateringen.\n\n\nnotebook\n\ndata_07495 = filbeskrivelse.round_data(data_07495)  # For å ta vare på endringene, så må du skrive tilbake over variabelen",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-statbank-client"
    ]
  },
  {
    "objectID": "statistikkere/statistikkbanken.html#annen-funksjonalitet-du-kan-lese-om-i-readme-en-er",
    "href": "statistikkere/statistikkbanken.html#annen-funksjonalitet-du-kan-lese-om-i-readme-en-er",
    "title": "dapla-statbank-client",
    "section": "",
    "text": "En date-widget for å visuelt endre til en valid dato.\nLagring av overføring og filbeskrivelses-objekter til json-filer\nUthenting av logg fra klienten\nHenting av publisert data fra statbanken\nRotering av tabeller fra statbanken",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-statbank-client"
    ]
  },
  {
    "objectID": "statistikkere/hvorfor-dapla.html",
    "href": "statistikkere/hvorfor-dapla.html",
    "title": "Hvorfor Dapla?",
    "section": "",
    "text": "Hvorfor Dapla?\nSom dataplattform skal Dapla stimulerere til økt kvalitet på statistikk og forskning, samtidig som den gjør organisasjonen mer tilpasningsdyktig i møte med fremtiden.\n\n\nDen nye skybaserte dataplattformen (Dapla) skal bli viktig for å effektivisere arbeids-og produksjonsprosesser, den skal sikre effektiv lagring og gjenfinning av data og metadata, og støtte opp under deling av data på tvers av statistikkområder.\n\nKilde: Langtidsplan for SSB (2022-2024)\n\n\n\nMålet med Dapla er å tilby tjenester og verktøy som lar statistikkprodusenter og forskere produsere resultater på en sikker og effektiv måte."
  },
  {
    "objectID": "statistikkere/navnestandard.html",
    "href": "statistikkere/navnestandard.html",
    "title": "Navnestandard",
    "section": "",
    "text": "Data i de permanente datatilstandene inndata, klargjorte data, statistikkdata og utdata skal lagres i Google Cloud Storage (GCS) bøtter og følge en definert navnestandard. Standarden gjelder for både statistikkprodukter og dataprodukter (se forklaringsboks under). Navnestandarden beskrevet i dette kapitlet er derfor gjeldende for alle data som lagres i bøtter i standardprosjektet, som f.eks. produktbøtta og delt-bøttene.\nDatatilstanden kildedata omfattes ikke av navnestandarden. Grunnen er at kildedata mottas av SSB i mange former/strukturer og de deles sjelden med andre team. De unike egenskapene til kildedata er også grunnen til at de ikke har samme krav til dokumentasjon i metadatasystemene heller.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Navnestandard"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard.html#mappestruktur",
    "href": "statistikkere/navnestandard.html#mappestruktur",
    "title": "Navnestandard",
    "section": "Mappestruktur",
    "text": "Mappestruktur\nNavnestandarden for lagring av data innfører obligatoriske mapper som alle statistikk- og dataprodukter må følge, samt valgfrie deler hvor teamet selv kan bestemme sin mappestruktur.\n\nObligatoriske mapper\nIfølge navnestandarden skal følgende mappenivåer alltid eksistere først i en lagringsbøtte:\n\nStatistikkproduktets eller dataproduktets kortnavn\nDatatilstand\n\nAnta at det er team som heter dapla-example som har produserer statistikkproduktene ledstill og sykefra. I tillegg produserer de et dataprodukt som heter ameld. Deres mappestruktur i produktbøtta vil da se slik ut:\n\n\nObligatoriske mapper\n\nssb-dapla-example-data-produkt-prod/  \n└─ ledstill/  \n   ├── inndata/\n   ├── klargjorte-data/\n   ├── statistikk/\n   └── utdata/\n└─ sykefra/  \n   ├── inndata/\n   ├── klargjorte-data/\n   ├── statistikk/\n   └── utdata/\n└─ ameld_data/  \n   ├── inndata/\n   ├── klargjorte-data/\n   ├── statistikk/\n   └── utdata/                    \n\n\n\nValgfrie mapper\nDe to første mappenivåene er bestemt og obligatoriske. Teamene kan likevel opprette egendefinerte mapper der det er behov. Det kan gjøres i følgende tilfeller:\n\nTeamet ønsker å organisere dataene i undermapper for hver datatilstand.\nTeamet trenger å lagre temporære data.\n\nDet er anbefalt å lage en temp-mappe på første nivå etter bøttenavn, men det er også tillatt å opprette temp-mapper andre steder i mappe-hierarkiet, f.eks. ../inndata/temp/ eller ../klargjorte-data/temp/.\n\nTeamet utfører oppdrag og ønsker et eget sted å lagre data knyttet til dette. Det kan kun gjøres i en oppdrag-mappe på første nivå etter bøttenavn.\n\nUnder er et nytt eksempel i produktbøtta for team dapla-example, men nå har de kun statistikkproduktet ledstill, en temp-mappe og en oppdrag-mappe. I tillegg så ønsker de å skille mellom data som er produsert på Dapla og data som er migrert fra tidligere plattform. De gjør det ved å opprette de egendefinerte mappene on-prem og dapla for hver datatilstand.\n\n\nObligatoriske og egendefinerte mapper\n\nssb-dapla-example-data-produkt-prod/  \n└─ ledstill/  \n   ├── inndata/\n       ├── on-prem/\n       ├── dapla/\n   ├── klargjorte-data/\n       ├── on-prem/\n       ├── dapla/\n   ├── statistikk/\n       ├── on-prem/\n       ├── dapla/\n   └── utdata/\n       ├── on-prem/\n       ├── dapla/\n└─ temp/\n└─ oppdrag/                     \n\n\n\n\n\n\n\nMappe for oppdrag\n\n\n\nNår man oppretter en mappe for oppdrag så er det viktig å kunne knytte dataene til et Websak-saksnummer. Det er derfor anbefalt at det opprettes en undermappe med saksnummeret eller at saksnummeret er med i filnavnet.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Navnestandard"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard.html#filnavn",
    "href": "statistikkere/navnestandard.html#filnavn",
    "title": "Navnestandard",
    "section": "Filnavn",
    "text": "Filnavn\nFilnavn skal ha en fast struktur som inneholder: en kort beskrivelse, periode, versjon og filtype, slik som vist i Figur 1.\n\n\n\n\n\n\nFigur 1: De ulike delene av et standardisert filnavn\n\n\n\nEksempelet i Figur 1 har varehandel som kort beskrivelse, dataene er gyldige for 2018Q1, versjon er 1 og filtypen er parquet. I tillegg ser vi at periodeangivelse alltid skal prefixes med p og versjon med v. Elementene i filnavnet skal skilles med understrek.\nDet er også verdt å merke seg at mellomrom og særnorske bokstaver som æ, ø og å ikke forekommer i filnavnet. Følgende alfanumeriske tegn kan benyttes i fil- og mappenavn:\n\na-z og A-Z2.\n0-9\nBruk bindestrek -, eller understrek _, og ikke mellomrom.\n\nTabell 1 viser en mer inngående beskrivelse av hva som inngår i de ulike delene av et filnavn.\n\n\n\nTabell 1: Autonomitet og tilganger i IaC-repo\n\n\n\n\n\nElement\nForklaring\n\n\n\n\nKort beskrivelse\nKort tekst som forklarer datasettets innhold, f.eks. “varehandel”, “personinntekt”, “grensehandel_imputert” eller “framskrevne-befolkningsendringer”\n\n\nPeriode - inneholder data f.o.m. dato\nDatasettet inneholder data fra og med dato/tidspunkt. I filnavnet må perioden prefikses med “_p”, eksempel “_p2022” eller “_p2022-01-01”. “_p” er en forkortelse for “periode”. Se også gyldige formater for periode (dato/tidspunkt)\n\n\nPeriode - inneholder data t.o.m. dato\nDatasettet inneholder data til og med dato/tidspunkt. Denne brukes ved behov, eksempelvis for datasett som inneholder forløpsdata eller datasett med flere perioder/årganger.\n\n\nVersjon\nVersjon av datasettet. I filnavnet må versjonsnummeret prefikses med “_v”, eksempel “v1”, “v2” eller “v3”.\n\n\nFiltype\nFilendelse som sier noen om filtypen, f.eks. “.json”, “.csv”, “.xml” eller “.parquet”.\n\n\n\n\n\n\n\nEksempler på gyldige filnavn\nUnder finner du et utvalg eksempler på gyldige filnavn for ulike tidsspenn.\n\n\n\n\n\n\n\nTidsspenn\nEksempel på gyldige filnavn\n\n\n\n\nÉn årgang med data\nflygende_objekter_p2019_v1.parquet\n\n\nTo årganger med data\nufo_observasjoner_p2019_p2020_v1.parquet\n\n\nFra 2019 til 2050\nframskrevne-befolkningsendringer_p2019_p2050_v1.parquet\n\n\nFra 01.01.2022 til 31.12.2022\nsykepenger_p2022-01-01_p2022-12-31_v1.parquet\n\n\nTverrsnittsdata (status) per 01.10.2022\nutanningsnivaa_p2022-10-01_v1.parquet\n\n\nOktober, november og desember 2022\ngrensehandel_imputert_p2022-10_p2022-12_v1.parquet\n\n\nUke-nummer 15\nomsetning_p2020W15_v1.parquet\n\n\nFørste bimester i 2022\nskipsanloep_p2022B1_v1.parquet\n\n\nFørste kvartal i 2018 (quarter)\npensjon_p2018Q1_v1.parquet\n\n\nFørste tertial i 2022\nnybilreg_p2022T1_v1.parquet\n\n\nFørste halvår i 2022\npersoninntekt_p2022H1_v1.parquet\n\n\nKvartalene 1, 2, 3 og 4 i 2018\nvarehandel_p2018Q1_p2018Q4_v1.parquet",
    "crumbs": [
      "Manual",
      "Standarder",
      "Navnestandard"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard.html#partisjonerte-data",
    "href": "statistikkere/navnestandard.html#partisjonerte-data",
    "title": "Navnestandard",
    "section": "Partisjonerte data",
    "text": "Partisjonerte data\nTeam som partisjonerer sine filer ved lagring skal fortsatt følge navnestandarden. Det som endrer seg er at filtype ikke blir en del av filnavnet, men heller kommer inn under partisjoneringen. Anta at team dapla-example partisjonerer et datasett i datatilstand inndata som heter skjema_p2018_p2020_v1. Anta også at de partisjonerer dataene med hensyn på kolonnen aar. Da vil de i henhold til navnestandarden opprette denne strukturen:\n\n\nMappestruktur partisjonert data\n\nssb-dapla-example-data-produkt-prod/  \n└─ ledstill/  \n   ├── inndata/\n        └── skjema_p2018_p2020_v1\n            └── aar=2018\n                └── data.parquet\n            └── aar=2019\n                └── data.parquet\n            └── aar=2020\n                └── data.parquet         \n   ├── klargjorte-data/\n   ├── statistikk/\n   └── utdata/                 \n\n\nEksempel: Produktbøtte for team dapla-example\nAnta at det er team som heter dapla-example med statistikkproduktene ledstill og sykefra, og de har et dataprodukt med kortnavnet ameld. Teamet har følgende mappestruktur i produktbøtta:\n\n\nProduktbøtta: ledstill, sykefra og ameld\n\nssb-dapla-example-data-produkt-prod/\n└─ ledstill/  \n    ├── inndata/\n    │   ├── skjema_p2024Q1_v1.parquet\n    │   ├── skjema_p2024Q2_v1.parquet\n    │   └── skjema_p2024Q2_v2.parquet\n    ├── klargjorte-data/\n    │   ├── editert_p2024Q1_v1.parquet\n    │   └── editert_p2024Q2_v1.parquet\n    ├── statistikk/\n    │   ├── aggregert_p2024Q1_v1.parquet\n    │   └── aggregert_p2024Q2_v1.parquet        \n    └── utdata/\n    │   ├── statbank_p2024Q1_v1.parquet\n    │   └── statbank_p2024Q2_v1.parquet   \n    │\n└─ sykefra/  \n    ├── inndata/\n    │   ├── egenmeldt_p2024Q1_v1.parquet\n    │   ├── egenmeldt_p2024Q2_v1.parquet\n    │   ├── legemeldt_p2024Q1_v1.parquet\n    │   └── legemeldt_p2024Q2_v1.parquet\n    ├── klargjorte-data/\n    │   ├── sykefravaer_p2024Q1_v1.parquet\n    │   └── sykefravaer_p2024Q2_v1.parquet\n    ├── statistikk/\n    │   ├── aggregert_p2024Q1_v1.parquet\n    │   └── aggregert_p2024Q2_v1.parquet\n    └── utdata/\n    │   ├── statbank_p2024Q1_v1.parquet\n    │   └── statbank_p2024Q2_v1.parquet\n    │\n└─ ameld_data/  \n    ├── inndata/\n    │   ├── ameldingen_p2024-11_v1.parquet\n    │   └── ameldingen_p2024-12_v1.parquet\n    └── klargjorte-data/\n    │   ├── ameldingen_p2024-11_v1.parquet\n    │   └── ameldingen_p2024-12_v1.parquet",
    "crumbs": [
      "Manual",
      "Standarder",
      "Navnestandard"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard.html#versjonering-av-datasett",
    "href": "statistikkere/navnestandard.html#versjonering-av-datasett",
    "title": "Navnestandard",
    "section": "Versjonering av datasett",
    "text": "Versjonering av datasett\nVersjonering er obligatorisk når man jobber med data på dapla. Hovedgrunnen til at vi versjonerer er for å dekke kravet om uforanderlighet og etterprøvbarehet: at data-konsumenter (menneske eller maskin) skal ha kontroll på endringer. Derfor skal et datasett som er brukt i statistikkproduksjon aldri slettes - det skal opprettes en ny versjon av datasettet. Les mer om prinsippet om uforanderlighet av data på confluence-siden til IT-Arkitektur.\nKort fortalt innebærer versjonering av data at datasettene har versjonsnummer før filendelsen. For eksempel: framskrevne-befolkningsendringer_p2019_p2050_v1.parquet\n\n\n\n\n\n\nUnntak til versjonering: nyeste versjon og temporære data\n\n\n\nNyeste versjon kan lagres uten versjonsnummer. Dette er for at man enkelt skal kunne lese inn siste versjon av et datasett (ved å utelate versjonssuffiks). I tilegg trenger man ikke versjonere temporære data.\n\n\n\nNår skal man lagre ny versjon?\nFølgende hendelser skaper ny versjon av et datasett:\n\nReberegninger av data med nye metoder.\nKorrigeringer av verdier.\nOservasjoner legges til eller fjernes.\nOppdatert eller erstattet kodeverk.\nVariabler fjernes eller legges til.\n\nHvis det gjøres vesentlige endringer (mange variabler) så bør det vurderes om dette er et helt nytt datasett.\n\nAndre strukturendringer, f.eks. bytte av datatyper eller formater.\n\nMed andre ord: enhver endring skaper en ny versjon!\n\n\nVersjonering i praksis\nFor hver versjon som oppstår av datasettet opprettes det en ny fysisk fil hvor versjonsnummeret økes med en. Alle gamle versjoner av et datasett skal også eksistere i mappen.\nEtterhvert som man får flere versjoner av et datasett kan det se slik ut:\n\n\nMappe med flere versjoner av et datasett\n\nssb-prod-team-personstatistikk-data-produkt-prod/  \n└── befolkningsframskrivinger/  \n    └── klargjorte_data/  \n        ├── framskrevne-befolkningsendringer_p2019_p2050.parquet  \n        ├── framskrevne-befolkningsendringer_p2019_p2050_v1.parquet  \n        ├── framskrevne-befolkningsendringer_p2019_p2050_v2.parquet  \n        └── framskrevne-befolkningsendringer_p2019_p2050_v3.parquet\n\nEksempelet over viser at siste versjon av en fil kan lagres med og uten versjonsnummer for å gjøre det lettere å lese inn nyeste versjon.\n\nVersjon 0: Deling av data som ikke har oppnådd stabil tilstand\nHvis det er behov for å dele data som fortsatt er under innsamling eller pågående klargjøring gjøres dette ved å bruke versjonsnummer 0 i filnavnet.\nDette versjonsnummeret skal kun brukes midlertidig fram til datasettet oppnår stabil tilstand. Ved stabil tilstand byttes versjonsnummer for datasettet til 1 eller høyere.\n\n\nEksempelkode: Finne neste versjonsnummer med python-kode\n\n\nPython kode fra SSB-fagfunksjoner for finne neste versjonsnummer\n\n# importer funksjonen get_next_version_number() fra ssb-fagfunksjoner\nfrom fagfunksjoner import get_next_version_number\n\nfilsti = 'gs://ssb-prod-team-personstatistikk-data-produkt-prod/befolkningsframskrivninger/klargjorte_data/framskrevne-befolkningsendringer_p2019_p2050.parquet'\n\nneste_versjon = get_next_version_number(filsti)\n\nprint(neste_versjon) # vil i dette tilfellet skrive ut 4\n\n\n\n\n\n\n\nPermanente data skal ikke endres eller slettes\n\n\n\nTidligere delte/publiserte data skal ikke slettes eller overskrives!\nDet må derfor lagres fysiske filer for hver versjon av datasettet. Dette er viktig for at SSB skal oppfylle krav om etterprøvbarhet av statistikkene.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Navnestandard"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard.html#footnotes",
    "href": "statistikkere/navnestandard.html#footnotes",
    "title": "Navnestandard",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nLes mer om hvordan man henter ut informasjon fra API-et til Statistikkregisteret i denne blogg-artikkelen.↩︎\nDet er anbefalt at æ, ø og å erstattes med ae, oe og aa, f.eks. naering, oekonomi eller levekaar.↩︎",
    "crumbs": [
      "Manual",
      "Standarder",
      "Navnestandard"
    ]
  },
  {
    "objectID": "statistikkere/spark.html",
    "href": "statistikkere/spark.html",
    "title": "Apache Spark",
    "section": "",
    "text": "Koding i R og Python har historisk sett foregått på en enkelt maskin og vært begrenset av minnet (RAM) og prosessorkraften på maskinen. For bearbeiding av små og mellomstore datasett er det sjelden et problem på kjøre på en enkelt maskin. Populære pakker som dplyr for R, og pandas for Python, blir ofte brukt i denne typen databehandling. I senere år har det også kommet pakker som er optimalisert for å kjøre kode parallelt på flere kjerner på en enkelt maskin, skrevet i minne-effektive språk som Rust og C++.\nMen selv om man kommer langt med å kjøre kode på en enkelt maskin, vil enkelte oppgaver kreve mer minne og prosessorkraft enn dette. For større datasett, eller store beregninger, kan det være nyttig å bruke et rammeverk som kan kjøre kode parallelt på flere maskiner. Et slikt rammeverk er Apache Spark.\nApache Spark er et rammeverk for å kjøre kode parallelt på flere maskiner. Det er bygget for å håndtere store datasett og store beregninger. Det er derfor et nyttig verktøy for å løse problemer som er for store for å kjøre på en enkelt maskin. Men det finnes også andre bruksområder som er nyttige for Apache Spark. Her er noen eksempler:\nDette er noen av bruksområdene der Spark kan løse problemer som er for store for å kjøre på en enkelt maskin med for eksempel Pandas eller dplyr.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Apache Spark"
    ]
  },
  {
    "objectID": "statistikkere/spark.html#spark-på-dapla",
    "href": "statistikkere/spark.html#spark-på-dapla",
    "title": "Apache Spark",
    "section": "Spark på Dapla",
    "text": "Spark på Dapla\nDapla kjører på et Kubernetes-kluster og er derfor er et svært egnet sted for å kjøre kode parallelt på flere maskiner. Jupyter på Dapla har også en flere klargjorte kernels for å kjøre kode i Apache Spark. Denne koden vil kjøre på et eget kluster av maskiner som er dedikert til Apache Spark, slik som vist i Figur 1.\n\n\n\n\n\n\n\n\n\n\n\n(a) PySpark på kubernetes\n\n\n\n\n\n\n\n\n\n\n\n(b) PySpark på 1 maskin\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) SparkR på kubernetes\n\n\n\n\n\n\n\n\n\n\n\n(d) SparkR på 1 maskin\n\n\n\n\n\n\n\nFigur 1: Ferdigkonfigurerte kernels for Spark på Dapla.\n\n\n\nFigur 1 (a) og Figur 1 (c) kan velges hvis du ønsker å bruke Spark for å kjøre store jobber på flere maskiner, for henholdsvis Python- og R-grensesnittene for Spark.\nFigur 1 (b) og Figur 1 (d) bør du velge hvis du ønsker å bruke Spark av andre grunner enn å kjøre store jobber på flere maskiner. For eksempel hvis du ønsker å bruke en av de mange pakker som er bygget på Spark, eller hvis du ønsker å bruke Spark til å lese og skrive data fra Dapla.\nHvis du ønsker å sette opp et eget virtuelt miljø for å kjøre Spark, så kan du bruke ssb-project. Se ssb-project for mer informasjon.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Apache Spark"
    ]
  },
  {
    "objectID": "statistikkere/spark.html#spark-i-og-python",
    "href": "statistikkere/spark.html#spark-i-og-python",
    "title": "Apache Spark",
    "section": "Spark i  og Python",
    "text": "Spark i  og Python\nSpark er implementert i programmeringsspråket Scala. Men det tilbys også mange grensesnitt for å bruke Spark fra andre språk. De mest populære grensesnittene er PySpark for Python og SparkR for R. Disse grensesnittene er bygget på Spark, og gir tilgang til Spark-funksjonalitet fra Python og R.\n\nPySpark\nPySpark er et Python-grensesnitt for Apache Spark. Det er en Python-pakke som gir tilgang til Spark-funksjonalitet fra Python. Det er enkelt å bruke, og har mange av de samme funksjonene som Pandas.\nUnder ser du datasettet som benyttes for i vedlagt notebook pyspark-intro.ipynb. Den viser hvordan man kan gjøre vanlige databehandling med PySpark. I eksempelet brukes kernel som vist i Figur 1 (b).\n\n\n\nCode\n# Legger til row index til DataFrame før join med dates_df\ndata_df = data_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n\n# Joiner de to datasettene\ndf = (\n    dates_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n    .join(data_df, \"row_index\")\n    .drop(\"row_index\")\n)\n\n# Legger til år, kvartal og mnd\ndf = df.withColumn(\"Year\", date_format(df.Date, \"yyyy\"))\ndf = df.withColumn(\"Quarter\", expr(\"quarter(Date)\"))\ndf = df.withColumn(\"Month\", date_format(df.Date, \"MM\"))\n\ndf.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows\n\n\n\nSource: Introduksjon til PySpark\nDet finnes også et Pandas API/grensesnitt mot Spark. Målet med en er å gjøre overgangen fra Pandas til Spark lettere for nybegynneren. Men hvis man skal gjøre litt mer avansert databehandling anbefales det at man bruker PySpark direkte og ikke Pandas API-et.\n\n\nSparkR\nSparkR er et R-grensesnitt for Apache Spark. Det er en R-pakke som gir tilgang til Spark-funksjonalitet fra R. Det er enkelt å bruke, og har mange av de samme funksjonene som dplyr. Se eksempel i notebook under:\n\n\nselectedColumns &lt;- select(file, \"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\")\nshowDF(selectedColumns, numRows = 5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows\n\n\nSource: Introduksjon til SparkR",
    "crumbs": [
      "Manual",
      "Appendix",
      "Apache Spark"
    ]
  },
  {
    "objectID": "statistikkere/spark.html#lakehouse-arkitektur",
    "href": "statistikkere/spark.html#lakehouse-arkitektur",
    "title": "Apache Spark",
    "section": "Lakehouse-arkitektur",
    "text": "Lakehouse-arkitektur\n\n\n\n\n\n\nWarning\n\n\n\nI denne delen viser vi hvordan funksjonalitet som kan bli relevant for SSB å benytte seg av i fremtiden. Men det er fortsatt under testing og ta det i betraktning før man eventuelt implementerer dette i produksjon.\n\n\nEn av utvidelsene som er laget rundt Apache Spark er den såkalte Lakehouse-arkitekturen. Kort fortalt kan den dekke behovene som et klassisk datavarehus har tatt seg av tidligere. I kontekst av SSB sine teknologier kan det også benyttes som et databaselag over Parquet-filer i bøtter. Det finnes flere open source løsninger for dette, men mest aktuelle er:\n\nDelta Lake\nApache Hudi\nApache Iceberg\n\nI det følgende omtaler vi hovedsakelig egenskapene til Delta Lake, men alle rammeverkene har mange av de samme egenskapene. Delta Lake kan også benyttes på Dapla nå.\nSentrale egenskaper ved Delta Lake er:\n\nACID-transactions som sikrer data-integritet og stabilitet, også når det skjer feil.\nMetadata som bli håndtert akkurat som all annen data og er veldig skalebar. Den støtter også egendefinert metadata.\nSchema Enforcement and Evolution sikrer at skjemaet til dataene blir håndhevet, og den tillater også den kan endres over tid.\nTime travel sikrer at alle versjoner av dataene er blitt lagret, og at du kan gå tilbake til tidligere versjoner av en fil.\nAudit history sikrer at du kan få full oversikt over hvilke operasjoner som utført på dataene.\nInserts, updates and deletes betyr at du lettere kan manipulere data enn hva som er tilfellet med vanlige Parquet-filer.\nIndexing er støttes for forbedre spørringer mot store datamengder.\n\nI vedlagt notebook deltalake-intro.ipynb finner du blant annet eksempler på hvordan du legger til følgende metadata i spesifikk versjon av en fil:\n\n\n\nCode\n# Lagrer egendefinert metadata i en json-fil\nimport json\n\nmetadata = {\n    \"comment\": \"Kontaktet oppgavegiver og kranglet!\",\n    \"manueltEditert\": \"True\",\n    \"maskineltEditert\": \"False\",\n}\nmetadata\n\n\n{'comment': 'Kontaktet oppgavegiver og kranglet!',\n 'manueltEditert': 'True',\n 'maskineltEditert': 'False'}\n\n\nSource: Introduksjon til Delta Lake",
    "crumbs": [
      "Manual",
      "Appendix",
      "Apache Spark"
    ]
  },
  {
    "objectID": "statistikkere/orkestrering.html",
    "href": "statistikkere/orkestrering.html",
    "title": "Orkestrering",
    "section": "",
    "text": "Orkestrering"
  },
  {
    "objectID": "statistikkere/kildomaten.html",
    "href": "statistikkere/kildomaten.html",
    "title": "Kildomaten",
    "section": "",
    "text": "Kildomaten er en tjeneste for å automatisere overgangen fra kildedata til inndata. Tjenesten lar statistikkere kjøre sine egne skript automatisk på alle nye filer i kildedatabøtta og skrive resultatet til produktbøtta. Formålet med tjenesten er minimere behovet for tilgang til kildedata samtidig som teamet selv bestemmer hvordan transformasjonn til inndata skal foregå. Statistikkproduksjon kan da starte i en tilstand der dataminimering og pseudonymisering allerede er gjennomført.\nProsessering som skal skje i overgangen fra kildedata til inndata har SSB definert til å være:\n\nDataminimering:\nFjerne alle felter som ikke er strengt nødvendig for å produsere statistikk.\nPseudonymisering:\nPseudonymisering av personidentifiserende data.\nKodeverk:\nLegge på standard kodeverk fra for eksempel Klass.\n\nStandardisering:\nTegnsett, datoformat, etc. endres til SSBs standardformat.\n\nUnder forklarer vi nærmere hvordan man bruker tjenesten. Da forutsetter vi at du har et Dapla-team med tjenesten er aktivert. les mer om hvordan du aktiverer tjenester her (lenker her).\n\n\nFør et Dapla-team kan ta i bruk Kildomaten må man tjenesten aktivert for teamet. Som standard får alle statistikkteam dette skrudd på i prod-miljøet som opprettes for teamet. Ønsker du å aktivere Kildomaten i test-miljøet kan dette gjøres selvbetjent som en feature. Alternativt kan man opprette en Kundeservice-sak og be om å hjelp til dette.\n\n\n\nI denne delen bryter vi ned prosessen med å sette opp Kildomaten i de stegene vi mener det er hensiktsmessig å gjøre det når den settes opp for første gang på en enkeltkilde. Senere i kapitlet forklarer vi hvordan man setter den opp flere kilder og hvordan man vedlikeholder tjenesten.\nHusk at alle på teamet kan gjøre det meste av arbeidet her, men det er data-admins som må godkjenne at tjenesten rulles ut1.\n\n\nOppsett av Kildomaten gjøres i teamets IaC-repo2. Når vi skal sette opp Kildomaten-kilde må vi gjøre gjøre endringer i teamets IaC-repo. Man finner teamets IaC-repo ved gå inn på SSBs GitHub-organisasjon og søke etter repoet som heter &lt;teamnavn&gt;-iac. Når du har funnet repoet så kan du gjøre følgende:\n\nKlon ned ditt teams Iac-repo git clone &lt;repo-url&gt;\nOpprett en ny branch: git checkout -b add-kildomaten-source\n\n\n\n\nFor at Kildomaten skal fungere så må det opprettes en bestemt mappestruktur i IaC-repoet til teamet. Når et team blir opprettet vil den grunnleggende mappestrukturen i IaC-repoet allerede være opprettet for prod-miljøet til teamet. F.eks. vil mappestrukturen se slik ut for team dapla-example:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n├── automation/\n│   └── source-data/\n│       ├── dapla-example-prod/\n│           └── README.md\n│ \n│...           \n\nSkal du sette opp Kildomaten i prod-miljøet så kan du følge oppskriften som kommer senere i kapitlet uten å gjøre noe mer enda.\nSkal du også bruke Kildomaten i test-miljøet så må opprette en ny mappe og lage en PR i IaC-repoet til teamet. Da vil strukturen se slik ut:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n├── automation/\n│   └── source-data/\n│       ├── dapla-example-prod/\n│       │    └── README.md\n│       ├── dapla-example-test/ \n│...           \n\nI mappestrukturen over så har vi klargjort den grunnleggende mappestrukturen for å ta i bruk Kildomaten i prod- og test-miljøet. Neste steg blir å legge de ulike kildene som egne mapper under dapla-example-prod og dapla-example-test. Det viser vi i neste avsnitt.\n\n\n\nKildomaten lar deg prosessere ulike filstier i kildebøtta med ulike python-script. Dette refereres til som at Kildomaten har flere kilder. For å sette opp en kilde så må man følge en definert mappestruktur i IaC-repoet der alle kildene ligger rett under &lt;teamnavn&gt;-prod- eller &lt;teamnavn&gt;-test-mappen. Du kan ikke ha undermapper under en kilde. Du velger selv navnet på kildene/mappene i IaC-repoet, og det vil være navnet på kildene i Kildomaten. Senere i kapitlet ser vi at vi må bruke navnet for trigge re-kjøring av kilder.\nUnder er et eksempel på hvordan det kan se ut for eksempel-teamet dapla-example:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n├── automation/\n│   └── source-data/\n│       ├── dapla-example-prod/\n│       │    └── altinn\n│       │    └── ameld\n│       ├── dapla-example-test/\n│       │    └── altinn\n│       │    └── ameld\n│       │    └── nudb\n│...           \n\nI eksempelet over ser vi at det er opprettet kildene altinn og ameld for både test- og prod-miljøet. I tillegg er det i test-miljøet kjørende en annen kilde som heter nudb. Hver av disse kildene kan kjøre et eget Pyton-script på alle filer som skrives til en gitt filsti som man definerer selv.\n\n\n\n\n\n\nAlle kilder på samme nivå\n\n\n\nEt team kan sette opp mange kilder som skal prosesseres med ulike skript. Men Kildomaten tillater bare et nivå under automation/source-data-&lt;teamnavn&gt;-&lt;miljø&gt;/. Det vil si at du ikke kan ha undermapper under en kilde. Det er også slik at man alltid må opprette en mappe for en kilde, selv om du kun har en kilde. F.eks. kan man ikke droppe mappen altinn i eksempelet over.\n\n\n\n\n\nNår mappestrukturen er opprettet kan man legge til filene som beskriver hvilke filer som skal prosesseres, og python-skriptet med koden som skal prosessere filene. Det er to filer som må eksistere for at tjenesten skal fungere:\n\nEn konfigurasjonsfil\nEt Python-skript\n\nNår du har opprettet de skal de ligge på denne måten i IaC-repoet:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n├── automation/\n│   └── source-data/\n│       ├── dapla-example-prod/\n│           └── altinn/\n│               ├── config.yaml\n│               └── process_source_data.py\n│...           \n\nUnder forklares hvordan skriver innholdet i de to filene.\n\n\nKildomaten trigges ved at det oppstår nye filer i kildebøtta til teamet. Hvorvidt den skal trigges på alle filer, eller kun filer i en gitt undermappe, bestemmer brukeren ved å konfigurere tjenesten i config.yaml. Her kan du også angi hvor mye ressurser prosesseringen skal få.\nHvis vi fortsetter eksempelet vårt fra tidligere med dapla-example, så kan vi tenkes oss at teamet ønsker å Kildomaten skal trigges på alle filer som oppstår i kildebøtta under filstien:\nssb-dapla-example-data-kilde-prod/ledstill/altinn/.\nFor å konfigurere tjenesten i Kildomaten må vi legge til en fil som heter config.yaml under automation/source-data/dapla-example-prod/altinn/, slik som vist i forrige kapitel. I dette tilfellet vil den ha innholdet som vist til venstre under:\n\n\n\n\nconfig.yaml\n\nfolder_prefix: ledstill/altinn\nmemory_size: 1  # GiB\n\n\n\n\n\n\nssb-dapla-example-data-kilde-prod/\n\n├── ledstill/\n│   └── altinn/\n│   └── aordningen/\n├── sykefra/\n│   └── altinn/\n│   └── freg/\n│...\n       \n\n\n\nMappestrukturen til høyre over viser hvordan vi mappestrukturen ser ut i kildebøtta, mens config.yaml-fila til venstre viser hvordan vi angir at Kildomaten kun skal trigges på nye filer som oppstår i undermappen ssb-dapla-example-data-kilde-prod/ledstill/altinn/. Vi bruker nøkkelen folder_prefix for å angi hvilken sti i kildebøtta som tjenesten skal trigges på. Nøkkelen memory_size lar brukeren angi hvor mye minne og CPU hver prosessering skal få.\n\n\n\n\n\n\nHvor mye minne og CPU er mulig?\n\n\n\nSom standard så får hver prosessering med Kildomaten 1 CPU-kjerne og 512MB minne/RAM. Hvis man skal gjøre mye prossesering, eller filene som prosesseres er veldig store, kan man sette memory_size opp til max 32GB minne. Antall CPU-kjerner blir satt implisitt ut i fra valg av memory_size iht til begrensningene som Google setter for Cloud Run. Se de nøyaktige verdiene som blir satt her.\n\n\n\n\n\n\n\n\n\n\n\nHusk dette når du skriver skriptet ditt\n\n\n\nNår du skal skrive et Python-skript for Kildomaten er det spesielt viktig å huske på 2 ting:\n\nSkriptet ditt kommer til å bli kjørt på en-og-en fil.\nSkriptet ditt må skrive ut et unikt navn på filen som skal skrives til produktbøtta, ellers risikerer du at filer blir overskrevet.\nPython-pakkene som kan benyttes er definert her. Ved behov for andre pakker, ta kontakt med Kundeservice.\n\n\n\n\nPython-skriptet er der teamet kan angi hva slags kode som skal kjøre på hver fil som dukker opp i den angitte mappen i kildebøtta. For at dette skal være mulig må koden følge disse reglene:\n\nKoden må ligge i en fil som heter process_source_data.py.\nKoden må pakkes inn i en funksjon som heter main().\n\nmain() er en funksjon med ett parameter som heter source_file som du alltid får av Kildomaten når en fil blir prosessert. Når du skriver koden kan man derfor anta at dette parameteret blir gitt til funksjonen, og du kan benytte det inne i main-funksjonen.\nsource_file-parameteret gir main() en ferdig definert filsti til filen som skal prosesseres. For eksempel så kan source_file ha verdien\ngs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv hvis filen test20231010.csv dukker opp i ssb-dapla-example-data-kilde-prod/ledstill/altinn/.\nHvis vi fortsetter eksempelet fra tidligere så ser mappen i IaC-repoet vårt slik ut nå:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\n├── automation/\n│   └── source-data/\n│       ├── dapla-example-prod/\n│           └── altinn/\n│               ├── config.yaml\n│               └── process_source_data.py\n│\n│...         \n\nVi ser nå at filene config.yaml og process_source_data.py ligger i mappen\nautomation/source-data/dapla-example-prod/ledstill/altinn/. Senere, når vi har rullet ut tjenesten, vil en ny fil i gs://ssb-dapla-exmaple-data-kilde-prod/ledstill/altinn/ trigge Kildomaten og kjøre koden i process_source_data.py på filen.\nUnder ser du et eksempel på hvordan en vanlig kodesnutt kan konverteres til å kjøre i Kildomaten:\n\n\n\n\nVanlig kode\n\nimport dapla as dp\n\n# Stien til filen\nsource_file = \"gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv\"\n\ndf = dp.read_pandas(source_file, file_type=\"csv\")\n\n# Dataminimerer ved å velge kolonner\ndf2 = df[['col1', 'col2', 'col3']]\n\n# Skriver ut fil til produktbøtta\ndp.write_pandas(df2, \n            \"gs://ssb-dapla-example-data-produkt-prod/test20231010.parquet\")\n\n\n\n\n\n\nKode i Kildomaten\n\nimport dapla as dp\n\ndef main(source_file):\n    df = dp.read_pandas(source_file, file_type=\"csv\")\n\n# Dataminimerer ved å velge kolonner\n    df2 = df[['col1', 'col2', 'col3']]\n\n# Bytter ut \"kilde\" med \"produkt\" i filstien\n    new_path = source_file.replace(\"kilde\", \"produkt\")\n\n# Bytter ut \".csv\" med \".parquet\" i filstien\n    new_path2 = new_path.replace(\".csv\", \".parquet\")\n\n# Skriver ut fil til produktbøtta\n    dp.write_pandas(df2, new_path2)\n\n\n\nDe to kodesnuttene over gir det samme resultatet, bare at koden til venstre kjøres som vanlig python-kode, mens koden til høyre kjøres i Kildomaten. Som vi ser av koden til høyre så trenger vi aldri å hardkode inn filstier i Kildomaten. Tjenesten gir oss filstien, og vi kan bruke den til å skrive ut filen til produktbøtta.\nStrukturen på filene som skrives bør tenkes nøye gjennom når man automatiseres prosessering av data. Hvis vi ikke har unike filnavn eller stier så kan du risikere at filer blir overskrevet. Typisk er dette allerede tenkt på når filer skrives til kildebøtta, så hvis man kopierer den strukturen, slik vi gjorde i eksempelet over, så vil det ikke være noe problem.\n\n\n\n\n\n\nHvilke Python-biblioteker kan jeg bruke?\n\n\n\nDu kan kun bruke biblioteker som er forhåndsinstallert i Kildomaten. Du kan se en liste over disse bibliotekene her. Ønsker du andre biblioteker så må du ta kontakt med Kunderservice.\n\n\n\n\n\n\n\n\n\n\n\n\nTilgang til kildedata\n\n\n\nTilgang til kildedata i prod-miljøet er det kun gruppen data-admins som kan aktivere ved å bruke tilgangsstyringsløsningen Just-in-Time Access (JIT). Les mer om hvordan JIT-løsningen fungerer her. Ønsker man å kunne liste ut innhold fra bøtta må man aktivere rollen ssb.buckets.list. Ønsker man i tillegg å lese/skrive til bøtta må man også aktivere ssb.bucket.write. Tilgang til kildebøtta i test-miljøet krever ikke JIT-tilgang.\n\n\nFør man ruller ut koden i tjenesten er det greit å teste at alt fungerer som det skal i Jupyterlab. Hvis vi tar utgangspunkt i eksempelet over så kan vi teste koden ved å kjøre følgende nederst i skriptet:\n\n\nprocess_source_data.py\n\nmain(\"gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv\")\n\nNår tjenesten er rullet ut så vil det være dette som kjøres når en fil dukker opp i gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/. Ved å kjøre det manuelt på denne måten får vi sett at ting fungerer som det skal.\nHusk å fjerne kjøringen av koden før du ruller ut tjenesten.\n\n\n\n\n\n\nPseudonymisering kan ikke kjøres fra en IDE i prod-miljøet\n\n\n\nDu kan teste kode fra Jupyter og andre IDE-er i prod-miljøet på Dapla. Men hvis prosesseringen innebærer bruk av pseudonymisering, så vil den ikke kunne kalles fra programmeringsmiljøer som Jupyter. Grunnen til dette er at det ikke er ønskelig å gjøre det lett å se upseudonymisert og pseudonymisert data samtidig. Hvis du ønsker å teste prosesseringen av pseudo-tjenesten, så kan du gjøre med testdata i test-miljøet.\n\n\n\n\n\nFor å rulle ut tjenesten gjør du følgende;\n\nPush branchen til GitHub og opprette en pull request.\nPull request må godkjennes av en data-admins på teamet.\n\n\n\n\nNår pull request er godkjent så sjekker du om alle tester, planer og utrullinger var vellykket, slik som vist i Figur 1.\nHvis alt er vellykket så kan du merge branchen inn i main-branchen.\n\n\n\n\n\n\n\n\n\nFigur 1: Suksessfulle tester på GitHub\n\n\n\n\n\nEtter at du har merget inn i main kan du følge med på utrullingen under Actions-fanen i repoet. Når den siste jobben lyser grønt er Kildomaten rullet ut og klar til bruk.\n\n\n\n\n\n\nVanlige feil ved utrulling\n\n\n\nFor å gi raskt tilbakemelding på noen mulige feilsituasjoner, så kjøres det enkel validering på config.yaml og process_source_data.py når en Pull request er opprettet. Følgende validering gjennomføres:\n\nEr det et python-script kalt process_source_data.py?\nEr det en config.yaml med en gyldig folder_prefix: definert?\nBruker process_source_data.py biblioteker som ikke er installert i Kildomaten?\nHar koden i process_source_data.py feil iht Pyflakes?\n\nDet kan også forekomme at Atlantis, verktøyet for å rulle ut endringer fra IaC-repoet til GCP, feiler. Da kan du prøve å skrive atlantis plan i kommentarfeltet til pull request’en, og testene vil kjøre på nytt. Hvis det fortsatt ikke fungerer så kontakter man Dapla kundeservice.\n\n\n\n\n\nNår du har rullet ut tjenesten kan teste tjenesten ved at data-admins flytter en fil til en den filstien Kildomaten er satt opp for. I eksempelet vi har brukt tidligere, gjør du følgende: 1. Aktiverer JIT-tilgangen til kildedata (som beskrevet over). 2. Kopier en fil over til filstien:\ngs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/ 3. Sjekk om filen ble skrevet til ønsket mappe i produktbøtta.\nDu kan også sjekke logger og monitorere Kildomaten. Les mer i neste avsnitt.\n\n\n\nNår en kilde i Kildomaten er satt opp og ruller ut, kan du monitere tjenesten i Google Cloud Console (GCC) ved å gjøre følgende:\n\nLogg deg inn med SSB-bruker på GCC.\nVelg standardprosjektet i prosjektvelgeren3.\nSøk opp Cloud Run i søkefeltet på toppen av siden og gå inn på siden.\n\nPå siden til Cloud Run vil du se en oversikt over alle kilder teamet har kjørende i Kildomaten. De har formen source-&lt;kildenavn&gt;-processor. I eksempelet med team dapla-example tidligere, vil man da se en kilde som heter source-altinn-processor, siden mappen de opprettet under\nautomation/source-data-dapla-example-prod/ i IaC-repoet heter altinn.\nTrykker man seg inn på hver enkelt kilde vil man kunne monitorere aktiviteten i kilden, og man kan trykke seg videre for å se loggene.\n\n\n\n\n\n\nSjekke logger\n\n\n\nDet er anbefalt å se på Kildomaten-loggene i Logs Explorer. Det kan man enkelt gjøre ved å trykke på “View in Logs Explorer” som vist på bildet under:\n\n\n\nÅpne Kildomaten-loggene i Logs Explorer\n\n\n\n\n\n\n\nKildomaten er satt opp for å kunne prosessere hver kilde i 5 parallelle intanser samtidig. F.eks. hvis det oppstår 10 nye filer i en mappe som trigger en Kildomaten-kilde, så kan det prosesseres 5 filer samtidig. Ta kontakt med Kundeservice hvis man har behov for ytterlige skalering.\n\n\n\nKildomaten tilbyr e-postvarsling til teamet når tjenesten feiler. Opprett en Kundeservice-sak for å få satt opp e-postvarsling for teamet ditt.\n\n\n\nMan kan sette opp så mange kilder man ønsker. Men når man setter det opp er det viktig å huske at alle kildene må spesifiseres rett under automation/source-data/&lt;teamnavn&gt;-&lt;miljø&gt;/. Her er et eksempel på hvordan team dapla-example har to kilder i Kildomaten:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n├── automation/\n│   └── source-data/\n│       ├── dapla-example-prod/\n│           └── altinn/\n│               ├── config.yaml\n│               └── process_source_data.py\n│           └── ledstill/\n│               ├── config.yaml\n│               └── process_source_data.py\n│...           \n\nI eksempelet over ser vi det er to kilder: altinn og ledstill. Hver kilde trigges på ulike filstier i kildebøtta, og python-koden som kjøres kan være ulik mellom kilder.\n\n\n\nI eksempelet som er brukt i dette kapitlet er Kildomaten satt opp i prodmiljøet til teamet. Mens egentlig burde man teste ut nye kilder i teamets test-miljø. Kildomaten er ikke satt opp i test-miljøet som standard, og derfor må det skrus på før man kan anvende det. Teamet kan gjøre det selv ved å følge denne beskrivelsen, eller de kan ta kontakt med Kundeservice og få hjelp til dette.\nEn av de store fordelene med å sette opp Kildomaten-kilder i test-miljøet før man gjør det i prod-miljøet, er at tilgangsstyringen til data er mye mindre streng. Det gjør det lettere for alle i teamet å utvikle koden som skal benyttes.\nNår man skal sette opp Kildomaten i test-miljøet så følger det samme oppskrift som vi har vist for prod-miljøet over. Den store forskjellen er at test-kilder skal legges under en egen mappe under automation/source-data/ i teamets IaC-repo. Eksempelet under med team dapla-example viser hvordan man kan sette opp kildene altinn og ledstill for både prod- og test-testmiljøet:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n├── automation/\n│   └── source-data/\n│       ├── dapla-example-prod/\n│       │       ├── altinn/\n│       │       │       ├── config.yaml\n│       │       │       └── process_source_data.py\n│       │       └── ledstill/\n│       │               ├── config.yaml\n│       │               └── process_source_data.py\n│       ├── dapla-example-test/\n│               ├── altinn/\n│               │       ├── config.yaml\n│               │       └── process_source_data.py\n│               └── ledstill/\n│                       ├── config.yaml\n│                       └── process_source_data.py\n│...           \n\nSom vi ser av mappestrukturen over så er det to mapper under\nautomation/source-data:\n\ndapla-example-prod\ndapla-example-test\n\nDisse to mappene angir om det er henholdsvis prod- eller test-miljøet vi setter opp kilder for.\n\n\n\nKildomaten er bygget for å trigge på nye filer som oppstår i en gitt filsti. Men noen ganger er det nødvendig å trigge kjøring av alle filer på nytt. Noen ganger ønsker man kanskje å kun trigge noen filer for en gitt kilde. Dette kan gjøres med en funksjon i Python-pakken dapla-toolbelt.\nFør du kan gjøre dette trenger du følgende informasjon:\n\nproject-id for standardprosjektet. Slik finner du prosjekt-id. Merk at dette ikke er kilde-prosjektet!\nfolder_prefix som du ønsker at koden skal trigges på. Dette fungerer likt som tidligere forklart for config.yaml, men her har du også mulighet til å kunne trigge prosesseringen på et undermappe av stien som var satt i kildens config.yaml.\nsource_name er navnet på kilden. Navnet på kilden i eksempelet med team dapla-example var altinn.\n\nHer er et kodeeksempel som viser hvordan man kan trigge prossesering av alle filer for kilde\n\n\nnotebook\n\nfrom dapla import trigger_source_data_processing\n\nproject_id = \"dapla-example-p\"\nsource_name = \"altinn\"\nfolder_prefix = \"ledstill/altinn/ra0678\"\n\ntrigger_source_data_processing(project_id, source_name, folder_prefix, kuben=True)          \n\nI eksempelet over trigger vi scriptet som er satt opp for kilden altinn til å kjøre på alle undermapper av\nssb-dapla-example-data-kilde-prod/ledstill/altinn/ra0678/.\n\n\n\n\nNår tjenesten er rullet ut så vil den kjøre automatisk på alle filer som dukker opp i filsti i kildebøtta. Etter hvert vil det være behov for å endre på skript, endre filstier som skal trigge tjenesten, eller prosessere alle filer på nytt. I denne delen forklarer vi hvordan du går frem for å gjøre dette.\n\n\nAlle på team kan endre på skriptet, men det er data-admins som må godkjenne endringene før de blir rullet ut. For å endre skriptet gjør du følgende:\n\nKlon repoet.\nGjør endringene du ønsker i en branch.\nPush opp endringene til GitHub og opprett en pull request.\nFå en data-admins på teamet til å godkjenne endringene.\nNår endringene er godkjent så kan du merge inn i main-branchen.\n\nEn endring i Python-skriptet krever ikke at tjenesten rulles ut på nytt. Derfor er det ikke like mange tester og kjøringer som gjøres som når man oppretter en helt ny kilde.\n\n\n\nAlle på teamet kan gjøre endringer i config.yaml, men det er data-admins som må godkjenne endringene før de blir rullet ut. For å endre config.yaml gjør du følgende:\n\nKlon repoet.\nGjør endringene du ønsker i en branch.\nPush opp endringene til GitHub og opprett en pull request.\nFå en data-admins på teamet til å godkjenne endringene.\nNår endringene er godkjent så kan du merge inn i main-branchen.\n\nEn endring i config.yaml krever at tjenesten rulles ut på nytt og tar derfor litt mer tid enn en endring i Python-skriptet.",
    "crumbs": [
      "Manual",
      "Datatjenester"
    ]
  },
  {
    "objectID": "statistikkere/kildomaten.html#forberedelser",
    "href": "statistikkere/kildomaten.html#forberedelser",
    "title": "Kildomaten",
    "section": "",
    "text": "Før et Dapla-team kan ta i bruk Kildomaten må man tjenesten aktivert for teamet. Som standard får alle statistikkteam dette skrudd på i prod-miljøet som opprettes for teamet. Ønsker du å aktivere Kildomaten i test-miljøet kan dette gjøres selvbetjent som en feature. Alternativt kan man opprette en Kundeservice-sak og be om å hjelp til dette.",
    "crumbs": [
      "Manual",
      "Datatjenester"
    ]
  },
  {
    "objectID": "statistikkere/kildomaten.html#sette-opp-tjenesten",
    "href": "statistikkere/kildomaten.html#sette-opp-tjenesten",
    "title": "Kildomaten",
    "section": "",
    "text": "I denne delen bryter vi ned prosessen med å sette opp Kildomaten i de stegene vi mener det er hensiktsmessig å gjøre det når den settes opp for første gang på en enkeltkilde. Senere i kapitlet forklarer vi hvordan man setter den opp flere kilder og hvordan man vedlikeholder tjenesten.\nHusk at alle på teamet kan gjøre det meste av arbeidet her, men det er data-admins som må godkjenne at tjenesten rulles ut1.\n\n\nOppsett av Kildomaten gjøres i teamets IaC-repo2. Når vi skal sette opp Kildomaten-kilde må vi gjøre gjøre endringer i teamets IaC-repo. Man finner teamets IaC-repo ved gå inn på SSBs GitHub-organisasjon og søke etter repoet som heter &lt;teamnavn&gt;-iac. Når du har funnet repoet så kan du gjøre følgende:\n\nKlon ned ditt teams Iac-repo git clone &lt;repo-url&gt;\nOpprett en ny branch: git checkout -b add-kildomaten-source\n\n\n\n\nFor at Kildomaten skal fungere så må det opprettes en bestemt mappestruktur i IaC-repoet til teamet. Når et team blir opprettet vil den grunnleggende mappestrukturen i IaC-repoet allerede være opprettet for prod-miljøet til teamet. F.eks. vil mappestrukturen se slik ut for team dapla-example:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n├── automation/\n│   └── source-data/\n│       ├── dapla-example-prod/\n│           └── README.md\n│ \n│...           \n\nSkal du sette opp Kildomaten i prod-miljøet så kan du følge oppskriften som kommer senere i kapitlet uten å gjøre noe mer enda.\nSkal du også bruke Kildomaten i test-miljøet så må opprette en ny mappe og lage en PR i IaC-repoet til teamet. Da vil strukturen se slik ut:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n├── automation/\n│   └── source-data/\n│       ├── dapla-example-prod/\n│       │    └── README.md\n│       ├── dapla-example-test/ \n│...           \n\nI mappestrukturen over så har vi klargjort den grunnleggende mappestrukturen for å ta i bruk Kildomaten i prod- og test-miljøet. Neste steg blir å legge de ulike kildene som egne mapper under dapla-example-prod og dapla-example-test. Det viser vi i neste avsnitt.\n\n\n\nKildomaten lar deg prosessere ulike filstier i kildebøtta med ulike python-script. Dette refereres til som at Kildomaten har flere kilder. For å sette opp en kilde så må man følge en definert mappestruktur i IaC-repoet der alle kildene ligger rett under &lt;teamnavn&gt;-prod- eller &lt;teamnavn&gt;-test-mappen. Du kan ikke ha undermapper under en kilde. Du velger selv navnet på kildene/mappene i IaC-repoet, og det vil være navnet på kildene i Kildomaten. Senere i kapitlet ser vi at vi må bruke navnet for trigge re-kjøring av kilder.\nUnder er et eksempel på hvordan det kan se ut for eksempel-teamet dapla-example:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n├── automation/\n│   └── source-data/\n│       ├── dapla-example-prod/\n│       │    └── altinn\n│       │    └── ameld\n│       ├── dapla-example-test/\n│       │    └── altinn\n│       │    └── ameld\n│       │    └── nudb\n│...           \n\nI eksempelet over ser vi at det er opprettet kildene altinn og ameld for både test- og prod-miljøet. I tillegg er det i test-miljøet kjørende en annen kilde som heter nudb. Hver av disse kildene kan kjøre et eget Pyton-script på alle filer som skrives til en gitt filsti som man definerer selv.\n\n\n\n\n\n\nAlle kilder på samme nivå\n\n\n\nEt team kan sette opp mange kilder som skal prosesseres med ulike skript. Men Kildomaten tillater bare et nivå under automation/source-data-&lt;teamnavn&gt;-&lt;miljø&gt;/. Det vil si at du ikke kan ha undermapper under en kilde. Det er også slik at man alltid må opprette en mappe for en kilde, selv om du kun har en kilde. F.eks. kan man ikke droppe mappen altinn i eksempelet over.\n\n\n\n\n\nNår mappestrukturen er opprettet kan man legge til filene som beskriver hvilke filer som skal prosesseres, og python-skriptet med koden som skal prosessere filene. Det er to filer som må eksistere for at tjenesten skal fungere:\n\nEn konfigurasjonsfil\nEt Python-skript\n\nNår du har opprettet de skal de ligge på denne måten i IaC-repoet:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n├── automation/\n│   └── source-data/\n│       ├── dapla-example-prod/\n│           └── altinn/\n│               ├── config.yaml\n│               └── process_source_data.py\n│...           \n\nUnder forklares hvordan skriver innholdet i de to filene.\n\n\nKildomaten trigges ved at det oppstår nye filer i kildebøtta til teamet. Hvorvidt den skal trigges på alle filer, eller kun filer i en gitt undermappe, bestemmer brukeren ved å konfigurere tjenesten i config.yaml. Her kan du også angi hvor mye ressurser prosesseringen skal få.\nHvis vi fortsetter eksempelet vårt fra tidligere med dapla-example, så kan vi tenkes oss at teamet ønsker å Kildomaten skal trigges på alle filer som oppstår i kildebøtta under filstien:\nssb-dapla-example-data-kilde-prod/ledstill/altinn/.\nFor å konfigurere tjenesten i Kildomaten må vi legge til en fil som heter config.yaml under automation/source-data/dapla-example-prod/altinn/, slik som vist i forrige kapitel. I dette tilfellet vil den ha innholdet som vist til venstre under:\n\n\n\n\nconfig.yaml\n\nfolder_prefix: ledstill/altinn\nmemory_size: 1  # GiB\n\n\n\n\n\n\nssb-dapla-example-data-kilde-prod/\n\n├── ledstill/\n│   └── altinn/\n│   └── aordningen/\n├── sykefra/\n│   └── altinn/\n│   └── freg/\n│...\n       \n\n\n\nMappestrukturen til høyre over viser hvordan vi mappestrukturen ser ut i kildebøtta, mens config.yaml-fila til venstre viser hvordan vi angir at Kildomaten kun skal trigges på nye filer som oppstår i undermappen ssb-dapla-example-data-kilde-prod/ledstill/altinn/. Vi bruker nøkkelen folder_prefix for å angi hvilken sti i kildebøtta som tjenesten skal trigges på. Nøkkelen memory_size lar brukeren angi hvor mye minne og CPU hver prosessering skal få.\n\n\n\n\n\n\nHvor mye minne og CPU er mulig?\n\n\n\nSom standard så får hver prosessering med Kildomaten 1 CPU-kjerne og 512MB minne/RAM. Hvis man skal gjøre mye prossesering, eller filene som prosesseres er veldig store, kan man sette memory_size opp til max 32GB minne. Antall CPU-kjerner blir satt implisitt ut i fra valg av memory_size iht til begrensningene som Google setter for Cloud Run. Se de nøyaktige verdiene som blir satt her.\n\n\n\n\n\n\n\n\n\n\n\nHusk dette når du skriver skriptet ditt\n\n\n\nNår du skal skrive et Python-skript for Kildomaten er det spesielt viktig å huske på 2 ting:\n\nSkriptet ditt kommer til å bli kjørt på en-og-en fil.\nSkriptet ditt må skrive ut et unikt navn på filen som skal skrives til produktbøtta, ellers risikerer du at filer blir overskrevet.\nPython-pakkene som kan benyttes er definert her. Ved behov for andre pakker, ta kontakt med Kundeservice.\n\n\n\n\nPython-skriptet er der teamet kan angi hva slags kode som skal kjøre på hver fil som dukker opp i den angitte mappen i kildebøtta. For at dette skal være mulig må koden følge disse reglene:\n\nKoden må ligge i en fil som heter process_source_data.py.\nKoden må pakkes inn i en funksjon som heter main().\n\nmain() er en funksjon med ett parameter som heter source_file som du alltid får av Kildomaten når en fil blir prosessert. Når du skriver koden kan man derfor anta at dette parameteret blir gitt til funksjonen, og du kan benytte det inne i main-funksjonen.\nsource_file-parameteret gir main() en ferdig definert filsti til filen som skal prosesseres. For eksempel så kan source_file ha verdien\ngs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv hvis filen test20231010.csv dukker opp i ssb-dapla-example-data-kilde-prod/ledstill/altinn/.\nHvis vi fortsetter eksempelet fra tidligere så ser mappen i IaC-repoet vårt slik ut nå:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\n├── automation/\n│   └── source-data/\n│       ├── dapla-example-prod/\n│           └── altinn/\n│               ├── config.yaml\n│               └── process_source_data.py\n│\n│...         \n\nVi ser nå at filene config.yaml og process_source_data.py ligger i mappen\nautomation/source-data/dapla-example-prod/ledstill/altinn/. Senere, når vi har rullet ut tjenesten, vil en ny fil i gs://ssb-dapla-exmaple-data-kilde-prod/ledstill/altinn/ trigge Kildomaten og kjøre koden i process_source_data.py på filen.\nUnder ser du et eksempel på hvordan en vanlig kodesnutt kan konverteres til å kjøre i Kildomaten:\n\n\n\n\nVanlig kode\n\nimport dapla as dp\n\n# Stien til filen\nsource_file = \"gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv\"\n\ndf = dp.read_pandas(source_file, file_type=\"csv\")\n\n# Dataminimerer ved å velge kolonner\ndf2 = df[['col1', 'col2', 'col3']]\n\n# Skriver ut fil til produktbøtta\ndp.write_pandas(df2, \n            \"gs://ssb-dapla-example-data-produkt-prod/test20231010.parquet\")\n\n\n\n\n\n\nKode i Kildomaten\n\nimport dapla as dp\n\ndef main(source_file):\n    df = dp.read_pandas(source_file, file_type=\"csv\")\n\n# Dataminimerer ved å velge kolonner\n    df2 = df[['col1', 'col2', 'col3']]\n\n# Bytter ut \"kilde\" med \"produkt\" i filstien\n    new_path = source_file.replace(\"kilde\", \"produkt\")\n\n# Bytter ut \".csv\" med \".parquet\" i filstien\n    new_path2 = new_path.replace(\".csv\", \".parquet\")\n\n# Skriver ut fil til produktbøtta\n    dp.write_pandas(df2, new_path2)\n\n\n\nDe to kodesnuttene over gir det samme resultatet, bare at koden til venstre kjøres som vanlig python-kode, mens koden til høyre kjøres i Kildomaten. Som vi ser av koden til høyre så trenger vi aldri å hardkode inn filstier i Kildomaten. Tjenesten gir oss filstien, og vi kan bruke den til å skrive ut filen til produktbøtta.\nStrukturen på filene som skrives bør tenkes nøye gjennom når man automatiseres prosessering av data. Hvis vi ikke har unike filnavn eller stier så kan du risikere at filer blir overskrevet. Typisk er dette allerede tenkt på når filer skrives til kildebøtta, så hvis man kopierer den strukturen, slik vi gjorde i eksempelet over, så vil det ikke være noe problem.\n\n\n\n\n\n\nHvilke Python-biblioteker kan jeg bruke?\n\n\n\nDu kan kun bruke biblioteker som er forhåndsinstallert i Kildomaten. Du kan se en liste over disse bibliotekene her. Ønsker du andre biblioteker så må du ta kontakt med Kunderservice.\n\n\n\n\n\n\n\n\n\n\n\n\nTilgang til kildedata\n\n\n\nTilgang til kildedata i prod-miljøet er det kun gruppen data-admins som kan aktivere ved å bruke tilgangsstyringsløsningen Just-in-Time Access (JIT). Les mer om hvordan JIT-løsningen fungerer her. Ønsker man å kunne liste ut innhold fra bøtta må man aktivere rollen ssb.buckets.list. Ønsker man i tillegg å lese/skrive til bøtta må man også aktivere ssb.bucket.write. Tilgang til kildebøtta i test-miljøet krever ikke JIT-tilgang.\n\n\nFør man ruller ut koden i tjenesten er det greit å teste at alt fungerer som det skal i Jupyterlab. Hvis vi tar utgangspunkt i eksempelet over så kan vi teste koden ved å kjøre følgende nederst i skriptet:\n\n\nprocess_source_data.py\n\nmain(\"gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv\")\n\nNår tjenesten er rullet ut så vil det være dette som kjøres når en fil dukker opp i gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/. Ved å kjøre det manuelt på denne måten får vi sett at ting fungerer som det skal.\nHusk å fjerne kjøringen av koden før du ruller ut tjenesten.\n\n\n\n\n\n\nPseudonymisering kan ikke kjøres fra en IDE i prod-miljøet\n\n\n\nDu kan teste kode fra Jupyter og andre IDE-er i prod-miljøet på Dapla. Men hvis prosesseringen innebærer bruk av pseudonymisering, så vil den ikke kunne kalles fra programmeringsmiljøer som Jupyter. Grunnen til dette er at det ikke er ønskelig å gjøre det lett å se upseudonymisert og pseudonymisert data samtidig. Hvis du ønsker å teste prosesseringen av pseudo-tjenesten, så kan du gjøre med testdata i test-miljøet.\n\n\n\n\n\nFor å rulle ut tjenesten gjør du følgende;\n\nPush branchen til GitHub og opprette en pull request.\nPull request må godkjennes av en data-admins på teamet.\n\n\n\n\nNår pull request er godkjent så sjekker du om alle tester, planer og utrullinger var vellykket, slik som vist i Figur 1.\nHvis alt er vellykket så kan du merge branchen inn i main-branchen.\n\n\n\n\n\n\n\n\n\nFigur 1: Suksessfulle tester på GitHub\n\n\n\n\n\nEtter at du har merget inn i main kan du følge med på utrullingen under Actions-fanen i repoet. Når den siste jobben lyser grønt er Kildomaten rullet ut og klar til bruk.\n\n\n\n\n\n\nVanlige feil ved utrulling\n\n\n\nFor å gi raskt tilbakemelding på noen mulige feilsituasjoner, så kjøres det enkel validering på config.yaml og process_source_data.py når en Pull request er opprettet. Følgende validering gjennomføres:\n\nEr det et python-script kalt process_source_data.py?\nEr det en config.yaml med en gyldig folder_prefix: definert?\nBruker process_source_data.py biblioteker som ikke er installert i Kildomaten?\nHar koden i process_source_data.py feil iht Pyflakes?\n\nDet kan også forekomme at Atlantis, verktøyet for å rulle ut endringer fra IaC-repoet til GCP, feiler. Da kan du prøve å skrive atlantis plan i kommentarfeltet til pull request’en, og testene vil kjøre på nytt. Hvis det fortsatt ikke fungerer så kontakter man Dapla kundeservice.\n\n\n\n\n\nNår du har rullet ut tjenesten kan teste tjenesten ved at data-admins flytter en fil til en den filstien Kildomaten er satt opp for. I eksempelet vi har brukt tidligere, gjør du følgende: 1. Aktiverer JIT-tilgangen til kildedata (som beskrevet over). 2. Kopier en fil over til filstien:\ngs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/ 3. Sjekk om filen ble skrevet til ønsket mappe i produktbøtta.\nDu kan også sjekke logger og monitorere Kildomaten. Les mer i neste avsnitt.\n\n\n\nNår en kilde i Kildomaten er satt opp og ruller ut, kan du monitere tjenesten i Google Cloud Console (GCC) ved å gjøre følgende:\n\nLogg deg inn med SSB-bruker på GCC.\nVelg standardprosjektet i prosjektvelgeren3.\nSøk opp Cloud Run i søkefeltet på toppen av siden og gå inn på siden.\n\nPå siden til Cloud Run vil du se en oversikt over alle kilder teamet har kjørende i Kildomaten. De har formen source-&lt;kildenavn&gt;-processor. I eksempelet med team dapla-example tidligere, vil man da se en kilde som heter source-altinn-processor, siden mappen de opprettet under\nautomation/source-data-dapla-example-prod/ i IaC-repoet heter altinn.\nTrykker man seg inn på hver enkelt kilde vil man kunne monitorere aktiviteten i kilden, og man kan trykke seg videre for å se loggene.\n\n\n\n\n\n\nSjekke logger\n\n\n\nDet er anbefalt å se på Kildomaten-loggene i Logs Explorer. Det kan man enkelt gjøre ved å trykke på “View in Logs Explorer” som vist på bildet under:\n\n\n\nÅpne Kildomaten-loggene i Logs Explorer\n\n\n\n\n\n\n\nKildomaten er satt opp for å kunne prosessere hver kilde i 5 parallelle intanser samtidig. F.eks. hvis det oppstår 10 nye filer i en mappe som trigger en Kildomaten-kilde, så kan det prosesseres 5 filer samtidig. Ta kontakt med Kundeservice hvis man har behov for ytterlige skalering.\n\n\n\nKildomaten tilbyr e-postvarsling til teamet når tjenesten feiler. Opprett en Kundeservice-sak for å få satt opp e-postvarsling for teamet ditt.\n\n\n\nMan kan sette opp så mange kilder man ønsker. Men når man setter det opp er det viktig å huske at alle kildene må spesifiseres rett under automation/source-data/&lt;teamnavn&gt;-&lt;miljø&gt;/. Her er et eksempel på hvordan team dapla-example har to kilder i Kildomaten:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n├── automation/\n│   └── source-data/\n│       ├── dapla-example-prod/\n│           └── altinn/\n│               ├── config.yaml\n│               └── process_source_data.py\n│           └── ledstill/\n│               ├── config.yaml\n│               └── process_source_data.py\n│...           \n\nI eksempelet over ser vi det er to kilder: altinn og ledstill. Hver kilde trigges på ulike filstier i kildebøtta, og python-koden som kjøres kan være ulik mellom kilder.\n\n\n\nI eksempelet som er brukt i dette kapitlet er Kildomaten satt opp i prodmiljøet til teamet. Mens egentlig burde man teste ut nye kilder i teamets test-miljø. Kildomaten er ikke satt opp i test-miljøet som standard, og derfor må det skrus på før man kan anvende det. Teamet kan gjøre det selv ved å følge denne beskrivelsen, eller de kan ta kontakt med Kundeservice og få hjelp til dette.\nEn av de store fordelene med å sette opp Kildomaten-kilder i test-miljøet før man gjør det i prod-miljøet, er at tilgangsstyringen til data er mye mindre streng. Det gjør det lettere for alle i teamet å utvikle koden som skal benyttes.\nNår man skal sette opp Kildomaten i test-miljøet så følger det samme oppskrift som vi har vist for prod-miljøet over. Den store forskjellen er at test-kilder skal legges under en egen mappe under automation/source-data/ i teamets IaC-repo. Eksempelet under med team dapla-example viser hvordan man kan sette opp kildene altinn og ledstill for både prod- og test-testmiljøet:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n├── automation/\n│   └── source-data/\n│       ├── dapla-example-prod/\n│       │       ├── altinn/\n│       │       │       ├── config.yaml\n│       │       │       └── process_source_data.py\n│       │       └── ledstill/\n│       │               ├── config.yaml\n│       │               └── process_source_data.py\n│       ├── dapla-example-test/\n│               ├── altinn/\n│               │       ├── config.yaml\n│               │       └── process_source_data.py\n│               └── ledstill/\n│                       ├── config.yaml\n│                       └── process_source_data.py\n│...           \n\nSom vi ser av mappestrukturen over så er det to mapper under\nautomation/source-data:\n\ndapla-example-prod\ndapla-example-test\n\nDisse to mappene angir om det er henholdsvis prod- eller test-miljøet vi setter opp kilder for.\n\n\n\nKildomaten er bygget for å trigge på nye filer som oppstår i en gitt filsti. Men noen ganger er det nødvendig å trigge kjøring av alle filer på nytt. Noen ganger ønsker man kanskje å kun trigge noen filer for en gitt kilde. Dette kan gjøres med en funksjon i Python-pakken dapla-toolbelt.\nFør du kan gjøre dette trenger du følgende informasjon:\n\nproject-id for standardprosjektet. Slik finner du prosjekt-id. Merk at dette ikke er kilde-prosjektet!\nfolder_prefix som du ønsker at koden skal trigges på. Dette fungerer likt som tidligere forklart for config.yaml, men her har du også mulighet til å kunne trigge prosesseringen på et undermappe av stien som var satt i kildens config.yaml.\nsource_name er navnet på kilden. Navnet på kilden i eksempelet med team dapla-example var altinn.\n\nHer er et kodeeksempel som viser hvordan man kan trigge prossesering av alle filer for kilde\n\n\nnotebook\n\nfrom dapla import trigger_source_data_processing\n\nproject_id = \"dapla-example-p\"\nsource_name = \"altinn\"\nfolder_prefix = \"ledstill/altinn/ra0678\"\n\ntrigger_source_data_processing(project_id, source_name, folder_prefix, kuben=True)          \n\nI eksempelet over trigger vi scriptet som er satt opp for kilden altinn til å kjøre på alle undermapper av\nssb-dapla-example-data-kilde-prod/ledstill/altinn/ra0678/.",
    "crumbs": [
      "Manual",
      "Datatjenester"
    ]
  },
  {
    "objectID": "statistikkere/kildomaten.html#vedlikehold",
    "href": "statistikkere/kildomaten.html#vedlikehold",
    "title": "Kildomaten",
    "section": "",
    "text": "Når tjenesten er rullet ut så vil den kjøre automatisk på alle filer som dukker opp i filsti i kildebøtta. Etter hvert vil det være behov for å endre på skript, endre filstier som skal trigge tjenesten, eller prosessere alle filer på nytt. I denne delen forklarer vi hvordan du går frem for å gjøre dette.\n\n\nAlle på team kan endre på skriptet, men det er data-admins som må godkjenne endringene før de blir rullet ut. For å endre skriptet gjør du følgende:\n\nKlon repoet.\nGjør endringene du ønsker i en branch.\nPush opp endringene til GitHub og opprett en pull request.\nFå en data-admins på teamet til å godkjenne endringene.\nNår endringene er godkjent så kan du merge inn i main-branchen.\n\nEn endring i Python-skriptet krever ikke at tjenesten rulles ut på nytt. Derfor er det ikke like mange tester og kjøringer som gjøres som når man oppretter en helt ny kilde.\n\n\n\nAlle på teamet kan gjøre endringer i config.yaml, men det er data-admins som må godkjenne endringene før de blir rullet ut. For å endre config.yaml gjør du følgende:\n\nKlon repoet.\nGjør endringene du ønsker i en branch.\nPush opp endringene til GitHub og opprett en pull request.\nFå en data-admins på teamet til å godkjenne endringene.\nNår endringene er godkjent så kan du merge inn i main-branchen.\n\nEn endring i config.yaml krever at tjenesten rulles ut på nytt og tar derfor litt mer tid enn en endring i Python-skriptet.",
    "crumbs": [
      "Manual",
      "Datatjenester"
    ]
  },
  {
    "objectID": "statistikkere/kildomaten.html#footnotes",
    "href": "statistikkere/kildomaten.html#footnotes",
    "title": "Kildomaten",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI tillegg er det data-admins som må teste tjenesten manuelt hvis det gjøres på skarpe data, siden det kun er data-admins som kan få tilgang til de dataene.↩︎\nInfrastructure-as-Code (IaC) er repo som definerer alle ressursene til teamet på Dapla. Alle Dapla-team har et eget IaC-repo på GiHub og du finner det ved å søke etter repoet -iac under statisticsnorway.↩︎\nStandardprosjektet har navnestrukturen &lt;teamnavn&gt;-p↩︎",
    "crumbs": [
      "Manual",
      "Datatjenester"
    ]
  },
  {
    "objectID": "statistikkere/lage-nettsider.html",
    "href": "statistikkere/lage-nettsider.html",
    "title": "Lage nettsider",
    "section": "",
    "text": "Lage nettsider"
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html",
    "href": "statistikkere/hva-er-dapla-team.html",
    "title": "Hva er Dapla-team?",
    "section": "",
    "text": "For å kunne jobbe på Dapla må man være en del av et Dapla-team. Et Dapla-team er en gruppe personer som har tilgang til spesifikke ressurser på Dapla. Ressursene kan være data, kode eller tjenester. Følgelig er teamet helt sentral for tilgangsstyringen på Dapla. Derfor er det viktig at alle som jobber på Dapla gjør seg godt kjent med innholdet i denne delen.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#opprette-dapla-team",
    "href": "statistikkere/hva-er-dapla-team.html#opprette-dapla-team",
    "title": "Hva er Dapla-team?",
    "section": "Opprette Dapla-team",
    "text": "Opprette Dapla-team\nAlle Dapla-team tilhører en seksjon og opprettes av seksjonslederen i den seksjonen. Dapla-team opprettes i applikasjonen Dapla-Ctrl.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#autonomitetsnivå",
    "href": "statistikkere/hva-er-dapla-team.html#autonomitetsnivå",
    "title": "Hva er Dapla-team?",
    "section": "Autonomitetsnivå",
    "text": "Autonomitetsnivå\nEt team på Dapla er i en av følgende autonomitetsnivåer:\n\nManaged\nSemi-Managed\nSelf-Managed\n\nNivået er definert i metadataene til teamet og vises i Teamvisningen i Dapla Ctrl. Det er kun plattformteamene som kan endre nivået til et team.\nFormålet med å definere autonomiten til et team er å tydeliggjøre hvem som har hvilket ansvar for de produktene teamet benytter på Dapla. Nivået settes når teamet opprettes, men kan også endres senere ved behov.\nEt Managed team benytter seg kun av tjenestene/features som tilbys av plattformen, og kan være sikker på at disse er satt opp i iht de krav som gjelder i SSB. Typisk vil de fleste statistikkteam være managed, og eksempler på tjenester er standard lagringsbøtter, Transfer Service, Kildomaten, osv.. Et Managed team kan kun benytte seg av tilgangsgruppene managers, data-admins og developers.\nEt Semi-Managed team benytter seg av tjenestene som tilbys på plattformen, akkurat som et Managed team, men de ønsker også ta i bruk noe funksjonalitet som ikke tilbys enda. F.eks. kan det være et statistikkproduserende team som ønsker å ta i bruk en Google-tjeneste som ikke tilbys på Dapla. I disse tilfellene kan teamet velge å ta et større ansvar for denne tjenesten og få noe bredere tilganger på plattformen. Ansvaret fordrer at teamet har kompetansen til å ta dette ansvaret, og de spesifikke detaljene vil avhenge hvilken tjeneste det er snakk om, og om de ønsker å benytte tjenesten i prod- eller test-miljøet til teamet.\nEt Self-Managed team vil typisk være team som består IT-utviklere med god kompetanse på skyutvikling i Google Cloud Platform. Teamet har kompetanse til å ta i bruk de tjenestene de mener er best for å løse sine oppgaver.\nI resten av dette kapitlet beskrives hovedsakelig Managed teams.\n\n\n\n\n\n\nAutonomitetsnivå og tilganger i IaC-repo\n\n\n\n\n\nSiden hvert team får definert sine ressurser i et eget IaC-repo, så er det nær sammenheng mellom Autonomitetsnivå og hvilke tilganger teamet har til å gjøre endringer i IaC-repoet. Tabell 1 viser hvilke tilganger de ulike nivåene gir.\n\n\n\nTabell 1: Autonomitet og tilganger i IaC-repo\n\n\n\n\n\n\n\n\n\n\n\n\nManaged\nSemi-Managed\nSelf-Managed\n\n\n\n\nKan lage PR på IaC-kode\nJa\nJa\nJa\n\n\nTilgang på IaC-kode\nKun yaml-“støttefiler” (team-info, iam, buckets-shared, projects) og filer de endrer (dapla-filer)\nJa\nJa\n\n\nKan ta i bruk funksjonalitet utover dapla-features\nNei\nJa\nJa\n\n\nIaC-filstruktur (under infra-mappen)\nPredefinert\nPredefinert + egne filer\nFritt\n\n\n\n\n\n\nLes mer her.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#roller-i-teamet",
    "href": "statistikkere/hva-er-dapla-team.html#roller-i-teamet",
    "title": "Hva er Dapla-team?",
    "section": "Roller i teamet",
    "text": "Roller i teamet\nMedlemskap i et Dapla-team gir tilgang på spesifikke ressurser på Dapla. Men siden kildedataene til alle team er klassifisert som sensitive, så kan ikke alle på teamet ha lik tilgang til alle ressurser. Av den grunn er det definert 3 ulike roller på et team. To av disse, data-admins og developers, er forbeholdt de som jobber med data på teamet. Mens den tredje, managers, skal innehas av de som er ansvarlige for teamet. I de fleste tilfeller vil managers være seksjonslederen som er ansvarlige for statistikkproduktene teamet leverer. Under forklarer vi nærmere hva de ulike rollene innebærer.\n\nManagers\nRollen managers skal bestå av en eller flere data-ansvarlige (ofte omtalt som data-eiere eller seksjonsledere). managers har ansvar for følgende i teamet:\n\nopprette teamet\nhvem i teamet som får tilgang til hvilke data og tjenester.\nat teamet følger SSBs retningslinjer for tilgangsstyring.\nat teamet følger SSBs retningslinjer for klassifisering av data.\nvedlikehold og monitorering av tilganger.\nat teamet følger og forstår hvordan sensitive data skal behandles i SSB.\n\nManager-rollen krever ingen tilgang til data eller databehandlende tjenester på Dapla.\n\n\nData-admins\nRollen data-admins er en priveligert rolle blant de som jobber med data i teamet. Rollen skal kun tildeles 2-3 personer på et team. data-admins har tilgang til samme data og ressurser som developers, med følgende unntak:\n\nde er forhåndsgodkjent til å gi seg selv tidsbegrenset tilgang til kildedata ved behov. Tilgang til kildedata skal kun aktiveres i særskilte tilfeller, der eneste løsning er å se på data i klartekst. Tilgang til kildedata skal kun gis i en begrenset periode, og krever en skriftlig begrunnelse. managers skal lett kunne monitere hvem som aktiverer denne tilgangen og hvor ofte.\nde kan godkjenne endringer i automatiske jobber som prosesserer kildedata til inndata.\nde kan overføre kildedata mellom bakke og sky.\n\n\n\nDevelopers\nRollen developers er den mest vanlige rollen på et Dapla-team. Denne rollen skal tildeles alle som jobber med data i teamet som ikke har data-admins-rollen. developers har tilgang til følgende ressurser:\n\nalt av teamets data, med unntak av kildedata.\nalle ressurser som behandler datatilstandene fra inndata til utdata.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#ressurser",
    "href": "statistikkere/hva-er-dapla-team.html#ressurser",
    "title": "Hva er Dapla-team?",
    "section": "Ressurser",
    "text": "Ressurser\nNår du oppretter et dapla-team så får man en grunnpakke med ressurser som de fleste i SSB vil trenge for å kunne jobbe med data på Dapla. I tillegg kan teamet selvbetjent skru på andre tjenester hvis man ønsker det. I det følgende forklarer vi hva som er inkludert i grunnpakken, og hva som er tilgjengelig for å skru på ved behov.\n\nGrunnpakken\nFigur 1 viser et overordnet bilde av hvilke ressurser som er inkludert i “grunnpakken”. Et Dapla-team får et testmiljø og prodmiljø. Det er i prodmiljøet at man jobber med skarpe data, mens testmiljøet er forbeholdt arbeid med testdata. I hvert miljø får teamet to Google-prosjekter. Ett for kildedata og et for datatilstandene inndata, klargjorte data, statistikkdata og utdata. Sistnevnte prosjekt kaller vi for standardprosjektet, siden det er her mesteparten av databehandlingen skjer.\n\n\n\n\n\n\nFigur 1: Diagram over hvilke miljøer, Google-prosjekter og bøtter et Dapla-team som et får ved opprettelse.\n\n\n\nAv Figur 1 ser vi at prosjektene i prodmiljøet får noen flere bøtter enn prosjektene i testmiljøet. Disse ekstrabøttene er forbeholdt synkronisering av data mellom bakke og sky, noe vi ikke legger til rette for i testmiljøet1. Les mer om overføring av data mellom bakke og sky her.\nRessursene som opprettes for et Dapla-team reflekterer i stor grad at kildedata er klassifisert som sensitive. Dette er grunnen til at det opprettes et eget prosjekt for kildedata, og at det kun er data-admins som potensielt kan få tilgang til dataene her. Opprettelsen av et eget testmiljø skyldes at Dapla-team i større grad enn før forventes å jobbe med testdata istedenfor skarpe data.\nAlle ressursene som opprettes for teamet er definert i tekstfiler i et GitHub-repo. Dette repoet kaller vi for et IaC-repo (Infrastructure as Code). IaC-repoet er en del av grunnpakken, og er tilgjengelig for alle på teamet. Statistikkere trenger ikke å forholde seg til dette repoet i stor grad, med unntak av når de skal aktivere/deaktivere features og når de skal sette opp Kildomaten.\n\n\nFeatures\nI tillegg til grunnpakken med ressurser, så kan teamet selvbetjent skru på følgende features eller tjenester ved behov:\n\nTransfer Service kan skrus på hvis teamet trenger å synkronisere data mellom ulike lagringssystemer. For eksempel mellom bakke og sky, eller mellom to ulike skytjenester.\nKildomaten kan skrus på hvis teamet trenger å automatisere overgangen fra kildedata til inndata.\nShared-buckets kan skrus på hvis teamet trenger å opprette delt-bøtter.\n\nForeløpig er det kun disse tre features som er tilgjengelig. Det vil komme flere etterhvert som behovene melder seg.\nLes mer om features her.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#github-team",
    "href": "statistikkere/hva-er-dapla-team.html#github-team",
    "title": "Hva er Dapla-team?",
    "section": "GitHub-team",
    "text": "GitHub-team\nVed opprettelsen av et Dapla-team så blir det også opprettet et tilsvarende GitHub-team med samme navn som Dapla-teamet. Grunnen til at det blir opprettet et GitHub-team er at GitHub er en sentral del av Dapla. Alle ressurser som skal opprettes på plattformen defineres av GitHub-repoer, og vi ønsker at tilganger her også skal reflektere tilgangene på Dapla.\nFor eksempel vil et team med navnet dapla-example få et GitHub-team med navnet dapla-example. Alle som er medlem av Dapla-teamet vil automatisk bli medlem av GitHub-teamet. I tillegg vil gruppetilhørighet og tilgangsroller på GitHub-teamet reflektere tilgangsroller på Dapla-teamet. For eksempel så kan dapla-example-data-admins gis tilgang til repo, og da vil alle som er medlem av Dapla-teamet med rollen data-admins få tilgang til repoet. Dette benyttes blant annet for å gi teamet tilgang til automation-mappen i sitt IaC-repo. I tillegg kan teamet bruke GitHub-teamet til å gi tilgang til andre GitHub-repoer som er relevante for teamet, for eksempel kodenbasen til en statistikkproduksjon eller lignende. Fordelen er at tilganger er gitt på teamnivå og ikke på personnivå. For eksempel hvis manager for teamet fjerner en ansatt fra developers-gruppa, så mister de all tilgang til data, tjenester og kode på GitHub som er tilgjengelig for developers.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#navnestruktur",
    "href": "statistikkere/hva-er-dapla-team.html#navnestruktur",
    "title": "Hva er Dapla-team?",
    "section": "Navnestruktur",
    "text": "Navnestruktur\nNår du oppretter et Dapla-team så må du velge et navn på teamet. Teamet velger selv et navn som reflekterer domene og subdomene. For eksempel kan et team som jobber med statistikkproduksjonen skattestatistikk for næringslivet velge å kalle teamet Skatt næring. Hvis vi bruker dette teamet som et eksempel, så vil det få opprettet et teknisk navn som følger denne strukturen: skatt-naering. Dette navnet er det som brukes i tekniske sammenhenger, for eksempel som navn på GitHub-teamet, IaC-repoet, Google-prosjektene og bøttene. Tabell 2 viser en tabell over hvordan ressursene for dette teamet vil se ut:\n\n\n\nTabell 2: Navnestruktur for teamet Skatt næring sine ressurser\n\n\n\n\n\n\n\n\n\nNavn\nBeskrivelse\n\n\n\n\nskatt-naering\nTeknisk teamnavn\n\n\nskatt-naering-managers\nAD-gruppe for managers\n\n\nskatt-naering-data-admins\nAD-gruppe for data-admins og et GitHub-team\n\n\nskatt-naering-developers\nAD-gruppe for developers og et GitHub-team\n\n\nskatt-naering-kilde-p\nNavn på kildeprosjekt i prod\n\n\nskatt-naering-p\nNavn på standardprosjekt i prod\n\n\nskatt-naering-kilde-t\nNavn på kildeprosjekt i test\n\n\nskatt-naering-t\nNavn på standardprosjekt i test\n\n\n\n\n\n\nI Tabell 2 ser vi at teamet får opprettet 3 AD-grupper og 4 Google-prosjekter. AD-gruppene brukes til å gi tilgang til ressursene på Dapla, mens Google-prosjektene brukes til å organisere ressursene. I tillegg er det en fast navnestruktur for bøttene i hvert prosjekt, slikt som vist i Tabell 3.\n\n\n\nTabell 3: Navnestruktur for teamet Skatt næring sine bøtter\n\n\n\n\n\nProsjektnavn\nBøttenavn\n\n\n\n\nskatt-naering-kilde-p\nssb-skatt-naering-data-kilde-prod\n\n\n\nssb-skatt-naering-data-kilde-frasky-prod\n\n\n\nssb-skatt-naering-data-kilde-tilsky-prod\n\n\nskatt-naering-p\nssb-skatt-naering-data-produkt-prod\n\n\n\nssb-skatt-naering-data-frasky-prod\n\n\n\nssb-skatt-naering-data-tilsky-prod\n\n\nskatt-naering-kilde-t\nssb-skatt-naering-data-kilde-test\n\n\nskatt-naering-t\nssb-skatt-naering-data-produkt-test",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#footnotes",
    "href": "statistikkere/hva-er-dapla-team.html#footnotes",
    "title": "Hva er Dapla-team?",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nTa kontakt med produkteier for Dapla hvis du trenger å synkronisere testdata mellom bakke og sky↩︎",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/dapla-pseudo.html",
    "href": "statistikkere/dapla-pseudo.html",
    "title": "dapla-toolbelt-pseudo",
    "section": "",
    "text": "dapla-toolbelt-pseudo er en python-pakke som har som sitt hovedformål å gi Dapla-brukere muligheten til å pseudonymisere, de-pseudonymisere og re-pseudonymisere data. Det skal sikre at brukerne av Dapla har verktøyene de trenger for å jobbe med direkte identifiserende opplysninger i henhold til lovverk og SSBs tolkninger av disse.\nSiden tilgang til direkte identifiserende opplysninger er underlagt strenge regler, så krever bruken av dapla-pseudo-toolbelt at man forholder seg til vedtatte standarder som datatilstander og systemer som Kildomaten. I tillegg er det en streng tilgangsstyring til hvor man kan kalle funksjonaliteten fra. Tjenestene er satt opp på en slik måte at Dapla-team skal være selvbetjent i bruken av funksjonaliteten, samtidig som regler, prosesser og standarder etterleves på enklest mulig måte.\n\n\n\n\n\n\nStandardisert klassifisering av datatilstander\n\n\n\nI SSB er det bestemt at all data skal klassifiseres på en standardisert måte basert på datatilstander for å avgjøre om de er sensitive, skjermet eller åpen. Den eneste datatilstanden som klassifiseres som sensitiv er kildedata. Det er derfor bestemt at pseudonymisering er en av prosesseringene som skal skje mellom datatilstandene kildedata og inndata.\n\n\n\n\nFør man tar i bruk funksjonaliteten er det viktig at man kjenner godt til tilgangstyring i Dapla-team og Kildomaten, og har diskutert med seksjonen hvordan man skal behandle direkte identifiserende opplysninger i de aktuelle dataene.\nFor at et Dapla-team skal kunne bruke dapla-toolbelt-pseudo må Kildomaten være skrudd på for miljøet1 man ønsker å jobbe fra. Som standard får alle statistikkteam skrudd på Kildomaten i prod-miljøet og ikke i test-miljøet. Ønsker du å aktivere Kildomaten i test-miljøet kan dette gjøres selvbetjent som en feature.\n\n\n\nTilgang til å funksjonalitet i dapla-toolbelt-pseudo kan regnes som sensitivt i seg selv, og derfor er det en streng tilgangsstyring for bruk av tjenesten. I prod-miljøet kan man kun ta i bruk funksjonaliteten ved å prosessere dataene i Kildomaten, og det er bare tilgangsgruppen data-admins som har tilgang til å godkjenne slike automatiske prosesseringer. I test-miljøet derimot kan alle på teamet benytte seg av all funksjonalitet, siden det aldri skal forekomme ekte data her.\n\n\n\nTabell 1: Tilgangsstyring til dapla-pseudo-toolbelt\n\n\n\n\n\n(a) Test-miljø\n\n\n\n\n\n\n\n\n\n\n\n\nAktør\nValidator\nPseudonymize()\nDepseudonymize()\nRepseudonymize()\n\n\n\n\nKildomaten\n✅\n✅\n✅\n✅\n\n\ndata-admins (interaktivt)\n✅\n✅\n✅\n✅\n\n\ndevelopers (interaktivt)\n✅\n✅\n✅\n✅\n\n\n\n\n\n\n\n\n\n(b) Prod-miljø\n\n\n\n\n\n\n\n\n\n\n\n\nAktør\nValidator\nPseudonymize()\nDepseudonymize()\nRepseudonymize()\n\n\n\n\nKildomaten\n✅\n✅\n🚫\n🚫\n\n\ndata-admins (interaktivt)\n🚫\n🚫\n🚫\n🚫\n\n\ndevelopers (interaktivt)\n🚫\n🚫\n🚫\n🚫\n\n\n\n\n\n\n\n\n\nI Tabell 1 ser vi fra Tabell 1 (a) at man i test-miljøet har full tilgang til funksjonaliteten i dapla-toolbelt-pseudo, både fra Kildomaten og når man jobber interaktivt2 i Jupyterlab. Tabell 1 (b) viser at det kun er tilgang til pseudonymize() og validator() fra Kildomaten i prod-miljøet, og man kan aldri interaktivt kan kalle på funksjoner som potensielt avslører et pseudonym. Av den grunn er det alltid anbefalt å teste ut koden sin i test-miljøet før den produksjonssettes i i prod-miljøet med Kildomaten.\n\n\n\nI denne delen viser vi hvilken funksjonalitet som tilbys gjennom dapla-toolbelt-pseudo. Eksempelkoden under viser hvordan man ville kjørt det fra en notebook i test-miljøet til teamet, og ikke hvordan koden må skrives når det skal kjøres i Kildomaten3.\n\n\ndapla-toolbelt-pseudo er ferdig installert i Kildomaten. Men ønsker du å bruke den i test-miljøet til teamet så kan du installere det i et ssb-project fra PyPI med denne kommandoen:\n\n\nTerminal\n\npoetry add dapla-toolbelt-pseudo\n\n\n\n\ndapla-toolbelt-pseudo støtter innlesing av følgende dataformater:\n\nCSV\nJSON\nPandas dataframes\nPolars dataframes\n\nEksemplene under viser hovedskelig innlesing av dataframes fra minnet4, men du kan lese mer om filbasert prosessering lenger ned i kapitlet.\n\n\n\nPseudonymisering tilbys via Pseudonymize-metoden i dapla-toolbelt-pseudo. Den følger et builder-pattern der man spesifiserer hva og i hvilken rekkefølge operasjonene skal gjøres. Anta at det finnes en Polars dataframe i minnet som heter df hvor kolonnen fnr skal pseudonymiseres. Da vil koden se slik ut:\n\n\nNotebook\n\nfrom dapla_pseudo import Pseudonymize\n\nresult_df = (\n    Pseudonymize.from_polars(df) \n    .on_fields(\"fnr\")\n    .with_default_encryption()                     \n    .run()                                \n    .to_polars()                                   \n)\n\nI koden over så angir from_polars(df) at kolonnen vi ønsker å pseudonymisere ligger i en Polars dataframe i minnet og heter df. Deretter spesifiserer vi at kolonnen fnr er den som skal behandles med funksjonen on_fields(\"fnr\"). Videre angir with_default_encryption() at fnr skal pseudonymiseres med dapla-toolbelt-speudo sin standard-algoritme5. Til slutt ber vi om at det ovennevnte blir kjørt med funksjonen run(), og at dataene skal returneres som en Polars dataframe med funksjonen to_polars().\nSe flere eksempler i dokumentasjonen.\n\n\n\nDe-pseudonymisering tilbys via Depseudonymize-metoden i dapla-toolbelt-pseudo. Den følger et builder-pattern der man spesifiserer hva og i hvilken rekkefølge operasjonene skal gjøres. Anta at det finnes i en Polars dataframe i minnet som heter df hvor kolonnen fnr skal de-pseudonymiseres. Da vil koden se slik ut:\n\n\nNotebook\n\nresult_df = (\n    Depseudonymize.from_polars(df)            \n    .on_fields(\"fnr\")                          \n    .with_default_encryption()                     \n    .run()                                         \n    .to_polars()                                   \n)\n\nOppbygningen av koden med Depseudonymize() er helt lik som for Pseudonomize(). Les beskrivelsen der for å se hva de ulike funksjonskallene gjør. Se flere eksempler i dokumentasjonen.\nDe-pseudonymisering er også støttet for informasjon som først er transformert til stabil ID og deretter pseudonymisert med Papis-nøkkelen. I disse tilfellene kan det også være behov for å spesifisere hvilken versjon av snr-katalogen man ønsker å benytte for å erstatte snr med fødselsnummer:\n\n\nNotebook\n\nfrom dapla_pseudo import Depseudonymize\n\nresult_df = (\n    Depseudonymize.from_pandas(df)            \n    .on_fields(\"fnr\")                          \n    .with_stable_id(\n      sid_snapshot_date=\"2023-05-29\")                    \n    .run()                                         \n    .to_pandas()                                   \n)\n\nI eksempelet over spesifiserer vi datoen 2023-05-29 og da benyttes snr-katalogen som ligger nærmest i tid til denne datoen. Hvis sid_snapshot_date ikke oppgis benyttes siste tilgjengelige versjon av katalogen.\n\n\n\n\n\n\nDe-pseudonymisering ikke tilgjengelig i prod-miljø\n\n\n\nForeløpig er det kun tilgang til å pseudonymisere i test-miljøet med test-data. Ta kontakt med Dapla-teamene dersom det dukker opp behov for å kunne de-pseudonymisere i prod-miljøet.\n\n\n\n\n\nUnder utvikling.\n\n\n\nI statistikkproduksjon og forskning er det viktig å kunne følge de samme personene over tid. Derfor har fødselsnummer ofte blitt oversatt til en mer stabilt identifikator, ofte kalt SNR eller stabil ID6. Funksjonene with_stable_id() og validator() bruker stabilID-katalogen til å henholdsvis bytte ut fødselsnummer med stabil ID, og for å validere om fødselsnummer er gyldige(se under).\nDu kan selv spesifisere hvilken versjon av SNR-katalogen du ønsker å bruke. Det gjør du ved å oppgi en gyldighetsdato og så finner dapla-toolbelt-pseudo hvilken versjon av katalogen som ligger nærmest i tid.\n\n\n\nDapla tilbyr samme pseudonym som Papis-prosjektet7. Denne kan benyttes på 2 måter:\n\nPseudonymisere hvilken som helst informasjon med samme nøkkel som Papis.\nTransformere fødselsnummer til snr-nummer og deretter pseudonymisere med samme nøkkel som Papis.\n\nPunkt 1 er nyttig for de som har pseudonymisert informasjon på bakken tidligere og vil ha samme pseudonym på Dapla8. Dette kan gjelde hvilken som helst informasjon, også direkte pseudonymisering av fødselsnummer, uten at det er gått via snr-nummer. Her er et eksempel på hvordan man pseudonymiserer på denne måten:\n\n\nNotebook\n\nresult = (\n    Pseudonymize.from_pandas(df)\n    .on_fields(\"fornavn\")                      \n    .with_papis_compatible_encryption()         \n    .run()                               \n    .to_pandas()                                  \n)\n\nPunkt 2 over er nok den som benyttes mest i SSB siden den sikrer at pseudonymisert fødselsnummer kan kobles med data som er pseudonymisert på bakken. Her er et eksempel på hvordan man pseudonymiserer snr med Papis-nøkkelen:\n\n\nNotebook\n\nresult = (\n    Pseudonymize.from_pandas(df)\n    .on_fields(\"fnr\")                      \n    .with_stable_id()         \n    .run()                               \n    .to_pandas()                                  \n)\n\n\n\n\n\n\n\nHva betyr det å tilby samme pseudonym?\n\n\n\nAt Papis og Dapla tilbyr samme pseudonum betyr egentlig at vi bruker samme krypteringsalgoritme, og en felles krypteringsnøkkel. Krypteringsalgoritmen som benyttes er formatpreserverende (FPE) og biblioteket som brukes er Tink FPE Python. En begrensning med algorimen er at kun karakterer som finnes i et forhåndsdefinert karaktersett (tall, store og små bokstaver fra a-z) blir vurdert. Andre karakterer f. eks æøå blir ikke kryptert. Papis-nøkkelen (som brukes f. eks for fnr og snr) benytter en SKIP-strategi for karakterer som faller utenom, som betyr at algoritmen simpelthen “hopper over” disse. FPE-algoritmen er også avhengig av størrelsen på det predefinterte karaktersettet for å avgjøre minimumslengden på teksten som pseudonymiseres. For Papis-nøkkelen betyr det at teksten minst må være 4 karakterer lang. Kortere tekster blir ikke kryptert.\n\n\n\n\n\nValidator-metoden kan benyttes til å sjekke om fødselsnummer finnes i SNR-katalogen (se over), og returnerer de ugyldige fødselsnummerne tilbake. Her kan man også spesifisere hvilken versjon av SNR-katalogen man ønsker å bruke. Standard, hvis ingenting velges, er at siste tilgjengelige versjon velges. Under er et eksempel på hvordan man validerer fødselsnummer for en gitt gyldighetsdato:\n\n\nNotebook\n\nfrom dapla_pseudo import Validator\n\nresult = (\n    Validator.from_polars(df)\n    .on_field(\"fnr\")\n    .validate_map_to_stable_id(\n        sid_snapshot_date=\"2023-08-29\"\n    )\n)\n# Vis hvilken versjon av SNR-katalogen som er benyttet\nresult.metadata\n# Vis fødselsnummer som ikke fikk treff i SNR-katalogen som en Polars dataframe\nresult.to_polars()\n\n\n\n\nSelv om mange av eksemplene i dette kapitlet viser hvordan man bruker dapla-toolbelt-pseudo ved å gi funksjonene et datasett fra minnet, så støtter den også å prosessere filer som er lagret på disk/bøtter. Dette kan være en fordel hvis dataene er for store for Kildomaten (&gt;32GB RAM) eller de har en dyp hierarkisk struktur (f.eks. json-filer).\ndapla-toolbelt-pseudo støtter følgende filtyper:\n\ncsv\njson\n\nHer er et eksempel på hvordan man pseudonymiserer en fil som ligger lagret på disk:\n\n\nNotebook\n\nfrom dapla_pseudo import Pseudonymize\nfrom dapla import AuthClient\n\n\nfile_path = \"gs://bucket/pseudo-examples/andeby_personer.csv\"\n\noptions = {\n    \"dtypes\": {\"fnr\": pl.Utf8, \"fornavn\": pl.Utf8, \"etternavn\": pl.Utf8, \"kjonn\": pl.Categorical, \"fodselsdato\": pl.Utf8}\n}\n\nresult_df = (\n    Pseudonymize.from_file(file_path)\n    .on_fields(\"fnr\")\n    .with_default_encryption()\n    .run()\n    .to_polars(**options)\n)\n\nSe flere eksempler i dokumentasjonen.\n\n\nKommer snart.\n\n\n\n\nKommer snart.\n\n\n\nKommer snart.\n\n\n\nDet genereres to typer av metadata når man pseudonymiserer:\n\nDatadoc\nMetadata\n\nDe to typene av metadata returneres til brukeren i to forskjellige objekter.\n\n\n\n\nDatadoc-metadata er på et format som er planlagt integrert i Datadoc9 på et senere tidspunkt. I koden til høyre så printes metadataene fra et kall til Pseudonomize ved å skrive print(result.datadoc). Da printer man objektet interaktivt i f.eks. Jupyterlab, noe som kun er mulig i test-miljøet. Skal man kjøre dette i Kildomaten så er det lettere å skrive filen direkte til riktig json-format med to_file-funksjonen. Da får får filen endelsen __DOC på slutten av filnavnet, og man slipper å tenke på om filen skrives med riktig formattering, osv..\n\n\n\n\n\nNotebook\n\nresult = (\n    Pseudonymize.from_polars(df)    \n    .on_fields(\"fnr\")           \n    .with_stable_id()\n    .run()                      \n)\n\n1print(result.datadoc)\n2result.to_file(\"gs://bucket/test.parquet\")\n\n\n1\n\nPrinter metadata i en Notebook.\n\n2\n\nSkriver metadata direkte til bøtte.\n\n\n\n\nNår man kjører pseudonymisering fra Kildomaten er det viktig å tenke på at felter som er pseudonymisert ikke må endres uten at metadataene også endrer. Da kan man risikere at metadatene ikke lenger beskriver riktig informasjon.\nUnder ser man hvilken informasjon som genereres fra pseudonymiseringen.\n\n\nDatadoc\n\n{\n  \"document_version\": \"0.0.1\",\n  \"datadoc\": null,\n  \"pseudonymization\": {\n    \"document_version\": \"0.1.0\",\n    \"pseudo_dataset\": null,\n    \"pseudo_variables\": [\n      {\n        \"short_name\": \"fnr\",   \n        \"data_element_path\": \"fnr\",\n        \"data_element_pattern\": \"**\",\n        \"stable_identifier_type\": \"FREG_SNR\",\n        \"stable_identifier_version\": \"2023-08-31\",\n        \"encryption_algorithm\": \"TINK-FPE\",\n        \"encryption_key_reference\": \"papis-common-key-1\",\n        \"encryption_algorithm_parameters\": [\n          {\n            \"keyId\": \"papis-common-key-1\"\n          },\n          {\n            \"strategy\": \"SKIP\"\n          }\n        ],\n        \"source_variable\": null,\n        \"source_variable_datatype\": null\n      }\n    ]\n  }\n}\n\nAv metadatene kan vi se fra pseudo_variables at det bare var feltet fnr som ble pseudonymisert. Vi ser også av stable_identifier_type ser vi at fnr ble oversatt til snr, og at versjonen av SNR-katalogen var fra 2023-08-31. encryption_algorithm angir at det var det var den formatpreserverende algoritmen TINK-FPE som ble benyttet. keyID: \"papis-common-key-1\" angir hvilken nøkkel-id som ble benyttet. strategy: \"SKIP\" refererer til at den format-preserverende algoritmen skal “hoppe over” ugyldige karakterer og la de være som de er.\nDenne informasjonen vil være svært nyttig i SSB hvis man senere skal kunne de-pseudonymisere eller re-pseudonymisere data.\n\n\n\nDen andre typen av metadata kan hentes ut etter et kall til Pseudonymize med kommandoen result.metadata. Den returnerer en python dictionary. Den inneholder hovedsaklig logginformasjon og metrikker foreløpig. For de som pseudonymiserer med with_stable_id() kan output se slik ut:\n\n\nMetadata\n\n{\n    'fnr': {\n        'logs': [\n            'No SID-mapping found for fnr 999999*****',\n            'No SID-mapping found for fnr XX',\n            'No SID-mapping found for fnr X8b7k2*'\n        ],\n        'metrics': [\n            {'MAPPED_SID': 10},\n            {'MISSING_SID': 3}\n        ]\n    }\n}\n\nI Kildomaten kan det vært nyttig å printe denne informasjonen til loggene. Av eksempelet over ser vi at verdier som er over 6 karakterer lange blir maskert.\n\n\n\n\n\nPå grunn av den strenge tilgangsstyringen til dapla-pseudo-toolbelt og kildedata er det anbefalt å utvikle kode for overgangen fra kildedata til inndata i test-miljøet til teamet. I denne delen viser vi hvordan denne arbeidsflyten kan se ut, fra testing til en automatisert produksjon med ekte data, med et helt konkret eksempel.\n\n\nFor å kunne kjøre pseudonymiseringen interaktivt i f.eks. en notebook i Jupyter, så må man jobbe i test-miljøet til teamet.\n\n\n\nKommer snart.\n\n\n\nKommer snart.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-pseudo"
    ]
  },
  {
    "objectID": "statistikkere/dapla-pseudo.html#forberedelser",
    "href": "statistikkere/dapla-pseudo.html#forberedelser",
    "title": "dapla-toolbelt-pseudo",
    "section": "",
    "text": "Før man tar i bruk funksjonaliteten er det viktig at man kjenner godt til tilgangstyring i Dapla-team og Kildomaten, og har diskutert med seksjonen hvordan man skal behandle direkte identifiserende opplysninger i de aktuelle dataene.\nFor at et Dapla-team skal kunne bruke dapla-toolbelt-pseudo må Kildomaten være skrudd på for miljøet1 man ønsker å jobbe fra. Som standard får alle statistikkteam skrudd på Kildomaten i prod-miljøet og ikke i test-miljøet. Ønsker du å aktivere Kildomaten i test-miljøet kan dette gjøres selvbetjent som en feature.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-pseudo"
    ]
  },
  {
    "objectID": "statistikkere/dapla-pseudo.html#tilgangsstyring",
    "href": "statistikkere/dapla-pseudo.html#tilgangsstyring",
    "title": "dapla-toolbelt-pseudo",
    "section": "",
    "text": "Tilgang til å funksjonalitet i dapla-toolbelt-pseudo kan regnes som sensitivt i seg selv, og derfor er det en streng tilgangsstyring for bruk av tjenesten. I prod-miljøet kan man kun ta i bruk funksjonaliteten ved å prosessere dataene i Kildomaten, og det er bare tilgangsgruppen data-admins som har tilgang til å godkjenne slike automatiske prosesseringer. I test-miljøet derimot kan alle på teamet benytte seg av all funksjonalitet, siden det aldri skal forekomme ekte data her.\n\n\n\nTabell 1: Tilgangsstyring til dapla-pseudo-toolbelt\n\n\n\n\n\n(a) Test-miljø\n\n\n\n\n\n\n\n\n\n\n\n\nAktør\nValidator\nPseudonymize()\nDepseudonymize()\nRepseudonymize()\n\n\n\n\nKildomaten\n✅\n✅\n✅\n✅\n\n\ndata-admins (interaktivt)\n✅\n✅\n✅\n✅\n\n\ndevelopers (interaktivt)\n✅\n✅\n✅\n✅\n\n\n\n\n\n\n\n\n\n(b) Prod-miljø\n\n\n\n\n\n\n\n\n\n\n\n\nAktør\nValidator\nPseudonymize()\nDepseudonymize()\nRepseudonymize()\n\n\n\n\nKildomaten\n✅\n✅\n🚫\n🚫\n\n\ndata-admins (interaktivt)\n🚫\n🚫\n🚫\n🚫\n\n\ndevelopers (interaktivt)\n🚫\n🚫\n🚫\n🚫\n\n\n\n\n\n\n\n\n\nI Tabell 1 ser vi fra Tabell 1 (a) at man i test-miljøet har full tilgang til funksjonaliteten i dapla-toolbelt-pseudo, både fra Kildomaten og når man jobber interaktivt2 i Jupyterlab. Tabell 1 (b) viser at det kun er tilgang til pseudonymize() og validator() fra Kildomaten i prod-miljøet, og man kan aldri interaktivt kan kalle på funksjoner som potensielt avslører et pseudonym. Av den grunn er det alltid anbefalt å teste ut koden sin i test-miljøet før den produksjonssettes i i prod-miljøet med Kildomaten.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-pseudo"
    ]
  },
  {
    "objectID": "statistikkere/dapla-pseudo.html#funksjonalitet",
    "href": "statistikkere/dapla-pseudo.html#funksjonalitet",
    "title": "dapla-toolbelt-pseudo",
    "section": "",
    "text": "I denne delen viser vi hvilken funksjonalitet som tilbys gjennom dapla-toolbelt-pseudo. Eksempelkoden under viser hvordan man ville kjørt det fra en notebook i test-miljøet til teamet, og ikke hvordan koden må skrives når det skal kjøres i Kildomaten3.\n\n\ndapla-toolbelt-pseudo er ferdig installert i Kildomaten. Men ønsker du å bruke den i test-miljøet til teamet så kan du installere det i et ssb-project fra PyPI med denne kommandoen:\n\n\nTerminal\n\npoetry add dapla-toolbelt-pseudo\n\n\n\n\ndapla-toolbelt-pseudo støtter innlesing av følgende dataformater:\n\nCSV\nJSON\nPandas dataframes\nPolars dataframes\n\nEksemplene under viser hovedskelig innlesing av dataframes fra minnet4, men du kan lese mer om filbasert prosessering lenger ned i kapitlet.\n\n\n\nPseudonymisering tilbys via Pseudonymize-metoden i dapla-toolbelt-pseudo. Den følger et builder-pattern der man spesifiserer hva og i hvilken rekkefølge operasjonene skal gjøres. Anta at det finnes en Polars dataframe i minnet som heter df hvor kolonnen fnr skal pseudonymiseres. Da vil koden se slik ut:\n\n\nNotebook\n\nfrom dapla_pseudo import Pseudonymize\n\nresult_df = (\n    Pseudonymize.from_polars(df) \n    .on_fields(\"fnr\")\n    .with_default_encryption()                     \n    .run()                                \n    .to_polars()                                   \n)\n\nI koden over så angir from_polars(df) at kolonnen vi ønsker å pseudonymisere ligger i en Polars dataframe i minnet og heter df. Deretter spesifiserer vi at kolonnen fnr er den som skal behandles med funksjonen on_fields(\"fnr\"). Videre angir with_default_encryption() at fnr skal pseudonymiseres med dapla-toolbelt-speudo sin standard-algoritme5. Til slutt ber vi om at det ovennevnte blir kjørt med funksjonen run(), og at dataene skal returneres som en Polars dataframe med funksjonen to_polars().\nSe flere eksempler i dokumentasjonen.\n\n\n\nDe-pseudonymisering tilbys via Depseudonymize-metoden i dapla-toolbelt-pseudo. Den følger et builder-pattern der man spesifiserer hva og i hvilken rekkefølge operasjonene skal gjøres. Anta at det finnes i en Polars dataframe i minnet som heter df hvor kolonnen fnr skal de-pseudonymiseres. Da vil koden se slik ut:\n\n\nNotebook\n\nresult_df = (\n    Depseudonymize.from_polars(df)            \n    .on_fields(\"fnr\")                          \n    .with_default_encryption()                     \n    .run()                                         \n    .to_polars()                                   \n)\n\nOppbygningen av koden med Depseudonymize() er helt lik som for Pseudonomize(). Les beskrivelsen der for å se hva de ulike funksjonskallene gjør. Se flere eksempler i dokumentasjonen.\nDe-pseudonymisering er også støttet for informasjon som først er transformert til stabil ID og deretter pseudonymisert med Papis-nøkkelen. I disse tilfellene kan det også være behov for å spesifisere hvilken versjon av snr-katalogen man ønsker å benytte for å erstatte snr med fødselsnummer:\n\n\nNotebook\n\nfrom dapla_pseudo import Depseudonymize\n\nresult_df = (\n    Depseudonymize.from_pandas(df)            \n    .on_fields(\"fnr\")                          \n    .with_stable_id(\n      sid_snapshot_date=\"2023-05-29\")                    \n    .run()                                         \n    .to_pandas()                                   \n)\n\nI eksempelet over spesifiserer vi datoen 2023-05-29 og da benyttes snr-katalogen som ligger nærmest i tid til denne datoen. Hvis sid_snapshot_date ikke oppgis benyttes siste tilgjengelige versjon av katalogen.\n\n\n\n\n\n\nDe-pseudonymisering ikke tilgjengelig i prod-miljø\n\n\n\nForeløpig er det kun tilgang til å pseudonymisere i test-miljøet med test-data. Ta kontakt med Dapla-teamene dersom det dukker opp behov for å kunne de-pseudonymisere i prod-miljøet.\n\n\n\n\n\nUnder utvikling.\n\n\n\nI statistikkproduksjon og forskning er det viktig å kunne følge de samme personene over tid. Derfor har fødselsnummer ofte blitt oversatt til en mer stabilt identifikator, ofte kalt SNR eller stabil ID6. Funksjonene with_stable_id() og validator() bruker stabilID-katalogen til å henholdsvis bytte ut fødselsnummer med stabil ID, og for å validere om fødselsnummer er gyldige(se under).\nDu kan selv spesifisere hvilken versjon av SNR-katalogen du ønsker å bruke. Det gjør du ved å oppgi en gyldighetsdato og så finner dapla-toolbelt-pseudo hvilken versjon av katalogen som ligger nærmest i tid.\n\n\n\nDapla tilbyr samme pseudonym som Papis-prosjektet7. Denne kan benyttes på 2 måter:\n\nPseudonymisere hvilken som helst informasjon med samme nøkkel som Papis.\nTransformere fødselsnummer til snr-nummer og deretter pseudonymisere med samme nøkkel som Papis.\n\nPunkt 1 er nyttig for de som har pseudonymisert informasjon på bakken tidligere og vil ha samme pseudonym på Dapla8. Dette kan gjelde hvilken som helst informasjon, også direkte pseudonymisering av fødselsnummer, uten at det er gått via snr-nummer. Her er et eksempel på hvordan man pseudonymiserer på denne måten:\n\n\nNotebook\n\nresult = (\n    Pseudonymize.from_pandas(df)\n    .on_fields(\"fornavn\")                      \n    .with_papis_compatible_encryption()         \n    .run()                               \n    .to_pandas()                                  \n)\n\nPunkt 2 over er nok den som benyttes mest i SSB siden den sikrer at pseudonymisert fødselsnummer kan kobles med data som er pseudonymisert på bakken. Her er et eksempel på hvordan man pseudonymiserer snr med Papis-nøkkelen:\n\n\nNotebook\n\nresult = (\n    Pseudonymize.from_pandas(df)\n    .on_fields(\"fnr\")                      \n    .with_stable_id()         \n    .run()                               \n    .to_pandas()                                  \n)\n\n\n\n\n\n\n\nHva betyr det å tilby samme pseudonym?\n\n\n\nAt Papis og Dapla tilbyr samme pseudonum betyr egentlig at vi bruker samme krypteringsalgoritme, og en felles krypteringsnøkkel. Krypteringsalgoritmen som benyttes er formatpreserverende (FPE) og biblioteket som brukes er Tink FPE Python. En begrensning med algorimen er at kun karakterer som finnes i et forhåndsdefinert karaktersett (tall, store og små bokstaver fra a-z) blir vurdert. Andre karakterer f. eks æøå blir ikke kryptert. Papis-nøkkelen (som brukes f. eks for fnr og snr) benytter en SKIP-strategi for karakterer som faller utenom, som betyr at algoritmen simpelthen “hopper over” disse. FPE-algoritmen er også avhengig av størrelsen på det predefinterte karaktersettet for å avgjøre minimumslengden på teksten som pseudonymiseres. For Papis-nøkkelen betyr det at teksten minst må være 4 karakterer lang. Kortere tekster blir ikke kryptert.\n\n\n\n\n\nValidator-metoden kan benyttes til å sjekke om fødselsnummer finnes i SNR-katalogen (se over), og returnerer de ugyldige fødselsnummerne tilbake. Her kan man også spesifisere hvilken versjon av SNR-katalogen man ønsker å bruke. Standard, hvis ingenting velges, er at siste tilgjengelige versjon velges. Under er et eksempel på hvordan man validerer fødselsnummer for en gitt gyldighetsdato:\n\n\nNotebook\n\nfrom dapla_pseudo import Validator\n\nresult = (\n    Validator.from_polars(df)\n    .on_field(\"fnr\")\n    .validate_map_to_stable_id(\n        sid_snapshot_date=\"2023-08-29\"\n    )\n)\n# Vis hvilken versjon av SNR-katalogen som er benyttet\nresult.metadata\n# Vis fødselsnummer som ikke fikk treff i SNR-katalogen som en Polars dataframe\nresult.to_polars()\n\n\n\n\nSelv om mange av eksemplene i dette kapitlet viser hvordan man bruker dapla-toolbelt-pseudo ved å gi funksjonene et datasett fra minnet, så støtter den også å prosessere filer som er lagret på disk/bøtter. Dette kan være en fordel hvis dataene er for store for Kildomaten (&gt;32GB RAM) eller de har en dyp hierarkisk struktur (f.eks. json-filer).\ndapla-toolbelt-pseudo støtter følgende filtyper:\n\ncsv\njson\n\nHer er et eksempel på hvordan man pseudonymiserer en fil som ligger lagret på disk:\n\n\nNotebook\n\nfrom dapla_pseudo import Pseudonymize\nfrom dapla import AuthClient\n\n\nfile_path = \"gs://bucket/pseudo-examples/andeby_personer.csv\"\n\noptions = {\n    \"dtypes\": {\"fnr\": pl.Utf8, \"fornavn\": pl.Utf8, \"etternavn\": pl.Utf8, \"kjonn\": pl.Categorical, \"fodselsdato\": pl.Utf8}\n}\n\nresult_df = (\n    Pseudonymize.from_file(file_path)\n    .on_fields(\"fnr\")\n    .with_default_encryption()\n    .run()\n    .to_polars(**options)\n)\n\nSe flere eksempler i dokumentasjonen.\n\n\nKommer snart.\n\n\n\n\nKommer snart.\n\n\n\nKommer snart.\n\n\n\nDet genereres to typer av metadata når man pseudonymiserer:\n\nDatadoc\nMetadata\n\nDe to typene av metadata returneres til brukeren i to forskjellige objekter.\n\n\n\n\nDatadoc-metadata er på et format som er planlagt integrert i Datadoc9 på et senere tidspunkt. I koden til høyre så printes metadataene fra et kall til Pseudonomize ved å skrive print(result.datadoc). Da printer man objektet interaktivt i f.eks. Jupyterlab, noe som kun er mulig i test-miljøet. Skal man kjøre dette i Kildomaten så er det lettere å skrive filen direkte til riktig json-format med to_file-funksjonen. Da får får filen endelsen __DOC på slutten av filnavnet, og man slipper å tenke på om filen skrives med riktig formattering, osv..\n\n\n\n\n\nNotebook\n\nresult = (\n    Pseudonymize.from_polars(df)    \n    .on_fields(\"fnr\")           \n    .with_stable_id()\n    .run()                      \n)\n\n1print(result.datadoc)\n2result.to_file(\"gs://bucket/test.parquet\")\n\n\n1\n\nPrinter metadata i en Notebook.\n\n2\n\nSkriver metadata direkte til bøtte.\n\n\n\n\nNår man kjører pseudonymisering fra Kildomaten er det viktig å tenke på at felter som er pseudonymisert ikke må endres uten at metadataene også endrer. Da kan man risikere at metadatene ikke lenger beskriver riktig informasjon.\nUnder ser man hvilken informasjon som genereres fra pseudonymiseringen.\n\n\nDatadoc\n\n{\n  \"document_version\": \"0.0.1\",\n  \"datadoc\": null,\n  \"pseudonymization\": {\n    \"document_version\": \"0.1.0\",\n    \"pseudo_dataset\": null,\n    \"pseudo_variables\": [\n      {\n        \"short_name\": \"fnr\",   \n        \"data_element_path\": \"fnr\",\n        \"data_element_pattern\": \"**\",\n        \"stable_identifier_type\": \"FREG_SNR\",\n        \"stable_identifier_version\": \"2023-08-31\",\n        \"encryption_algorithm\": \"TINK-FPE\",\n        \"encryption_key_reference\": \"papis-common-key-1\",\n        \"encryption_algorithm_parameters\": [\n          {\n            \"keyId\": \"papis-common-key-1\"\n          },\n          {\n            \"strategy\": \"SKIP\"\n          }\n        ],\n        \"source_variable\": null,\n        \"source_variable_datatype\": null\n      }\n    ]\n  }\n}\n\nAv metadatene kan vi se fra pseudo_variables at det bare var feltet fnr som ble pseudonymisert. Vi ser også av stable_identifier_type ser vi at fnr ble oversatt til snr, og at versjonen av SNR-katalogen var fra 2023-08-31. encryption_algorithm angir at det var det var den formatpreserverende algoritmen TINK-FPE som ble benyttet. keyID: \"papis-common-key-1\" angir hvilken nøkkel-id som ble benyttet. strategy: \"SKIP\" refererer til at den format-preserverende algoritmen skal “hoppe over” ugyldige karakterer og la de være som de er.\nDenne informasjonen vil være svært nyttig i SSB hvis man senere skal kunne de-pseudonymisere eller re-pseudonymisere data.\n\n\n\nDen andre typen av metadata kan hentes ut etter et kall til Pseudonymize med kommandoen result.metadata. Den returnerer en python dictionary. Den inneholder hovedsaklig logginformasjon og metrikker foreløpig. For de som pseudonymiserer med with_stable_id() kan output se slik ut:\n\n\nMetadata\n\n{\n    'fnr': {\n        'logs': [\n            'No SID-mapping found for fnr 999999*****',\n            'No SID-mapping found for fnr XX',\n            'No SID-mapping found for fnr X8b7k2*'\n        ],\n        'metrics': [\n            {'MAPPED_SID': 10},\n            {'MISSING_SID': 3}\n        ]\n    }\n}\n\nI Kildomaten kan det vært nyttig å printe denne informasjonen til loggene. Av eksempelet over ser vi at verdier som er over 6 karakterer lange blir maskert.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-pseudo"
    ]
  },
  {
    "objectID": "statistikkere/dapla-pseudo.html#brukerveiledning",
    "href": "statistikkere/dapla-pseudo.html#brukerveiledning",
    "title": "dapla-toolbelt-pseudo",
    "section": "",
    "text": "På grunn av den strenge tilgangsstyringen til dapla-pseudo-toolbelt og kildedata er det anbefalt å utvikle kode for overgangen fra kildedata til inndata i test-miljøet til teamet. I denne delen viser vi hvordan denne arbeidsflyten kan se ut, fra testing til en automatisert produksjon med ekte data, med et helt konkret eksempel.\n\n\nFor å kunne kjøre pseudonymiseringen interaktivt i f.eks. en notebook i Jupyter, så må man jobbe i test-miljøet til teamet.\n\n\n\nKommer snart.\n\n\n\nKommer snart.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-pseudo"
    ]
  },
  {
    "objectID": "statistikkere/dapla-pseudo.html#footnotes",
    "href": "statistikkere/dapla-pseudo.html#footnotes",
    "title": "dapla-toolbelt-pseudo",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nEt Dapla-team har både et test- og et prod-miljø. Kildomaten må være skrudd på i det miljøet du ønkser å benytte dapla-toolbelt-pseudo fra.↩︎\nMed interaktiv jobbing menes at man skriver og kode og får tilbake output i samme verktøy. F.eks. er Jupyterlab et eksempel på et verktøy som lar deg jobbe interaktivt med data.↩︎\nI Kildomaten må koden blant annet pakke inn i main()-funksjon.↩︎\nPandas og Polars dataframes er eksempler på dataformater som lever i minnet, og må konverteres før de skrives til et lagringsommråde. I praksis vil det ofte si at man jobber med dataframes når man jobber i verktøy som Jupyterlab, mens man skriver til lagringsområde når man er ferdig i Jupyterlab.↩︎\nStandardalgoritmen i dapla-toolbelt-pseudo er den deterministiske krypteringsalgoritmen Deterministic Authenticated Encryption with Associated Data, eller DAEAD-algoritmen.↩︎\nSNR-katalogen eies og tilbys av Team Register på Dapla.↩︎\nPapis var et prosjekt med fokus på bakkesystemene i SSB som skulle bringe SSBs behandling av personopplysninger, som brukes i statistikkproduksjon, i samsvar med GDPR gjennom en enhetlig pseudonymiseringsløsning.↩︎\nGenerelt sett er det ikke å anbefale å benytte denne nøkkelen på annen informasjon enn fødselsnummer. Grunnen er at den er svakere enn andre algoritmer, der blant annet tekst som er kortere enn 4 karakter lang ikke blir pseudonymisert.↩︎\nDatadoc er det nye systemet for dokumentasjon av datasett i SSB. Systemet er fortsatt under utvikling.↩︎",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-pseudo"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html",
    "href": "statistikkere/datatilstander.html",
    "title": "Datatilstander",
    "section": "",
    "text": "En datatilstand er et resultat av at et datasett har gått gjennom gitte operasjoner og prosesser (Standardutvalget 2023, 5). Denne siden er ment som en kort innføring i de forskjellige datatilstandene. Siden er basert på det interne dokumentet Datatilstander SSB - 2. utgave. Definisjonene er direkte utdrag fra dette dokumentet. Se interndokumentet for en mer grundig gjennomgang av datatilstander i SSB.\nI SSB skiller vi mellom fem datatilstander:\nAlle datatilstander er obligatoriske bortsett fra inndata. Figur 1 viser hvordan de forskjellige datatilstandene henger sammen.",
    "crumbs": [
      "Manual",
      "Standarder"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#kildedata",
    "href": "statistikkere/datatilstander.html#kildedata",
    "title": "Datatilstander",
    "section": "Kildedata",
    "text": "Kildedata\nKildedata er data lagret slik de ble levert til SSB fra dataeier. Eksempler på kildedata er: grunndata, transaksjonsdata, administrative data, statistiske data og aggregerte data og rapporter (Standardutvalget 2023, 7). Kildedata lagres i bøtten ssb-&lt;teamnavn&gt;-data-kilde-prod. Les mer om bøtter her og lagringsstandarder her.",
    "crumbs": [
      "Manual",
      "Standarder"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#inndata",
    "href": "statistikkere/datatilstander.html#inndata",
    "title": "Datatilstander",
    "section": "Inndata",
    "text": "Inndata\nInndata er kildedata som er transformert til SSBs standard lagringsformat (Standardutvalget 2023, 8). Denne transformeringer inkluderer blant annet at dataene skal benytte UTF-8 tegnsett. Les mer om SSBs standard lagringsformat her. Inndata kan også være andre statistikkers glargjorte data og/eller statistikkdata (Standardutvalget 2023, 8). Inndata er ikke en obligatorisk datatilstand. Inndata lagres i bøtten ssb-&lt;teamnavn&gt;-data-produkt-prod.",
    "crumbs": [
      "Manual",
      "Standarder"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#klargjorte-data",
    "href": "statistikkere/datatilstander.html#klargjorte-data",
    "title": "Datatilstander",
    "section": "Klargjorte data",
    "text": "Klargjorte data\nKlargjorte data er inndata hvor:\n\nvariablene er beregnet gjennom utregninger og koblinger mellom datasett\nnøyaktigheten er forbedret\n\nfor eksempel som resultat av editering eller imputering\n\nmetadata med variabeldefinisjoner er lagt til.\n\nEnhver endring som er gjort skal være sporbare og dokumentert slik at statistikkene skal være etterprøvbare. Klargjorte date er som regel ikke aggregerte - med mindre dataen vi mottar er aggregert. Med andre ord inneholder klargjorte data oftest enkeltobservasjoner - i likhet med kildedata og inndata (Standardutvalget 2023, 9). Klargjorte data lagres i bøtten ssb-&lt;teamnavn&gt;-data-produkt-prod.",
    "crumbs": [
      "Manual",
      "Standarder"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#statistikk",
    "href": "statistikkere/datatilstander.html#statistikk",
    "title": "Datatilstander",
    "section": "Statistikk",
    "text": "Statistikk\nStatistikk er “Tallfestede opplysninger om en gruppe eller et fenomen, og som kommer frem ved en sammenstilling og bearbeidelse av opplysninger om de enkelte enhetene i gruppen eller et utvalg av disse enhetene, eller ved systematisk observasjon av fenomenet” ifølge statistikkloven § 3a (Standardutvalget 2023, 10). Statistikk lagres i bøtten ssb-&lt;teamnavn&gt;-data-produkt-prod.\nStatistikk er ofte aggregerte data eller estimerte størrelser. Vi skiller mellom ujustert statistikk og justert statistikk. Indekser og sesongjusterte tall er eksempler på justert statistikk (Standardutvalget 2023, 10).\nStatistikk kan være inndata til andre statistikker, og kan dermed inneholde konfidensielle og detaljerte data som ikke publiseres.",
    "crumbs": [
      "Manual",
      "Standarder"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#utdata",
    "href": "statistikkere/datatilstander.html#utdata",
    "title": "Datatilstander",
    "section": "Utdata",
    "text": "Utdata\nUtdata er statistikk der kravene til konfidensialtet er ivaretatt. Dette er datatilstanden som publiseres. Eksempler inkluderer: statistikkbanktabeller, tabelloppdrag og internasjonal rapportering (Standardutvalget 2023, 11). Utdata lagres i bøtten ssb-&lt;teamnavn&gt;-data-produkt-produkt.",
    "crumbs": [
      "Manual",
      "Standarder"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#metadata-for-datatilstandene",
    "href": "statistikkere/datatilstander.html#metadata-for-datatilstandene",
    "title": "Datatilstander",
    "section": "Metadata for datatilstandene",
    "text": "Metadata for datatilstandene\nDet er forskjellige forventinger til metadata for de ulike datatilstandene. Forskjellene er skildret underdisse punktene:\n\nKildedata\n\nInformasjon på datasettnivå som dataeier, området dataene omhandler og tidsinformasjon\nMetadata om enkeltvariabler er begrenset til informasjonen dataeier selv avleverer.\n\n\n\nInndata\n\nI utgangspunktet samme som kildedata\n\n\n\nKlargjorte data\n\nVariabeldefinisjoner - beskrivelse av hver enkelt variabel og hvordan den er beregnet\nNøyaktighetsforbedrende tiltak som er utført\n\n\n\nStatistikk\n\nVariabeldefinisjoner\nHvilke metoder og programmer/kode som er benyttet for å produsere statistikken\n\n\n\nUtdata\n\nI utgangspunktet samme som for statistikk",
    "crumbs": [
      "Manual",
      "Standarder"
    ]
  },
  {
    "objectID": "statistikkere/dashboard.html",
    "href": "statistikkere/dashboard.html",
    "title": "Dash og dashboard",
    "section": "",
    "text": "Hvis en ønsker å lage ett dashbord som et brukergrensesnitt, så kan pakken Dash være et godt alternativ. Dash er ett rammeverk hvor man selv kan bygge opp applikasjoner i form av dashbord på en enklere måte, og det bygges oppå javascript pakker som plotly.js og react.js. Det er et produkt ved siden av og helintegrert med plotly, som også er en annen pakke i Python som gir oss interaktive grafer. Dash er et godt verktøy hvis en ønsker et dashbord som brukergrensesnitt for interaktiv visualisering av data. Dash kan kodes i Python og R, men også Julia og F#.\n\n\nI SSB kan man lage dashbord i virtuelle miljøer, gjerne satt opp med ssb-project, men man trenger helt spesifikke pakker for å få det oppe å gå. Mer info om å sette opp et eget miljø med ssb-project finner du her. Tabell under viser navn på pakkene, og gjeldende versjoner som er benyttet i skrivende stund av eksempel i neste avsnitt. Vel og merke kan nyere versjoner også fungere fint, noe man må prøve ut selv, men følgende versjoner fungerer fint.\n\n\n\nPakke\nVersjon (i skrivende stund)\n\n\n\n\ndash\n2.8.1\n\n\njupyter-dash\n0.4.2\n\n\njupyter-server-proxy\n3.2.2\n\n\njupyterlab-dash\n0.1.0a3\n\n\nipykernel\n6.21.3\n\n\ndash-bootstrap-components\n1.3.0 (hvis ønskelig)\n\n\n\nFor mer om håndtering av pakker i ett virtuelt miljø satt opp med ssb-project kan man se nærmere her. For å legge til disse pakkene kan man gjøre følgende i terminalen:\n\n\nterminal\n\npoetry add dash jupyter-dash jupyter-server-proxy jupyterlab-dash ipykernel\n\nOg hvis en ønsker Dash-Bootstrap-Components:\n\n\nterminal\n\npoetry add dash-bootstrap-components\n\nVel og merke så vil ikke denne pakken fungere uten at tilhørende filer er med i selve repoet. Dette fordi i hovedsak leter denne etter filer den trenger på internett. Pakken i seg selv har en fordel i at det er lettere å bygge opp utseende(layout) i dashbordet ditt, samt andre komponenter som ikke ligger i standard dash pakken.\n\n\n\nNoen ting er viktig å huske på at kommer i korrekt rekkefølge når en koder opp i JupyterLab. Her kommer ett veldig enkelt eksempel.\nFørste celle importerer vi alle nødvendige pakker\n\n\nnotebook\n\nfrom dash import html\nfrom jupyter_dash import JupyterDash\nfrom jupyter_dash.comms import _send_jupyter_config_comm_request\n_send_jupyter_config_comm_request()\n\nI Andre celle må følgende kjøres, men her er det VELDIG VIKTIG at man avventer 2-3 sekunder etter at forrige celle ble kjørt.\n\n\nnotebook\n\nJupyterDash.infer_jupyter_proxy_config()\n\nDeretter så er vi klare for å bygge opp selve dashbordet. så i Tredje celle kan en enkel kode for eksempel se slik ut:\n\n\nnotebook\n\napp = JupyterDash(__name__)\nporten = 8642 # Valgfritt fire sifret nummer\n\napp.layout = html.Div([\n    html.H1(\"Eget dashbord med Dash i SSB\")\n])\n\nif __name__ == \"__main__\":\n    app.run_server(debug=True, port=porten, mode=\"external\")\n\nDenne koden vil starte opp dashbordet i eget vindu i browseren ettersom mode=“external”. Dersom man vil ha et eget vindu i JupyterLab kan man sette denne til “jupyterlab”, eller hvis du vil at dashbordet skal dukke opp under cellen i notebooken din, så kan man sette denne til “inline”.\n\n\n\nDiverse som er verdt å se nærmere på når en bygger dashbord applikasjon med Dash. Det følger med mange gode eksempler for bruk av diverse komponenter i dokumentasjonene under her.\n\nStartsiden til Dash\nLage interaktive grafer i Python med Plotly\nDash Core Components\nDash HTML Components\nDash Bootstrap Components\n\nVel og merke henter denne diverse materialer fra internett, og vil ikke fungere uten tilgang. Det er heller anbefalt å ha de nødvendige filene lagret lokalt for bruk av denne pakken.",
    "crumbs": [
      "Manual",
      "Appendix"
    ]
  },
  {
    "objectID": "statistikkere/dashboard.html#anbefalte-nødvendige-pakker",
    "href": "statistikkere/dashboard.html#anbefalte-nødvendige-pakker",
    "title": "Dash og dashboard",
    "section": "",
    "text": "I SSB kan man lage dashbord i virtuelle miljøer, gjerne satt opp med ssb-project, men man trenger helt spesifikke pakker for å få det oppe å gå. Mer info om å sette opp et eget miljø med ssb-project finner du her. Tabell under viser navn på pakkene, og gjeldende versjoner som er benyttet i skrivende stund av eksempel i neste avsnitt. Vel og merke kan nyere versjoner også fungere fint, noe man må prøve ut selv, men følgende versjoner fungerer fint.\n\n\n\nPakke\nVersjon (i skrivende stund)\n\n\n\n\ndash\n2.8.1\n\n\njupyter-dash\n0.4.2\n\n\njupyter-server-proxy\n3.2.2\n\n\njupyterlab-dash\n0.1.0a3\n\n\nipykernel\n6.21.3\n\n\ndash-bootstrap-components\n1.3.0 (hvis ønskelig)\n\n\n\nFor mer om håndtering av pakker i ett virtuelt miljø satt opp med ssb-project kan man se nærmere her. For å legge til disse pakkene kan man gjøre følgende i terminalen:\n\n\nterminal\n\npoetry add dash jupyter-dash jupyter-server-proxy jupyterlab-dash ipykernel\n\nOg hvis en ønsker Dash-Bootstrap-Components:\n\n\nterminal\n\npoetry add dash-bootstrap-components\n\nVel og merke så vil ikke denne pakken fungere uten at tilhørende filer er med i selve repoet. Dette fordi i hovedsak leter denne etter filer den trenger på internett. Pakken i seg selv har en fordel i at det er lettere å bygge opp utseende(layout) i dashbordet ditt, samt andre komponenter som ikke ligger i standard dash pakken.",
    "crumbs": [
      "Manual",
      "Appendix"
    ]
  },
  {
    "objectID": "statistikkere/dashboard.html#eksempel-kode-i-jupyterlab",
    "href": "statistikkere/dashboard.html#eksempel-kode-i-jupyterlab",
    "title": "Dash og dashboard",
    "section": "",
    "text": "Noen ting er viktig å huske på at kommer i korrekt rekkefølge når en koder opp i JupyterLab. Her kommer ett veldig enkelt eksempel.\nFørste celle importerer vi alle nødvendige pakker\n\n\nnotebook\n\nfrom dash import html\nfrom jupyter_dash import JupyterDash\nfrom jupyter_dash.comms import _send_jupyter_config_comm_request\n_send_jupyter_config_comm_request()\n\nI Andre celle må følgende kjøres, men her er det VELDIG VIKTIG at man avventer 2-3 sekunder etter at forrige celle ble kjørt.\n\n\nnotebook\n\nJupyterDash.infer_jupyter_proxy_config()\n\nDeretter så er vi klare for å bygge opp selve dashbordet. så i Tredje celle kan en enkel kode for eksempel se slik ut:\n\n\nnotebook\n\napp = JupyterDash(__name__)\nporten = 8642 # Valgfritt fire sifret nummer\n\napp.layout = html.Div([\n    html.H1(\"Eget dashbord med Dash i SSB\")\n])\n\nif __name__ == \"__main__\":\n    app.run_server(debug=True, port=porten, mode=\"external\")\n\nDenne koden vil starte opp dashbordet i eget vindu i browseren ettersom mode=“external”. Dersom man vil ha et eget vindu i JupyterLab kan man sette denne til “jupyterlab”, eller hvis du vil at dashbordet skal dukke opp under cellen i notebooken din, så kan man sette denne til “inline”.",
    "crumbs": [
      "Manual",
      "Appendix"
    ]
  },
  {
    "objectID": "statistikkere/dashboard.html#aktuell-dokumentasjon",
    "href": "statistikkere/dashboard.html#aktuell-dokumentasjon",
    "title": "Dash og dashboard",
    "section": "",
    "text": "Diverse som er verdt å se nærmere på når en bygger dashbord applikasjon med Dash. Det følger med mange gode eksempler for bruk av diverse komponenter i dokumentasjonene under her.\n\nStartsiden til Dash\nLage interaktive grafer i Python med Plotly\nDash Core Components\nDash HTML Components\nDash Bootstrap Components\n\nVel og merke henter denne diverse materialer fra internett, og vil ikke fungere uten tilgang. Det er heller anbefalt å ha de nødvendige filene lagret lokalt for bruk av denne pakken.",
    "crumbs": [
      "Manual",
      "Appendix"
    ]
  },
  {
    "objectID": "statistikkere/datadoc.html",
    "href": "statistikkere/datadoc.html",
    "title": "Datadoc",
    "section": "",
    "text": "Datadoc er et verktøy for å dokumentere datasett og variablene som utgjør datasettet. Metadataene som skapes kommer til å bli selve ryggmargen til SSBs fremtidige datakatalog og skal gjør det mulig for SSB ansatte og det norske samfunnet å finne fram til faktagrunnlaget.\nDet er svært viktig for SSB at metadataene er komplette og riktige. Vi vil dermed gjøre det så lett som mulig for brukeren. Om man kommer på noen problemer, eller har forslag til forbedring, meld gjerne ifra på Dapla samfunnet på Viva Engage.\n\n\nFor å starte Datadoc må du logge deg inn på Dapla Lab. Start Datadoc tjenesten og du er klar til å dokumentere datasettet ditt.\n\n\n\n\n\n\nWarning\n\n\n\nInnlogging i Test-miljøet til Dapla Lab er ustabilt for tiden.\n\n\n\n\n\n\n\nFør man kan benytte Datadoc, må man åpne et Datasett. Det gjøres enkelt ved å lime inn stien til datasettet i Filsti tekstboksen (Punkt 1 i Figur 1) og trykke på Åpne fil knappen (Punkt 2 i Figur 1).\nDatadoc benytter brukerens innloggingsopplysninger for å aksessere data. Det betyr at man i utgangspunktet har tilgang til de samme filene som ellers på Dapla.\n\n\n\n\n\n\nWarning\n\n\n\nMan må inkludere gs:// på begynnelsen av stien når man jobber med et datasett i en bøtte.\n\n\n\n\n\n\n\n\nNote\n\n\n\nMan kan finne filstien gjennom Google konsollet eller ved å benytte Dapla toolbelt\n\n\n\n\n\n\n\n\nFigur 1: Åpne et datasett i Datadoc.\n\n\n\n\n\nEtter at man har trykket på Åpne fil knappen bør man se meldingen vist i Figur 2\n\n\n\n\n\n\nFigur 2: Meldingen at det var velykket å åpne datasettet.\n\n\n\n\n\n\nHvis man åpner en datasett som ikke følger navnestandarden, vil det komme en advarsel (Figur 3). Det er fullt mulig å bruke Datadoc fortsatt for å dokumentere datasettet, men ikke like mye metadata kan utledes automatisk (TODO: lenke til seksjonen om utledning).\n\n\n\n\n\n\nNote\n\n\n\nDette kan være en fin anledning til å justere på navngivning og strukturen i teamets bøtter slik at alt følger navnestandarden. Det er en lenke til navnestandarden i meldingen.\n\n\n\n\n\n\n\n\nFigur 3: Meldingen at datasettet ikke følger navnestandarden.\n\n\n\n\n\n\nHvis Datadoc klarer ikke åpne datasettet, vises en rød error melding (Figur 4). Dette forårsakes oftest at filen ikke finnes (for eksempel på grunn av en typo i filstien), men kan også være på grunn av at man ikke har tilgang til filen, eller at det er et brudd i nettverket.\n\n\n\n\n\n\nFigur 4: Meldingen at det var en feil ved åpning av datasettet.\n\n\n\n\n\n\nHvis et metadatadokument eksisterer, er det denne informasjonen som lastes inn. Det utledes ingenting fra datasettet.\n\n\n\n\nInformasjon som kan utledes vil bli fylt inn når du åpner datasettet. Informasjonen hentes enten fra filstien eller settes inn som en default verdi (*). Det er mulig å korrigere informasjonen i ettertid. Følgende felter blir forsøkt utledet:\nDatasett:\n\nVerdivurdering\nStatus (*)\nDatatilstand\nVersjon\nStatistikkområde\nInneholder data f.o.m.\nInneholder data t.o.m.\nGeografisk dekningsområde (*)\n\nVariabler:\n\nKortnavn\nDatatype\n\n\n\n\nDokumentasjon av datasettet som helthet gjøres i datasettfanen i Datadoc.\nAlle felter har en ordforklaring  du kan trykke på. Her vil du få en kort forklaring til hva som skal stå i feltet.\nFlere felter har verdilister hvor mange er hentet fra KLASS, mens noen er fritekstfelter. For noen av fritekstfeltene gjøres det en sjekk av innholdet og du vil få en feilmelding hvis kriteriene ikke er oppfylt.\n\n\nAlt som står under obligatorisk må fylles inn.\n\n\n\nAnbefalte felter er frivillig å fylle ut.\n\n\n\nFeltene her genereres automatisk og kan ikke redigeres. De er kun med til informasjon.\n\n\n\n\nDokumentasjon av variabelforekomster for et datasett kan gjøres i variabelfanen i Datadoc. Her vil man se en liste av alle kortnavnene til variabelforekomstene i datasettet. Ved å trykke seg inn på et av kortnavnene kan man dokumentere de obligatoriske og anbefalte feltene for en variabelforekomst.\n\n\nFor å forenkle dokumentasjonen av variabelforekomster vil noen felt arve verdiene som blir satt i datasettfanen. Dette gjelder følgende felter:\n\nDatakilde\nPopulasjon\nTemporalitetstype\nInneholder data f.o.m\nInneholder data t.o.m\n\nDet er mulig å redigere vediene i variabelforekomst fanen etter en verdi er satt i datasettfanen. Men hvis disse feltene blir endret i datasettfanen senere, vil de alltid overskrive det som er satt i variabelforekomst fanen.\n\n\n\nDet er mulig å søke gjennom variabelforekomstene sine kortnavn. Dette filtrerer på listen over variabelforekomster.\n\n\n\n\n\n\nFigur 5: Søk gjennom kortnavn til variabelforekomster\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nViktig informasjon\n\n\n\nDatadoc mellomlagrer ikke utfylt metadata. Pass på å lagre metadataene ofte ved å trykke lagre og legg merke til om du får en bekreftelse på at metadataene er lagret.\n\n\n\n\nNår du trykker Lagre metadata knappen vil du få en bekreftelse på vellykket lagring. \nHvis ikke alle obligatoriske felt er utfylt vil du få opp en advarsel for datasett og variabelforekomstene. Advarselen for datasett viser en liste over hvilke felt som mangler. For variabelforekomster vises både variabelens kortnavn og manglende felt.\nNår du fyller ut de manglende obligatoriske feltene må du lagre på nytt og advarslene vil forsvinne når alle obligatoriske felt er fylt ut.\n\n\n\nNår du trykker på Lagre metadata knappen i Datadoc så skrives alle metadata til en fil i samme mappe (katalog) som datafilen. Dette er en JSON-fil med nesten samme navn som datafilen. Navnekonvensjonen for metadatafilen er\n&lt;navn på datasettfilen uten endelse&gt;__DOC.json\nEksempelvis hvis datafilen har navnet skattedata_p2022_v1.parquet, så vil Datadoc lagre metadata i filen skattedata_p2022_v1__DOC.json.\nFordelen med å benytte en JSON-fil til å lagre metadata er at denne filen kan kopieres og flyttes like enkelt som selve datafilen. JSON-filer er strukturerte tekstfiler som kan leses av både maskiner (Python/R) og av mennesker (åpnes i en tekst-editor).\nSe et eksempel på JSON metadata-fil lagret av DataDoc.\n\n\n\n\nØnsker du å endre eller legge til metadata, åpner du et datasett slik som beskrevet i Åpne et datasett. Da vil innholdet fra metadata-filen leses inn i Datadoc og kan redigeres videre. Endringene blir lagret når man trykker Lagre metadata.\n\n\n\nKildekoden til Datadoc er offentlig tilgjengelig på Github: https://github.com/statisticsnorway/datadoc",
    "crumbs": [
      "Manual",
      "Metadata",
      "Datadoc"
    ]
  },
  {
    "objectID": "statistikkere/datadoc.html#innlogging",
    "href": "statistikkere/datadoc.html#innlogging",
    "title": "Datadoc",
    "section": "",
    "text": "For å starte Datadoc må du logge deg inn på Dapla Lab. Start Datadoc tjenesten og du er klar til å dokumentere datasettet ditt.\n\n\n\n\n\n\nWarning\n\n\n\nInnlogging i Test-miljøet til Dapla Lab er ustabilt for tiden.",
    "crumbs": [
      "Manual",
      "Metadata",
      "Datadoc"
    ]
  },
  {
    "objectID": "statistikkere/datadoc.html#hvordan-dokumentere-et-datasett",
    "href": "statistikkere/datadoc.html#hvordan-dokumentere-et-datasett",
    "title": "Datadoc",
    "section": "",
    "text": "Før man kan benytte Datadoc, må man åpne et Datasett. Det gjøres enkelt ved å lime inn stien til datasettet i Filsti tekstboksen (Punkt 1 i Figur 1) og trykke på Åpne fil knappen (Punkt 2 i Figur 1).\nDatadoc benytter brukerens innloggingsopplysninger for å aksessere data. Det betyr at man i utgangspunktet har tilgang til de samme filene som ellers på Dapla.\n\n\n\n\n\n\nWarning\n\n\n\nMan må inkludere gs:// på begynnelsen av stien når man jobber med et datasett i en bøtte.\n\n\n\n\n\n\n\n\nNote\n\n\n\nMan kan finne filstien gjennom Google konsollet eller ved å benytte Dapla toolbelt\n\n\n\n\n\n\n\n\nFigur 1: Åpne et datasett i Datadoc.\n\n\n\n\n\nEtter at man har trykket på Åpne fil knappen bør man se meldingen vist i Figur 2\n\n\n\n\n\n\nFigur 2: Meldingen at det var velykket å åpne datasettet.\n\n\n\n\n\n\nHvis man åpner en datasett som ikke følger navnestandarden, vil det komme en advarsel (Figur 3). Det er fullt mulig å bruke Datadoc fortsatt for å dokumentere datasettet, men ikke like mye metadata kan utledes automatisk (TODO: lenke til seksjonen om utledning).\n\n\n\n\n\n\nNote\n\n\n\nDette kan være en fin anledning til å justere på navngivning og strukturen i teamets bøtter slik at alt følger navnestandarden. Det er en lenke til navnestandarden i meldingen.\n\n\n\n\n\n\n\n\nFigur 3: Meldingen at datasettet ikke følger navnestandarden.\n\n\n\n\n\n\nHvis Datadoc klarer ikke åpne datasettet, vises en rød error melding (Figur 4). Dette forårsakes oftest at filen ikke finnes (for eksempel på grunn av en typo i filstien), men kan også være på grunn av at man ikke har tilgang til filen, eller at det er et brudd i nettverket.\n\n\n\n\n\n\nFigur 4: Meldingen at det var en feil ved åpning av datasettet.\n\n\n\n\n\n\nHvis et metadatadokument eksisterer, er det denne informasjonen som lastes inn. Det utledes ingenting fra datasettet.\n\n\n\n\nInformasjon som kan utledes vil bli fylt inn når du åpner datasettet. Informasjonen hentes enten fra filstien eller settes inn som en default verdi (*). Det er mulig å korrigere informasjonen i ettertid. Følgende felter blir forsøkt utledet:\nDatasett:\n\nVerdivurdering\nStatus (*)\nDatatilstand\nVersjon\nStatistikkområde\nInneholder data f.o.m.\nInneholder data t.o.m.\nGeografisk dekningsområde (*)\n\nVariabler:\n\nKortnavn\nDatatype\n\n\n\n\nDokumentasjon av datasettet som helthet gjøres i datasettfanen i Datadoc.\nAlle felter har en ordforklaring  du kan trykke på. Her vil du få en kort forklaring til hva som skal stå i feltet.\nFlere felter har verdilister hvor mange er hentet fra KLASS, mens noen er fritekstfelter. For noen av fritekstfeltene gjøres det en sjekk av innholdet og du vil få en feilmelding hvis kriteriene ikke er oppfylt.\n\n\nAlt som står under obligatorisk må fylles inn.\n\n\n\nAnbefalte felter er frivillig å fylle ut.\n\n\n\nFeltene her genereres automatisk og kan ikke redigeres. De er kun med til informasjon.\n\n\n\n\nDokumentasjon av variabelforekomster for et datasett kan gjøres i variabelfanen i Datadoc. Her vil man se en liste av alle kortnavnene til variabelforekomstene i datasettet. Ved å trykke seg inn på et av kortnavnene kan man dokumentere de obligatoriske og anbefalte feltene for en variabelforekomst.\n\n\nFor å forenkle dokumentasjonen av variabelforekomster vil noen felt arve verdiene som blir satt i datasettfanen. Dette gjelder følgende felter:\n\nDatakilde\nPopulasjon\nTemporalitetstype\nInneholder data f.o.m\nInneholder data t.o.m\n\nDet er mulig å redigere vediene i variabelforekomst fanen etter en verdi er satt i datasettfanen. Men hvis disse feltene blir endret i datasettfanen senere, vil de alltid overskrive det som er satt i variabelforekomst fanen.\n\n\n\nDet er mulig å søke gjennom variabelforekomstene sine kortnavn. Dette filtrerer på listen over variabelforekomster.\n\n\n\n\n\n\nFigur 5: Søk gjennom kortnavn til variabelforekomster",
    "crumbs": [
      "Manual",
      "Metadata",
      "Datadoc"
    ]
  },
  {
    "objectID": "statistikkere/datadoc.html#lagre-metadata",
    "href": "statistikkere/datadoc.html#lagre-metadata",
    "title": "Datadoc",
    "section": "",
    "text": "Viktig informasjon\n\n\n\nDatadoc mellomlagrer ikke utfylt metadata. Pass på å lagre metadataene ofte ved å trykke lagre og legg merke til om du får en bekreftelse på at metadataene er lagret.\n\n\n\n\nNår du trykker Lagre metadata knappen vil du få en bekreftelse på vellykket lagring. \nHvis ikke alle obligatoriske felt er utfylt vil du få opp en advarsel for datasett og variabelforekomstene. Advarselen for datasett viser en liste over hvilke felt som mangler. For variabelforekomster vises både variabelens kortnavn og manglende felt.\nNår du fyller ut de manglende obligatoriske feltene må du lagre på nytt og advarslene vil forsvinne når alle obligatoriske felt er fylt ut.\n\n\n\nNår du trykker på Lagre metadata knappen i Datadoc så skrives alle metadata til en fil i samme mappe (katalog) som datafilen. Dette er en JSON-fil med nesten samme navn som datafilen. Navnekonvensjonen for metadatafilen er\n&lt;navn på datasettfilen uten endelse&gt;__DOC.json\nEksempelvis hvis datafilen har navnet skattedata_p2022_v1.parquet, så vil Datadoc lagre metadata i filen skattedata_p2022_v1__DOC.json.\nFordelen med å benytte en JSON-fil til å lagre metadata er at denne filen kan kopieres og flyttes like enkelt som selve datafilen. JSON-filer er strukturerte tekstfiler som kan leses av både maskiner (Python/R) og av mennesker (åpnes i en tekst-editor).\nSe et eksempel på JSON metadata-fil lagret av DataDoc.",
    "crumbs": [
      "Manual",
      "Metadata",
      "Datadoc"
    ]
  },
  {
    "objectID": "statistikkere/datadoc.html#oppdatere-eksisterende-metadata",
    "href": "statistikkere/datadoc.html#oppdatere-eksisterende-metadata",
    "title": "Datadoc",
    "section": "",
    "text": "Ønsker du å endre eller legge til metadata, åpner du et datasett slik som beskrevet i Åpne et datasett. Da vil innholdet fra metadata-filen leses inn i Datadoc og kan redigeres videre. Endringene blir lagret når man trykker Lagre metadata.",
    "crumbs": [
      "Manual",
      "Metadata",
      "Datadoc"
    ]
  },
  {
    "objectID": "statistikkere/datadoc.html#kildekode",
    "href": "statistikkere/datadoc.html#kildekode",
    "title": "Datadoc",
    "section": "",
    "text": "Kildekoden til Datadoc er offentlig tilgjengelig på Github: https://github.com/statisticsnorway/datadoc",
    "crumbs": [
      "Manual",
      "Metadata",
      "Datadoc"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html",
    "href": "statistikkere/rstudio.html",
    "title": "Rstudio",
    "section": "",
    "text": "Kommer snart",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html",
    "href": "statistikkere/vscode-python.html",
    "title": "Vscode-python",
    "section": "",
    "text": "Vscode-python er en tjeneste på Dapla Lab for utvikling av kode i Python1. Målgruppen for tjenesten er brukere som skal skrive produksjonskode i Python.\nSiden tjenesten er ment for produksjonskode så er det veldig få forhåndsinstallerte Python-pakker som er installert. Antagelsen er at brukerene/teamet heller bør installere de pakkene de trenger, framfor at alle pakker som alle brukere/team er forhåndsinstallert og skal dekke behovet til alle. Det reduserer kompleksitet i tjenesten og dermed sannsynligheten for feilsituasjoner.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#forberedelser",
    "href": "statistikkere/vscode-python.html#forberedelser",
    "title": "Vscode-python",
    "section": "Forberedelser",
    "text": "Forberedelser\nFør man starter Vscode-python bør man ha lest kapitlet om Dapla Lab og satt opp Git- og GitHub-konfigurasjonen under Min konto. Deretter gjør du følgende:\n\nLogg deg inn på Dapla Lab\nUnder Tjenestekatalog trykker du på Start-knappen for Vscode-python\nGi tjenesten et navn\nÅpne Vscode-python konfigurasjoner",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#konfigurasjon",
    "href": "statistikkere/vscode-python.html#konfigurasjon",
    "title": "Vscode-python",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nFør man åpner en tjeneste kan man konfigurere hvor mye ressurser man ønsker, hvilket team man skal representere, om et GitHub-repo skal klones ved oppstart, og mange andre ting. Valgene man gjør kan også lagres slik at man å slipper å gjøre samme jobb senere. ?@fig-dapla-lab-tjenestekonf-detail viser første fane i tjenestekonfigurasjonen for Vscode-python.\n\n\n\n\n\n\nFigur 1: Detaljert tjenestekonfigurasjon i Dapla Lab\n\n\n\n\nService\nI fanen Service kan man velge hvilke PullPolicy som skal gjelde ved oppstart av, samt Version av tjenesten. PullPolicy trenger brukerne sjelden å endre på, mens Version lar brukeren velge hvilken versjon av tjenesten som skal benyttes.\nSiden Jupyter-tjenesten kommer installert med både R- og Python så er det her man velger hvilke versjoner av disse man ønsker. Man kan velge mellom alle tidligere tilbudte kombinasjoner av R og Python. I Figur 1 ser vi av navnet vscode-python:-py311 at tjenesten som default vil startes med versjon 3.11 av Python. Etter hvert som nye versjoner av Python kommer kan disse tilgjengeliggjøres i tjenesten, men brukeren kan velge å starte en eldre versjon av tjenesten og Python.\n\n\nBuckets\nUnder Buckets kan man velge Enable for å få tilgang til data fra bøtter i tjenesten. I tillegg må man velge hvilket team og tilgangsgruppe man skal representere.\n\n\n\n\n\n\nFigur 2: Detaljert tjenestekonfigurasjon for bøttetilgang i Dapla Lab\n\n\n\nFigur 2 viser at man kan velge blant alle developers-gruppene i alle team man er medlem i. Velger man f.eks. å representere Dapla group teama-developers så kan man få alle standard-bøtter (produkt, frasky, tilsky og egne delt-bøtter) for det teamet. Men man får ikke tilgang til bøtter som man har tilgang til gjennom andre team.\nMan kan også spesifisere andre bøtter som skal tilgjengliggjøres i tjenesten teamet og tilgangsgruppen har tilgang til. Typisk vil dette være andre sine delt-bøtter som teamet har fått tilgang til.\n\n\nResources\nUnder fanen Resources kan man velge hvor mye CPU og RAM man ønsker i tjenesten, slik som vist i Figur 3. Velg så lite som trengs for å gjøre jobben du skal gjøre.\n\n\n\n\n\n\nFigur 3: Konfigurasjon av ressurser for Vscode-python-tjenesten i Dapla Lab\n\n\n\n\n\nPersistence\nSom default får alle som starter en instans av Vscode-python-tjenesten en lokal disk på 10GB inne i tjenesten. Under Persistence-fanen kan man velge å øke størrelsen på disken eller ikke noe disk i det hele tatt. Siden lokal disk i tjenesten hovedsakelig skal benyttes til å lagre en lokal kopi av koden som lagres på GitHub mens man gjør endringer, så bør ikke størrelsen på disken være større enn nødvendig.@fig-dapla-lab-resources viser valgene som kan gjøres under Resource-fanen.\n\n\n\n\n\n\nFigur 4: Konfigurasjon av lokal disk for Jupyter-tjenesten i Dapla Lab\n\n\n\n\n\nGit\nUnder fanen Git kan man konfigurere Git og GitHub slik at det blir lettere å jobbe med inne i tjenesten. Som default arves informasjonen som er lagret under Min konto-Git i Dapla Lab. Informasjonen under tjenestekonfigurasjonen blir tilgjengeliggjort som miljøvariabler i tjenesten. Informasjonen blir også lagt i $HOME/.netrc slik at man kan benytte ikke trenger å gjøre noe mer for å jobbe mot GitHub fra tjenesten.\n\n\n\n\n\n\nFigur 5: Konfigurasjon av Git for Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#datatilgang",
    "href": "statistikkere/vscode-python.html#datatilgang",
    "title": "Vscode-python",
    "section": "Datatilgang",
    "text": "Datatilgang\nHvis man har valgt å tilgjengeliggjøre data fra et team sitt bøtter i tjenesten, så kan man inspisere dataene fra en terminal inne i tjenesten:\n\nÅpne en instans av Vscode-python med data fra bøtter\nÅpne en terminal inne i Vscode-python\nGå til mappen med bøttene ved å kjøre dette fra terminalen cd /buckets\nKjør ls -ahl i teminalen for å se på hvilke bøtter som er montert.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#installere-pakker",
    "href": "statistikkere/vscode-python.html#installere-pakker",
    "title": "Vscode-python",
    "section": "Installere pakker",
    "text": "Installere pakker\nSiden det nesten ikke er installert noen pakker i tjenesten så kan brukeren opprette et ssb-project og installere pakker som vanlig.\nFor å bygge et eksisterende ssb-project så kan brukeren også bruke ssb-project.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#slette-tjenesten",
    "href": "statistikkere/vscode-python.html#slette-tjenesten",
    "title": "Vscode-python",
    "section": "Slette tjenesten",
    "text": "Slette tjenesten\nFor å slette tjenesten kan man trykke på Slette-knappen i Dapla Lab under Mine tjenester. Når man sletter en tjeneste så sletter man hele disken inne i tjenesten og frigjør alle ressurser som er reservert. Siden pakkene som er installert også ligger lagret på disken, betyr dette at pakkene må installeres på nytt etter at en tjeneste er slettet. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#pause-tjenesten",
    "href": "statistikkere/vscode-python.html#pause-tjenesten",
    "title": "Vscode-python",
    "section": "Pause tjenesten",
    "text": "Pause tjenesten\nMan kan pause tjenesten ved å trykke på Pause-knappen i Dapla Lab under Mine tjenester. Når man pauser så slettes alt påden lokale disken som ikke er lagret under $HOME/work. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#monitorering",
    "href": "statistikkere/vscode-python.html#monitorering",
    "title": "Vscode-python",
    "section": "Monitorering",
    "text": "Monitorering\n\n\nMan kan moniterere en instans av Vscode-python ved å trykke på Vscode-python-teksten under Mine tjenester i Dapla Lab, slik som vist i Figur 6.\nDenne funksjonaliteten er under arbeid og mer informasjon kommer snart.\n\n\n\n\n\n\n\n\n\nFigur 6: Monitorering av Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#footnotes",
    "href": "statistikkere/vscode-python.html#footnotes",
    "title": "Vscode-python",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nVscode-python er web-versjonen av VS Code og er ikke helt identisk med desktop-versjonen av VS Code mange er kjent med. Blant annet er det kun extensions fra Open VSX Registry som kan installeres.↩︎",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/jit.html",
    "href": "statistikkere/jit.html",
    "title": "Just-in-Time Access",
    "section": "",
    "text": "Just-in-Time Access (JIT) er en applikasjon der data-admins kan gi seg selv midlertidig tilgang til kildedata. På Dapla benyttes for dette for å unngå at data-admins har permanent tilgang til sensitive data, siden all kildedata er definert som sensitiv informasjon. data-admins må bruke JIT-applikasjonen for å gi seg selv korte, begrunnede tilganger til kildedata ved behov. Den som er ansvarlig for teamet kan monitere i hvilken grad teamets data-admins benytter seg av denne muligheten.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Just-in-Time Access"
    ]
  },
  {
    "objectID": "statistikkere/jit.html#hvordan-bruke-jit",
    "href": "statistikkere/jit.html#hvordan-bruke-jit",
    "title": "Just-in-Time Access",
    "section": "Hvordan bruke JIT",
    "text": "Hvordan bruke JIT\nFor å bruke JIT må du være data-admin for teamet som eier kildedataene du ønsker tilgang til. Du aktiverer tilganger i JIT-appen ved gjøre følgende:\n\nGå inn på nettsiden https://jitaccess.dapla.ssb.no/1\nOppgi Prosjekt-id for prosjektet der kildedataene du ønsker tilgang til er lagret.\nHuk av hvilke tilgangsroller du ønsker aktivert. Se beskrivelse av roller i Tabell 1.\nVelg hvor lenge tilgangen du ønsker at tilgangen skal vare. Den kan maksimalt vare i 8 timer.\nOppgi en begrunnelse for at aktiverer tilgangen.\nTil slutt trykker du på Request access.\n\nTilgangen vil deretter være aktiv ila. noen minutter.\n\n\n\n\n\n\nHvordan bør begrunnelsen skrives?\n\n\n\nPrøv å gjør begrunnelsen forståelig for andre enn deg selv på det tidspunktet. Den som er ansvarlig for teamet skal kunne forstå begrunnelsen når de ser på loggene. I tillegg vil sentralt i SSB monitere i hvilken grad Dapla-teamene benytter seg av tilgangene.\n\n\n\n\n\nTabell 1: De mest relevante tilgangene som kan aktiveres i JIT-applikasjonen\n\n\n\n\n\n\n\n\n\nRoller\nHva?\n\n\n\n\nssb.buckets.list\nListe ut bøttene i kildeprosjektet.\n\n\nstoragetransfer.admin\nSette opp jobb med Transfer Service i kildeprosjektet.\n\n\nssb.bucket.write\nLese og skrive til kildebøtta.\n\n\n\n\n\n\nSkal du sette opp en Transfer Service overføring med kildedata så må du aktivere ssb.bucket.write og storagetransfer.admin. SKal du skrive og lese fra et miljø som Jupyter, så må du aktivere rollene ssb.bucket.write og ssb.buckets.list.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Just-in-Time Access"
    ]
  },
  {
    "objectID": "statistikkere/jit.html#footnotes",
    "href": "statistikkere/jit.html#footnotes",
    "title": "Just-in-Time Access",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nHvis du jobber fra hjemmekontor så må du være på VPN for å få tilgang til JIT-appen.↩︎",
    "crumbs": [
      "Manual",
      "Appendix",
      "Just-in-Time Access"
    ]
  },
  {
    "objectID": "statistikkere/statistikkproduksjon.html",
    "href": "statistikkere/statistikkproduksjon.html",
    "title": "Statistikkproduksjon",
    "section": "",
    "text": "Statistikkproduksjon\nI forrige del beskrev vi noen grunnleggende ting rundt Dapla. I denne delen tar vi for oss hvordan du kan begynne å jobbe med skarpe data på plattformen.\nKapittelet som beskriver hvordan man logger seg inn på Dapla vil fungere uten at du må gjøre noen forberedelser. Er man koblet på SSB sitt nettverk så vil alle SSB-ansatte kunne gå inn på plattformen og kode i Python og R. Men du får ikke tilgang til SSBs område for datalagring på plattformen. I praksis vil det si at man kan generere data med kode, men man kan ikke jobbe med skarpe data.\nFor å få muligheten til å jobbe med skarpe data MÅ du først opprette et dapla-team. Dette er det første naturlige steget å ta når man skal begynne å jobbe med statistikkproduksjon på Dapla. I dette kapittelet vil vi forklare det du trenger å vite om det å opprette og jobbe innenfor et team."
  },
  {
    "objectID": "statistikkere/gcp-overview.html",
    "href": "statistikkere/gcp-overview.html",
    "title": "Google Cloud Platform (GCP)",
    "section": "",
    "text": "Google Cloud Platform (GCP)"
  },
  {
    "objectID": "statistikkere/gcc.html",
    "href": "statistikkere/gcc.html",
    "title": "Google Cloud Console (GCC)",
    "section": "",
    "text": "Google Cloud Console (GCC) er et web-basert grensesnitt for å administrere ressurser og tjenester på Google Cloud Platform (GCP). Alle i SSB kan logge seg inn i GCC med sin SSB-bruker. Dapla-team har sjelden mulighet til å opprette nye ressurser fra dette grensesnittet, siden vi ønsker at det skal gjøres med kode. Men det er likevel et nyttig verktøy for å se på ressurser og gjøre endringer på eksisterende ressurser. I SSB bruker bruker vi GCC hovedsakelig til følgende:",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Google Cloud Console (GCC)"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html#innlogging",
    "href": "statistikkere/gcc.html#innlogging",
    "title": "Google Cloud Console (GCC)",
    "section": "Innlogging",
    "text": "Innlogging\nFor å logge inn i GCC så gjør du følgende:\n\nÅpne Google Cloud Console i en nettleser.\nLogg in med din SSB-bruker.\n\nHvis du også har en privat Google-konto som benyttes i samme nettleser, må du noen ganger passe på at du er logget inn med riktig konto. Dette kan du sjekke ved å trykke på profilbildet ditt øverst til høyre i GCC. Hvis du ikke er logget inn med riktig konto, så trykker du på Logg ut og logger inn på nytt med riktig konto.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Google Cloud Console (GCC)"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html#prosjektvelger",
    "href": "statistikkere/gcc.html#prosjektvelger",
    "title": "Google Cloud Console (GCC)",
    "section": "Prosjektvelger",
    "text": "Prosjektvelger\nEtter at du har logget deg på med din SSB-bruker, så må du velge hvilket av ditt teams prosjekter du ønsker å jobbe med. Dette gjør du ved å trykke på prosjektvelgeren øverst til venstre på siden. Vidoen under viser hvordan du velger et prosjekt og lister ut hvilke bøtter som finnes i prosjektet.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Google Cloud Console (GCC)"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html#filutforsker",
    "href": "statistikkere/gcc.html#filutforsker",
    "title": "Google Cloud Console (GCC)",
    "section": "Filutforsker",
    "text": "Filutforsker\nFor å utforske bøtter og filer i et Dapla-team sitt Google-prosjekt så kan man bruke Cloud Storage-grensesnittet i GCC. For å bruke denne funksjonaliteten gjør du følgende:\n\nBruk prosjektvelgeren til å velge ønsket prosjekt.\nDeretter søker du opp Google Storage i søkefeltet øverst på siden.\n\nDa får du en oversikt over alle bøttene i prosjektet. Velg ønsker bøtte for å utforske innholdet i bøtta.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Google Cloud Console (GCC)"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html#logger",
    "href": "statistikkere/gcc.html#logger",
    "title": "Google Cloud Console (GCC)",
    "section": "Logger",
    "text": "Logger\nKommer snart.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Google Cloud Console (GCC)"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html#transfer-service",
    "href": "statistikkere/gcc.html#transfer-service",
    "title": "Google Cloud Console (GCC)",
    "section": "Transfer Service",
    "text": "Transfer Service\nLes mer om hvordan man bruker Transfer Service her.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Google Cloud Console (GCC)"
    ]
  },
  {
    "objectID": "statistikkere/introduksjon.html",
    "href": "statistikkere/introduksjon.html",
    "title": "Introduksjon",
    "section": "",
    "text": "Introduksjon\nMålet med dette kapittelet er å gi en grunnleggende innføring i hva som legges i ordet Dapla. I tillegg gis en forklaring på hvorfor disse valgene er tatt."
  },
  {
    "objectID": "statistikkere/kildedata.html",
    "href": "statistikkere/kildedata.html",
    "title": "Kildedata",
    "section": "",
    "text": "Kildedata er data lagret slik de ble levert til SSB fra dataeier, det vil si på dataeiers dataformat og med informasjon om tidspunkt og rekkefølge for avlevering. Kildedata er en del av statistikkenes dokumentasjon, og kan være en nødvendig kilde for forskning og nye statistikker. Uten kildedataene vil det ikke være mulig å etterprøve SSB sine statistikker. De originale kildedataene vil ofte komprimeres og krypteres etter at relevante deler er transformert til inndata.\n\n(Standardutvalget 2021, 7)\n\nStatistikkloven § 9 Informasjonssikkerhet stiller krav om at direkte identifiserende opplysninger skal behandles og lagres adskilt fra øvrige opplysninger, med mindre det vil være uforenlig med formålet med behandlingen eller åpenbart unødvendig. I henhold til policy om Datatilstander er kildedata i utgangspunktet den eneste datatilstanden som kan inneholde denne type data. I øvrige tilstander skal direkteidentifiserende opplysninger som hovedregel være pseudonymisert. Avvik skal dokumenteres og godkjennes av seksjonsleder som er ansvarlig for avviket.\n\n(Direktørmøtet 2022, 2)\nFordi Kildedata kan inneholde PII1 implementerer Dapla følgende tiltak:\n\nKildedata er lagret adskilt fra andre datatilstander.\nTilgang til dataene begrenses så langt som mulig, kun en begrenset gruppe personer2 har tilgang til kildedata.\nProsessering av kildedata utføres automatisk for minske behov for tilgang til dataene."
  },
  {
    "objectID": "statistikkere/kildedata.html#footnotes",
    "href": "statistikkere/kildedata.html#footnotes",
    "title": "Kildedata",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPersonidentifiserende Informasjon↩︎\nData admins↩︎"
  },
  {
    "objectID": "statistikkere/dapla-lab.html",
    "href": "statistikkere/dapla-lab.html",
    "title": "Dapla Lab",
    "section": "",
    "text": "Dapla Lab er navnet på SSBs arbeidsbenk for statistikkproduksjon og forskning. Det er plattform som gjør det lett å tilby nye tjenester på Dapla, og som gjør det enkelt for SSB-ere å orientere seg i hvilke verktøy som er tilgjengelig på plattformen. Her skal man finne verktøy for koding (f.eks. Jupyterlab, VS Code, RStudio), visualisere data, metadata, osv. Tjenestene som tilbys på Dapla Lab vil typisk de som startes opp ved behov og ikke tjenester som står og kjører kontinuerlig.",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/dapla-lab.html#innlogging",
    "href": "statistikkere/dapla-lab.html#innlogging",
    "title": "Dapla Lab",
    "section": "Innlogging",
    "text": "Innlogging\nAlle som er på SSBs nettverk kan logge seg inn i Dapla ved å gå inn på nettadressen https://lab.dapla.ssb.no/ og velge Logg inn øverst i høyre hjørne. Figur 1 viser landingssiden som møter brukeren.\n\n\n\n\n\n\nFigur 1: Landingsside for Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/dapla-lab.html#funksjonalitet",
    "href": "statistikkere/dapla-lab.html#funksjonalitet",
    "title": "Dapla Lab",
    "section": "Funksjonalitet",
    "text": "Funksjonalitet\n\n\nSelv om Dapla Lab en løsning for å tilby tjenester så er det også en del nyttig funksjonalitet Dapla Lab. Figur 2 viser menyen i Dapla Lab som gir en oversikt over funksjonaliteten som finnes. Under beskriver vi nærmere hvordan man blant annet kan:\n\ndefinere brukernavn og e-post for Git\nlagre GitHub-token\nlagre tjenstekonfigurasjon\nfå oversikt over hvilke tjenester man har kjørende\npause en tjeneste\n\n\n\n\n\n\n\n\n\n\nFigur 2: Menyen i Dapla Lab\n\n\n\n\n\n\nHjem\nHjem tar deg til landingssiden i Dapla Lab, slik som vist i Figur 1. Her finner du nyttige lenker til læringsressurser for Dapla, felleskap på Viva Engage og opprettelse av Dapla-team.\n\n\nMin konto\nNår man logger seg inn i Dapla Lab så skjer det SSB-kontoen til brukeren. Under Min konto kan man se informasjon om sin konto og konfigurere noen nyttige verdier knyttet til brukeren din.\n\nKontoinformasjon\nUnder denne fanen kan lese ut hvilken bruker-id som er benyttet for innloggingen i Dapla Lab, ditt fulle navn og e-postadresse i Dapla Lab. Informasjonen blir definert ved innlogging og kan ikke endres i Dapla Lab.\n\n\n\n\n\n\nFigur 3: Kontoinformasjon under Min konto i Dapla Lab\n\n\n\n\n\nGit\nUnder fanen Git kan du definere brukernavn og e-post for Git, og et personlig tilgangstoken for GitHub. Dette vil deretter kunne brukes i tjenester som brukeren starter i Dapla Lab.\n\n\n\n\n\n\nFigur 4: Git-konfigurasjon under Min konto i Dapla Lab\n\n\n\n\n\nGrensesnittpreferanser\nUnder fanen Grensesnittpreferanser kan man tilpasse Dapla Lab til sine preferanser ved å velge om man blant annet ønsker Dark mode eller ikke. I tillegg kan man definere hvilket språk man ønsker i Dapla Lab. Det finnes også avanserte valg for avanserte brukere. F.eks. man ønsker å se hvilke Helm-kommandoer som kjøres i bakgrunnen når man starter en tjeneste.\n\n\n\n\n\n\nFigur 5: Grensesnittpreferanser under Min konto i Dapla Lab\n\n\n\n\n\n\nTjenestekatalog\nUnder Tjenestekatalogen ligger alle tjenestene som brukeren kan velge å starte.\n\n\n\n\n\n\nFigur 6: Tjenestekatalogen i Dapla Lab\n\n\n\nFigur 6 viser hvilke tjenester som nå er tilgjengelig i Dapla Lab, inkludert en kort beskrivelse av bruksområdet for hver tjeneste. Figur 7 viser hva som møter når de starter Jupyter-tjenesten.\n\n\n\n\n\n\nFigur 7: Tjenestekonfigurasjon i Dapla Lab\n\n\n\nUnder Vennlig navn kan man velge å gi instansen et eget navn. Dette er nyttig siden man kan ønske å starte flere instanser av Jupyter samtidig. I tillegg er det nyttig hvis man ønsker å lagre konfigurasjonen for tjenester i tilfeller der man alltid konfigurerer en tjeneste på samme måte. Man kan lagre en tjenestekonfigurasjon ved å trykke på Lagre-ikonet øverst til høyre.\nVidere kan man velge Versjon av tjenesten. Dette er noe brukeren sjelden vil trenge å forholde seg til.\nEtter dette kan man velge å starte tjenesten ved å trykke på Start-knappen. Men teamet i SSB som tilbyr tjenesten kan også tilby ytterlige tjenestekonfigurasjon som kan være svært nyttig. Siden disse kan variere mellom tjenester så er de forklart i dokumentasjonen til tjenesten.\n\n\nMine tjenester\nUnder Mine tjenester får man oversikt over hvilke tjenester som er startet av brukeren. Figur 8 viser en bruker som har 3 tjenester kjørende. Her får man informasjon om hvilken tjeneste som er starter, hvor lenge den har kjørt, og muligheten til å pause eller avslutte tjenesten.\n\n\n\n\n\n\nFigur 8: Oversikt over brukerens kjørende tjenester\n\n\n\nHvis man trykker på søppelkasse-ikonet så avsluttes tjenesten og alt som er lagret inne i tjenesten blir slettet. Hvis man trykker på pause-knappen så bevares alt som brukeren har lagret under $HOME/work, mens alt annet blir slettet.\n\n\n\n\n\n\nViktigheten av å avslutte ubrukte tjenester\n\n\n\nEn tjeneste som står som aktiv vil reservere ressursene (CPU, GPU, RAM, etc.) som brukeren valgte ved oppstart. Hvis tjenesten ikke benyttes bør derfor brukeren enten avslutte eller pause tjenesten, slik at SSB ikke må betale for ubrukte ressurser.\n\n\n\nMonitorering (kommer snart)\nEn bruker kan trykke på navnet til tjenesten under Mine tjenester for å monitorere tjenesten.",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/dapla-lab.html#datatilgang",
    "href": "statistikkere/dapla-lab.html#datatilgang",
    "title": "Dapla Lab",
    "section": "Datatilgang",
    "text": "Datatilgang\nAlle tjenester som tilbys i Dapla Lab skal håndheve at man kun representere et team i en instans av tjenesten. Dvs. at man må bestemme seg for hvilket team sine data man skal jobbe med, hvis man er medlem av flere team, før man åpner en tjeneste. Grunnen til regelen er at brukerne ofte er medlem av flere team, med tilgang til forskjellig data, som ikke skal behandles samtidig. Tilgangsstyring til data skjer på teamnivå, og medlemskap i et team skal gi brukeren de tilgangene de trenger for å produsere den statistikken eller forskningen.\n\nBøtter som filsystem\nTjenestetilbyderne i Dapla Lab kan tilby datatilgang til bøtter gjennom å tilgjengeliggjøring av data i bøtter som filsystem inne i tjenesten. Det vil si at man kan referere til data som man er vant til på vanlige filsystem, og man kan bruke biblioteker uten å autentisere seg mot bøtter.\nAlle tjenester som tilgjengeliggjør data fra bøtter monterer filsystemet på stien /buckets/. Hvis man åpner en tjeneste med denne løsningen så kan liste ut hvilke bøtter som ble tilgjengeliggjort.",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html",
    "href": "statistikkere/jupyter.html",
    "title": "Jupyter",
    "section": "",
    "text": "Jupyter er en tjeneste på Dapla Lab som lar brukerne kode i Jupyterlab. Tjenesten kommer med R og Python og noen vanlige Jupyterlab-extensions ferdig installert. Målgruppen for tjenesten er brukere som skal skrive produksjonskode i Jupyterlab.\nSiden tjenesten er ment for produksjonskode så er det veldig få R- og Python-pakker som er forhåndsinstallert. Antagelsen er at brukeren/teamet heller bør installere de pakkene de selv trenger, framfor at det ligger ferdiginstallerte pakker som skal dekke behovet til alle brukere/team i SSB. Det reduserer kompleksitet i tjenesten og dermed sannsynligheten for feilsituasjoner.\nFor uerfarne brukere finnes det en egen tjeneste som heter Jupyter-playground. Her er mange av de vanlige R- og Python-pakkene installert og det er opprettet en ferdig kernel som lar brukerne komme i gang fort med koding. Denne tjenesten er ikke tenkt for bruk i produksjonskode.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#forberedelser",
    "href": "statistikkere/jupyter.html#forberedelser",
    "title": "Jupyter",
    "section": "Forberedelser",
    "text": "Forberedelser\nFør man starter Jupyter-tjenesten bør man ha lest kapitlet om Dapla Lab og satt opp Git- og GitHub-konfigurasjonen under Min konto. Deretter gjør du følgende:\n\nLogg deg inn på Dapla Lab\nUnder Tjenestekatalog trykker du på Start-knappen for Jupyter\nGi tjenesten et navn\nÅpne Jupyter konfigurasjoner",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#konfigurasjon",
    "href": "statistikkere/jupyter.html#konfigurasjon",
    "title": "Jupyter",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nFør man åpner en tjeneste kan man konfigurere hvor mye ressurser man ønsker, hvilket team man skal representere, om et GitHub-repo skal klones ved oppstart, og mange andre ting. Valgene man gjør kan også lagres slik at man å slipper å gjøre samme jobb senere. Figur 1 viser første fane i tjenestekonfigurasjonen for Jupyter.\n\n\n\n\n\n\nFigur 1: Detaljert tjenestekonfigurasjon i Dapla Lab\n\n\n\n\nService\nI fanen Service kan man velge hvilke PullPolicy som skal gjelde ved oppstart av Jupyter, samt Version av tjenesten. PullPolicy trenger brukerne sjelden å endre på, mens Version lar brukeren velge hvilken versjon av tjenesten som skal benyttes.\nSiden Jupyter-tjenesten kommer installert med både R- og Python så er det her man velger hvilke versjoner av disse man ønsker. Man kan velge mellom alle tidligere tilbudte kombinasjoner av R og Python. I Figur 1 ser vi av navnet jupyter:r4.4.0-py311 at tjenesten som default vil startes versjon 4.4.0 av R og 3.11 for Python. Etter hvert som nye versjoner av R og Python kommer kan disse tilgjengeliggjøres i tjenesten, men brukeren kan velge å starte en eldre versjon av tjenesten.\n\n\nBuckets\nUnder Buckets kan man velge Enable for å få tilgang til data fra bøtter i tjenesten. I tillegg må man velge hvilket team og tilgangsgruppe man skal representere.\n\n\n\n\n\n\nFigur 2: Detaljert tjenestekonfigurasjon for bøttetilgang i Dapla Lab\n\n\n\nFigur 2 viser at man kan velge blant alle developers-gruppene i alle team man er medlem i. Velger man f.eks. å representere Dapla group teama-developers så kan man få alle standard-bøtter (produkt, frasky, tilsky og egne delt-bøtter) for det teamet. Men man får ikke tilgang til bøtter som man har tilgang til gjennom andre team.\nMan kan også spesifisere andre bøtter som skal tilgjengliggjøres i tjenesten teamet og tilgangsgruppen har tilgang til. Typisk vil dette være andre sine delt-bøtter som teamet har fått tilgang til.\n\n\nResources\nUnder fanen Resources kan man velge hvor mye CPU og RAM man ønsker i tjenesten, slik som vist i Figur 3. Velg så lite trengs for å gjøre jobben du skal gjøre.\n\n\n\n\n\n\nFigur 3: Konfigurasjon av ressurser for Jupyter-tjenesten i Dapla Lab\n\n\n\n\n\nPersistence\nSom default får alle som starter en instans av Jupyter-tjenesten en lokal disk på 10GB inne i tjenesten. Under Persistence-fanen kan man velge å øke størrelsen på disken eller ikke noe disk i det hele tatt. Siden lokal disk i tjenesten hovedsakelig skal benyttes til å lagre en lokal kopi av koden som lagres på GitHub mens man gjør endringer, så bør ikke størrelsen på disken være større enn nødvendig.@fig-dapla-lab-resources viser valgene som kan gjøres under Resource-fanen.\n\n\n\n\n\n\nFigur 4: Konfigurasjon av lokal disk for Jupyter-tjenesten i Dapla Lab\n\n\n\n\n\nGit\nUnder fanen Git kan man konfigurere Git og GitHub slik at det blir lettere å jobbe med inne i tjenesten. Som default arves informasjonen som er lagret under Min konto-Git i Dapla Lab. Informasjonen under tjenestekonfigurasjonen blir tilgjengeliggjort som miljøvariabler i tjenesten. Informasjonen blir også lagt i $HOME/.netrc slik at man kan benytte ikke trenger å gjøre noe mer for å jobbe mot GitHub fra tjenesten.\n\n\n\n\n\n\nFigur 5: Konfigurasjon av Git for Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#datatilgang",
    "href": "statistikkere/jupyter.html#datatilgang",
    "title": "Jupyter",
    "section": "Datatilgang",
    "text": "Datatilgang\nHvis man har valgt å tilgjengeliggjøre data fra et team sitt bøtter i tjenesten, så kan man inspisere dataene fra en terminal inne i tjenesten:\n\nÅpne en instans av Jupyter med data fra bøtter\nÅpne en terminal inne i Jupyter\nGå til mappen med bøttene ved å kjøre dette fra terminalen cd /buckets\nKjør ls -ahl i teminalen for å se på hvilke bøtter som er montert.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#installere-pakker",
    "href": "statistikkere/jupyter.html#installere-pakker",
    "title": "Jupyter",
    "section": "Installere pakker",
    "text": "Installere pakker\nSiden det nesten ikke er installert noen pakker i tjenesten så kan brukeren opprette et ssb-project og installere pakker som vanlig.\nFor å bygge et eksisterende ssb-project så kan brukeren også bruke ssb-project.\nFor å installere R-pakker følger man beskrivelsen for renv.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#slette-tjenesten",
    "href": "statistikkere/jupyter.html#slette-tjenesten",
    "title": "Jupyter",
    "section": "Slette tjenesten",
    "text": "Slette tjenesten\nFor å slette tjenesten kan man trykke på Slette-knappen i Dapla Lab under Mine tjenester. Når man sletter en tjeneste så sletter man hele disken inne i tjenesten og frigjør alle ressurser som er reservert. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#pause-tjenesten",
    "href": "statistikkere/jupyter.html#pause-tjenesten",
    "title": "Jupyter",
    "section": "Pause tjenesten",
    "text": "Pause tjenesten\nMan kan pause tjenesten ved å trykke på Pause-knappen i Dapla Lab under Mine tjenester. Når man pauser så slettes alt påden lokale disken som ikke er lagret under $HOME/work. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#monitorering",
    "href": "statistikkere/jupyter.html#monitorering",
    "title": "Jupyter",
    "section": "Monitorering",
    "text": "Monitorering\n\n\nMan kan moniterere en instans av Jupyter ved å trykke på Jupyter-teksten under Mine tjenester i Dapla Lab, slik som vist i Figur 6.\nDenne funksjonaliteten er under arbeid og mer informasjon kommer snart.\n\n\n\n\n\n\n\n\n\nFigur 6: Monitorering av Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html",
    "href": "statistikkere/jobbe-med-kode.html",
    "title": "Jobbe med kode",
    "section": "",
    "text": "På Dapla jobber vi med utvikling av Python- og R-kode i et Jupyter-miljø. For de som ønsker det, er det mulig å enkelt åpne en notebook med en av våre forhåndskonfigurerte kernels1. Man kan umiddelbart begynne å skrive kode og deretter lagre den i det lokale filsystemet. Dette er ideelt for enkel datautforskning eller for pedagogiske formål.\nNår koden skal settes i produksjon, er det essensielt å ta hensyn til følgende:\n\nResultater bør være reproduserbare.\nKoden må kunne deles med andre.\nKoden bør være organisert slik at den er gjenkjennelig for kollegaer.\n\nFor å lette etterlevelsen av beste praksis for kodeutvikling på Dapla, har vi utviklet et verktøy kalt ssb-project. Dette er et CLI-verktøy2 som enkelt lar deg opprette et prosjekt med en standard mappestruktur, et virtuelt miljø og integrasjon med Git for versjonshåndtering. Som en bonus kan det også opprette et GitHub-repositorium for deg ved behov.\nI dette kapitlet vil vi veilede deg gjennom bruken av ssb-project. Du vil lære å opprette et nytt prosjekt, installere pakker, håndtere versjoner med Git, bygge et eksisterende prosjekt og vedlikeholde prosjektet over tid.\n\n\n\n\n\n\nSSB-project støtter ikke R enda\n\n\n\nPer nå støtter SSB-project kun prosjekter skrevet i Python. Dette skyldes begrensninger ved det populære virtuelle miljø-verktøyet for R, renv. Mens renv effektivt håndterer versjoner av R-pakker, har det ikke kapasitet til å ta vare på spesifikke R-installasjonsversjoner. Dette kan potensielt gjøre det mer utfordrende å reprodusere tidligere publiserte resultater ved bruk av ssb-project. Vi arbeider mot en løsning for å inkludere støtte for R i fremtiden.",
    "crumbs": [
      "Manual",
      "Kode"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#forberedelser",
    "href": "statistikkere/jobbe-med-kode.html#forberedelser",
    "title": "Jobbe med kode",
    "section": "Forberedelser",
    "text": "Forberedelser\nFør du kan ta i bruk ssb-project så er det et par ting som må være på plass:\n\nDu må ha konfigurert Git etter SSB sin standard (les mer om hvordan her).\nHvis du ønsker at ssb-project også skal opprette et GitHub-repo for deg må du også følgende være på plass:\n\nDu må ha en GitHub-bruker (les hvordan her)\nSkru på 2-faktor autentifisering for GitHub-brukeren din (les hvordan her)\nVære koblet mot SSBs organisasjon statisticsnorway på GitHub (les hvordan her)\nOpprette Personal Access Token (PAT) og godkjenne det for bruk mot statisticsnorway (les hvordan her)\n\n\nDet er også å anbefale at du lagrer PAT lokalt slik at du ikke trenger å forholde deg til det når jobber med Git og GitHub. Hvis du har alt dette på plass så kan du bare fortsette å følge de neste kapitlene.",
    "crumbs": [
      "Manual",
      "Kode"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#opprett-ssb-project",
    "href": "statistikkere/jobbe-med-kode.html#opprett-ssb-project",
    "title": "Jobbe med kode",
    "section": "Opprett ssb-project",
    "text": "Opprett ssb-project\n\n\n\n\n\n\nHar du Github bruker? Noe funksjonalitet i ssb-project krever det. Finn ut hvordan ved å lese forrige kapittel.\n\n\n\nssb-project lar deg opprette en prosjekt-mappe med og uten GitHub-repo. La oss ta for oss hver av alternativene.\n\nUten GitHub-repo\nFor å opprette et nytt ssb-project uten GitHub-repo gjør du følgende:\n\nÅpne en terminal. De fleste vil gjøre dette i Jupyterlab på bakke eller sky og da kan de bare trykke på det blå ➕-tegnet i Jupyterlab og velge Terminal.\nFør vi kjører programmet må vi være obs på at ssb-project vil opprette en ny mappe der vi står. Gå derfor til den mappen du ønsker å ha den nye prosjektmappen. For å opprette et prosjekt som heter stat-testprod så skriver du følgende i terminalen:\n\n\n\nterminal\n\nssb-project create stat-testprod\n\n\n\nHvis du stod i hjemmemappen din på når du skrev inn kommandoen over i terminalen, så har du fått mappestrukturen som vises i Figur 1. 3. Den inneholder følgende :\n\n.git-mappe som blir opprettet for å versjonshåndtere med Git.\nsrc-mappe som skal inneholde all koden som utgjør produksjonsløpet.\ntests-mappe som inneholder tester du skriver for koden din.\nLICENCE-fil som skal benyttes for public-repos i SSB.\npoetry.lock-fil som inneholder alle versjoner av Python-pakker som blir brukt.\npyproject.toml-fil som inneholder informasjon om prosjektet og hvilke pakker som blir brukt.\nREADME.md-fil som brukes for tekstlig innhold på GitHub-siden for prosjektet.\n\n\n\n\n\n\n\n\n\nFigur 1: Mappen som ble opprettet av ssb-project.\n\n\n\n\n\n\n\nMed Github-repo\nOver så opprettet vi et ssb-project uten å opprette et GitHub-repo. Hvis du ønsker å opprette et GitHub-repo også må du endre kommandoen over til:\n\n\nterminal\n\nssb-project create stat-testprod --github --github-token='blablabla'\n\nKommandoen over oppretter en mappestruktur slik vi så tidligere, men også et ssb-project som heter stat-testprod med et GitHub-repo med samme navn. Som du ser så må vi da sende med opsjonen --github og PAT med opsjonen --github-token='blablabla'. Repoet i GitHub ser da ut som i Figur 2. Hvis du ønsker å slippe måtte forholde deg til PAT hver gang interagerer med GitHub, kan du følge denne beskrivelsen for å lagre den lokalt. Da kan droppe --github-token='blablabla' fra kommandoen over.\n\n\n\n\n\n\nFigur 2: GitHub-repo som er opprettet av ssb-project\n\n\n\n\n\n\n\n\n\nNår du har opprettet et nytt ssb-project, eller bygget et eksisterende prosjekt, så kan det ta rundt 30 sekunder før kernelen viser seg i Jupterlab-launcher. Vær tålmodig!",
    "crumbs": [
      "Manual",
      "Kode"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#installere-pakker",
    "href": "statistikkere/jobbe-med-kode.html#installere-pakker",
    "title": "Jobbe med kode",
    "section": "Installere pakker",
    "text": "Installere pakker\nNår du har opprettet et ssb-project så kan du installere de python-pakkene du trenger fra PyPI. Men før du installerer en pakke bør gjøre følgende for å sikre deg at du ikke installerer en pakke med skadelig kode:\n\nSøk opp pakken på PyPI.\nSjekk om pakken er et populært/velkjent prosjekt ved å besøke repoet der koden ligger. Antall Stars og Forks på gitHub er en grei indikasjon på dette.\nHvis du er i tvil om pakken er trygg å installere, så kan du spørre kollegaer om de har erfaring med den, eller spørre på en egnet Yammer-kanal i SSB.\nHvis du fortsatt ønsker å installere pakken så anbefaler vi å copy-paste navnet fra PyPi, ikke skrive det inn manuelt når du installerer.\n\nSelve installeringen av pakken gjøres enkelt på følgende måte:\n\nÅpne en terminal i Jupyterlab.\nGå inn i prosjektmappen din ved å skrive:\n\n\n\nterminal\n\ncd &lt;sti til prosjektmappe&gt;\n\n\nLag en branch/utviklingsbranch som f.eks. heter install-pandas:\n\n\n\nterminal\n\ngit checkout -b install-pandas\n\n\nInstaller, f.eks. Pandas, ved å skrive følgende:\n\n\n\nterminal\n\npoetry add pandas\n\n\n\n\n\n\n\nFigur 3: Installasjon av Pandas med ssb-project\n\n\n\nFigur 3 viser hvordan dette vil se ut i en Jupyterlab-terminal. Kommandoen for å installere noe er poetry add etterfulgt av pakkenavnet. Vi ser også at den automatisk legger til Pandas-versjonen i filen poetry.lock.\nDu kan også spesifisere en konkret versjon av pakken som skal installeres med følgende kommando:\n\n\nterminal\n\npoetry add pandas@1.2.3\n\n\nAvinstallere pakker\nOfte eksperimenterer man med nye pakker, og alle blir ikke med videre i produksjonskoden. Det er god praksis å fjerne pakker som ikke brukes, blant annet for å unngå at de blir en sikkerhetsrisiko. Det gjør du enkelt ved å skrive følgende i terminalen:\n\n\nterminal\n\npoetry remove pandas\n\n\n\nOppdatere pakker\nHvis det kommer en ny versjon av en pakke du bruker, så kan du oppdatere den med følgende kommando:\n\n\nterminal\n\npoetry update pandas\n\nhvis du kjører poetry update uten noe pakkenavn, så vil alle pakkene dine oppdateres til siste versjon, med mindre du har spesifisert versjonsbegrensninger i pyproject.toml-filen.\n\n\nUndersøk avhengigheter\nHvis du lurer på hvilke pakker som har hvilke avhengigheter, så kan du lett liste ut dette i terminalen med følgende kommando:\n\n\nterminal\n\npoetry show --tree\n\nDet vil gi en grafisk fremstilling av avhengighetene som vist i Figur 4.\n\n\n\n\n\n\nFigur 4: Visning av pakke-avhengigheter i ssb-project",
    "crumbs": [
      "Manual",
      "Kode"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#push-til-github",
    "href": "statistikkere/jobbe-med-kode.html#push-til-github",
    "title": "Jobbe med kode",
    "section": "Push til GitHub",
    "text": "Push til GitHub\nNår du nå har installert en pakke så har filen poetry.lock endret seg. For at dine samarbeidspartnere skal få tilgang til denne endringen i et SSB-project, så må du pushe en ny versjon av poetry.lock-filen opp Github, og kollegaene må pulle ned og bygge prosjektet på nytt. Du kan gjøre dette på følgende måte etter at du har installert en ny pakke:\n\nVi kan stage alle endringer med følgende kommando i terminalen når vi står i prosjektmappen:\n\n\n\nterminal\n\ngit add -A\n\n\nDeretter commite en endring, dvs. ta et stillbilde av koden i dette øyeblikket, ved å skrive følgende:\n\n\n\nterminal\n\ngit commit -m \"Installert pandas\"\n\n\nPush det opp til GitHub4. Anta at vi gjorde dette i branchen install-pandas som ble opprettet tidligere. Da kan vi skrive følgende :\n\n\n\nterminal\n\ngit push --set-upstream origin install-pandas\n\nDeretter kan kollegaene dine pulle ned endringene og bygge prosjektet på nytt. Vi forklarer hvordan man kan bygge prosjektet på nytt senere i kapitlet.",
    "crumbs": [
      "Manual",
      "Kode"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#dependabot",
    "href": "statistikkere/jobbe-med-kode.html#dependabot",
    "title": "Jobbe med kode",
    "section": "Dependabot",
    "text": "Dependabot\nNår man installerer pakker så vil det etter hvert utvikle seg et sett av potensielt kompliserte avhengigheter mellom disse pakkene. Dette skyldes at en pakke kan benytte seg av funksjonalitet i andre pakker, som igjen benytter seg av funksjonalitet i andre pakker, osv.. Hvis noen finner en sikkerhetssårbarhet i en pakke så kan det fikses ved at en ny versjon av den pakken slippes, som igjen kan få konsekvenser for pakker som er avhengig av denne.\nI SSB er det tilrettelagt for at alle som versjonshåndterer koden sin på GitHub kan skanne pakkene sine for sårbarheter og nye versjoner av pakker med Dependabot. Dependabot hjelper oss med å finne og fikse sårbarheter og gamle pakkeversjoner. Dette er spesielt viktig når man installerer sine egne pakker.\nDependabot sjekker med jevne mellomrom om det finnes oppdateringer i pakkene som er listet i din pyproject.toml-fil, og den tilhørende poetry.lock. Hvis det finnes oppdateringer så vil den lage en pull request som du kan godkjenne. Når du godkjenner den så vil den oppdatere poetry.lock-filen og lage en ny commit som du kan pushe til GitHub. Dependabot gir også en sikkerhetsvarslinger hvis det finnes kjente sårbarheter i pakkene du bruker.\nDet er anbefalt at alle som installerer sine egne pakker i SSB skrur på Dependabot i sine GitHub-repoer.\n\nAktivere Dependabot\nDu kan aktivere Dependabot ved å gi inn i GitHub-repoet ditt og gjøre følgende:\n\nGå inn repoet\nTrykk på Settings for det repoet som vist på Figur 5.\n\n\n\n\n\n\n\nFigur 5: Åpne Settings for et GitHub-repo.\n\n\n\n\nI menyen til venstre velger du Code security and analysis\nUnder seksjonen Dependabot velger Enable på minst Dependabot alerts og Dependabot security updates, slik som vist i Figur 6.\n\n\n\n\n\n\n\nFigur 6: Skru på Dependabot i GitHub.\n\n\n\nNår du har gjort dette vil GitHub varsle deg hvis det finnes en kjent sårbarhet i pakkene som benyttes.\n\n\nOppdatere pakker\nHvis en av pakkene du bruker kommer med en oppdatering, så vil Dependabot lage en pull request (PR) i GitHub som du kan godkjenne. Dependabot sjekker også om oppdateringen er i konflikt med andre pakker du bruker. Hvis det er tilfellet så vil den lage en pull request som oppdaterer alle pakkene som er i konflikt.\nHvis en av pakkene du bruker har en kjent sikkerhetssårbarhet, så vil Dependabot varsle deg om dette under Security-fanen i GitHub-repoet ditt. Hvis du trykker på View Dependabot alerts så vil du få en oversikt over alle sårbarhetene som er funnet, og hvilken alvorlighetsgrad den har. Hvis du trykker på en av sårbarhetene så vil du få mer informasjon om den, og du kan trykke på Create pull request for å oppdatere pakken.\nSom nevnt over kan du enkelt oppdatere pakker fra GitHub ved hjelp av Dependabot. Men det finnes tilfeller der du vil teste om en oppdatering gjør at deler av koden din ikke fungerer lenger. Anta at du bruker en Python-pakken Pandas i koden din, og at du får en pull request fra Dependabot om å oppdatere den fra versjon 1.5 til 2.0. Hvis du ønsker å teste om koden din fortsatt fungerer med den nye versjonen av Pandas, så kan du gjøre dette i Jupyterlab ved å følge ved å lage en branch som f.eks. heter update-pandas. Deretter kan du installere den nye versjonen av Pandas med følgende kommando fra terminalen:\n\n\nterminal\n\npoetry update pandas@2.0\n\nHvis du nå kjører koden din kan du teste om den fortsatt fungerer som forventet. Gjør den ikke det kan du tilpasse koden din og pushe endringene til Github. Deretter kan du godkjenne pull requesten fra Dependabot og merge den. Etter dette kan du slette den PR-en som Dependabot lagde for deg.",
    "crumbs": [
      "Manual",
      "Kode"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#bygg-eksisterende-ssb-project",
    "href": "statistikkere/jobbe-med-kode.html#bygg-eksisterende-ssb-project",
    "title": "Jobbe med kode",
    "section": "Bygg eksisterende ssb-project",
    "text": "Bygg eksisterende ssb-project\nNår vi skal samarbeide med andre om kode så gjør vi dette via GitHub. Når du pusher koden din til GitHub, så kan samarbeidspartnere pulle ned koden og jobbe videre med den. Men når de henter ned koden så vil de bare hente ned selve koden, ikke pakker og Python-versjonen som ble brukt. De må installere alt som du hadde installert. I tillegg trenger de en kernel hvis de skal jobbe i Jupyterlab. ssb-project gjør det svært enkelt å bygge opp det du trenger, siden det virtuelle miljøet har all informasjon om hva som trengs.\nFor at samarbeidspartneren din skal kunne bygge miljøet på nytt, må de ha gjort en minimal konfigurering av Git. Les mer om hvordan du frem for å gjøre dette her.\nFor å bygge opp et eksisterende miljø gjør du følgende:\n\nFørst må du kopiere prosjektet ned lokalt, eller klone repoet med git-terminologi\n\n\n\nterminal\n\ngit clone https://github.com/statisticsnorway/&lt;prosjektnavn&gt;\n\n\nGå inn i mappen du klonet\n\n\n\nterminal\n\ncd &lt;prosjektnavn&gt;\n\n\nSkape et virtuelt miljø og installere en tilsvarende Jupyter kernel med\n\n\n\nterminal\n\nssb-project build",
    "crumbs": [
      "Manual",
      "Kode"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#slette-ssb-project",
    "href": "statistikkere/jobbe-med-kode.html#slette-ssb-project",
    "title": "Jobbe med kode",
    "section": "Slette ssb-project",
    "text": "Slette ssb-project\nDet vil være tilfeller hvor man ønsker å slette et ssb-project, enten fordi man ikke trenger koden lenger eller fordi man bare testet litt.\n\nLokalt\nHvis man jobber med flere prosjekter så kan det fort bli mange Jupyter kerneler hengende igjen. Derfor er det også mulighet å kjøre\n\n\nterminal\n\nssb-project clean stat-testprod\n\nsom sletter Jupyter-kernelen og de installerte pakkene i prosjektet. Hvis du også ønsker å slette selve mappen med kode må du gjøre det manuelt5:\n\n\nterminal\n\nrm -rf ~/stat-testprod/\n\nProsjektmappen over lå direkte i hjemmemappen min og hjemmemappen på Linux kan alltid referes til med et tilda-tegn ~.\n\n\nArkiver GitHub-repo\nGitHub-repoer som er opprettet under SSB sin organinasjons statisticsnorway på GitHub kan ikke slettes, bare arkiveres. Grunnen er at hvis man oppdager en sårbarhet senere så er det viktig å kunne se repoet for å forstå hva som har skjedd.\nHvis du ikke trenger et GitHub-repo lenger kan man arkivere repoet. Det gjør du på følgende måte:\n\nGi inn i repoet Settings slik som vist med rød pil i Figur 7.\n\n\n\n\n\n\n\nFigur 7: Settings for repoet.\n\n\n\n\nUnder General scroller du deg ned til delen som heter Danger Zone og velger Archive this repository, slik som vist på Figur 8.\n\n\n\n\n\n\n\nFigur 8: Arkivering av et repo.\n\n\n\n\nI dialogboksen som dukker opp fyller du inn reponavnet som beskrevet og trykker på I understand the consequences, archive this repository.\n\nNår det er gjort så er repoet lesbart, men man kan ikke jobbe med det. Men som vi ser av @#fig-github-repo-settings-archive-warning kan man omgjøre arkiveringen senere hvis det skulle være ønskelig.",
    "crumbs": [
      "Manual",
      "Kode"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#spark-i-ssb-project",
    "href": "statistikkere/jobbe-med-kode.html#spark-i-ssb-project",
    "title": "Jobbe med kode",
    "section": "Spark i ssb-project",
    "text": "Spark i ssb-project\nFor å kunne bruke Spark i et ssb-project må man først installere pyspark. Det gjør du ved å skrive følgende i en terminal:\n\n\nterminal\n\npoetry add pyspark==$(pip show pyspark | grep Version | egrep -o \"([0-9]{1,}\\.)+[0-9]{1,}\") --no-dev\n\nHer installerer du samme versjon av pyspark som på Jupyterlab.\nVidere kan vi konfigurere Spark til å enten kjøre på lokal maskin eller på flere maskiner (såkalte clusters). Under beskriver vi begge variantene.\n\nLokal maskin\nOppsettet for Pyspark på lokal maskin er det enkleste å sette opp siden Pyspark vil ha direkte tilgang til det lokale filsystemet. Man kan bruke miljøvariabelen PYSPARK_PYTHON til å peke på det virtuelle miljøet, og dermed vil Pyspark også ha tilgang til alle pakkene som er installert der. I en notebook vil dette kunne settes opp slik:\n\n\nnotebook\n\nimport os\nimport subprocess\n\n# Finner filstien til det virtuelle miljøet\npython_path = subprocess.run(['poetry', 'run', 'which', 'python'],\n                             capture_output=True, text=True).stdout.rstrip('\\n')\n\nos.environ[\"PYSPARK_PYTHON\"] = python_path\nos.environ[\"PYSPARK_SUBMIT_ARGS\"] = os.environ[\"PYSPARK_LOCAL_SUBMIT_ARGS\"]\n\nNår du oppretter en Notebook og bruker den kernelen du har laget så må du alltid ha denne på toppen av notebooken. Det initialiserer Spark slik at det kan brukes i notebooken.\n\n\nnotebook\n\n%run /usr/local/share/jupyter/kernels/pyspark_local/init.py\n\nDette scriptet vil sette et spark objekt som brukes for å kalle API’et til pyspark.\n\n\nCluster\nHvis man vil kjøre Pyspark i et cluster (dvs. på flere maskiner) så vil databehandlingen foregå på andre maskiner som ikke har tilgang til det lokale filsystemet. Man må dermed lage en “pakke” av det virtuelle miljøet på lokal maskin og tilgjengeliggjøre dette for alle maskinene i clusteret. For å lage en slik “pakke” kan man bruke et bibliotek som heter venv-pack. Dette kan kjøres fra et terminalvindu slik:\n\n\nterminal\n\nvenv-pack -p .venv -o pyspark_venv.tar.gz\n\nMerk at kommandoen over må kjøres fra rot-mappen i prosjektet ditt. Her er pyspark_venv.tar.gz et tilfeldig valgt filnavn, men dette filnavnet skal brukes videre i notebooken.\n\n\nnotebook\n\nimport os\nimport subprocess\n\n# Miljøvariabel som peker på en utpakket versjon av det virtuelle miljøet\nos.environ[\"PYSPARK_PYTHON\"] = \"./environment/bin/python\"\n\n# Legg til et flagg, --archives, som peker på \"pakken\" med det virtuelle miljøet\nconf = os.environ[\"PYSPARK_K8S_SUBMIT_ARGS\"].split(' ')\nlast_index = conf.index('pyspark-shell')\nconf[last_index:last_index] = ['--archives', 'pyspark_venv.tar.gz#environment']\nos.environ[\"PYSPARK_SUBMIT_ARGS\"] = ' '.join(conf)\n\nNår du oppretter en Notebook og bruker den kernelen du har laget så må du alltid ha denne på toppen av notebooken. Det initialiserer Spark slik at det kan brukes i notebooken.\n\n\nnotebook\n\n%run /usr/local/share/jupyter/kernels/pyspark_k8s/init.py\n\nDette scriptet vil sette et spark objekt som brukes for å kalle API’et til pyspark.",
    "crumbs": [
      "Manual",
      "Kode"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#tips-og-triks",
    "href": "statistikkere/jobbe-med-kode.html#tips-og-triks",
    "title": "Jobbe med kode",
    "section": "Tips og triks",
    "text": "Tips og triks\nI denne delen av kapitlet vil vi gi deg noen tips og triks som kan være nyttige når du jobber med ssb-project.\n\nPoetry\nssb-project bruker Poetry for å håndtere virtuelle miljøer. Poetry er et verktøy som gjør det enkelt å installere pakker og håndtere versjoner av disse. Det er også Poetry som håndterer Jupyter-kernelen for deg.\nHvis du etterlyser funksjonalitet i et ssb-project så kan det være nyttig å lese dokumentasjonen til Poetry for å se om det er mulig å få til det du ønsker.\n\n\nFull disk på Dapla\nDet “lokale” filsystemet på Dapla har kun 10GB diskplass. Har du mange virtuelle miljøer på denne disken kan det fort bli fullt, siden alle pakker blir installert her. Vanligvis er det 2 grunner til at disken blir full:\n\nFor mange virtuelle miljøer (ssb-projects) lagret lokalt.\nDette vil ofte kunne løses ved å slette virtuelle miljøer som ikke lenger er i bruk. Hvis du har 5 virtuelle miljøer som hver bruker 1GB, og du kun jobber på en av de nå, så vil du frigjøre 40% av disken ved å slette 4 av dem. Husk at det permanente lagringsstedet for kode er på GitHub, og du kan alltid klone ned et prosjekt senere og bygge det hvis det trengs.\n/home/jovyan/.cache/ har blitt for stort.\nDette er en mappe som brukes av applikasjoner til å lagre midlertidig data slik at de kan kjøre raskere. Denne kan bli ganske stor etter hvert. Ofte kan man frigjøre flere GB ved å slette denne. Du sletter denne mappen ved å skrive følgende i en terminal:\n\n\n\nterminal\n\nrm -rf /home/jovyan/.cache/\n\nHvis du opplever at disken er full, så kan det anbefales å undersøke hvilke mapper som tar størst plass med følgende kommando i terminalen:\n\n\nterminal\n\ncd ~ && du -h --max-depth=5 | sort -rh | head -n 10 && cd -\n\nKommandoen over sjekker, fra hjemmemappen din, hvilke mapper og undermapper som tar mest plass. Den viser de 10 største mappene. Hvis du ønsker å se flere mapper så kan du endre tallet etter head -n. Hvis du ønsker å se alle mapper så kan du fjerne head -n. --max-depth=5 betyr at den kun sjekker mapper som er 5 mapper dype fra hjemmemappen din.\nNår du har gjort det kan selv vurdere hvilke som kan slettes for å frigjøre plass.\n\n\nHold prosjektet oppdatert\nHvis du sitter med en lokal kopi av prosjektet ditt, og flere andre jobber med den samme kodebasen, så er det viktig at du holder din lokale kopi oppdatert. Hvis du jobber i en branch på en lokal kopi, bør du holde denne oppdatert med main-branchen på GitHub. Det er vanlig Git-praksis. Når man også bruker ssb-project, så man huske å også bygge prosjektet på nytt hver gang det er endringer som er gjort av andre i poetry.lock.-filen.",
    "crumbs": [
      "Manual",
      "Kode"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#footnotes",
    "href": "statistikkere/jobbe-med-kode.html#footnotes",
    "title": "Jobbe med kode",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nEn kernel refererer til en Python- eller R-installasjon som er optimalisert for bruk med Jupyterlab Notebooks.↩︎\nCLI = Command-Line-Interface, som betyr et program designet for bruk i terminalen med kommandoer.↩︎\nFiler og mapper som starter med punktum er skjulte med mindre man ber om å se dem. I Jupyterlab kan disse vises i filutforskeren ved å velge View fra menylinjen, og deretter velge Show hidden files. I en terminal skriver man ls -a for å se de.↩︎\nÅ pushe til GitHub uten å sende ved Personal Access Token fordrer at du har lagret det lokalt så Git kan finne det. Her et eksempel på hvordan det kan gjøres.↩︎\nDette kan også gjøres ved å høyreklikke på mappen i Jupyterlab sin filutforsker og velge Delete.↩︎",
    "crumbs": [
      "Manual",
      "Kode"
    ]
  },
  {
    "objectID": "statistikkere/git-og-github.html",
    "href": "statistikkere/git-og-github.html",
    "title": "Git og Github",
    "section": "",
    "text": "I SSB anbefales det man versjonhåndterer koden sin med Git og deler koden via GitHub. For å lære seg å bruke disse verktøyene på en god måte er det derfor viktig å forstå forskjellen mellom Git og Github. Helt overordnet er forskjellen følgende:\n\nGit er programvare som er installert på maskinen du jobber på og som sporer endringer i koden din.\nGitHub er et slags felles mappesystem på internett som lar deg dele og samarbeide med andre om kode.\n\nAv definisjonene over så skjønner vi at det er Git som gir oss all funksjonalitet for å lagre versjoner av koden vår. GitHub er mer som et valg av mappesystem. Men måten kodemiljøene våre er satt opp på Dapla så har vi ingen fellesmappe som alle kan kjøre koden fra. Man utvikler kode i sin egen hjemmemappe, som bare du har tilgang til, og når du skal samarbeide med andre, så må du sende koden til GitHub. De du samarbeider med må deretter hente ned denne koden før de kan kjøre den.\nI dette kapittelet ser vi nærmere på Git og Github og hvordan de er implementert i SSB. Selv om SSB har laget programmet ssb-project for å gjøre det lettere å bl.a. forholde seg til Git og GitHub, så vil vi dette kapittelet forklare nærmere hvordan det funker uten dette hjelpemiddelet. Forhåpentligvis vil det gjøre det lettere å håndtere mer kompliserte situasjoner som oppstår i arbeidshverdagen som statistikker.\n\n\nGit er terminalprogram som installert på maskinen du jobber. Hvis man ikke liker å bruke terminalen finnes det mange pek-og-klikk versjoner av Git, blant annet i Jupyterlab, SAS EG og RStudio. Men typisk vil det en eller annen gang oppstå situasjoner der det ikke finnes løsninger i pek-og-klikk versjonen, og man må ordne opp i terminalen. Av den grunn velger vi her å fokusere på hvordan Git fungerer fra terminalen. Vi vil også fokusere på hvordan Git fungerer fra terminalen i Jupyterlab på Dapla.\n\n\nGit er en programvare for distribuert versjonshåndtering av filer. Det vil si at den tar vare på historien til koden din. At Git er distribuert betyr at alle som jobber med koden har en kopi av koden hos seg selv. Når man ønsker å dele koden med andre, så laster man opp koden til et felles kodelager som GitHub. Typisk vil Git brukes til å passe på historien til rene tekstfiler, f.eks. kode-script, hvor det gir mening å se på forskjeller mellom filen på ulike tidspunkter. Men når det er sagt, så kan Git også brukes til å følge med på endringer i andre filtyper, f.eks. binære filer som bilder, PDF-filer, etc.. Men binære filer er ikke så vanlig inkludere i Git-prosjekter siden forskjellene i historikken til filene er vanskelig å forstå for mennesker.\nMan aktiverer Git på en mappe i filsystemet sitt med kommandoen git init når man står i mappen som skal versjonshånderes. Da vil Git versjonshåndtere alle filer som er i den mappen og i eventuelle undermapper. Når du så gjør endringer på en fil i mappen, så vil Git registrere endringer. Ønsker du at endringen skal bli et punkt i historikken til prosjektet, så må du først legge til filen i Git med kommandoen git add filnavn. Når du har gjort dette, så kan du lagre endringen med kommandoen git commit -m \"Din melding her\". Når du har gjort dette, så vil endringen være lagret i Git. Når du har gjort mange endringer, så kan du sende endringene til GitHub med kommandoen git push. Når du har gjort dette, så vil endringene være synlige for alle som har tilgang til GitHub-prosjektet.\nI SSB anbefaler vi at du starter et nytt Git-prosjekt ved å benytte ssb-project. Da vil du ikke bare aktivere Git, men du kan også få implementert en del andre gode praksiser for å holde koden din ryddig, oversiktlig og sikker.\nMen før vi kan begynne å bruke Git må vi konfigurere vår egen bruker for Git, som er forklart i neste seksjon.\nLes mer om Git på https://git-scm.com/.\n\n\n\n\n\n\n\n\n\nBruk SSB sin konfigurasjon!\n\n\n\nAlle som bruker GitHub i SSB bør bruke SSB sin Git-konfigurasjon. Hvis ikke kan man bl.a. risikere at output fra Notebooks blir pushet til GitHub. Du kan bruke SSBs konfigurasjon ved å kjøre ssb-gitconfig.py i terminalen og svare på spørsmålene som dukker opp.\n\n\nFor å jobbe med Git så må man konfigurere brukeren sin slik at Git vet hvem som gjør endringer i koden. I praksis betyr det at du må ha filen .gitconfig på hjemmeområdet ditt (f.eks. /home/jovyan/.gitconfig på Dapla) med noe grunnleggende informasjon:\n\n\n.gitconfig\n\n# /home/jovyan/.gitconfig på Dapla\n[user]\n        name = Ola Nordmann\n        email = &lt;ini&gt;@ssb.no\n\nMed denne konfigurasjonen så kan man bruke Git lokalt. Men skal man også bruke GitHub i SSB, dvs. dele kode med andre, så må man også legge til mer informasjon i .gitconfig. Heldigvis er det skrevet et script som gjør dette for deg. For å få anbefalt konfigurasjon for Git så kan du kjøre følgende kommando i terminalen:\n\n\nterminal\n\nssb_gitconfig.py\n\nDette scriptet vil spørre deg om ditt brukernavn i SSB, og så vil det opprette en fil som heter .gitconfig i hjemmeområdet ditt. Denne filen vil inneholde informasjon om brukernavn, e-post og tilgang til GitHub. I tillegg vil den sørge for at ditt ssb-project ikke sender output fra notebooks til GitHub.\n\n\n\nPå Dapla er det Jupyterlab som er utviklingsmiljøet for kode. I Jupyterlab vil man ofte jobbe i notebooks, og det har noen konsekvenser for versjonshåndtering. En notebook er en ipynb-fil som inneholder både tekst og kode. Åpner vi disse filene i Jupyterlab så ser vi det kjente notebook-grensesnittet med kode- og markdown-celler. Men egentlig er det bare et grensesnitt bygd over en json-fil1. Denne json-filen inneholder metadata og masse annet som gjør det vanskelig å se forskjellen på en fil over tid. Dette er derfor noe som å fikses før Git blir et nyttig verktøy for disse filene.\nI SSB sin anbefalte konfigurasjon setter opp nbdime for å få leselige differ mellom notebooks. Har du satt opp Git med ssb-gitconfig.py så vil dette være automatisk satt opp for deg.\nDet finnes også alternativer til å bruke nbdime. På Dapla er Jupytext installert for de som ikke ønsker å versjonshåndtere ipynb-filer, men heller vil bruke rene script-filer som .py- eller .R-filer. Måten Jupytext gjør dette på er konvertere f.eks. en ipynb-fil py-fil for deg, Den kan gjøre det automatisk når du lagrer, eller du kan gjøre det manuelt. Med denne tilnærmingen så kan du be Git ignorere alle ipynb-filer og bare versjonshåndtere de rene script.filene. Oppsett er Jupytext er ikke en del av SSB sin standard-konfigurasjon og er noe du må sett opp selv.\n\n\n\nGit er veldig sterkt verktøy med mange muligheter. Det er ikke tenkt at alt skal adresseres i dette kapitlet. Men det er noen vanlig operasjoner som er så vanlige at alle som jobber med kode i SSB bør kjenne dem.\nVi har tidligere nevnt at kommandoen for å aktivere versjonshåndtering med Git på en mappe, er git init. Dette gjøres også automatisk når man oppretter et nytt ssb-project.\nHva skjer hvis man gjør en endring i en fil i mappa? For det første kan du kjøre kommandoen git status for å se hvilke endringer Git har oppdaget. Men det betyr ikke at endringene er under versjonskontroll enda. For å fortelle Git om at disse endringene skal registreres så må du kjøre git add &lt;filnavn&gt;. Men fortsatt er ikke endringene et punkt i historien til koden din. For å gjøre det må du kjøre git commit -m \"Din melding her\". Ved å gjøre det så har du laget et unikt punkt i historien til koden din, med en egen ID, som du kan gå tilbake til senere hvis du ønsker.\nNår man utvikler kode så gjør man det fra såkalte branches2. Hvis vi tenker oss at din eksisterende kodebase er stammen på et tre (ofte kalt master eller main), så legger Git opp til at man gjør endringer på denne koden via branches eller grener av treet. Med andre ord holder vi stammen/master urørt helt til vi vet at endringen fungerer som den skal. Til slutt merger vi den inn i master. Vi kan opprette en ny branch og gå inn i den ved å skrive git checkout -b &lt;branch navn&gt;. Da står du i en branch og kan bruke kommandoer som git add og git commit som vist tidligere.\nVi kan merge endringene fra vår branch inn i main ved å først gå inn i main, git switch main, og deretter skrive git merge &lt;branch navn&gt;. Men typisk vil ikke dette være fremgangsmåten i SSB. Når man er fornøyd med endringene i en branch, så vil man pushe den opp til GitHub, slik at en kollega kan vurdere om den skal merges inn i main. Dermed gjøres selve mergen i GitHub-grensenittet. Vi skal se nærmere på GitHub i neste kapittel.\n\n\n\n\nGitHub er et nettsted som bl.a. fungerer som vårt felles mappesystem for deling av kode. SSB har sin egen organisasjonskonto med navn statisticsnorway. Men selv om SSB har en organisasjonskonto på GitHub må alle ansatte opprette sin egen brukerprofil, og knytte den mot SSB sin organisasjonskonto. Under forklarer vi hvordan du gjør dette.\n\n\nDette kapittelet er bare relevant hvis man ikke har en GitHub-brukerkonto fra før. For å bruke ssb-project-programmet til å generere et remote repo på GitHub må du ha en konto. Derfor starter vi med å gjøre dette. Det er en engangsjobb og du trenger aldri gjøre det igjen.\n\n\n\n\n\n\nSSB har valgt å ikke sette opp SSB-brukerne til de ansatte som GitHub-brukere. En viktig årsak er at er en GitHub-konto ofte regnes som en del av den ansattes CV. For de som aldri har brukt GitHub før kan det virke fremmed, men det er nok en fordel på sikt når alle blir godt kjent med denne arbeidsformen.\n\n\n\nSlik gjør du det:\n\nGå til https://github.com/\nTrykk Sign up øverst i høyre hjørne\nI dialogboksen som åpnes, se Figur 1, skriver du inn din e-postkonto og lager et passord. Dette trenger ikke være din SSB-bruker og e-post. Vi anbefaler at du bruker en personlig e-postkonto og velger ditt eget passord. Det samme gjelder brukernavn også.\n\n\n\n\n\n\n\nFigur 1: Dialogboks for opprettelse av GitHub-bruker.\n\n\n\nDu har nå laget en egen GitHub-bruker. I neste steg skal vi knytte denne kontoen til din SSB-bruker.\n\n\n\nHvis du har fullført forrige steg så har du nå en GitHub-konto. Hvis du står på din profil-side så ser den ut som i Figur 2.\n\n\n\n\n\n\nFigur 2: Et eksempel på hjemmeområdet til en GitHub-bruker\n\n\n\nDet neste vi må gjøre er å aktivere 2-faktor autentisering, siden det er dette som benyttes i SSB. Hvis du står på siden i bildet over, så gjør du følgende for å aktivere 2-faktor autentisering mot GitHub:\n\n\n\nTrykk på den lille pilen øverst til høyre og velg Settings(se Figur 3).\nDeretter velger du Password and authentification i menyen til venstre.\nUnder Two-factor authentication trykker du på Enable.\n\n\n\n\n\n\n\n\n\nFigur 3: Åpne settings for din GitHub-bruker.\n\n\n\n\n\n\nFigur 4 viser dialogboksen som vises. Velg Enable two-factor authentification.\n\n\n\n\n\n\n\nFigur 4: Dialogboks som åpnes når 2FA skrus på første gang.\n\n\n\n\nFigur 5 viser dialogboksen som vises for å velge hvordan man skal autentisere seg. Her anbefales det å velge Set up using an app, slik at du kan bruke Microsoft Authenticator-appen på din mobil.\n\n\n\n\n\n\n\nFigur 5: Dialogboks for å velge hvordan man skal autentisere seg med 2FA.\n\n\n\nFigur 6 viser QR-koden som vises. Denne skal vi bruke i neste steg.\n\n\n\n\n\n\nFigur 6: QR-kode som skannes av Microsoft Authenticator.\n\n\n\n\n\n\nStrekkoden over skal skannes i din Microsoft Authenticator-app på mobilen, som vist i Figur 7. Åpne appen, trykk på Bekreftede ID-er, og til slutt trykk på Skann QR-kode. Deretter skanner du QR-koden fra punkt 5.\nNår koden er skannet har du fått opp følgende bilde på appens hovedside (se bilde til høyre). Skriv inn den 6-siffer koden på GitHub-siden med QR-koden.\nTil slutt lagrer du Recovery-codes et trygt sted som bare du har tilgang til.\n\n\n\n\n\n\n\n\n\nFigur 7: Mobilappen Microsoft authenticator\n\n\n\n\n\nNå har vi aktivert 2-faktor autentisering for GitHub og er klare til å knytte vår personlige konto til vår SSB-bruker på SSBs “Github organisation” statisticsnorway.\n\n\n\nI forrige steg aktiverte vi 2-faktor autentisering for GitHub. Det neste vi må gjøre er å koble oss til Single Sign On (SSO) for SSB sin organisasjon på GitHub:\n\nTrykk på lenken https://github.com/orgs/statisticsnorway/sso\nI dialogboksen som dukker opp trykker du på Continue, slik som vist i Figur 8.\n\n\n\n\n\n\n\nFigur 8: Single Sign on (SSO) for SSB sin organisasjon på GitHub\n\n\n\nNår du har gjennomført dette så har du tilgang til statisticsnorway på GitHub. Går du inn på denne lenken så skal du nå kunne lese både Public, Private og Internal repoer, slik som vist i Figur 9.\n\n\n\n\n\n\nFigur 9: Medlemsvisning for SSB sin GitHub-organisasjon.\n\n\n\n\n\n\nNår vi skal jobbe med SSB-kode som ligger lagret hos statistcsnorway på GitHub, så må vi autentisere oss. Måten vi gjøre det på er ved å generere et Personal Access Token (ofte forkortet PAT) som vi oppgir når vi vil hente eller oppdatere kode på GitHub. Da sender vi med PAT for å autentisere oss for GitHub.\n\n\nFor å lage en PAT som er godkjent mot statisticsnorway så gjør man følgende:\n\nGå til din profilside på GitHub og åpne Settings slik som ble vist Seksjon 1.2.2.\nVelg Developer Settings i menyen til venstre.\nI menyen til venstre velger du Personal Access Token, og deretter Tokens (classic).\nVelg Generate new token og deretter Generate new token (classic).\nUnder Note kan du gi PAT’en et navn. Velg et navn som er intuitivt for deg. Hvis du skal bruke PAT til å jobbe mot Dapla, så ville jeg ganske enkelt kalt den dapla. Hvis du skal bruke den mot bakkemiljøet ville jeg kalt den prodsone eller noe annet som gjør det lett for det skjønne innholdet i ettertid.\nUnder Expiration velger du hvor lang tid som skal gå før PAT blir ugyldig. Dette er en avvening mellom sikkerhet og hva som er praktisk. Det anbefales at du velger 365 dager. Når PAT utløper må du gjenta stegene i dette kapittelet.\nUnder Select scopes velger du repo og workflow slik som vist i Figur 10.\n\n\n\n\n\n\n\nFigur 10: Gi token et kort og beskrivende navn\n\n\n\n\nTrykk på Generate token nederst på siden og du får noe lignende det du ser i Figur 11.\n\n\n\n\n\n\n\nFigur 11: Token som ble generert.\n\n\n\n\nKopier deretter PAT til en midlertidig fil. Grunnen er at du aldri vil se det igjen her etter at vi har gjennomført neste steg.\nDeretter trykker du på Configure SSO og velger Authorize ved siden statisticsnorway, slik som vist i Figur 12. Svar deretter på spørsmålene som dukker opp.\n\n\n\n\n\n\n\nFigur 12: Autorisering av Token mot SSBs GiHub-organisasjon.\n\n\n\nVi har nå opprettet en PAT som er godkjent for bruk mot SSB sin kode på GitHub. Det betyr at hvis vi vil jobbe med Git på SSB sine maskiner i sky eller på bakken, så må vi sendte med dette tokenet for å få lov til å jobbe med koden som ligger på statisticsnorway på GitHub.\n\n\n\nDet er ganske upraktisk å måtte sende med tokenet hver gang vi skal jobbe med GitHub. Vi bør derfor lagre det lokalt der vi jobber, slik at Git automatisk finner det. Det finnes mange måter å gjøre dette på og det er ikke bestemt hva som skal være beste-praksis i SSB. Men en måte å gjøre det er via en .netrc-fil. Vi oppretter da en fil som heter .netrc på vårt hjemmeområde, og legger følgende informasjon på en (hvilken som helst) linje i filen:\n\n\n~/.netrc\n\nmachine github.com login &lt;github-bruker&gt; password &lt;Personal Access Token&gt;\n\nGitHub-bruker er da din personlige bruker og IKKE brukernavnet ditt i SSB. Personal Access Token er det vi lagde i forrige kapittelet.\nEn veldig enkel måte å lagre dette er som følger. Anta at min personlige GitHub-bruker er SSB-Chad og at min Personal Access Token er blablabla. Da kan jeg gjøre følgende for å lagre det i .netrc:\n\nGå inn i Jupyterlab og åpne en Python-notebook.\nI den første kodecellen skriver du:\n\n\n\nterminal\n\n!echo \"machine github.com login SSB-Chad password blablabla\" &gt;&gt; ~/.netrc\n\nAlternativt kan du droppe det utropstegnet og kjøre det direkte i en terminal. Det vil gi samme resultat. Koden over legger til en linje med teksten\nmachine github.com login SSB-Chad password blablabla\ni en .netrc-fil på din hjemmeområdet, uanvhengig av om du har en fra før eller ikke. Hvis du har en fil fra før som allerede har et token fra GitHub, ville jeg nok slettet det før jeg legger en et nytt token.\nHver gang du jobber mot GitHub vil Git sjekke om informasjon om autentisering ligger i denne filen, og bruke den hvis den ligger der.\n\n\n\nI eksempelet over lagde vi en PAT som var gyldig i 90 dager. Dermed vil du ikke kunne jobbe mot GitHub med dette tokenet etter 90 dager. For å oppdatere tokenet gjør du følgende:\n\nLag et nytt PAT ved å repetere Seksjon 1.2.4.1.\nI miljøet der du skal jobbe med Git og GitHub går du inn i din .netrc og bytter ut token med det nye.\n\nOg med det er du klar til å jobbe mot statisticsnorway på GitHub.",
    "crumbs": [
      "Manual",
      "Kode",
      "Git og Github"
    ]
  },
  {
    "objectID": "statistikkere/git-og-github.html#git",
    "href": "statistikkere/git-og-github.html#git",
    "title": "Git og Github",
    "section": "",
    "text": "Git er terminalprogram som installert på maskinen du jobber. Hvis man ikke liker å bruke terminalen finnes det mange pek-og-klikk versjoner av Git, blant annet i Jupyterlab, SAS EG og RStudio. Men typisk vil det en eller annen gang oppstå situasjoner der det ikke finnes løsninger i pek-og-klikk versjonen, og man må ordne opp i terminalen. Av den grunn velger vi her å fokusere på hvordan Git fungerer fra terminalen. Vi vil også fokusere på hvordan Git fungerer fra terminalen i Jupyterlab på Dapla.\n\n\nGit er en programvare for distribuert versjonshåndtering av filer. Det vil si at den tar vare på historien til koden din. At Git er distribuert betyr at alle som jobber med koden har en kopi av koden hos seg selv. Når man ønsker å dele koden med andre, så laster man opp koden til et felles kodelager som GitHub. Typisk vil Git brukes til å passe på historien til rene tekstfiler, f.eks. kode-script, hvor det gir mening å se på forskjeller mellom filen på ulike tidspunkter. Men når det er sagt, så kan Git også brukes til å følge med på endringer i andre filtyper, f.eks. binære filer som bilder, PDF-filer, etc.. Men binære filer er ikke så vanlig inkludere i Git-prosjekter siden forskjellene i historikken til filene er vanskelig å forstå for mennesker.\nMan aktiverer Git på en mappe i filsystemet sitt med kommandoen git init når man står i mappen som skal versjonshånderes. Da vil Git versjonshåndtere alle filer som er i den mappen og i eventuelle undermapper. Når du så gjør endringer på en fil i mappen, så vil Git registrere endringer. Ønsker du at endringen skal bli et punkt i historikken til prosjektet, så må du først legge til filen i Git med kommandoen git add filnavn. Når du har gjort dette, så kan du lagre endringen med kommandoen git commit -m \"Din melding her\". Når du har gjort dette, så vil endringen være lagret i Git. Når du har gjort mange endringer, så kan du sende endringene til GitHub med kommandoen git push. Når du har gjort dette, så vil endringene være synlige for alle som har tilgang til GitHub-prosjektet.\nI SSB anbefaler vi at du starter et nytt Git-prosjekt ved å benytte ssb-project. Da vil du ikke bare aktivere Git, men du kan også få implementert en del andre gode praksiser for å holde koden din ryddig, oversiktlig og sikker.\nMen før vi kan begynne å bruke Git må vi konfigurere vår egen bruker for Git, som er forklart i neste seksjon.\nLes mer om Git på https://git-scm.com/.\n\n\n\n\n\n\n\n\n\nBruk SSB sin konfigurasjon!\n\n\n\nAlle som bruker GitHub i SSB bør bruke SSB sin Git-konfigurasjon. Hvis ikke kan man bl.a. risikere at output fra Notebooks blir pushet til GitHub. Du kan bruke SSBs konfigurasjon ved å kjøre ssb-gitconfig.py i terminalen og svare på spørsmålene som dukker opp.\n\n\nFor å jobbe med Git så må man konfigurere brukeren sin slik at Git vet hvem som gjør endringer i koden. I praksis betyr det at du må ha filen .gitconfig på hjemmeområdet ditt (f.eks. /home/jovyan/.gitconfig på Dapla) med noe grunnleggende informasjon:\n\n\n.gitconfig\n\n# /home/jovyan/.gitconfig på Dapla\n[user]\n        name = Ola Nordmann\n        email = &lt;ini&gt;@ssb.no\n\nMed denne konfigurasjonen så kan man bruke Git lokalt. Men skal man også bruke GitHub i SSB, dvs. dele kode med andre, så må man også legge til mer informasjon i .gitconfig. Heldigvis er det skrevet et script som gjør dette for deg. For å få anbefalt konfigurasjon for Git så kan du kjøre følgende kommando i terminalen:\n\n\nterminal\n\nssb_gitconfig.py\n\nDette scriptet vil spørre deg om ditt brukernavn i SSB, og så vil det opprette en fil som heter .gitconfig i hjemmeområdet ditt. Denne filen vil inneholde informasjon om brukernavn, e-post og tilgang til GitHub. I tillegg vil den sørge for at ditt ssb-project ikke sender output fra notebooks til GitHub.\n\n\n\nPå Dapla er det Jupyterlab som er utviklingsmiljøet for kode. I Jupyterlab vil man ofte jobbe i notebooks, og det har noen konsekvenser for versjonshåndtering. En notebook er en ipynb-fil som inneholder både tekst og kode. Åpner vi disse filene i Jupyterlab så ser vi det kjente notebook-grensesnittet med kode- og markdown-celler. Men egentlig er det bare et grensesnitt bygd over en json-fil1. Denne json-filen inneholder metadata og masse annet som gjør det vanskelig å se forskjellen på en fil over tid. Dette er derfor noe som å fikses før Git blir et nyttig verktøy for disse filene.\nI SSB sin anbefalte konfigurasjon setter opp nbdime for å få leselige differ mellom notebooks. Har du satt opp Git med ssb-gitconfig.py så vil dette være automatisk satt opp for deg.\nDet finnes også alternativer til å bruke nbdime. På Dapla er Jupytext installert for de som ikke ønsker å versjonshåndtere ipynb-filer, men heller vil bruke rene script-filer som .py- eller .R-filer. Måten Jupytext gjør dette på er konvertere f.eks. en ipynb-fil py-fil for deg, Den kan gjøre det automatisk når du lagrer, eller du kan gjøre det manuelt. Med denne tilnærmingen så kan du be Git ignorere alle ipynb-filer og bare versjonshåndtere de rene script.filene. Oppsett er Jupytext er ikke en del av SSB sin standard-konfigurasjon og er noe du må sett opp selv.\n\n\n\nGit er veldig sterkt verktøy med mange muligheter. Det er ikke tenkt at alt skal adresseres i dette kapitlet. Men det er noen vanlig operasjoner som er så vanlige at alle som jobber med kode i SSB bør kjenne dem.\nVi har tidligere nevnt at kommandoen for å aktivere versjonshåndtering med Git på en mappe, er git init. Dette gjøres også automatisk når man oppretter et nytt ssb-project.\nHva skjer hvis man gjør en endring i en fil i mappa? For det første kan du kjøre kommandoen git status for å se hvilke endringer Git har oppdaget. Men det betyr ikke at endringene er under versjonskontroll enda. For å fortelle Git om at disse endringene skal registreres så må du kjøre git add &lt;filnavn&gt;. Men fortsatt er ikke endringene et punkt i historien til koden din. For å gjøre det må du kjøre git commit -m \"Din melding her\". Ved å gjøre det så har du laget et unikt punkt i historien til koden din, med en egen ID, som du kan gå tilbake til senere hvis du ønsker.\nNår man utvikler kode så gjør man det fra såkalte branches2. Hvis vi tenker oss at din eksisterende kodebase er stammen på et tre (ofte kalt master eller main), så legger Git opp til at man gjør endringer på denne koden via branches eller grener av treet. Med andre ord holder vi stammen/master urørt helt til vi vet at endringen fungerer som den skal. Til slutt merger vi den inn i master. Vi kan opprette en ny branch og gå inn i den ved å skrive git checkout -b &lt;branch navn&gt;. Da står du i en branch og kan bruke kommandoer som git add og git commit som vist tidligere.\nVi kan merge endringene fra vår branch inn i main ved å først gå inn i main, git switch main, og deretter skrive git merge &lt;branch navn&gt;. Men typisk vil ikke dette være fremgangsmåten i SSB. Når man er fornøyd med endringene i en branch, så vil man pushe den opp til GitHub, slik at en kollega kan vurdere om den skal merges inn i main. Dermed gjøres selve mergen i GitHub-grensenittet. Vi skal se nærmere på GitHub i neste kapittel.",
    "crumbs": [
      "Manual",
      "Kode",
      "Git og Github"
    ]
  },
  {
    "objectID": "statistikkere/git-og-github.html#github",
    "href": "statistikkere/git-og-github.html#github",
    "title": "Git og Github",
    "section": "",
    "text": "GitHub er et nettsted som bl.a. fungerer som vårt felles mappesystem for deling av kode. SSB har sin egen organisasjonskonto med navn statisticsnorway. Men selv om SSB har en organisasjonskonto på GitHub må alle ansatte opprette sin egen brukerprofil, og knytte den mot SSB sin organisasjonskonto. Under forklarer vi hvordan du gjør dette.\n\n\nDette kapittelet er bare relevant hvis man ikke har en GitHub-brukerkonto fra før. For å bruke ssb-project-programmet til å generere et remote repo på GitHub må du ha en konto. Derfor starter vi med å gjøre dette. Det er en engangsjobb og du trenger aldri gjøre det igjen.\n\n\n\n\n\n\nSSB har valgt å ikke sette opp SSB-brukerne til de ansatte som GitHub-brukere. En viktig årsak er at er en GitHub-konto ofte regnes som en del av den ansattes CV. For de som aldri har brukt GitHub før kan det virke fremmed, men det er nok en fordel på sikt når alle blir godt kjent med denne arbeidsformen.\n\n\n\nSlik gjør du det:\n\nGå til https://github.com/\nTrykk Sign up øverst i høyre hjørne\nI dialogboksen som åpnes, se Figur 1, skriver du inn din e-postkonto og lager et passord. Dette trenger ikke være din SSB-bruker og e-post. Vi anbefaler at du bruker en personlig e-postkonto og velger ditt eget passord. Det samme gjelder brukernavn også.\n\n\n\n\n\n\n\nFigur 1: Dialogboks for opprettelse av GitHub-bruker.\n\n\n\nDu har nå laget en egen GitHub-bruker. I neste steg skal vi knytte denne kontoen til din SSB-bruker.\n\n\n\nHvis du har fullført forrige steg så har du nå en GitHub-konto. Hvis du står på din profil-side så ser den ut som i Figur 2.\n\n\n\n\n\n\nFigur 2: Et eksempel på hjemmeområdet til en GitHub-bruker\n\n\n\nDet neste vi må gjøre er å aktivere 2-faktor autentisering, siden det er dette som benyttes i SSB. Hvis du står på siden i bildet over, så gjør du følgende for å aktivere 2-faktor autentisering mot GitHub:\n\n\n\nTrykk på den lille pilen øverst til høyre og velg Settings(se Figur 3).\nDeretter velger du Password and authentification i menyen til venstre.\nUnder Two-factor authentication trykker du på Enable.\n\n\n\n\n\n\n\n\n\nFigur 3: Åpne settings for din GitHub-bruker.\n\n\n\n\n\n\nFigur 4 viser dialogboksen som vises. Velg Enable two-factor authentification.\n\n\n\n\n\n\n\nFigur 4: Dialogboks som åpnes når 2FA skrus på første gang.\n\n\n\n\nFigur 5 viser dialogboksen som vises for å velge hvordan man skal autentisere seg. Her anbefales det å velge Set up using an app, slik at du kan bruke Microsoft Authenticator-appen på din mobil.\n\n\n\n\n\n\n\nFigur 5: Dialogboks for å velge hvordan man skal autentisere seg med 2FA.\n\n\n\nFigur 6 viser QR-koden som vises. Denne skal vi bruke i neste steg.\n\n\n\n\n\n\nFigur 6: QR-kode som skannes av Microsoft Authenticator.\n\n\n\n\n\n\nStrekkoden over skal skannes i din Microsoft Authenticator-app på mobilen, som vist i Figur 7. Åpne appen, trykk på Bekreftede ID-er, og til slutt trykk på Skann QR-kode. Deretter skanner du QR-koden fra punkt 5.\nNår koden er skannet har du fått opp følgende bilde på appens hovedside (se bilde til høyre). Skriv inn den 6-siffer koden på GitHub-siden med QR-koden.\nTil slutt lagrer du Recovery-codes et trygt sted som bare du har tilgang til.\n\n\n\n\n\n\n\n\n\nFigur 7: Mobilappen Microsoft authenticator\n\n\n\n\n\nNå har vi aktivert 2-faktor autentisering for GitHub og er klare til å knytte vår personlige konto til vår SSB-bruker på SSBs “Github organisation” statisticsnorway.\n\n\n\nI forrige steg aktiverte vi 2-faktor autentisering for GitHub. Det neste vi må gjøre er å koble oss til Single Sign On (SSO) for SSB sin organisasjon på GitHub:\n\nTrykk på lenken https://github.com/orgs/statisticsnorway/sso\nI dialogboksen som dukker opp trykker du på Continue, slik som vist i Figur 8.\n\n\n\n\n\n\n\nFigur 8: Single Sign on (SSO) for SSB sin organisasjon på GitHub\n\n\n\nNår du har gjennomført dette så har du tilgang til statisticsnorway på GitHub. Går du inn på denne lenken så skal du nå kunne lese både Public, Private og Internal repoer, slik som vist i Figur 9.\n\n\n\n\n\n\nFigur 9: Medlemsvisning for SSB sin GitHub-organisasjon.\n\n\n\n\n\n\nNår vi skal jobbe med SSB-kode som ligger lagret hos statistcsnorway på GitHub, så må vi autentisere oss. Måten vi gjøre det på er ved å generere et Personal Access Token (ofte forkortet PAT) som vi oppgir når vi vil hente eller oppdatere kode på GitHub. Da sender vi med PAT for å autentisere oss for GitHub.\n\n\nFor å lage en PAT som er godkjent mot statisticsnorway så gjør man følgende:\n\nGå til din profilside på GitHub og åpne Settings slik som ble vist Seksjon 1.2.2.\nVelg Developer Settings i menyen til venstre.\nI menyen til venstre velger du Personal Access Token, og deretter Tokens (classic).\nVelg Generate new token og deretter Generate new token (classic).\nUnder Note kan du gi PAT’en et navn. Velg et navn som er intuitivt for deg. Hvis du skal bruke PAT til å jobbe mot Dapla, så ville jeg ganske enkelt kalt den dapla. Hvis du skal bruke den mot bakkemiljøet ville jeg kalt den prodsone eller noe annet som gjør det lett for det skjønne innholdet i ettertid.\nUnder Expiration velger du hvor lang tid som skal gå før PAT blir ugyldig. Dette er en avvening mellom sikkerhet og hva som er praktisk. Det anbefales at du velger 365 dager. Når PAT utløper må du gjenta stegene i dette kapittelet.\nUnder Select scopes velger du repo og workflow slik som vist i Figur 10.\n\n\n\n\n\n\n\nFigur 10: Gi token et kort og beskrivende navn\n\n\n\n\nTrykk på Generate token nederst på siden og du får noe lignende det du ser i Figur 11.\n\n\n\n\n\n\n\nFigur 11: Token som ble generert.\n\n\n\n\nKopier deretter PAT til en midlertidig fil. Grunnen er at du aldri vil se det igjen her etter at vi har gjennomført neste steg.\nDeretter trykker du på Configure SSO og velger Authorize ved siden statisticsnorway, slik som vist i Figur 12. Svar deretter på spørsmålene som dukker opp.\n\n\n\n\n\n\n\nFigur 12: Autorisering av Token mot SSBs GiHub-organisasjon.\n\n\n\nVi har nå opprettet en PAT som er godkjent for bruk mot SSB sin kode på GitHub. Det betyr at hvis vi vil jobbe med Git på SSB sine maskiner i sky eller på bakken, så må vi sendte med dette tokenet for å få lov til å jobbe med koden som ligger på statisticsnorway på GitHub.\n\n\n\nDet er ganske upraktisk å måtte sende med tokenet hver gang vi skal jobbe med GitHub. Vi bør derfor lagre det lokalt der vi jobber, slik at Git automatisk finner det. Det finnes mange måter å gjøre dette på og det er ikke bestemt hva som skal være beste-praksis i SSB. Men en måte å gjøre det er via en .netrc-fil. Vi oppretter da en fil som heter .netrc på vårt hjemmeområde, og legger følgende informasjon på en (hvilken som helst) linje i filen:\n\n\n~/.netrc\n\nmachine github.com login &lt;github-bruker&gt; password &lt;Personal Access Token&gt;\n\nGitHub-bruker er da din personlige bruker og IKKE brukernavnet ditt i SSB. Personal Access Token er det vi lagde i forrige kapittelet.\nEn veldig enkel måte å lagre dette er som følger. Anta at min personlige GitHub-bruker er SSB-Chad og at min Personal Access Token er blablabla. Da kan jeg gjøre følgende for å lagre det i .netrc:\n\nGå inn i Jupyterlab og åpne en Python-notebook.\nI den første kodecellen skriver du:\n\n\n\nterminal\n\n!echo \"machine github.com login SSB-Chad password blablabla\" &gt;&gt; ~/.netrc\n\nAlternativt kan du droppe det utropstegnet og kjøre det direkte i en terminal. Det vil gi samme resultat. Koden over legger til en linje med teksten\nmachine github.com login SSB-Chad password blablabla\ni en .netrc-fil på din hjemmeområdet, uanvhengig av om du har en fra før eller ikke. Hvis du har en fil fra før som allerede har et token fra GitHub, ville jeg nok slettet det før jeg legger en et nytt token.\nHver gang du jobber mot GitHub vil Git sjekke om informasjon om autentisering ligger i denne filen, og bruke den hvis den ligger der.\n\n\n\nI eksempelet over lagde vi en PAT som var gyldig i 90 dager. Dermed vil du ikke kunne jobbe mot GitHub med dette tokenet etter 90 dager. For å oppdatere tokenet gjør du følgende:\n\nLag et nytt PAT ved å repetere Seksjon 1.2.4.1.\nI miljøet der du skal jobbe med Git og GitHub går du inn i din .netrc og bytter ut token med det nye.\n\nOg med det er du klar til å jobbe mot statisticsnorway på GitHub.",
    "crumbs": [
      "Manual",
      "Kode",
      "Git og Github"
    ]
  },
  {
    "objectID": "statistikkere/git-og-github.html#footnotes",
    "href": "statistikkere/git-og-github.html#footnotes",
    "title": "Git og Github",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPrøv selv å åpne en ipynb som json ved høreklikke på fila i Jupyterlab, velge Open with, og velg json. Da vil du se den underliggende json-filen↩︎\nBranches kan oversettes til grener på norsk. Men i denne boken velger vi å bruke det engelske ordet branches. Grunnen er at det erfaringsmessig er lettere forholde seg til det engelske ordet når man skal søke etter informasjon i annen dokumentasjon↩︎",
    "crumbs": [
      "Manual",
      "Kode",
      "Git og Github"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-botter.html",
    "href": "statistikkere/hva-er-botter.html",
    "title": "Hva er bøtter?",
    "section": "",
    "text": "På Dapla er det Google Cloud Storage (GCS) som benyttes til å lagre data og filer. Følgelig er det GCS som erstatter det vi kjente som Linux-stammene i prodsonen tidligere. I SSB har vi vært vant til å jobbe med data lagret på filsystemer i et Linux-miljø1. GCS-bøttene skiller seg fra klassiske filsystemer på flere måter, og det er viktig å være klar over disse forskjellene. I denne kapitlet vil vi gå gjennom noen av de viktigste forskjellene og hvordan man gjør vanlige operasjoner mot bøtter i GCS.\n\n\nI et Linux- eller Windows-filsystem, som vi har vært vant til tidligere, så er filer og mapper organisert i en hierarkisk struktur på et operativsystem (OS). I SSB har OS-ene vært installert på fysiske maskiner som vi vedlikeholder selv.\nEn bøtte i GCS er derimot en kjøpt tjeneste som lar brukeren lagre alle typer objekter i en container. Man trenger altså ikke å tenke på om filene ligger i et hierarki, hvilket operativsystem det kjører på, eller hvor mye diskplass som er tilgjengelig.\nAt data er lagret som objekter i en bøtte har noen praktiske implikasjoner for hvordan vi jobber med data i SSB:\n\nMange er vant til å jobbe direkte med filer i en Linux-terminal eller via systemkall fra språk som SAS, Pyton eller R. For å gjøre det samme i Jupyter mot en bøtte, så kan vi bruke Python-pakken gcsfs. Se eksempler under.\nNår vi bruker Python- eller R-pakker for lese eller skrive data fra bøtter, så er vi avhengig av at pakkene tilbyr integrasjon mot bøtter. Mange pakker gjør det, men ikke alle. For de som ikke gjør det kan vi bruke ofte bruke gcsfs til å gi oss et python-objekt, som igjen kan brukes av de fleste pakker.\nDet finnes egentlig ikke noen mapper i bøtter. I motsetning til et vanlig filsystem så er det ikke en hierarkisk mappestruktur i en bøtte. Det vil si at alt ligger et sted, men de kan ha lange navn med /-tegn slik at det ligner på et klassisk filsystem. Bruker du / i objekt-navnet så vil også Google Cloud Console vise det som mapper, men det er bare for å gjøre det enklere å forholde seg til. En praktisk konsekvens av dette er f.eks. at man ikke trenger å opprette en mappe før man legger et fil/objekt i den. Det er bare en tekststreng som er en del av objekt-navnet.\n\nUnder kommer det en del eksempler på hvordan man kan jobbe med objekter i bøtter på samme måte som filer i et filsystem.\n\n\n\nPå Dapla skal data lagres i bøtter. Men når du åpner Jupyterlab så får du også et “lokalt” eller klassisk filsystem, slik vi definerte det i forrige kapittel. Det er dette filsystemet du ser i filutforskeren i Jupyterlab, slik som vist til venstre i Figur 1. Det er også dette filsystemet du ser når du f.eks. bruker ls-kommandoen i en terminal i Jupyterlab.\n\n\n\n\n\n\nFigur 1: Til venstre i bildet ser du filutforskeren i Jupyterlab\n\n\n\nDette filsystemet er ment for å lagre kode midlertidig mens du jobber med dem. Det er ikke ment for å lagre data. Det er heller ikke ment som et permanent lagringssted for kode. Permanent lagring av kode skal gjøres på GitHub. Selv om filene du lagrer der fortsetter å eksistere for hver gang du logger deg inn i Jupyterlab, så bør kode du ønsker å bevare pushes til GitHub før du avslutter en sesjon i Jupyterlab. Se en mer teknisk beskrivelse av hvordan dette fungerer i boksen under.\n\n\n\n\n\n\nHvem er egentlig denne Jovyan?\n\n\n\nNår du logger deg inn i Jupyterlab på Dapla, så ser du at brukeren din på det lokale filsystemet heter jovyan. Grunnen er at det er en Linux-container som kjører et Jupyterlab Docker-image, og alle bruker denne containeren. Det er derfor vi kaller det et “lokalt” filsystem, fordi det er lokalt i containeren.\nI tillegg er det satt opp persistent volum (PV) og en persistent volume claim (PVC) som er knyttet til denne containeren. Det er dette som gjør at filene du lagrer i Jupyterlab blir lagret mellom hver gang du logger deg inn. I tillegg tas det en backup av filsystemene hver dag, slik at vi kan gjenopprette filene dine hvis noe skulle skje med PV-en.\nLagringsplassen på PV-en er begrenset til 10 GB per bruker. Grunnen til at vi ikke tilbyr mer lagringsplass er fordi det kun er kode skal lagres her, og at denne lagringsplassen er relativt dyrt sammenlignet med GCS-bøtter. Hvis du jobber i virtuelle miljøer og lagrer mange miljøer lokalt, kan du oppleve at disken blir full. Les mer her om hvordan man bør håndere dette.\n\n\n\n\n\nTidligere har vi diskutert forskjellene mellom bøtter og filsystemer. Mange kjenner hvordan man gjør systemkommandoer2 i klassiske filsystemer fra en terminal eller fra språk som Python, R og SAS. I dette kapitlet viser vi hvordan dette kan gjøres mot bøtter fra Jupyterlab.\ngcsfs er en Python-pakke som lar deg jobbe med objekter i bøtter på nesten samme måte som filer i et filsystem. For å kunne gjøre det må vi først sette opp en filsystem-instans som lar oss bruke en bøtte som et filsystem. Pakken dapla-toolbelt lar oss gjøre det ganske enkelt:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs er nå et filsystem-versjon av bøttene vi har tilgang til på GCS. Vi kan nå bruk fs til å gjøre typiske operasjoner vi har vært vant til å gjøre i en Linux-terminal. Se en fullstendig liste over kommandoer her.\nUnder vises noen flere eksempler på nyttige funksjoner enn det som ble vist i kapitlet tidligere i boken\n\n\n\n\n\n\nWarning\n\n\n\nSelv om ordet mapper blir brukt i eksemplene under, så er det viktig å huske at det ikke finnes noen mapper i bøtter. Det er bare en tekststreng som er en del av objekt-navnet. Men siden vi her jobber med et filsystem-representasjon av bøtten, så tillater vi oss å gjøre det for å gjøre det enklere å lese.\n\n\n\n\nfs.glob() lar oss søke etter filer i bøtten. Vi kan bruke *, **, ? og [..] som wildcard for å finne det vi trenger. Det som returneres er enten en liste eller dictionary avhengig av hva vi velger å gjøre.\nHent en liste over alle filer i en undermappe R_smoke_test i bøtta gs://ssb-dapla-felles-data-delt-prod:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/*\")\n\nNår vi legger til * på slutten av filstien så returnerer den alle filer i den eksakte undermappen. Men hvis vi ønsker å å få alle filer i alle undermapper, så kan vi bruke ** på denne måten:\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/**\").\nVi kan også søke mer avansert ved ved å bruke ?. ?-tegnet sier at en enkeltkarakter kan være hva som helst. F.eks. kunne vi skrevet\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/**/R_smoke_test_??.csv\")\nfor å rekursivt liste ut alle filer som starter med R_smoke_test_, etterfulgt av to karakterer av hvilken som helst type, og slutter med .csv.\nHvis vi viste at de to karakterene ikke skulle være av hvilken som helst karakterer, men det var en liten bokstav, etterfulgt av et tall mellom 2-6, så kunne vi brukt [a-z] og [2-6] for å spesifisere dette. F.eks. kunne vi skrevet:\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/**/R_smoke_test_[a-z][2-6].csv\")\nSom vi ser er fs.glob() et verktøy som git oss mye den funksjonaliteten vi er vant til fra Linux-terminalen. Men i tillegg kan vi bruke den til å hente inn metadataene til de filene/objektene vi får treff på, ved å bruke argumentet detail=True. Her er et eksempel:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\n    \"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/**/R_smoke_test_/[a-z][2-6].csv\",\n    detail=True,\n)\n\nMetadataene gir deg da informasjon om filstørrelse, tidspunkt for opprettelse, tidspunkt for siste endring, og hvem som har opprettet og endret filen.\n\n\n\nI noen tilfeller kan det være nyttig å sjekke om en fil eksisterer i bøtten. Det kan vi gjøre med fs.exists():\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.exists(\"gs://ssb-dapla-felles-data-delt-prod/altinn3/form_dc551844cd74.xml\")\n\nKoden returnerer enten True eller False avhengig av om filen eksisterer eller ikke.\n\n\n\nHvis du lurer på hvor mange GB data du har i en bøtte, så kan du bruke fs.du():\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\ntotal_bytes = fs.du(\n    \"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/\",\n)\n\ntotal_size_gb = total_bytes / (1024**3)\nprint(f\"Total size: {total_size_gb:.3f} GB\")\n\n\n\n\nHvert objekt i bøtta har metadata knyttet til seg som kan hentes inn i Jupyter og benyttes.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\nfile = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/form_dc551844cd74.xml\"\n\nfs.info(file)\n\nDette gir deg blant annet informasjon om filstørrelse, tidspunkt for opprettelse, og tidspunkt for siste endring. Hvis du ønsker dette for flere filer så kan man også bruke fs.glob(&lt;pattern&gt;, details=True) som vi så på tidligere.\n\n\n\nfs.ls() brukes for å gi en liste av filer i et område. Det kan brukes for både bøtter og mapper.\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.ls(\"gs://ssb-dapla-felles-data-delt-prod/altinn3\")\n\nKoden returnerer en liste.\n\n\n\nfs.open() lar oss åpne en fil i bøtta for lesing og skriving med vanlige Python-funksjoner. Funksjonen returnerer et fil-aktig objekt som kan brukes med vanlige som Python-biblioteker som Pandas og NumPy.\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\nfile_path = \"gs://ssb-dapla-felles-data-delt-prod/dapla-metrics/number-of-teams.csv\"\n\n# Bruker fs.open() til å åpne fil, og deretter leses den med Pandas\nwith fs.open(file_path, \"r\") as file:\n    df = pd.read_csv(file)\ndf\n\nDu kan også bruke fs.open() til å skrive til en fil i bøtta. Her er et eksempel på hvordan man skriver en parquet-fil med Pandas og PyArrow:\n\n\nnotebook\n\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom dapla import FileClient\n\nfile_path = \"gs://ssb-dapla-felles-data-delt-prod/dapla-metrics/test.parquet\"\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\n# Lager eksempeldata\ndata = {\"A\": [1, 2, 3], \"B\": [4, 5, 6]}\ndf = pd.DataFrame(data)\n\n# Skriver parquet-fil\nwith fs.open(file_path, \"wb\") as file:\n    pq.write_table(pa.Table.from_pandas(df), file)\n\nOver brukte vi wb for å åpne den binære filen for skriving. Hvis du ønsker å lese fra en binær fil så bruker du rb. Skulle du jobbet en ren tekstfil, så hadde man brukt w til å skrive og r til å lese.\n\n\n\nfs.touch() lar deg opprette en tom fil i bøtta, eller oppdatere metadataene til objektet for når den sist ble modifisert.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nfs.touch(\"gs://ssb-dapla-felles-data-delt-prod/dapla-metrics/test.parquet\")\n\n\n\n\nfs.put() lar deg laste opp en fil fra Jupyter-filsystemet3 til bøtta. Husk at filstien til ditt hjemmeområde på Jupyter er /home/jovyan/. Her er et eksempel på hvordan man kan bruke det på enkelt-filer:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"/home/jovyan/custom.scss\"\ndestination = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/test2.csv\"\n\nfs.put(source, destination)\n\nDu kan også kopiere hele mapper mellom jovyan og bøttene:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"/home/jovyan/sesongjustering/\"\ndestination = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/sesongjustering/\"\n\nfs.put(source, destination, recursive=True)\n\nEt brukstilfellet for å kopiere mellom bøtter og jovyan er ved sesongjustering med Jdemetra+ og JWSACruncher. Siden Jdemetra+ ikke kan lese/skrive fra bøtter, så må vi midlertidig kopiere dataene til jovyan med fs.put() før vi kan kjøre sesongjusteringen. Når vi er ferdige med kjøringen kopierer vi dataene tilbake til bøtta med fs.get().\n\n\n\nfs.get() gjør det samme som fs.put(), bare motsatt vei. Den kopierer fra en bøtte til jovyan.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/sesongjustering/\"\ndestination = \"/home/jovyan/sesongjustering/\"\n\nplaceholder = fs.get(source, destination, recursive=True)\n\nfs.get() returnerer potensielt mye output. Hvis du ikke vil ha det, så kan du bare definere det som et objekt. F.eks. placeholder som i eksempelet over.\n\n\n\nfs.mv() lar deg flytte filer og mapper mellom bøtter, samt å gi objekter nye navn.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\nsource = \"gs://ssb-dapla-felles-data-delt-prod/dapla-metrics/number-of-teams.csv\"\ndestination = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/number-of-teams.csv\"\nfs.mv(source, destination)\n\n\n\n\nfs.copy() lar deg kopiere filer og mapper mellom bøtter. I eksempelet under kopierer vi rekursivt:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfrom_path = \"gs://ssb-prod-arbmark-skjema-data-kilde/ledstill/altinn/2022/11/21/\"\nto_path = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/\"\nfs.copy(from_path, to_path, recursive=True)\n\n\n\n\nfs.rm() lar deg slette filer og mapper i bøtta. Her sletter vi en enkeltfil:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfs.rm(\"gs://ssb-dapla-felles-data-delt-prod/altinn3/number-of-teams.csv\")\n\nOgså denne funksjonen tar et recursive-argument hvis du ønsker å slette en hel mappe.",
    "crumbs": [
      "Manual",
      "Data",
      "Hva er bøtter?"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-botter.html#bøtter-vs-filsystemer",
    "href": "statistikkere/hva-er-botter.html#bøtter-vs-filsystemer",
    "title": "Hva er bøtter?",
    "section": "",
    "text": "I et Linux- eller Windows-filsystem, som vi har vært vant til tidligere, så er filer og mapper organisert i en hierarkisk struktur på et operativsystem (OS). I SSB har OS-ene vært installert på fysiske maskiner som vi vedlikeholder selv.\nEn bøtte i GCS er derimot en kjøpt tjeneste som lar brukeren lagre alle typer objekter i en container. Man trenger altså ikke å tenke på om filene ligger i et hierarki, hvilket operativsystem det kjører på, eller hvor mye diskplass som er tilgjengelig.\nAt data er lagret som objekter i en bøtte har noen praktiske implikasjoner for hvordan vi jobber med data i SSB:\n\nMange er vant til å jobbe direkte med filer i en Linux-terminal eller via systemkall fra språk som SAS, Pyton eller R. For å gjøre det samme i Jupyter mot en bøtte, så kan vi bruke Python-pakken gcsfs. Se eksempler under.\nNår vi bruker Python- eller R-pakker for lese eller skrive data fra bøtter, så er vi avhengig av at pakkene tilbyr integrasjon mot bøtter. Mange pakker gjør det, men ikke alle. For de som ikke gjør det kan vi bruke ofte bruke gcsfs til å gi oss et python-objekt, som igjen kan brukes av de fleste pakker.\nDet finnes egentlig ikke noen mapper i bøtter. I motsetning til et vanlig filsystem så er det ikke en hierarkisk mappestruktur i en bøtte. Det vil si at alt ligger et sted, men de kan ha lange navn med /-tegn slik at det ligner på et klassisk filsystem. Bruker du / i objekt-navnet så vil også Google Cloud Console vise det som mapper, men det er bare for å gjøre det enklere å forholde seg til. En praktisk konsekvens av dette er f.eks. at man ikke trenger å opprette en mappe før man legger et fil/objekt i den. Det er bare en tekststreng som er en del av objekt-navnet.\n\nUnder kommer det en del eksempler på hvordan man kan jobbe med objekter i bøtter på samme måte som filer i et filsystem.",
    "crumbs": [
      "Manual",
      "Data",
      "Hva er bøtter?"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-botter.html#lokalt-filsystem-på-dapla",
    "href": "statistikkere/hva-er-botter.html#lokalt-filsystem-på-dapla",
    "title": "Hva er bøtter?",
    "section": "",
    "text": "På Dapla skal data lagres i bøtter. Men når du åpner Jupyterlab så får du også et “lokalt” eller klassisk filsystem, slik vi definerte det i forrige kapittel. Det er dette filsystemet du ser i filutforskeren i Jupyterlab, slik som vist til venstre i Figur 1. Det er også dette filsystemet du ser når du f.eks. bruker ls-kommandoen i en terminal i Jupyterlab.\n\n\n\n\n\n\nFigur 1: Til venstre i bildet ser du filutforskeren i Jupyterlab\n\n\n\nDette filsystemet er ment for å lagre kode midlertidig mens du jobber med dem. Det er ikke ment for å lagre data. Det er heller ikke ment som et permanent lagringssted for kode. Permanent lagring av kode skal gjøres på GitHub. Selv om filene du lagrer der fortsetter å eksistere for hver gang du logger deg inn i Jupyterlab, så bør kode du ønsker å bevare pushes til GitHub før du avslutter en sesjon i Jupyterlab. Se en mer teknisk beskrivelse av hvordan dette fungerer i boksen under.\n\n\n\n\n\n\nHvem er egentlig denne Jovyan?\n\n\n\nNår du logger deg inn i Jupyterlab på Dapla, så ser du at brukeren din på det lokale filsystemet heter jovyan. Grunnen er at det er en Linux-container som kjører et Jupyterlab Docker-image, og alle bruker denne containeren. Det er derfor vi kaller det et “lokalt” filsystem, fordi det er lokalt i containeren.\nI tillegg er det satt opp persistent volum (PV) og en persistent volume claim (PVC) som er knyttet til denne containeren. Det er dette som gjør at filene du lagrer i Jupyterlab blir lagret mellom hver gang du logger deg inn. I tillegg tas det en backup av filsystemene hver dag, slik at vi kan gjenopprette filene dine hvis noe skulle skje med PV-en.\nLagringsplassen på PV-en er begrenset til 10 GB per bruker. Grunnen til at vi ikke tilbyr mer lagringsplass er fordi det kun er kode skal lagres her, og at denne lagringsplassen er relativt dyrt sammenlignet med GCS-bøtter. Hvis du jobber i virtuelle miljøer og lagrer mange miljøer lokalt, kan du oppleve at disken blir full. Les mer her om hvordan man bør håndere dette.",
    "crumbs": [
      "Manual",
      "Data",
      "Hva er bøtter?"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-botter.html#systemkommandoer-mot-bøttter",
    "href": "statistikkere/hva-er-botter.html#systemkommandoer-mot-bøttter",
    "title": "Hva er bøtter?",
    "section": "",
    "text": "Tidligere har vi diskutert forskjellene mellom bøtter og filsystemer. Mange kjenner hvordan man gjør systemkommandoer2 i klassiske filsystemer fra en terminal eller fra språk som Python, R og SAS. I dette kapitlet viser vi hvordan dette kan gjøres mot bøtter fra Jupyterlab.\ngcsfs er en Python-pakke som lar deg jobbe med objekter i bøtter på nesten samme måte som filer i et filsystem. For å kunne gjøre det må vi først sette opp en filsystem-instans som lar oss bruke en bøtte som et filsystem. Pakken dapla-toolbelt lar oss gjøre det ganske enkelt:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs er nå et filsystem-versjon av bøttene vi har tilgang til på GCS. Vi kan nå bruk fs til å gjøre typiske operasjoner vi har vært vant til å gjøre i en Linux-terminal. Se en fullstendig liste over kommandoer her.\nUnder vises noen flere eksempler på nyttige funksjoner enn det som ble vist i kapitlet tidligere i boken\n\n\n\n\n\n\nWarning\n\n\n\nSelv om ordet mapper blir brukt i eksemplene under, så er det viktig å huske at det ikke finnes noen mapper i bøtter. Det er bare en tekststreng som er en del av objekt-navnet. Men siden vi her jobber med et filsystem-representasjon av bøtten, så tillater vi oss å gjøre det for å gjøre det enklere å lese.\n\n\n\n\nfs.glob() lar oss søke etter filer i bøtten. Vi kan bruke *, **, ? og [..] som wildcard for å finne det vi trenger. Det som returneres er enten en liste eller dictionary avhengig av hva vi velger å gjøre.\nHent en liste over alle filer i en undermappe R_smoke_test i bøtta gs://ssb-dapla-felles-data-delt-prod:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/*\")\n\nNår vi legger til * på slutten av filstien så returnerer den alle filer i den eksakte undermappen. Men hvis vi ønsker å å få alle filer i alle undermapper, så kan vi bruke ** på denne måten:\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/**\").\nVi kan også søke mer avansert ved ved å bruke ?. ?-tegnet sier at en enkeltkarakter kan være hva som helst. F.eks. kunne vi skrevet\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/**/R_smoke_test_??.csv\")\nfor å rekursivt liste ut alle filer som starter med R_smoke_test_, etterfulgt av to karakterer av hvilken som helst type, og slutter med .csv.\nHvis vi viste at de to karakterene ikke skulle være av hvilken som helst karakterer, men det var en liten bokstav, etterfulgt av et tall mellom 2-6, så kunne vi brukt [a-z] og [2-6] for å spesifisere dette. F.eks. kunne vi skrevet:\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/**/R_smoke_test_[a-z][2-6].csv\")\nSom vi ser er fs.glob() et verktøy som git oss mye den funksjonaliteten vi er vant til fra Linux-terminalen. Men i tillegg kan vi bruke den til å hente inn metadataene til de filene/objektene vi får treff på, ved å bruke argumentet detail=True. Her er et eksempel:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\n    \"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/**/R_smoke_test_/[a-z][2-6].csv\",\n    detail=True,\n)\n\nMetadataene gir deg da informasjon om filstørrelse, tidspunkt for opprettelse, tidspunkt for siste endring, og hvem som har opprettet og endret filen.\n\n\n\nI noen tilfeller kan det være nyttig å sjekke om en fil eksisterer i bøtten. Det kan vi gjøre med fs.exists():\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.exists(\"gs://ssb-dapla-felles-data-delt-prod/altinn3/form_dc551844cd74.xml\")\n\nKoden returnerer enten True eller False avhengig av om filen eksisterer eller ikke.\n\n\n\nHvis du lurer på hvor mange GB data du har i en bøtte, så kan du bruke fs.du():\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\ntotal_bytes = fs.du(\n    \"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/\",\n)\n\ntotal_size_gb = total_bytes / (1024**3)\nprint(f\"Total size: {total_size_gb:.3f} GB\")\n\n\n\n\nHvert objekt i bøtta har metadata knyttet til seg som kan hentes inn i Jupyter og benyttes.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\nfile = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/form_dc551844cd74.xml\"\n\nfs.info(file)\n\nDette gir deg blant annet informasjon om filstørrelse, tidspunkt for opprettelse, og tidspunkt for siste endring. Hvis du ønsker dette for flere filer så kan man også bruke fs.glob(&lt;pattern&gt;, details=True) som vi så på tidligere.\n\n\n\nfs.ls() brukes for å gi en liste av filer i et område. Det kan brukes for både bøtter og mapper.\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.ls(\"gs://ssb-dapla-felles-data-delt-prod/altinn3\")\n\nKoden returnerer en liste.\n\n\n\nfs.open() lar oss åpne en fil i bøtta for lesing og skriving med vanlige Python-funksjoner. Funksjonen returnerer et fil-aktig objekt som kan brukes med vanlige som Python-biblioteker som Pandas og NumPy.\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\nfile_path = \"gs://ssb-dapla-felles-data-delt-prod/dapla-metrics/number-of-teams.csv\"\n\n# Bruker fs.open() til å åpne fil, og deretter leses den med Pandas\nwith fs.open(file_path, \"r\") as file:\n    df = pd.read_csv(file)\ndf\n\nDu kan også bruke fs.open() til å skrive til en fil i bøtta. Her er et eksempel på hvordan man skriver en parquet-fil med Pandas og PyArrow:\n\n\nnotebook\n\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom dapla import FileClient\n\nfile_path = \"gs://ssb-dapla-felles-data-delt-prod/dapla-metrics/test.parquet\"\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\n# Lager eksempeldata\ndata = {\"A\": [1, 2, 3], \"B\": [4, 5, 6]}\ndf = pd.DataFrame(data)\n\n# Skriver parquet-fil\nwith fs.open(file_path, \"wb\") as file:\n    pq.write_table(pa.Table.from_pandas(df), file)\n\nOver brukte vi wb for å åpne den binære filen for skriving. Hvis du ønsker å lese fra en binær fil så bruker du rb. Skulle du jobbet en ren tekstfil, så hadde man brukt w til å skrive og r til å lese.\n\n\n\nfs.touch() lar deg opprette en tom fil i bøtta, eller oppdatere metadataene til objektet for når den sist ble modifisert.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nfs.touch(\"gs://ssb-dapla-felles-data-delt-prod/dapla-metrics/test.parquet\")\n\n\n\n\nfs.put() lar deg laste opp en fil fra Jupyter-filsystemet3 til bøtta. Husk at filstien til ditt hjemmeområde på Jupyter er /home/jovyan/. Her er et eksempel på hvordan man kan bruke det på enkelt-filer:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"/home/jovyan/custom.scss\"\ndestination = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/test2.csv\"\n\nfs.put(source, destination)\n\nDu kan også kopiere hele mapper mellom jovyan og bøttene:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"/home/jovyan/sesongjustering/\"\ndestination = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/sesongjustering/\"\n\nfs.put(source, destination, recursive=True)\n\nEt brukstilfellet for å kopiere mellom bøtter og jovyan er ved sesongjustering med Jdemetra+ og JWSACruncher. Siden Jdemetra+ ikke kan lese/skrive fra bøtter, så må vi midlertidig kopiere dataene til jovyan med fs.put() før vi kan kjøre sesongjusteringen. Når vi er ferdige med kjøringen kopierer vi dataene tilbake til bøtta med fs.get().\n\n\n\nfs.get() gjør det samme som fs.put(), bare motsatt vei. Den kopierer fra en bøtte til jovyan.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/sesongjustering/\"\ndestination = \"/home/jovyan/sesongjustering/\"\n\nplaceholder = fs.get(source, destination, recursive=True)\n\nfs.get() returnerer potensielt mye output. Hvis du ikke vil ha det, så kan du bare definere det som et objekt. F.eks. placeholder som i eksempelet over.\n\n\n\nfs.mv() lar deg flytte filer og mapper mellom bøtter, samt å gi objekter nye navn.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\nsource = \"gs://ssb-dapla-felles-data-delt-prod/dapla-metrics/number-of-teams.csv\"\ndestination = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/number-of-teams.csv\"\nfs.mv(source, destination)\n\n\n\n\nfs.copy() lar deg kopiere filer og mapper mellom bøtter. I eksempelet under kopierer vi rekursivt:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfrom_path = \"gs://ssb-prod-arbmark-skjema-data-kilde/ledstill/altinn/2022/11/21/\"\nto_path = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/\"\nfs.copy(from_path, to_path, recursive=True)\n\n\n\n\nfs.rm() lar deg slette filer og mapper i bøtta. Her sletter vi en enkeltfil:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfs.rm(\"gs://ssb-dapla-felles-data-delt-prod/altinn3/number-of-teams.csv\")\n\nOgså denne funksjonen tar et recursive-argument hvis du ønsker å slette en hel mappe.",
    "crumbs": [
      "Manual",
      "Data",
      "Hva er bøtter?"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-botter.html#footnotes",
    "href": "statistikkere/hva-er-botter.html#footnotes",
    "title": "Hva er bøtter?",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nEgentlig har vi jobbet med data-filer på både Linux- og Windows-filsystemer. Men Linux-stammene har vært det anbefalte stedet å lagre datafiler.↩︎\nMed systemkommandoer så mener vi bash-kommandoer som ls og mv, eller implementasjoner av disse kommandoene i Python, R eller SAS.↩︎\nJupyter-miljøet har sitt eget filsystem, ofte kalt jovyan. Det er som et vanlig Linux-filsystem, og vil være det vi omtaler som “lokalt” på maskinen din i Jupyter.↩︎",
    "crumbs": [
      "Manual",
      "Data",
      "Hva er bøtter?"
    ]
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html",
    "href": "notebooks/spark/deltalake-intro.html",
    "title": "Introduksjon til Delta Lake",
    "section": "",
    "text": "Delta Lake er et databaselag som kan legges over parquet-filer i bøtter. Det kan gi oss mye av den funksjonaliteten vi har vært vant til i relasjonelle databaser og datavarehus. I denne notebooken vises hvordan man kan ta det i bruk på Dapla."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#oppsett",
    "href": "notebooks/spark/deltalake-intro.html#oppsett",
    "title": "Introduksjon til Delta Lake",
    "section": "Oppsett",
    "text": "Oppsett\nHvis du logger deg inn på https://jupyter.dapla.ssb.no/ kan du ta i bruk Delta Lake via Spark. Men for å gjøre det må du installere delta-spark. For å installere pakken må du jobbe i et ssb-project. I tillegg må du installere delta-spark-versjon som er kompatibel med Spark-versjonen som er installert på Dapla. Gjør derfor følgende:\n\nOpprett et ssb-project med kommandoen:\nssb-project create test-delta-lake\nI terminalen skriver du følgende for å sjekke Spark-versjonen som er installert:\nspark-shell --version\nSjekk hvilken delta-spark-versjon som er kompatibel med din Spark-versjon og installer den med kommandoen1:\npoetry add delta-spark@2.3\nÅpne en ny notebook og velg kernel test-delta-lake.\n\nNå har du satt opp et virtuelt miljø med en PySpark-kernel som kjører en maskin (såkalt Pyspark local kernel), der du har installert delta-spark. Vi kan nå importere de bibliotekene vi trenger og sette igang en Spark-session.\n\n# Importerer biblioteker\nimport pyspark\nfrom delta import *\n\nDeretter initialiserer jeg en Spark-session. %%capture_output er med for å forhindre at det ikke blir printet ut sensitiv informasjon i en notebook som skal inn i Dapla-manualen.\n\n%%capture captured_output\n%run /usr/local/share/jupyter/kernels/pyspark_local/init.py"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#genererer-noe-data",
    "href": "notebooks/spark/deltalake-intro.html#genererer-noe-data",
    "title": "Introduksjon til Delta Lake",
    "section": "Genererer noe data",
    "text": "Genererer noe data\n\n# Genererer noe data med Spark\ndata = spark.range(10, 15)\ndata.show()\n\n+---+\n| id|\n+---+\n| 10|\n| 11|\n| 12|\n| 13|\n| 14|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#skriver-ut-en-delta-lake-table",
    "href": "notebooks/spark/deltalake-intro.html#skriver-ut-en-delta-lake-table",
    "title": "Introduksjon til Delta Lake",
    "section": "Skriver ut en Delta Lake Table",
    "text": "Skriver ut en Delta Lake Table\nLa oss skrive ut Spark DataFrame som en Delta Lake Table og bli kjent med strukturen i objektet:\n\n%%time\ndata.write.format(\"delta\").mode(\"overwrite\").save(\n    \"gs://ssb-dapla-felles-data-delt-prod/temp4\"\n)\n\nCPU times: user 3.38 ms, sys: 1.7 ms, total: 5.08 ms\nWall time: 5.59 s\n\n\nVi kan deretter printe ut hva som ble opprettet når vi skrev ut en Delta Lake Table:\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/temp4/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp4/_delta_log',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000000.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-9b3b81a9-2771-4fb4-9f0f-659fd160d643-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-0f2f8ba5-3161-41e8-b5d1-2084128a5bed-c000.snappy.parquet']\n\n\n\nDelta Lake Tabellstruktur\nMappenstrukturen du ser over er typisk for en Delta Lake-tabell. La oss bryte ned komponentene:\n\n_delta_log/:\n\nDette er transaksjonsloggmappen for Delta Lake-tabellen. Den inneholder en serie med JSON-filer som registrerer alle transaksjoner gjort på tabellen.\nTransaksjonsloggen er avgjørende for Delta Lakes ACID-transaksjonsegenskaper, som muliggjør funksjoner som atomiske forpliktelser, tilbakerullinger og tid-reise-forespørsler.\n\n_delta_log/00000000000000000000.json:\n\nDette er en JSON-fil innenfor transaksjonsloggkatalogen. Hver JSON-fil i denne mappen tilsvarer en transaksjon (eller en batch av transaksjoner) gjort på tabellen.\nFilnavnet er null-fylt og representerer transaksjonsversjonen. I dette tilfellet representerer 00000000000000000000.json den aller første versjonen (versjon 0) av tabellen. Etter hvert som flere transaksjoner blir gjort, vil nye filer bli lagt til med økende versjonsnumre.\n\npart-00000-450715bd-6b0c-4827-bb8a-b0265505ca72-c000.snappy.parquet og part-00001-977ed55f-5ce5-469f-8a1d-9eafb143c215-c000.snappy.parquet:\n\nDette er de faktiske datafilene til Delta Lake-tabellen, lagret i Parquet-format.\n.snappy.parquet-utvidelsen indikerer at dataene er komprimert ved hjelp av Snappy-komprimeringsalgoritmen.\nFilnavnene er typiske for Sparks navngivningskonvensjon for datadeler. Prefiksene part-00000 og part-00001 indikerer partisjonsnumre. De lange UUID-lignende strengene er unike identifikatorer for datafilene. Suffikset c000 indikerer Spark-oppgaven som skrev filen.\n\n\nMappen representerer en Delta Lake-tabell med data lagret i Parquet-format og en transaksjonslogg som sporer endringer i tabellen. Tilstedeværelsen av _delta_log-mappen og innholdet er det som skiller en Delta Lake-tabell fra et vanlig Parquet-datasett."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#lese-inn-tabell",
    "href": "notebooks/spark/deltalake-intro.html#lese-inn-tabell",
    "title": "Introduksjon til Delta Lake",
    "section": "Lese inn tabell",
    "text": "Lese inn tabell\n\ndeltaTable = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-delt-prod/temp4\")\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 13|\n| 14|\n| 10|\n| 11|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#modifisere-datasettet",
    "href": "notebooks/spark/deltalake-intro.html#modifisere-datasettet",
    "title": "Introduksjon til Delta Lake",
    "section": "Modifisere datasettet",
    "text": "Modifisere datasettet\nLa oss modifisere datasettet ved å bytte ut verdien 13 med 15 i id-kolonnen:\n\nfrom pyspark.sql.functions import col, lit\n\n# Update the cell with value 13 to 15\ndeltaTable.update(condition=(col(\"id\") == 13), set={\"id\": lit(15)})\n\nEt viktig poeng å få med seg her er at vi nå oppdaterte Delta Lake Table objektet både i minnet og på disk. La oss bevise det ved å lese inn fra disk:\n\ndeltaTable2 = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-delt-prod/temp4\")\ndeltaTable2.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n+---+\n\n\n\nOg deretter ved å printe ut det opprinnelige objektet vi leste inn:\n\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#append-data",
    "href": "notebooks/spark/deltalake-intro.html#append-data",
    "title": "Introduksjon til Delta Lake",
    "section": "Append data",
    "text": "Append data\nLa oss legge til verdiene 20 og 21 til datasettet. Først lager vi en Spark dataframe:\n\nnew_data = [(20,), (21,)]\nnew_df = spark.createDataFrame(new_data, [\"id\"])\nnew_df.show()\n\n+---+\n| id|\n+---+\n| 20|\n| 21|\n+---+\n\n\n\nDeretter kan vi appendere det til vår opprinnelige dataframe:\n\nnew_df.write.format(\"delta\").mode(\"append\").save(\n    \"gs://ssb-dapla-felles-data-delt-prod/temp4\"\n)\n\n\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n| 21|\n| 20|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#historien-og-metadata-til-filen",
    "href": "notebooks/spark/deltalake-intro.html#historien-og-metadata-til-filen",
    "title": "Introduksjon til Delta Lake",
    "section": "Historien og metadata til filen",
    "text": "Historien og metadata til filen\nNå som vi har gjort noen endringer kan vi se på historien til filen:\n\n# Lister ut filene i bøtta\nfs = FileClient.get_gcs_file_system()\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/temp4/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp4/_delta_log',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000000.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000001.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000002.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-73e5052f-1b82-48da-ab37-2cbc01bb46c1-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-9b3b81a9-2771-4fb4-9f0f-659fd160d643-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-d04d0ca2-8e8b-42e9-a8a3-0fed9a0e4e41-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-0f2f8ba5-3161-41e8-b5d1-2084128a5bed-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-30d707e4-dd9a-4bfd-a4c7-7fbb1933e9ae-c000.snappy.parquet']\n\n\nVi ser av transaksjonsloggen i _delta_log-mappen at det nå har vært 3 transaksjoner på datasettet. vi ser også av navnene på parquet-filene, part-00000 og part-00001, at det finnes to versjoner av filen. Hvis vi ønsker å bli bedre kjent med hva slags informasjon som blir lagret fra transaksjonene, så kan vi printe ut den siste transaksjonen som heter 00000000000000000002.json:\n\nimport json\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\n# Filsti\npath = \"gs://ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000002.json\"\n\nwith fs.open(path, \"r\") as f:\n    for line in f:\n        data = json.loads(line)\n        pretty_content = json.dumps(data, indent=4)\n        print(pretty_content)\n        print(\"-\" * 50)  # Print separator for clarity\n\n{\n    \"commitInfo\": {\n        \"timestamp\": 1696942544879,\n        \"operation\": \"WRITE\",\n        \"operationParameters\": {\n            \"mode\": \"Append\",\n            \"partitionBy\": \"[]\"\n        },\n        \"readVersion\": 1,\n        \"isolationLevel\": \"Serializable\",\n        \"isBlindAppend\": true,\n        \"operationMetrics\": {\n            \"numFiles\": \"2\",\n            \"numOutputRows\": \"2\",\n            \"numOutputBytes\": \"956\"\n        },\n        \"engineInfo\": \"Apache-Spark/3.3.1 Delta-Lake/2.3.0\",\n        \"txnId\": \"a3dcd582-8362-4fc2-a8ce-57613d2eb2b8\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00000-73e5052f-1b82-48da-ab37-2cbc01bb46c1-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 478,\n        \"modificationTime\": 1696942544755,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":20},\\\"maxValues\\\":{\\\"id\\\":20},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00001-30d707e4-dd9a-4bfd-a4c7-7fbb1933e9ae-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 478,\n        \"modificationTime\": 1696942544833,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":21},\\\"maxValues\\\":{\\\"id\\\":21},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n\n\nHer ser vi at vi får masse informasjon om endringen, både metadata om transaksjonen i commitInfo, og informasjon om nye data-filer i add-delen. Vi ser at det er en veldig rik beskrivelse av endringer, men det kan være vanskeig å lese innholdet direkte. La oss heller bruke history()-funksjonen som kommer med delta:\n\nhistory = deltaTable.history()\nhistory.show()\n\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n|      2|2023-10-10 12:55:...|  null|    null|    WRITE|{mode -&gt; Append, ...|null|    null|     null|          1|  Serializable|         true|{numFiles -&gt; 2, n...|        null|Apache-Spark/3.3....|\n|      1|2023-10-10 12:55:...|  null|    null|   UPDATE|{predicate -&gt; (id...|null|    null|     null|          0|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n|      0|2023-10-10 12:55:...|  null|    null|    WRITE|{mode -&gt; Overwrit...|null|    null|     null|       null|  Serializable|        false|{numFiles -&gt; 2, n...|        null|Apache-Spark/3.3....|\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n\n\n\nSiden det blit trangt i tabellen over så kan vi velge hvilke variabler vi ønsker å se på:\n\n# Oversikt over alle kolonner som finnes i historien\nhistory.columns\n\n['version',\n 'timestamp',\n 'userId',\n 'userName',\n 'operation',\n 'operationParameters',\n 'job',\n 'notebook',\n 'clusterId',\n 'readVersion',\n 'isolationLevel',\n 'isBlindAppend',\n 'operationMetrics',\n 'userMetadata',\n 'engineInfo']\n\n\n\n# Velger de kolonnene jeg er interessert i\nselected_history = deltaTable.history().select(\n    \"version\", \"timestamp\", \"operation\", \"operationParameters\"\n)\n\n\n# Display the selected columns\nselected_history.show(truncate=50)\n\n+-------+-----------------------+---------+--------------------------------------+\n|version|              timestamp|operation|                   operationParameters|\n+-------+-----------------------+---------+--------------------------------------+\n|      2|2023-10-10 12:55:45.014|    WRITE|   {mode -&gt; Append, partitionBy -&gt; []}|\n|      1|2023-10-10 12:55:37.054|   UPDATE|        {predicate -&gt; (id#4452L = 13)}|\n|      0|2023-10-10 12:55:29.048|    WRITE|{mode -&gt; Overwrite, partitionBy -&gt; []}|\n+-------+-----------------------+---------+--------------------------------------+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#egendefinert-metadata",
    "href": "notebooks/spark/deltalake-intro.html#egendefinert-metadata",
    "title": "Introduksjon til Delta Lake",
    "section": "Egendefinert metadata",
    "text": "Egendefinert metadata\nDelta Lake støtter også egendefinert metadata. Det kan for eksempel være nyttig hvis man ønsker å bruke Delta Lake som en backend for et GUI som lar brukeren oppdatere en tabell fra GUI-et. Da ønsker man typisk å lagre hvem som gjorde endringer og når det ble gjort. La oss legge på noe slik metadata:\n\n# Leser inn filen\ndf = spark.read.format(\"delta\").load(\"gs://ssb-dapla-felles-data-delt-prod/temp4\")\n\n\n\nCode\n# Lagrer egendefinert metadata i en json-fil\nimport json\n\nmetadata = {\n    \"comment\": \"Kontaktet oppgavegiver og kranglet!\",\n    \"manueltEditert\": \"True\",\n    \"maskineltEditert\": \"False\",\n}\nmetadata\n\n\n{'comment': 'Kontaktet oppgavegiver og kranglet!',\n 'manueltEditert': 'True',\n 'maskineltEditert': 'False'}\n\n\nVi kan deretter appende dette til den siste versjonen av fila.\n\n(\n    df.write.format(\"delta\")\n    .mode(\"append\")\n    .option(\"userMetadata\", json.dumps(metadata))  # Serialize metadata to a string\n    .save(\"gs://ssb-dapla-felles-data-delt-prod/temp4\")\n)\n\n\n# Laster inn tabellen\ndeltaTable = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-delt-prod/temp4\")\n\n# Henter ut historien\nhistory_df = deltaTable.history()\n\n\n# Show the operation details, including metadata\nhistory_df.select(\n    \"version\", \"timestamp\", \"operation\", \"operationParameters\", \"userMetadata\"\n).show(truncate=10)\n\n+-------+----------+---------+-------------------+------------+\n|version| timestamp|operation|operationParameters|userMetadata|\n+-------+----------+---------+-------------------+------------+\n|      3|2023-10...|    WRITE|         {mode -...|  {\"comme...|\n|      2|2023-10...|    WRITE|         {mode -...|        null|\n|      1|2023-10...|   UPDATE|         {predic...|        null|\n|      0|2023-10...|    WRITE|         {mode -...|        null|\n+-------+----------+---------+-------------------+------------+\n\n\n\n\nhistory_df.select(\"version\", \"userMetadata\").show(truncate=50)\n\n+-------+--------------------------------------------------+\n|version|                                      userMetadata|\n+-------+--------------------------------------------------+\n|      3|{\"comment\": \"Kontaktet oppgavegiver og kranglet...|\n|      2|                                              null|\n|      1|                                              null|\n|      0|                                              null|\n+-------+--------------------------------------------------+\n\n\n\nVi ser at vi la til vår egen metadata i versjon 3 av fila. Vi kan printe ut den rå transaksjonsloggen som tidligere, men nå er vi på transaksjon 3 00000000000000000003.json:\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\n# Filsti\npath = \"gs://ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000003.json\"\n\nwith fs.open(path, \"r\") as f:\n    for line in f:\n        data = json.loads(line)\n        pretty_content = json.dumps(data, indent=4)\n        print(pretty_content)\n        print(\"-\" * 50)  # Print separator for clarity\n\n{\n    \"commitInfo\": {\n        \"timestamp\": 1696942553907,\n        \"operation\": \"WRITE\",\n        \"operationParameters\": {\n            \"mode\": \"Append\",\n            \"partitionBy\": \"[]\"\n        },\n        \"readVersion\": 2,\n        \"isolationLevel\": \"Serializable\",\n        \"isBlindAppend\": false,\n        \"operationMetrics\": {\n            \"numFiles\": \"2\",\n            \"numOutputRows\": \"7\",\n            \"numOutputBytes\": \"989\"\n        },\n        \"userMetadata\": \"{\\\"comment\\\": \\\"Kontaktet oppgavegiver og kranglet!\\\", \\\"manueltEditert\\\": \\\"True\\\", \\\"maskineltEditert\\\": \\\"False\\\"}\",\n        \"engineInfo\": \"Apache-Spark/3.3.1 Delta-Lake/2.3.0\",\n        \"txnId\": \"e7de92bf-b0f9-4341-8bbb-9b382f2f3eb6\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00000-96369f3d-fe4a-4365-a0df-00c813027399-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 503,\n        \"modificationTime\": 1696942553856,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":5,\\\"minValues\\\":{\\\"id\\\":10},\\\"maxValues\\\":{\\\"id\\\":15},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00001-0f1bc8e6-093b-49a9-ad0b-78d5a148cfb6-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 486,\n        \"modificationTime\": 1696942553853,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":2,\\\"minValues\\\":{\\\"id\\\":20},\\\"maxValues\\\":{\\\"id\\\":21},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n\n\nVi er da at metadataene ligger som forventet i metadata-delen."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#footnotes",
    "href": "notebooks/spark/deltalake-intro.html#footnotes",
    "title": "Introduksjon til Delta Lake",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI eksempelet er det Spark V3.3.1 som er installert og jeg installerer derfor delta-spark v2.3↩︎"
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.html",
    "href": "notebooks/spark/sparkr-intro.html",
    "title": "Introduksjon til SparkR",
    "section": "",
    "text": "Akkurat som PySpark så gir SparkR oss et grensesnitt mot Apache Spark fra R. I denne notebooken viser vi noen eksempler hvordan du gjøre vanlige operasjoner med SparkR."
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.html#oppsett",
    "href": "notebooks/spark/sparkr-intro.html#oppsett",
    "title": "Introduksjon til SparkR",
    "section": "Oppsett",
    "text": "Oppsett\nEksemplene i notebooken bruker SparkR (k8s cluster) på https://jupyter.dapla.ssb.no/. Det vil si at den kan distribuere kjøringene på flere maskiner i Kubernetes.\n\nspark\n\nJava ref type org.apache.spark.sql.SparkSession id 1"
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.html#lese-inn-fil",
    "href": "notebooks/spark/sparkr-intro.html#lese-inn-fil",
    "title": "Introduksjon til SparkR",
    "section": "Lese inn fil",
    "text": "Lese inn fil\n\nfile = read.parquet(\"gs://ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet\")\n\n\nselectedColumns &lt;- select(file, \"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\")\nshowDF(selectedColumns, numRows = 5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html",
    "href": "notebooks/spark/deltalake-intro.out.html",
    "title": "Introduksjon til Delta Lake",
    "section": "",
    "text": "Delta Lake er et databaselag som kan legges over parquet-filer i bøtter. Det kan gi oss mye av den funksjonaliteten vi har vært vant til i relasjonelle databaser og datavarehus. I denne notebooken vises hvordan man kan ta det i bruk på Dapla."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#oppsett",
    "href": "notebooks/spark/deltalake-intro.out.html#oppsett",
    "title": "Introduksjon til Delta Lake",
    "section": "Oppsett",
    "text": "Oppsett\nHvis du logger deg inn på https://jupyter.dapla.ssb.no/ kan du ta i bruk Delta Lake via Spark. Men for å gjøre det må du installere delta-spark. For å installere pakken må du jobbe i et ssb-project. I tillegg må du installere delta-spark-versjon som er kompatibel med Spark-versjonen som er installert på Dapla. Gjør derfor følgende:\n\nOpprett et ssb-project med kommandoen:\nssb-project create test-delta-lake\nI terminalen skriver du følgende for å sjekke Spark-versjonen som er installert:\nspark-shell --version\nSjekk hvilken delta-spark-versjon som er kompatibel med din Spark-versjon og installer den med kommandoen[1]:\npoetry add delta-spark@2.3\nÅpne en ny notebook og velg kernel test-delta-lake.\n\nNå har du satt opp et virtuelt miljø med en PySpark-kernel som kjører en maskin (såkalt Pyspark local kernel), der du har installert delta-spark. Vi kan nå importere de bibliotekene vi trenger og sette igang en Spark-session.\n[1] I eksempelet er det Spark V3.3.1 som er installert og jeg installerer derfor delta-spark v2.3\n\n# Importerer biblioteker\nimport pyspark\nfrom delta import *\n\nDeretter initialiserer jeg en Spark-session. %%capture_output er med for å forhindre at det ikke blir printet ut sensitiv informasjon i en notebook som skal inn i Dapla-manualen.\n\n%%capture captured_output\n%run /usr/local/share/jupyter/kernels/pyspark_local/init.py"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#genererer-noe-data",
    "href": "notebooks/spark/deltalake-intro.out.html#genererer-noe-data",
    "title": "Introduksjon til Delta Lake",
    "section": "Genererer noe data",
    "text": "Genererer noe data\n\n# Genererer noe data med Spark\ndata = spark.range(10, 15)\ndata.show()\n\n+---+\n| id|\n+---+\n| 10|\n| 11|\n| 12|\n| 13|\n| 14|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#skriver-ut-en-delta-lake-table",
    "href": "notebooks/spark/deltalake-intro.out.html#skriver-ut-en-delta-lake-table",
    "title": "Introduksjon til Delta Lake",
    "section": "Skriver ut en Delta Lake Table",
    "text": "Skriver ut en Delta Lake Table\nLa oss skrive ut Spark DataFrame som en Delta Lake Table og bli kjent med strukturen i objektet:\n\n%%time\ndata.write.format(\"delta\").mode(\"overwrite\").save(\n    \"gs://ssb-dapla-felles-data-delt-prod/temp4\"\n)\n\nCPU times: user 3.38 ms, sys: 1.7 ms, total: 5.08 ms\nWall time: 5.59 s\n\n\nVi kan deretter printe ut hva som ble opprettet når vi skrev ut en Delta Lake Table:\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/temp4/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp4/_delta_log',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000000.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-9b3b81a9-2771-4fb4-9f0f-659fd160d643-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-0f2f8ba5-3161-41e8-b5d1-2084128a5bed-c000.snappy.parquet']\n\n\n\nDelta Lake Tabellstruktur\nMappenstrukturen du ser over er typisk for en Delta Lake-tabell. La oss bryte ned komponentene:\n\n_delta_log/:\n\nDette er transaksjonsloggmappen for Delta Lake-tabellen. Den inneholder en serie med JSON-filer som registrerer alle transaksjoner gjort på tabellen.\nTransaksjonsloggen er avgjørende for Delta Lakes ACID-transaksjonsegenskaper, som muliggjør funksjoner som atomiske forpliktelser, tilbakerullinger og tid-reise-forespørsler.\n\n_delta_log/00000000000000000000.json:\n\nDette er en JSON-fil innenfor transaksjonsloggkatalogen. Hver JSON-fil i denne mappen tilsvarer en transaksjon (eller en batch av transaksjoner) gjort på tabellen.\nFilnavnet er null-fylt og representerer transaksjonsversjonen. I dette tilfellet representerer 00000000000000000000.json den aller første versjonen (versjon 0) av tabellen. Etter hvert som flere transaksjoner blir gjort, vil nye filer bli lagt til med økende versjonsnumre.\n\npart-00000-450715bd-6b0c-4827-bb8a-b0265505ca72-c000.snappy.parquet og part-00001-977ed55f-5ce5-469f-8a1d-9eafb143c215-c000.snappy.parquet:\n\nDette er de faktiske datafilene til Delta Lake-tabellen, lagret i Parquet-format.\n.snappy.parquet-utvidelsen indikerer at dataene er komprimert ved hjelp av Snappy-komprimeringsalgoritmen.\nFilnavnene er typiske for Sparks navngivningskonvensjon for datadeler. Prefiksene part-00000 og part-00001 indikerer partisjonsnumre. De lange UUID-lignende strengene er unike identifikatorer for datafilene. Suffikset c000 indikerer Spark-oppgaven som skrev filen.\n\n\nMappen representerer en Delta Lake-tabell med data lagret i Parquet-format og en transaksjonslogg som sporer endringer i tabellen. Tilstedeværelsen av _delta_log-mappen og innholdet er det som skiller en Delta Lake-tabell fra et vanlig Parquet-datasett."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#lese-inn-tabell",
    "href": "notebooks/spark/deltalake-intro.out.html#lese-inn-tabell",
    "title": "Introduksjon til Delta Lake",
    "section": "Lese inn tabell",
    "text": "Lese inn tabell\n\ndeltaTable = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-delt-prod/temp4\")\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 13|\n| 14|\n| 10|\n| 11|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#modifisere-datasettet",
    "href": "notebooks/spark/deltalake-intro.out.html#modifisere-datasettet",
    "title": "Introduksjon til Delta Lake",
    "section": "Modifisere datasettet",
    "text": "Modifisere datasettet\nLa oss modifisere datasettet ved å bytte ut verdien 13 med 15 i id-kolonnen:\n\nfrom pyspark.sql.functions import col, lit\n\n# Update the cell with value 13 to 15\ndeltaTable.update(condition=(col(\"id\") == 13), set={\"id\": lit(15)})\n\nEt viktig poeng å få med seg her er at vi nå oppdaterte Delta Lake Table objektet både i minnet og på disk. La oss bevise det ved å lese inn fra disk:\n\ndeltaTable2 = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-delt-prod/temp4\")\ndeltaTable2.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n+---+\n\n\nOg deretter ved å printe ut det opprinnelige objektet vi leste inn:\n\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#append-data",
    "href": "notebooks/spark/deltalake-intro.out.html#append-data",
    "title": "Introduksjon til Delta Lake",
    "section": "Append data",
    "text": "Append data\nLa oss legge til verdiene 20 og 21 til datasettet. Først lager vi en Spark dataframe:\n\nnew_data = [(20,), (21,)]\nnew_df = spark.createDataFrame(new_data, [\"id\"])\nnew_df.show()\n\n+---+\n| id|\n+---+\n| 20|\n| 21|\n+---+\n\n\nDeretter kan vi appendere det til vår opprinnelige dataframe:\n\nnew_df.write.format(\"delta\").mode(\"append\").save(\n    \"gs://ssb-dapla-felles-data-delt-prod/temp4\"\n)\n\n\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n| 21|\n| 20|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#historien-og-metadata-til-filen",
    "href": "notebooks/spark/deltalake-intro.out.html#historien-og-metadata-til-filen",
    "title": "Introduksjon til Delta Lake",
    "section": "Historien og metadata til filen",
    "text": "Historien og metadata til filen\nNå som vi har gjort noen endringer kan vi se på historien til filen:\n\n# Lister ut filene i bøtta\nfs = FileClient.get_gcs_file_system()\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/temp4/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp4/_delta_log',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000000.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000001.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000002.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-73e5052f-1b82-48da-ab37-2cbc01bb46c1-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-9b3b81a9-2771-4fb4-9f0f-659fd160d643-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-d04d0ca2-8e8b-42e9-a8a3-0fed9a0e4e41-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-0f2f8ba5-3161-41e8-b5d1-2084128a5bed-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-30d707e4-dd9a-4bfd-a4c7-7fbb1933e9ae-c000.snappy.parquet']\n\n\nVi ser av transaksjonsloggen i _delta_log-mappen at det nå har vært 3 transaksjoner på datasettet. vi ser også av navnene på parquet-filene, part-00000 og part-00001, at det finnes to versjoner av filen. Hvis vi ønsker å bli bedre kjent med hva slags informasjon som blir lagret fra transaksjonene, så kan vi printe ut den siste transaksjonen som heter 00000000000000000002.json:\n\nimport json\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\n# Filsti\npath = \"gs://ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000002.json\"\n\nwith fs.open(path, \"r\") as f:\n    for line in f:\n        data = json.loads(line)\n        pretty_content = json.dumps(data, indent=4)\n        print(pretty_content)\n        print(\"-\" * 50)  # Print separator for clarity\n\n{\n    \"commitInfo\": {\n        \"timestamp\": 1696942544879,\n        \"operation\": \"WRITE\",\n        \"operationParameters\": {\n            \"mode\": \"Append\",\n            \"partitionBy\": \"[]\"\n        },\n        \"readVersion\": 1,\n        \"isolationLevel\": \"Serializable\",\n        \"isBlindAppend\": true,\n        \"operationMetrics\": {\n            \"numFiles\": \"2\",\n            \"numOutputRows\": \"2\",\n            \"numOutputBytes\": \"956\"\n        },\n        \"engineInfo\": \"Apache-Spark/3.3.1 Delta-Lake/2.3.0\",\n        \"txnId\": \"a3dcd582-8362-4fc2-a8ce-57613d2eb2b8\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00000-73e5052f-1b82-48da-ab37-2cbc01bb46c1-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 478,\n        \"modificationTime\": 1696942544755,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":20},\\\"maxValues\\\":{\\\"id\\\":20},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00001-30d707e4-dd9a-4bfd-a4c7-7fbb1933e9ae-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 478,\n        \"modificationTime\": 1696942544833,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":21},\\\"maxValues\\\":{\\\"id\\\":21},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n\n\nHer ser vi at vi får masse informasjon om endringen, både metadata om transaksjonen i commitInfo, og informasjon om nye data-filer i add-delen. Vi ser at det er en veldig rik beskrivelse av endringer, men det kan være vanskeig å lese innholdet direkte. La oss heller bruke history()-funksjonen som kommer med delta:\n\nhistory = deltaTable.history()\nhistory.show()\n\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n|      2|2023-10-10 12:55:...|  null|    null|    WRITE|{mode -&gt; Append, ...|null|    null|     null|          1|  Serializable|         true|{numFiles -&gt; 2, n...|        null|Apache-Spark/3.3....|\n|      1|2023-10-10 12:55:...|  null|    null|   UPDATE|{predicate -&gt; (id...|null|    null|     null|          0|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n|      0|2023-10-10 12:55:...|  null|    null|    WRITE|{mode -&gt; Overwrit...|null|    null|     null|       null|  Serializable|        false|{numFiles -&gt; 2, n...|        null|Apache-Spark/3.3....|\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n\n\nSiden det blit trangt i tabellen over så kan vi velge hvilke variabler vi ønsker å se på:\n\n# Oversikt over alle kolonner som finnes i historien\nhistory.columns\n\n['version',\n 'timestamp',\n 'userId',\n 'userName',\n 'operation',\n 'operationParameters',\n 'job',\n 'notebook',\n 'clusterId',\n 'readVersion',\n 'isolationLevel',\n 'isBlindAppend',\n 'operationMetrics',\n 'userMetadata',\n 'engineInfo']\n\n\n\n# Velger de kolonnene jeg er interessert i\nselected_history = deltaTable.history().select(\n    \"version\", \"timestamp\", \"operation\", \"operationParameters\"\n)\n\n\n# Display the selected columns\nselected_history.show(truncate=50)\n\n+-------+-----------------------+---------+--------------------------------------+\n|version|              timestamp|operation|                   operationParameters|\n+-------+-----------------------+---------+--------------------------------------+\n|      2|2023-10-10 12:55:45.014|    WRITE|   {mode -&gt; Append, partitionBy -&gt; []}|\n|      1|2023-10-10 12:55:37.054|   UPDATE|        {predicate -&gt; (id#4452L = 13)}|\n|      0|2023-10-10 12:55:29.048|    WRITE|{mode -&gt; Overwrite, partitionBy -&gt; []}|\n+-------+-----------------------+---------+--------------------------------------+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#egendefinert-metadata",
    "href": "notebooks/spark/deltalake-intro.out.html#egendefinert-metadata",
    "title": "Introduksjon til Delta Lake",
    "section": "Egendefinert metadata",
    "text": "Egendefinert metadata\nDelta Lake støtter også egendefinert metadata. Det kan for eksempel være nyttig hvis man ønsker å bruke Delta Lake som en backend for et GUI som lar brukeren oppdatere en tabell fra GUI-et. Da ønsker man typisk å lagre hvem som gjorde endringer og når det ble gjort. La oss legge på noe slik metadata:\n\n# Leser inn filen\ndf = spark.read.format(\"delta\").load(\"gs://ssb-dapla-felles-data-delt-prod/temp4\")\n\n\n# Lagrer egendefinert metadata i en json-fil\nimport json\n\nmetadata = {\n    \"comment\": \"Kontaktet oppgavegiver og kranglet!\",\n    \"manueltEditert\": \"True\",\n    \"maskineltEditert\": \"False\",\n}\nmetadata\n\n{'comment': 'Kontaktet oppgavegiver og kranglet!',\n 'manueltEditert': 'True',\n 'maskineltEditert': 'False'}\n\n\nVi kan deretter appende dette til den siste versjonen av fila.\n\n(\n    df.write.format(\"delta\")\n    .mode(\"append\")\n    .option(\"userMetadata\", json.dumps(metadata))  # Serialize metadata to a string\n    .save(\"gs://ssb-dapla-felles-data-delt-prod/temp4\")\n)\n\n\n# Laster inn tabellen\ndeltaTable = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-delt-prod/temp4\")\n\n# Henter ut historien\nhistory_df = deltaTable.history()\n\n\n# Show the operation details, including metadata\nhistory_df.select(\n    \"version\", \"timestamp\", \"operation\", \"operationParameters\", \"userMetadata\"\n).show(truncate=10)\n\n+-------+----------+---------+-------------------+------------+\n|version| timestamp|operation|operationParameters|userMetadata|\n+-------+----------+---------+-------------------+------------+\n|      3|2023-10...|    WRITE|         {mode -...|  {\"comme...|\n|      2|2023-10...|    WRITE|         {mode -...|        null|\n|      1|2023-10...|   UPDATE|         {predic...|        null|\n|      0|2023-10...|    WRITE|         {mode -...|        null|\n+-------+----------+---------+-------------------+------------+\n\n\n\nhistory_df.select(\"version\", \"userMetadata\").show(truncate=50)\n\n+-------+--------------------------------------------------+\n|version|                                      userMetadata|\n+-------+--------------------------------------------------+\n|      3|{\"comment\": \"Kontaktet oppgavegiver og kranglet...|\n|      2|                                              null|\n|      1|                                              null|\n|      0|                                              null|\n+-------+--------------------------------------------------+\n\n\nVi ser at vi la til vår egen metadata i versjon 3 av fila. Vi kan printe ut den rå transaksjonsloggen som tidligere, men nå er vi på transaksjon 3 00000000000000000003.json:\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\n# Filsti\npath = \"gs://ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000003.json\"\n\nwith fs.open(path, \"r\") as f:\n    for line in f:\n        data = json.loads(line)\n        pretty_content = json.dumps(data, indent=4)\n        print(pretty_content)\n        print(\"-\" * 50)  # Print separator for clarity\n\n{\n    \"commitInfo\": {\n        \"timestamp\": 1696942553907,\n        \"operation\": \"WRITE\",\n        \"operationParameters\": {\n            \"mode\": \"Append\",\n            \"partitionBy\": \"[]\"\n        },\n        \"readVersion\": 2,\n        \"isolationLevel\": \"Serializable\",\n        \"isBlindAppend\": false,\n        \"operationMetrics\": {\n            \"numFiles\": \"2\",\n            \"numOutputRows\": \"7\",\n            \"numOutputBytes\": \"989\"\n        },\n        \"userMetadata\": \"{\\\"comment\\\": \\\"Kontaktet oppgavegiver og kranglet!\\\", \\\"manueltEditert\\\": \\\"True\\\", \\\"maskineltEditert\\\": \\\"False\\\"}\",\n        \"engineInfo\": \"Apache-Spark/3.3.1 Delta-Lake/2.3.0\",\n        \"txnId\": \"e7de92bf-b0f9-4341-8bbb-9b382f2f3eb6\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00000-96369f3d-fe4a-4365-a0df-00c813027399-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 503,\n        \"modificationTime\": 1696942553856,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":5,\\\"minValues\\\":{\\\"id\\\":10},\\\"maxValues\\\":{\\\"id\\\":15},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00001-0f1bc8e6-093b-49a9-ad0b-78d5a148cfb6-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 486,\n        \"modificationTime\": 1696942553853,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":2,\\\"minValues\\\":{\\\"id\\\":20},\\\"maxValues\\\":{\\\"id\\\":21},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n\n\nVi er da at metadataene ligger som forventet i metadata-delen."
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "Spørsmål og svar",
    "section": "",
    "text": "Prosjekt-ID-en til et Google-prosjekt er en unik identifikator som brukes til å identifisere prosjektet i Google Cloud Platform. Prosjekt-ID-en er en streng som består av små bokstaver, tall og bindestrek. Prosjekt-ID-en er ikke det samme som prosjektnavnet, som kan inneholde store bokstaver og mellomrom.\nDu finner prosjekt-ID ved logge deg inn på GCC, åpne prosjektvelgeren, søk opp ditt prosjekt, og så ser du det i høyre kolonne, slik som vist i denne sladdete kolonnen i Figur 1.\n\n\n\n\n\n\nFigur 1: Prosjektvelgeren i Google Cloud Console"
  },
  {
    "objectID": "faq.html#footnotes",
    "href": "faq.html#footnotes",
    "title": "Spørsmål og svar",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nFeaturen dapla-buckets inkluderer produkt- og kildebøtta i henholdsvis standard- og kildeprosjektet som de fleste statistikkproduserende team får ved opprettelse.↩︎\nI produktbøtta blir noncurrent versjoner slettet hvis det er mer enn 2 nyere versjoner, mens for kildebøtta er grensen på 3 nyere versjoner↩︎"
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html",
    "title": "Fra arkiv til parquet",
    "section": "",
    "text": "I arkivet til SSB ligger data lagret som posisjonerte flatfiler, også kalt fastbredde-fil eller fixed width file på engelsk. I Datadok ligger det spesifisert hvordan du leser inn disse filene fra dat eller txt i arkivet til sas7bdat-formatet, men ikke hvordan man konverterer til Parquet-formatet. I denne artikkelen deler jeg hvordan jeg gikk frem for å konvertere arkivfiler til Parquet."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#hva-er-en-fastbredde-fil",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#hva-er-en-fastbredde-fil",
    "title": "Fra arkiv til parquet",
    "section": "Hva er en fastbredde-fil?",
    "text": "Hva er en fastbredde-fil?\nEn fastbredde-fil er en fil der hver rad har en fast lengde, og hver kolonne har en fast posisjon. Det er ingen komma eller andre tegn som skiller kolonnene, slik som i en CSV-fil. En fastbredde-fil er en ren tekstfil, dvs. at du kan åpne den opp i teksteditor og kikke på innholdet direkte.\nUnder er et eksempel hvor samme data er lagret både på CSV-formatet og som fastbredde-fil:\n\n\n\n\ncsv\n\n012345;;Ola Nordmann;\n345678;Kvinne;Kari Nordmann;\n\n\n\n\n\n\nfastbredde-fil\n\n012345      Ola Nordmann \n345678KvinneKari Nordmann\n\n\n\nI csv-filen over til venstre ser vi at hver kolonne er separert med et semikolon, og at hver rad er separert med et linjeskift. I fastbredde-filen til høyre ser vi at hver kolonne har en fast lengde, den tomme kjønnsvariabelen på rad 1 fylles med spaces, hver rad har dermed den samme lengden i antall tegn. I tillegg er det et ekstra mellomrom etter Ola Nordmann ift. Kari Nordmann. Dette er fordi Ola Nordmann er 12 tegn lang, mens Kari Nordmann er 13 tegn lang."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lese-med-pandas",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lese-med-pandas",
    "title": "Fra arkiv til parquet",
    "section": "Lese med Pandas",
    "text": "Lese med Pandas\nVi kan bruke pandas-funksjonen read_fwf() for å lese inn en fastbredde-fil. Denne funksjonen tar inn en filsti, og en liste med bredder for hver kolonne. I tillegg kan vi spesifisere navn på kolonnene, og hvilken datatype kolonnene skal ha og hvordan missing-verdier skal representeres.\nVi er helt avhengig av å vite bredden på hver kolonne for å kunne lese inn en fastbredde-fil. Dette kan vi finne ut ved å åpne filen i en teksteditor og telle/gjette antall tegn i hver kolonne. Alternativt kan vi bruke innlesingsskriptet for SAS som finnes i Datadok, siden breddene er spesifisert der. Under er et ekspempel på hvordan vi kan lese inn en fastbredde-fil fra forrige avsnitt1:\n\nimport pandas as pd\nfrom io import StringIO  # Nødvendig siden vi sender en streng, ikke en filsti til .read_fwf\ninstring = \"112345      Ola Nordmann \\n345678KvinneKari Nordmann\\n\"\ndf = pd.read_fwf(StringIO(instring),\n                 names=['pers_id', 'kjonn', 'navn'],  # Navngi kolonner\n                 dtype='object',  # Alle kolonnene settes til \"object\"\n                 na_values=['.', ' .'],  # Hvilke karakterer bruker SAS for tom verdi?\n                 widths=[6, 6, 13])  # Tell/regn ut dissa sjøl\ndf\n\n\n\n\n\n\n\n\npers_id\nkjonn\nnavn\n\n\n\n\n0\n112345\nNaN\nOla Nordmann\n\n\n1\n345678\nKvinne\nKari Nordmann\n\n\n\n\n\n\n\nKoden over returnerer en Pandas Dataframe i minnet. Den kan vi lett lagre til Parquet-formatet. Men innlesingen måtte vi spesifisere en masse detaljer manuelt. Hvis vi skal lese inn mange filer med ulik struktur, så er ikke denne fremgangsmåten skalerbar. Dette er en fremgangsmåte for å lese inn noen få filer."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#datadok",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#datadok",
    "title": "Fra arkiv til parquet",
    "section": "Datadok",
    "text": "Datadok\nSom nevnt over så finnes det et innlesingsskript for SAS i Datadok. Dette skriptet kan vi bruke til å lese inn en fastbredde-fil i Python. Vi kan også bruke det til å finne breddene på hver kolonne. Et slik skript har denne formen:\n\n\ninnlesingsskript.sas\n\nDATA sas_data;\n   INFILE '/ssb/bruker/felles/flatfileksempel_dapla_manual_blogg.dat' MISSOVER PAD LRECL=36;\n   INPUT\n      @1 pers_id 6.0\n      @7 kjonn $CHAR6.0\n      @13 navn $CHAR13.0\n   ;\nRUN;\n\nVi kunne lest av informasjonen her og omsatt innholdet til argumentene read_fwf() trenger. Men fortsatt innebærer dette potensielt en del manuelt arbeid."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lese-med-saspy",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lese-med-saspy",
    "title": "Fra arkiv til parquet",
    "section": "Lese med saspy",
    "text": "Lese med saspy\nEn annen tilnærming enn å bruke .read_fwf fra Pandas er å bruke biblioteket saspy. Dette biblioteket lar oss kjøre SAS-kode fra Python, på SAS-serverene i prodsonen, og få Dataframes tilbake. Vi kan bruke det til å kjøre sas-skript hentet fra Datadok, konvertere til en pandas dataframe, og til slutt skrive til Parquet. I det følgende antar vi at du jobber i Jupyterlab i prodsonen (sl-jupyter-p), og at du har lagret innlesingsskriptet i en variabel, slik som vist under:\n\n\npython\n\nscript = \"\"\"\nDATA sas_data;\n   INFILE '/ssb/bruker/felles/flatfileksempel_dapla_manual_blogg.dat' MISSOVER PAD LRECL=36;\n   INPUT\n      @1 pers_id 6.0\n      @7 kjonn $CHAR6.0\n      @13 navn $CHAR13.0\n   ;\nRUN;\n\"\"\"\n\nLa oss deretter kjøre følgende kode fra Jupyterlab:\n\n\npython\n\nfrom fagfunksjoner import saspy_session\n\n# Kobler til sas-serverne\nsas = saspy_session()\n\n# Vi bruker tilkoblingen til å sende inn Datadok-skriptet\nresult = sas.submit(script)\n\n# Lagre sas-loggen i en variabel\nlog = result[\"LOG\"]\n\n# Ber om å få dataframe tilbake\ndf_frasas = sas.sd2df(\"sas_data\", \"work\")\n\n# Lukker koblingen til sas-serverne\nsas._endsas()\n\n# Printer ut datasettet\ndf_frasas\n\nI koden over har vi brukt en pakke som heter ssb-fagfunksjoner for å opprette koblingen til sas-serveren. Pakken inneholder et overbygg over saspy, og koden over forutsetter at du har lagret passordet ditt på en spesiell måte2.\n\nDatatyper\nVi har nå en pandas dataframe med datatyper påført, men disse er basert på den lave mengden datatyper i SAS. Ofte bør det ryddes i datatyper før man skriver til Parquet. Spesielt bør du tenke på følgende:\n\nCharacter mappes gjerne til object i pandas, ikke den strengere varianten string eller den mer spesifikke string[pyarrow].\nNumeric mappes stort sett til float64 i pandas, vi får som regel ikke heltall direkte Int64 uten videre behandling.\n\nDu kan la Pandas gjøre ett nytt forsøk på å gjette datatyper ved å kjøre følgende kode:\n\n\npython\n\ndf_pd_dtypes = df_frasas.convert_dtypes()\ndf_pd_dtypes.dtypes\n\nOm du vil teste min selvskrevne funksjon for å gjette på datatyper så ligger den i fellesfunksjons-pakken:\n\n\npython\n\nfrom fagfunksjoner import auto_dtype\ndf_auto = auto_dtype(df_frasas)\ndf_auto.dtypes\n\nSjekk gjerne ut parameteret cardinality_threshold på auto_dtype, om du er interessert i å automatisk sette categorical dtypes.\n\n\nSkalering\nHvis du har mange arkivfiler, med mange forskjellige innlesingsskript, så kan du lagre alle skriptene i en mappe, og så hente innholdet programmatisk. Her er koden for én slik “henting”.\n\n\npython\n\nsas_script_path = \"/ssb/bruker/felles/flatfileksempel_dapla_manual_blogg.sas\"\nwith open(sas_script_path, \"r\", encoding=\"latin1\") as sas_script:\n    script = sas_script.read().strip()\n    script = \"DATA \" + script.split(\"DATA \")[1] # Forkort ned scriptet til det vi trenger\nprint(script)\n\nHer henter jeg inn et innlesingsskript fra Datadok som jeg har lagret som en tekstfil i en mappe på linux-serveren i prodsonen. Deretter gjør jeg den om til et streng-objekt i minnet som kan sendes til saspy-koden som er vist over. Dermed er det bare å finne en logikk som gjør at du vet hvilket innlesingskript som skal brukes til hvilke arkivfiler (siste valide datadok-script før datafil oppstod feks), og du kan jobbe veldig effektivt med konvertering. Når alt er konvertert kan du f.eks. kjøre et script som validerer datatypene på tvers av alle årganger og filnavn."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lagre-dataframen-til-parquet",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lagre-dataframen-til-parquet",
    "title": "Fra arkiv til parquet",
    "section": "Lagre dataframen til parquet",
    "text": "Lagre dataframen til parquet\nNå er det veldig lett å skrive filen til Parquet-formatet.\n\n\npython\n\ndf_auto.to_parquet(\n    \"/ssb/bruker/felles/flatfileksempel_dapla_manual_blogg.parquet\"\n    )"
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#nudb",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#nudb",
    "title": "Fra arkiv til parquet",
    "section": "NUDB",
    "text": "NUDB\nI omleggingen av NUDB (Nasjonal utdanningsdatabase), måtte vi konvertere hele arkivet vårt på 750+ dat-filer.\nDet var ønskelig å slippe å lagre til sas7bdat i mellom, for å slippe mye dataduplikasjon og arbeidsprosesser. Målet vårt var pseudonymiserte parquetfiler i sky.\nI stor grad kunne dette arbeidet automatiseres (bortsett fra å lagre ut innlastingsscript fra gamle datadok). Funksjonene jeg utviklet for dette, ligger stort sett i denne filen:\ngithub.com/utd-nudb/prodsone/konverter_arkiv/archive.py"
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#footnotes",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#footnotes",
    "title": "Fra arkiv til parquet",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\n/n i strengen 112345      Ola Nordmann \\n345678KvinneKari Nordmann\\n betyr linjeskift.↩︎\nHvis du ønsker kan du bruker ssb-fagfunksjoner til å lagre passordet ditt i kryptert form. Da kan du lagre passordet i en fil på din egen maskin, og slipper å skrive det inn hver gang du skal koble til SAS. Funksjonen heter fagfunksjoner.prodsone.saspy_ssb.set_password().↩︎"
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html",
    "title": "Fra Fame til Python",
    "section": "",
    "text": "Mange i SSB har data lagret i Fame som de ønsker å bearbeide med Python og R. Dette er spesielt relevant når man skal flytte statistikkproduksjon til Dapla. fython er en Python-pakke som gjør dette på en enkel måte for deg. Den lar deg eksportere data fra Fame med en enkel funksjon, og kan returnere dataene som enten CSV eller Pandas DataFrame.\nPakken finner du på GitHub."
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#installasjon",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#installasjon",
    "title": "Fra Fame til Python",
    "section": "Installasjon",
    "text": "Installasjon\nPakken er avhengig av at Fame er installert miljøet der den benyttes. Siden den er installert på sl-fame-1.ssb.no1 så vil de færreste har behov for å installere den selv.\nSkulle du likevel ønske å installere pakken selv kan det gjøres med Poetry på følgende måte:\n\n\nterminal\n\npoetry add git+https://github.com/statisticsnorway/ssb-fame-to-python.git"
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#bruk-av-funksjonene",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#bruk-av-funksjonene",
    "title": "Fra Fame til Python",
    "section": "Bruk av funksjonene",
    "text": "Bruk av funksjonene\nfython har to funksjoner: fame_to_csv og fame_to_df. Begge disse funksjonene tar inn de samme argumentene og de er listet opp i Tabell 1.\n\n\n\nTabell 1: Forklaring av argumentene i funksjonene til fython\n\n\n\n\n\n\n\n\n\n\n\nArgument\nForklaring\nfame_to_csv()\nfame_to_pandas()\n\n\n\n\ndatabases\nList of Fame databases to access (with full path).\n✓\n✓\n\n\nfrequency\nFrequency of the data (‘a’, ‘q’, ‘m’).\n✓\n✓\n\n\ndate_from\nStart date for the data in Fame syntax (e.g., ‘2023:1’ for quarterly, ‘2023’ for annual).\n✓\n✓\n\n\ndate_to\nEnd date for the data in Fame syntax (e.g., ‘2023:1’ for quarterly, ‘2023’ for annual).\n✓\n✓\n\n\nsearch_string\nQuery string for fetching specific data. The search is not case sensitive, and “^” and “?” are wildcards (for exactly one and any number of characters, respectively)\n✓\n✓\n\n\ndecimals\nNumber of decimal places in the fetched data (default is 10).\n✓\n✓\n\n\npath\nPath to write the csv-file.\n✓\n\n\n\n\n\n\n\nLa se på noen eksempler.\n\nEksempler\nDersom vi ønsker å hente alt i database1.db og database2.db fra januar 2012 til desember 2022, og få det returnert i en DataFrame, kan vi skrive følgende kode:\n\n\npython\n\nfrom fython import fame_to_pandas\n\ndf = fame_to_pandas(\n  ['sti/til/database1.db', 'sti/til/database2.db', 'm', '2012:1', '2022:12', '?']\n  )\n\nDersom vi i stedet ønsker å hente alle serier som begynner på abc, slutter på d etterfulgt av ett vilkårlig tegn, kan vi skrive følgende kode:\n\n\npython\n\nfrom fython import fame_to_pandas\n\ndf = fame_to_pandas(\n  ['sti/til/database1.db', 'sti/til/database2.db', 'm', '2012:1', '2022:12', 'abc?d^']\n  )\n\n? og ^ er altså jokertegn/wildcards som representerer henholdvis et vilkårlig antall tegn og nøyaktig ett tegn.\nDersom vi i stedet vil lagre dataene til en csv-fil kan vi skrive\n\n\npython\n\nfrom fython import fame_to_csv\n\nfame_to_csv(\n  ['sti/til/database1.db', 'sti/til/database2.db', 'm', '2012:1', '2022:12', 'abc?d^', 'sti/til/csv-fil.csv']\n  )\n\n\n\n\n\n\n\nWarning\n\n\n\nDet er viktig å påpeke at enhver serie kun skrives én gang, og da fra den første databasen den finnes i (kronologisk iht. til listen med databaser)."
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#kjøringer-på-serveren",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#kjøringer-på-serveren",
    "title": "Fra Fame til Python",
    "section": "Kjøringer på serveren",
    "text": "Kjøringer på serveren\nNår du skal bruke fython så må du ta hensyn til hvilken server Fame er installert på, og hvilken server du har tenkt til å jobbe på. Fame er som sagt installert på sl-fame-1.ssb.no, mens Jupyterlab er installert på sl-jupyter-p.ssb.no. Dvs. at hvis du ønsker å bruke fython i en notebook i Jupyterlab, så må du bruke ssh til å koble deg til sl-fame-1.ssb.no, og så kjøre koden derfra. Koden din kan skrive en fil til ønsket stammeområdet, som du igjen kan lese inn direkte i Jupyterlab."
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#automatiserte-uttrekk",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#automatiserte-uttrekk",
    "title": "Fra Fame til Python",
    "section": "Automatiserte uttrekk",
    "text": "Automatiserte uttrekk\nHvis man ønsker at utrekk fra Fame skal skje automatisk på gitte tidspunkter eller intervaller, så kan man ta kontakt med Kundeservice. Fordelen med dette er at man ikke trenger å bruke ssh slik som beskrevet over. Man kan lese inn direkte fra stammeområdet."
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#overføre-data-til-dapla",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#overføre-data-til-dapla",
    "title": "Fra Fame til Python",
    "section": "Overføre data til Dapla",
    "text": "Overføre data til Dapla\nHvis man ønsker å overføre data fra Fame til Dapla, så kan dette settes opp som en MoveIt-operasjon. For å sette opp en MoveIt-jobb må ma kontakte Kundeservice. Overføring til Dapla forutsetter at man har et Dapla-team, og at man setter opp en synkroniseringjobb med Transfer Service."
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#footnotes",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#footnotes",
    "title": "Fra Fame til Python",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPakken er installert i Python-versjon 3.6 på serveren. Du kan åpne et Python-shell i terminalen på sl-fame-1.ssb.no ved å skrive: python3.6.↩︎"
  },
  {
    "objectID": "om-dapla.html",
    "href": "om-dapla.html",
    "title": "Om Dapla",
    "section": "",
    "text": "Om Dapla\nDapla står for dataplattform, og er en skybasert løsning for statistikkproduksjon og forskning.\n\n\nHvorfor Dapla?\nSom dataplattform skal Dapla stimulerere til økt kvalitet på statistikk og forskning, samtidig som den gjør organisasjonen mer tilpasningsdyktig i møte med fremtiden.\n\n\nDen nye skybaserte dataplattformen (Dapla) skal bli viktig for å effektivisere arbeids-og produksjonsprosesser, den skal sikre effektiv lagring og gjenfinning av data og metadata, og støtte opp under deling av data på tvers av statistikkområder.\n\nKilde: Langtidsplan for SSB (2022-2024)\n\n\n\nMålet med Dapla er å tilby tjenester og verktøy som lar statistikkprodusenter og forskere produsere resultater på en sikker og effektiv måte."
  }
]