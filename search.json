[
  {
    "objectID": "notebooks/spark/sparkr-intro.html",
    "href": "notebooks/spark/sparkr-intro.html",
    "title": "Introduksjon til SparkR",
    "section": "",
    "text": "Akkurat som PySpark så gir SparkR oss et grensesnitt mot Apache Spark fra R. I denne notebooken viser vi noen eksempler hvordan du gjøre vanlige operasjoner med SparkR."
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.html#oppsett",
    "href": "notebooks/spark/sparkr-intro.html#oppsett",
    "title": "Introduksjon til SparkR",
    "section": "Oppsett",
    "text": "Oppsett\nEksemplene i notebooken bruker SparkR (k8s cluster) på https://jupyter.dapla.ssb.no/. Det vil si at den kan distribuere kjøringene på flere maskiner i Kubernetes.\n\nspark"
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.html#lese-inn-fil",
    "href": "notebooks/spark/sparkr-intro.html#lese-inn-fil",
    "title": "Introduksjon til SparkR",
    "section": "Lese inn fil",
    "text": "Lese inn fil\n\nfile = read.parquet(\"gs://ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet\")\n\n\nselectedColumns &lt;- select(file, \"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\")\nshowDF(selectedColumns, numRows = 5)"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html",
    "href": "notebooks/spark/pyspark-intro.html",
    "title": "Introduksjon til PySpark",
    "section": "",
    "text": "Apache Spark er et sterkt verktøy som utvider mulighetsrommet for databehandling med R og Python. Kjernefunksjonaliteten ligger i at den lar deg kjøre en jobb på flere maskiner samtidig, noe som ikke er mulig med klassiske rammeverk som Pandas og Tidyverse. Følgelig er det et rammeverk som blant annet er veldig egnet for å prosessere store datamengder eller gjøre store beregninger. Les om mer om Apache Spark i Dapla-manualen\nI denne notebooken vises noen enkle eksempler på hvordan du kan jobbe med data med PySpark, et Python-grensesnitt mot Spark."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#oppsett",
    "href": "notebooks/spark/pyspark-intro.html#oppsett",
    "title": "Introduksjon til PySpark",
    "section": "Oppsett",
    "text": "Oppsett\nNår du logger deg inn på Dapla kan du velge mellom 2 ferdigoppsatte kernels for å jobbe med PySpark:\n\nPyspark (local)\nPyspark (k8s cluster)\n\nDen første lar deg bruke Spark på en enkeltmaskin, mens den andre lar deg distribuere kjøringen på mange maskiner avhengig av hvor store jobbene er. I eksemplene under brukes Pyspark (local).\n\n# Importer biblioteker\nimport numpy as np\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import date_format, explode, expr, sequence, sum, avg\nfrom pyspark.sql.types import DateType, DoubleType, StructField, StructType\n\n# Initialierer en SparkSession\nspark = (\n    SparkSession.builder.master(\"local[2]\")\n    .appName(\"Dapla-manual-example\")\n    .getOrCreate()\n)\n\nI koden over importerer vi de bibliotekene vi skal bruke under. Grunnen til at vi importerer pyspark.sql er at dette er at Spark SQL er Apache Spark sin modul for å jobbe med strukturerte data. Og som navnet tilsier vil det si at vi kan blande Python og SQL i koden vår. Vi skal nærmere på hvordan man bruke SQL fra PySpark-notebook senere.\nSpark tilbyr også et eget grensesnitt, Spark UI, for å monitorere hva som skjer under en SparkSession. Vi kan bruke følgende kommando for å få opp en lenke til Spark UI i notebooken vår:\n\nspark.sparkContext\n\nKlikker du på Spark UI-lenken så tar den deg til dashboard som lar deg monitorere, debugge, optimalisere og forstå kjøringene dine. Det kan være et svært nyttig verktøy i mange tilfeller."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#generere-data",
    "href": "notebooks/spark/pyspark-intro.html#generere-data",
    "title": "Introduksjon til PySpark",
    "section": "Generere data",
    "text": "Generere data\nVi kan begynne med å generere en Spark DataFrame med en kolonne som inneholder månedlige datoer for perioden 2000M1-2023M8.\n\n# Genererer månedlige data\ndates_df = spark.range(1).select(\n    explode(\n        sequence(\n            start=expr(\"date '2000-01-01'\"),\n            stop=expr(\"date '2023-08-01'\"),\n            step=expr(\"interval 1 month\"),\n        )\n    ).alias(\"Date\")\n)\ndates_df.show(5)\n\nEn Spark DataFrame er en distribuert samling av data som er organisert inn i kolonner. Siden Spark lar deg distribuere kjøringer på flere maskiner, er DataFrames optimalisert for å kunne splittes opp slik at de kan brukes på flere maskiner. Med andre er dette ikke det samme som en Pandas dataframe mange kjenner fra før.\nOver genererte vi en datokolonne. For å få litt mer data kan vi også generere 100 kolonner med tidsseriedata og så printer vi de 2 første av disse:\n\n# Genererer random walk data\nschema = StructType(\n    [StructField(f\"serie{i:02d}\", DoubleType(), True) for i in range(100)]\n)\n\ndata = [\n    tuple((10 + np.random.normal(0, 1, 100)).cumsum().tolist())\n    for _ in range(284)  # 284 months from 2000-01 to 2023-08\n]\n\ndata_df = spark.createDataFrame(data, schema=schema)\n\ndata_df.select(\"serie00\", \"serie01\").show(5)\n\nTil slutt kan vi joine de to datasettene sammen og lage noen kolonner som viser år, kvartal og måned. Deretter printer vi ut noen av kolonnene med kommandoen show().\n\n\nCode\n# Legger til row index til DataFrame før join med dates_df\ndata_df = data_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n\n# Joiner de to datasettene\ndf = (\n    dates_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n    .join(data_df, \"row_index\")\n    .drop(\"row_index\")\n)\n\n# Legger til år, kvartal og mnd\ndf = df.withColumn(\"Year\", date_format(df.Date, \"yyyy\"))\ndf = df.withColumn(\"Quarter\", expr(\"quarter(Date)\"))\ndf = df.withColumn(\"Month\", date_format(df.Date, \"MM\"))\n\ndf.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n\nOg med det har vi noe data vi kan jobbe med i resten av notebooken."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#skrive-til-parquet",
    "href": "notebooks/spark/pyspark-intro.html#skrive-til-parquet",
    "title": "Introduksjon til PySpark",
    "section": "Skrive til Parquet",
    "text": "Skrive til Parquet\nPySpark tilbyr mange opsjoner ved skriving til parquet-filer som vi kanskje ikke er vant til å forholde oss til med enklere rammeverk som Pandas. Den enkleste måten å skrive ut en fil er som følger:\ndf.write.parquet(\n    \"gs://ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet\"\n)\nDette vil fungere hvis filen ikke finnes fra før. Hvis den finnes fra før så vil den feile. Grunnen er at vi ikke har spesifisert hva vi ønsker at den skal gjøre. Vi kan velge mellom overwrite, append, ignore eller errorifexists. Sistnevnte er også default-oppførsel hvis du ikke ber den gjøre noe annet.\nUnder bruker vi opsjonen overwrite, det vil si at den skriver over en evt eksisterende fil med samme navn.\n\ndf.write.mode(\"overwrite\").parquet(\n    \"gs://ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet\"\n)\n\nVi kan inspisere hva som ble skrevet ved å liste ut innholder i bøtta.\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/temp/**\")\n\nHvis denne parquet-filen hadde vært partisjonert etter en kolonne, så ville det vært egne undermapper med navnestruktur column_name=value som indikerte hva filen er partisjonert på. Siden vi her bruker en maskin og har et lite datasett, valgte Spark å ikke partisjonere."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#lese-inn-parquet",
    "href": "notebooks/spark/pyspark-intro.html#lese-inn-parquet",
    "title": "Introduksjon til PySpark",
    "section": "Lese inn Parquet",
    "text": "Lese inn Parquet\nApache Spark kan lese inn flere parquet-filer samtidig. Syntaxen er like enkel som den for å skrive ut.\n\ndf_ts = spark.read.parquet(\n    \"gs://ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet\"\n)\ndf_ts.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie66\", \"serie55\").show(5)"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#pyspark-og-sql",
    "href": "notebooks/spark/pyspark-intro.html#pyspark-og-sql",
    "title": "Introduksjon til PySpark",
    "section": "PySpark og SQL",
    "text": "PySpark og SQL\nDu kan også skrive SQL med Spark. For å skrive SQL må vi først lage et temporary view. Under kaller vi viewt for tidsserie.\n\ndf.createOrReplaceTempView(\"tidsserie\")\n\nVi kan deretter skrive en SQL-statement som vi ønsker å kjøre på viewet:\n\nquery = \"SELECT * FROM tidsserie WHERE Year = 2010\"\n\nDeretter kan vi bruke det til å filtrere datasettet:\n\nresult_df = spark.sql(query)\nresult_df.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#aggregering",
    "href": "notebooks/spark/pyspark-intro.html#aggregering",
    "title": "Introduksjon til PySpark",
    "section": "Aggregering",
    "text": "Aggregering\nvi kan aggregere opp enten med PySpark-syntax eller SQL-syntax. Under viser vi begge:\n\nfrom pyspark.sql import functions as F\n\n# Assuming df_ts is your DataFrame\naggregated_df = df_ts.groupBy(\"Quarter\").agg(F.sum(\"serie00\").alias(\"Sum\"),\n                                             F.avg(\"serie00\").alias(\"Average\"),\n                                             F.max(\"serie00\").alias(\"Maximum\"))\n\n# Show the result\naggregated_df.show()\n\nLa oss gjøre det samme med SQL, men grupperer etter to variabler og sorterer output etterpå.\n\n# Assuming df_ts is your DataFrame\ndf_ts.createOrReplaceTempView(\"temp_table\")\n\n# Now you can run a SQL query\nquery = \"\"\"\n    SELECT\n        Year,\n        Quarter,\n        SUM(serie00) AS Sum,\n        AVG(serie00) AS Average,\n        MAX(serie00) AS Maximum\n    FROM \n        temp_table\n    GROUP BY \n        Year, Quarter\n    ORDER BY\n        YEAR, QUARTER\n\"\"\"\n\naggregated_df_sql = spark.sql(query)\n\n# Show the result\naggregated_df_sql.show(10)"
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "Spørsmål og svar",
    "section": "",
    "text": "Hvordan løser jeg feilmeldinger knyttet til at data rate exceeded i Jupyter?\nNår du mottar følgende melding i Jupyter:\n\nFeilmelding:\nIOPub data rate exceeded.\nThe Jupyter server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--ServerApp.iopub_data_rate_limit`.\n\nCurrent values:\nServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\nServerApp.rate_limit_window=3.0 (secs)\n\nbetyr det at mengden data som sendes fra jupyter-kernelen til jupyterlab-frontend overskrider den tillatte grensen. Selv om det er mulig å justere ServerApp.iopub_data_rate_limit og ServerApp.rate_limit_window for å endre denne grensen, ønsker vi ikke dette. Å endre disse verdiene kan påvirke jupyterlabs ytelse negativt.\n\nHer er noen løsningsforslag:\n\nReduser datamengden: Prøv å redusere datamengden du prøver å vise. Hvis du for eksempel viser en stor pandas dataframe, kan du vise kun toppradene med df.head() eller et tilfeldig utvalg med df.sample(10).\nLegg til forsinkelse: Bruk time.sleep()-funksjonen i Python for å legge til en pause mellom hver utskrift. Dette kan spre utdataene over en lengre tidsperiode, noe som kan hjelpe med å unngå å overskride datagrensen.\nSkriv til en fil: I stedet for å skrive utdata direkte i Jupyter, kan du vurdere å skrive dataene til en fil. Dette omgår IOPub-datahastighetsgrensen, og du kan se gjennom dataene i ettertid.\nUnngå utskrift: Hvis du kun trenger å utføre beregninger eller operasjoner på dataene, vurder å gjøre det uten å skrive ut resultatene i Jupyter."
  },
  {
    "objectID": "utviklere/forord.html",
    "href": "utviklere/forord.html",
    "title": "Forord",
    "section": "",
    "text": "Forord\ndløfjdskljfkldsj"
  },
  {
    "objectID": "utviklere/tldr/kultur.html",
    "href": "utviklere/tldr/kultur.html",
    "title": "Kultur",
    "section": "",
    "text": "Kultur"
  },
  {
    "objectID": "utviklere/tldr/utvikling.html",
    "href": "utviklere/tldr/utvikling.html",
    "title": "Utvikling",
    "section": "",
    "text": "Utvikling"
  },
  {
    "objectID": "utviklere/tldr/infrastruktur.html",
    "href": "utviklere/tldr/infrastruktur.html",
    "title": "Infrastruktur",
    "section": "",
    "text": "Infrastruktur"
  },
  {
    "objectID": "utviklere/tldr/versjonskontroll.html",
    "href": "utviklere/tldr/versjonskontroll.html",
    "title": "Versjonskontroll",
    "section": "",
    "text": "Versjonskontroll\n\nMÅ legge alt av kildekode under versjonskontroll.\nMÅ følge SSBs retningslinjer når nye repoer opprettes.\nMÅ følge SSBs retningslinjer for åpenkildekode.\nMÅ anvende Feature Branch mønster.\nMÅ unngå å legge hemmeligheter under versjonskontroll."
  },
  {
    "objectID": "utviklere/tldr/kjoeremiljoe.html",
    "href": "utviklere/tldr/kjoeremiljoe.html",
    "title": "Kjøremiljø",
    "section": "",
    "text": "Kjøremiljø\n\nBØR kjøre tjenester på NAIS.\nMÅ bruke en Service Account for autentisering fra en tjeneste."
  },
  {
    "objectID": "utviklere/index.html",
    "href": "utviklere/index.html",
    "title": "Hurtigstart",
    "section": "",
    "text": "Note\n\n\n\nstart.dapla.ssb.no er ikke klar for opprettelse av team i ny struktur.\n\n\nOpprettelse av team gjøres via start.dapla.ssb.no"
  },
  {
    "objectID": "utviklere/index.html#få-opprettet-et-team-på-dapla",
    "href": "utviklere/index.html#få-opprettet-et-team-på-dapla",
    "title": "Hurtigstart",
    "section": "",
    "text": "Note\n\n\n\nstart.dapla.ssb.no er ikke klar for opprettelse av team i ny struktur.\n\n\nOpprettelse av team gjøres via start.dapla.ssb.no"
  },
  {
    "objectID": "utviklere/hurtigstart.html",
    "href": "utviklere/hurtigstart.html",
    "title": "Kom i gang",
    "section": "",
    "text": "Note\n\n\n\nstart.dapla.ssb.no er ikke klar for opprettelse av team i ny struktur.\n\n\nOpprettelse av team gjøres via start.dapla.ssb.no"
  },
  {
    "objectID": "utviklere/hurtigstart.html#få-opprettet-et-team-på-dapla",
    "href": "utviklere/hurtigstart.html#få-opprettet-et-team-på-dapla",
    "title": "Kom i gang",
    "section": "",
    "text": "Note\n\n\n\nstart.dapla.ssb.no er ikke klar for opprettelse av team i ny struktur.\n\n\nOpprettelse av team gjøres via start.dapla.ssb.no"
  },
  {
    "objectID": "statistikkere/pakke-install-bakken.html",
    "href": "statistikkere/pakke-install-bakken.html",
    "title": "Installere pakker",
    "section": "",
    "text": "Installering av pakker i Jupyter miljøer på bakken (f.eks https://sl-jupyter-p.ssb.no) foregår stort sett helt lik som på Dapla. Det er én viktig forskjell, og det er at installasjon skjer via en proxy som heter Nexus.\n\n\nPip er ferdig konfigurert for bruk av Nexus og kan kjøres som beskrevet for Dapla\n\n\n\nHvis man bruker Poetry for håndtering av pakker i et prosjekt, så må man kjøre følgende kommando i prosjekt-mappe etter prosjektet er opprettet.\n\n\nterminal\n\npoetry source add --default nexus `echo $PIP_INDEX_URL`\n\nDa får man installere pakker som vanlig f.eks\n\n\nterminal\n\npoetry add matplotlib\n\n\n\n\n\n\n\nHvis man forsøker å installere prosjektet i et annet miljø (f.eks Dapla), så må man først fjerne nexus som kilde ved å kjøre:\n\n\nterminal\n\npoetry source remove nexus\n\n\n\n\n\n\n\n\nProsessen med å installere pakker for R på bakken er det samme som på Dapla. Noen pakker (for eksempel devtools) kan foreløpig ikke installeres på bakken på egenhånd pga 3. parti avhengigheter. Vi jobber med å finne en løsning til dette.\nFor å installere arrow, kopier og kjør følgende kommando i R:\n\n\nnotebook\n\ninstall.packages(\"/ssb/bruker/felles/R_pakker/arrow_11.0.0.2.tar.gz\",\n                repos = NULL,\n                type = \"source\")"
  },
  {
    "objectID": "statistikkere/pakke-install-bakken.html#python",
    "href": "statistikkere/pakke-install-bakken.html#python",
    "title": "Installere pakker",
    "section": "",
    "text": "Installering av pakker i Jupyter miljøer på bakken (f.eks https://sl-jupyter-p.ssb.no) foregår stort sett helt lik som på Dapla. Det er én viktig forskjell, og det er at installasjon skjer via en proxy som heter Nexus.\n\n\nPip er ferdig konfigurert for bruk av Nexus og kan kjøres som beskrevet for Dapla\n\n\n\nHvis man bruker Poetry for håndtering av pakker i et prosjekt, så må man kjøre følgende kommando i prosjekt-mappe etter prosjektet er opprettet.\n\n\nterminal\n\npoetry source add --default nexus `echo $PIP_INDEX_URL`\n\nDa får man installere pakker som vanlig f.eks\n\n\nterminal\n\npoetry add matplotlib\n\n\n\n\n\n\n\nHvis man forsøker å installere prosjektet i et annet miljø (f.eks Dapla), så må man først fjerne nexus som kilde ved å kjøre:\n\n\nterminal\n\npoetry source remove nexus"
  },
  {
    "objectID": "statistikkere/pakke-install-bakken.html#r",
    "href": "statistikkere/pakke-install-bakken.html#r",
    "title": "Installere pakker",
    "section": "",
    "text": "Prosessen med å installere pakker for R på bakken er det samme som på Dapla. Noen pakker (for eksempel devtools) kan foreløpig ikke installeres på bakken på egenhånd pga 3. parti avhengigheter. Vi jobber med å finne en løsning til dette.\nFor å installere arrow, kopier og kjør følgende kommando i R:\n\n\nnotebook\n\ninstall.packages(\"/ssb/bruker/felles/R_pakker/arrow_11.0.0.2.tar.gz\",\n                repos = NULL,\n                type = \"source\")"
  },
  {
    "objectID": "statistikkere/orkestrering.html",
    "href": "statistikkere/orkestrering.html",
    "title": "Orkestrering",
    "section": "",
    "text": "Orkestrering"
  },
  {
    "objectID": "statistikkere/versjonering-av-datasett.html",
    "href": "statistikkere/versjonering-av-datasett.html",
    "title": "Versjonering av datasett",
    "section": "",
    "text": "Versjonering av data(sett) er viktig for å dekke kravet om uforanderlighet og etterprøvbarhet. Hovedpoenget med versjonering er at data-konsumenter (menneske eller maskin) skal ha kontroll på endringer, dvs. tilgang både til en stabil versjon (uforanderlighet), men også tilgang til eventuelle nye data- versjoner som oppstår. Se mer om dette i SSBs produksjonsarkitektur og prinsippet om “uforandelighet av data”\n\n\n\n\n\n\nNote\n\n\n\nDet finnes i dag ingen internasjonale standarder eller spesifikasjoner for hvordan endringer av data skal versjoneres. Dette påpekes også av w3.org (https://www.w3.org/TR/dwbp/#dataVersioning) :\nDatasets published on the Web may change over time. Some datasets are updated on a scheduled basis, and other datasets are changed as improvements in collecting the data make updates worthwhile. In order to deal with these changes, new versions of a dataset may be created. Unfortunately, there is no consensus about when changes to a dataset should cause it to be considered a different dataset altogether rather than a new version. In the following, we present some scenarios where most publishers would agree that the revision should be considered a new version of the existing dataset.\n\nScenario 1: a new bus stop is created and it should be added to the dataset;\nScenario 2: an existing bus stop is removed and it should be deleted from the dataset;\nScenario 3: an error was identified in one of the existing bus stops stored in the dataset and this error must be corrected.\n\nAustralian National Data Service (ANDS) beskriver også i sitt dokument https://www.ands.org.au/working-with-data/data-management/data-versioning behovet for versjonering, men også utfordringene med å implementere data- versjonering i praksis.\nErfaringer fra versjonering i microdata.no viser at data- versjoneringen i SSB bør baseres på prinsippet om “breaking changes” ( major changes ) fra Semantic Versioning (SemVer), dvs. at alle endringer som gjør at vi ikke kan gjenskape samme resultat vil medføre at det opprettes en ny versjon av datasettet (en ny versjon i tillegg til gammel/forrige versjon av datasettet).\n\n\n\n\nFølgende hendelser skaper en ny versjon av et datasett:\n\nReberegninger av data med nye metoder.\nKorrigeringer av verdier for eksisterende observasjoner/enheter i datasettet.\n\nSelv manuell endring av bare én data-verdi (celle) i et stort datasett skaper en ny versjon!\n\nLagt til nye og/eller fjernet observasjoner/enheter i datasettet.\nOmkodinger, dvs. oppdatert/erstattet kodeverk.\nLagt til nye variabler. Dette skaper en ny versjon i og med at dette kan påvirke prosesser (programkode).\n\nHvis det gjøres vesentlige endringer (mange nye variabler) så bør det vurderes om dette er et annet (et helt nytt) datasett, ikke en ny versjon av et eksisterende datasett.\n\nFjernet variabler.\n\nVed fjerning av variabler fra et datasett bør det vurderes om dette egentlig er et annet (et helt nytt) datasett, ikke en ny versjon av et eksisterende datasett!\n\nAndre strukturendringer, f.eks. bytte av datatyper eller formater.\n\n\n\n\n\nData som er under utarbeiding skal ikke deles/publiseres, og det er derfor ikke behov for å versjonere slike data/datasett. Disse må betraktes som temporære og “versjonsløse”. Bearbeiding av data bør derfor foregå i egne temporære dataområder1. Det er først på det tidspunktet et datasett er ferdig bearbeidet og klart for deling/publisering at det skal versjoneres og dokumenteres.\n1 Med temporære dataområder menes f.eks. egne mapper med temporære datafiler i bøtter (noe tilsvarende “wk-katalogene” i SSBs eksisterende stammekataloger på Linux). For en del statistikkområder vil databearbeidingen også foregå i temporære datasett i datatjenester som Google BigQuery og CloudSQL.\n\n\n\nVed første gangs deling/publisering av et ferdig bearbeidet datasett oppstår “versjon 1”. Dette er datasett som må bevares uforandret for ettertiden for å dekke kravet til etterprøvbarhet av SSBs statistikk. I tillegg til selve versjonsnummeret er det viktig å dokumentere versjonstidspunktet (metadata om tidspunktet versjonen ble frigitt for bruk) samt en beskrivelse av hvorfor det er utarbeidet en ny versjon. Dette er informasjon data-konsumentene trenger for å kunne reprodusere statistikk med gamle versjoner av data.\nVersjonsnummer skal legges på som et eget element i filnavnet. For versjon 1 vil dette være “_v1”, eksempelvis “framskrevne-befolkningsendringer_p2019_p2050_v1.parquet”\nEksempel på versjon 1 av et datasett i en mappe-struktur:\nssb-prod-team-personstatistikk-data-produkt/  \n└── befolkningsframskrivinger/  \n    └── klargjorte_data/  \n        └── framskrevne-befolkningsendringer_p2019_p2050_v1.parquet\n\n\n\nFor hver versjon som oppstår av datasettet opprettes det en ny fysisk fil hvor versjonsnummeret økes med 1. Alle gamle versjoner av et datasett skal også eksistere i mappen.\n\n\n\n\n\n\nPermanente data skal ikke endres eller slettes\n\n\n\nTidligere delte/publiserte data skal ikke slettes eller overskrives!\nDet må derfor lagres fysiske filer for hver versjon av datasettet. Dette er viktig for at SSB skal oppfylle krav om etterprøvbarhet av statistikkene.\n\n\nEksempel på versjon 1, 2 og 3 av et datasett i en mappe-struktur:\nssb-prod-team-personstatistikk-data-produkt/  \n└── befolkningsframskrivinger/  \n    └── klargjorte_data/  \n        ├── framskrevne-befolkningsendringer_p2019_p2050_v1.parquet  \n        ├── framskrevne-befolkningsendringer_p2019_p2050_v2.parquet  \n        └── framskrevne-befolkningsendringer_p2019_p2050_v3.parquet\n\n\n\n\n\n\nEksempel på versjonsinformasjon og bearbeidingshistorikk for et datasett:\n\n\n\nVersjonsinformasjon:\nDatasett: framskrevne-befolkningsendringer_p2019_p2050\nVersjon: 1\nVersjonstidspunkt: 2019-01-01T10:00:00\nVersjonsbeskrivelse: Første deling/publisering\nFilnavn: framskrevne-befolkningsendringer_p2019_p2050_v1.parquet\nVersjon: 2\nVersjonstidspunkt: 2020-02-15T08:00:00\nVersjonsbeskrivelse: Reberegning med nytt datagrunnlag\nFilnavn: framskrevne-befolkningsendringer_p2019_p2050_v2.parquet\nVersjon: 3\nVersjonstidspunkt: 2021-05-31T10:00:00\nVersjonsbeskrivelse: Revisjon og reberegning med nye framskrivingsmetoder\nFilnavn: framskrevne-befolkningsendringer_p2019_p2050_v3.parquet\n\n\n\n\n\nHvis det er behov for å dele data som er som er i “fart”, dvs. data som fortsatt er under innsamling eller pågående klargjøring, gjøres dette ved å bruke versjonsnummer 0 i filnavnet. Versjonsnummer 0 skal kun brukes midlertidig fram til datasettet oppnår stabil tilstand (ved stabil tilstand byttes versjonsnummer for datasettet til 1 eller høyere).\nEksempel på deling av “versjon 0” av et datasett:\nssb-prod-team-personstatistikk-data-produkt/  \n└── befolkningsframskrivinger/  \n    └── klargjorte_data/  \n        └── framskrevne-befolkningsendringer_p2019_p2050_v0.parquet\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nw3.org anbefaler at gjeldende versjon av et datasett alltid skal være tilgjengelig også uten versjonsnummer. Dette vil også være svært nyttig for statistikkproduksjonen i SSB, i og med at det i de aller fleste tilfeller er siste versjon (den mest oppdaterte) av de klargjorte datasettene som skal benyttes. Den samme programkoden (Python/R) kan da kjøres ved hver statistikkproduksjon uten at filnavnet må endres i programkoden. Det er kun i de få tilfellene hvor gamle versjoner skal benyttes at programkoden må endres, f.eks. ved “reprodusering” av gamle tall.\nEksempel fra w3.org:\n\n/data_example/transport/dataset/bus/stops is the “generic URI” at which the current version of a dataset is always available\n/data_example/transport/dataset/bus/stops_v2 is the versioned URI for the current dataset\n/data_example/transport/dataset/bus/stops_v1 is the versioned URI of the prior version of the dataset\n\n\n\n\n\nEksempel på “versjon 1” (hvor versjonen også er tilgjengelig uten versjonsnummer i datasettnavnet):\nssb-prod-personstatistikk-data-produkt/  \n└── befolkningsframskrivinger/  \n    └── klargjorte-data/  \n        ├── framskrevne-befolkningsendringer_p2019_p2050_v1.parquet  \n        └── framskrevne-befolkningsendringer_p2019_p2050.parquet\n\n\n\n\n\n\nNote\n\n\n\nw3.org anbefaling er at siste versjon også er tilgjengelig uten versjonselement i navnet. I eksempelet over er derfor filen framskrevne-befolkningsendringer_p2019_p2050.parquet helt identisk med framskrevne-befolkningsendringer_p2019_p2050_v1.parquet (versjon 1).\n\n\nEksempel med versjon 1, 2, 3 og 4 (hvor versjon 4 også er tilgjengelig uten versjonsnummer i datasettnavnet)\nssb-prod-personstatistikk-data-produkt/  \n└── befolkningsframskrivinger/  \n    └── klargjorte-data/  \n        ├── framskrevne-befolkningsendringer_p2019_p2050_v1.parquet  \n        ├── framskrevne-befolkningsendringer_p2019_p2050_v2.parquet  \n        ├── framskrevne-befolkningsendringer_p2019_p2050_v3.parquet  \n        ├── framskrevne-befolkningsendringer_p2019_p2050_v4.parquet  \n        └── framskrevne-befolkningsendringer_p2019_p2050.parquet\n\n\n\n\n\n\nNote\n\n\n\nw3.org anbefaling er at siste versjon også er tilgjengelig uten versjonselement i navnet. I eksempelet over er derfor filen framskrevne-befolkningsendringer_p2019_p2050.parquet helt identisk med framskrevne-befolkningsendringer_p2019_p2050_v4.parquet (versjon 4).\n\n\n\n\n\nFor å unngå dobbeltlagring av data i form av at siste versjon av et datasett skal lagres som en fysisk datafil både med og uten versjonsnummer (som vist i kapittelet over), anbefales det at det utvikles felles SSB-programbibliotek i Python og R for utlede denne informasjonen. Da vil da kun være nødvendig å lagre filer med fullt versjonsnummer, men statistikkprodusentene kan bruke en funksjon for å finne siste versjon - eksempelvis gi meg siste versjon av datasettet “framskrevne-befolkningsendringer_p2019_p2050“.\nNedenfor vises en enkel Python-funksjon for hvordan dette kan fungere i praksis. Denne funksjonaliteten er imidlertid ikke tilgjengelig i Dapla i skrivende stund, så dette er bare et forslag til løsning.\n# Eksempel på en felles SSB-funksjon for å hente riktig fysisk filnavn til siste versjon\n# av et datasettt i en mappe (katalog) i en bøtte hvor det eksisterer flere versjoner\n# (flere filversjoner) av datasettet.\n# NB! Dette er kun ment som et eksempel (konsept), og er ikke en produksjonsklar løsning!\n\nimport operator\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\ndef get_current_dataset_version(path:str,\n                                dataset_name_without_version:str,\n                                file_type:str = \"parquet\"\n                               ) -&gt; str:    \n    gcs_dataset_files = fs.glob(path=path + dataset_name_without_version + \"*.\" + file_type)\n\n    file_list = []\n    for file in gcs_dataset_files:\n        file_version = file.split(\"_v\")[-1].split(\".\")[0]\n        if file_version.isnumeric():\n            file_list.append({\"file_path\": str(file), \"file_version\": int(file_version) })\n\n    file_list.sort(key=operator.itemgetter('file_version'))\n    if len(file_list) &gt; 0:\n        return file_list[-1][\"file_path\"]\n    else:\n        return None\n\n\n### Eksempel på bruk ###\n# Hent sti og filnavn til siste versjon av datasettet \"framskrevne-befolkningsendringer_p2019_p2050\"\n# i mappen gs://ssb-prod-befolkning-data-produkt/befolkningsendringer/klargjorte-data/\n# som inneholder 4 versjoner (\"framskrevne-befolkningsendringer_p2019_p2050_v1\" \n# til \"framskrevne-befolkningsendringer_p2019_p2050_v4\")\n\nget_current_dataset_version(\n    path=\"gs://ssb-prod-befolkning-data-produkt/befolkningsendringer/klargjorte-data/\",\n    dataset_name_without_version=\"framskrevne-befolkningsendringer_p2019_p2050\",\n    file_type=\"parquet\")\n\n\n### Eksempel på resultat (sti og filnavn til versjon 4 av datasettet) ###\ngs://ssb-prod-befolkning-data-produkt/befolkningsendringer/klargjorte-data/framskrevne-befolkningsendringer_p2019_p2050_v4"
  },
  {
    "objectID": "statistikkere/versjonering-av-datasett.html#regler-for-hva-som-skaper-en-ny-versjon-av-et-datasett-breaking-changes",
    "href": "statistikkere/versjonering-av-datasett.html#regler-for-hva-som-skaper-en-ny-versjon-av-et-datasett-breaking-changes",
    "title": "Versjonering av datasett",
    "section": "",
    "text": "Følgende hendelser skaper en ny versjon av et datasett:\n\nReberegninger av data med nye metoder.\nKorrigeringer av verdier for eksisterende observasjoner/enheter i datasettet.\n\nSelv manuell endring av bare én data-verdi (celle) i et stort datasett skaper en ny versjon!\n\nLagt til nye og/eller fjernet observasjoner/enheter i datasettet.\nOmkodinger, dvs. oppdatert/erstattet kodeverk.\nLagt til nye variabler. Dette skaper en ny versjon i og med at dette kan påvirke prosesser (programkode).\n\nHvis det gjøres vesentlige endringer (mange nye variabler) så bør det vurderes om dette er et annet (et helt nytt) datasett, ikke en ny versjon av et eksisterende datasett.\n\nFjernet variabler.\n\nVed fjerning av variabler fra et datasett bør det vurderes om dette egentlig er et annet (et helt nytt) datasett, ikke en ny versjon av et eksisterende datasett!\n\nAndre strukturendringer, f.eks. bytte av datatyper eller formater.\n\n\n\n\n\nData som er under utarbeiding skal ikke deles/publiseres, og det er derfor ikke behov for å versjonere slike data/datasett. Disse må betraktes som temporære og “versjonsløse”. Bearbeiding av data bør derfor foregå i egne temporære dataområder1. Det er først på det tidspunktet et datasett er ferdig bearbeidet og klart for deling/publisering at det skal versjoneres og dokumenteres.\n1 Med temporære dataområder menes f.eks. egne mapper med temporære datafiler i bøtter (noe tilsvarende “wk-katalogene” i SSBs eksisterende stammekataloger på Linux). For en del statistikkområder vil databearbeidingen også foregå i temporære datasett i datatjenester som Google BigQuery og CloudSQL.\n\n\n\nVed første gangs deling/publisering av et ferdig bearbeidet datasett oppstår “versjon 1”. Dette er datasett som må bevares uforandret for ettertiden for å dekke kravet til etterprøvbarhet av SSBs statistikk. I tillegg til selve versjonsnummeret er det viktig å dokumentere versjonstidspunktet (metadata om tidspunktet versjonen ble frigitt for bruk) samt en beskrivelse av hvorfor det er utarbeidet en ny versjon. Dette er informasjon data-konsumentene trenger for å kunne reprodusere statistikk med gamle versjoner av data.\nVersjonsnummer skal legges på som et eget element i filnavnet. For versjon 1 vil dette være “_v1”, eksempelvis “framskrevne-befolkningsendringer_p2019_p2050_v1.parquet”\nEksempel på versjon 1 av et datasett i en mappe-struktur:\nssb-prod-team-personstatistikk-data-produkt/  \n└── befolkningsframskrivinger/  \n    └── klargjorte_data/  \n        └── framskrevne-befolkningsendringer_p2019_p2050_v1.parquet\n\n\n\nFor hver versjon som oppstår av datasettet opprettes det en ny fysisk fil hvor versjonsnummeret økes med 1. Alle gamle versjoner av et datasett skal også eksistere i mappen.\n\n\n\n\n\n\nPermanente data skal ikke endres eller slettes\n\n\n\nTidligere delte/publiserte data skal ikke slettes eller overskrives!\nDet må derfor lagres fysiske filer for hver versjon av datasettet. Dette er viktig for at SSB skal oppfylle krav om etterprøvbarhet av statistikkene.\n\n\nEksempel på versjon 1, 2 og 3 av et datasett i en mappe-struktur:\nssb-prod-team-personstatistikk-data-produkt/  \n└── befolkningsframskrivinger/  \n    └── klargjorte_data/  \n        ├── framskrevne-befolkningsendringer_p2019_p2050_v1.parquet  \n        ├── framskrevne-befolkningsendringer_p2019_p2050_v2.parquet  \n        └── framskrevne-befolkningsendringer_p2019_p2050_v3.parquet\n\n\n\n\n\n\nEksempel på versjonsinformasjon og bearbeidingshistorikk for et datasett:\n\n\n\nVersjonsinformasjon:\nDatasett: framskrevne-befolkningsendringer_p2019_p2050\nVersjon: 1\nVersjonstidspunkt: 2019-01-01T10:00:00\nVersjonsbeskrivelse: Første deling/publisering\nFilnavn: framskrevne-befolkningsendringer_p2019_p2050_v1.parquet\nVersjon: 2\nVersjonstidspunkt: 2020-02-15T08:00:00\nVersjonsbeskrivelse: Reberegning med nytt datagrunnlag\nFilnavn: framskrevne-befolkningsendringer_p2019_p2050_v2.parquet\nVersjon: 3\nVersjonstidspunkt: 2021-05-31T10:00:00\nVersjonsbeskrivelse: Revisjon og reberegning med nye framskrivingsmetoder\nFilnavn: framskrevne-befolkningsendringer_p2019_p2050_v3.parquet\n\n\n\n\n\nHvis det er behov for å dele data som er som er i “fart”, dvs. data som fortsatt er under innsamling eller pågående klargjøring, gjøres dette ved å bruke versjonsnummer 0 i filnavnet. Versjonsnummer 0 skal kun brukes midlertidig fram til datasettet oppnår stabil tilstand (ved stabil tilstand byttes versjonsnummer for datasettet til 1 eller høyere).\nEksempel på deling av “versjon 0” av et datasett:\nssb-prod-team-personstatistikk-data-produkt/  \n└── befolkningsframskrivinger/  \n    └── klargjorte_data/  \n        └── framskrevne-befolkningsendringer_p2019_p2050_v0.parquet"
  },
  {
    "objectID": "statistikkere/versjonering-av-datasett.html#funksjonalitet-for-å-hente-siste-gjeldende-versjon-av-et-datasett",
    "href": "statistikkere/versjonering-av-datasett.html#funksjonalitet-for-å-hente-siste-gjeldende-versjon-av-et-datasett",
    "title": "Versjonering av datasett",
    "section": "",
    "text": "Note\n\n\n\nw3.org anbefaler at gjeldende versjon av et datasett alltid skal være tilgjengelig også uten versjonsnummer. Dette vil også være svært nyttig for statistikkproduksjonen i SSB, i og med at det i de aller fleste tilfeller er siste versjon (den mest oppdaterte) av de klargjorte datasettene som skal benyttes. Den samme programkoden (Python/R) kan da kjøres ved hver statistikkproduksjon uten at filnavnet må endres i programkoden. Det er kun i de få tilfellene hvor gamle versjoner skal benyttes at programkoden må endres, f.eks. ved “reprodusering” av gamle tall.\nEksempel fra w3.org:\n\n/data_example/transport/dataset/bus/stops is the “generic URI” at which the current version of a dataset is always available\n/data_example/transport/dataset/bus/stops_v2 is the versioned URI for the current dataset\n/data_example/transport/dataset/bus/stops_v1 is the versioned URI of the prior version of the dataset\n\n\n\n\n\nEksempel på “versjon 1” (hvor versjonen også er tilgjengelig uten versjonsnummer i datasettnavnet):\nssb-prod-personstatistikk-data-produkt/  \n└── befolkningsframskrivinger/  \n    └── klargjorte-data/  \n        ├── framskrevne-befolkningsendringer_p2019_p2050_v1.parquet  \n        └── framskrevne-befolkningsendringer_p2019_p2050.parquet\n\n\n\n\n\n\nNote\n\n\n\nw3.org anbefaling er at siste versjon også er tilgjengelig uten versjonselement i navnet. I eksempelet over er derfor filen framskrevne-befolkningsendringer_p2019_p2050.parquet helt identisk med framskrevne-befolkningsendringer_p2019_p2050_v1.parquet (versjon 1).\n\n\nEksempel med versjon 1, 2, 3 og 4 (hvor versjon 4 også er tilgjengelig uten versjonsnummer i datasettnavnet)\nssb-prod-personstatistikk-data-produkt/  \n└── befolkningsframskrivinger/  \n    └── klargjorte-data/  \n        ├── framskrevne-befolkningsendringer_p2019_p2050_v1.parquet  \n        ├── framskrevne-befolkningsendringer_p2019_p2050_v2.parquet  \n        ├── framskrevne-befolkningsendringer_p2019_p2050_v3.parquet  \n        ├── framskrevne-befolkningsendringer_p2019_p2050_v4.parquet  \n        └── framskrevne-befolkningsendringer_p2019_p2050.parquet\n\n\n\n\n\n\nNote\n\n\n\nw3.org anbefaling er at siste versjon også er tilgjengelig uten versjonselement i navnet. I eksempelet over er derfor filen framskrevne-befolkningsendringer_p2019_p2050.parquet helt identisk med framskrevne-befolkningsendringer_p2019_p2050_v4.parquet (versjon 4).\n\n\n\n\n\nFor å unngå dobbeltlagring av data i form av at siste versjon av et datasett skal lagres som en fysisk datafil både med og uten versjonsnummer (som vist i kapittelet over), anbefales det at det utvikles felles SSB-programbibliotek i Python og R for utlede denne informasjonen. Da vil da kun være nødvendig å lagre filer med fullt versjonsnummer, men statistikkprodusentene kan bruke en funksjon for å finne siste versjon - eksempelvis gi meg siste versjon av datasettet “framskrevne-befolkningsendringer_p2019_p2050“.\nNedenfor vises en enkel Python-funksjon for hvordan dette kan fungere i praksis. Denne funksjonaliteten er imidlertid ikke tilgjengelig i Dapla i skrivende stund, så dette er bare et forslag til løsning.\n# Eksempel på en felles SSB-funksjon for å hente riktig fysisk filnavn til siste versjon\n# av et datasettt i en mappe (katalog) i en bøtte hvor det eksisterer flere versjoner\n# (flere filversjoner) av datasettet.\n# NB! Dette er kun ment som et eksempel (konsept), og er ikke en produksjonsklar løsning!\n\nimport operator\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\ndef get_current_dataset_version(path:str,\n                                dataset_name_without_version:str,\n                                file_type:str = \"parquet\"\n                               ) -&gt; str:    \n    gcs_dataset_files = fs.glob(path=path + dataset_name_without_version + \"*.\" + file_type)\n\n    file_list = []\n    for file in gcs_dataset_files:\n        file_version = file.split(\"_v\")[-1].split(\".\")[0]\n        if file_version.isnumeric():\n            file_list.append({\"file_path\": str(file), \"file_version\": int(file_version) })\n\n    file_list.sort(key=operator.itemgetter('file_version'))\n    if len(file_list) &gt; 0:\n        return file_list[-1][\"file_path\"]\n    else:\n        return None\n\n\n### Eksempel på bruk ###\n# Hent sti og filnavn til siste versjon av datasettet \"framskrevne-befolkningsendringer_p2019_p2050\"\n# i mappen gs://ssb-prod-befolkning-data-produkt/befolkningsendringer/klargjorte-data/\n# som inneholder 4 versjoner (\"framskrevne-befolkningsendringer_p2019_p2050_v1\" \n# til \"framskrevne-befolkningsendringer_p2019_p2050_v4\")\n\nget_current_dataset_version(\n    path=\"gs://ssb-prod-befolkning-data-produkt/befolkningsendringer/klargjorte-data/\",\n    dataset_name_without_version=\"framskrevne-befolkningsendringer_p2019_p2050\",\n    file_type=\"parquet\")\n\n\n### Eksempel på resultat (sti og filnavn til versjon 4 av datasettet) ###\ngs://ssb-prod-befolkning-data-produkt/befolkningsendringer/klargjorte-data/framskrevne-befolkningsendringer_p2019_p2050_v4"
  },
  {
    "objectID": "statistikkere/slette-data.html",
    "href": "statistikkere/slette-data.html",
    "title": "Slette data fra bøtter",
    "section": "",
    "text": "Slette data fra bøtter\nSletting av filer og mapper fra bøtter kan gjøres fra Google Cloud Console. Søk opp “Cloud Storage” i søkefeltet og klikk på den bøtten hvor filen er lagret under “Buckets”.\nKryss av filen/katalogen som du ønsker å slette og trykk “Delete” (Figur 1)\n\n\n\nFigur 1: Sletting av en fil\n\n\nSiden bøtter på Dapla har versjonering får man opp en dialogboks som informerer om at objektet (dvs. filen) er versjonert (Figur 2). Trykk på “Delete”.\n\n\n\nFigur 2: Bekreft sletting av fil\n\n\nSlettingen kan ta noe tid. Når denne er ferdig vil filen være slettet, men den kan fortsatt gjenopprettes. Hvis du ønsker at filen skal slettes permanent, gjør følgende:\n\nSkru på visning av slettede filer med å bruke radioknappen “Show deleted data” (Figur 3)\n\n\n\n\nFigur 3: Skru på visning av slettede filer\n\n\n\nFinn frem til den slettede filen og trykk på linken “1 noncurrent version” eller tilsvarende (Figur 4). Dette vil ta deg direkte til en side som viser filens versjonshistorikk.\n\n\n\n\nFigur 4: Velg versjonshistorikk\n\n\n\nVelg alle versjoner som vist på Figur 5 og trykk “Delete”\n\n\n\n\nFigur 5: Slett alle versioner\n\n\n\nTil slutt må man bekrefte at man ønsker å slette alle versioner (Figur 6) med å skrive inn DELETE og trykke på den blå “Delete”-knappen:\n\n\n\n\nFigur 6: Bekreft sletting av alle versjoner"
  },
  {
    "objectID": "statistikkere/tilgangsstyring.html",
    "href": "statistikkere/tilgangsstyring.html",
    "title": "Tilgangsstyring",
    "section": "",
    "text": "Nevne tilgangsstyringspolicyen i SSB\nHvert Dapla-team har sine egne lagringsområder for data som ingen andre har tilgang til, med mindre teamet eksplisitt velger å dele data med andre team. I tillegg har teamet tilgang til egne ressurser for å behandle dataene.\nDet er tilgangsgruppen managers som bestemmer hvilke personer som skal ha hvilke roller i et team, og dermed hvilke data de ulike team-medlemmene får tilgang til. Den som jobber med data kan bli plassert i tilgangsgruppene data-admins eller developers. Sistnevnte får tilgang til alle datatilstander utenom kildedata, mens data-admins er forhåndsgodkjent til å også å aksessere kildedata ved behov. Dermed er data-admins en priveligert rolle på teamet som er forbeholdt noen få personer.\n\n\n\nFigur 1: Datatilstander som et team sitt medlemmer har ilgang til.\n\n\nFigur 1 viser hvem som har tilgang til hvilke datatilstander. Som nevnt er data-admins ansett som forhåndsgodkjent til å aksessere kildedata ved behov. Måten dette er implementert på er at data-admins må aktivere denne tilgangen selv, ved å bruke et JIT-grensesnitt (Just-In-Time Access). Tilgangen krever en begrunnelse og bruken kan løpende monitoreres av managers for teamet."
  },
  {
    "objectID": "statistikkere/transfer-service.html",
    "href": "statistikkere/transfer-service.html",
    "title": "Transfer Service",
    "section": "",
    "text": "Storage Transfer Service1 er en Google-tjeneste for å flytte data mellom lagringsområder. I SSB bruker vi hovedsakelig tjenesten til å:\nTjenesten støtter både automatiserte og ad-hoc overføringer, og den inkluderer et brukergrensesnitt for å sette opp og administrere overføringene i Google Cloud Console (GCC)."
  },
  {
    "objectID": "statistikkere/transfer-service.html#tilgangsstyring",
    "href": "statistikkere/transfer-service.html#tilgangsstyring",
    "title": "Transfer Service",
    "section": "Tilgangsstyring",
    "text": "Tilgangsstyring\nTilgangsstyringen til data gjelder også for overføringer av data med Transfer Service. Det betyr at du må ha tilgang til dataene du skal sette opp overføringsjobber for. Ved bruk av Transfer Service for overføring av data mellom bakke og sky så er det satt opp en dedikerte mapper for dette i prodsonen. Også her følges tilgangsstyringen til dataene, med unntak av at data-admins har permanent tilgang til kildedata som er synkronisert ned til bakken, mens man på Dapla må de gi seg selv korte, begrunnede tilganger ved behov.\n\n\nPå Dapla så er det opprettet dedikerte bøtter for overføring av data mellom bakke og sky. Disse heter tilsky og frasky. Tanken med disse “mellomstasjonene” for overføring av data er at de skal beskytte Dapla-team fra å overskrive data ved en feil. Ved å ha egne bøtter som data blir synkronisert gjennom, så legges det opp til at man deretter manuelt3 flytter dataene til riktig bøtte.\nMen det er ikke lagt noen sperrer for synkronisere direkte til en annen bøtte man har tilgang til. Systembrukeren (se forklaringsboks) som kjører Transfer Service har tilgang til alle bøttene i prosjektet. Det betyr at en data-admin kan velge å synkronisere data direkte inn i kildebøtta hvis man mener at det er hensiktsmessig. Det samme gjelder for developers som setter opp dataoverføringer i standardprosjektet. Men da er det som sagt viktig å være bevisst på hvordan man setter opp reglene for overskriving av data hvis filene har like navn. Disse opsjonene forklares nærmere senere i kapitlet.\n\n\n\n\n\n\n\n\n\nPersonlig bruker vs systembruker\n\n\n\nNår du setter opp en overføringsjobb med Transfer Service så setter du opp en jobb som kjøres av en systembruker4 og ikke din egen personlige bruker. Dette er spesielt viktig å være klar over når man setter opp automatiserte overføringsjobber. En konsekves av dette er at automatiske overføringsjobber vil fortsette å kjøre selv om din tilgang til dataene er midlertidig, siden det er en systembruker som faktisk kjører jobben."
  },
  {
    "objectID": "statistikkere/transfer-service.html#forberedelser",
    "href": "statistikkere/transfer-service.html#forberedelser",
    "title": "Transfer Service",
    "section": "Forberedelser",
    "text": "Forberedelser\nFørste gang du bruker Transfer Service må du sjekke at tjenesten er aktivert for teamet. Transfer Service er en såkalt feature som teamet kan skru av og på selv. For å sjekke om den er skrudd på går du inn i teamets IaC-repo5 og sjekker filen ./infra/projects.yaml.\n\n\ndapla-example-iac/infra/projects.yaml\n\nteam_uniform_name: dapla-example\n\nprojects:\n  - project_name: dapla-example\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service\n\nI filen over ser du at teamet har skrudd på tjenesten i prod-miljøet, siden den transfer-service er listet under features. Hvis tjenesten ikke er skrudd på kan du lese om hvordan du skrur den på i feature-dokumentasjonen."
  },
  {
    "objectID": "statistikkere/transfer-service.html#overføring-av-data",
    "href": "statistikkere/transfer-service.html#overføring-av-data",
    "title": "Transfer Service",
    "section": "Overføring av data",
    "text": "Overføring av data\n\n\n\n\n\n\nOverføring av kildedata\n\n\n\nOverføring av kildedata må gjøres av en data-admin i teamet som har aktivert sin forhåndsgodkjente tilgang til kildedata. Tilgangen aktiveres ved å gå inn i JIT-applikasjonen og velge prosjekt-id. Deretter velger du rollene ssb.bucket.write, ssb.buckets.list og storagetransfer.admin, og hvor lenge du ønsker tilgangen. Til slutt oppgir du en begrunnelse for hvorfor du trenger tilgangentilgangen og trykker Request access. Når du har gjort dette vil du få en bekreftelse på at tilgangen er aktivert, og det tar ca 1 minutt før den aktiverte tilgangen er synlig i GCC.\n\n\nGrensesnittet for å sette opp overføringsjobber i Transfer Service er tilgjengelig i Google Cloud Console (GCC).\n\n\n\nGå inn på Google Cloud Console i en nettleser.\nSjekk, øverst i høyre hjørne, at du er logget inn med din SSB-konto (xxx@ssb.no).\nVelg prosjektet6 som overføringen skal settes opp under.\nEtter at du har valgt prosjekt kan du søke etter Storage Transfer i søkefeltet øverst på siden, og gå inn på siden.\n\n\n\n\n\n\n\n\n\n\nHva er mitt prosjektnavn?\n\n\n\nNår det opprettes et Dapla-team, så opprettes det flere Google-prosjekter for teamet. Når du skal velge hvilket prosjekt du skal jobbe på i GCC, så følger de en fast navnestruktur. For eksempel så vil et team med navnet dapla-example få et standardprosjekt som heter dapla-example-p. Det blir også opprettet et kildeprosjekt som heter dapla-example-kilde-p.\n\n\n\n\nFørste gang du bruker Storage Transfer må man gjøre en engangsjobb for å bruke tjenesten. Dette gjøres kun første gang din bruker setter opp en jobb, og deretter trenger du ikke å gjøre det flere ganger.\nNår du kommer inn på siden til Storage Transfer så trykker du på Set Up Connection. Når du trykker på denne vil det dukke opp et nytt felt hvor du får valget Create Pub-Sub Resources. Trykk på den blå Create-knappen, og deretter trykk på Close lenger nede. Da er engangsjobben gjort, og du kan begynne å sette opp overføringsjobber.\n\nI navigasjonsmenyen til venstre trykk Transfer jobs, og deretter trykk på + Create transfer job øverst på siden for å opprette en ny overføringsjobb. Da får du opp bildet som vist i Figur 1.\n\n\n\n\nFigur 1: Opprett overføringsjobb i Google Cloud Console.\n\n\nVidere vil det variere om man skal overføre data mellom bøtter eller mellom Dapla og prodsonen. Under forklarer vi begge fremgangsmåtene.\n\nProdsonen og Dapla\nOverføring mellom bakke og sky er en overføring av data mellom en bøtte på Dapla og en mappe i prodsonen. Siden tilgangsstyring til kildedata er strengere enn tilgangsstyring til annen data, så er det det to litt fremgangsmåter for å sette opp overføringsjobber for disse.\nSiden stegene er litt forskjellig avhengig av om man skal flytte kildedata eller annen data, så deler vi denne delen i to. Figur 3 viser hvordan dette er satt opp. Kildeprosjektet på Dapla har en tilsky-bøtte for å flytting av data fra prodsonen til Dapla, og den har en frasky-bøtte for å flytte data fra Dapla til prodsonen. Standardprosjektet på Dapla har også en tilsky-bøtte for å flytte data fra prodsonen til Dapla, og den har en frasky-bøtte for å flytte data fra Dapla til prodsonen.\n\n\n\nFigur 2: Overføring av data mellom prodsonen og Dapla.\n\n\nVidere viser vi hvordan man overfører fra Dapla til prodsonen. Overføring motsatt vei innebærer bare at man bytter om på Source type og Destination type.\n\nI fanen Get started velger du:\n\nSource type: Google Cloud Storage\nDestination type: POSIX filesystem\n\nI fanen Choose a source trykker du på Browse, velger hvilken bøtte eller “undermappe” i en bøtte du skal overføre fra, og trykker Select7.\nI fanen Choose a destination velger du transfer_service_default under Agent pool. Under Destination directory path velger du hvilken undermappe av som filen skal overføres til. Tjenesten vet allerede om du er i kilde- eller standardprosjektet, så du trenger kun å skrive inn frasky/ eller tilsky/ her, og evt. undermappenavn hvis det er aktuelt (f.eks. frasky/data/8). Trykk Next step.\nI fanen Choose when to run job velger du hvor ofte og hvordan jobber skal kjøre. Tabell 3 viser hvilke valg du kan ta. Trykk Next step.\n\n\n\nTabell 1: Valg under Choose when to run job\n\n\n\n\n\n\nValg\nFrekvens\n\n\n\n\nRun once\nEngangoverføringer\n\n\nRun every day\nSynkroniser hver dag\n\n\nRun every week\nSynkroniser hver uke\n\n\nRun with custom frequency\nSynkroniser inntill hver time\n\n\nRun on demand\nSynkroniserer når du manuelt trigger jobben\n\n\n\n\n\nI fanen Choose settings kan du velge hvordan detaljer knyttet til overføringen skal håndteres. Tabell 4 viser hvilke valg du kan ta.\n\n\n\nTabell 2: Valg under Choose settings\n\n\n\n\n\n\n\nValg\nUndervalg\nHandling\n\n\n\n\nIdentify your job\n\nBeskriv jobben kort.\n\n\nManifest file\n\nIkke relevant. Bruk default valg.\n\n\nChoose how to handle your data\nMetadata options\nIkke relevant. Bruk default valg.\n\n\n\nWhen to overwrite\nTenk nøye gjennom hva du velger her.\n\n\n\nWhen to delete\nTenk nøye gjennom hva du velger her.\n\n\nChoose how to keep track of transfer progress\nLogging options\nSkru på logging.\n\n\n\n\nValgene When to overwrite og When to delete er det viktig at tenkes nøye gjennom, spesielt ved automatiske synkroniseringer. When to overwrite er spesielt siden det kan føre til data blir overskrevet eller tapt.\n\nTrykk på den blå Create-knappen for å opprette overføringsjobben. Du vil kunne se kjørende jobber under menyen Transfer jobs.\n\n\nMappestrukturen i prodsonen\nMappestrukturen for overføringer med Transfer Service mellom bakke og sky har en fast struktur som er likt for alle team. Hvis du logger deg inn i terminalen på en av Linux-serverne i prodsonen, åpner du mappen ved å skrive cd /ssb/cloud_sync. Under denne mappen finner du en mappe for hvert team som har aktivert Transfer Service. Hvis et team for eksempel heter dapla-example så vil det være en mappe som heter dapla-example. Her kan teamet hente og levere data som skal synkroniseres mellom bakke og sky. Videre er det undermapper for kilde- og standardprosjektet til teamet. Det er kun data-admins som har tilgang til kildeprosjektet, og det er kun developers som har tilgang til standardprosjektet. Under finner du en oversikt over hvordan mappene ser ut for et team som heter dapla-example.\n\n\n/ssb/cloud_sync/dapla-example/\n\ndapla-example\n│\n├── kilde\n│   │\n│   │── tilsky\n│   │\n│   └── frasky\n│\n└── standard\n    │\n    │── tilsky\n    │\n    └── frasky\n\n\n\n\nBøtte til bøtte\nOverføring mellom bøtter er en overføring av data mellom to bøtter på Dapla. Fremgangsmåten er helt likt som beskrevet tidligere, men at du nå velger Google Cloud Storage som både kilde og destinasjon. Igjen er vi avhengig av at systembrukeren som utfører jobben har tilgang til begge bøttene som er involvert i overføringen. Default er at et team kan overføre mellom bøtter i kildeprosjektet, og at de kan overføre mellom bøtter i standardprosjektet, men aldri mellom de to. Hvis du ønsker å overføre mellom bøtter i ditt prosjekt og et annet teams prosjekt, så må du be det andre teamet om å gi din systembruker tilgang til dette.\n\n\nProdsonen og Dapla\nOverføring mellom bakke og sky er en overføring av data mellom en bøtte på Dapla og en mappe i prodsonen. Siden tilgangsstyring til kildedata er strengere enn tilgangsstyring til annen data, så er det det to litt fremgangsmåter for å sette opp overføringsjobber for disse.\nSiden stegene er litt forskjellig avhengig av om man skal flytte kildedata eller annen data, så deler vi denne delen i to. Figur 3 viser hvordan dette er satt opp. Kildeprosjektet på Dapla har en synk-opp-bøtte for å flytting av data fra prodsonen til Dapla, og den har en synk-ned-bøtte for å flytte data fra Dapla til prodsonen. Standardprosjektet på Dapla har også en synk-opp-bøtte for å flytte data fra prodsonen til Dapla, og den har en synk-ned-bøtte for å flytte data fra Dapla til prodsonen.\n\n\n\nFigur 3: Overføring av data mellom prodsonen og Dapla.\n\n\nVidere viser vi hvordan man overfører fra Dapla til prodsonen. Overføring motsatt vei innebærer bare at man bytter om på Source type og Destination type.\n\nI fanen Get started velger du:\n\nSource type: Google Cloud Storage\nDestination type: POSIX filesystem\n\nI fanen Choose a source trykker du på Browse, velger hvilken bøtte eller “undermappe” i en bøtte du skal overføre fra, og trykker Select9.\nI fanen Choose a destination velger du transfer_service_default under Agent pool. Under Destination directory path velger du hvilken undermappe av som filen skal overføres til. Tjenesten vet allerede om du er i kilde- eller standardprosjektet, så du trenger kun å skrive inn frasky/ eller tilsky/ her, og evt. undermappenavn hvis det er aktuelt (f.eks. frasky/data/10). Trykk Next step.\nI fanen Choose when to run job velger du hvor ofte og hvordan jobber skal kjøre. Tabell 3 viser hvilke valg du kan ta. Trykk Next step.\n\n\n\nTabell 3: Valg under Choose when to run job\n\n\n\n\n\n\nValg\nFrekvens\n\n\n\n\nRun once\nEngangoverføringer\n\n\nRun every day\nSynkroniser hver dag\n\n\nRun every week\nSynkroniser hver uke\n\n\nRun with custom frequency\nSynkroniser inntill hver time\n\n\nRun on demand\nSynkroniserer når du manuelt trigger jobben\n\n\n\n\n\nI fanen Choose settings kan du velge hvordan detaljer knyttet til overføringen skal håndteres. Tabell 4 viser hvilke valg du kan ta.\n\n\n\nTabell 4: Valg under Choose settings\n\n\n\n\n\n\n\nValg\nUndervalg\nHandling\n\n\n\n\nIdentify your job\n\nBeskriv jobben kort.\n\n\nManifest file\n\nIkke relevant. Bruk default valg.\n\n\nChoose how to handle your data\nMetatdata options\nIkke relevant. Bruk default valg.\n\n\n\nWhen to overwrite\nTenk nøye gjennom hva du velger her.\n\n\n\nWhen to delete\nTenk nøye gjennom hva du velger her.\n\n\nChoose how to keep track of transfer progress\nLogging options\nSkru på logging.\n\n\n\n\nValgene When to overwrite og When to delete er det viktig at tenkes nøye gjennom, spesielt ved automatiske synkroniseringer. When to overwrite er spesielt siden det kan føre til data blir overskrevet eller tapt.\n\nTrykk på den blå Create-knappen for å opprette overføringsjobben. Du vil kunne se kjørende jobber under menyen Transfer jobs.\n\n\nMappestrukturen i prodsonen\nMappestrukturen for overføringer med Transfer Service mellom bakke og sky har en fast struktur som er likt for alle team. Hvis du logger deg inn i terminalen på en av Linux-serverne i prodsonen, åpner du mappen ved å skrive cd /ssb/cloud_sync. Under denne mappen finner du en mappe for hvert team som har aktivert Transfer Service. Hvis et team for eksempel heter dapla-example så vil det være en mappe som heter dapla-example-p. Her kan teamet hente og levere data som skal synkroniseres mellom bakke og sky. Videre er det undermapper for kilde- og standardprosjektet til teamet. Det er kun data-admins som har tilgang til kildeprosjektet, og det er kun developers som har tilgang til standardprosjektet. Under finner du en oversikt over hvordan mappene ser ut for et team som heter dapla-example.\n\n\n/ssb/cloud_sync/dapla-example-p/\n\ndapla-example-p\n│\n├── kilde\n│   │\n│   │── tilsky\n│   │\n│   └── frasky\n│\n└── standard\n    │\n    │── tilsky\n    │\n    └── frasky\n\n\n\n\nBøtte til bøtte\nOverføring mellom bøtter er en overføring av data mellom to bøtter på Dapla. Fremgangsmåten er helt likt som beskrevet tidligere, men at du nå velger Google Cloud Storage som både kilde og destinasjon. Igjen er vi avhengig av at systembrukeren som utfører jobben har tilgang til begge bøttene som er involvert i overføringen. Default er at et team kan overføre mellom bøtter i kildeprosjektet, og at de kan overføre mellom bøtter i standardprosjektet, men aldri mellom de to. Hvis du ønsker å overføre mellom bøtter i ditt prosjekt og et annet teams prosjekt, så må du be det andre teamet om å gi din systembruker tilgang til dette."
  },
  {
    "objectID": "statistikkere/transfer-service.html#footnotes",
    "href": "statistikkere/transfer-service.html#footnotes",
    "title": "Transfer Service",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI SSB kaller vi tjenesten for Transfer Service, men du kan oppleve at Google kaller den litt forskjellige ting. Den blir omtalt som Storage Transfer Service noen steder, mens i Google Cloud Console blir den omtalt som Data Transfer eller Storage Transfer↩︎\nFlytting av data mellom bøtter krever at prosjektets Transfer Service har tilgang til begge bøttene.↩︎\nMed manuelt menes her at man går inn og flytter filer fra en bøtte til en annen. Men det kan også bety at man flytter data til riktig bøtte som en del produksjonskoden sin, som igjen kan kjøres automatisk.↩︎\nSystembrukere heter Service Accounts på engelsk og blir ofte referert til som SA-er i dagligtale.↩︎\nDu finner teamets IaC-repo ved å gå inn på https://github.com/orgs/statisticsnorway/repositories og søke etter ditt teamnavn og åpne den som har navnestrukturen teamnavn-iac. For eksempel vil et team som heter dapla-example har et IaC-repo som heter dapla-example-iac.↩︎\nDu kan velge prosjekt øverst på siden, til høyre for teksten Google Cloud. I bildet under ser du at hvordan det ser ut når prosjektet dapla-felles-p er valgt.↩︎\nNår du skal velge en undermappe i en bøtte så er grensesnittet litt lite intuitivt. Du kan ikke trykke på navnet, men du på trykke på -tegnet for å se undermappene.↩︎\nNår du skal synkronisere fra Dapla til en undermappe i prodsonen, så må mappen i prodsonen allerede eksisterere. Hvis den ikke gjør det vil jobben feile. Ved synkronsiering fra prodsonen til Dapla trenger ikke undermappen eksistere, siden bøtter egentlig ikke har undermapper og filstien fra prodsonen bare blir til filnavnet i bøtta.↩︎\nNår du skal velge en undermappe i en bøtte så er grensesnittet litt lite intuitivt. Du kan ikke trykke på navnet, men du på trykke på -tegnet for å se undermappene.↩︎\nNår du skal synkronisere fra Dapla til en undermappe i prodsonen, så må mappen i prodsonen allerede eksisterere. Hvis den ikke gjør det vil jobben feile. Ved synkronsiering fra prodsonen til Dapla trenger ikke undermappen eksistere, siden bøtter egentlig ikke har undermapper og filstien fra prodsonen bare blir til filnavnet i bøtta.↩︎"
  },
  {
    "objectID": "statistikkere/appendix.html",
    "href": "statistikkere/appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "Appendix"
  },
  {
    "objectID": "statistikkere/metadata-overview.html",
    "href": "statistikkere/metadata-overview.html",
    "title": "Metadata",
    "section": "",
    "text": "Metadata"
  },
  {
    "objectID": "statistikkere/ordforklaringer.html",
    "href": "statistikkere/ordforklaringer.html",
    "title": "Ordforklaringer",
    "section": "",
    "text": "Ordforklaringer\n\nbip\nbip er det tidligere navnet på den underliggende plattformen som SSB bygger i GCP, hovedsakelig ment for utviklere som bygger tjenester på Dapla. Plattformen skulle være selvbetjent for utviklere og basert på DevOps-prinsipper. bip eksisterer fortsatt, men er nå blitt en del av det større begrepet dapla.\n\n\nbucket\nbucket (eller bøtte på norsk) er en lagringsenhet på Dapla. Det ligner litt på en klassisk diskstasjon, for eksempel X-disken eller C-disken på en lokal maskin. I en bøtte kan det ligge undermapper slik som i et klassisk filsystem.\n\n\nconsumer\nconsumer er en AD-gruppe som gir tilgang til et Dapla-team sin delt-bøtte. En SSB-ansatt som skal bruke data fra et Dapla-team må være medlem av consumer-gruppen til det aktuelle Dapla-teamet.\n\n\ndapla\nDapla er et akronym for den nye dataplattformen til SSB, der Da står for Data og pla står for Plattform. Dapla er en plattform for lagring, prosessering og deling av SSB sine data. Den består både av Jupyter-miljøet, som er et verktøy for å utføre beregninger og analysere data, og et eget område for lagre data. I tillegg inkluderer begrepet Dapla også en rekke andre verktøy som er nødvendige for å kunne bruke plattformen.\n\n\ndapla-team\nKommer snart.\n\n\ndapla-toolbelt\nKommer snart.\n\n\ndata-admin\ndata-admin er en AD-gruppe som gir de videste tilgangene i et dapla-team. En SSB-ansatt som har data-admin-rollen i et Dapla-team har tilgang til alle bøtter for det teamet, inkludert kilde-bøtta som kan inneha sensitive data.\nKommer snart.\n\n\ndapla-start\n*dapla-start** er et brukergrensesnitt der SSB-ansatte kan søke om å få opprettet et nytt dapla-team.\n\n\ndelt-bøtte\nKommer snart.\n\n\ndeveloper\nKommer snart.\n\n\nPersonidentifiserende Informasjon (PII)\nPII er variabler som kan identifisere en person i et datasett.\nMer informasjon finnes hos Datatilsynet.\n\n\ngoogle cloud platform (gcp)\nAllmenn skyplattform utviklet og levert av Google. Konkurrent med Amazon Web Services (AWS) og Microsoft Azure. Dapla primært benytter seg av tjenester på GCP.\nVideo som forklarer hva GCP er.\n\n\ngcp\nForkortelse for Google Cloud Platform. Se forklaring under google cloud platform (GCP).\n\n\nInfrastructure as Code (IaC)\nInfrastuktur som kode på norsk. Kode som defineres ressurser, typisk på en allmenn skyplatform som GCP. Eksempler av ressurser er bøtter, databaser, virtuelle maskiner, nettverk og sikkerhetsregler.\n\n\nkilde-bøtte\nKommer snart.\n\n\nprodukt-bøtte\nKommer snart.\n\n\nPull Request (PR)\nEn PR er en Github konsept, som gir et forum for kodegjennomgang, diskusjon og ikke minst dokumentasjon av kodeendringer.\nDette er anbefalt av KVAKK som måten å endre kode på i SSB.\n\n\nssb-project\nKommer snart.\n\n\ntransfer service\nKommer snart.\n\n\nPyflakes\nPyflakes er et enkelt kodeanalyseverktøy som finner feil i Python kode. Les mer om Pyflakes på deres PyPi side"
  },
  {
    "objectID": "statistikkere/kartdata.html",
    "href": "statistikkere/kartdata.html",
    "title": "Kartdata",
    "section": "",
    "text": "Det er tilrettelagt kartdata i Dapla til analyse, statistikkproduksjon og visualisering. Det er de samme dataene som er i GEODB i vanlig produksjonssone. Kartdataene er lagret på ssb-prod-kart-data-delt. De er lagret som parquetfiler i standardprojeksjonen vi bruker i SSB (UTM sone 33N) hvis ikke annet er angitt. Det er også SSBs standard-rutenett i ulike størrelser samt Eurostats rutenett over Norge.\nMan søker om tilgang til dataene til kundeservice (tilgangsrollen kart-consumers), men bruk gjerne standard LDA-prosedyre som for øvrige data.\nI tillegg ligger det noe testdata i fellesbøtta her: ssb-dapla-felles-data-delt-prod/GIS/testdata\n\n\n\n\nGeopandas er et Python-bibliotek som integrerer Pandas med GIS-analysering og kartlegging. Geopandas er bygd opp rundt GeoDataFrames, som er en pandas DataFrame med geografisk informasjon. Disse kan lastes inn direkte hvis man har geografiske filer, eller man kan konvertere fra DataFrames med koordinat-kolonner (se eksempel under).\nGeopandas kan blant annet brukes til å kartlegge dataene, beregne avstander og labe variabler for nærmiljø ved å koble datasett sammen basert på geografisk overlapp.\nFor lesing og skriving av geodata i Dapla, kan man bruke SSB-pakken sgis. Med denne kan man blant annet også beregne reisetider langs veg og lage publiseringsverdige kart.\nPakkene kan installeres i Dapla sånn her:\n\n\nterminal\n\npoetry add geopandas\npoetry add ssb-sgis\n\nOg så importeres det i Python på vanlig vis.\n\n\nnotebook\n\nimport geopandas as gpd\nimport sgis as sg\n\nEksempel på lesing/skriving av kartdata\nMan kan lese/skrive geodata i Dapla med sgis-funksjonene read_geopandas og write_geopandas. Støttede formater er blant annet (geo)parquet, geojson og gml. En rekke kartdata for hele landet til analyse- og visualiseringsformål ligger i bøtta “kart”. For eksempel kommuneflater til analyse eller statistikk:\n\n\nnotebook\n\nfilsti = \"ssb-prod-kart-data-delt/kartdata_analyse/klargjorte-data/2023/ABAS_kommune_flate_p2023_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\n\nEksempel på lesing av kommuneflater for visualisering. Disse dataene er generalisert slik at de egner seg for å lage kart, men blir unøyaktige til statistikk:\n\n\nnotebook\n\nfilsti = \"ssb-prod-kart-data-delt/kartdata_visualisering/klargjorte-data/2023/parquet/N5000_kommune_flate_p2023_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\n\nTilsvarende for skriving til parquet eller annet geodataformat:\n\n\nnotebook\n\nsg.write_geopandas(kommuner, filsti_ny) \n\n\n\n\nI tillegg til kartdataene, ligger det testdata i fellesbøtta som man kan leke seg med. Testdataene finnes for Oslo og Kongsvinger, og kan leses sånn her:\n\n\nnotebook\n\ntestdatasti = \"ssb-dapla-felles-data-delt-prod/GIS/testdata\"\nveger = sg.read_geopandas(f\"{testdatasti}/veger_oslo.parquet\")\n\nUnder følger noen eksempler på GIS-prosessering med testdataene.\nEksempel på avstandsberegning\nAvstand i luftlinje finner man enkelt med sjoin_nearest. Sånn her finner man avstand fra boligbygg (1000 tilfeldige) i Oslo til nærmeste butikkbygg.\n\n\nnotebook\n\nboligbygg = sg.read_geopandas(f\"{testdatasti}/noen_boligbygg_oslo.parquet\")\nbutikkbygg = sg.read_geopandas(f\"{testdatasti}/butikkbygg_oslo.parquet\")\n\nboligbygg_med_butikkavstand = boligbygg.sjoin_nearest(butikkbygg, distance_col=\"meter_til_butikk\")\n\nFor å finne avstand eller reisetid langs veier, kan man gjøre nettverksanalyse med sgis. Man må først klargjøre vegnettet og bestemme regler for beregningen(e):\n\n\nnotebook\n\nveger_klargjort = (\n    sg.get_connected_components(veger)\n    .query(\"connected == 1\")\n    .pipe(sg.make_directed_network_norway)\n)\n\nregler = sg.NetworkAnalysisRules(directed=True, weight=\"minutes\")\n\nnwa = sg.NetworkAnalysis(network=veger_klargjort, rules=regler)\n\nSå kan man beregne reisetider fra boligbygg til butikkbygg:\n\n\nnotebook\n\navstander = nwa.od_cost_matrix(boligbygg, butikkbygg)\n\nKorteste reisetid per bolig kan kobles på som kolonne i boligdataene sånn her:\n\n\nnotebook\n\nboligbygg[\"minutter_til_butikk\"] = avstander.groupby(\"origin\")[\"minutes\"].min()\n\nUndersøk resultatene i interaktivt kart:\n\n\nnotebook\n\nsg.explore(boligbygg, butikkbygg, \"minutter_til_butikk\")\n\nSe mer informasjon og eksempler i dokumentasjonen.\nOg flere nettverksanalyse-eksempler her.\nEksempel på geografisk kobling\nDatasett kan kobles basert på geografisk overlapp med geopandas.sjoin. Denne fungerer akkurat som pandas.merge, bare at det kobles basert på geometrien.\nKodesnutten under returnerer én kommuneflate for hvert bygg som overlapper, med kolonner fra begge datasettene. Inner-join er default, men med left-join beholder man også geometriene som ikke overlapper.\n\n\nnotebook\n\nkommuner = sg.read_geopandas(f\"{testdatasti}/enkle_kommuner.parquet\")\nkommuner_med_boliginfo = kommuner.sjoin(boligbygg, how=\"left\")\n\nMed motsatt rekkefølge, får man ett boligbygg per kommune den overlapper med. Siden byggene er punkter, vil hvert bygg havne i maksimalt én kommune.\n\n\nnotebook\n\nboligbygg_med_kommuneinfo = boligbygg.sjoin(kommuner, how=\"left\")\n\nEksempel på å lage kart\nMan kan lage tematiske kart med sgis-klassen ThematicMap. Her er et enkelt eksempel på et kart over arealet i kommuner.\n\n\nnotebook\n\nkommuner[\"Kvadratmeter\"] = kommuner.area\n\nm = sg.ThematicMap(kommuner, column=\"Kvadratmeter\", size=15)\nm.title = \"Areal i kommunene\"\nm.plot()\n\nSe flere kart-eksempler her.\nKonvertere DataFrame til GeoDataFrame\nEn pandas.DataFrame med koordinat-kolonner, kan konverteres til GeoDataFrame sånn her:\n\n\nnotebook\n\nimport pandas as pd\ndf = pd.DataFrame({\"x\": [10.8, 10.7, 10.9], \"y\": [59.9, 60, 59.85]})\n\ngdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.x, df.y), crs=4326)\n\nFor å beregne avtand i meter og kunne koble med annen geodata i Dapla, må man ha UTM-koordinater (hvis man ikke hadde det fra før):\n\n\nnotebook\n\ngdf = gdf.to_crs(25833)\n\nSe også geopandas dokumentasjon for mer utfyllende informasjon.\n\n\n\n\nDen viktigste GIS-pakken i R er sf. Pakken er bygd opp rundt sf-objekter, som er data.frames med geografisk informasjon. Man kan gjøre standard tidyverse-opersjoner på sf-objektene, pluss GIS-operasjoner.\nSindre Mikael Haugen har lagd funksjoner for å lese og skrive blant annet geodata i Dapla. For å få geodata, setter man parametret ‘sf’ til TRUE:\n\n\nnotebook\n\nlibrary(fellesr)\n\nveger = read_SSB(\"ssb-dapla-felles-data-delt-prod/GIS/testdata/veger_oslo.parquet\", sf = TRUE)\n\nHaugen har også lagd en pakke for å gjøre nettverksanalyse, som også lar deg geokode adresser, altså å finne adressenes koordinater.\n\n\nnotebook\n\nlibrary(GISSB)\n\nLite eksempel på kobling basert geografisk sammenfall (enkle kommuner er egentlig til visualisering). Her får man ett bygg per kommune som overlapper (som maksimalt er én kommune siden dette er bygningspunkter):\n\n\nnotebook\n\nlibrary(sf)\n\nboligbygg = read_SSB(\"ssb-dapla-felles-data-delt-prod/GIS/testdata/noen_boligbygg_oslo.parquet\", sf = TRUE)\nenkle_kommuner = read_SSB(\"ssb-dapla-felles-data-delt-prod/GIS/testdata/enkle_kommuner.parquet\", sf = TRUE)\n\nboligbygg_med_kommuneinfo = st_join(boligbygg, enkle_kommuner)\n\nMed motsatt rekkefølge, får man én kommuneflate per bolig som overlapper:\n\n\nnotebook\n\nkommuner_med_boliginfo = st_join(enkle_kommuner, boligbygg)"
  },
  {
    "objectID": "statistikkere/kartdata.html#python",
    "href": "statistikkere/kartdata.html#python",
    "title": "Kartdata",
    "section": "",
    "text": "Geopandas er et Python-bibliotek som integrerer Pandas med GIS-analysering og kartlegging. Geopandas er bygd opp rundt GeoDataFrames, som er en pandas DataFrame med geografisk informasjon. Disse kan lastes inn direkte hvis man har geografiske filer, eller man kan konvertere fra DataFrames med koordinat-kolonner (se eksempel under).\nGeopandas kan blant annet brukes til å kartlegge dataene, beregne avstander og labe variabler for nærmiljø ved å koble datasett sammen basert på geografisk overlapp.\nFor lesing og skriving av geodata i Dapla, kan man bruke SSB-pakken sgis. Med denne kan man blant annet også beregne reisetider langs veg og lage publiseringsverdige kart.\nPakkene kan installeres i Dapla sånn her:\n\n\nterminal\n\npoetry add geopandas\npoetry add ssb-sgis\n\nOg så importeres det i Python på vanlig vis.\n\n\nnotebook\n\nimport geopandas as gpd\nimport sgis as sg\n\nEksempel på lesing/skriving av kartdata\nMan kan lese/skrive geodata i Dapla med sgis-funksjonene read_geopandas og write_geopandas. Støttede formater er blant annet (geo)parquet, geojson og gml. En rekke kartdata for hele landet til analyse- og visualiseringsformål ligger i bøtta “kart”. For eksempel kommuneflater til analyse eller statistikk:\n\n\nnotebook\n\nfilsti = \"ssb-prod-kart-data-delt/kartdata_analyse/klargjorte-data/2023/ABAS_kommune_flate_p2023_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\n\nEksempel på lesing av kommuneflater for visualisering. Disse dataene er generalisert slik at de egner seg for å lage kart, men blir unøyaktige til statistikk:\n\n\nnotebook\n\nfilsti = \"ssb-prod-kart-data-delt/kartdata_visualisering/klargjorte-data/2023/parquet/N5000_kommune_flate_p2023_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\n\nTilsvarende for skriving til parquet eller annet geodataformat:\n\n\nnotebook\n\nsg.write_geopandas(kommuner, filsti_ny) \n\n\n\n\nI tillegg til kartdataene, ligger det testdata i fellesbøtta som man kan leke seg med. Testdataene finnes for Oslo og Kongsvinger, og kan leses sånn her:\n\n\nnotebook\n\ntestdatasti = \"ssb-dapla-felles-data-delt-prod/GIS/testdata\"\nveger = sg.read_geopandas(f\"{testdatasti}/veger_oslo.parquet\")\n\nUnder følger noen eksempler på GIS-prosessering med testdataene.\nEksempel på avstandsberegning\nAvstand i luftlinje finner man enkelt med sjoin_nearest. Sånn her finner man avstand fra boligbygg (1000 tilfeldige) i Oslo til nærmeste butikkbygg.\n\n\nnotebook\n\nboligbygg = sg.read_geopandas(f\"{testdatasti}/noen_boligbygg_oslo.parquet\")\nbutikkbygg = sg.read_geopandas(f\"{testdatasti}/butikkbygg_oslo.parquet\")\n\nboligbygg_med_butikkavstand = boligbygg.sjoin_nearest(butikkbygg, distance_col=\"meter_til_butikk\")\n\nFor å finne avstand eller reisetid langs veier, kan man gjøre nettverksanalyse med sgis. Man må først klargjøre vegnettet og bestemme regler for beregningen(e):\n\n\nnotebook\n\nveger_klargjort = (\n    sg.get_connected_components(veger)\n    .query(\"connected == 1\")\n    .pipe(sg.make_directed_network_norway)\n)\n\nregler = sg.NetworkAnalysisRules(directed=True, weight=\"minutes\")\n\nnwa = sg.NetworkAnalysis(network=veger_klargjort, rules=regler)\n\nSå kan man beregne reisetider fra boligbygg til butikkbygg:\n\n\nnotebook\n\navstander = nwa.od_cost_matrix(boligbygg, butikkbygg)\n\nKorteste reisetid per bolig kan kobles på som kolonne i boligdataene sånn her:\n\n\nnotebook\n\nboligbygg[\"minutter_til_butikk\"] = avstander.groupby(\"origin\")[\"minutes\"].min()\n\nUndersøk resultatene i interaktivt kart:\n\n\nnotebook\n\nsg.explore(boligbygg, butikkbygg, \"minutter_til_butikk\")\n\nSe mer informasjon og eksempler i dokumentasjonen.\nOg flere nettverksanalyse-eksempler her.\nEksempel på geografisk kobling\nDatasett kan kobles basert på geografisk overlapp med geopandas.sjoin. Denne fungerer akkurat som pandas.merge, bare at det kobles basert på geometrien.\nKodesnutten under returnerer én kommuneflate for hvert bygg som overlapper, med kolonner fra begge datasettene. Inner-join er default, men med left-join beholder man også geometriene som ikke overlapper.\n\n\nnotebook\n\nkommuner = sg.read_geopandas(f\"{testdatasti}/enkle_kommuner.parquet\")\nkommuner_med_boliginfo = kommuner.sjoin(boligbygg, how=\"left\")\n\nMed motsatt rekkefølge, får man ett boligbygg per kommune den overlapper med. Siden byggene er punkter, vil hvert bygg havne i maksimalt én kommune.\n\n\nnotebook\n\nboligbygg_med_kommuneinfo = boligbygg.sjoin(kommuner, how=\"left\")\n\nEksempel på å lage kart\nMan kan lage tematiske kart med sgis-klassen ThematicMap. Her er et enkelt eksempel på et kart over arealet i kommuner.\n\n\nnotebook\n\nkommuner[\"Kvadratmeter\"] = kommuner.area\n\nm = sg.ThematicMap(kommuner, column=\"Kvadratmeter\", size=15)\nm.title = \"Areal i kommunene\"\nm.plot()\n\nSe flere kart-eksempler her.\nKonvertere DataFrame til GeoDataFrame\nEn pandas.DataFrame med koordinat-kolonner, kan konverteres til GeoDataFrame sånn her:\n\n\nnotebook\n\nimport pandas as pd\ndf = pd.DataFrame({\"x\": [10.8, 10.7, 10.9], \"y\": [59.9, 60, 59.85]})\n\ngdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.x, df.y), crs=4326)\n\nFor å beregne avtand i meter og kunne koble med annen geodata i Dapla, må man ha UTM-koordinater (hvis man ikke hadde det fra før):\n\n\nnotebook\n\ngdf = gdf.to_crs(25833)\n\nSe også geopandas dokumentasjon for mer utfyllende informasjon."
  },
  {
    "objectID": "statistikkere/kartdata.html#r",
    "href": "statistikkere/kartdata.html#r",
    "title": "Kartdata",
    "section": "",
    "text": "Den viktigste GIS-pakken i R er sf. Pakken er bygd opp rundt sf-objekter, som er data.frames med geografisk informasjon. Man kan gjøre standard tidyverse-opersjoner på sf-objektene, pluss GIS-operasjoner.\nSindre Mikael Haugen har lagd funksjoner for å lese og skrive blant annet geodata i Dapla. For å få geodata, setter man parametret ‘sf’ til TRUE:\n\n\nnotebook\n\nlibrary(fellesr)\n\nveger = read_SSB(\"ssb-dapla-felles-data-delt-prod/GIS/testdata/veger_oslo.parquet\", sf = TRUE)\n\nHaugen har også lagd en pakke for å gjøre nettverksanalyse, som også lar deg geokode adresser, altså å finne adressenes koordinater.\n\n\nnotebook\n\nlibrary(GISSB)\n\nLite eksempel på kobling basert geografisk sammenfall (enkle kommuner er egentlig til visualisering). Her får man ett bygg per kommune som overlapper (som maksimalt er én kommune siden dette er bygningspunkter):\n\n\nnotebook\n\nlibrary(sf)\n\nboligbygg = read_SSB(\"ssb-dapla-felles-data-delt-prod/GIS/testdata/noen_boligbygg_oslo.parquet\", sf = TRUE)\nenkle_kommuner = read_SSB(\"ssb-dapla-felles-data-delt-prod/GIS/testdata/enkle_kommuner.parquet\", sf = TRUE)\n\nboligbygg_med_kommuneinfo = st_join(boligbygg, enkle_kommuner)\n\nMed motsatt rekkefølge, får man én kommuneflate per bolig som overlapper:\n\n\nnotebook\n\nkommuner_med_boliginfo = st_join(enkle_kommuner, boligbygg)"
  },
  {
    "objectID": "statistikkere/contribution.html",
    "href": "statistikkere/contribution.html",
    "title": "Bidra til Dapla-manualen",
    "section": "",
    "text": "Alle i SSB kan bidra til denne manualen. Endringer må godkjennes av noen i Team Statistikktjenester, si gjerne i fra at det ligger en PR å se på.\n\n\n\n\n\n\nWarning\n\n\n\nDenne nettsiden er åpen og hvem som helst kan lese det som er skrevet her. Hold det i tankene når du skriver.\n\n\n\n\n\nMan trenger basis git kompetanse, det ligger en fin beskrivelse av det på Beste Praksis siden fra KVAKK.\nMan trenger en konto på Github, det kan man opprette ved å følge instruksjonene her.\nMan kan lære seg Markdown (enkel tekst formatering), en fin oversikt finnes her.\nVerktøyet Quarto burde installeres for å kunne se endringene slik som de ser ut på nettsiden. Installasjon instruksjoner finnes her.\n\n\n\n\n\nKlone repoet\n\n\n\nterminal\n\ngit clone https://github.com/statisticsnorway/dapla-manual.git\n\n\nLage en ny gren\nGjøre endringen\nKjør følgende og følge lenken for å sjekke at alt ser bra ut på nettsiden:\n\n\n\nterminal\n\nquarto preview dapla-manual\n\n\nÅpne en PR\nBe noen å gjennomgå endringen og eventuelt godkjenne den\nMerge PRen\nEtter et par minutter skal din endring være synlig!\n\n\n\nQuarto tilbyr å legge ved (embed) notebooks inn i nettsiden. Dette er en fin måte å dele kode og output på. Men det krever at vi tenker gjennom hvor output`en genereres. Siden Dapla-manualen renderes med GitHub-action, så ønsker vi ikke å introdusere kompleksiteten det innebærer å generere output fra kode her. I tillegg er det mange miljø-spesifikke konfigurasjoner som bestemmer hvordan output kan genereres. Derfor anbefaler vi følgende tilnærming når man skal legge ved en notebook til et kapittel:\n\nLogg deg inn i miljøet du ønsker å bruke.\nKlone ned repoet til Dapla-manualen.\nOpprett en notebook i mappen ./dapla-manual/notebooks/notebook-navn.ipynb\nSkriv kode og generer output som du ønsker. Husk å bare bruke data som alle har tilgang til eller de kan generere selv.\nEmbed output i kapitlet du ønsker iht til denne beskrivelsen\nPå toppen av notebooken lager du en raw-celle der du skriver:\n\n\n\nnotebook\n\n---\nfreeze: true\n---\n\n\nKjør denne notebooken fra terminalen med kommandoen:\n\n\n\nterminal\n\nquarto render ./notebooks/&lt;notebook-navn.ipynb&gt; --execute\n\nDette sikrer at vi kan se output etter at boken bygges i GitHub Actions. Husk at notebooks ikke blir strippet for output på vanlig måte, slik at kun åpne data skal benyttes.\nSpør Team Statistikktjenester om du lurer på noe."
  },
  {
    "objectID": "statistikkere/contribution.html#forutsetninger",
    "href": "statistikkere/contribution.html#forutsetninger",
    "title": "Bidra til Dapla-manualen",
    "section": "",
    "text": "Man trenger basis git kompetanse, det ligger en fin beskrivelse av det på Beste Praksis siden fra KVAKK.\nMan trenger en konto på Github, det kan man opprette ved å følge instruksjonene her.\nMan kan lære seg Markdown (enkel tekst formatering), en fin oversikt finnes her.\nVerktøyet Quarto burde installeres for å kunne se endringene slik som de ser ut på nettsiden. Installasjon instruksjoner finnes her."
  },
  {
    "objectID": "statistikkere/contribution.html#fremgangsmåten",
    "href": "statistikkere/contribution.html#fremgangsmåten",
    "title": "Bidra til Dapla-manualen",
    "section": "",
    "text": "Klone repoet\n\n\n\nterminal\n\ngit clone https://github.com/statisticsnorway/dapla-manual.git\n\n\nLage en ny gren\nGjøre endringen\nKjør følgende og følge lenken for å sjekke at alt ser bra ut på nettsiden:\n\n\n\nterminal\n\nquarto preview dapla-manual\n\n\nÅpne en PR\nBe noen å gjennomgå endringen og eventuelt godkjenne den\nMerge PRen\nEtter et par minutter skal din endring være synlig!\n\n\n\nQuarto tilbyr å legge ved (embed) notebooks inn i nettsiden. Dette er en fin måte å dele kode og output på. Men det krever at vi tenker gjennom hvor output`en genereres. Siden Dapla-manualen renderes med GitHub-action, så ønsker vi ikke å introdusere kompleksiteten det innebærer å generere output fra kode her. I tillegg er det mange miljø-spesifikke konfigurasjoner som bestemmer hvordan output kan genereres. Derfor anbefaler vi følgende tilnærming når man skal legge ved en notebook til et kapittel:\n\nLogg deg inn i miljøet du ønsker å bruke.\nKlone ned repoet til Dapla-manualen.\nOpprett en notebook i mappen ./dapla-manual/notebooks/notebook-navn.ipynb\nSkriv kode og generer output som du ønsker. Husk å bare bruke data som alle har tilgang til eller de kan generere selv.\nEmbed output i kapitlet du ønsker iht til denne beskrivelsen\nPå toppen av notebooken lager du en raw-celle der du skriver:\n\n\n\nnotebook\n\n---\nfreeze: true\n---\n\n\nKjør denne notebooken fra terminalen med kommandoen:\n\n\n\nterminal\n\nquarto render ./notebooks/&lt;notebook-navn.ipynb&gt; --execute\n\nDette sikrer at vi kan se output etter at boken bygges i GitHub Actions. Husk at notebooks ikke blir strippet for output på vanlig måte, slik at kun åpne data skal benyttes.\nSpør Team Statistikktjenester om du lurer på noe."
  },
  {
    "objectID": "statistikkere/test.html",
    "href": "statistikkere/test.html",
    "title": "Notebooks",
    "section": "",
    "text": "Notebooks\nsdløkfjlksdjfkl"
  },
  {
    "objectID": "statistikkere/kode-overview.html",
    "href": "statistikkere/kode-overview.html",
    "title": "Kode",
    "section": "",
    "text": "Kode"
  },
  {
    "objectID": "statistikkere/jupyterlab.html",
    "href": "statistikkere/jupyterlab.html",
    "title": "Jupyterlab",
    "section": "",
    "text": "Mer kommer.\n\n\n\nMå nevne operativsystemet og at noe programvare ligger installert her (git, jwsacruncher, quarto, ++)\n\n\n\nNoe er i base-image, noe bør gjøres i virtuelle miløer. Hvordan liste ut pakker som er pre-installert?\n\n\n\nJupyterlab er en samling extension. Kan bare installeres av admin. Sikkerhet. Hvilke extension har vi tilgjengeliggjort?\n\n\n\nSane defaults for Jupyterlab."
  },
  {
    "objectID": "statistikkere/jupyterlab.html#hva-er-jupyterlab",
    "href": "statistikkere/jupyterlab.html#hva-er-jupyterlab",
    "title": "Jupyterlab",
    "section": "",
    "text": "Mer kommer."
  },
  {
    "objectID": "statistikkere/jupyterlab.html#terminalen",
    "href": "statistikkere/jupyterlab.html#terminalen",
    "title": "Jupyterlab",
    "section": "",
    "text": "Må nevne operativsystemet og at noe programvare ligger installert her (git, jwsacruncher, quarto, ++)"
  },
  {
    "objectID": "statistikkere/jupyterlab.html#pakkeinstallasjoner",
    "href": "statistikkere/jupyterlab.html#pakkeinstallasjoner",
    "title": "Jupyterlab",
    "section": "",
    "text": "Noe er i base-image, noe bør gjøres i virtuelle miløer. Hvordan liste ut pakker som er pre-installert?"
  },
  {
    "objectID": "statistikkere/jupyterlab.html#extensions",
    "href": "statistikkere/jupyterlab.html#extensions",
    "title": "Jupyterlab",
    "section": "",
    "text": "Jupyterlab er en samling extension. Kan bare installeres av admin. Sikkerhet. Hvilke extension har vi tilgjengeliggjort?"
  },
  {
    "objectID": "statistikkere/jupyterlab.html#tips-triks",
    "href": "statistikkere/jupyterlab.html#tips-triks",
    "title": "Jupyterlab",
    "section": "",
    "text": "Sane defaults for Jupyterlab."
  },
  {
    "objectID": "statistikkere/kildedata.html",
    "href": "statistikkere/kildedata.html",
    "title": "Kildedata",
    "section": "",
    "text": "Kildedata er data lagret slik de ble levert til SSB fra dataeier, det vil si på dataeiers dataformat og med informasjon om tidspunkt og rekkefølge for avlevering. Kildedata er en del av statistikkenes dokumentasjon, og kan være en nødvendig kilde for forskning og nye statistikker. Uten kildedataene vil det ikke være mulig å etterprøve SSB sine statistikker. De originale kildedataene vil ofte komprimeres og krypteres etter at relevante deler er transformert til inndata.\n\n(Standardutvalget 2021, 7)\n\nStatistikkloven § 9 Informasjonssikkerhet stiller krav om at direkte identifiserende opplysninger skal behandles og lagres adskilt fra øvrige opplysninger, med mindre det vil være uforenlig med formålet med behandlingen eller åpenbart unødvendig. I henhold til policy om Datatilstander er kildedata i utgangspunktet den eneste datatilstanden som kan inneholde denne type data. I øvrige tilstander skal direkteidentifiserende opplysninger som hovedregel være pseudonymisert. Avvik skal dokumenteres og godkjennes av seksjonsleder som er ansvarlig for avviket.\n\n(Direktørmøtet 2022, 2)\nFordi Kildedata kan inneholde PII1 implementerer Dapla følgende tiltak:\n\nKildedata er lagret adskilt fra andre datatilstander.\nTilgang til dataene begrenses så langt som mulig, kun en begrenset gruppe personer2 har tilgang til kildedata.\nProsessering av kildedata utføres automatisk for minske behov for tilgang til dataene."
  },
  {
    "objectID": "statistikkere/kildedata.html#footnotes",
    "href": "statistikkere/kildedata.html#footnotes",
    "title": "Kildedata",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPersonidentifiserende Informasjon↩︎\nData admins↩︎"
  },
  {
    "objectID": "statistikkere/standardisering.html",
    "href": "statistikkere/standardisering.html",
    "title": "Standardisering",
    "section": "",
    "text": "Standardisering"
  },
  {
    "objectID": "statistikkere/jobbe-med-data.html",
    "href": "statistikkere/jobbe-med-data.html",
    "title": "Jobbe med data",
    "section": "",
    "text": "Når vi oppretter et dapla-team så får vi tildelt et eget området for lagring av data. For å kunne lese og skrive data fra Jupyter til disse områdene må vi autentisere oss, siden Jupyter og lagringsområdet er to separate sikkerhetsoner.\nFigur 1 viser dette klarer skillet mellom hvor vi koder og hvor dataene ligger på Dapla1. I dette kapitlet beskriver vi nærmere hvordan du kan jobbe med dataene dine på Dapla.\n\n\n\nFigur 1: Tydelig skille mellom kodemiljø og datalagring på Dapla.\n\n\n\n\nFor å gjøre det enklere å jobbe data på tvers av Jupyter og lagringsområdet er det laget noen egne SSB-utviklede biblioteker for å gjøre vanlige operasjoner mot lagringsområdet. Siden både R og Python skal brukes på Dapla, så er det laget to biblioteker, en for hver av disse språkene. fellesr er biblioteket for R, og dapla-toolbelt er biblioteket for Python.\n\n\ndapla-toolbelt er en en pakke som lar deg enkelt lese og skrive til lagringsområdet uten å måtte autentifisere deg manuelt. Den har en Pandas-aktig syntaks som forhåpentlig er gjenkjennbar for de fleste. Pakken er installert i alle Python-kernels på Dapla, så du trenger ikke å installere den selv hvis du åpner en notebook med Python3 for eksempel. For å importere hele biblioteket i en notebook skriver du bare\n\n\nnotebook\n\nimport dapla as dp\n\ndapla-toolbelt bruker en pakke som heter gcsfs for å kommunisere med lagringsområdet. gcsfs er en pakke som lar deg bruke Google Cloud Storage (GCS) som om det var en filsystem. Det betyr at du kan bruke samme syntaks som du bruker for å lese og skrive til filer på din egen maskin. Du kan lese mulighetene i gcsfs her. Et eksempel på hvordan de to pakkene kan brukes sammen ser du her:\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Example of how you can use gcsfs and dapla-toolbelt together\nfs.touch(\"gs://my-bucket/my-folder/\")\n\nI koden over brukte jeg kommandoen touch fra gcsfs og FileClient fra dapla-toolbelt for å opprette en mappe i lagringsområdet.\nI kapitlene under finner du konkrete eksempler på hvordan du kan bruke dapla-toolbelt til å jobbe med data i SSBs lagringsområdet.\n\n\n\nR-pakken fellesr er under utvikling og gir mye av den samme funksjonaliteten som dapla-toolbelt gir for Python. I tillegg til å kunne lese og skrive til lagringsområdet på Dapla, så har fellesr også funksjoner for å jobbe med metadata på Dapla.\nfellesr er installert på Dapla og funksjoner kan benyttes ved:\n\n\nnotebook\n\nlibrary(fellesr)\n\nHvis du benytte en renv miljø, må pakken installeres en gang. Dette kan gjøres ved:\n\n\nnotebook\n\nrenv::install(\"statisticsnorway/fellesr\")\n\n\n\n\n\nI denne delen viser vi hvordan man gjør veldig vanlige operasjoner når man koder et produksonsløp for en statistikk. Flere eksempler på nyttige systemkommandoer finner du her.\n\n\n\n\n\n\n\n\nEksempeldata\n\n\n\nDet finnes et område som alle SSB-ansatte har lese- og skrivetilgang til. Det er\ngs://ssb-dapla-felles-data-delt-prod/ i prod-miljøet på Dapla, og\ngs://ssb-dapla-felles-data-delt-test/ i staging-miljøet. Eksemplene under bruker førstnevnte i koden, slik at alle kan kjøre koden selv.\nKode-eksemplene finnes for både R og Python, og du kan velge hvilken du skal se ved å trykke på den arkfanen du er interessert i.\n\n\nÅ liste ut innhold i et gitt mappe på Dapla er ganske enkelt. Under ser du hvordan du kan liste ut innholdet i følgende mappe:\ngs://ssb-dapla-felles-data-delt-prod/felles/veiledning/python/eksempler/purchases\n\nPython \n\n\nVi bruker modulen FileClient fra dapla-toolbelt for å liste ut innholdet i en mappe.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Set path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\nFileClient.ls(f\"{bucket}/{folder}\")\n\nMed kommandoen over får du listet ut alle filnavn i mappen. Hvis du vil ha mer informasjon om filene så kan du bruke ls-kommandoen med detail = True, som under:\n\n\nnotebook\n\nFileClient.ls(f\"{bucket}/{folder}\", detail = True)\n\nSyntaksen med ls er veldig lik det man kjenner fra en Linux-terminal. Men når vi bruker detail = True blir metadata om filene returnert som en Python-liste med dictionaries. Det kan være svært nyttig når du f.eks. trenger å vite dato og tidspunkt for når en fil ble opprettet, eller når den sist ble oppdatert.\n\n\n\n\nnotebook\n\n# Loading functions into notebook\nlibrary(fellesr)\n\n# Path to folder\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod/\"\nfolder &lt;- \"felles/veiledning/python/eksempler/purchases\"\n\n# List files in folder \nlist.files(paste0(bucket, folder))\n\nMerknad: Når du spesifisere bøtter i R, trenger du ikke “gs://” foran.\n\n\n\n\n\n\nÅ skrive filer til et lagringsområde på Dapla er også ganske enkelt. Det ligner mye på den syntaksen vi er kjent med fra vanlige R- og Python-pakker, med noen små unntak.\n\n\nUnder lager vi en dataframe i en notebook og skriver den ut til en parquet-fil.\n\nPython \n\n\nNår vi leser en Parquet-fil med dapla-toolbelt så bruker den pyarrow i bakgrunnen. Dette er en av de raskeste måtene å lese og skrive Parquet-filer på.\n\n\nnotebook\n\nimport dapla as dp\nimport pandas as pd\nimport numpy as np\n\n# Set path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Create pandas dataframe\npurchases = pd.DataFrame(np.random.randn(10, 5),\n                        columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\n# Write pandas dataframe as parquet to GCS bucket\ndp.write_pandas(df = purchases,\n                gcs_path = f\"{bucket}/{folder}/data.parquet\",\n                file_format = \"parquet\",)\n\nNår vi kalte write_pandas over så spesifiserte vi at filformatet skulle være parquet. Dette er default, så vi kunne også ha skrevet det slik:\n\n\nnotebook\n\ndp.write_pandas(df = purchases,\n                gcs_path = f\"{bucket}/{folder}/data.parquet\")\n\nMen for de andre filformatene må vi altså spesifisere dette.\n\n\nNår vi jobber med Parquet-fil i R, bruker vi pakken arrow. Dette er en del av fellesr pakken så du trenger kun å kalle inn dette. Pakken inneholder funksjonen write_SSB som kan brukes til å skrive data til bøtte på Dapla.\n\n\nnotebook\n\nlibrary(fellesr)\n\n# Set stien til hvor data skal lagres\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"felles/veiledning/r/eksempler/purchases\"\n\n# Lage en eksempel dataframe\npurchases = data.frame(A = runif(10), B= runif(10), C=runif(10))\n\n# Skrive data til bøttet som en parquet\nwrite_SSB(purchases, file.path(bucket, folder, \"purchases.parquet\"))\n\nMerknad: Når du spesifisere bøtter i R, trenger du ikke “gs://” foran.\n\n\n\n\n\n\nNoen ganger ønsker vi å lagre data i andre formatter slik som CSV, JSON og XML.\n\nPython \n\n\ndapla-toolbelt kan skrive ut json, csv og posisjonsfiler (fixed-width-files/fwf) til lagringsområdet. Måten den gjør det på er å bruke Pandas sine funksjoner read_json, read_csv, read_fwf under panseret. Dette kan være nyttig å vite for skjønne hvordan dapla-toolbelt håndterer ulike strukturer i (spesielt hierarkiske) tekstfiler. Under ser du hvordan du kan skrive ut en dataframe til en json-fil.\n\n\nnotebook\n\nimport numpy as np\nimport pandas as pd\nfrom dapla import FileClient\n\n# Set path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Create a dataframe with Pandas\ndf = pd.DataFrame(np.random.randn(10, 5), columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\n# Save dataframe as json with dapla-toolbelt\ndp.write_pandas(df = df,\n                gcs_path = f\"{bucket}/{folder}/test.json\",\n                file_format = \"json\")\n\nSom vi ser at syntaksen over så kunne vi skrevet ut til noe annet enn json ved å endre verdien i argumentet file_format.\n\n\nPakken fellesr kan også brukes til å skrive andre type filer, for eksempel csv, til bøtter. Dette gjøres med funksjonen write_SSB og spesifisere ønsket filtype i filnavn.\nFørst kaller vi biblioteket og lage noe test data ved:\n\n\nnotebook\n\nlibrary(fellesr)\n\n# Set stien til hvor data skal lagres\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"felles/veiledning/r/eksempler/purchases\"\n\n# Lage en eksempel dataframe\npurchases = data.frame(A = runif(10), B= runif(10), C=runif(10))\n\n# Skrive til csv\nwrite_SSB(purchases, file.path(bucket, folder, \"purchases.csv\")\n\n\n\n\n\n\n\nDet er ikke anbefalt å bruke xlsx-formatet, men her ser du hvordan det kan skrives ut. Mer kommer.\n\nPython \n\n\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import AuthClient\n\n# Henter token for å kunne lese fra Dapla\ntoken = AuthClient.fetch_google_credentials()\n\n# Filsti\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\ndf.to_excel(f\"{bucket}/{folder}/test.xlsx\",\n           storage_options={\"token\": token})\n\n\n\nKommer snart\n\n\n\n\n\n\n\nUnder finner du eksempler på hvordan du kan lese inn data til en Jupyter Notebooks på Dapla.\n\n\n\nPython \n\n\n\n\nnotebook\n\nimport dapla as dp\n\n# Set path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Read path into pandas dataframe \ndp.read_pandas(gcs_path= f\"{bucket}/{folder}/data.parquet\",\n               file_format = \"parquet\",\n               columns = None,)\n\nSom vi så med write_pandas så er file_format default satt til parquet, og default for columns = None, så vi kunne også ha skrevet det slik:\ndp.read_pandas(gcs_path= f\"{bucket}/{folder}/data.parquet\")\ncolumns-argumentet er en liste med kolonnenavn som vi ønsker å lese inn. Hvis vi ikke spesifiserer noen kolonner så vil alle kolonnene leses inn.\n\n\nPakken fellesr kan brukes til å lese inn data. Funksjonen read_SSB() kan lese inn filer i flere format inkluderende parquet.\nHer er et eksempel av å lese inn parquet fil “1987”.\n\n\nnotebook\n\nlibrary(fellesr)\n\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"R_smoke_test\"\n\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.parquet\"))\n\nVi kan også filtrere hvilke variabel vi ønsker å lese inn ved å spesifisere parameter col_select. For eksempel:\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.parquet\"),\n                    col_select = c(\"Year\", \"Month\"))\nInnlesning av parquet som er kartdata finner du her: Lese kartdata\n\n\n\n\n\n\nKommer mer snart. Python-koden under bygger på eksempelet over.\n\nPython \n\n\n\n\nnotebook\n\nimport dapla as dp\n\n# Path to write to\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Read in json-file from dapla-storage\ndf = dp.read_pandas(gcs_path = f\"{bucket}/{folder}/test3.json\",\n               file_format = \"json\")\n\n\n\nFunksjonen read_SSB() kan lese inn flere type av fil-format, slik som csv og json. Du trenger ikke å endre koden, kun spesifisere hele filnavn.\nFørst kaller vi inn biblioteket fellesr og spesifisere bøtte/mappen:\n\n\nnotebook\n\nlibrary(fellesr)\n\n# Filsti\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"R_smoke_test\"\n\n# Lese inn CSV-fil\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.csv\"))\n\nFor å lese inn en json-fil kan skrive følgende:\n\n\nnotebook\n\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.json\"))\n\n\n\n\n\n\n\n\nPython \n\n\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import AuthClient\n\n# Hent token\ntoken = AuthClient.fetch_google_credentials()\n\n# Les inn fil\ndf = pd.read_excel(\"gs://ssb-prod-arbmark-skjema-data-produkt/test_gcp.xlsx\",\n    storage_options={\"token\": token})\n\n\n\nKommer snart\n\n\n\n\n\n\nHer er et eksempel på hvordan man leser inn en sas7bdat-fil på Dapla som har blitt generert i prodsonen.\n\nPython \n\n\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nsti = \"gs://ssb-dapla-felles-data-delt-prod/felles/veiledning/sas/statbank_ledstill.sas7bdat\"\n\nwith fs.open(sti) as sas:\n    df = pd.read_sas(sas, format=\"sas7bdat\", encoding=\"latin1\")\n\n\n\nI produksjon sone (på bakken) kan R-pakken haven benyttes for å lese inn .sas7bdat filer. Dette er ikke implementert i fellesr enda for lesing fra Dapla bøtte.\nMer om dette kommer.\n\n\n\n\n\n\n\nÅ slette filer fra lagringsområdet kan gjøres på flere måter. I kapitlet om sletting av data viste vi hvordan man gjør det med pek-og-klikk i Google Cloud Console. Under ser du hvordan du kan slette filer med dapla-toolbelt og gcsfs eller fellesr.\n\nPython \n\n\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\nfs.rm(f\"{bucket}/{from_folder}/df.json\")\n\n\n\nFunksjonen gc_delete_object kan brukes til å slette data på lagringsområdet.\n\n\nnotebook\n\nlibrary(fellesr)\n\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"felles/veiledning/r/eksempler/purchases\"\n\ngcs_delete_object(file.path(bucket, folder, \"purchases.parquet\"))\n\n\n\n\n\n\n\nÅ kopiere filer mellom mapper på et Linux-filsystem innebærer som regel bruke cp-kommandoen. På Dapla er det ikke så mye forskjell. Vi bruker en ligende tilnærming nå vi skal kopiere mellom bøtter eller mapper på lagringsområdet til SSB. Under ser du hvordan du kan kopiere en fil fra en mappe til en annen.\n\nPython \n\n\nLa oss begynne med et eksempel der vi kopierer en fil fra en mappe til en annen i samme bøtte.\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Path to folders\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfrom_folder = \"felles/veiledning/python/eksempler/purchases\"\nto_folder = \"felles/veiledning/python/eksempler\"\n\n# Copy file\nfs.cp(f\"{bucket}/{from_folder}/data.parquet\",\n      f\"{bucket}/{to_folder}/data_copy.parquet\")\n\nDet også fungere for å kopiere filer mellom bøtter.\nEt annet scenario vi ofte vil støte på er at vi ønsker å kopiere en fil fra vårt Jupyter-filsystem til en mappe på lagringsområdet. Her kan vi bruke fs.put-metoden.\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Create a new file in your home directory called test.txt\nwith open('/home/jovyan/test.txt', 'w') as f:\n    f.write('Create a new text file!')\n\n#Path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler\"\n\n# Copy file from local to remote file system\nfs.put(lpath=f\"/home/jovyan/test.txt\", rpath=f\"{bucket}/{folder}/test.txt\")\n\nØnsker vi å kopiere en hel mappe fra lagringsområdet til Jupyter-filsystemet, kan vi bruke fs.get-metoden, med opsjonen recursive=True.\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Copy file\nfs.get(&lt;from_bucket&gt;,\n      \"/home/jovyan/sesongjustering/\",\n      recursive=True)\n\n\n\nKommer snart\n\n\n\n\n\n\n\nPython \n\n\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfrom_folder = \"felles/veiledning/python/eksempler/purchases\"\nto_folder = \"felles/veiledning/python/eksempler\"\n\nfs.mv(f\"{bucket}/{from_folder}/data.parquet\", f\"{bucket}/{to_folder}/data.parquet\")\n\n\n\nKommer snart\n\n\n\n\n\n\nSelv om bøtter ikke har mapper med en hierarkisk struktur slik man er kjent med fra klassike filsystemer, så kan man opprette det som ser ut som mapper i objektnavnet. I realiteten blir bare / oppfattet som en del av navnet på objektet. Skulle du likevel ønske å opprette dette så kan du gjøre det følgende måte:\n\nPython \n\n\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n#Path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler\"\n\n# Create folder\nfs.touch(f\"{bucket}/{folder}/testmappe/\")\n\n\n\nKommer snart"
  },
  {
    "objectID": "statistikkere/jobbe-med-data.html#ssb-biblioteker",
    "href": "statistikkere/jobbe-med-data.html#ssb-biblioteker",
    "title": "Jobbe med data",
    "section": "",
    "text": "For å gjøre det enklere å jobbe data på tvers av Jupyter og lagringsområdet er det laget noen egne SSB-utviklede biblioteker for å gjøre vanlige operasjoner mot lagringsområdet. Siden både R og Python skal brukes på Dapla, så er det laget to biblioteker, en for hver av disse språkene. fellesr er biblioteket for R, og dapla-toolbelt er biblioteket for Python.\n\n\ndapla-toolbelt er en en pakke som lar deg enkelt lese og skrive til lagringsområdet uten å måtte autentifisere deg manuelt. Den har en Pandas-aktig syntaks som forhåpentlig er gjenkjennbar for de fleste. Pakken er installert i alle Python-kernels på Dapla, så du trenger ikke å installere den selv hvis du åpner en notebook med Python3 for eksempel. For å importere hele biblioteket i en notebook skriver du bare\n\n\nnotebook\n\nimport dapla as dp\n\ndapla-toolbelt bruker en pakke som heter gcsfs for å kommunisere med lagringsområdet. gcsfs er en pakke som lar deg bruke Google Cloud Storage (GCS) som om det var en filsystem. Det betyr at du kan bruke samme syntaks som du bruker for å lese og skrive til filer på din egen maskin. Du kan lese mulighetene i gcsfs her. Et eksempel på hvordan de to pakkene kan brukes sammen ser du her:\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Example of how you can use gcsfs and dapla-toolbelt together\nfs.touch(\"gs://my-bucket/my-folder/\")\n\nI koden over brukte jeg kommandoen touch fra gcsfs og FileClient fra dapla-toolbelt for å opprette en mappe i lagringsområdet.\nI kapitlene under finner du konkrete eksempler på hvordan du kan bruke dapla-toolbelt til å jobbe med data i SSBs lagringsområdet.\n\n\n\nR-pakken fellesr er under utvikling og gir mye av den samme funksjonaliteten som dapla-toolbelt gir for Python. I tillegg til å kunne lese og skrive til lagringsområdet på Dapla, så har fellesr også funksjoner for å jobbe med metadata på Dapla.\nfellesr er installert på Dapla og funksjoner kan benyttes ved:\n\n\nnotebook\n\nlibrary(fellesr)\n\nHvis du benytte en renv miljø, må pakken installeres en gang. Dette kan gjøres ved:\n\n\nnotebook\n\nrenv::install(\"statisticsnorway/fellesr\")"
  },
  {
    "objectID": "statistikkere/jobbe-med-data.html#vanlige-operasjoner",
    "href": "statistikkere/jobbe-med-data.html#vanlige-operasjoner",
    "title": "Jobbe med data",
    "section": "",
    "text": "I denne delen viser vi hvordan man gjør veldig vanlige operasjoner når man koder et produksonsløp for en statistikk. Flere eksempler på nyttige systemkommandoer finner du her.\n\n\n\n\n\n\n\n\nEksempeldata\n\n\n\nDet finnes et område som alle SSB-ansatte har lese- og skrivetilgang til. Det er\ngs://ssb-dapla-felles-data-delt-prod/ i prod-miljøet på Dapla, og\ngs://ssb-dapla-felles-data-delt-test/ i staging-miljøet. Eksemplene under bruker førstnevnte i koden, slik at alle kan kjøre koden selv.\nKode-eksemplene finnes for både R og Python, og du kan velge hvilken du skal se ved å trykke på den arkfanen du er interessert i.\n\n\nÅ liste ut innhold i et gitt mappe på Dapla er ganske enkelt. Under ser du hvordan du kan liste ut innholdet i følgende mappe:\ngs://ssb-dapla-felles-data-delt-prod/felles/veiledning/python/eksempler/purchases\n\nPython \n\n\nVi bruker modulen FileClient fra dapla-toolbelt for å liste ut innholdet i en mappe.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Set path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\nFileClient.ls(f\"{bucket}/{folder}\")\n\nMed kommandoen over får du listet ut alle filnavn i mappen. Hvis du vil ha mer informasjon om filene så kan du bruke ls-kommandoen med detail = True, som under:\n\n\nnotebook\n\nFileClient.ls(f\"{bucket}/{folder}\", detail = True)\n\nSyntaksen med ls er veldig lik det man kjenner fra en Linux-terminal. Men når vi bruker detail = True blir metadata om filene returnert som en Python-liste med dictionaries. Det kan være svært nyttig når du f.eks. trenger å vite dato og tidspunkt for når en fil ble opprettet, eller når den sist ble oppdatert.\n\n\n\n\nnotebook\n\n# Loading functions into notebook\nlibrary(fellesr)\n\n# Path to folder\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod/\"\nfolder &lt;- \"felles/veiledning/python/eksempler/purchases\"\n\n# List files in folder \nlist.files(paste0(bucket, folder))\n\nMerknad: Når du spesifisere bøtter i R, trenger du ikke “gs://” foran.\n\n\n\n\n\n\nÅ skrive filer til et lagringsområde på Dapla er også ganske enkelt. Det ligner mye på den syntaksen vi er kjent med fra vanlige R- og Python-pakker, med noen små unntak.\n\n\nUnder lager vi en dataframe i en notebook og skriver den ut til en parquet-fil.\n\nPython \n\n\nNår vi leser en Parquet-fil med dapla-toolbelt så bruker den pyarrow i bakgrunnen. Dette er en av de raskeste måtene å lese og skrive Parquet-filer på.\n\n\nnotebook\n\nimport dapla as dp\nimport pandas as pd\nimport numpy as np\n\n# Set path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Create pandas dataframe\npurchases = pd.DataFrame(np.random.randn(10, 5),\n                        columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\n# Write pandas dataframe as parquet to GCS bucket\ndp.write_pandas(df = purchases,\n                gcs_path = f\"{bucket}/{folder}/data.parquet\",\n                file_format = \"parquet\",)\n\nNår vi kalte write_pandas over så spesifiserte vi at filformatet skulle være parquet. Dette er default, så vi kunne også ha skrevet det slik:\n\n\nnotebook\n\ndp.write_pandas(df = purchases,\n                gcs_path = f\"{bucket}/{folder}/data.parquet\")\n\nMen for de andre filformatene må vi altså spesifisere dette.\n\n\nNår vi jobber med Parquet-fil i R, bruker vi pakken arrow. Dette er en del av fellesr pakken så du trenger kun å kalle inn dette. Pakken inneholder funksjonen write_SSB som kan brukes til å skrive data til bøtte på Dapla.\n\n\nnotebook\n\nlibrary(fellesr)\n\n# Set stien til hvor data skal lagres\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"felles/veiledning/r/eksempler/purchases\"\n\n# Lage en eksempel dataframe\npurchases = data.frame(A = runif(10), B= runif(10), C=runif(10))\n\n# Skrive data til bøttet som en parquet\nwrite_SSB(purchases, file.path(bucket, folder, \"purchases.parquet\"))\n\nMerknad: Når du spesifisere bøtter i R, trenger du ikke “gs://” foran.\n\n\n\n\n\n\nNoen ganger ønsker vi å lagre data i andre formatter slik som CSV, JSON og XML.\n\nPython \n\n\ndapla-toolbelt kan skrive ut json, csv og posisjonsfiler (fixed-width-files/fwf) til lagringsområdet. Måten den gjør det på er å bruke Pandas sine funksjoner read_json, read_csv, read_fwf under panseret. Dette kan være nyttig å vite for skjønne hvordan dapla-toolbelt håndterer ulike strukturer i (spesielt hierarkiske) tekstfiler. Under ser du hvordan du kan skrive ut en dataframe til en json-fil.\n\n\nnotebook\n\nimport numpy as np\nimport pandas as pd\nfrom dapla import FileClient\n\n# Set path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Create a dataframe with Pandas\ndf = pd.DataFrame(np.random.randn(10, 5), columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\n# Save dataframe as json with dapla-toolbelt\ndp.write_pandas(df = df,\n                gcs_path = f\"{bucket}/{folder}/test.json\",\n                file_format = \"json\")\n\nSom vi ser at syntaksen over så kunne vi skrevet ut til noe annet enn json ved å endre verdien i argumentet file_format.\n\n\nPakken fellesr kan også brukes til å skrive andre type filer, for eksempel csv, til bøtter. Dette gjøres med funksjonen write_SSB og spesifisere ønsket filtype i filnavn.\nFørst kaller vi biblioteket og lage noe test data ved:\n\n\nnotebook\n\nlibrary(fellesr)\n\n# Set stien til hvor data skal lagres\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"felles/veiledning/r/eksempler/purchases\"\n\n# Lage en eksempel dataframe\npurchases = data.frame(A = runif(10), B= runif(10), C=runif(10))\n\n# Skrive til csv\nwrite_SSB(purchases, file.path(bucket, folder, \"purchases.csv\")\n\n\n\n\n\n\n\nDet er ikke anbefalt å bruke xlsx-formatet, men her ser du hvordan det kan skrives ut. Mer kommer.\n\nPython \n\n\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import AuthClient\n\n# Henter token for å kunne lese fra Dapla\ntoken = AuthClient.fetch_google_credentials()\n\n# Filsti\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\ndf.to_excel(f\"{bucket}/{folder}/test.xlsx\",\n           storage_options={\"token\": token})\n\n\n\nKommer snart\n\n\n\n\n\n\n\nUnder finner du eksempler på hvordan du kan lese inn data til en Jupyter Notebooks på Dapla.\n\n\n\nPython \n\n\n\n\nnotebook\n\nimport dapla as dp\n\n# Set path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Read path into pandas dataframe \ndp.read_pandas(gcs_path= f\"{bucket}/{folder}/data.parquet\",\n               file_format = \"parquet\",\n               columns = None,)\n\nSom vi så med write_pandas så er file_format default satt til parquet, og default for columns = None, så vi kunne også ha skrevet det slik:\ndp.read_pandas(gcs_path= f\"{bucket}/{folder}/data.parquet\")\ncolumns-argumentet er en liste med kolonnenavn som vi ønsker å lese inn. Hvis vi ikke spesifiserer noen kolonner så vil alle kolonnene leses inn.\n\n\nPakken fellesr kan brukes til å lese inn data. Funksjonen read_SSB() kan lese inn filer i flere format inkluderende parquet.\nHer er et eksempel av å lese inn parquet fil “1987”.\n\n\nnotebook\n\nlibrary(fellesr)\n\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"R_smoke_test\"\n\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.parquet\"))\n\nVi kan også filtrere hvilke variabel vi ønsker å lese inn ved å spesifisere parameter col_select. For eksempel:\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.parquet\"),\n                    col_select = c(\"Year\", \"Month\"))\nInnlesning av parquet som er kartdata finner du her: Lese kartdata\n\n\n\n\n\n\nKommer mer snart. Python-koden under bygger på eksempelet over.\n\nPython \n\n\n\n\nnotebook\n\nimport dapla as dp\n\n# Path to write to\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Read in json-file from dapla-storage\ndf = dp.read_pandas(gcs_path = f\"{bucket}/{folder}/test3.json\",\n               file_format = \"json\")\n\n\n\nFunksjonen read_SSB() kan lese inn flere type av fil-format, slik som csv og json. Du trenger ikke å endre koden, kun spesifisere hele filnavn.\nFørst kaller vi inn biblioteket fellesr og spesifisere bøtte/mappen:\n\n\nnotebook\n\nlibrary(fellesr)\n\n# Filsti\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"R_smoke_test\"\n\n# Lese inn CSV-fil\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.csv\"))\n\nFor å lese inn en json-fil kan skrive følgende:\n\n\nnotebook\n\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.json\"))\n\n\n\n\n\n\n\n\nPython \n\n\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import AuthClient\n\n# Hent token\ntoken = AuthClient.fetch_google_credentials()\n\n# Les inn fil\ndf = pd.read_excel(\"gs://ssb-prod-arbmark-skjema-data-produkt/test_gcp.xlsx\",\n    storage_options={\"token\": token})\n\n\n\nKommer snart\n\n\n\n\n\n\nHer er et eksempel på hvordan man leser inn en sas7bdat-fil på Dapla som har blitt generert i prodsonen.\n\nPython \n\n\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nsti = \"gs://ssb-dapla-felles-data-delt-prod/felles/veiledning/sas/statbank_ledstill.sas7bdat\"\n\nwith fs.open(sti) as sas:\n    df = pd.read_sas(sas, format=\"sas7bdat\", encoding=\"latin1\")\n\n\n\nI produksjon sone (på bakken) kan R-pakken haven benyttes for å lese inn .sas7bdat filer. Dette er ikke implementert i fellesr enda for lesing fra Dapla bøtte.\nMer om dette kommer.\n\n\n\n\n\n\n\nÅ slette filer fra lagringsområdet kan gjøres på flere måter. I kapitlet om sletting av data viste vi hvordan man gjør det med pek-og-klikk i Google Cloud Console. Under ser du hvordan du kan slette filer med dapla-toolbelt og gcsfs eller fellesr.\n\nPython \n\n\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\nfs.rm(f\"{bucket}/{from_folder}/df.json\")\n\n\n\nFunksjonen gc_delete_object kan brukes til å slette data på lagringsområdet.\n\n\nnotebook\n\nlibrary(fellesr)\n\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"felles/veiledning/r/eksempler/purchases\"\n\ngcs_delete_object(file.path(bucket, folder, \"purchases.parquet\"))\n\n\n\n\n\n\n\nÅ kopiere filer mellom mapper på et Linux-filsystem innebærer som regel bruke cp-kommandoen. På Dapla er det ikke så mye forskjell. Vi bruker en ligende tilnærming nå vi skal kopiere mellom bøtter eller mapper på lagringsområdet til SSB. Under ser du hvordan du kan kopiere en fil fra en mappe til en annen.\n\nPython \n\n\nLa oss begynne med et eksempel der vi kopierer en fil fra en mappe til en annen i samme bøtte.\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Path to folders\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfrom_folder = \"felles/veiledning/python/eksempler/purchases\"\nto_folder = \"felles/veiledning/python/eksempler\"\n\n# Copy file\nfs.cp(f\"{bucket}/{from_folder}/data.parquet\",\n      f\"{bucket}/{to_folder}/data_copy.parquet\")\n\nDet også fungere for å kopiere filer mellom bøtter.\nEt annet scenario vi ofte vil støte på er at vi ønsker å kopiere en fil fra vårt Jupyter-filsystem til en mappe på lagringsområdet. Her kan vi bruke fs.put-metoden.\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Create a new file in your home directory called test.txt\nwith open('/home/jovyan/test.txt', 'w') as f:\n    f.write('Create a new text file!')\n\n#Path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler\"\n\n# Copy file from local to remote file system\nfs.put(lpath=f\"/home/jovyan/test.txt\", rpath=f\"{bucket}/{folder}/test.txt\")\n\nØnsker vi å kopiere en hel mappe fra lagringsområdet til Jupyter-filsystemet, kan vi bruke fs.get-metoden, med opsjonen recursive=True.\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Copy file\nfs.get(&lt;from_bucket&gt;,\n      \"/home/jovyan/sesongjustering/\",\n      recursive=True)\n\n\n\nKommer snart\n\n\n\n\n\n\n\nPython \n\n\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfrom_folder = \"felles/veiledning/python/eksempler/purchases\"\nto_folder = \"felles/veiledning/python/eksempler\"\n\nfs.mv(f\"{bucket}/{from_folder}/data.parquet\", f\"{bucket}/{to_folder}/data.parquet\")\n\n\n\nKommer snart\n\n\n\n\n\n\nSelv om bøtter ikke har mapper med en hierarkisk struktur slik man er kjent med fra klassike filsystemer, så kan man opprette det som ser ut som mapper i objektnavnet. I realiteten blir bare / oppfattet som en del av navnet på objektet. Skulle du likevel ønske å opprette dette så kan du gjøre det følgende måte:\n\nPython \n\n\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n#Path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler\"\n\n# Create folder\nfs.touch(f\"{bucket}/{folder}/testmappe/\")\n\n\n\nKommer snart"
  },
  {
    "objectID": "statistikkere/jobbe-med-data.html#footnotes",
    "href": "statistikkere/jobbe-med-data.html#footnotes",
    "title": "Jobbe med data",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI de tidligere systemene på bakken så var det ikke nødvendig med autentisering mellom kodemiljø og datalagringen↩︎"
  },
  {
    "objectID": "statistikkere/index.html",
    "href": "statistikkere/index.html",
    "title": "Velkommen",
    "section": "",
    "text": "Denne boken tar sikte på å gi SSB-ansatte mulighet til å ta i bruk grunnleggende funksjonalitet på DAPLA uten hjelp fra eksperter. Boken er bygget opp som den reisen vi mener en statistikker skal gjennom når de flytter sin produksjon fra bakke til sky1. Første del inneholder en del grunnleggende kunnskap som vi mener er viktig å ha før man skal starte å jobbe i skyen. Andre del forklarer hvordan man søker om å opprette et Dapla-team, en forutsetning for å drive databehandling på plattformen. Det vil ofte være første steget i ta i bruk plattformen, siden det er slik man får et sted å lagre data. Her forklarer vi hvilke tjenester som inkluderes i et statistikkteam og hvordan man bruker og administerer dem. Den tredje delen tar utgangspunkt i at man skal starte å kode opp sin statistikkproduksjon eller kjøre eksisterende kode. ssb-project er et verktøy som er utviklet i SSB for å gjøre denne prosessen så enkel som mulig. Da kan brukerne implementere det som anses som god praksis i SSB med noen få tastetrykk, samtidig som vi også forklarer mer detaljert hva som skjer under panseret.\nDet er tilrettelagt for en treningsarena i bakkemiljøet. Dette miljøet er nesten identisk med det som møter deg på Dapla, med unntak av at du her har tilgang til mange av de gamle systemene og mye mindre hestekrefter i maskinene. Ideen er at SSB-ere ofte vil ønske å lære seg de nye verktøyene2 i kjente og kjære omgivelser først, og deretter flytte et ferdig skrevet produksjonsløp til Dapla. Del 4 av denne boken beskriver mer utfyllende hvordan dette miljøet skiller seg fra Dapla, og hvordan man gjør en del vanlige operasjoner mot de gamle bakkesystemene.\nSiste delen av boken kaller vi Avansert og tar for seg ulike emner som mer avanserte brukere typisk trenger informasjon om. Her finner man blant annet informasjon om hvilke databaser man kan bruke og hvilke formål de er egnet for. Her beskrives også hvordan man kan bruke andre IDE-er enn Jupyterlab hvis man ønsker det. Tjenester for schedulerte kjøringer av Notebooks blir også diskutert.\nForhåpentligvis senker denne boken terskelen for å ta i bruk Dapla. Kommentarer og ønsker vedrørende boken tas imot med åpne armer.\nGod fornøyelse😁"
  },
  {
    "objectID": "statistikkere/index.html#footnotes",
    "href": "statistikkere/index.html#footnotes",
    "title": "Velkommen",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI denne boken omtaler vi den gamle produksjonssonen, ofte kalt prodsonen, som bakke, og det nye skymiljøet Google Cloud som sky. Det er ikke helt presist men duger for formålene i denne boken.↩︎\nDet som omtales som nye verktøy er vil som regel bety R, Python, Git, GitHub og Jupyterlab.↩︎"
  },
  {
    "objectID": "statistikkere/hvorfor-dapla.html",
    "href": "statistikkere/hvorfor-dapla.html",
    "title": "Hvorfor Dapla?",
    "section": "",
    "text": "Hvorfor Dapla?\nSom dataplattform skal Dapla stimulerere til økt kvalitet på statistikk og forskning, samtidig som den gjør organisasjonen mer tilpasningsdyktig i møte med fremtiden.\n\n\nDen nye skybaserte dataplattformen (Dapla) skal bli viktig for å effektivisere arbeids-og produksjonsprosesser, den skal sikre effektiv lagring og gjenfinning av data og metadata, og støtte opp under deling av data på tvers av statistikkområder.\n\nKilde: Langtidsplan for SSB (2022-2024)\n\n\n\nMålet med Dapla er å tilby tjenester og verktøy som lar statistikkprodusenter og forskere produsere resultater på en sikker og effektiv måte."
  },
  {
    "objectID": "statistikkere/datatilstander.html",
    "href": "statistikkere/datatilstander.html",
    "title": "Datatilstander",
    "section": "",
    "text": "Datatilstander\n\nDet er naturlig at hovedfokus i SSBs kvalitetsarbeid er rettet mot statistikkene. Samtidig har bådeforventninger og krav til SSBs evne til å dele data økt betydelig de senere år. Det betyr at i tillegg til å produsere hovedproduktet «statistikk», så vil mange statistikkteam ha økt fokus på å produseregjenbrukbare datasett med høy kvalitet. En viktig forutsetning for gjenbruk er at de som vil brukedataene, kan vite hvilke endringer dataene har gjennomgått. Det må også være mulig for andre å finne og forstå dataene. Kvalitetssikret bruk av data i SSB og gjenbruk i og utenfor SSB fordrer godemetadata. Definisjoner av datatilstander og andre statistikkbegreper må derfor i størst mulig gradvære avstemt med internasjonale statistiske rammeverk og definisjoner.\nBegrepet «etterprøvbarhet» brukes flere steder i notatet, og det legges til grunn at vi bør ha som et ideal å produsere statistikk på en slik måte at ettertiden eller en uavhengig instans med tilgang til dataene og vår dokumentasjon vil komme til samme statistiske resultater som oss selv.\nTilstandene som beskrives er kildedata, inndata, klargjorte data, statistikk og utdata. De tre første er i hovedsak mikrodata som gir informasjon om enkeltenheter, mens statistikk og utdata i hovedsak er aggregerte data.\n\n(Standardutvalget 2021, 5)\n\n\n\n\n\nReferanser\n\nStandardutvalget. 2021. “Datatilstander i SSB.” Statistisk sentralbyrå. https://ssbno.sharepoint.com/sites/Internedokumenter/Delte%20dokumenter/Forms/AllItems.aspx?id=%2Fsites%2FInternedokumenter%2FDelte%20dokumenter%2FInterne%20dokumenter%202021%2F2021%2D17%20Datatilstander%20i%20SSB%20%2Epdf&parent=%2Fsites%2FInternedokumenter%2FDelte%20dokumenter%2FInterne%20dokumenter%202021."
  },
  {
    "objectID": "statistikkere/datalagring-overview.html",
    "href": "statistikkere/datalagring-overview.html",
    "title": "Datalagring",
    "section": "",
    "text": "Datalagring"
  },
  {
    "objectID": "statistikkere/kildedata-prosessering.html",
    "href": "statistikkere/kildedata-prosessering.html",
    "title": "Kildedata prosessering",
    "section": "",
    "text": "Denne tjenesten er under utvikling og kan ikke anses som klar for produksjon.\n\n\n\nFor å minske aksessering av PII1, oppfordres alle team på Dapla å benytte seg av automatisering av kildedata prosessering. Automatisering av kildedata er en tjeneste som er tilgjengelig for team å ta i bruk 100% selv-betjent. Kildedata (Standardutvalget 2021, 5) prosesseres til inndata gjennom et bestemt utvalg av operasjoner. Kildedata prosesseres som individuelle filer for å holde oppsettet enkelt og målrettet mot de definerte operasjoner. Mer kompleks operasjoner som går på tvers av flere filer burde utføres på inndata eller senere datatilstander.\n\n\n\n\n\n\nDet er kun teamets kildedataansvarlige som skal aksessere kildedata.\n\n\n\n\n\n\n\n\n\nTeamets kildedataansvarlige tar ansvar for å prosessere kildedata til inndata på en forsvarlig måte.\n\n\n\n\n\n\n\n\nFigur 1: Operasjoner som inngår i kildedata prosessering\n\n\n\nInndata er kildedata som er transformert til SSBs standard lagringsformat. Variabelnavn og -innhold er uendret bortsett fra at\n\ndirekte identifiserende variabler (f.eks. fødselsnummer) er pseudonymisert\ntegnsett, datoformat, adresse mm er endret til SSBs standardformat\ndet benyttes standard kodeverk (Klass) der det er mulig (f.eks. kjønn)\ndataene er minimert slik at kun variablene som er nødvendige i den videre produksjonsprosessen, inngår.\n\n\n(Standardutvalget 2021, 8)\nDet er ikke anbefalt å gjennomføre operasjoner som:\n\nGår på tvers av flere filer\nLegge til nye felt\nEndre navn på felt\nAggregerer data\nosv.\n\n\n\n\n\n\nFølg instruksjonene her for å koble prosjektet ditt til Github.\n\n\n\n\n\nKilder konfigureres i et teams Infrastructure as Code (IaC) repo på Github. Det kan finnes basert på følgende formulering: github.com/statisticsnorway/&lt;teamnavn&gt;-iac. Kilder konfigureres under stien automation/source_data på repoet.\n\n\n\nHver kilde konfigureres ved hjelp av to filer:\n\nconfig.yaml som blant annet konfigurer hvilke stier i teamets kildedatabøtte prosesseres.\nprocess_source_data.py som kjøres når en kildedatafil prosesseres. Her må man skrive en python funksjon på en viss format.\n\nDisse filene er lagt til en mappe per kilde, under automation/source_data i IaC repoet. Se eksemplet under for en detaljert forklaring.\n\n\n\n\n\n\nMappenavnet for kilder i IaC repoene er brukt som navn for ressurser. Dette i praksis betyr at det enesete tillatte tegnene i mappenavnet er bokstaver, tall og bindestrek. Det er ikke tillatt med mellomrom eller andre spesialtegn. Mappenavnet kan bestå av opptil 20 tegn.\n\n\n\n\n\n\nDette går ut på om prosesseringsscriptet kan enkelt håndtere variasjonen i filene som samles inn.\nGrunn til å opprette en ny kilde kan være: - Kildedatafilen har en annen format (f.eks xml eller json) - Kildedataen har ulike felter - Kildedataen inneholder PII2 eller ikke\n\n\n\n\n\n\n\n\n\nDisse instruksjoner forutsetter at prosjektet ditt er koblet til Github allerede.\n\n\n\n\nSkrive skriptet process_source_data.py som prosesserer kildedatafilen til inndata. Dette kan testes av kildedataansvarlige manuelt på Jupyter for å verifisere at dataene blir prosessert som ønsket.\nI en branch i teamets IaC repo, legge til config.yaml og process_source_data.py i en mappe under automation/source_data. Se eksemplet under for en detaljert forklaring av formatet.\nLag en PR på grenen og få den godkjent av kildedataansvarlige.\nVent til alle tester er ferdige. Det skal stå “All checks have passed” før man går videre, hvis testene feiler følg stegene her. \nSkrive atlantis apply i en kommentar på PRen for å opprette det nødvendige infrastruktur for å prosessere kilden.\nMerge PRen.\nSjekk resultatet av det automatiske bygget.\nVerifisere at nye filer lagt i kildedatabøtten blir prosessert til inndata som forventet.\n\n\n\n\n\nLa oss si at et team (småbakst) har to datakilder levert av ulik dataeiere på ulik formater. Den ene er om boller og er på csv format og den andre er om rundstykker og er på json format. Kildedataansvarlige i teamet bestemmer seg for at filene i boller/ er like nok å prosesseres som en kilde, og at filene i rundstykker/ kan prosesseres som en annen kilde.\n\n\nssb-prod-smaabakst-data-kilde\n├── boller\n│   ├── hvetebolle\n│   │   ├── 2018-salg.csv\n│   │   ├── 2019-salg.csv\n│   │   ├── ...\n│   ├── kanelbolle\n│   │   ├── 2018-salg.csv\n│   │   ├── 2019-salg.csv\n│   │   ├── ...\n│   └── skolebolle\n│       ├── 2018-salg.csv\n│       ├── 2019-salg.csv\n│       ├── ...\n└── rundstykker\n    ├── haandverker\n    │   ├── apr-2022-resultater.json\n    │   ├── aug-2022-resultater.json\n    │   ├── ...\n    └── havre\n        ├── apr-2022-resultater.json\n        ├── aug-2022-resultater.json\n        ├── ...\n\n\n\nsmaabakst-iac\n└── automation\n    └── source_data\n        ├── boller\n        │   ├── config.yaml\n        │   └── process_source_data.py\n        └── rundstykker\n            ├── config.yaml\n            └── process_source_data.py\n\n\n\n\n\nconfig.yaml\n\nfolder_prefix: boller\n\nVerdien for folder_prefix tilsvarer en “fil sti” i kildedatabøtte. I dette tilfellet vil nye filer lagt til under boller/ trigge en prosessering.\n\n\n\n\n\nconfig.yaml\n\nfolder_prefix: rundstykker\n\n\n\n\n\n\nMed prosesseringsscriptet mener vi filen process_source_data.py som ligger i en mappe per kilde under automation/source_data. Dette scriptet vil bli kalt hver gang det blir lagt til en ny fil i kildedatabøtten. Metodesignaturen ser slik ut:\n\n\nprocess_source_data.py\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n\nDette betyr at hvis f.eks. 10 filer blir lagt til i kildedatabøtten samtidig så vil det startes opp 10 individuelle Python-prosesser som kaller denne main-metoden med forskjellig file_path. Parameteren file_path vil inneholde hele filstien inkl. filnavn. Så en enkel flytteoperasjon fra kildedatabøtten til inndatebøtten (uten noen form for konvertering) vil kunne uttrykkes slik:\n\n\nprocess_source_data.py\n\nimport dapla as dp\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n    source_bucket_name = \"ssb-prod-my-project-data-kilde\"\n    destination_bucket_name = \"ssb-prod-my-project-data-produkt\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    df = dp.read_pandas(file_path)\n    dp.write_pandas(df, destination_bucket_path)\n\nAlternativt…\n\n\nprocess_source_data.py\n\nfrom dapla import FileClient\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n    source_bucket_name = \"ssb-prod-my-project-data-kilde\"\n    destination_bucket_name = \"ssb-prod-my-project-data-produkt\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    fs = FileClient.get_gcs_file_system()\n    fs.copy(file_path, destination_path)\n\n\n\nDet anbefales å bruke Pythons logging modul for å logge, og ikke bruke print eller skrive til stdout/stderr. Det er satt opp en standard logger-konfigurasjon som skriver informasjonsmeldinger (log level info) til stdout og feilmeldinger (log level warning eller error) til stderr. Feil som ikke blir håndtert blir automatisk fanget opp og logget av automatiseringsløsningen. Eksemplet nedenfor logger en informasjonsmelding, en advarsel og en feilmelding:\n\n\nprocess_source_data.py\n\nimport logging\nlogging.info('Til info')\nlogging.warning('Advarsel!')\nlogging.error('En feil oppstod!')"
  },
  {
    "objectID": "statistikkere/kildedata-prosessering.html#operasjoner-som-inngår-i-kildedata-prosessering",
    "href": "statistikkere/kildedata-prosessering.html#operasjoner-som-inngår-i-kildedata-prosessering",
    "title": "Kildedata prosessering",
    "section": "",
    "text": "Figur 1: Operasjoner som inngår i kildedata prosessering\n\n\n\nInndata er kildedata som er transformert til SSBs standard lagringsformat. Variabelnavn og -innhold er uendret bortsett fra at\n\ndirekte identifiserende variabler (f.eks. fødselsnummer) er pseudonymisert\ntegnsett, datoformat, adresse mm er endret til SSBs standardformat\ndet benyttes standard kodeverk (Klass) der det er mulig (f.eks. kjønn)\ndataene er minimert slik at kun variablene som er nødvendige i den videre produksjonsprosessen, inngår.\n\n\n(Standardutvalget 2021, 8)\nDet er ikke anbefalt å gjennomføre operasjoner som:\n\nGår på tvers av flere filer\nLegge til nye felt\nEndre navn på felt\nAggregerer data\nosv."
  },
  {
    "objectID": "statistikkere/kildedata-prosessering.html#ta-tjenesten-i-bruk",
    "href": "statistikkere/kildedata-prosessering.html#ta-tjenesten-i-bruk",
    "title": "Kildedata prosessering",
    "section": "",
    "text": "Følg instruksjonene her for å koble prosjektet ditt til Github.\n\n\n\n\n\nKilder konfigureres i et teams Infrastructure as Code (IaC) repo på Github. Det kan finnes basert på følgende formulering: github.com/statisticsnorway/&lt;teamnavn&gt;-iac. Kilder konfigureres under stien automation/source_data på repoet.\n\n\n\nHver kilde konfigureres ved hjelp av to filer:\n\nconfig.yaml som blant annet konfigurer hvilke stier i teamets kildedatabøtte prosesseres.\nprocess_source_data.py som kjøres når en kildedatafil prosesseres. Her må man skrive en python funksjon på en viss format.\n\nDisse filene er lagt til en mappe per kilde, under automation/source_data i IaC repoet. Se eksemplet under for en detaljert forklaring.\n\n\n\n\n\n\nMappenavnet for kilder i IaC repoene er brukt som navn for ressurser. Dette i praksis betyr at det enesete tillatte tegnene i mappenavnet er bokstaver, tall og bindestrek. Det er ikke tillatt med mellomrom eller andre spesialtegn. Mappenavnet kan bestå av opptil 20 tegn.\n\n\n\n\n\n\nDette går ut på om prosesseringsscriptet kan enkelt håndtere variasjonen i filene som samles inn.\nGrunn til å opprette en ny kilde kan være: - Kildedatafilen har en annen format (f.eks xml eller json) - Kildedataen har ulike felter - Kildedataen inneholder PII2 eller ikke\n\n\n\n\n\n\n\n\n\nDisse instruksjoner forutsetter at prosjektet ditt er koblet til Github allerede.\n\n\n\n\nSkrive skriptet process_source_data.py som prosesserer kildedatafilen til inndata. Dette kan testes av kildedataansvarlige manuelt på Jupyter for å verifisere at dataene blir prosessert som ønsket.\nI en branch i teamets IaC repo, legge til config.yaml og process_source_data.py i en mappe under automation/source_data. Se eksemplet under for en detaljert forklaring av formatet.\nLag en PR på grenen og få den godkjent av kildedataansvarlige.\nVent til alle tester er ferdige. Det skal stå “All checks have passed” før man går videre, hvis testene feiler følg stegene her. \nSkrive atlantis apply i en kommentar på PRen for å opprette det nødvendige infrastruktur for å prosessere kilden.\nMerge PRen.\nSjekk resultatet av det automatiske bygget.\nVerifisere at nye filer lagt i kildedatabøtten blir prosessert til inndata som forventet.\n\n\n\n\n\nLa oss si at et team (småbakst) har to datakilder levert av ulik dataeiere på ulik formater. Den ene er om boller og er på csv format og den andre er om rundstykker og er på json format. Kildedataansvarlige i teamet bestemmer seg for at filene i boller/ er like nok å prosesseres som en kilde, og at filene i rundstykker/ kan prosesseres som en annen kilde.\n\n\nssb-prod-smaabakst-data-kilde\n├── boller\n│   ├── hvetebolle\n│   │   ├── 2018-salg.csv\n│   │   ├── 2019-salg.csv\n│   │   ├── ...\n│   ├── kanelbolle\n│   │   ├── 2018-salg.csv\n│   │   ├── 2019-salg.csv\n│   │   ├── ...\n│   └── skolebolle\n│       ├── 2018-salg.csv\n│       ├── 2019-salg.csv\n│       ├── ...\n└── rundstykker\n    ├── haandverker\n    │   ├── apr-2022-resultater.json\n    │   ├── aug-2022-resultater.json\n    │   ├── ...\n    └── havre\n        ├── apr-2022-resultater.json\n        ├── aug-2022-resultater.json\n        ├── ...\n\n\n\nsmaabakst-iac\n└── automation\n    └── source_data\n        ├── boller\n        │   ├── config.yaml\n        │   └── process_source_data.py\n        └── rundstykker\n            ├── config.yaml\n            └── process_source_data.py\n\n\n\n\n\nconfig.yaml\n\nfolder_prefix: boller\n\nVerdien for folder_prefix tilsvarer en “fil sti” i kildedatabøtte. I dette tilfellet vil nye filer lagt til under boller/ trigge en prosessering.\n\n\n\n\n\nconfig.yaml\n\nfolder_prefix: rundstykker"
  },
  {
    "objectID": "statistikkere/kildedata-prosessering.html#skrive-prosesseringsscriptet",
    "href": "statistikkere/kildedata-prosessering.html#skrive-prosesseringsscriptet",
    "title": "Kildedata prosessering",
    "section": "",
    "text": "Med prosesseringsscriptet mener vi filen process_source_data.py som ligger i en mappe per kilde under automation/source_data. Dette scriptet vil bli kalt hver gang det blir lagt til en ny fil i kildedatabøtten. Metodesignaturen ser slik ut:\n\n\nprocess_source_data.py\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n\nDette betyr at hvis f.eks. 10 filer blir lagt til i kildedatabøtten samtidig så vil det startes opp 10 individuelle Python-prosesser som kaller denne main-metoden med forskjellig file_path. Parameteren file_path vil inneholde hele filstien inkl. filnavn. Så en enkel flytteoperasjon fra kildedatabøtten til inndatebøtten (uten noen form for konvertering) vil kunne uttrykkes slik:\n\n\nprocess_source_data.py\n\nimport dapla as dp\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n    source_bucket_name = \"ssb-prod-my-project-data-kilde\"\n    destination_bucket_name = \"ssb-prod-my-project-data-produkt\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    df = dp.read_pandas(file_path)\n    dp.write_pandas(df, destination_bucket_path)\n\nAlternativt…\n\n\nprocess_source_data.py\n\nfrom dapla import FileClient\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n    source_bucket_name = \"ssb-prod-my-project-data-kilde\"\n    destination_bucket_name = \"ssb-prod-my-project-data-produkt\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    fs = FileClient.get_gcs_file_system()\n    fs.copy(file_path, destination_path)\n\n\n\nDet anbefales å bruke Pythons logging modul for å logge, og ikke bruke print eller skrive til stdout/stderr. Det er satt opp en standard logger-konfigurasjon som skriver informasjonsmeldinger (log level info) til stdout og feilmeldinger (log level warning eller error) til stderr. Feil som ikke blir håndtert blir automatisk fanget opp og logget av automatiseringsløsningen. Eksemplet nedenfor logger en informasjonsmelding, en advarsel og en feilmelding:\n\n\nprocess_source_data.py\n\nimport logging\nlogging.info('Til info')\nlogging.warning('Advarsel!')\nlogging.error('En feil oppstod!')"
  },
  {
    "objectID": "statistikkere/kildedata-prosessering.html#footnotes",
    "href": "statistikkere/kildedata-prosessering.html#footnotes",
    "title": "Kildedata prosessering",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPersonidentifiserende Informasjon↩︎\nPersonidentifiserende Informasjon↩︎"
  },
  {
    "objectID": "statistikkere/hurtigstart.html",
    "href": "statistikkere/hurtigstart.html",
    "title": "Hurtigstart",
    "section": "",
    "text": "Hurtigstart"
  },
  {
    "objectID": "statistikkere/overforing-av-data.html",
    "href": "statistikkere/overforing-av-data.html",
    "title": "Overføring av data",
    "section": "",
    "text": "For å overføre data mellom bakke og sky brukes Data Transfer, som er en tjeneste i Google Cloud Console. Denne tjenesten kan brukes til å flytte data både til og fra Linuxstammen og Dapla, og er tilgjengelig for teamets kildedataansvarlige.\nFor å få tilgang til å overføre filer må man be om dette ved opprettelsen av teamet. Ber man om det skjer følgende:\n\nEn mappe blir opprettet på Linux i prodsonen under /ssb/cloud_sync/\nEt Google Project blir opprettet med navn &lt;team navn&gt;-ts.\n\nDette Google-prosjektet er ikke det samme som der du lagrer annen data. Det har navnet &lt;team navn&gt;-ts, og filstiene på bakken og sky vises i Figur 1.\n\n\n\nFigur 1: Hvordan Transfer Service kan flytte filer mellom bakke og sky.\n\n\nTeamets kildedataansvarlige vil være spesifisert som en del av å opprette et Dapla-team.\n\n\nEnten man skal overføre filer opp til sky eller ned til bakken så bruker man den samme Data Transfer tjenesten. For å få tilgang til denne må man først logge seg inn i Google Cloud Console. Sjekk at du er logget inn med din SSB-konto (xxx@ssb.no).\nØverst på siden, til høyre for teksten Google Cloud finnes det en prosjektvelger, og her er det viktig å velge korrekt Google prosjekt. Hvis du trykker på prosjektvelgeren vil det åpnes opp et nytt vindu. Sjekk at det står SSB.NO øverst i dette vinduet. Trykk deretter på fanen ALL for å få opp alle tilgjengelige Google-prosjekter under organisasjonen ssb.no (Figur 2)\n\n\n\nFigur 2: Prosjektvelgeren i Google Cloud Console\n\n\nUnder ssb.no vil det ligge flere mapper. Åpne mappen som heter production og let frem en undermappe som har navnet på ditt Dapla-team. Strukturen skal se slik ut:\n    ssb.no\n    ├── production\n        └── &lt;teamnavn&gt;\n            ├── prod-&lt;teamnavn&gt;\n            └── &lt;teamnavn&gt;-ts\nDet underste nivået (prod-&lt;teamnavn&gt; og &lt;teamnavn&gt;-ts) viser prosjektene, nivået i mellom er mapper, og toppnivået er organisasjonen (ssb.no). Prosjektet &lt;teamnavn&gt;-ts er et separat prosjekt som bare teamets kildedataansvarlige har tilgang til, og det er her tjenesten Data Transfer skal settes opp.\n\nVelg derfor prosjektet &lt;teamnavn&gt;-ts.\nI søkefeltet til Google Cloud Console, skriv Data transfer og trykk på det valget som kommer opp.\nFørste gang man kommer inn på siden til Transfer Services vil man bli vist en blå knapp med teksten Set Up Connection. Når du trykker på denne vil det dukke opp et nytt felt hvor du får valget Create Pub-Sub Resources. Dette er noe som bare trengs å gjøre én gang. Trykk på den blå CREATE knappen, og deretter trykk på Close lenger nede.\nI navigasjonsmenyen til venstre trykk Transfer jobs, og deretter trykk på + Create transfer job øverst på siden for å opprette en ny overføringsjobb.\n\n\n\nFølgende oppskrift tar utgangspunkt i siden Create a transfer job (Figur 3):\n\n\n\nFigur 3: Opprett overføringsjobb i Google Cloud Console\n\n\n\nVelg POSIX filesystem under “Source type” og Google cloud storage under “Destination type” (eller motsatt hvis overføringsjobben skal gå fra Dapla til Linuxstammen). Trykk Next step\nNedtrekkslisten “Agent pool” skal normalt bare ha ett valg: transfer_service_default. Velg denne.\nI feltet “Source directory path” skal man kun skrive data/tilsky siden overføringsagenten kun har tilgang til mapper som ligger relativt plassert under /ssb/cloud_sync/&lt;teamnavn&gt;/. Trykk Next step\nVelg en destinasjon for overføringsjobben. Trykk på Browse og velg bøtten med navn som passer til ssb-prod-&lt;teamnavn&gt;-data-synk-opp. Vi anbefaler at du også oppretter en mappe inne i denne bøtten. Det gjøres ved å trykke på mappeikonet med et +-tegn foran. Skriv inn et passende mappenavn og trykk Select i bunnen av siden. Trykk deretter Next step\nNeste steg “Choose how and when to run this job” er opp til brukeren å bestemme. Hvis man f.eks. velger at Data Transfer skal overføre data en gang i uken, vil den kun starte en overføring hvis det finnes nye data. Trykk Next step\nBeskriv overføringsjobben, f.eks: “Flytter data for  til sky.”. Resten av feltene er opp til brukeren å bestemme. Standardverdiene er OK.\n\nTrykk til slutt på den blå Create-knappen. Du vil kunne se kjørende jobber under menyen Transfer jobs.\nFor å sjekke om data har blitt overført, skriv inn cloud storage i søkefeltet øverst på siden og trykk på det første valget som kommer opp. Her vil du finne en oversikt over alle teamets bøtter, deriblant en med navn ssb-prod-&lt;team-name&gt;-data-synk-opp. Når overføringsjobben er ferdig vil du kunne finne igjen dataene i den mappen som ble definert i stegene overnfor.\n\n\n\nOverføringsjobben settes opp nesten identisk med Overføring fra Linuxstammen til Dapla med unntak av følgende:\n\nSteg 1: Velg Google cloud storage under “Source type” og POSIX filesystem under “Destination type”\nSteg 2: Velg bøtten ssb-prod-&lt;team-name&gt;-data-synk-ned\nStep 3: Velg transfer_service_default som “Agent pool” og skriv data/frasky inn i feltet for “Destination directory path”.\n\nFor å se om data har blitt overført til Linuxstammen må du nå gå til mappen /ssb/cloud_sync/&lt;team-name&gt;/data/frasky fra FileZilla.\nHusk: Du kan alltids gå tilbake og se på tidligere fullførte jobber, og starte en overføringsjobb manuelt fra menyen Transfer jobs.\n\n\n\n\nNår du har satt opp en, enten for å overføre fra sky eller til sky, kan du skrive ut data til mappen eller bøtten som du har bedt Transfer Service om å overføre data fra.\nHvis du skal overføre data fra bakken/prodsonen til sky, så må teamets kildedataansvarlige skrive ut data til Linux-mappen /ssb/cloud_sync/&lt;team navn&gt;/data/tilsky, og det vil ende opp i Dapla-bøtta gs://ssb-prod-&lt;team navn&gt;-data-synk-opp Dette kan du gjøre med alle programmeringsverktøy som har en kobling til Linux-stammene der dataene ligger. For eksempel:\n\nSAS EG\nSAS-installasjon på Linux\nJupyterlab i prodsonen\nRstudio på sl-stata-03\n\nSkal du flytte data fra Dapla til bakken/prodsonen, så må teamets kildedataansvarlige skrive ut data til gs://ssb-prod-&lt;team navn&gt;-data-synk-opp-bøtta på Dapla. Det er noe man typisk gjør fra Jupyterlab på Dapla."
  },
  {
    "objectID": "statistikkere/overforing-av-data.html#sette-opp-overføringsjobber",
    "href": "statistikkere/overforing-av-data.html#sette-opp-overføringsjobber",
    "title": "Overføring av data",
    "section": "",
    "text": "Enten man skal overføre filer opp til sky eller ned til bakken så bruker man den samme Data Transfer tjenesten. For å få tilgang til denne må man først logge seg inn i Google Cloud Console. Sjekk at du er logget inn med din SSB-konto (xxx@ssb.no).\nØverst på siden, til høyre for teksten Google Cloud finnes det en prosjektvelger, og her er det viktig å velge korrekt Google prosjekt. Hvis du trykker på prosjektvelgeren vil det åpnes opp et nytt vindu. Sjekk at det står SSB.NO øverst i dette vinduet. Trykk deretter på fanen ALL for å få opp alle tilgjengelige Google-prosjekter under organisasjonen ssb.no (Figur 2)\n\n\n\nFigur 2: Prosjektvelgeren i Google Cloud Console\n\n\nUnder ssb.no vil det ligge flere mapper. Åpne mappen som heter production og let frem en undermappe som har navnet på ditt Dapla-team. Strukturen skal se slik ut:\n    ssb.no\n    ├── production\n        └── &lt;teamnavn&gt;\n            ├── prod-&lt;teamnavn&gt;\n            └── &lt;teamnavn&gt;-ts\nDet underste nivået (prod-&lt;teamnavn&gt; og &lt;teamnavn&gt;-ts) viser prosjektene, nivået i mellom er mapper, og toppnivået er organisasjonen (ssb.no). Prosjektet &lt;teamnavn&gt;-ts er et separat prosjekt som bare teamets kildedataansvarlige har tilgang til, og det er her tjenesten Data Transfer skal settes opp.\n\nVelg derfor prosjektet &lt;teamnavn&gt;-ts.\nI søkefeltet til Google Cloud Console, skriv Data transfer og trykk på det valget som kommer opp.\nFørste gang man kommer inn på siden til Transfer Services vil man bli vist en blå knapp med teksten Set Up Connection. Når du trykker på denne vil det dukke opp et nytt felt hvor du får valget Create Pub-Sub Resources. Dette er noe som bare trengs å gjøre én gang. Trykk på den blå CREATE knappen, og deretter trykk på Close lenger nede.\nI navigasjonsmenyen til venstre trykk Transfer jobs, og deretter trykk på + Create transfer job øverst på siden for å opprette en ny overføringsjobb.\n\n\n\nFølgende oppskrift tar utgangspunkt i siden Create a transfer job (Figur 3):\n\n\n\nFigur 3: Opprett overføringsjobb i Google Cloud Console\n\n\n\nVelg POSIX filesystem under “Source type” og Google cloud storage under “Destination type” (eller motsatt hvis overføringsjobben skal gå fra Dapla til Linuxstammen). Trykk Next step\nNedtrekkslisten “Agent pool” skal normalt bare ha ett valg: transfer_service_default. Velg denne.\nI feltet “Source directory path” skal man kun skrive data/tilsky siden overføringsagenten kun har tilgang til mapper som ligger relativt plassert under /ssb/cloud_sync/&lt;teamnavn&gt;/. Trykk Next step\nVelg en destinasjon for overføringsjobben. Trykk på Browse og velg bøtten med navn som passer til ssb-prod-&lt;teamnavn&gt;-data-synk-opp. Vi anbefaler at du også oppretter en mappe inne i denne bøtten. Det gjøres ved å trykke på mappeikonet med et +-tegn foran. Skriv inn et passende mappenavn og trykk Select i bunnen av siden. Trykk deretter Next step\nNeste steg “Choose how and when to run this job” er opp til brukeren å bestemme. Hvis man f.eks. velger at Data Transfer skal overføre data en gang i uken, vil den kun starte en overføring hvis det finnes nye data. Trykk Next step\nBeskriv overføringsjobben, f.eks: “Flytter data for  til sky.”. Resten av feltene er opp til brukeren å bestemme. Standardverdiene er OK.\n\nTrykk til slutt på den blå Create-knappen. Du vil kunne se kjørende jobber under menyen Transfer jobs.\nFor å sjekke om data har blitt overført, skriv inn cloud storage i søkefeltet øverst på siden og trykk på det første valget som kommer opp. Her vil du finne en oversikt over alle teamets bøtter, deriblant en med navn ssb-prod-&lt;team-name&gt;-data-synk-opp. Når overføringsjobben er ferdig vil du kunne finne igjen dataene i den mappen som ble definert i stegene overnfor.\n\n\n\nOverføringsjobben settes opp nesten identisk med Overføring fra Linuxstammen til Dapla med unntak av følgende:\n\nSteg 1: Velg Google cloud storage under “Source type” og POSIX filesystem under “Destination type”\nSteg 2: Velg bøtten ssb-prod-&lt;team-name&gt;-data-synk-ned\nStep 3: Velg transfer_service_default som “Agent pool” og skriv data/frasky inn i feltet for “Destination directory path”.\n\nFor å se om data har blitt overført til Linuxstammen må du nå gå til mappen /ssb/cloud_sync/&lt;team-name&gt;/data/frasky fra FileZilla.\nHusk: Du kan alltids gå tilbake og se på tidligere fullførte jobber, og starte en overføringsjobb manuelt fra menyen Transfer jobs."
  },
  {
    "objectID": "statistikkere/overforing-av-data.html#skrive-ut-data",
    "href": "statistikkere/overforing-av-data.html#skrive-ut-data",
    "title": "Overføring av data",
    "section": "",
    "text": "Når du har satt opp en, enten for å overføre fra sky eller til sky, kan du skrive ut data til mappen eller bøtten som du har bedt Transfer Service om å overføre data fra.\nHvis du skal overføre data fra bakken/prodsonen til sky, så må teamets kildedataansvarlige skrive ut data til Linux-mappen /ssb/cloud_sync/&lt;team navn&gt;/data/tilsky, og det vil ende opp i Dapla-bøtta gs://ssb-prod-&lt;team navn&gt;-data-synk-opp Dette kan du gjøre med alle programmeringsverktøy som har en kobling til Linux-stammene der dataene ligger. For eksempel:\n\nSAS EG\nSAS-installasjon på Linux\nJupyterlab i prodsonen\nRstudio på sl-stata-03\n\nSkal du flytte data fra Dapla til bakken/prodsonen, så må teamets kildedataansvarlige skrive ut data til gs://ssb-prod-&lt;team navn&gt;-data-synk-opp-bøtta på Dapla. Det er noe man typisk gjør fra Jupyterlab på Dapla."
  },
  {
    "objectID": "statistikkere/dapla-team-overview.html",
    "href": "statistikkere/dapla-team-overview.html",
    "title": "Dapla-team",
    "section": "",
    "text": "Dapla-team\ndlfkdslfj"
  },
  {
    "objectID": "statistikkere/lage-nettsider.html",
    "href": "statistikkere/lage-nettsider.html",
    "title": "Lage nettsider",
    "section": "",
    "text": "Lage nettsider"
  },
  {
    "objectID": "statistikkere/innlogging.html",
    "href": "statistikkere/innlogging.html",
    "title": "Innlogging",
    "section": "",
    "text": "Innlogging på Dapla er veldig enkelt. Dapla er en nettadresse som alle SSB-ere kan gå inn på hvis de er logget på SSB sitt nettverk. Å være logget på SSB sitt nettverk betyr i denne sammenhengen at man er logget på med VPN, enten man er på kontoret eller på hjemmekontor. For å gjøre det enda enklere har vi laget en fast snarvei til denne nettadressen på vårt intranett/Byrånettet(se Figur 1).\n\n\n\nFigur 1: Snarvei til Dapla fra intranett\n\n\nMen samtidig som det er lett å logge seg på, så er det noen kompliserende ting som fortjener en forklaring. Noe skyldes at vi mangler et klart språk for å definere bakkemiljøet og skymiljøet slik at alle skjønner hva man snakker om. I denne boken definerer bakkemiljøet som stedet der man har drevet med statistikkproduksjon de siste tiårene. Skymiljøet er den nye dataplattformen Dapla på Google Cloud.\nDet som gjør ting litt komplisert er at vi har 2 Jupyter-miljøer på både bakke og sky. Årsaken er at vi har ett test- og ett prod-område for hver, og det blir i alt 4 Jupyter-miljøer. Figur 2 viser dette.\n\n\n\nFigur 2: De 4 Jupyter-miljøene i SSB. Et test-miljø og et prod-miljø på bakke og sky/Dapla\n\n\nHver av disse miljøene har sin egen nettadresse og sitt eget bruksområde.\n\n\nI de fleste tilfeller vil en statistikker eller forsker ønske å logge seg inn i prod-miljøet. Det er her man skal kjøre koden sin i et produksjonsløp som skal publiseres eller utvikles. I noen tilfeller hvor man ber om å få tilgjengliggjort en ny tjeneste så vil denne først rulles ut i testområdet som vi kaller staging-området. Årsaken er at vi ønsker å beskytte prod-miljøet fra software som potensielt ødelegger for eksisterende funksjonalitet. Derfor ruller vi ut nye ting i staging først. Av den grunn vil de fleste oppleve å bli bedt om å logge seg inn der for testing en eller annen gang. Under forklarer vi hvordan man går frem for å logge seg på de to ulike miljøene på Dapla.\n\n\nFor å logge seg inn inn i prod-miljøet på Dapla kan man gjøre følgende:\n\nGå inn på lenken https://jupyter.dapla.ssb.no/ i en Chrome-nettleser eller klikk på lenken på Byrånettet som vist i Figur 1.\nAlle i SSB har en Google Cloud-konto som må brukes når man logger seg på Dapla. Brukernavnet i Google er det samme som din korte epost-adresse (f.eks. cth@ssb.no). Hvis du ikke allerede er logget inn i Google vil du få spørsmål om å velge hvilken Google-konto som skal brukes (Figur 3). Logg inn med din Google-konto (ssb.no) og ditt AD-passord.\n\n\n\n\nFigur 3: Velg en Google-konto\n\n\n\nDeretter blir man spurt om man godtar at ssb.no (altså Dapla) kan bruke din Google Cloud-konto (Figur 4). Trykk Allow.\n\n\n\n\nFigur 4: Tillat at ssb.no får bruke din Google Cloud-konto\n\n\n\nDeretter lander man på en side som lar deg avgjøre hvor mye maskinkraft som skal holdes av til deg (Figur 5). Det øverste alternativet er valgt som standard, og er tilstrekkelig for de fleste.\n\n\n\n\nFigur 5: Velg hvor mye maskinkraft du trenger\n\n\n\nVent til maskinen din starter opp (Figur 6). Oppstartstiden kan variere.\n\n\n\n\nFigur 6: Starter opp Jupyter\n\n\nEtter dette er man logget inn i et Jupyter-miljø som kjører på en minimal Ubuntu-maskin. Hvis man er del av et Dapla-team får man også tilgang til alt teamet har tilgang til.\n\n\n\nInnlogging til staging-miljøet er identisk med innloggingen til prod-miljøet, med ett viktig unntak: nettadressen er nå https://jupyter.dapla-staging.ssb.no/.\nLitt mer om hva som er tilgjenglig her kommer.\n\n\n\n\n\nNoen ganger kan man komme i en situasjon hvor løsningen for Single Sign-On (pålogging på tvers av flere systemer) gir en feilmelding a la Figur 7:\n\n\n\nFigur 7: Feil som kan oppstå ved pålogging\n\n\nI denne situasjonen må man trykke på knappen “Add to existing account”. Da vil skjermbildet Figur 8 dukke opp:\n\n\n\nFigur 8: Klikk på Google-knappen for å logge på igjen\n\n\nHer må man tykke på Google-knappen (se pil), og deretter logge inn som vist i Figur 3 tidligere i dette avsnittet.\n\n\n\n\n\nJupyter-miljøet på bakken bruker samme base-image1 for å installere Jupyterlab, og er derfor identisk på mange måter. Men innloggingen er ganske forskjellig.\n\n\n\n\n\n\nNote\n\n\n\nFom. 5. desember 2022 har vi byttet ut Jupyter-miljøet på bakken. Beskrivelsene under gjelder derfor det nye miljøet. Fram til 15. januar vil du kunne bruke det gamle miljøet ved å gå inn på lenken https://jupyter-prod.ssb.no/ manuelt i Google Chrome. Etter 15. januar blir det gamle Jupyter-miljøet avviklet.\n\n\n\n\nDu logger deg inn på prod i bakkemiljøet på følgende måte:\n\nLogg deg inn på Citrix-Windows i bakkemiljøet. Det kan gjøres ved å bruke lenken Citrix på Byrånettet, som også vises i Figur 1.\nTrykk på Jupyterlab-ikonet, som vist på Figur 9, og logg deg inn med vanlig brukernavn og passord.\n\n\n\n\nFigur 9: Jupyterlab-ikon på Skrivebordet i Citrix-Windows.\n\n\nNår du trykker på ikonet blir du tatt til nettadressen https://sl-jupyter-p.ssb.no/. Du kunne også åpnet Jupyterlab ved åpne Chrome-nettleseren og skrive inn adressen manuelt.\n\n\n\nInnlogging til staging-miljøet har ingen snarvei på Skrivebordet, og du må gjøre følgende for å åpne miljøet:\n\nÅpne Chrome-nettleseren i Citrix-Windows.\nSkriv inn url-en https://sl-jupyter-t.ssb.no/"
  },
  {
    "objectID": "statistikkere/innlogging.html#dapla",
    "href": "statistikkere/innlogging.html#dapla",
    "title": "Innlogging",
    "section": "",
    "text": "I de fleste tilfeller vil en statistikker eller forsker ønske å logge seg inn i prod-miljøet. Det er her man skal kjøre koden sin i et produksjonsløp som skal publiseres eller utvikles. I noen tilfeller hvor man ber om å få tilgjengliggjort en ny tjeneste så vil denne først rulles ut i testområdet som vi kaller staging-området. Årsaken er at vi ønsker å beskytte prod-miljøet fra software som potensielt ødelegger for eksisterende funksjonalitet. Derfor ruller vi ut nye ting i staging først. Av den grunn vil de fleste oppleve å bli bedt om å logge seg inn der for testing en eller annen gang. Under forklarer vi hvordan man går frem for å logge seg på de to ulike miljøene på Dapla.\n\n\nFor å logge seg inn inn i prod-miljøet på Dapla kan man gjøre følgende:\n\nGå inn på lenken https://jupyter.dapla.ssb.no/ i en Chrome-nettleser eller klikk på lenken på Byrånettet som vist i Figur 1.\nAlle i SSB har en Google Cloud-konto som må brukes når man logger seg på Dapla. Brukernavnet i Google er det samme som din korte epost-adresse (f.eks. cth@ssb.no). Hvis du ikke allerede er logget inn i Google vil du få spørsmål om å velge hvilken Google-konto som skal brukes (Figur 3). Logg inn med din Google-konto (ssb.no) og ditt AD-passord.\n\n\n\n\nFigur 3: Velg en Google-konto\n\n\n\nDeretter blir man spurt om man godtar at ssb.no (altså Dapla) kan bruke din Google Cloud-konto (Figur 4). Trykk Allow.\n\n\n\n\nFigur 4: Tillat at ssb.no får bruke din Google Cloud-konto\n\n\n\nDeretter lander man på en side som lar deg avgjøre hvor mye maskinkraft som skal holdes av til deg (Figur 5). Det øverste alternativet er valgt som standard, og er tilstrekkelig for de fleste.\n\n\n\n\nFigur 5: Velg hvor mye maskinkraft du trenger\n\n\n\nVent til maskinen din starter opp (Figur 6). Oppstartstiden kan variere.\n\n\n\n\nFigur 6: Starter opp Jupyter\n\n\nEtter dette er man logget inn i et Jupyter-miljø som kjører på en minimal Ubuntu-maskin. Hvis man er del av et Dapla-team får man også tilgang til alt teamet har tilgang til.\n\n\n\nInnlogging til staging-miljøet er identisk med innloggingen til prod-miljøet, med ett viktig unntak: nettadressen er nå https://jupyter.dapla-staging.ssb.no/.\nLitt mer om hva som er tilgjenglig her kommer.\n\n\n\n\n\nNoen ganger kan man komme i en situasjon hvor løsningen for Single Sign-On (pålogging på tvers av flere systemer) gir en feilmelding a la Figur 7:\n\n\n\nFigur 7: Feil som kan oppstå ved pålogging\n\n\nI denne situasjonen må man trykke på knappen “Add to existing account”. Da vil skjermbildet Figur 8 dukke opp:\n\n\n\nFigur 8: Klikk på Google-knappen for å logge på igjen\n\n\nHer må man tykke på Google-knappen (se pil), og deretter logge inn som vist i Figur 3 tidligere i dette avsnittet."
  },
  {
    "objectID": "statistikkere/innlogging.html#bakkemiljøet",
    "href": "statistikkere/innlogging.html#bakkemiljøet",
    "title": "Innlogging",
    "section": "",
    "text": "Jupyter-miljøet på bakken bruker samme base-image1 for å installere Jupyterlab, og er derfor identisk på mange måter. Men innloggingen er ganske forskjellig.\n\n\n\n\n\n\nNote\n\n\n\nFom. 5. desember 2022 har vi byttet ut Jupyter-miljøet på bakken. Beskrivelsene under gjelder derfor det nye miljøet. Fram til 15. januar vil du kunne bruke det gamle miljøet ved å gå inn på lenken https://jupyter-prod.ssb.no/ manuelt i Google Chrome. Etter 15. januar blir det gamle Jupyter-miljøet avviklet.\n\n\n\n\nDu logger deg inn på prod i bakkemiljøet på følgende måte:\n\nLogg deg inn på Citrix-Windows i bakkemiljøet. Det kan gjøres ved å bruke lenken Citrix på Byrånettet, som også vises i Figur 1.\nTrykk på Jupyterlab-ikonet, som vist på Figur 9, og logg deg inn med vanlig brukernavn og passord.\n\n\n\n\nFigur 9: Jupyterlab-ikon på Skrivebordet i Citrix-Windows.\n\n\nNår du trykker på ikonet blir du tatt til nettadressen https://sl-jupyter-p.ssb.no/. Du kunne også åpnet Jupyterlab ved åpne Chrome-nettleseren og skrive inn adressen manuelt.\n\n\n\nInnlogging til staging-miljøet har ingen snarvei på Skrivebordet, og du må gjøre følgende for å åpne miljøet:\n\nÅpne Chrome-nettleseren i Citrix-Windows.\nSkriv inn url-en https://sl-jupyter-t.ssb.no/"
  },
  {
    "objectID": "statistikkere/innlogging.html#footnotes",
    "href": "statistikkere/innlogging.html#footnotes",
    "title": "Innlogging",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nHva er base-image?↩︎"
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html",
    "href": "statistikkere/hva-er-dapla-team.html",
    "title": "Hva er Dapla-team?",
    "section": "",
    "text": "For å kunne jobbe på Dapla må man være en del av et Dapla-team. Et Dapla-team er en gruppe personer som har tilgang til spesifikke ressurser på Dapla. Ressursene kan være data, kode eller tjenester. Følgelig er teamet helt sentral for tilgangsstyringen på Dapla. Derfor er det viktig at alle som jobber på Dapla gjør seg godt kjent med innholdet i dette delen."
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#opprette-dapla-team",
    "href": "statistikkere/hva-er-dapla-team.html#opprette-dapla-team",
    "title": "Hva er Dapla-team?",
    "section": "Opprette Dapla-team",
    "text": "Opprette Dapla-team\nDapla-team opprettes av dataeier for teamets kildedata. I de fleste tilfeller vil dette være en seksjonsleder i SSB. Selve opprettelsen av teamet gjøres i Manager-portalen.\n\n\n\n\n\n\nManager-portalen er under arbeid\n\n\n\nManager-portalen er under arbeid og vil være klar i 1. kvartal 2024. I mellomtiden kan man bruke den gamle portalen for å opprette Dapla-team."
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#roller-i-teamet",
    "href": "statistikkere/hva-er-dapla-team.html#roller-i-teamet",
    "title": "Hva er Dapla-team?",
    "section": "Roller i teamet",
    "text": "Roller i teamet\nMedlemskap i et Dapla-team gir tilgang på spesifikke ressurser på Dapla. Men siden kildedataene til alle team er klassifisert som sensitive, så kan ikke alle på teamet ha lik tilgang til alle ressurser. Av den grunn er det definert 3 ulike roller på et team. To av disse, data-admins og developers, er forbeholdt de som jobber med data på teamet. Mens den tredje, managers, skal innehas av de som er ansvarlige for teamet. I de fleste tilfeller vil managers være seksjonslederen som er ansvarlige for statistikkproduktene teamet leverer. Under forklarer vi nærmere hva de ulike rollene innebærer.\n\nManagers\nRollen managers skal bestå av en eller flere data-ansvarlige (ofte omtalt som data-eiere eller seksjonsledere). managers har ansvar for følgende i teamet:\n\nopprette teamet\nhvem i teamet som får tilgang til hvilke data og tjenester.\nat teamet følger SSBs retningslinjer for tilgangsstyring.\nat teamet følger SSBs retningslinjer for klassifisering av data.\nvedlikehold og monitorering av tilganger.\nat teamet følger og forstår hvordan sensitive data skal behandles i SSB.\n\nManager-rollen krever ingen tilgang til data eller databehandlende tjenester på Dapla.\n\n\nData-admins\nRollen data-admins er en priveligert rolle blant de som jobber med data i teamet. Rollen skal kun tildeles 2-3 personer på et team. data-admins har tilgang til samme data og ressurser som developers, med følgende unntak:\n\nde er forhåndsgodkjent til å gi seg selv tidsbegrenset tilgang til kildedata ved behov. Tilgang til kildedata skal kun aktiveres i særskilte tilfeller, der eneste løsning er å se på data i klartekst. Tilgang til kildedata skal kun gis i en begrenset periode, og krever en skriftlig begrunnelse. managers skal lett kunne monitere hvem som aktiverer denne tilgangen og hvor ofte.\nde kan godkjenne endringer i automatiske jobber som prosesserer kildedata til inndata.\nde kan overføre kildedata mellom bakke og sky.\n\n\n\nDevelopers\nRollen developers er den mest vanlige rollen på et Dapla-team. Denne rollen skal tildeles alle som jobber med data i teamet som ikke har data-admins-rollen. developers har tilgang til følgende ressurser:\n\nalt av teamets data, med unntak av kildedata.\nalle ressurser som behandler datatilstandene fra inndata til utdata."
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#ressurser",
    "href": "statistikkere/hva-er-dapla-team.html#ressurser",
    "title": "Hva er Dapla-team?",
    "section": "Ressurser",
    "text": "Ressurser\nNår du oppretter et dapla-team så får man en grunnpakke med ressurser som de fleste i SSB vil trenge for å kunne jobbe med data på Dapla. I tillegg kan teamet selvbetjent skru på andre tjenester hvis man ønsker det. I det følgende forklarer vi hva som er inkludert i grunnpakken, og hva som er tilgjengelig for å skru på ved behov.\n\nGrunnpakken\nFigur 1 viser et overordnet bilde av hvilke ressurser som er inkludert i “grunnpakken”. Et Dapla-team får et testmiljø og prodmiljø. Det er i prodmiljøet at man jobber med skarpe data, mens testmiljøet er forbeholdt arbeid med testdata. I hvert miljø får teamet to Google-prosjekter. Ett for kildedata og et for datatilstandene inndata, klargjorte data, statistikkdata og utdata. Sistnevnte prosjekt kaller vi for standardprosjektet, siden det er her mesteparten av databehandlingen skjer.\n\n\n\nFigur 1: Diagram over hvilke miljøer, Google-prosjekter og bøtter et Dapla-team som et får ved opprettelse.\n\n\nAv Figur 1 ser vi at prosjektene i prodmiljøet får noen flere bøtter enn prosjektene i testmiljøet. Disse ekstrabøttene er forbeholdt synkronisering av data mellom bakke og sky, noe vi ikke legge til rette for i testmiljøet1. Les mer om overføring av data mellom bakke og sky her.\nRessursene som opprettes for et Dapla-team reflekterer i stor grad at kildedata er klassifisert som sensitive. Dette er grunnen til at det opprettes et eget prosjekt for kildedata, og at det kun er data-admins som potensielt kan få tilgang til dataene her. Opprettelsen av et eget testmiljø skyldes at Dapla-team i større grad enn før forventes å jobbe med testdata istedenfor skarpe data.\nAlle ressursene som opprettes for teamet er definert i tekstfiler i et GitHub-repo. Dette repoet kaller vi for et IaC-repo (Infrastructure as Code). IaC-repoet er en del av grunnpakken, og er tilgjengelig for alle på teamet. Statistikkere trenger ikke å forholde seg til dette repoet i stor grad, med unntak av når de skal aktivere/deaktivere features og når de skal sette opp Kildomaten.\n\n\nFeatures\nI tillegg til grunnpakken med ressurser, så kan teamet selvbetjent skru på følgende features eller tjenester ved behov:\n\nTransfer Service kan skrus på hvis teamet trenger å synkronisere data mellom ulike lagringssystemer. For eksempel mellom bakke og sky, eller mellom to ulike skytjenester.\nKildomaten kan skrus på hvis teamet trenger å automatisere overgangen fra kildedata til inndata.\n\nForeløpig er det kun disse to features som er tilgjengelig. Det vil komme flere etterhvert som behovene melder seg.\nLes mer om features her."
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#github-team",
    "href": "statistikkere/hva-er-dapla-team.html#github-team",
    "title": "Hva er Dapla-team?",
    "section": "GitHub-team",
    "text": "GitHub-team\nVed opprettelsen av et Dapla-team så blir det også opprettet et tilsvarende GitHub-team med samme navn som Dapla-teamet. Grunnen til at det blir opprettet et GitHub-team er at GitHub er en sentral del av Dapla. Alle ressurser som skal opprettes på plattformen defineres av GitHub-repoer, og vi ønsker at tilganger her også skal reflektere tilgangene på Dapla.\nFor eksempel vil et team med navnet dapla-example få et GitHub-team med navnet dapla-example. Alle som er medlem av Dapla-teamet vil automatisk bli medlem av GitHub-teamet. I tillegg vil gruppetilhørighet og tilgangsroller på GitHub-teamet reflektere tilgangsroller på Dapla-teamet. For eksempel så kan dapla-example-data-admins gis tilgang til repo, og da vil alle som er medlem av Dapla-teamet med rollen data-admins få tilgang til repoet. Dette benyttes blant annet for å gi teamet tilgang til automation-mappen i sitt IaC-repo. I tillegg kan teamet bruke GitHub-teamet til å gi tilgang til andre GitHub-repoer som er relevante for teamet, for eksempel kodenbasen til en statistikkproduksjon eller lignende. Fordelen er at tilganger er gitt på teamnivå og ikke på personnivå. For eksempel hvis manager for teamet fjerner en ansatt fra developers-gruppa, så mister de all tilgang til data, tjenester og kode på GitHub som er tilgjengelig for developers."
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#navnestruktur",
    "href": "statistikkere/hva-er-dapla-team.html#navnestruktur",
    "title": "Hva er Dapla-team?",
    "section": "Navnestruktur",
    "text": "Navnestruktur\nNår du oppretter et Dapla-team så må du velge et navn på teamet. Teamet velger selv et navn som reflekterer domene og subdomene. For eksempel kan et team som jobber med statistikkproduksjonen skattestatistikk for næringslivet velge å kalle teamet Skatt næring. Hvis vi bruker dette teamet som et eksempel, så vil det få opprettet et teknisk navn som følger denne strukturen: skatt-naering. Dette navnet er det som brukes i tekniske sammenhenger, for eksempel som navn på GitHub-teamet, IaC-repoet, Google-prosjektene og bøttene. Tabell 1 viser en tabell over hvordan ressursene for dette teamet vil se ut:\n\n\nTabell 1: Navnestruktur for teamet Skatt næring sine ressurser\n\n\n\n\n\n\nNavn\nBeskrivelse\n\n\n\n\nskatt-naering\nTeknisk teamnavn\n\n\nskatt-naering-managers\nAD-gruppe for managers\n\n\nskatt-naering-data-admins\nAD-gruppe for data-admins og et GitHub-team\n\n\nskatt-naering-developers\nAD-gruppe for developers og et GitHub-team\n\n\nskatt-naering-kilde-p\nNavn på kildeprosjekt i prod\n\n\nskatt-naering-p\nNavn på standardprosjekt i prod\n\n\nskatt-naering-kilde-t\nNavn på kildeprosjekt i test\n\n\nskatt-naering-t\nNavn på standardprosjekt i test\n\n\n\n\nI Tabell 1 ser vi at teamet får opprettet 3 AD-grupper og 4 Google-prosjekter. AD-gruppene brukes til å gi tilgang til ressursene på Dapla, mens Google-prosjektene brukes til å organisere ressursene. I tillegg er det en fast navnestruktur for bøttene i hvert prosjekt, slikt som vist i Tabell 2.\n\n\nTabell 2: Navnestruktur for teamet Skatt næring sine bøtter\n\n\nProsjektnavn\nBøttenavn\n\n\n\n\nskatt-naering-kilde-p\nssb-skatt-naering-data-kilde-prod\n\n\n\nssb-skatt-naering-data-kilde-frasky-prod\n\n\n\nssb-skatt-naering-data-kilde-tilsky-prod\n\n\nskatt-naering-p\nssb-skatt-naering-data-prod\n\n\n\nssb-skatt-naering-data-frasky-prod\n\n\n\nssb-skatt-naering-data-tilsky-prod\n\n\nskatt-naering-kilde-t\nssb-skatt-naering-data-kilde-test\n\n\nskatt-naering-t\nssb-skatt-naering-data-test"
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#footnotes",
    "href": "statistikkere/hva-er-dapla-team.html#footnotes",
    "title": "Hva er Dapla-team?",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nTa kontakt med produkteier for Dapla hvis du trenger å synkronisere testdata mellom bakke og sky↩︎"
  },
  {
    "objectID": "statistikkere/introduksjon.html",
    "href": "statistikkere/introduksjon.html",
    "title": "Introduksjon",
    "section": "",
    "text": "Introduksjon\nMålet med dette kapittelet er å gi en grunnleggende innføring i hva som legges i ordet Dapla. I tillegg gis en forklaring på hvorfor disse valgene er tatt."
  },
  {
    "objectID": "statistikkere/kildomaten.html",
    "href": "statistikkere/kildomaten.html",
    "title": "Kildomaten",
    "section": "",
    "text": "Kildomaten er en tjeneste for å automatisere overgangen fra kildedata til inndata. Tjenesten lar statistikkere kjøre sine egne skript automatisk på alle nye filer i kildedatabøtta og skrive resultatet til produktbøtta. Formålet med tjenesten er minimere behovet for tilgang til kildedata samtidig som teamet selv bestemmer hvordan transformasjonn til inndata skal foregå. Statistikkproduksjon kan da starte i en tilstand der dataminimering og pseudonymisering allerede er gjennomført.\nProsessering som skal skje i overgangen fra kildedata til inndata har SSB definert til å være:\n\nDataminimering:\nFjerne alle felter som ikke er strengt nødvendig for å produsere statistikk.\nPseudonymisering:\nPseudonymisering av personidentifiserende data.\nKodeverk:\nLegge på standard kodeverk fra for eksempel Klass.\n\nStandardisering:\nTegnsett, datoformat, etc. endres til SSBs standardformat.\n\nUnder forklarer vi nærmere hvordan man bruker tjenesten. Da forutsetter vi at du har et Dapla-team med tjenesten er aktivert. les mer om hvordan du aktiverer tjenester her (lenker her).\n\n\nFør et Dapla-team kan ta i bruk Kildomaten må man koble teamets IaC-repo1 på GitHub, til kildeprosjektet slik som beskrevet her. Det er en engangsjobb som må gjøre av teamets data-admin.\n\n\n\nI denne delen bryter vi ned prosessen med å sette opp Kildomaten i de stegene vi mener det er hensiktsmessig å gjøre det når den settes opp for første gang på en enkeltkilde. Senere i kapitlet forklarer vi hvordan man setter den opp flere kilder og hvordan man vedlikeholder tjenesten.\nHusk at alle på teamet kan gjøre det meste av arbeidet her, men det er data-admins som må godkjenne at tjenesten rulles ut2.\n\n\nSiden vi skal endre på teamets IaC-repo så må vi først klone ned repoet. Man finner teamets IaC-repo ved gå inn på SSBs GitHub-organisasjon og søke etter repoet som heter &lt;teamnavn&gt;-iac. Når du har funnet repoet så kan du gjøre følgende:\n\nKlon ned ditt teams Iac-repo git clone &lt;repo-url&gt;\nOpprett en ny branch: git checkout -b add-kildomaten-source\n\n\n\n\nFor at Kildomaten skal fungere så må vi følge en bestemt mappestruktur i IaC-repoet. Denne strukturen er nødvendig for at tjenesten skal vite hvor den skal hente kildedata fra, og hvor den skal legge inndata. Denne strukturen blir ikke laget ved opprettelsen av teamet, siden ikke alle team kommer til å bruke tjenesten. Vi kan derfor starte med å opprette denne strukturen. Den skal se slik ut:\n\n\ngithub.com/statisticsnorway/&lt;teamnavn&gt;-iac\n\ndapla-example-iac\n├── automation/\n│   └── source_data/\n│       ├── prod/\n│       │   \n│       └── test/\n│...           \n\nHusk at du kun skal opprette mappen som heter automation og dets undermapper. Resten av mappene skal allerede være opprettet og du skal ikke endre de.\n\n\n\n\n\n\nPass deg for ipynb_checkpoints/-mappen\n\n\n\nHvis du jobber i Notebooks så vil det automatisk genereres mapper som heter ipynb_checkpoints som håndterer autosave. Hvis disse blir pushet til teamets IaC-repoet vil det trolig føre til at noe feiler. Pass på at det ikke ligger noen slike mapper under automation/ ved å gjøre git status før du stager endringene dine.\n\n\n\n\n\nSiden tjenesten kan trigges på filer som skrives til undermapper i en bøtte, og ulike undermapper kan trigge ulike python-skript, så er det viktig å tenke gjennom hvordan man organiserer filer i bøtta.\n\n\n\n\nBøtte\n\nssb-dapla-example-data-kilde-prod/\n├── ledstill/\n│   └── altinn/\n│   └── aordningen/\n├── sykefra/\n│   └── altinn/\n│   └── freg/\n│\n│\n│\n       \n\n\n\n\n\n\nIaC-repo\n\ndapla-example-iac\n├── automation/\n│   └── source_data/\n│       ├── prod/\n│       │   └── ledstill/\n│       │       └── altinn/\n│       │       └── aordningen/\n│       │   └── sykefra/\n│       │              \n│       └── test/\n│          \n\n\n\nLa oss anta at et team har organisert dataene i kildebøtta si på en måten vi ser til venstre over. De ønsker å sette opp kildomaten slik at alle filer som dukker opp i gs://ssb-dapla-exmaple-data-kilde-prod/ledstill/altinn/ skal prosesseres med et skript. Da må de opprette mappene automation/source_data/prod/ledstill/altinn/ i IaC-repoet sitt også. Og dette blir mappen hvor vi skal legge python-scriptet og en konfigurasjonsfil for å prosessere filene som kommer inn.\n\n\n\nFor å konfigurere tjenesten så legger vi en fil ved navn config.yaml under automation/source_data/prod/ledstill/altinn/. Denne filen skal inneholde informasjon om hvilken mappe i kildebøtta som skal trigge tjenesten, og hvor mye ram du ønsker på maskinene som kjører. Filen skal se slik ut:\n\n\nconfig.yaml\n\nfolder_prefix: ledstill/altinn\nmemory_size: 1  # GiB\n\n\n\n\nPython-skriptet er der brukeren kan angi hva slags kode som skal kjøre på hver fil som dukker opp i mappen. For at dette skal være mulig må koden følge disse reglene:\n\nKoden må ligge i en fil som heter process_source_data.py.\nKoden må pakkes inn i en funksjon som heter main(). main() er funksjon med et parameter som heter source_file. Tjenesten gir deg verdien for source_file og du kan bare kode uten å tenke på å definere den.\n\nHvis vi fortsetter eksempelet fra tidligere så ser mappen i IaC-repoet vårt slik ut nå:\n\n\nIaC-repo\n\ndapla-example-iac\n├── automation/\n│   └── source_data/\n│       ├── prod/\n│       │   └── ledstill/\n│       │       └── altinn/\n│       │           └── config.yaml\n│       │           └── process_source_data.py\n│       │       └── aordningen/\n│       │   └── sykefra/\n│       │              \n│       └── test/\n│          \n\nVi ser nå at filene config.yaml og process_source_data.py ligger i mappen automation/source_data/prod/ledstill/altinn/. Senere, når vi har rullet ut tjenesten, vil en ny fil i gs://ssb-dapla-exmaple-data-kilde-prod/ledstill/altinn/ trigge kildomaten og kjøre koden i process_source_data.py på filen.\nUnder ser du et eksempel på hvordan en vanlig kodesnutt kan konverteres til å kjøre i Kildomaten:\n\n\n\n\nVanlig kode\n\nimport dapla as dp\n\n# Stien til filen\nsource_file = \"gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv\"\n\ndf = dp.read_pandas(source_file, file_type=\"csv\")\n\n# Dataminimerer ved å velge kolonner\ndf2 = df[['col1', 'col2', 'col3']]\n\n# Skriver ut fil til produktbøtta\ndp.write_pandas(df2, \n            \"gs://ssb-dapla-example-data-produkt-prod/test20231010.parquet\")\n\n\n\n\n\n\nKode i Kildomaten\n\nimport dapla as dp\n\ndef main(source_file):\n    df = dp.read_pandas(source_file, file_type=\"csv\")\n\n# Dataminimerer ved å velge kolonner\n    df2 = df[['col1', 'col2', 'col3']]\n\n# Bytter ut \"kilde\" med \"produkt\" i filstien\n    new_path = source_file.replace(\"kilde\", \"produkt\")\n\n# Bytter ut \".csv\" med \".parquet\" i filstien\n    new_path2 = new_path.replace(\".csv\", \".parquet\")\n\n# Skriver ut fil til produktbøtta\n    dp.write_pandas(df2, new_path2)\n\n\n\nDe to kodesnuttene over gir det samme resultatet, bare at koden til venstre kjøres som vanlig python-kode, mens koden til høyre kjøres i Kildomaten. Som vi ser av koden til høyre så trenger vi aldri å hardkode inn filstier i Kildomaten. Tjenesten gir oss filstien, og vi kan bruke den til å skrive ut filen til produktbøtta. Strukturen på filene som skrives bør tenkes nøye gjennom når man automatiseres prosessering av data. Hvis vi ikke har unike filnavn eller stier så kan du risikere at filer blir overskrevet. Typisk er dette allerede tenkt på når filer skrives til kildebøtta, så hvis man kopierer den strukturen, slik vi gjorde i eksempelet over, så vil det ikke være noe problem.\n\n\n\nFør man ruller ut koden i tjenesten er det greit å teste at alt fungerer som det skal i Jupyterlab. Hvis vi tar utgangspunkt i eksempelet over så kan vi teste koden ved å kjøre følgende nederst i skriptet:\n\n\nprocess_source_data.py\n\nmain(\"gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv\")\n\nNår tjenesten er rullet ut så vil det være dette som kjøres når en fil dukker opp i gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/. Ved å kjøre det manuelt på denne måten får vi sett at ting fungerer som det skal.\nHusk å fjerne kjøringen av koden før du ruller ut tjenesten.\n\n\n\nFor å rulle ut tjenesten gjør du følgende;\n\nPush branchen til GitHub og opprette en pull request.\nPull request må godkjennes av en data-admins.\n\n\n\n\nNår pull request er godkjent så sjekker du om alle testene er vellykket, slik som vist i Figur 1.\nDeretter skriver du atlantis applyi kommentarfeltet til pull requesten. Vent til den er ferdig kjørt før du går til neste steg.\n\n\n\n\n\n\nFigur 1: Suksessfulle tester på GitHub\n\n\n\n\n\nMerge branchen in i main-branchen.\n\n\n\n\n\ndkjfkl"
  },
  {
    "objectID": "statistikkere/kildomaten.html#forberedelser",
    "href": "statistikkere/kildomaten.html#forberedelser",
    "title": "Kildomaten",
    "section": "",
    "text": "Før et Dapla-team kan ta i bruk Kildomaten må man koble teamets IaC-repo1 på GitHub, til kildeprosjektet slik som beskrevet her. Det er en engangsjobb som må gjøre av teamets data-admin."
  },
  {
    "objectID": "statistikkere/kildomaten.html#sette-opp-tjenesten",
    "href": "statistikkere/kildomaten.html#sette-opp-tjenesten",
    "title": "Kildomaten",
    "section": "",
    "text": "I denne delen bryter vi ned prosessen med å sette opp Kildomaten i de stegene vi mener det er hensiktsmessig å gjøre det når den settes opp for første gang på en enkeltkilde. Senere i kapitlet forklarer vi hvordan man setter den opp flere kilder og hvordan man vedlikeholder tjenesten.\nHusk at alle på teamet kan gjøre det meste av arbeidet her, men det er data-admins som må godkjenne at tjenesten rulles ut2.\n\n\nSiden vi skal endre på teamets IaC-repo så må vi først klone ned repoet. Man finner teamets IaC-repo ved gå inn på SSBs GitHub-organisasjon og søke etter repoet som heter &lt;teamnavn&gt;-iac. Når du har funnet repoet så kan du gjøre følgende:\n\nKlon ned ditt teams Iac-repo git clone &lt;repo-url&gt;\nOpprett en ny branch: git checkout -b add-kildomaten-source\n\n\n\n\nFor at Kildomaten skal fungere så må vi følge en bestemt mappestruktur i IaC-repoet. Denne strukturen er nødvendig for at tjenesten skal vite hvor den skal hente kildedata fra, og hvor den skal legge inndata. Denne strukturen blir ikke laget ved opprettelsen av teamet, siden ikke alle team kommer til å bruke tjenesten. Vi kan derfor starte med å opprette denne strukturen. Den skal se slik ut:\n\n\ngithub.com/statisticsnorway/&lt;teamnavn&gt;-iac\n\ndapla-example-iac\n├── automation/\n│   └── source_data/\n│       ├── prod/\n│       │   \n│       └── test/\n│...           \n\nHusk at du kun skal opprette mappen som heter automation og dets undermapper. Resten av mappene skal allerede være opprettet og du skal ikke endre de.\n\n\n\n\n\n\nPass deg for ipynb_checkpoints/-mappen\n\n\n\nHvis du jobber i Notebooks så vil det automatisk genereres mapper som heter ipynb_checkpoints som håndterer autosave. Hvis disse blir pushet til teamets IaC-repoet vil det trolig føre til at noe feiler. Pass på at det ikke ligger noen slike mapper under automation/ ved å gjøre git status før du stager endringene dine.\n\n\n\n\n\nSiden tjenesten kan trigges på filer som skrives til undermapper i en bøtte, og ulike undermapper kan trigge ulike python-skript, så er det viktig å tenke gjennom hvordan man organiserer filer i bøtta.\n\n\n\n\nBøtte\n\nssb-dapla-example-data-kilde-prod/\n├── ledstill/\n│   └── altinn/\n│   └── aordningen/\n├── sykefra/\n│   └── altinn/\n│   └── freg/\n│\n│\n│\n       \n\n\n\n\n\n\nIaC-repo\n\ndapla-example-iac\n├── automation/\n│   └── source_data/\n│       ├── prod/\n│       │   └── ledstill/\n│       │       └── altinn/\n│       │       └── aordningen/\n│       │   └── sykefra/\n│       │              \n│       └── test/\n│          \n\n\n\nLa oss anta at et team har organisert dataene i kildebøtta si på en måten vi ser til venstre over. De ønsker å sette opp kildomaten slik at alle filer som dukker opp i gs://ssb-dapla-exmaple-data-kilde-prod/ledstill/altinn/ skal prosesseres med et skript. Da må de opprette mappene automation/source_data/prod/ledstill/altinn/ i IaC-repoet sitt også. Og dette blir mappen hvor vi skal legge python-scriptet og en konfigurasjonsfil for å prosessere filene som kommer inn.\n\n\n\nFor å konfigurere tjenesten så legger vi en fil ved navn config.yaml under automation/source_data/prod/ledstill/altinn/. Denne filen skal inneholde informasjon om hvilken mappe i kildebøtta som skal trigge tjenesten, og hvor mye ram du ønsker på maskinene som kjører. Filen skal se slik ut:\n\n\nconfig.yaml\n\nfolder_prefix: ledstill/altinn\nmemory_size: 1  # GiB\n\n\n\n\nPython-skriptet er der brukeren kan angi hva slags kode som skal kjøre på hver fil som dukker opp i mappen. For at dette skal være mulig må koden følge disse reglene:\n\nKoden må ligge i en fil som heter process_source_data.py.\nKoden må pakkes inn i en funksjon som heter main(). main() er funksjon med et parameter som heter source_file. Tjenesten gir deg verdien for source_file og du kan bare kode uten å tenke på å definere den.\n\nHvis vi fortsetter eksempelet fra tidligere så ser mappen i IaC-repoet vårt slik ut nå:\n\n\nIaC-repo\n\ndapla-example-iac\n├── automation/\n│   └── source_data/\n│       ├── prod/\n│       │   └── ledstill/\n│       │       └── altinn/\n│       │           └── config.yaml\n│       │           └── process_source_data.py\n│       │       └── aordningen/\n│       │   └── sykefra/\n│       │              \n│       └── test/\n│          \n\nVi ser nå at filene config.yaml og process_source_data.py ligger i mappen automation/source_data/prod/ledstill/altinn/. Senere, når vi har rullet ut tjenesten, vil en ny fil i gs://ssb-dapla-exmaple-data-kilde-prod/ledstill/altinn/ trigge kildomaten og kjøre koden i process_source_data.py på filen.\nUnder ser du et eksempel på hvordan en vanlig kodesnutt kan konverteres til å kjøre i Kildomaten:\n\n\n\n\nVanlig kode\n\nimport dapla as dp\n\n# Stien til filen\nsource_file = \"gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv\"\n\ndf = dp.read_pandas(source_file, file_type=\"csv\")\n\n# Dataminimerer ved å velge kolonner\ndf2 = df[['col1', 'col2', 'col3']]\n\n# Skriver ut fil til produktbøtta\ndp.write_pandas(df2, \n            \"gs://ssb-dapla-example-data-produkt-prod/test20231010.parquet\")\n\n\n\n\n\n\nKode i Kildomaten\n\nimport dapla as dp\n\ndef main(source_file):\n    df = dp.read_pandas(source_file, file_type=\"csv\")\n\n# Dataminimerer ved å velge kolonner\n    df2 = df[['col1', 'col2', 'col3']]\n\n# Bytter ut \"kilde\" med \"produkt\" i filstien\n    new_path = source_file.replace(\"kilde\", \"produkt\")\n\n# Bytter ut \".csv\" med \".parquet\" i filstien\n    new_path2 = new_path.replace(\".csv\", \".parquet\")\n\n# Skriver ut fil til produktbøtta\n    dp.write_pandas(df2, new_path2)\n\n\n\nDe to kodesnuttene over gir det samme resultatet, bare at koden til venstre kjøres som vanlig python-kode, mens koden til høyre kjøres i Kildomaten. Som vi ser av koden til høyre så trenger vi aldri å hardkode inn filstier i Kildomaten. Tjenesten gir oss filstien, og vi kan bruke den til å skrive ut filen til produktbøtta. Strukturen på filene som skrives bør tenkes nøye gjennom når man automatiseres prosessering av data. Hvis vi ikke har unike filnavn eller stier så kan du risikere at filer blir overskrevet. Typisk er dette allerede tenkt på når filer skrives til kildebøtta, så hvis man kopierer den strukturen, slik vi gjorde i eksempelet over, så vil det ikke være noe problem.\n\n\n\nFør man ruller ut koden i tjenesten er det greit å teste at alt fungerer som det skal i Jupyterlab. Hvis vi tar utgangspunkt i eksempelet over så kan vi teste koden ved å kjøre følgende nederst i skriptet:\n\n\nprocess_source_data.py\n\nmain(\"gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv\")\n\nNår tjenesten er rullet ut så vil det være dette som kjøres når en fil dukker opp i gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/. Ved å kjøre det manuelt på denne måten får vi sett at ting fungerer som det skal.\nHusk å fjerne kjøringen av koden før du ruller ut tjenesten.\n\n\n\nFor å rulle ut tjenesten gjør du følgende;\n\nPush branchen til GitHub og opprette en pull request.\nPull request må godkjennes av en data-admins.\n\n\n\n\nNår pull request er godkjent så sjekker du om alle testene er vellykket, slik som vist i Figur 1.\nDeretter skriver du atlantis applyi kommentarfeltet til pull requesten. Vent til den er ferdig kjørt før du går til neste steg.\n\n\n\n\n\n\nFigur 1: Suksessfulle tester på GitHub\n\n\n\n\n\nMerge branchen in i main-branchen."
  },
  {
    "objectID": "statistikkere/kildomaten.html#vedlikehold",
    "href": "statistikkere/kildomaten.html#vedlikehold",
    "title": "Kildomaten",
    "section": "",
    "text": "dkjfkl"
  },
  {
    "objectID": "statistikkere/kildomaten.html#footnotes",
    "href": "statistikkere/kildomaten.html#footnotes",
    "title": "Kildomaten",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nIaC står for Infrastructure-as-Code. Det er et repo på GitHub som definerer alle ressursene som er tilgjengelig for et team på Dapla. Hvert Dapla-team har sitt eget IaC-repo. Skal man endre eller legge til ressurser for team, så endre man koden i IaC-repoet, og deretter ruller det ut på plattformen. Les mer om IaC-repoer i SSB her.↩︎\nI tillegg er det data-admins som må teste tjenesten manuelt hvis det gjøres på skarpe data, siden det kun er data-admins som kan få tilgang til de dataene.↩︎"
  },
  {
    "objectID": "statistikkere/gcp-overview.html",
    "href": "statistikkere/gcp-overview.html",
    "title": "Google Cloud Platform (GCP)",
    "section": "",
    "text": "Google Cloud Platform (GCP)"
  },
  {
    "objectID": "statistikkere/ssbproject.html",
    "href": "statistikkere/ssbproject.html",
    "title": "SSB-project",
    "section": "",
    "text": "SSB-project\nI forrige del forklarte vi hvordan man jobber med skarpe data på Dapla. Det neste steget vil ofte være å begynne å utvikle kode i Python og/eller R. Dette innebærer at man helst skal:\n\nversjonshåndtere med Git\nopprette et GitHub-repo\nopprette et virtuelt miljø som husker hvilke versjoner av pakker og programmeringsspråk du brukte\n\nI tillegg må alt dette konfigureres for hvordan SSB sine systemer er satt opp. Dette har vist seg å være unødvendig krevende for mange. Team Statistikktjenester har derfor utviklet et program som gjør alt dette for deg på en enkel måte som heter ssb-project.\nVi mener at ssb-project er et naturlig sted å starte når man skal bygge opp koden i Python eller R. Det gjelder både på bakken og på sky. I denne delen av boken forklarer vi først hvordan du bruker ssb-project i det første kapittelet. Siden programmet skjuler mye av kompleksiteten rundt dette, så bruker vi de andre kapitlene til å forklare hvordan man ville satt opp dette uten hjelp av programmet. Dermed vil det være lett for SSB-ansatte å skjønne hva som gjøres og hvorfor det er nødvendig.\nHer kan du kan lese mer om hvordan et SSB-project kan opprettes."
  },
  {
    "objectID": "statistikkere/virtual-env.html",
    "href": "statistikkere/virtual-env.html",
    "title": "Virtuelle miljøer",
    "section": "",
    "text": "Et python virtuelt miljø inneholder en spesifikk versjon av python og et sett med pakker. Pakkene er kun tilgjengelige når det virtuelle miljøet er aktivert. Dette gjør at man ungår avhengighetskonflikter på tvers av prosjekter.\nSe her for mer informasjon om virtuelle miljøer.\n\n\nDet er anbefalt å benytte verktøyet poetry for å administrere prosjekter og deres virtuelle miljø.\nPoetry setter opp virtuelt miljø, gjør det enkelt å oppdatere avhengigheter, sette versjonsbegrensninger og reprodusere prosjektet.\nPoetry gjør dette ved å lagre avhengigheters eksakte versjon i prosjektets “poetry.lock”. Og eventuelle begrensninger i “pyproject.toml”. Dette gjør det enkelt for andre å bygge prosjektet med akkurat de samme pakkene og begrensningene."
  },
  {
    "objectID": "statistikkere/virtual-env.html#python",
    "href": "statistikkere/virtual-env.html#python",
    "title": "Virtuelle miljøer",
    "section": "",
    "text": "Et python virtuelt miljø inneholder en spesifikk versjon av python og et sett med pakker. Pakkene er kun tilgjengelige når det virtuelle miljøet er aktivert. Dette gjør at man ungår avhengighetskonflikter på tvers av prosjekter.\nSe her for mer informasjon om virtuelle miljøer.\n\n\nDet er anbefalt å benytte verktøyet poetry for å administrere prosjekter og deres virtuelle miljø.\nPoetry setter opp virtuelt miljø, gjør det enkelt å oppdatere avhengigheter, sette versjonsbegrensninger og reprodusere prosjektet.\nPoetry gjør dette ved å lagre avhengigheters eksakte versjon i prosjektets “poetry.lock”. Og eventuelle begrensninger i “pyproject.toml”. Dette gjør det enkelt for andre å bygge prosjektet med akkurat de samme pakkene og begrensningene."
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html",
    "href": "statistikkere/jobbe-med-kode.html",
    "title": "Jobbe med kode",
    "section": "",
    "text": "På Dapla jobber vi med utvikling av Python- og R-kode i et Jupyter-miljø. For de som ønsker det, er det mulig å enkelt åpne en notebook med en av våre forhåndskonfigurerte kernels1. Man kan umiddelbart begynne å skrive kode og deretter lagre den i det lokale filsystemet. Dette er ideelt for enkel datautforskning eller for pedagogiske formål.\nNår koden skal settes i produksjon, er det essensielt å ta hensyn til følgende:\n\nResultater bør være reproduserbare.\nKoden må kunne deles med andre.\nKoden bør være organisert slik at den er gjenkjennelig for kollegaer.\n\nFor å lette etterlevelsen av beste praksis for kodeutvikling på Dapla, har vi utviklet et verktøy kalt ssb-project. Dette er et CLI-verktøy2 som enkelt lar deg opprette et prosjekt med en standard mappestruktur, et virtuelt miljø og integrasjon med Git for versjonshåndtering. Som en bonus kan det også opprette et GitHub-repositorium for deg ved behov.\nI dette kapitlet vil vi veilede deg gjennom bruken av ssb-project. Du vil lære å opprette et nytt prosjekt, installere pakker, håndtere versjoner med Git, bygge et eksisterende prosjekt og vedlikeholde prosjektet over tid.\n\n\n\n\n\n\nSSB-project støtter ikke R enda\n\n\n\nPer nå støtter SSB-project kun prosjekter skrevet i Python. Dette skyldes begrensninger ved det populære virtuelle miljø-verktøyet for R, renv. Mens renv effektivt håndterer versjoner av R-pakker, har det ikke kapasitet til å ta vare på spesifikke R-installasjonsversjoner. Dette kan potensielt gjøre det mer utfordrende å reprodusere tidligere publiserte resultater ved bruk av ssb-project. Vi arbeider mot en løsning for å inkludere støtte for R i fremtiden."
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#forberedelser",
    "href": "statistikkere/jobbe-med-kode.html#forberedelser",
    "title": "Jobbe med kode",
    "section": "Forberedelser",
    "text": "Forberedelser\nFør du kan ta i bruk ssb-project så er det et par ting som må være på plass:\n\nDu må ha konfigurert Git etter SSB sin standard (les mer om hvordan her).\nHvis du ønsker at ssb-project også skal opprette et GitHub-repo for deg må du også følgende være på plass:\n\nDu må ha en GitHub-bruker (les hvordan her)\nSkru på 2-faktor autentifisering for GitHub-brukeren din (les hvordan her)\nVære koblet mot SSBs organisasjon statisticsnorway på GitHub (les hvordan her)\nOpprette Personal Access Token (PAT) og godkjenne det for bruk mot statisticsnorway (les hvordan her)\n\n\nDet er også å anbefale at du lagrer PAT lokalt slik at du ikke trenger å forholde deg til det når jobber med Git og GitHub. Hvis du har alt dette på plass så kan du bare fortsette å følge de neste kapitlene."
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#opprett-ssb-project",
    "href": "statistikkere/jobbe-med-kode.html#opprett-ssb-project",
    "title": "Jobbe med kode",
    "section": "Opprett ssb-project",
    "text": "Opprett ssb-project\n\n\n\n\n\n\nHar du Github bruker? Noe funksjonalitet i ssb-project krever det. Finn ut hvordan ved å lese forrige kapittel.\n\n\n\nssb-project lar deg opprette en prosjekt-mappe med og uten GitHub-repo. La oss ta for oss hver av alternativene.\n\nUten GitHub-repo\nFor å opprette et nytt ssb-project uten GitHub-repo gjør du følgende:\n\nÅpne en terminal. De fleste vil gjøre dette i Jupyterlab på bakke eller sky og da kan de bare trykke på det blå ➕-tegnet i Jupyterlab og velge Terminal.\nFør vi kjører programmet må vi være obs på at ssb-project vil opprette en ny mappe der vi står. Gå derfor til den mappen du ønsker å ha den nye prosjektmappen. For å opprette et prosjekt som heter stat-testprod så skriver du følgende i terminalen:\n\n\n\nterminal\n\nssb-project create stat-testprod\n\n\n\nHvis du stod i hjemmemappen din på når du skrev inn kommandoen over i terminalen, så har du fått mappestrukturen som vises i Figur 1. 3. Den inneholder følgende :\n\n.git-mappe som blir opprettet for å versjonshåndtere med Git.\nsrc-mappe som skal inneholde all koden som utgjør produksjonsløpet.\ntests-mappe som inneholder tester du skriver for koden din.\nLICENCE-fil som skal benyttes for public-repos i SSB.\npoetry.lock-fil som inneholder alle versjoner av Python-pakker som blir brukt.\npyproject.toml-fil som inneholder informasjon om prosjektet og hvilke pakker som blir brukt.\nREADME.md-fil som brukes for tekstlig innhold på GitHub-siden for prosjektet.\n\n\n\n\n\n\nFigur 1: Mappen som ble opprettet av ssb-project.\n\n\n\n\n\n\nMed Github-repo\nOver så opprettet vi et ssb-project uten å opprette et GitHub-repo. Hvis du ønsker å opprette et GitHub-repo også må du endre kommandoen over til:\n\n\nterminal\n\nssb-project create stat-testprod --github --github-token='blablabla'\n\nKommandoen over oppretter en mappestruktur slik vi så tidligere, men også et ssb-project som heter stat-testprod med et GitHub-repo med samme navn. Som du ser så må vi da sende med opsjonen --github og PAT med opsjonen --github-token='blablabla'. Repoet i GitHub ser da ut som i Figur 2. Hvis du ønsker å slippe måtte forholde deg til PAT hver gang interagerer med GitHub, kan du følge denne beskrivelsen for å lagre den lokalt. Da kan droppe --github-token='blablabla' fra kommandoen over.\n\n\n\nFigur 2: GitHub-repo som er opprettet av ssb-project\n\n\n\n\n\n\n\n\nNår du har opprettet et nytt ssb-project, eller bygget et eksisterende prosjekt, så kan det ta rundt 30 sekunder før kernelen viser seg i Jupterlab-launcher. Vær tålmodig!"
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#installere-pakker",
    "href": "statistikkere/jobbe-med-kode.html#installere-pakker",
    "title": "Jobbe med kode",
    "section": "Installere pakker",
    "text": "Installere pakker\nNår du har opprettet et ssb-project så kan du installere de python-pakkene du trenger fra PyPI. Men før du installerer en pakke bør gjøre følgende for å sikre deg at du ikke installerer en pakke med skadelig kode:\n\nSøk opp pakken på PyPI.\nSjekk om pakken er et populært/velkjent prosjekt ved å besøke repoet der koden ligger. Antall Stars og Forks på gitHub er en grei indikasjon på dette.\nHvis du er i tvil om pakken er trygg å installere, så kan du spørre kollegaer om de har erfaring med den, eller spørre på en egnet Yammer-kanal i SSB.\nHvis du fortsatt ønsker å installere pakken så anbefaler vi å copy-paste navnet fra PyPi, ikke skrive det inn manuelt når du installerer.\n\nSelve installeringen av pakken gjøres enkelt på følgende måte:\n\nÅpne en terminal i Jupyterlab.\nGå inn i prosjektmappen din ved å skrive:\n\n\n\nterminal\n\ncd &lt;sti til prosjektmappe&gt;\n\n\nLag en branch/utviklingsbranch som f.eks. heter install-pandas:\n\n\n\nterminal\n\ngit checkout -b install-pandas\n\n\nInstaller, f.eks. Pandas, ved å skrive følgende:\n\n\n\nterminal\n\npoetry add pandas\n\n\n\n\nFigur 3: Installasjon av Pandas med ssb-project\n\n\nFigur 3 viser hvordan dette vil se ut i en Jupyterlab-terminal. Kommandoen for å installere noe er poetry add etterfulgt av pakkenavnet. Vi ser også at den automatisk legger til Pandas-versjonen i filen poetry.lock.\nDu kan også spesifisere en konkret versjon av pakken som skal installeres med følgende kommando:\n\n\nterminal\n\npoetry add pandas@1.2.3\n\n\nAvinstallere pakker\nOfte eksperimenterer man med nye pakker, og alle blir ikke med videre i produksjonskoden. Det er god praksis å fjerne pakker som ikke brukes, blant annet for å unngå at de blir en sikkerhetsrisiko. Det gjør du enkelt ved å skrive følgende i terminalen:\n\n\nterminal\n\npoetry remove pandas\n\n\n\nOppdatere pakker\nHvis det kommer en ny versjon av en pakke du bruker, så kan du oppdatere den med følgende kommando:\n\n\nterminal\n\npoetry update pandas\n\nhvis du kjører poetry update uten noe pakkenavn, så vil alle pakkene dine oppdateres til siste versjon, med mindre du har spesifisert versjonsbegrensninger i pyproject.toml-filen.\n\n\nUndersøk avhengigheter\nHvis du lurer på hvilke pakker som har hvilke avhengigheter, så kan du lett liste ut dette i terminalen med følgende kommando:\n\n\nterminal\n\npoetry show --tree\n\nDet vil gi en grafisk fremstilling av avhengighetene som vist i Figur 4.\n\n\n\nFigur 4: Visning av pakke-avhengigheter i ssb-project"
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#push-til-github",
    "href": "statistikkere/jobbe-med-kode.html#push-til-github",
    "title": "Jobbe med kode",
    "section": "Push til GitHub",
    "text": "Push til GitHub\nNår du nå har installert en pakke så har filen poetry.lock endret seg. For at dine samarbeidspartnere skal få tilgang til denne endringen i et SSB-project, så må du pushe en ny versjon av poetry.lock-filen opp Github, og kollegaene må pulle ned og bygge prosjektet på nytt. Du kan gjøre dette på følgende måte etter at du har installert en ny pakke:\n\nVi kan stage alle endringer med følgende kommando i terminalen når vi står i prosjektmappen:\n\n\n\nterminal\n\ngit add -A\n\n\nDeretter commite en endring, dvs. ta et stillbilde av koden i dette øyeblikket, ved å skrive følgende:\n\n\n\nterminal\n\ngit commit -m \"Installert pandas\"\n\n\nPush det opp til GitHub4. Anta at vi gjorde dette i branchen install-pandas som ble opprettet tidligere. Da kan vi skrive følgende :\n\n\n\nterminal\n\ngit push --set-upstream origin install-pandas\n\nDeretter kan kollegaene dine pulle ned endringene og bygge prosjektet på nytt. Vi forklarer hvordan man kan bygge prosjektet på nytt senere i kapitlet."
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#dependabot",
    "href": "statistikkere/jobbe-med-kode.html#dependabot",
    "title": "Jobbe med kode",
    "section": "Dependabot",
    "text": "Dependabot\nNår man installerer pakker så vil det etter hvert utvikle seg et sett av potensielt kompliserte avhengigheter mellom disse pakkene. Dette skyldes at en pakke kan benytte seg av funksjonalitet i andre pakker, som igjen benytter seg av funksjonalitet i andre pakker, osv.. Hvis noen finner en sikkerhetssårbarhet i en pakke så kan det fikses ved at en ny versjon av den pakken slippes, som igjen kan få konsekvenser for pakker som er avhengig av denne.\nI SSB er det tilrettelagt for at alle som versjonshåndterer koden sin på GitHub kan skanne pakkene sine for sårbarheter og nye versjoner av pakker med Dependabot. Dependabot hjelper oss med å finne og fikse sårbarheter og gamle pakkeversjoner. Dette er spesielt viktig når man installerer sine egne pakker.\nDependabot sjekker med jevne mellomrom om det finnes oppdateringer i pakkene som er listet i din pyproject.toml-fil, og den tilhørende poetry.lock. Hvis det finnes oppdateringer så vil den lage en pull request som du kan godkjenne. Når du godkjenner den så vil den oppdatere poetry.lock-filen og lage en ny commit som du kan pushe til GitHub. Dependabot gir også en sikkerhetsvarslinger hvis det finnes kjente sårbarheter i pakkene du bruker.\nDet er anbefalt at alle som installerer sine egne pakker i SSB skrur på Dependabot i sine GitHub-repoer.\n\nAktivere Dependabot\nDu kan aktivere Dependabot ved å gi inn i GitHub-repoet ditt og gjøre følgende:\n\nGå inn repoet\nTrykk på Settings for det repoet som vist på Figur 5.\n\n\n\n\nFigur 5: Åpne Settings for et GitHub-repo.\n\n\n\nI menyen til venstre velger du Code security and analysis\nUnder seksjonen Dependabot velger Enable på minst Dependabot alerts og Dependabot security updates, slik som vist i Figur 6.\n\n\n\n\nFigur 6: Skru på Dependabot i GitHub.\n\n\nNår du har gjort dette vil GitHub varsle deg hvis det finnes en kjent sårbarhet i pakkene som benyttes.\n\n\nOppdatere pakker\nHvis en av pakkene du bruker kommer med en oppdatering, så vil Dependabot lage en pull request (PR) i GitHub som du kan godkjenne. Dependabot sjekker også om oppdateringen er i konflikt med andre pakker du bruker. Hvis det er tilfellet så vil den lage en pull request som oppdaterer alle pakkene som er i konflikt.\nHvis en av pakkene du bruker har en kjent sikkerhetssårbarhet, så vil Dependabot varsle deg om dette under Security-fanen i GitHub-repoet ditt. Hvis du trykker på View Dependabot alerts så vil du få en oversikt over alle sårbarhetene som er funnet, og hvilken alvorlighetsgrad den har. Hvis du trykker på en av sårbarhetene så vil du få mer informasjon om den, og du kan trykke på Create pull request for å oppdatere pakken.\nSom nevnt over kan du enkelt oppdatere pakker fra GitHub ved hjelp av Dependabot. Men det finnes tilfeller der du vil teste om en oppdatering gjør at deler av koden din ikke fungerer lenger. Anta at du bruker en Python-pakken Pandas i koden din, og at du får en pull request fra Dependabot om å oppdatere den fra versjon 1.5 til 2.0. Hvis du ønsker å teste om koden din fortsatt fungerer med den nye versjonen av Pandas, så kan du gjøre dette i Jupyterlab ved å følge ved å lage en branch som f.eks. heter update-pandas. Deretter kan du installere den nye versjonen av Pandas med følgende kommando fra terminalen:\n\n\nterminal\n\npoetry update pandas@2.0\n\nHvis du nå kjører koden din kan du teste om den fortsatt fungerer som forventet. Gjør den ikke det kan du tilpasse koden din og pushe endringene til Github. Deretter kan du godkjenne pull requesten fra Dependabot og merge den. Etter dette kan du slette den PR-en som Dependabot lagde for deg."
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#bygg-eksisterende-ssb-project",
    "href": "statistikkere/jobbe-med-kode.html#bygg-eksisterende-ssb-project",
    "title": "Jobbe med kode",
    "section": "Bygg eksisterende ssb-project",
    "text": "Bygg eksisterende ssb-project\nNår vi skal samarbeide med andre om kode så gjør vi dette via GitHub. Når du pusher koden din til GitHub, så kan samarbeidspartnere pulle ned koden og jobbe videre med den. Men når de henter ned koden så vil de bare hente ned selve koden, ikke pakker og Python-versjonen som ble brukt. De må installere alt som du hadde installert. I tillegg trenger de en kernel hvis de skal jobbe i Jupyterlab. ssb-project gjør det svært enkelt å bygge opp det du trenger, siden det virtuelle miljøet har all informasjon om hva som trengs.\nFor at samarbeidspartneren din skal kunne bygge miljøet på nytt, må de ha gjort en minimal konfigurering av Git. Les mer om hvordan du frem for å gjøre dette her.\nFor å bygge opp et eksisterende miljø gjør du følgende:\n\nFørst må du kopiere prosjektet ned lokalt, eller klone repoet med git-terminologi\n\n\n\nterminal\n\ngit clone https://github.com/statisticsnorway/&lt;prosjektnavn&gt;\n\n\nGå inn i mappen du klonet\n\n\n\nterminal\n\ncd &lt;prosjektnavn&gt;\n\n\nSkape et virtuelt miljø og installere en tilsvarende Jupyter kernel med\n\n\n\nterminal\n\nssb-project build"
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#slette-ssb-project",
    "href": "statistikkere/jobbe-med-kode.html#slette-ssb-project",
    "title": "Jobbe med kode",
    "section": "Slette ssb-project",
    "text": "Slette ssb-project\nDet vil være tilfeller hvor man ønsker å slette et ssb-project, enten fordi man ikke trenger koden lenger eller fordi man bare testet litt.\n\nLokalt\nHvis man jobber med flere prosjekter så kan det fort bli mange Jupyter kerneler hengende igjen. Derfor er det også mulighet å kjøre\n\n\nterminal\n\nssb-project clean stat-testprod\n\nsom sletter Jupyter-kernelen og de installerte pakkene i prosjektet. Hvis du også ønsker å slette selve mappen med kode må du gjøre det manuelt5:\n\n\nterminal\n\nrm -rf ~/stat-testprod/\n\nProsjektmappen over lå direkte i hjemmemappen min og hjemmemappen på Linux kan alltid referes til med et tilda-tegn ~.\n\n\nArkiver GitHub-repo\nGitHub-repoer som er opprettet under SSB sin organinasjons statisticsnorway på GitHub kan ikke slettes, bare arkiveres. Grunnen er at hvis man oppdager en sårbarhet senere så er det viktig å kunne se repoet for å forstå hva som har skjedd.\nHvis du ikke trenger et GitHub-repo lenger kan man arkivere repoet. Det gjør du på følgende måte:\n\nGi inn i repoet Settings slik som vist med rød pil i Figur 7.\n\n\n\n\nFigur 7: Settings for repoet.\n\n\n\nUnder General scroller du deg ned til delen som heter Danger Zone og velger Archive this repository, slik som vist på Figur 8.\n\n\n\n\nFigur 8: Arkivering av et repo.\n\n\n\nI dialogboksen som dukker opp fyller du inn reponavnet som beskrevet og trykker på I understand the consequences, archive this repository.\n\nNår det er gjort så er repoet lesbart, men man kan ikke jobbe med det. Men som vi ser av @#fig-github-repo-settings-archive-warning kan man omgjøre arkiveringen senere hvis det skulle være ønskelig."
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#spark-i-ssb-project",
    "href": "statistikkere/jobbe-med-kode.html#spark-i-ssb-project",
    "title": "Jobbe med kode",
    "section": "Spark i ssb-project",
    "text": "Spark i ssb-project\nFor å kunne bruke Spark i et ssb-project må man først installere pyspark. Det gjør du ved å skrive følgende i en terminal:\n\n\nterminal\n\npoetry add pyspark==$(pip show pyspark | grep Version | egrep -o \"([0-9]{1,}\\.)+[0-9]{1,}\") --no-dev\n\nHer installerer du samme versjon av pyspark som på Jupyterlab.\nVidere kan vi konfigurere Spark til å enten kjøre på lokal maskin eller på flere maskiner (såkalte clusters). Under beskriver vi begge variantene.\n\nLokal maskin\nOppsettet for Pyspark på lokal maskin er det enkleste å sette opp siden Pyspark vil ha direkte tilgang til det lokale filsystemet. Man kan bruke miljøvariabelen PYSPARK_PYTHON til å peke på det virtuelle miljøet, og dermed vil Pyspark også ha tilgang til alle pakkene som er installert der. I en notebook vil dette kunne settes opp slik:\n\n\nnotebook\n\nimport os\nimport subprocess\n\n# Finner filstien til det virtuelle miljøet\npython_path = subprocess.run(['poetry', 'run', 'which', 'python'],\n                             capture_output=True, text=True).stdout.rstrip('\\n')\n\nos.environ[\"PYSPARK_PYTHON\"] = python_path\nos.environ[\"PYSPARK_SUBMIT_ARGS\"] = os.environ[\"PYSPARK_LOCAL_SUBMIT_ARGS\"]\n\nNår du oppretter en Notebook og bruker den kernelen du har laget så må du alltid ha denne på toppen av notebooken. Det initialiserer Spark slik at det kan brukes i notebooken.\n\n\nnotebook\n\n%run /usr/local/share/jupyter/kernels/pyspark_local/init.py\n\nDette scriptet vil sette et spark objekt som brukes for å kalle API’et til pyspark.\n\n\nCluster\nHvis man vil kjøre Pyspark i et cluster (dvs. på flere maskiner) så vil databehandlingen foregå på andre maskiner som ikke har tilgang til det lokale filsystemet. Man må dermed lage en “pakke” av det virtuelle miljøet på lokal maskin og tilgjengeliggjøre dette for alle maskinene i clusteret. For å lage en slik “pakke” kan man bruke et bibliotek som heter venv-pack. Dette kan kjøres fra et terminalvindu slik:\n\n\nterminal\n\nvenv-pack -p .venv -o pyspark_venv.tar.gz\n\nMerk at kommandoen over må kjøres fra rot-mappen i prosjektet ditt. Her er pyspark_venv.tar.gz et tilfeldig valgt filnavn, men dette filnavnet skal brukes videre i notebooken.\n\n\nnotebook\n\nimport os\nimport subprocess\n\n# Miljøvariabel som peker på en utpakket versjon av det virtuelle miljøet\nos.environ[\"PYSPARK_PYTHON\"] = \"./environment/bin/python\"\n\n# Legg til et flagg, --archives, som peker på \"pakken\" med det virtuelle miljøet\nconf = os.environ[\"PYSPARK_K8S_SUBMIT_ARGS\"].split(' ')\nlast_index = conf.index('pyspark-shell')\nconf[last_index:last_index] = ['--archives', 'pyspark_venv.tar.gz#environment']\nos.environ[\"PYSPARK_SUBMIT_ARGS\"] = ' '.join(conf)\n\nNår du oppretter en Notebook og bruker den kernelen du har laget så må du alltid ha denne på toppen av notebooken. Det initialiserer Spark slik at det kan brukes i notebooken.\n\n\nnotebook\n\n%run /usr/local/share/jupyter/kernels/pyspark_k8s/init.py\n\nDette scriptet vil sette et spark objekt som brukes for å kalle API’et til pyspark."
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#tips-og-triks",
    "href": "statistikkere/jobbe-med-kode.html#tips-og-triks",
    "title": "Jobbe med kode",
    "section": "Tips og triks",
    "text": "Tips og triks\nI denne delen av kapitlet vil vi gi deg noen tips og triks som kan være nyttige når du jobber med ssb-project.\n\nPoetry\nssb-project bruker Poetry for å håndtere virtuelle miljøer. Poetry er et verktøy som gjør det enkelt å installere pakker og håndtere versjoner av disse. Det er også Poetry som håndterer Jupyter-kernelen for deg.\nHvis du etterlyser funksjonalitet i et ssb-project så kan det være nyttig å lese dokumentasjonen til Poetry for å se om det er mulig å få til det du ønsker.\n\n\nFull disk på Dapla\nDet “lokale” filsystemet på Dapla har kun 10GB diskplass. Har du mange virtuelle miljøer på denne disken kan det fort bli fullt, siden alle pakker blir installert her. Vanligvis er det 2 grunner til at disken blir full:\n\nFor mange virtuelle miljøer (ssb-projects) lagret lokalt.\nDette vil ofte kunne løses ved å slette virtuelle miljøer som ikke lenger er i bruk. Hvis du har 5 virtuelle miljøer som hver bruker 1GB, og du kun jobber på en av de nå, så vil du frigjøre 40% av disken ved å slette 4 av dem. Husk at det permanente lagringsstedet for kode er på GitHub, og du kan alltid klone ned et prosjekt senere og bygge det hvis det trengs.\n/home/jovyan/.cache/ har blitt for stort.\nDette er en mappe som brukes av applikasjoner til å lagre midlertidig data slik at de kan kjøre raskere. Denne kan bli ganske stor etter hvert. Ofte kan man frigjøre flere GB ved å slette denne. Du sletter denne mappen ved å skrive følgende i en terminal:\n\n\n\nterminal\n\nrm -rf /home/jovyan/.cache/\n\nHvis du opplever at disken er full, så kan det anbefales å undersøke hvilke mapper som tar størst plass med følgende kommando i terminalen:\n\n\nterminal\n\ncd ~ && du -h --max-depth=5 | sort -rh | head -n 10 && cd -\n\nKommandoen over sjekker, fra hjemmemappen din, hvilke mapper og undermapper som tar mest plass. Den viser de 10 største mappene. Hvis du ønsker å se flere mapper så kan du endre tallet etter head -n. Hvis du ønsker å se alle mapper så kan du fjerne head -n. --max-depth=5 betyr at den kun sjekker mapper som er 5 mapper dype fra hjemmemappen din.\nNår du har gjort det kan selv vurdere hvilke som kan slettes for å frigjøre plass.\n\n\nHold prosjektet oppdatert\nHvis du sitter med en lokal kopi av prosjektet ditt, og flere andre jobber med den samme kodebasen, så er det viktig at du holder din lokale kopi oppdatert. Hvis du jobber i en branch på en lokal kopi, bør du holde denne oppdatert med main-branchen på GitHub. Det er vanlig Git-praksis. Når man også bruker ssb-project, så man huske å også bygge prosjektet på nytt hver gang det er endringer som er gjort av andre i poetry.lock.-filen."
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#footnotes",
    "href": "statistikkere/jobbe-med-kode.html#footnotes",
    "title": "Jobbe med kode",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nEn kernel refererer til en Python- eller R-installasjon som er optimalisert for bruk med Jupyterlab Notebooks.↩︎\nCLI = Command-Line-Interface, som betyr et program designet for bruk i terminalen med kommandoer.↩︎\nFiler og mapper som starter med punktum er skjulte med mindre man ber om å se dem. I Jupyterlab kan disse vises i filutforskeren ved å velge View fra menylinjen, og deretter velge Show hidden files. I en terminal skriver man ls -a for å se de.↩︎\nÅ pushe til GitHub uten å sende ved Personal Access Token fordrer at du har lagret det lokalt så Git kan finne det. Her et eksempel på hvordan det kan gjøres.↩︎\nDette kan også gjøres ved å høyreklikke på mappen i Jupyterlab sin filutforsker og velge Delete.↩︎"
  },
  {
    "objectID": "statistikkere/automatisering.html",
    "href": "statistikkere/automatisering.html",
    "title": "Automatisering",
    "section": "",
    "text": "For å redusere tilgang til PII1 oppfordres alle Dapla-team til å ha en automatisert prosessering av kildedata. Dapla tilbyr dette som en 100% selvbetjent løsning. Kildedata (Standardutvalget 2021, 5) prosesseres til inndata gjennom et gitt sett av operasjoner(se Figur 1). Kildedataprosesseringsløsningen som tilbys på Dapla er bygget på at hver kildedatafil behandles individuelt. Mer komplekse operasjoner som går på tvers av flere filer bør utføres på inndata eller senere datatilstander.\n\n\n\n\n\n\nDet er kun teamets kildedataansvarlige som skal aksessere kildedata.\n\n\n\n\n\n\n\n\n\nTeamets kildedataansvarlige tar ansvar for å prosessere kildedata til inndata på en forsvarlig måte.\n\n\n\n\n\n\n\n\nFigur 1: Operasjoner som inngår i kildedataprosessering\n\n\n\nInndata er kildedata som er transformert til SSBs standard lagringsformat. Variabelnavn og -innhold er uendret bortsett fra at:\n\nDirekte identifiserende variabler (f.eks. fødselsnummer) er pseudonymisert\nTegnsett, datoformat, adresse m.m. er endret til SSBs standardformat\nDet benyttes standard kodeverk (Klass) der det er mulig (f.eks. kjønn)\nDataene er minimert slik at kun variablene som er nødvendige i den videre produksjonsprosessen inngår.\n\n\n(Standardutvalget 2021, 8)\nVed transformasjon fra kildedata til inndata er det ikke anbefalt å gjennomføre operasjoner som:\n\nGår på tvers av flere filer\nLegger til nye felt\nEndrer navn på felt\nAggregerer data\n\n\n\n\n\n\nAutomatiseringsløsningen krever at teamets Google-prosjekt kan lese fra teamets Infrastructure as Code (IaC) repo på Github. Følg instruksjonene her for å sette opp dette. Dette er en engangsjobb som må gjøres av en som har administratortilgang til IaC-repoet.\n\n\n\n\n\nKilder konfigureres i teamets Infrastructure as Code (IaC) repo på Github. Det kan finnes basert på følgende formulering: github.com/statisticsnorway/&lt;teamnavn&gt;-iac. Kilder konfigureres under stien automation/source_data/prod på repoet.\n\n\n\n\n\n\nHer referer vi til filstiene som automation/source_data/prod, men under testing burde man alltid jobbe i staging-miljøet med automation/source_data/staging.\n\n\n\n\n\n\nHver kilde konfigureres ved hjelp av to filer:\n\nconfig.yaml som konfigurer hvilken filsti som skal prosesseres i teamets kildedatabøtte. Eksempel:\n\n\n\nconfig.yaml\n\nfolder_prefix: source-folder/2022\n\n\nprocess_source_data.py som kjøres når en kildedatafil blir prosessert. Her må man skrive en python-funksjon med en bestemt metodesignatur som ser slik ut:\n\n\n\nprocess_source_data.py\n\n def main(source_file):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         source_file: fully qualified file name of the source data file.\n         Example: gs://ssb-prod-my-project-data-kilde/source-folder/2022/data.xml\n     \"\"\"\n\nDisse filene må legges til i en mappe per kilde under automation/source_data/prod i IaC repoet. Se eksemplet under for en detaljert forklaring.\n\n\n\n\n\n\nMappenavnet for kilder i IaC repoene blir brukt som navn på ressurser. Dette betyr at de enesete tillatte tegnene i mappenavnet er bokstaver, tall, bindestrek og underscore. Det er ikke tillatt med mellomrom eller andre spesialtegn. Mappenavnet kan bestå av opptil 20 tegn.\n\n\n\n\n\n\nDette avhenger av om prosesseringsscriptet kan behandle alle kildefilene på samme måte, eller om det vil være variasjoner som gjør at prosesseringen bør splittes opp i uavhengige prosesseringsscript.\nGrunner til å differensiere mellom kilder kan være:\n\nKildedatafilene har forskjellig filformat (f.eks xml eller json)\nKildedataene har ulike felter\nKildedataene inneholder PII2 eller ikke\n\n\n\n\n\n\n\n\n\n\nDisse instruksjonene forutsetter at ditt Google-prosjekt er koblet til Github.\n\n\n\n\nOpprette skriptet process_source_data.py som prosesserer kildedatafilen til inndata. Dette kan testes av kildedataansvarlige manuelt på Jupyter for å verifisere at dataene blir prosessert som ønsket.\nI en branch i teamets IaC repo, legg til filene config.yaml og process_source_data.py i en ny mappe (valgfritt navn) under automation/source_data/prod. Se eksemplet under for en detaljert forklaring av formatet.\nLag en PR på branchen og få den godkjent av kildedataansvarlige.\nVent til alle tester er ferdige. Det skal stå “All checks have passed” før man går videre, hvis testene feiler følg stegene her. \nSkrive atlantis apply i en kommentar på PRen for å opprette det nødvendige infrastruktur for å prosessere kilden.\nMerge PRen.\nSjekk resultatet av det automatiske bygget.\nVerifisere at nye filer lagt i kildedatabøtten blir prosessert til inndata som forventet.\n\n\n\n\n\nLa oss si at et team (småbakst) har to datakilder levert av ulik dataeiere på ulik formater. Den ene er om boller og er på csv format og den andre er om rundstykker og er på json format. Kildedataansvarlige i teamet bestemmer seg for at filene i boller/ er like nok å prosesseres som en kilde, og at filene i rundstykker/ kan prosesseres som en annen kilde.\n\n\nssb-prod-smaabakst-data-kilde\n├── boller\n│   ├── hvetebolle\n│   │   ├── 2018-salg.csv\n│   │   ├── 2019-salg.csv\n│   │   ├── ...\n│   ├── kanelbolle\n│   │   ├── 2018-salg.csv\n│   │   ├── 2019-salg.csv\n│   │   ├── ...\n│   └── skolebolle\n│       ├── 2018-salg.csv\n│       ├── 2019-salg.csv\n│       ├── ...\n└── rundstykker\n    ├── haandverker\n    │   ├── apr-2022-resultater.json\n    │   ├── aug-2022-resultater.json\n    │   ├── ...\n    └── havre\n        ├── apr-2022-resultater.json\n        ├── aug-2022-resultater.json\n        ├── ...\n\n\n\nsmaabakst-iac\n└── automation\n    └── source_data\n        └── prod\n            ├── boller\n            │   ├── config.yaml\n            │   └── process_source_data.py\n            └── rundstykker\n                ├── config.yaml\n                └── process_source_data.py\n\n\n\n\n\nconfig.yaml\n\nfolder_prefix: boller\n\nVerdien for folder_prefix tilsvarer en “fil sti” i kildedatabøtte. I dette tilfellet vil nye filer lagt til under boller/ trigge en prosessering.\n\n\n\n\n\nconfig.yaml\n\nfolder_prefix: rundstykker\n\n\n\n\n\n\nMed prosesseringsscriptet mener vi filen process_source_data.py som ligger i en mappe per kilde under automation/source_data/prod. Dette scriptet vil bli kalt hver gang det blir lagt til en ny fil i kildedatabøtten.\nDette betyr at hvis f.eks. 10 filer blir lagt til i kildedatabøtten samtidig så vil det startes opp 10 individuelle Python-prosesser som kaller denne main-metoden med forskjellig source_file. Parameteren source_file vil inneholde hele filstien inkl. filnavn. Så en enkel flytteoperasjon fra kildedatabøtten til inndatebøtten (uten noen form for konvertering) vil kunne uttrykkes slik:\n\n\nprocess_source_data.py\n\nimport dapla as dp\n\ndestination_bucket_name = \"ssb-prod-smaabakst-data-produkt\"\n\n def main(source_file):\n    # source_file er f.eks: gs://ssb-prod-smaabakst-data-kilde/boller/hveteboller/2018-salg.csv\n    df = dp.read_pandas(source_file,  file_format=\"csv\")\n\n    # Eksempel på konvertering fra xml til parquet-format\n    dp.write_pandas(df, f\"gs://{destination_bucket_name}/inndata/boller/hveteboller/2018-salg.parquet\")\n\nAlternativt (uten noen form for konvertering)…\n\n\nprocess_source_data.py\n\nfrom dapla import FileClient\n\nsource_bucket_name = \"ssb-prod-smaabakst-data-kilde\"\ndestination_bucket_name = \"ssb-prod-smaabakst-data-produkt\"\n\n def main(source_file):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         source_file: fully qualified file name of the source data file.\n     \"\"\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    fs = FileClient.get_gcs_file_system()\n    fs.copy(file_path, destination_path)\n\n\n\nDet anbefales å bruke Pythons logging modul for å logge, og ikke bruke print eller skrive til stdout/stderr. Det er satt opp en standard logger-konfigurasjon som skriver informasjonsmeldinger (log level info) til stdout og feilmeldinger (log level warning eller error) til stderr. Feil som ikke blir håndtert blir automatisk fanget opp og logget av automatiseringsløsningen. Eksemplet nedenfor logger en informasjonsmelding, en advarsel og en feilmelding:\n\n\nprocess_source_data.py\n\nimport logging\nlogging.info('Til info')\nlogging.warning('Advarsel!')\nlogging.error('En feil oppstod!')"
  },
  {
    "objectID": "statistikkere/automatisering.html#operasjoner-som-inngår-i-kildedataprosessering",
    "href": "statistikkere/automatisering.html#operasjoner-som-inngår-i-kildedataprosessering",
    "title": "Automatisering",
    "section": "",
    "text": "Figur 1: Operasjoner som inngår i kildedataprosessering\n\n\n\nInndata er kildedata som er transformert til SSBs standard lagringsformat. Variabelnavn og -innhold er uendret bortsett fra at:\n\nDirekte identifiserende variabler (f.eks. fødselsnummer) er pseudonymisert\nTegnsett, datoformat, adresse m.m. er endret til SSBs standardformat\nDet benyttes standard kodeverk (Klass) der det er mulig (f.eks. kjønn)\nDataene er minimert slik at kun variablene som er nødvendige i den videre produksjonsprosessen inngår.\n\n\n(Standardutvalget 2021, 8)\nVed transformasjon fra kildedata til inndata er det ikke anbefalt å gjennomføre operasjoner som:\n\nGår på tvers av flere filer\nLegger til nye felt\nEndrer navn på felt\nAggregerer data"
  },
  {
    "objectID": "statistikkere/automatisering.html#ta-tjenesten-i-bruk",
    "href": "statistikkere/automatisering.html#ta-tjenesten-i-bruk",
    "title": "Automatisering",
    "section": "",
    "text": "Automatiseringsløsningen krever at teamets Google-prosjekt kan lese fra teamets Infrastructure as Code (IaC) repo på Github. Følg instruksjonene her for å sette opp dette. Dette er en engangsjobb som må gjøres av en som har administratortilgang til IaC-repoet.\n\n\n\n\n\nKilder konfigureres i teamets Infrastructure as Code (IaC) repo på Github. Det kan finnes basert på følgende formulering: github.com/statisticsnorway/&lt;teamnavn&gt;-iac. Kilder konfigureres under stien automation/source_data/prod på repoet.\n\n\n\n\n\n\nHer referer vi til filstiene som automation/source_data/prod, men under testing burde man alltid jobbe i staging-miljøet med automation/source_data/staging.\n\n\n\n\n\n\nHver kilde konfigureres ved hjelp av to filer:\n\nconfig.yaml som konfigurer hvilken filsti som skal prosesseres i teamets kildedatabøtte. Eksempel:\n\n\n\nconfig.yaml\n\nfolder_prefix: source-folder/2022\n\n\nprocess_source_data.py som kjøres når en kildedatafil blir prosessert. Her må man skrive en python-funksjon med en bestemt metodesignatur som ser slik ut:\n\n\n\nprocess_source_data.py\n\n def main(source_file):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         source_file: fully qualified file name of the source data file.\n         Example: gs://ssb-prod-my-project-data-kilde/source-folder/2022/data.xml\n     \"\"\"\n\nDisse filene må legges til i en mappe per kilde under automation/source_data/prod i IaC repoet. Se eksemplet under for en detaljert forklaring.\n\n\n\n\n\n\nMappenavnet for kilder i IaC repoene blir brukt som navn på ressurser. Dette betyr at de enesete tillatte tegnene i mappenavnet er bokstaver, tall, bindestrek og underscore. Det er ikke tillatt med mellomrom eller andre spesialtegn. Mappenavnet kan bestå av opptil 20 tegn.\n\n\n\n\n\n\nDette avhenger av om prosesseringsscriptet kan behandle alle kildefilene på samme måte, eller om det vil være variasjoner som gjør at prosesseringen bør splittes opp i uavhengige prosesseringsscript.\nGrunner til å differensiere mellom kilder kan være:\n\nKildedatafilene har forskjellig filformat (f.eks xml eller json)\nKildedataene har ulike felter\nKildedataene inneholder PII2 eller ikke\n\n\n\n\n\n\n\n\n\n\nDisse instruksjonene forutsetter at ditt Google-prosjekt er koblet til Github.\n\n\n\n\nOpprette skriptet process_source_data.py som prosesserer kildedatafilen til inndata. Dette kan testes av kildedataansvarlige manuelt på Jupyter for å verifisere at dataene blir prosessert som ønsket.\nI en branch i teamets IaC repo, legg til filene config.yaml og process_source_data.py i en ny mappe (valgfritt navn) under automation/source_data/prod. Se eksemplet under for en detaljert forklaring av formatet.\nLag en PR på branchen og få den godkjent av kildedataansvarlige.\nVent til alle tester er ferdige. Det skal stå “All checks have passed” før man går videre, hvis testene feiler følg stegene her. \nSkrive atlantis apply i en kommentar på PRen for å opprette det nødvendige infrastruktur for å prosessere kilden.\nMerge PRen.\nSjekk resultatet av det automatiske bygget.\nVerifisere at nye filer lagt i kildedatabøtten blir prosessert til inndata som forventet.\n\n\n\n\n\nLa oss si at et team (småbakst) har to datakilder levert av ulik dataeiere på ulik formater. Den ene er om boller og er på csv format og den andre er om rundstykker og er på json format. Kildedataansvarlige i teamet bestemmer seg for at filene i boller/ er like nok å prosesseres som en kilde, og at filene i rundstykker/ kan prosesseres som en annen kilde.\n\n\nssb-prod-smaabakst-data-kilde\n├── boller\n│   ├── hvetebolle\n│   │   ├── 2018-salg.csv\n│   │   ├── 2019-salg.csv\n│   │   ├── ...\n│   ├── kanelbolle\n│   │   ├── 2018-salg.csv\n│   │   ├── 2019-salg.csv\n│   │   ├── ...\n│   └── skolebolle\n│       ├── 2018-salg.csv\n│       ├── 2019-salg.csv\n│       ├── ...\n└── rundstykker\n    ├── haandverker\n    │   ├── apr-2022-resultater.json\n    │   ├── aug-2022-resultater.json\n    │   ├── ...\n    └── havre\n        ├── apr-2022-resultater.json\n        ├── aug-2022-resultater.json\n        ├── ...\n\n\n\nsmaabakst-iac\n└── automation\n    └── source_data\n        └── prod\n            ├── boller\n            │   ├── config.yaml\n            │   └── process_source_data.py\n            └── rundstykker\n                ├── config.yaml\n                └── process_source_data.py\n\n\n\n\n\nconfig.yaml\n\nfolder_prefix: boller\n\nVerdien for folder_prefix tilsvarer en “fil sti” i kildedatabøtte. I dette tilfellet vil nye filer lagt til under boller/ trigge en prosessering.\n\n\n\n\n\nconfig.yaml\n\nfolder_prefix: rundstykker"
  },
  {
    "objectID": "statistikkere/automatisering.html#skrive-prosesseringsscriptet",
    "href": "statistikkere/automatisering.html#skrive-prosesseringsscriptet",
    "title": "Automatisering",
    "section": "",
    "text": "Med prosesseringsscriptet mener vi filen process_source_data.py som ligger i en mappe per kilde under automation/source_data/prod. Dette scriptet vil bli kalt hver gang det blir lagt til en ny fil i kildedatabøtten.\nDette betyr at hvis f.eks. 10 filer blir lagt til i kildedatabøtten samtidig så vil det startes opp 10 individuelle Python-prosesser som kaller denne main-metoden med forskjellig source_file. Parameteren source_file vil inneholde hele filstien inkl. filnavn. Så en enkel flytteoperasjon fra kildedatabøtten til inndatebøtten (uten noen form for konvertering) vil kunne uttrykkes slik:\n\n\nprocess_source_data.py\n\nimport dapla as dp\n\ndestination_bucket_name = \"ssb-prod-smaabakst-data-produkt\"\n\n def main(source_file):\n    # source_file er f.eks: gs://ssb-prod-smaabakst-data-kilde/boller/hveteboller/2018-salg.csv\n    df = dp.read_pandas(source_file,  file_format=\"csv\")\n\n    # Eksempel på konvertering fra xml til parquet-format\n    dp.write_pandas(df, f\"gs://{destination_bucket_name}/inndata/boller/hveteboller/2018-salg.parquet\")\n\nAlternativt (uten noen form for konvertering)…\n\n\nprocess_source_data.py\n\nfrom dapla import FileClient\n\nsource_bucket_name = \"ssb-prod-smaabakst-data-kilde\"\ndestination_bucket_name = \"ssb-prod-smaabakst-data-produkt\"\n\n def main(source_file):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         source_file: fully qualified file name of the source data file.\n     \"\"\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    fs = FileClient.get_gcs_file_system()\n    fs.copy(file_path, destination_path)\n\n\n\nDet anbefales å bruke Pythons logging modul for å logge, og ikke bruke print eller skrive til stdout/stderr. Det er satt opp en standard logger-konfigurasjon som skriver informasjonsmeldinger (log level info) til stdout og feilmeldinger (log level warning eller error) til stderr. Feil som ikke blir håndtert blir automatisk fanget opp og logget av automatiseringsløsningen. Eksemplet nedenfor logger en informasjonsmelding, en advarsel og en feilmelding:\n\n\nprocess_source_data.py\n\nimport logging\nlogging.info('Til info')\nlogging.warning('Advarsel!')\nlogging.error('En feil oppstod!')"
  },
  {
    "objectID": "statistikkere/automatisering.html#footnotes",
    "href": "statistikkere/automatisering.html#footnotes",
    "title": "Automatisering",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPersonidentifiserende Informasjon↩︎\nPersonidentifiserende Informasjon↩︎"
  },
  {
    "objectID": "statistikkere/statistikkbanken.html",
    "href": "statistikkere/statistikkbanken.html",
    "title": "dapla-statbank-client",
    "section": "",
    "text": "Pakken “dapla-statbank-client” kan brukes til å overføre tabeller til Statistikkbanken fra Jupyterlab i prodsonen og på Dapla. Den henter også “filbeskrivelsen” som beskriver formen dataene skal ha når de sendes inn til Statistikkbanken. Og den kan også hente publiserte data fra Statistikkbanken. Pakken er en python-pakke som baserer seg på at dataene (deltabellene) lastes inn i en eller flere pandas DataFrames før overføring. Ved å hente ned “filbeskrivelsen” kan man validere dataene sine (dataframene) mot denne lokalt, uten å sende dataene til Statistikkbanken. Dette kan være til hjelp under setting av formen på dataene. Å hente publiserte data fra Statistikkbanken kan gjøres gjennom løse funksjoner, eller via “klienten”.\nLenker: - Pakken ligger her på Pypi. Og kan installeres via poetry med: poetry add dapla-statbank-client - Kodebasen for pakken ligger her, readme-en gir en teknisk innføring som du kan følge og kopiere kode fra, og om du finner noe du vil rapportere om bruken av pakken så gjør det gjerne under “issues” på github-sidene. - Noe demokode ligger i repoet, og kan være ett godt utgangspunkt å kopiere og endre fra.\n\n\nStatistikkbanken opererer med ulike databaser for testing og produksjonssetting. Om du vil at tallene faktisk skal oppdateres på nettsidene så må du sende til Statistikkbankens “PROD”-database. Om du kun vil teste innsending skal du sende til databasen “TEST”. Disse databasene har de samme lastebrukerne, men passordene er ulike. Om du vil teste innsending må du derfor skaffe deg “test-passordet” til den lastebrukeren som du har tilgjengelig. For å gjøre tester via pakken må du være i staging på dapla: https://jupyter.dapla-staging.ssb.no/ eller staging i prodosonen: https://sl-jupyter-t.ssb.no/ Om du faktisk vil sende inn data til publisering i Statistikkbanken må du være i dapla-prod (den vanlige): https://jupyter.dapla.ssb.no/ eller i prodsonen på: https://sl-jupyter-p.ssb.no/ For å teste er det fint å skaffe seg noe data fra fjorårets publisering på et produksjonsløp man kjenner fra før. Evt. kan man hente data fra Statistikkbanken og sende disse tilbake snudd rett, og med riktig antall prikke-kolonner lagt til.\n\n\n\nSe mer detaljer i readme-en på prosjektets kodebase.\n\n\nFor å kunne bruke pakken må du importere klienten:\n\n\nnotebook\n\nfrom statbank import StatbankClient\n\nSå initialiserer du klienten med de innstillingene som oftest er faste på tvers av alle innsendingene fra ett produksjonsløp:\n\n\nnotebook\n\nstatcli = StatbankClient(loaduser=\"LAST360\", date=\"2050-01-01\", overwrite=True, approve=2)\n\nHer vil du bli bedt om å skrive inn passordet til lastebrukeren.\n\n\n\nOm du har dataene klare er det bare å overføre, men du må vite navnet på deltabell-dat-filene. (Statistikkbanken lagrer disse ned igjen som dat-filer.)\n\n\nnotebook\n\ndata_07495 = {\"kargrs01fylker1.dat\" : df_07495_fylker,\n              \"kargrs01landet1.dat\" : df_07495_landet,}\nstatcli.transfer(data_07495, tableid=\"07495\")\n\nEtter innsending kommer det en link til Statistikkbankens GUI for å følge med på om innsendingen gikk bra hos dem. Om det var det du ønsket, så er du nå ferdig… Men det finnes mer funksjonalitet her…\n\n\n\nFor å hente filbeskrivelsen til en hovedtabell bruker du denne metoden under klienten:\n\n\nnotebook\n\nfilbeskrivelse = statcli.get_description(tableid=\"07495\")\nprint(filbeskrivelse)\n\nMed filbeskrivelsen kan du lett få en mal på dictionaryet du må plassere dataene i:\n\n\nnotebook\n\nfilbeskrivelse.transferdata_template()\n\nDu kan også validere dataene dine mot filbeskrivelsen:\n\n\nnotebook\n\nfilbeskrivelse.validate(data_07495, tableid=\"07495\")\n\n\n\n\n\nDet tas noe ekstra ansvar i pakken rundt avrunding av desimaltall, da filbeskrivelsen inneholder informasjon om hvor mange desimaler som blir lagret per kolonne, kan vi runde av til rett antall samtidig som vi tar ett bevisst valg rundt “hvilken vei vi skal runde av”. På barneskolen lærte vi at ved 2,5 avrundet til 0 desimaler, så runder vi opp til 3. Det samme skjer i utgangspunktet i SAS og Excel. Python og R runder derimot “mot nærmeste partall”, så fra 2,5 blir det rundet til 2, men fra 1,5 blir det også rundet til 2. Dette er for å forhindre bias i en retning. Dvs. om alle tall rundes opp, vil en hel kolonne med tall “dras oppover”, ved å gjøre annenhver opp og ned, vil ikke helheten bli “dratt en spesifikk vei”. Siden “round to even” ikke er det folk er vandte til, gjør vi derfor noe annet i denne pakken, enn det som er vanlig oppførsel i Python. Vi runder opp. Om du bruker følgende metoden under filbeskrivelsen på dataene, så vil denne runde oppover, samtidig som den konverterer til en streng for å bevare formateringen.\n\n\nnotebook\n\ndata_07495 = filbeskrivelse.round_data(data_07495)  # For å ta vare på endringene, så må du skrive tilbake over variabelen\n\n\n\n\n\nEn date-widget for å visuelt endre til en valid dato.\nLagring av overføring og filbeskrivelses-objekter til json-filer\nUthenting av logg fra klienten\nHenting av publisert data fra statbanken\nRotering av tabeller fra statbanken"
  },
  {
    "objectID": "statistikkere/statistikkbanken.html#testoverføring-fra-staging---faktisk-oppdatering-fra-prod",
    "href": "statistikkere/statistikkbanken.html#testoverføring-fra-staging---faktisk-oppdatering-fra-prod",
    "title": "dapla-statbank-client",
    "section": "",
    "text": "Statistikkbanken opererer med ulike databaser for testing og produksjonssetting. Om du vil at tallene faktisk skal oppdateres på nettsidene så må du sende til Statistikkbankens “PROD”-database. Om du kun vil teste innsending skal du sende til databasen “TEST”. Disse databasene har de samme lastebrukerne, men passordene er ulike. Om du vil teste innsending må du derfor skaffe deg “test-passordet” til den lastebrukeren som du har tilgjengelig. For å gjøre tester via pakken må du være i staging på dapla: https://jupyter.dapla-staging.ssb.no/ eller staging i prodosonen: https://sl-jupyter-t.ssb.no/ Om du faktisk vil sende inn data til publisering i Statistikkbanken må du være i dapla-prod (den vanlige): https://jupyter.dapla.ssb.no/ eller i prodsonen på: https://sl-jupyter-p.ssb.no/ For å teste er det fint å skaffe seg noe data fra fjorårets publisering på et produksjonsløp man kjenner fra før. Evt. kan man hente data fra Statistikkbanken og sende disse tilbake snudd rett, og med riktig antall prikke-kolonner lagt til."
  },
  {
    "objectID": "statistikkere/statistikkbanken.html#kode-eksempler",
    "href": "statistikkere/statistikkbanken.html#kode-eksempler",
    "title": "dapla-statbank-client",
    "section": "",
    "text": "Se mer detaljer i readme-en på prosjektets kodebase.\n\n\nFor å kunne bruke pakken må du importere klienten:\n\n\nnotebook\n\nfrom statbank import StatbankClient\n\nSå initialiserer du klienten med de innstillingene som oftest er faste på tvers av alle innsendingene fra ett produksjonsløp:\n\n\nnotebook\n\nstatcli = StatbankClient(loaduser=\"LAST360\", date=\"2050-01-01\", overwrite=True, approve=2)\n\nHer vil du bli bedt om å skrive inn passordet til lastebrukeren.\n\n\n\nOm du har dataene klare er det bare å overføre, men du må vite navnet på deltabell-dat-filene. (Statistikkbanken lagrer disse ned igjen som dat-filer.)\n\n\nnotebook\n\ndata_07495 = {\"kargrs01fylker1.dat\" : df_07495_fylker,\n              \"kargrs01landet1.dat\" : df_07495_landet,}\nstatcli.transfer(data_07495, tableid=\"07495\")\n\nEtter innsending kommer det en link til Statistikkbankens GUI for å følge med på om innsendingen gikk bra hos dem. Om det var det du ønsket, så er du nå ferdig… Men det finnes mer funksjonalitet her…\n\n\n\nFor å hente filbeskrivelsen til en hovedtabell bruker du denne metoden under klienten:\n\n\nnotebook\n\nfilbeskrivelse = statcli.get_description(tableid=\"07495\")\nprint(filbeskrivelse)\n\nMed filbeskrivelsen kan du lett få en mal på dictionaryet du må plassere dataene i:\n\n\nnotebook\n\nfilbeskrivelse.transferdata_template()\n\nDu kan også validere dataene dine mot filbeskrivelsen:\n\n\nnotebook\n\nfilbeskrivelse.validate(data_07495, tableid=\"07495\")"
  },
  {
    "objectID": "statistikkere/statistikkbanken.html#problematikk-knyttet-til-avrunding",
    "href": "statistikkere/statistikkbanken.html#problematikk-knyttet-til-avrunding",
    "title": "dapla-statbank-client",
    "section": "",
    "text": "Det tas noe ekstra ansvar i pakken rundt avrunding av desimaltall, da filbeskrivelsen inneholder informasjon om hvor mange desimaler som blir lagret per kolonne, kan vi runde av til rett antall samtidig som vi tar ett bevisst valg rundt “hvilken vei vi skal runde av”. På barneskolen lærte vi at ved 2,5 avrundet til 0 desimaler, så runder vi opp til 3. Det samme skjer i utgangspunktet i SAS og Excel. Python og R runder derimot “mot nærmeste partall”, så fra 2,5 blir det rundet til 2, men fra 1,5 blir det også rundet til 2. Dette er for å forhindre bias i en retning. Dvs. om alle tall rundes opp, vil en hel kolonne med tall “dras oppover”, ved å gjøre annenhver opp og ned, vil ikke helheten bli “dratt en spesifikk vei”. Siden “round to even” ikke er det folk er vandte til, gjør vi derfor noe annet i denne pakken, enn det som er vanlig oppførsel i Python. Vi runder opp. Om du bruker følgende metoden under filbeskrivelsen på dataene, så vil denne runde oppover, samtidig som den konverterer til en streng for å bevare formateringen.\n\n\nnotebook\n\ndata_07495 = filbeskrivelse.round_data(data_07495)  # For å ta vare på endringene, så må du skrive tilbake over variabelen"
  },
  {
    "objectID": "statistikkere/statistikkbanken.html#annen-funksjonalitet-du-kan-lese-om-i-readme-en-er",
    "href": "statistikkere/statistikkbanken.html#annen-funksjonalitet-du-kan-lese-om-i-readme-en-er",
    "title": "dapla-statbank-client",
    "section": "",
    "text": "En date-widget for å visuelt endre til en valid dato.\nLagring av overføring og filbeskrivelses-objekter til json-filer\nUthenting av logg fra klienten\nHenting av publisert data fra statbanken\nRotering av tabeller fra statbanken"
  },
  {
    "objectID": "statistikkere/hva-er-botter.html",
    "href": "statistikkere/hva-er-botter.html",
    "title": "Hva er bøtter?",
    "section": "",
    "text": "På Dapla er det Google Cloud Storage (GCS) som benyttes til å lagre data og filer. Følgelig er det GCS som erstatter det vi kjente som Linux-stammene i prodsonen tidligere. I SSB har vi vært vant til å jobbe med data lagret på filsystemer i et Linux-miljø1. GCS-bøttene skiller seg fra klassiske filsystemer på flere måter, og det er viktig å være klar over disse forskjellene. I denne kapitlet vil vi gå gjennom noen av de viktigste forskjellene og hvordan man gjør vanlige operasjoner mot bøtter i GCS.\n\n\nI et Linux- eller Windows-filsystem, som vi har vært vant til tidligere, så er filer og mapper organisert i en hierarkisk struktur på et operativsystem (OS). I SSB har OS-ene vært installert på fysiske maskiner som vi vedlikeholder selv.\nEn bøtte i GCS er derimot en kjøpt tjeneste som lar brukeren lagre alle typer objekter i en container. Man trenger altså ikke å tenke på om filene ligger i et hierarki, hvilket operativsystem det kjører på, eller hvor mye diskplass som er tilgjengelig.\nAt data er lagret som objekter i en bøtte har noen praktiske implikasjoner for hvordan vi jobber med data i SSB:\n\nMange er vant til å jobbe direkte med filer i en Linux-terminal eller via systemkall fra språk som SAS, Pyton eller R. For å gjøre det samme i Jupyter mot en bøtte, så kan vi bruke Python-pakken gcsfs. Se eksempler under.\nNår vi bruker Python- eller R-pakker for lese eller skrive data fra bøtter, så er vi avhengig av at pakkene tilbyr integrasjon mot bøtter. Mange pakker gjør det, men ikke alle. For de som ikke gjør det kan vi bruke ofte bruke gcsfs til å gi oss et python-objekt, som igjen kan brukes av de fleste pakker.\nDet finnes egentlig ikke noen mapper i bøtter. I motsetning til et vanlig filsystem så er det ikke en hierarkisk mappestruktur i en bøtte. Det vil si at alt ligger et sted, men de kan ha lange navn med /-tegn slik at det ligner på et klassisk filsystem. Bruker du / i objekt-navnet så vil også Google Cloud Console vise det som mapper, men det er bare for å gjøre det enklere å forholde seg til. En praktisk konsekvens av dette er f.eks. at man ikke trenger å opprette en mappe før man legger et fil/objekt i den. Det er bare en tekststreng som er en del av objekt-navnet.\n\nUnder kommer det en del eksempler på hvordan man kan jobbe med objekter i bøtter på samme måte som filer i et filsystem.\n\n\n\nPå Dapla skal data lagres i bøtter. Men når du åpner Jupyterlab så får du også et “lokalt” eller klassisk filsystem, slik vi definerte det i forrige kapittel. Det er dette filsystemet du ser i filutforskeren i Jupyterlab, slik som vist til venstre i Figur 1. Det er også dette filsystemet du ser når du f.eks. bruker ls-kommandoen i en terminal i Jupyterlab.\n\n\n\nFigur 1: Til venstre i bildet ser du filutforskeren i Jupyterlab\n\n\nDette filsystemet er ment for å lagre kode midlertidig mens du jobber med dem. Det er ikke ment for å lagre data. Det er heller ikke ment som et permanent lagringssted for kode. Permanent lagring av kode skal gjøres på GitHub. Selv om filene du lagrer der fortsetter å eksistere for hver gang du logger deg inn i Jupyterlab, så bør kode du ønsker å bevare pushes til GitHub før du avslutter en sesjon i Jupyterlab. Se en mer teknisk beskrivelse av hvordan dette fungerer i boksen under.\n\n\n\n\n\n\nHvem er egentlig denne Jovyan?\n\n\n\nNår du logger deg inn i Jupyterlab på Dapla, så ser du at brukeren din på det lokale filsystemet heter jovyan. Grunnen er at det er en Linux-container som kjører et Jupyterlab Docker-image, og alle bruker denne containeren. Det er derfor vi kaller det et “lokalt” filsystem, fordi det er lokalt i containeren.\nI tillegg er det satt opp persistent volum (PV) og en persistent volume claim (PVC) som er knyttet til denne containeren. Det er dette som gjør at filene du lagrer i Jupyterlab blir lagret mellom hver gang du logger deg inn. I tillegg tas det en backup av filsystemene hver dag, slik at vi kan gjenopprette filene dine hvis noe skulle skje med PV-en.\nLagringsplassen på PV-en er begrenset til 10 GB per bruker. Grunnen til at vi ikke tilbyr mer lagringsplass er fordi det kun er kode skal lagres her, og at denne lagringsplassen er relativt dyrt sammenlignet med GCS-bøtter. Hvis du jobber i virtuelle miljøer og lagrer mange miljøer lokalt, kan du oppleve at disken blir full. Les mer her om hvordan man bør håndere dette.\n\n\n\n\n\nTidligere har vi diskutert forskjellene mellom bøtter og filsystemer. Mange kjenner hvordan man gjør systemkommandoer2 i klassiske filsystemer fra en terminal eller fra språk som Python, R og SAS. I dette kapitlet viser vi hvordan dette kan gjøres mot bøtter fra Jupyterlab.\ngcsfs er en Python-pakke som lar deg jobbe med objekter i bøtter på nesten samme måte som filer i et filsystem. For å kunne gjøre det må vi først sette opp en filsystem-instans som lar oss bruke en bøtte som et filsystem. Pakken dapla-toolbelt lar oss gjøre det ganske enkelt:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs er nå et filsystem-versjon av bøttene vi har tilgang til på GCS. Vi kan nå bruk fs til å gjøre typiske operasjoner vi har vært vant til å gjøre i en Linux-terminal. Se en fullstendig liste over kommandoer her.\nUnder vises noen flere eksempler på nyttige funksjoner enn det som ble vist i kapitlet tidligere i boken\n\n\n\n\n\n\nWarning\n\n\n\nSelv om ordet mapper blir brukt i eksemplene under, så er det viktig å huske at det ikke finnes noen mapper i bøtter. Det er bare en tekststreng som er en del av objekt-navnet. Men siden vi her jobber med et filsystem-representasjon av bøtten, så tillater vi oss å gjøre det for å gjøre det enklere å lese.\n\n\n\n\nfs.glob() lar oss søke etter filer i bøtten. Vi kan bruke *, **, ? og [..] som wildcard for å finne det vi trenger. Det som returneres er enten en liste eller dictionary avhengig av hva vi velger å gjøre.\nHent en liste over alle filer i en undermappe R_smoke_test i bøtta gs://ssb-dapla-felles-data-delt-prod:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/*\")\n\nNår vi legger til * på slutten av filstien så returnerer den alle filer i den eksakte undermappen. Men hvis vi ønsker å å få alle filer i alle undermapper, så kan vi bruke ** på denne måten:\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/**\").\nVi kan også søke mer avansert ved ved å bruke ?. ?-tegnet sier at en enkeltkarakter kan være hva som helst. F.eks. kunne vi skrevet\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/**/R_smoke_test_??.csv\")\nfor å rekursivt liste ut alle filer som starter med R_smoke_test_, etterfulgt av to karakterer av hvilken som helst type, og slutter med .csv.\nHvis vi viste at de to karakterene ikke skulle være av hvilken som helst karakterer, men det var en liten bokstav, etterfulgt av et tall mellom 2-6, så kunne vi brukt [a-z] og [2-6] for å spesifisere dette. F.eks. kunne vi skrevet:\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/**/R_smoke_test_[a-z][2-6].csv\")\nSom vi ser er fs.glob() et verktøy som git oss mye den funksjonaliteten vi er vant til fra Linux-terminalen. Men i tillegg kan vi bruke den til å hente inn metadataene til de filene/objektene vi får treff på, ved å bruke argumentet detail=True. Her er et eksempel:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\n    \"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/**/R_smoke_test_/[a-z][2-6].csv\",\n    detail=True,\n)\n\nMetadataene gir deg da informasjon om filstørrelse, tidspunkt for opprettelse, tidspunkt for siste endring, og hvem som har opprettet og endret filen.\n\n\n\nI noen tilfeller kan det være nyttig å sjekke om en fil eksisterer i bøtten. Det kan vi gjøre med fs.exists():\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.exists(\"gs://ssb-dapla-felles-data-delt-prod/altinn3/form_dc551844cd74.xml\")\n\nKoden returnerer enten True eller False avhengig av om filen eksisterer eller ikke.\n\n\n\nHvis du lurer på hvor mange GB data du har i en bøtte, så kan du bruke fs.du():\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\ntotal_bytes = fs.du(\n    \"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/\",\n)\n\ntotal_size_gb = total_bytes / (1024**3)\nprint(f\"Total size: {total_size_gb:.3f} GB\")\n\n\n\n\nHvert objekt i bøtta har metadata knyttet til seg som kan hentes inn i Jupyter og benyttes.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\nfile = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/form_dc551844cd74.xml\"\n\nfs.info(file)\n\nDette gir deg blant annet informasjon om filstørrelse, tidspunkt for opprettelse, og tidspunkt for siste endring. Hvis du ønsker dette for flere filer så kan man også bruke fs.glob(&lt;pattern&gt;, details=True) som vi så på tidligere.\n\n\n\nfs.open() lar oss åpne en fil i bøtta for lesing og skriving med vanlige Python-funksjoner. Funksjonen returnerer et fil-aktig objekt som kan brukes med vanlige som Python-biblioteker som Pandas og NumPy.\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\nfile_path = \"gs://ssb-dapla-felles-data-delt-prod/dapla-metrics/number-of-teams.csv\"\n\n# Bruker fs.open() til å åpne fil, og deretter leses den med Pandas\nwith fs.open(file_path, \"r\") as file:\n    df = pd.read_csv(file)\ndf\n\nDu kan også bruke fs.open() til å skrive til en fil i bøtta. Her er et eksempel på hvordan man skriver en parquet-fil med Pandas og PyArrow:\n\n\nnotebook\n\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom dapla import FileClient\n\nfile_path = \"gs://ssb-dapla-felles-data-delt-prod/dapla-metrics/test.parquet\"\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\n# Lager eksempeldata\ndata = {\"A\": [1, 2, 3], \"B\": [4, 5, 6]}\ndf = pd.DataFrame(data)\n\n# Skriver parquet-fil\nwith fs.open(file_path, \"wb\") as file:\n    pq.write_table(pa.Table.from_pandas(df), file)\n\nOver brukte vi wb for å åpne den binære filen for skriving. Hvis du ønsker å lese fra en binær fil så bruker du rb. Skulle du jobbet en ren tekstfil, så hadde man brukt w til å skrive og r til å lese.\n\n\n\nfs.touch() lar deg opprette en tom fil i bøtta, eller oppdatere metadataene til objektet for når den sist ble modifisert.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nfs.touch(\"gs://ssb-dapla-felles-data-delt-prod/dapla-metrics/test.parquet\")\n\n\n\n\nfs.put() lar deg laste opp en fil fra Jupyter-filsystemet3 til bøtta. Husk at filstien til ditt hjemmeområde på Jupyter er /home/jovyan/. Her er et eksempel på hvordan man kan bruke det på enkelt-filer:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"/home/jovyan/custom.scss\"\ndestination = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/test2.csv\"\n\nfs.put(source, destination)\n\nDu kan også kopiere hele mapper mellom jovyan og bøttene:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"/home/jovyan/sesongjustering/\"\ndestination = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/sesongjustering/\"\n\nfs.put(source, destination, recursive=True)\n\nEt brukstilfellet for å kopiere mellom bøtter og jovyan er ved sesongjustering med Jdemetra+ og JWSACruncher. Siden Jdemetra+ ikke kan lese/skrive fra bøtter, så må vi midlertidig kopiere dataene til jovyan med fs.put() før vi kan kjøre sesongjusteringen. Når vi er ferdige med kjøringen kopierer vi dataene tilbake til bøtta med fs.get().\n\n\n\nfs.get() gjør det samme som fs.put(), bare motsatt vei. Den kopierer fra en bøtte til jovyan.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/sesongjustering/\"\ndestination = \"/home/jovyan/sesongjustering/\"\n\nplaceholder = fs.get(source, destination, recursive=True)\n\nfs.get() returnerer potensielt mye output. Hvis du ikke vil ha det, så kan du bare definere det som et objekt. F.eks. placeholder som i eksempelet over.\n\n\n\nfs.mv() lar deg flytte filer og mapper mellom bøtter, samt å gi objekter nye navn.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\nsource = \"gs://ssb-dapla-felles-data-delt-prod/dapla-metrics/number-of-teams.csv\"\ndestination = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/number-of-teams.csv\"\nfs.mv(source, destination)\n\n\n\n\nfs.copy() lar deg kopiere filer og mapper mellom bøtter. I eksempelet under kopierer vi rekursivt:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfrom_path = \"gs://ssb-prod-arbmark-skjema-data-kilde/ledstill/altinn/2022/11/21/\"\nto_path = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/\"\nfs.copy(from_path, to_path, recursive=True)\n\n\n\n\nfs.rm() lar deg slette filer og mapper i bøtta. Her sletter vi en enkeltfil:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfs.rm(\"gs://ssb-dapla-felles-data-delt-prod/altinn3/number-of-teams.csv\")\n\nOgså denne funksjonen tar et recursive-argument hvis du ønsker å slette en hel mappe."
  },
  {
    "objectID": "statistikkere/hva-er-botter.html#bøtter-vs-filsystemer",
    "href": "statistikkere/hva-er-botter.html#bøtter-vs-filsystemer",
    "title": "Hva er bøtter?",
    "section": "",
    "text": "I et Linux- eller Windows-filsystem, som vi har vært vant til tidligere, så er filer og mapper organisert i en hierarkisk struktur på et operativsystem (OS). I SSB har OS-ene vært installert på fysiske maskiner som vi vedlikeholder selv.\nEn bøtte i GCS er derimot en kjøpt tjeneste som lar brukeren lagre alle typer objekter i en container. Man trenger altså ikke å tenke på om filene ligger i et hierarki, hvilket operativsystem det kjører på, eller hvor mye diskplass som er tilgjengelig.\nAt data er lagret som objekter i en bøtte har noen praktiske implikasjoner for hvordan vi jobber med data i SSB:\n\nMange er vant til å jobbe direkte med filer i en Linux-terminal eller via systemkall fra språk som SAS, Pyton eller R. For å gjøre det samme i Jupyter mot en bøtte, så kan vi bruke Python-pakken gcsfs. Se eksempler under.\nNår vi bruker Python- eller R-pakker for lese eller skrive data fra bøtter, så er vi avhengig av at pakkene tilbyr integrasjon mot bøtter. Mange pakker gjør det, men ikke alle. For de som ikke gjør det kan vi bruke ofte bruke gcsfs til å gi oss et python-objekt, som igjen kan brukes av de fleste pakker.\nDet finnes egentlig ikke noen mapper i bøtter. I motsetning til et vanlig filsystem så er det ikke en hierarkisk mappestruktur i en bøtte. Det vil si at alt ligger et sted, men de kan ha lange navn med /-tegn slik at det ligner på et klassisk filsystem. Bruker du / i objekt-navnet så vil også Google Cloud Console vise det som mapper, men det er bare for å gjøre det enklere å forholde seg til. En praktisk konsekvens av dette er f.eks. at man ikke trenger å opprette en mappe før man legger et fil/objekt i den. Det er bare en tekststreng som er en del av objekt-navnet.\n\nUnder kommer det en del eksempler på hvordan man kan jobbe med objekter i bøtter på samme måte som filer i et filsystem."
  },
  {
    "objectID": "statistikkere/hva-er-botter.html#lokalt-filsystem-på-dapla",
    "href": "statistikkere/hva-er-botter.html#lokalt-filsystem-på-dapla",
    "title": "Hva er bøtter?",
    "section": "",
    "text": "På Dapla skal data lagres i bøtter. Men når du åpner Jupyterlab så får du også et “lokalt” eller klassisk filsystem, slik vi definerte det i forrige kapittel. Det er dette filsystemet du ser i filutforskeren i Jupyterlab, slik som vist til venstre i Figur 1. Det er også dette filsystemet du ser når du f.eks. bruker ls-kommandoen i en terminal i Jupyterlab.\n\n\n\nFigur 1: Til venstre i bildet ser du filutforskeren i Jupyterlab\n\n\nDette filsystemet er ment for å lagre kode midlertidig mens du jobber med dem. Det er ikke ment for å lagre data. Det er heller ikke ment som et permanent lagringssted for kode. Permanent lagring av kode skal gjøres på GitHub. Selv om filene du lagrer der fortsetter å eksistere for hver gang du logger deg inn i Jupyterlab, så bør kode du ønsker å bevare pushes til GitHub før du avslutter en sesjon i Jupyterlab. Se en mer teknisk beskrivelse av hvordan dette fungerer i boksen under.\n\n\n\n\n\n\nHvem er egentlig denne Jovyan?\n\n\n\nNår du logger deg inn i Jupyterlab på Dapla, så ser du at brukeren din på det lokale filsystemet heter jovyan. Grunnen er at det er en Linux-container som kjører et Jupyterlab Docker-image, og alle bruker denne containeren. Det er derfor vi kaller det et “lokalt” filsystem, fordi det er lokalt i containeren.\nI tillegg er det satt opp persistent volum (PV) og en persistent volume claim (PVC) som er knyttet til denne containeren. Det er dette som gjør at filene du lagrer i Jupyterlab blir lagret mellom hver gang du logger deg inn. I tillegg tas det en backup av filsystemene hver dag, slik at vi kan gjenopprette filene dine hvis noe skulle skje med PV-en.\nLagringsplassen på PV-en er begrenset til 10 GB per bruker. Grunnen til at vi ikke tilbyr mer lagringsplass er fordi det kun er kode skal lagres her, og at denne lagringsplassen er relativt dyrt sammenlignet med GCS-bøtter. Hvis du jobber i virtuelle miljøer og lagrer mange miljøer lokalt, kan du oppleve at disken blir full. Les mer her om hvordan man bør håndere dette."
  },
  {
    "objectID": "statistikkere/hva-er-botter.html#systemkommandoer-mot-bøttter",
    "href": "statistikkere/hva-er-botter.html#systemkommandoer-mot-bøttter",
    "title": "Hva er bøtter?",
    "section": "",
    "text": "Tidligere har vi diskutert forskjellene mellom bøtter og filsystemer. Mange kjenner hvordan man gjør systemkommandoer2 i klassiske filsystemer fra en terminal eller fra språk som Python, R og SAS. I dette kapitlet viser vi hvordan dette kan gjøres mot bøtter fra Jupyterlab.\ngcsfs er en Python-pakke som lar deg jobbe med objekter i bøtter på nesten samme måte som filer i et filsystem. For å kunne gjøre det må vi først sette opp en filsystem-instans som lar oss bruke en bøtte som et filsystem. Pakken dapla-toolbelt lar oss gjøre det ganske enkelt:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs er nå et filsystem-versjon av bøttene vi har tilgang til på GCS. Vi kan nå bruk fs til å gjøre typiske operasjoner vi har vært vant til å gjøre i en Linux-terminal. Se en fullstendig liste over kommandoer her.\nUnder vises noen flere eksempler på nyttige funksjoner enn det som ble vist i kapitlet tidligere i boken\n\n\n\n\n\n\nWarning\n\n\n\nSelv om ordet mapper blir brukt i eksemplene under, så er det viktig å huske at det ikke finnes noen mapper i bøtter. Det er bare en tekststreng som er en del av objekt-navnet. Men siden vi her jobber med et filsystem-representasjon av bøtten, så tillater vi oss å gjøre det for å gjøre det enklere å lese.\n\n\n\n\nfs.glob() lar oss søke etter filer i bøtten. Vi kan bruke *, **, ? og [..] som wildcard for å finne det vi trenger. Det som returneres er enten en liste eller dictionary avhengig av hva vi velger å gjøre.\nHent en liste over alle filer i en undermappe R_smoke_test i bøtta gs://ssb-dapla-felles-data-delt-prod:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/*\")\n\nNår vi legger til * på slutten av filstien så returnerer den alle filer i den eksakte undermappen. Men hvis vi ønsker å å få alle filer i alle undermapper, så kan vi bruke ** på denne måten:\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/**\").\nVi kan også søke mer avansert ved ved å bruke ?. ?-tegnet sier at en enkeltkarakter kan være hva som helst. F.eks. kunne vi skrevet\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/**/R_smoke_test_??.csv\")\nfor å rekursivt liste ut alle filer som starter med R_smoke_test_, etterfulgt av to karakterer av hvilken som helst type, og slutter med .csv.\nHvis vi viste at de to karakterene ikke skulle være av hvilken som helst karakterer, men det var en liten bokstav, etterfulgt av et tall mellom 2-6, så kunne vi brukt [a-z] og [2-6] for å spesifisere dette. F.eks. kunne vi skrevet:\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/**/R_smoke_test_[a-z][2-6].csv\")\nSom vi ser er fs.glob() et verktøy som git oss mye den funksjonaliteten vi er vant til fra Linux-terminalen. Men i tillegg kan vi bruke den til å hente inn metadataene til de filene/objektene vi får treff på, ved å bruke argumentet detail=True. Her er et eksempel:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\n    \"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/**/R_smoke_test_/[a-z][2-6].csv\",\n    detail=True,\n)\n\nMetadataene gir deg da informasjon om filstørrelse, tidspunkt for opprettelse, tidspunkt for siste endring, og hvem som har opprettet og endret filen.\n\n\n\nI noen tilfeller kan det være nyttig å sjekke om en fil eksisterer i bøtten. Det kan vi gjøre med fs.exists():\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.exists(\"gs://ssb-dapla-felles-data-delt-prod/altinn3/form_dc551844cd74.xml\")\n\nKoden returnerer enten True eller False avhengig av om filen eksisterer eller ikke.\n\n\n\nHvis du lurer på hvor mange GB data du har i en bøtte, så kan du bruke fs.du():\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\ntotal_bytes = fs.du(\n    \"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/\",\n)\n\ntotal_size_gb = total_bytes / (1024**3)\nprint(f\"Total size: {total_size_gb:.3f} GB\")\n\n\n\n\nHvert objekt i bøtta har metadata knyttet til seg som kan hentes inn i Jupyter og benyttes.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\nfile = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/form_dc551844cd74.xml\"\n\nfs.info(file)\n\nDette gir deg blant annet informasjon om filstørrelse, tidspunkt for opprettelse, og tidspunkt for siste endring. Hvis du ønsker dette for flere filer så kan man også bruke fs.glob(&lt;pattern&gt;, details=True) som vi så på tidligere.\n\n\n\nfs.open() lar oss åpne en fil i bøtta for lesing og skriving med vanlige Python-funksjoner. Funksjonen returnerer et fil-aktig objekt som kan brukes med vanlige som Python-biblioteker som Pandas og NumPy.\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\nfile_path = \"gs://ssb-dapla-felles-data-delt-prod/dapla-metrics/number-of-teams.csv\"\n\n# Bruker fs.open() til å åpne fil, og deretter leses den med Pandas\nwith fs.open(file_path, \"r\") as file:\n    df = pd.read_csv(file)\ndf\n\nDu kan også bruke fs.open() til å skrive til en fil i bøtta. Her er et eksempel på hvordan man skriver en parquet-fil med Pandas og PyArrow:\n\n\nnotebook\n\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom dapla import FileClient\n\nfile_path = \"gs://ssb-dapla-felles-data-delt-prod/dapla-metrics/test.parquet\"\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\n# Lager eksempeldata\ndata = {\"A\": [1, 2, 3], \"B\": [4, 5, 6]}\ndf = pd.DataFrame(data)\n\n# Skriver parquet-fil\nwith fs.open(file_path, \"wb\") as file:\n    pq.write_table(pa.Table.from_pandas(df), file)\n\nOver brukte vi wb for å åpne den binære filen for skriving. Hvis du ønsker å lese fra en binær fil så bruker du rb. Skulle du jobbet en ren tekstfil, så hadde man brukt w til å skrive og r til å lese.\n\n\n\nfs.touch() lar deg opprette en tom fil i bøtta, eller oppdatere metadataene til objektet for når den sist ble modifisert.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nfs.touch(\"gs://ssb-dapla-felles-data-delt-prod/dapla-metrics/test.parquet\")\n\n\n\n\nfs.put() lar deg laste opp en fil fra Jupyter-filsystemet3 til bøtta. Husk at filstien til ditt hjemmeområde på Jupyter er /home/jovyan/. Her er et eksempel på hvordan man kan bruke det på enkelt-filer:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"/home/jovyan/custom.scss\"\ndestination = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/test2.csv\"\n\nfs.put(source, destination)\n\nDu kan også kopiere hele mapper mellom jovyan og bøttene:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"/home/jovyan/sesongjustering/\"\ndestination = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/sesongjustering/\"\n\nfs.put(source, destination, recursive=True)\n\nEt brukstilfellet for å kopiere mellom bøtter og jovyan er ved sesongjustering med Jdemetra+ og JWSACruncher. Siden Jdemetra+ ikke kan lese/skrive fra bøtter, så må vi midlertidig kopiere dataene til jovyan med fs.put() før vi kan kjøre sesongjusteringen. Når vi er ferdige med kjøringen kopierer vi dataene tilbake til bøtta med fs.get().\n\n\n\nfs.get() gjør det samme som fs.put(), bare motsatt vei. Den kopierer fra en bøtte til jovyan.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/sesongjustering/\"\ndestination = \"/home/jovyan/sesongjustering/\"\n\nplaceholder = fs.get(source, destination, recursive=True)\n\nfs.get() returnerer potensielt mye output. Hvis du ikke vil ha det, så kan du bare definere det som et objekt. F.eks. placeholder som i eksempelet over.\n\n\n\nfs.mv() lar deg flytte filer og mapper mellom bøtter, samt å gi objekter nye navn.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\nsource = \"gs://ssb-dapla-felles-data-delt-prod/dapla-metrics/number-of-teams.csv\"\ndestination = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/number-of-teams.csv\"\nfs.mv(source, destination)\n\n\n\n\nfs.copy() lar deg kopiere filer og mapper mellom bøtter. I eksempelet under kopierer vi rekursivt:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfrom_path = \"gs://ssb-prod-arbmark-skjema-data-kilde/ledstill/altinn/2022/11/21/\"\nto_path = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/\"\nfs.copy(from_path, to_path, recursive=True)\n\n\n\n\nfs.rm() lar deg slette filer og mapper i bøtta. Her sletter vi en enkeltfil:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfs.rm(\"gs://ssb-dapla-felles-data-delt-prod/altinn3/number-of-teams.csv\")\n\nOgså denne funksjonen tar et recursive-argument hvis du ønsker å slette en hel mappe."
  },
  {
    "objectID": "statistikkere/hva-er-botter.html#footnotes",
    "href": "statistikkere/hva-er-botter.html#footnotes",
    "title": "Hva er bøtter?",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nEgentlig har vi jobbet med data-filer på både Linux- og Windows-filsystemer. Men Linux-stammene har vært det anbefalte stedet å lagre datafiler.↩︎\nMed systemkommandoer så mener vi bash-kommandoer som ls og mv, eller implementasjoner av disse kommandoene i Python, R eller SAS.↩︎\nJupyter-miljøet har sitt eget filsystem, ofte kalt jovyan. Det er som et vanlig Linux-filsystem, og vil være det vi omtaler som “lokalt” på maskinen din i Jupyter.↩︎"
  },
  {
    "objectID": "statistikkere/opprette-dapla-team.html",
    "href": "statistikkere/opprette-dapla-team.html",
    "title": "Opprette Dapla-team",
    "section": "",
    "text": "Opprette Dapla-team\nFor å komme i gang med å opprette et Dapla-team trengs det en oversikt over teamets medlemmer og hvilke tilgangsgrupper medlemmene skal være med i. Det trengs også informasjon om hvilke Dapla-tjenester som er aktuelle for teamet å ta i bruk. Derfor har det blitt opprettet en egen veileder for dette kalt Dapla Start.\n\n\n\n\n\n\nGå til Dapla Start for starte bestilling av et nytt Dapla-team.\n\n\n\nNår teamet er opprettet får alle medlemmene tilgang til sitt eget prosjekt i Google Cloud Platform (GCP), som er SSBs leverandør av skytjenester. Videre får hvert prosjekt et sett med tjenester og tilganger som knyttes til teamet. Det opprettes også datalagringsområder (kalt bøtter) som bare kan aksesseres av brukere som er med i teamets tilgangsgrupper.\nDapla-teamet vil også få sin egen gruppe i SSBs Active Directory slik at medlemskapet i gruppen kan administreres av Kundeservice."
  },
  {
    "objectID": "statistikkere/produksjonsløp.html",
    "href": "statistikkere/produksjonsløp.html",
    "title": "Produksjonsløp",
    "section": "",
    "text": "Produksjonsløp\nI forrige del beskrev vi noen grunnleggende ting rundt Dapla. I denne delen tar vi for oss hvordan du kan begynne å jobbe med skarpe data på plattformen.\nKapittelet som beskriver hvordan man logger seg inn på Dapla vil fungere uten at du må gjøre noen forberedelser. Er man koblet på SSB sitt nettverk så vil alle SSB-ansatte kunne gå inn på plattformen og kode i Python og R. Men du får ikke tilgang til SSBs område for datalagring på plattformen. I praksis vil det si at man kan generere data med kode, men man kan ikke jobbe med skarpe data.\nFor å få muligheten til å jobbe med skarpe data MÅ du først opprette et dapla-team. Dette er det første naturlige steget å ta når man skal begynne å jobbe med statistikkproduksjon på Dapla. I dette kapittelet vil vi forklare det du trenger å vite om det å opprette og jobbe innenfor et team."
  },
  {
    "objectID": "statistikkere/github-app-integrasjon.html",
    "href": "statistikkere/github-app-integrasjon.html",
    "title": "Koble prosjektet til Github",
    "section": "",
    "text": "For at automatiseringsløsningen på Dapla skal kunne settes opp automatisk må denne ha tilgang til å lese fra prosjektets IAC-repo1. Dette avsnittet vil beskrive denne prosessen. Merk at dette er en engangsjobb som må gjøres av prosjektets kildedataansvarlige.\n\n\n\n\n\n\nViktig: Prosjektets kildedataansvarlige også må ha administrator-rettigheter til IAC-repoet i Github.\n\n\n\n\nLogg inn på Google Cloud Console og velg det prosjektet som skal konfigureres øverst venstre hjørte. Søk opp Cloud Build i søkefeltet og trykk på det valget som kommer opp.\nDet skal nå være en venstremeny tilgjengelig med tittel Cloud Build. Trykk på menyvalget som heter Triggers (Figur 1)\n\n\n\n\nFigur 1: Bilde av venstremeny\n\n\n\nI nedtrekkslisten Region sørg for at europe-north1 er valgt (Figur 2)\n\n\n\n\nFigur 2: Velg korrekt region\n\n\n\nTrykk deretter på en link som heter CONNECT REPOSITORY ca. midt på siden.\n\n\n\n\nFigur 3: Oversikt over triggers\n\n\n\nNå vil det dukke opp et vindu på høyre side med overskrift Connect repository (Figur 4). Velg GitHub (Cloud Build GitHub App) og trykk på CONTINUE\n\n\n\n\nFigur 4: Vindu for å velge Cloud Build Github App\n\n\n\nEt pop-up vindu tilsvarende Figur 5 vil komme opp. Trykk på Authorize. Vinduet vil etter hvert lukke seg og man kommer videre til et steg som heter Select repository (Figur 6)\n\n\n\n\nFigur 5: Pop-up vindu for Github\n\n\n\n\n\nFigur 6: Valg av Github repository\n\n\n\nTrykk på nedtrekkslisten Repository og skriv inn teamets navn. Huk av boksen ved teamets IAC-repo og trykk OK.\n\n\n\n\nFigur 7: Gi Google Build tilgang til Github repository\n\n\n\nKryss så av i sjekkboksen som i (Figur 8) og trykk CONNECT.\n\n\n\n\nFigur 8: Bekreft nytt Github repository\n\n\n\nTil slutt vil skjermbildet se ut som vist i Figur 9. Det siste steget Create a trigger kan du hoppe over. Dette vil bli satt opp av automatiseringsløsningen senere. Trykk på knappen DONE\n\n\n\n\nFigur 9: Siste steg - Create a trigger"
  },
  {
    "objectID": "statistikkere/github-app-integrasjon.html#footnotes",
    "href": "statistikkere/github-app-integrasjon.html#footnotes",
    "title": "Koble prosjektet til Github",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nIAC-repo er et en kodebase i Github på formen https://github.com/statisticsnorway/team-navn-iac.↩︎"
  },
  {
    "objectID": "statistikkere/datadoc.html",
    "href": "statistikkere/datadoc.html",
    "title": "DataDoc",
    "section": "",
    "text": "For å kunne gjenfinne data i SSB er man helt avhengig av at det finnes et enhetlig system for metadata knyttet til dataene. DataDoc er SSBs system for å dokumentere datasett på den nye dataplattformen Dapla.\nDet er bygget et grensesnitt i Python for å gjøre det enklest mulig å dokumentere et datasett. Foreløpig støtter løsningen følgende filformater:\n\nparquet\nsas7bdat\n\nUnder finner du beskrivelse av hvordan du kan begynne å bruke løsningen til å dokumentere datasett.\n\n\n\n\n\n\nWarning\n\n\n\nVi ønsker at du skal teste DataDoc-applikasjonen. Den viktigste funksjonaliteten skal være tilgjengelig, og det er fullt mulig å benytte DataDoc i SSBs Jupyter-miljøer. Det er imidlertid viktig å være klar over at applikasjonen fortsatt er i en utviklings- og testfase (beta-løsning) og kan inneholde feil og mangler.\nHar du spørsmål, eventuelt vil rapporterer om feil og mangler, så setter vi pris på om du gjør dette i Yammer-gruppa Dapla.\n\n\n\n\nFør du tar i bruk DataDoc-applikasjonen er det viktig å forstå hvilken informasjon som skal til for å dokumentere et datasett. I DataDoc-applikasjonen skal du fylle ut flere felter om både datasettet og variablene som inngår i datasettet, eksempelvis\n\nkortnavn\nnavn\ndatatilstand\npopulasjonsbeskrivelse\n++\n\nDet er utarbeidet en detaljert beskrivelse hva hvert felt betyr, og hvordan de skal fylles ut både for datasett og variabler: - DataDoc - hvordan dokumentere et datasett - DataDoc - hvordan dokumentere variablene (variabelforekomstene) som inngår i datasettet\nDataDoc skal være installert i alle Jupyter-miljøene i SSB, så du trenger ikke installere pakken selv.\n\n\n\n\n\n\nViktig informasjon\n\n\n\nDataDoc kan foreløpig ikke kjøres i Jupyter notebook med virtuelle miljøer (f.eks. et ssb-project), men må startes i den vanlige kernelen i en notebook.\n\n\n\n\n\nLa oss lage et test-datasett slik at vi kan leke oss litt med DataDoc:\n\n\nnotebook\n\nimport pandas as pd\nfrom datadoc import main\n  \n# Create fake data\ndata = {'id': ['9999999999', '8888888888', '7777777777', '6666666666'],\n        'fylke': [\"01\", \"02\", \"03\", \"03\"],\n        'inntekt': [500000, 250000, 400000, 440000],\n        'rente': [3.2, 4.1, 3.3, 3.4]}\n  \n# Creates a Pandas dataframe\ndf = pd.DataFrame(data)\n\n# Write a Parquet-file to current folder\ndf.to_parquet(\"./test.parquet\")\n\nNå har vi en fil som heter test.parquet i mappen vi står. Da kan vi åpne DataDoc-grensesnittet for å legge inn metadataene:\n\n\nnotebook\n\nmain(\"./test.parquet\")\n\nFigur 1 viser hvordan DataDoc-grensesnittet ser ut.\n\n\n\nFigur 1: Gif som viser hvordan DataDoc-grensesnittet ser ut.\n\n\n\n\n\nNår du trykker på Lagre-knappen i DataDoc så skrives alle metadata til en fil i samme mappe (katalog) som datafilen. Dette er en JSON-fil med nesten samme navn som datafilen. Navnekonvensjonen for metadatafilen er\n&lt;navn på datafil uten endelse&gt;__DOC.json\nEksempelvis hvis datafilen har navnet skattedata_p2022_v1.parquet, så vil DataDoc lagre metadata i filen skattedata_p2022_v1__DOC.json.\nFordelen med å benytte en JSON-fil til å lagre metadata er at denne filen kan kopieres og flyttes like enkelt som selve datafilen. JSON-filer er strukturerte tekstfiler som kan leses av både maskiner (Python/R) og av mennesker (åpnes i en tekst-editor).\nSe et eksempel på JSON metadata-fil lagret av DataDoc.\n\n\n\n\n\n\nInformasjon\n\n\n\nI Dapla skal det bygges en felles datakatalog for SSB. Tanken er at alle metadata, eksempelvis datasett-dokumentasjon fra DataDoc (JSON-filene), skal inngå i SSBs datakatalog. Datakatalogen gjør det mulig å finne (søke etter), forstår og gjenbruke data både internt og ekstern."
  },
  {
    "objectID": "statistikkere/datadoc.html#hvordan-dokumentere-datasett-og-variabler-med-datadoc",
    "href": "statistikkere/datadoc.html#hvordan-dokumentere-datasett-og-variabler-med-datadoc",
    "title": "DataDoc",
    "section": "",
    "text": "Før du tar i bruk DataDoc-applikasjonen er det viktig å forstå hvilken informasjon som skal til for å dokumentere et datasett. I DataDoc-applikasjonen skal du fylle ut flere felter om både datasettet og variablene som inngår i datasettet, eksempelvis\n\nkortnavn\nnavn\ndatatilstand\npopulasjonsbeskrivelse\n++\n\nDet er utarbeidet en detaljert beskrivelse hva hvert felt betyr, og hvordan de skal fylles ut både for datasett og variabler: - DataDoc - hvordan dokumentere et datasett - DataDoc - hvordan dokumentere variablene (variabelforekomstene) som inngår i datasettet\nDataDoc skal være installert i alle Jupyter-miljøene i SSB, så du trenger ikke installere pakken selv.\n\n\n\n\n\n\nViktig informasjon\n\n\n\nDataDoc kan foreløpig ikke kjøres i Jupyter notebook med virtuelle miljøer (f.eks. et ssb-project), men må startes i den vanlige kernelen i en notebook."
  },
  {
    "objectID": "statistikkere/datadoc.html#prøve-datadoc",
    "href": "statistikkere/datadoc.html#prøve-datadoc",
    "title": "DataDoc",
    "section": "",
    "text": "La oss lage et test-datasett slik at vi kan leke oss litt med DataDoc:\n\n\nnotebook\n\nimport pandas as pd\nfrom datadoc import main\n  \n# Create fake data\ndata = {'id': ['9999999999', '8888888888', '7777777777', '6666666666'],\n        'fylke': [\"01\", \"02\", \"03\", \"03\"],\n        'inntekt': [500000, 250000, 400000, 440000],\n        'rente': [3.2, 4.1, 3.3, 3.4]}\n  \n# Creates a Pandas dataframe\ndf = pd.DataFrame(data)\n\n# Write a Parquet-file to current folder\ndf.to_parquet(\"./test.parquet\")\n\nNå har vi en fil som heter test.parquet i mappen vi står. Da kan vi åpne DataDoc-grensesnittet for å legge inn metadataene:\n\n\nnotebook\n\nmain(\"./test.parquet\")\n\nFigur 1 viser hvordan DataDoc-grensesnittet ser ut.\n\n\n\nFigur 1: Gif som viser hvordan DataDoc-grensesnittet ser ut."
  },
  {
    "objectID": "statistikkere/datadoc.html#hvor-lagres-datadoc-dokumentasjonen-metadata",
    "href": "statistikkere/datadoc.html#hvor-lagres-datadoc-dokumentasjonen-metadata",
    "title": "DataDoc",
    "section": "",
    "text": "Når du trykker på Lagre-knappen i DataDoc så skrives alle metadata til en fil i samme mappe (katalog) som datafilen. Dette er en JSON-fil med nesten samme navn som datafilen. Navnekonvensjonen for metadatafilen er\n&lt;navn på datafil uten endelse&gt;__DOC.json\nEksempelvis hvis datafilen har navnet skattedata_p2022_v1.parquet, så vil DataDoc lagre metadata i filen skattedata_p2022_v1__DOC.json.\nFordelen med å benytte en JSON-fil til å lagre metadata er at denne filen kan kopieres og flyttes like enkelt som selve datafilen. JSON-filer er strukturerte tekstfiler som kan leses av både maskiner (Python/R) og av mennesker (åpnes i en tekst-editor).\nSe et eksempel på JSON metadata-fil lagret av DataDoc.\n\n\n\n\n\n\nInformasjon\n\n\n\nI Dapla skal det bygges en felles datakatalog for SSB. Tanken er at alle metadata, eksempelvis datasett-dokumentasjon fra DataDoc (JSON-filene), skal inngå i SSBs datakatalog. Datakatalogen gjør det mulig å finne (søke etter), forstår og gjenbruke data både internt og ekstern."
  },
  {
    "objectID": "statistikkere/statistikkproduksjon.html",
    "href": "statistikkere/statistikkproduksjon.html",
    "title": "Statistikkproduksjon",
    "section": "",
    "text": "Statistikkproduksjon\nI forrige del beskrev vi noen grunnleggende ting rundt Dapla. I denne delen tar vi for oss hvordan du kan begynne å jobbe med skarpe data på plattformen.\nKapittelet som beskriver hvordan man logger seg inn på Dapla vil fungere uten at du må gjøre noen forberedelser. Er man koblet på SSB sitt nettverk så vil alle SSB-ansatte kunne gå inn på plattformen og kode i Python og R. Men du får ikke tilgang til SSBs område for datalagring på plattformen. I praksis vil det si at man kan generere data med kode, men man kan ikke jobbe med skarpe data.\nFor å få muligheten til å jobbe med skarpe data MÅ du først opprette et dapla-team. Dette er det første naturlige steget å ta når man skal begynne å jobbe med statistikkproduksjon på Dapla. I dette kapittelet vil vi forklare det du trenger å vite om det å opprette og jobbe innenfor et team."
  },
  {
    "objectID": "statistikkere/navnestandard-datalagring.html",
    "href": "statistikkere/navnestandard-datalagring.html",
    "title": "Navnestandard datalagring",
    "section": "",
    "text": "Krav til dokumentasjon av datasett\n\n\n\nI SSB er det et krav at datasett i datatilstandene1 (inndata), klargjorte data, statistikk og utdata dokumenteres. Det er spesielt viktig at klargjorte data, som er grunnlag både for statistikk, arkiv og datadeling, dokumenteres godt. Dokumentasjon er nødvendig for at vi skal finne, forstå og (gjen)bruke SSBs data. De fysiske datasettene og variablene som inngår i disse skal dokumenteres med bruk av metadataløsningene DataDoc (datasettdokumentasjon), VarDef 2 (variabeldefinisjoner) og KLASS (kodelister og klassifikasjoner).\nSe også detaljerte krav til datasett-metadata i dokumentene DataDoc - Krav til dokumentasjon av datasett og for variablene som inngår i datasettet DataDoc - Variabelforekomst\nEn viktig del av datadokumentasjonsarbeidet er hvordan datasett versjoneres og organiseres i en standardisert mappestruktur (katalogstruktur). Dette dokumentet beskriver en navnestandard for mappestruktur og datasett (datafilene), i tillegg til regler for versjonering av datasett (datafilene).\n1 Datatilstanden “kildedata” omfattes ikke av denne navnestandarden i og med at kildedata mottas av SSB i mange former/strukturer, og at kildedata i liten grad skal deles internt og eksternt (begrenset tilgang). Det må eventuelt vurderes om det skal utarbeides egne retningslinjer for lagring av kildedata. Datatilstanden “inndata” er nevnt i parentes i og med at denne datatilstanden ikke er obligatorisk.\n2 Utvikling av nye VarDef er i skrivende stund ikke påbegynt. SSBs eksisterende variabeldefinisjonsløsning Vardok skal derfor benyttes fram til ny løsning er klar.\n\n\n\n\n\n\n\n\nKrav til lagringsformater for datasett\n\n\n\nDokumentet “Standardformater for datalagring i SSB” beskriver krav til filformater og dataformater ved lagring av klargjorte data som skal benyttes til utarbeiding av statistikk, datadeling og arkivering."
  },
  {
    "objectID": "statistikkere/navnestandard-datalagring.html#mapper-kataloger",
    "href": "statistikkere/navnestandard-datalagring.html#mapper-kataloger",
    "title": "Navnestandard datalagring",
    "section": "Mapper (kataloger)",
    "text": "Mapper (kataloger)\n\nStandard lagringsområder (bøtter) som opprettes for alle statistikkprodukt-team i SSB\nFølgende lagringsområder (bøtter) operettes for alle team:\n\nssb-prod-&lt;teamnavn&gt;-data-kilde : Inneholder ubehandlede rådata fra datakildene.\nssb-prod-&lt;teamnavn&gt;-data-produkt : Inneholder data knyttet til statistikkproduktet.\nssb-prod-&lt;teamnavn&gt;-data-delt : Inneholder data knyttet til statistikkproduktet som kan deles med andre statistikkteam.\n\nDenne navnestandarden gjelder primært for lagringsområdene data-produkt og data-delt, men er også anbefalt brukt for data-kilde*.\n\n\nDatatilstander, prosesser, permanente data og temporære data\nI en statistikkproduksjon skal det for hver datatilstand lagres permanente datasett (datafiler). Disse datasettene skal følge denne navnestandarden inkludert kravet til versjonering og dokumentasjon (metadata). Det er imidlertid viktig å skille mellom behovet for permanente data og temporære data. I prosessene som kjøres mellom hver datatilstand, f.eks. klargjøringsprosessene mellom inndata og klargjorte data, vil det være behov for temporær datalagring. Temporære data skal aldri deles, og det stilles derfor ingen krav til verken navnestandard, versjonering eller dokumentasjon (metadata) av disse. Det er helt opp til hvert produkt-team hvordan de vil organisere temporære data. Ved å skille mellom permanent datalagring og temporære datalagring oppnår vi en optimal løsning både for dataprodusenter (statistikkseksjonene) og data-konsumentene (interne og eksterne brukere). Produsentene får nødvendig fleksibilitet til å prosessere data i temporære områder, mens konsumentene får godt dokumenterte og versjonerte data i en standardisert mappe-struktur og tilgjengelig for gjenfinning i en søkbar datakatalog.\n\n\n\nFigur 1: Datatilstander, permanente og temporære data\n\n\n\n\nStatistikkprodukter og dataprodukter\n\nStatistikkprodukter\nAlle SSBs tidligere og nåværende statistikkprodukter inngår Statistikkregisteret. Før publisering på ssb.no må alle statistikkprodukter være registrert i Statistikkregisteret med informasjon om bl.a. statistikkens navn, emne-område, eierseksjon og publiseringstidspunkt. I tillegg får statistikkene tildelt et kortnavn. Eksempler på statistikk-kortnavn er:\n\n\"kpi\" for konsumprisindeksen\n\"reise\" for reiseundersøkelsen\n\"ftot\" for næringslivstjenester, omsetning etter tjenestetype\n\nKortnavnene er unike og stabilt over tid (uforanderlige). De er derfor valgt som grunnlag for kategorisering/inndeling av datasett i Dapla, dvs. benyttes som grunnlag for navn på mappene i lagringsområdene (bøttene).\nStatistikkregisteret har også et API for å hente informasjon om alle SSBs statistikker i json- format.\n\n\nDataprodukter\nDet er imidlertid ikke slik at alle data i SSB kan knyttes direkte til en statistikk i Statistikkregisteret. Flere statistikkseksjoner i SSB bearbeider også data til andre bruksområder og formål, eksempelvis klargjøring av data til forskning og utlån, bearbeiding av data som skal inngå som en del av andre statistikker, og data som inngår i populasjonsregistre. Denne typen data omtales i dette dokumentet som “dataprodukter”, og i navnestandarden skiller vi mellom “dataprodukter” og “statistikkprodukter”. Det eksisterer ikke et register med “kortnavn” for data-produktene i SSB, men hvert team må lage kortnavn også for dataproduktene. Eksempler på dataprodukt-kortnavn er “nudb” (utdanningsdatabasen) og “fd_trygd” (forløpsdatabasen for trygdedata).\n\n\n\n\n\n\n“_data” - navnekonvensjon for dataprodukter\n\n\n\nFor å skille mellom dataprodukter og statistikkprodukter skal navnet på alle mapper som representerer dataprodukter ha endelsen “_data”, f.eks. “nudb_data” og “fd_trygd_data”.\n\n\n\n\n\nMappestruktur for organisering av datasett i Dapla-teamenes lagringsområder (bøtter)\nMed utgangspunkt i standard bøtter som opprettes for alle statistikk-team i DAPLA, Statistikkregisteret og datatilstander, er det utarbeidet følgende regler for mappestrukturen og navngiving av mappene i bøttene (gjelder for data-produkt-bøtte og data-delt-bøtte, men anbefalt også for data-kilde- bøtte) :\nssb-prod-&lt;team-name&gt;-data-produkt/  \n└─ &lt;statistikk-kortnavn&gt; | &lt;dataprodukt&gt;_data/  \n   └── &lt;datatilstand&gt;/  \n       ├── [datasett-1]  \n       ├── [datasett-2]  \n       └── [datasett-NN]\n\n\n\n\n\n\nForklaring av mappe-nivåer i navenstandarden\n\n\n\n\nFørste nivå er lagringsområdet (bøtte)\nAndre nivå er:\n-&gt; enten kortnavn fra Statistikkregisteret\n-&gt; eller et dataprodukt-navn (“kortnavn”)\nTredje nivå er datatilstand\nFjerde nivå er datasett (datafiler)\n\n\n\n\nStøtte for “egendefinerte under-mapper” ved behov for organisering av datasett i flere nivåer\nVed behov er det tillatt å utvide mappestrukturen med med flere egendefinerte nivåer (nivå 4, 5, 6, .., N). Dette kan være nyttig for team som har veldig mange datasett og har behov for å gruppere disse i flere undermapper:\nssb-prod-&lt;team-name&gt;-data-produkt/  \n└─ &lt;statistikk-kortnavn&gt; | &lt;dataprodukt&gt;_data/  \n   └── &lt;datatilstand&gt;/  \n       └── &lt;egen under-mappe&gt;/  \n           └── &lt;egen under-under-mappe&gt;/  \n               └── &lt;.. osv.&gt;/  \n                   ├── [datasett-1]  \n                   ├── [datasett-2]  \n                   └── [datasett-NN]\n\n\n\n\n\n\nEventuell bruk av egendefinerte nivåer i mappestrukturen\n\n\n\n\nFørste nivå er lagringsområdet (bøtte)\nAndre nivå er:\n-&gt; enten kortnavn fra Statistikkregisteret\n-&gt; eller et dataprodukt-navn (“kortnavn”)\nTredje nivå er datatilstand\nFjerde nivå er datasett (datafiler)\nNivå 4, 5, osv. er egendefinerte under-mapper\nNederste nivå er datasett (datafiler)\n\n\n\n\n\nStøtte for temporære data (temp-mappe) og oppdragsdata (oppdrag-mappe)\nVed behov for lagring av temporære data (tilsvarende wk-katalogene på Linux på bakken) er det støtte for å opprette en temp -mappe. Temporære data er kun tillatt i data-produkt-bøtten, bør fjernes etter en viss tid, og skal ikke deles med andre (kun tilgjengelige innenfor eget team).\nDet er også anbefalt å opprette en oppdrag -mappe for team som jobber med oppdragsvirksomhet. Egne regler gjelder for behandling og oppbevaring av oppdragsdata. Det er derfor ønskelig at disse organiseres i en egen mappe. Utover dette er det anbefalt å ha med WebSak-saksnummer til oppdraget enten som en undermappe eller som en del av datasett-navnet.\nssb-prod-&lt;team-name&gt;-data-produkt/  \n└─ &lt;statistikk-kortnavn&gt; | &lt;dataprodukt&gt;_data/  \n   └── &lt;datatilstand&gt;/  \n   └── &lt;datatilstand&gt;/  \n       ├── [datasett-1]  \n       ├── [datasett-2]  \n       └── [datasett-NN]\n└─ temp/  \n   ├── [temp-datasett-A]  \n   └── [temp-datasett-X]  \n└─ oppdrag/  \n   └── &lt;WebSak-saksnummer&gt;/  \n       ├── [oppdrag-datasett-Y]  \n       └── [oppdrag-datasett-Z]\n\n\n\n\n\n\nBruk av temp-mappe og oppdrag-mappe\n\n\n\n\nFørste nivå er lagringsområdet (bøtte)\nAndre nivå er:\n-&gt; enten kortnavn fra Statistikkregisteret\n-&gt; eller et dataprodukt-navn (“kortnavn”)\nTredje nivå er datatilstand\nFjerde nivå er datasett (datafiler)\n\nHer vises også:\n\ntemp-mappe for temporære data\noppdrag-mappe for oppdragsdata\n\n\n\n\n\n\nEksempel på mappestruktur i data-produkt-bøtte\nEksempel “Team overnaturlig”\nNedenfor vises et eksempel på hvordan et tenkt team “ Team overnaturlig ” kan organisere sine tenkte statistikkprodukter “ufo” og “superhelt” i en mappestruktur:\nssb-prod-team-overnaturlig-data-produkt/  \n└── ufo/  \n    ├── inndata/  \n    ├── klargjorte-data/  \n    ├── statistikk/  \n    └── utdata/  \n└── superhelt/  \n    ├── inndata/  \n    ├── klargjorte-data/  \n    ├── statistikk/  \n    └── utdata/  \n└── temp/  \n└── oppdrag/  \nEksempel “Team reiseliv” - Seksjon for næringslivets konjunkturer (S422)\nTeamet har ansvar for 3 statistikk-produkter (kortnavn “overnatting”, “reise” og “grensehandel”)\n\nEtt ALTINN-skjema og 2 utvalgsinnsamlinger med intervjuer på telefon/CATI\nProduksjonsløpet har fokus på statistikkprodukter fra kildedata til utdata\n\nssb-prod-reiseliv-data-produkt/  \n└── overnatting/  \n     ├── inndata/  \n     ├── klargjorte-data/  \n     ├── statistikk/  \n     └── utdata/  \n└── reise/  \n     ├── inndata/  \n     ├── klargjorte-data/  \n     ├── statistikk/  \n     └── utdata/  \n└── grensehandel/  \n     ├── inndata/  \n     ├── klargjorte-data/  \n     ├── statistikk/  \n     └── utdata/  \n└── temp/\nEksempel “Team trygd” - Seksjon for inntekts- og levekårsstatistikk (S350)\nTeamet har datainnsamling fra flere av NAV sine register\n\nData klargjøres og brukes til flere formål, bl.a. utlån av data til forskere (FD-Trygd) og levering til microdata.no (S380)\n\n\n\n\n\n\n\nEksempel på dataprodukt\n\n\n\nTeam trygd klargjør dataprodukter, ikke statistikkprodukter. Alle dataprodukt-kortnavn har derfor endelsen “_data” i eksempel-mappestrukturen nedenfor.\n\n\nssb-prod-trygd-data-produkt/  \n└── barnetrygd_data/  \n    ├── inndata/  \n    ├── klargjorte-data/  \n    ├── statistikk/  \n    └── utdata/  \n└── foedsykp_data/  \n    ├── inndata/  \n    ├── klargjorte-data/  \n    ├── statistikk/  \n    └── utdata/  \n└── pensj_data/  \n    ├── inndata/  \n    ├── klargjorte-data/  \n    ├── statistikk/  \n    └── utdata/  \n└──` … osv.  \n└── temp/  \n└── oppdrag/"
  },
  {
    "objectID": "statistikkere/navnestandard-datalagring.html#filnavn-for-datasett",
    "href": "statistikkere/navnestandard-datalagring.html#filnavn-for-datasett",
    "title": "Navnestandard datalagring",
    "section": "Filnavn for datasett",
    "text": "Filnavn for datasett\nFilnavnet til datasettet skal bygges opp av følgende elementer:\n\n\n\n\nElement\nForklaring\n\n\n\n\n1\nKort beskrivelse\nKort tekst som forklarer datasettets innhold, f.eks. “varehandel, “personinntekt”, “grensehandel_imputert“ eller “framskrevne-befolkningsendringer“\n\n\n2\nPeriode - inneholder data f.o.m. dato\nDatasettet inneholder data fra og med dato/tidspunkt. I filnavnet må perioden prefikses med “_p”, eksempel “_p2022” eller “_p2022-01-01”. “_p” er en forkortelse for “periode”. Se også gyldige formater for periode (dato/tidspunkt) \n\n\n3\nPeriode - inneholder data t.o.m. dato\nDatasettet inneholder data til og med dato/tidspunkt. Denne brukes ved behov, eksempelvis for datasett som inneholder forløpsdata eller datasett med flere perioder/årganger.\n\n\n4\nVersjon\nVersjon av datasettet. I filnavnet må versjonsnummeret prefikses med “_v, eksempel “v1”, “v2” eller “v3”. Se også eget kapittel om regler for versjonering av datasett.\n\n\n5\nFiltype\nFilendelse som sier noen om filtypen, f.eks. “.json”, “.csv”, “.xml” eller “.parquet”.\n\n\n\n\nFormat for filnavn (datasettnavn)\nFilnavnet skal bygges opp på følgende måte:\n&lt;kort-beskrivelse&gt;_p&lt;periode-fra-og-med&gt;_p&lt;perode-til-og- med&gt;_v&lt;versjon&gt;.&lt;filtype&gt;\n\nNoen eksempler på gyldige filnavn :\nflygende_objekter_p2019_v1.parquet (inneholder en årgang med data)\nufo_observasjoner_p2019_p2020_v1.parquet (inneholder 2 årganger med data)\nframskrevne-befolkningsendringer_p2019_p2050_v1.parquet (inneholder data fra 2019 til 2050)\nsykepenger_p2022-01-01_p2022-12-31_v1.parquet (inneholder data fra 01.01.2022 til 31.12.2022)\nutanningsnivaa_p2022-10-01_v1.parquet (inneholder tverrsnittsdata (status) per 01.10.2022)\ngrensehandel_imputert_p2022-10_p2022-12_v1.parquet (inneholder data for okt., nov. og des. 2022)\nomsetning_p2020W15_v1.parquet (inneholder data for uke-nummer 15 (week))\nskipsanloep_p2022B1_v1.parquet (inneholder data for første 2 måneders-periode i 2022 (bimester))\npensjon_p2018Q1_v1.parquet (inneholder data for første kvartal (3-måneders-periode) i 2018 (quarter))\nnybilreg_p2022T1_v1.parquet (inneholder data for første tertial (4 måneders-periode) i 2022)\npersoninntekt_p2022H1_v1.parquet (inneholder data for første halvår (6-måneders-periode) i 2022)\nvarehandel_p2018Q1_p2018Q4_v1.parquet (inneholder data for kvartalene 1, 2,3 og 4 i 2018)\n\n\nEksempel på datasett-filer i en mappestruktur\nNedenfor vises et eksempel på hvordan “ Team overnaturlig ” har organisert sine datasett-filer i en mappe-struktur for sin “ ufo-statistikk ”:\nssb-prod-team-overnaturlig-data-produkt/  \n└── ufo/  \n    └── inndata/  \n         ├── lysfenomen_p2019_v1.parquet  \n         ├── lysfenomen_p2020_v1.parquet  \n         ├── flygende_objekter_p2019_v1.parquet  \n         └── flygende_objekter_p2020_v1.parquet  \n    └── klargjorte-data/  \n         ├── ufo_observasjoner_samlet_p2019_v1.parquet  \n         └── ufo_observasjoner_samlet_p2020_v1.parquet  \n    └── statistikk/  \n         └── ufo_statistikk_p2019_p2020_v1.parquet  \n    └── utdata/  \n         ├── ufo_statistikk_fylke_p2019_p2020_v1.csv  \n         └── ufo_statistikk_landet_p2019_p2020_v1.csv  \n└── … osv.\n\n\n\nTillatte tegn for bruk i filnavn og mappe-navn\nDet er kun tillatt å bruke alfanumerisk tegn begrenset til:\n\na-z og A-Z\n0-9\n- (bindestrek)\n_ (understrek)\n\n\n\n\n\n\n\nAndre krav til filnavn og mappe-navn\n\n\n\nIkke bruk bokstavene “æ”, “ø” og “å” i filnavn eller i mappe-navn.\nAnbefalingen er at disse erstattes med “ae”, “oe” og “aa”.\n\nEksempel: “naering”, “oekonomi” og “levekaar”\n\nMellomrom/ordskiller (space) erstattes med bindestrek (“-”) eller understrek (“_”).\n\nEksempel: “skatt_for_personer” og “vann-og-avloep”\n\nPunktum (“.”) er kun tillatt brukt for å skille filnavnet fra filendelsen (filtypen).\n\nEksempel: “persondata.parquet” og “persondata.csv”\n\nIngen andre spesialtegn er tillatt brukt i filnavn eller mappe-navn.\n\n\n\n\n“Datapartisjonering” - alternativ organisering av datasett med flere mapper og filer\nDatatjenester/program-bibliotek som PySpark, PyArrow, Pandas og Dask har funksjonalitet for datapartisjonering. Dette er en teknikk som benyttes for å splitte opp veldig store datasett til flere små datasett og deretter plassere disse filene i en mappe-struktur. En av fordelene med dette er at konsumenter (brukere) kan jobbe med mindre deler av store datasett. En vanlig praksis er å dele opp (partisjonere) et stort datasett med mange årganger/perioder i flere små årgangsfiler. Da vil navnet på root-mappen tilsvare navnet på filen (hvis vi kun hadde én stor fil) , og underkatalogene vil være periodeinndeling, f.eks. “/aargang2019” og “/aargang2020”. Det er også mulig å “partisjonere” data på andre måter for å støtte parallell-prosessering av store datasett i f.eks. Spark.\n\nEksempel på partisjonering av stort datasett\n“Team overnaturlig” har et stort datasett med mange årganger med observasjoner av “flygende objekter”.\nssb-prod-team-overnaturlig-data-produkt/  \n└── ufo/  \n    └── inndata/  \n        └── flygende_objekter_p1980_p2020_v1.parquet\nTeamet ønsker nå å “partisjonere” denne store datafilen i flere små årgangsfiler (med filtype parquet).\nDet finnes flere bibliotek og verktøy som støtter partisjonering av store datafiler, eksempelvis Pandas hvor det er mulig å partisjonere en dataframe på følgende måte når den skrives til en parquet-fil(er):\ndf.to_parquet('./flygende_objekter_p1980_p2020_v1', partition_cols=['aar'])\nSe mer informasjon om datapartisjonering med Pandas: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html?highlight=example\nNedenfor vises et eksempel på hvordan en mappestruktur med datasett-filer kan se ut etter utført data-partisjonering.\nssb-prod-team-overnaturlig-data-produkt/  \n└── ufo/  \n    └── inndata/  \n        └── flygende_objekter_p1980_p2020_v1/  \n            └── aar=1980/  \n                └── data.parquet  \n            └── aar=1981/  \n                └── data.parquet  \n            └── aar=1982/  \n                └── data.parquet  \n            … osv.  \n            └── aar=2020/  \n                └── data.parquet"
  },
  {
    "objectID": "statistikkere/altinn3.html",
    "href": "statistikkere/altinn3.html",
    "title": "Altinn 3",
    "section": "",
    "text": "Frem mot sommeren 2025 skal alle skjema-undersøkelser i SSB som gjennomføres på Altinn 2 flyttes over til Altinn 3. Skjemaer som flyttes til Altinn 3 vil motta sine data på Dapla, og ikke på bakken som tidligere. Datafangsten håndteres av Team SUV, mens statistikkseksjonene henter sine data fra Team SUV sitt lagringsområde på Dapla. I dette kapitlet beskriver vi nærmere hvordan statistikkseksjonene kan jobbe med Altinn3-data på Dapla. Kort oppsummert består det av disse stegene:\n\nStatistikkprodusenten avtaler overføring av skjema fra Altinn 2 til Altinn 3 med planleggere på S821, som koordinerer denne jobben.\nNår statistikkprodusentene får beskjed om at Altinn3-skjemaet skal sendes ut til oppgavegiverne, så må de opprette et Dapla-team.\nNår Dapla-teamet er opprettet, og første skjema er sendt inn, så ber de Team SUV om å gi statistikkteamet tilgang til dataene som har kommet inn fra Altinn 3. I tillegg ber de om at Team SUV gir tilgang til teamets Transfer Service instans. 1 Merk at det må gis separate tilganger for data i staging- og produksjonsmiljø.\nStatistikkprodusenten setter opp en automatisk overføring av skjemadata med Transfer Service, fra Team SUV sitt lagringsområde over til Dapla-teamet sin kildebøtte.\nStatistikkprodusentene kan begynne å jobbe med dataene i Dapla. Blant annet tilbyr Dapla en automatiseringstjeneste man kan bruke for å prosessere dataene fra kildedata til inndata2.\n\nUnder forklarer vi mer med mer detaljer hvordan man går frem for gjennomføre steg 4-5 over.\n\n\n\n\n\n\nAnsvar for kildedata\n\n\n\nSelv om Team SUV tar ansvaret for datafangst fra Altinn3, så er det statistikkteamet som har ansvaret for langtidslagring av dataene i sin kildebøtte. Det vil si at at statistikkteamet må sørge for at data overføres til sin kildebøtte, og at de kan ikke regne med at Team SUV tar vare på en backup av dataene.\n\n\n\n\nNår skjema-dataene kommer inn fra Altinn 3 til Team SUV sitt lagringsområde, så er det en del ting som er verdt å tenke på:\n\nHvordan er filene organisert hos Team SUV?\nFilstrukturen der Team SUV lagrer dataene som kommer inn fra Altinn 3, har en klar struktur. Du kan selv gå inn å kikke på dataene, enten fra Jupyter eller Google Cloud Console, og bli bedre kjent med strukturen3. Figur 1 viser en hvordan en typisk filsti ser ut på lagringsområdet til Team SUV. Det starter med navnet til bøtta som Team SUV har opprettet for skjemaet. Deretter viser den innvitteringsdato, deretter et teknisk navn, og til slutt selve skjemanavnet.\n\n\n\n\nFigur 1: Typisk filsti for et Altinn3-skjema.\n\n\n\nHvordan organisere dataene i din kildebøtte?\nNår vi bruker Transfer Service til å synkronisere innholdet i Team SUV sitt lagringsområde til Dapla-teamet sitt lagringsområde, så er det mest hensiktmessig å fortsette å bruke mapppe-strukturen som Team SUV har. Grunnen er at vi ikke kan legge på noe logikk som lager en ny struktur. Tjenesten bare kopierer over data. Men du kan kan lage et nytt toppnivå-mappe som du ønsker at dataene skal synkroniseres til. F.eks. at alle Altinn-dataene legger seg inn i en mappe som heter altinn. Det er spesielt nyttig hvis du har flere datakilder som skal ligge i samme kildebøtte.\nUnike skjemanavn\nSkjemanavnet du ser i Figur 1 er unike. Dvs. at hvis en oppgavegiver sender inn flere skjemaer på samme dag, så er fortsatt skjemanavnet unikt. Det er viktig å være klar over når man bruker automatiseringstjenesten for kildedata senere. Siden tjenesten trigges per fil, og man er avhengig av unike navn for å ikke skrive over filer, så er det nyttig å vite at man kan videreføre skjemanavnet i overgangen fra kildedata til inndata.\n\n\n\n\nNår vi skal overføre filer fra Team SUV sin bøtte til vår kildebøtte, så kan vi gjøre det manuelt fra Jupyter som forklart her.. Men det er en bedre løsning å bruke en tjeneste som gjør dette for deg. Transfer Service er en tjeneste som kan brukes til å synkronisere innholdet mellom bøtter på Dapla, samt mellom bakke og sky. Når du skal ta i bruk tjenesten for å overføre data mellom en bøtte fra Team SUV sitt prosjekt altinn-data-prod, til en kildedata-bøtte i Dapla-teamet ditt, så gjør du følgende:\n\nFølg denne beskrivelsen hvordan man setter opp overføringsjobber.\nEtter at du har trykket på Create Transfer Job velger du Google Cloud Storage på begge alternativene under Get Started. Deretter går du videre ved å klikke på Next Step.\nUnder Choose a source så skal du velge hvor du skal kopiere data fra. Trykk på Browse. I vinduet som dukker opp trykker du på 🔻-ikonet ved siden av Project ID. I søkevinduet som dukker opp søker du opp altinn-data-prod og trykker på navnet. Da får du listet opp alle bøttene i altinn-data-prod prosjektet. Til slutt trykker du på bøtta som Team SUV har opprettet for undersøkelsen4 og klikker Select til nederst på siden. Trykk deretter Next step for å gå videre.\nUnder Choose a destination så skal du velge hvor du skal kopiere data til. Dette steget er ganske likt som forrige, men du skal nå velge ditt eget projekt og kildebøtta der. Trykk på Browse. I vinduet som dukker opp trykker du på 🔻-ikonet ved siden av Project ID. I søkevinduet som dukker opp søker du opp prod-&lt;ditt teamnavn&gt; og trykker på navnet. Da får du listet opp alle bøttene i ditt team sitt prosjekt. Velg kildebøtta som har navnet ssb-prod-&lt;teamnavn&gt;-data-kilde. Hvis du ønsker å kopiere data til en undermappe i bøtta, så trykker du på &gt;-ikonet ved bøttenavnet og velger ønsket undermappe5. Til slutt trykker du på Select til nederst på siden. Trykk deretter Next step for å gå videre.\nUnder Choose how and when to run job velger du Run with custom frequency og Starting now i Batch-modus. Hvis du ønsker å overføre så ofte som mulig, så velger du Custom frequency og Repeat every 1 Hours. Til slutt trykker du Next Step nederst på siden.\nUnder Choose Settings så legger du til en kort beskrivelse av jobben du har opprettet. Under de andre valgene kan du gjøre følgende:\n\nUnder Advanced transfer Options trenger du ikke gjøre noen endringer.\nUnder When to overwrite kan du velge If different.\nUnder When to delete kan du velge Never.\nUnder Manifest trenger du ikke huke av.\nUnder Logging options velger du samme som vist i Figur 2.\n\n\n\n\n\nFigur 2: Valg av opsjoner for logging i Transfer Service\n\n\nTil slutt trykker du Create for å aktivere tjenesten. Den vil da sjekke Team SUV sin bøtte hver time og kopiere over alle filer som ikke allerede eksisterer statistikkteamets kildebøtte.\n\n\n\nNår du har satt opp Transfer Service til å kopiere over filer fra Team SUV sin bøtte til statistikkteamets kildebøtte, så vil det potensielt komme inn nye skjemaer hver time. Disse kan du lese inn i Jupyter og jobbe med hvis du er kildedata-ansvarlig i teamet. Hvis du ikke er det så må du vente til dataene er tilgjengeliggjort i produkt-bøtta til teamet.\nSiden få personer innehar rollen som kildedata-ansvarlig så er det laget en automatiseringstjeneste som kan bearbeide alle filer som kommer inn i kildebøtta. Den lar deg kjøre et python-script på alle filer som kommer inn i kildebøtta.\nLes mer om hvordan du kan bruker tjenesten her.\n\n\n\nI denne delen deles noen tips og triks for å jobbe med Altinn3-dataene på Dapla. Fokuset vil være på hvordan du kan lese inn og transformere xml-filer fra skjema-dataene.\n\n\nFor å se innholdet i en mappe gir det mest mening å bruke Google Cloud Console. Her kan du se både filer og mapper i et pek-og-klikk grensesnitt. Men du kan ikke se på innholdet i filene der. Til det må du bruke Jupyter.\nAnta at vi ønsker å liste ut alle som leverte skjema den 10. mars 2023. Da kan vi bruke gcsfs til å gjøre det6:\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/**.xml\")\n\n# Print the list of parquet files\nxml_files\nHer bruker vi fs.glob()-funksjonen fra gcsfs til å loope gjennom alle undermapper av gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/, og legge alle filer som slutter med .xml til listen xml_files. Dermed har vi et Python-objekt som kun brukes til å hente inn de filene vi ønsker. Vi bruker da det vi vet om mappestrukturen i Team SUV sin bøtte som vi så tidligere i Figur 1.\n\n\n\nNoen ganger kan det være nyttig å se en xml-fil direkte. Da kan vi lese den inn i Jupyter og printe den ut. Her er et eksempel på hvordan vi kan gjøre det, samtidig som vi formatterer den med xml.dom.minidom:\nfrom xml.dom.minidom import parseString\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\n# Sett inn filstien din her\nfile = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/form_dc551844cd74.xml\"\n\ndom = parseString(fs.cat_file(file))\npretty_xml = dom.toprettyxml(indent=\"  \")\nprint(pretty_xml)\nVi brukte da fs.cat_file() til å hente inn innholdet i mappen, gjorde det om til xml-object, og formatterer outputen med xml.dom.minidom. Resultatet vil da se ut noe sånt som dette:\n&lt;?xml version=\"1.0\" ?&gt;\n&lt;melding xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" dataFormatProvider=\"SERES\" dataFormatId=\"7079\" dataFormatVersion=\"47315\"&gt;\n  &lt;InternInfo&gt;\n    &lt;raNummer&gt;RA-XXXX&lt;/raNummer&gt;\n    &lt;skjemaVersjon&gt;1.0&lt;/skjemaVersjon&gt;\n    &lt;undersoekelsesNr&gt;XXXXX&lt;/undersoekelsesNr&gt;\n    &lt;visOppgaveByrde&gt;0&lt;/visOppgaveByrde&gt;\n    &lt;visBrukeropplevelse&gt;0&lt;/visBrukeropplevelse&gt;\n    &lt;delregNr&gt;2XXXXX&lt;/delregNr&gt;\n    &lt;periodeFritekst&gt;3. kvartal 2022&lt;/periodeFritekst&gt;\n    &lt;periodeFomDato&gt;2022-07-01&lt;/periodeFomDato&gt;\n    &lt;periodeTomDato&gt;2022-09-30&lt;/periodeTomDato&gt;\n    &lt;periodeType&gt;KVRT&lt;/periodeType&gt;\n    &lt;periodeNummer&gt;3&lt;/periodeNummer&gt;\n    &lt;periodeAAr&gt;2022&lt;/periodeAAr&gt;\n    &lt;periodeDato&gt; &lt;/periodeDato&gt;\n    &lt;enhetsIdent&gt;XXXXXX&lt;/enhetsIdent&gt;\n    &lt;enhetsType&gt;BEDR&lt;/enhetsType&gt;\n    &lt;enhetsOrgNr&gt;XXXXXXXXX&lt;/enhetsOrgNr&gt;\n    &lt;enhetsNavn&gt;STATISTISK SENTRALBYRÅ &lt;/enhetsNavn&gt;\n    &lt;enhetsGateadresse&gt;Akersveien 26&lt;/enhetsGateadresse&gt;\n    &lt;enhetsPostnr&gt;0177&lt;/enhetsPostnr&gt;\n    &lt;enhetsPoststed&gt;OSLO&lt;/enhetsPoststed&gt;\n    &lt;enhetsAvdeling&gt;AVD XXXX&lt;/enhetsAvdeling&gt;\n    &lt;reporteeOrgNr&gt;XXXXXXXXX&lt;/reporteeOrgNr&gt;\n  &lt;/InternInfo&gt;\n  &lt;Kontakt&gt;\n    &lt;kontaktPersonNavn&gt;OLA NORDMANN&lt;/kontaktPersonNavn&gt;\n    &lt;kontaktPersonEpost&gt;ola@tull.no&lt;/kontaktPersonEpost&gt;\n    &lt;kontaktPersonTelefon&gt; &lt;/kontaktPersonTelefon&gt;\n    &lt;kontaktInfoBekreftet&gt;1&lt;/kontaktInfoBekreftet&gt;\n    &lt;kontaktInfoKommentar&gt;Dette er en et eksempel for Dapla-manualen&lt;/kontaktInfoKommentar&gt;\n  &lt;/Kontakt&gt;\n  &lt;Skjemadata&gt;\n    &lt;ledigeStillinger&gt;75&lt;/ledigeStillinger&gt;\n    &lt;datoPrefill&gt; &lt;/datoPrefill&gt;\n  &lt;/Skjemadata&gt;\n&lt;/melding&gt;\n\n\n\nDe færreste ønsker å jobbe direkte med XML-filer. Derfor er det nyttig å kunne transformere XML-filene til et mer brukervennlig format, f.eks. en Pandas Dataframe. Her er et eksempel på hvordan vi kan gjøre det med Pandas:\n\nimport xml.etree.ElementTree as ET\nimport pandas as pd\nfrom dapla import FileClient\n\ndef single_xml_to_dataframe(file_path: str) -&gt; pd.DataFrame:\n    fs = FileClient.get_gcs_file_system()\n\n    with fs.open(file_path, mode=\"r\") as f:\n        single_xml = f.read()\n\n    root = ET.fromstring(single_xml)\n    intern_info = root.find(\"InternInfo\")\n    kontakt = root.find(\"Kontakt\")\n    skjemadata = root.find(\"Skjemadata\")\n\n    data = []\n    all_tags = set()\n\n    for element in intern_info:\n        all_tags.add(element.tag)\n\n    for element in kontakt:\n        all_tags.add(element.tag)\n\n    for element in skjemadata:\n        all_tags.add(element.tag)\n\n    for tag in all_tags:\n        element = intern_info.find(tag)\n        if element is None:\n            element = kontakt.find(tag)\n        if element is None:\n            element = skjemadata.find(tag)\n        if element is not None:\n            value = element.text\n            data.append(value)\n        else:\n            data.append(None)\n\n    # Include the full path to the XML file in the data list\n    data.append(file_path)\n\n    # Create the DataFrame inside the function\n    df = pd.DataFrame([data], columns=list(all_tags) + [\"pathToXmlFile\"])\n    path_to_xml_file_split = df[\"pathToXmlFile\"].str.split(\"/\")\n    return df\n\n# Run the function\nfile_path = \"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/24/28c34dce4232_e7bed8a4-def5-42dc-b1e2-b9fc66beb769/form_28c34dce4232.xml\"\ndf = single_xml_to_dataframe(file_path=file_path)\nI funksjonen så søker vi etter alle elementer under taggene intern_info, kontakt og skjemadata. Dvs. at vi lager en dataframe med en rad, der vi tar med all data fra xml-filen. Til slutt legger vi til en kolonne med filstien til filen. Det kan være nyttig senere hvis man gå tilbake til xml-filen for å sjekke om konverteringen til en dataframe er riktig.\nFunksjonen single_xml_to_dataframe() kan brukes til å loope over en liste med filstier (vi viste hvordan man lager en slik liste tidligere i kapitlet). Men et typisk problem som vil oppstå da er at xml-filene har forskjellig antall elementer i seg, som gir dataframes med ulikt antall kolonner, og derfor vil ikke Pandas vite hvordan man konkatinerer disse. For å fikse dette må du modifisere funksjonen til å ta høyde for dette.\n\n\n\nHvis vi ønsker å kopiere filer manuelt (dvs. ikke med Transfer Service) fra Team SUV sine bøtter til egen kildebøtte, kan vi gjøre det fra Jupyter. Vi har tidligere sett hvordan vi kan kopiere enkeltfiler mellom to bøtter. Under viser vi hvordan man kan kopiere alle filer under en viss filsti med kommandoen fs.copy():\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfrom_path = \"gs://ssb-prod-arbmark-skjema-data-kilde/ledstill/altinn/2022/11/21/\"\nto_path = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/\"\nfs.copy(from_path, to_path, recursive=True)\nI koden over så kopierer vi alle filer under from_path til to_path. Vi bruker recursive=True for å sørge for at vi kopierer alle filer under from_path.\nI eksempelet over så kopierer vi over alt som er from_path og dets undermapper. I tilfellet med Altinn-data så ligger det også pdf-filer av skjemaet som kanskje ikke ønsker å kopiere. I de tilfellene kan vi først søke etter de filene vi ønsker å kopiere og legge de i en liste. Deretter kan vi kopiere over en og en fil fra listen. Denne tilnærmingen er veldig nyttig hvis vi ønsker å filtrere ut filer som ikke er XML-filer, eller vi ønsker en annen mappestruktur en den som ligger i from_path. Her er en måte vi kan gjøre det på:\nfrom dapla import FileClient\n\n# Lager en filsystem-instans av GCS\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/**.xml\")\n\n# Stien du ønsker å kopiere til.\n# Koden under foutsetter at du har med gs:// først\nto_folder = \"gs://ssb-dapla-felles-data-delt-prod/\"\n\n# Kopierer over filene\nfor file in xml_files:\n    from_bucket = file.split(\"/\")[0]\n    to_bucket = to_folder.split(\"/\")[2]\n    to_path = file.replace(\n        from_bucket, to_bucket\n    )\n    fs.cp(file, to_path)\nI koden over så bruker vi fs.glob() og ** til å søke rekursivt etter alle xml-filer under filstien gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10. Deretter kopierer vi over filene til egen kildebøtte med fs.cp(). Når vi skal kopiere over til en ny bøtte må vi bestemme oss for hvor filene skal ligge og hvor mye av den gamle filstien vi skal beholde. Anta at vi skal beholde hele mappestrukturen i Team SUV sin bøtte. Da kan vi egentlig bare erstatte ra0678-01-altinn-data-prod-e17d-ssb-altinn/ med den nye bøtte-navnet, og vi vil få den samme strukturen som i Team SUV sin bøtte."
  },
  {
    "objectID": "statistikkere/altinn3.html#forberedelse",
    "href": "statistikkere/altinn3.html#forberedelse",
    "title": "Altinn 3",
    "section": "",
    "text": "Når skjema-dataene kommer inn fra Altinn 3 til Team SUV sitt lagringsområde, så er det en del ting som er verdt å tenke på:\n\nHvordan er filene organisert hos Team SUV?\nFilstrukturen der Team SUV lagrer dataene som kommer inn fra Altinn 3, har en klar struktur. Du kan selv gå inn å kikke på dataene, enten fra Jupyter eller Google Cloud Console, og bli bedre kjent med strukturen3. Figur 1 viser en hvordan en typisk filsti ser ut på lagringsområdet til Team SUV. Det starter med navnet til bøtta som Team SUV har opprettet for skjemaet. Deretter viser den innvitteringsdato, deretter et teknisk navn, og til slutt selve skjemanavnet.\n\n\n\n\nFigur 1: Typisk filsti for et Altinn3-skjema.\n\n\n\nHvordan organisere dataene i din kildebøtte?\nNår vi bruker Transfer Service til å synkronisere innholdet i Team SUV sitt lagringsområde til Dapla-teamet sitt lagringsområde, så er det mest hensiktmessig å fortsette å bruke mapppe-strukturen som Team SUV har. Grunnen er at vi ikke kan legge på noe logikk som lager en ny struktur. Tjenesten bare kopierer over data. Men du kan kan lage et nytt toppnivå-mappe som du ønsker at dataene skal synkroniseres til. F.eks. at alle Altinn-dataene legger seg inn i en mappe som heter altinn. Det er spesielt nyttig hvis du har flere datakilder som skal ligge i samme kildebøtte.\nUnike skjemanavn\nSkjemanavnet du ser i Figur 1 er unike. Dvs. at hvis en oppgavegiver sender inn flere skjemaer på samme dag, så er fortsatt skjemanavnet unikt. Det er viktig å være klar over når man bruker automatiseringstjenesten for kildedata senere. Siden tjenesten trigges per fil, og man er avhengig av unike navn for å ikke skrive over filer, så er det nyttig å vite at man kan videreføre skjemanavnet i overgangen fra kildedata til inndata."
  },
  {
    "objectID": "statistikkere/altinn3.html#transfer-service",
    "href": "statistikkere/altinn3.html#transfer-service",
    "title": "Altinn 3",
    "section": "",
    "text": "Når vi skal overføre filer fra Team SUV sin bøtte til vår kildebøtte, så kan vi gjøre det manuelt fra Jupyter som forklart her.. Men det er en bedre løsning å bruke en tjeneste som gjør dette for deg. Transfer Service er en tjeneste som kan brukes til å synkronisere innholdet mellom bøtter på Dapla, samt mellom bakke og sky. Når du skal ta i bruk tjenesten for å overføre data mellom en bøtte fra Team SUV sitt prosjekt altinn-data-prod, til en kildedata-bøtte i Dapla-teamet ditt, så gjør du følgende:\n\nFølg denne beskrivelsen hvordan man setter opp overføringsjobber.\nEtter at du har trykket på Create Transfer Job velger du Google Cloud Storage på begge alternativene under Get Started. Deretter går du videre ved å klikke på Next Step.\nUnder Choose a source så skal du velge hvor du skal kopiere data fra. Trykk på Browse. I vinduet som dukker opp trykker du på 🔻-ikonet ved siden av Project ID. I søkevinduet som dukker opp søker du opp altinn-data-prod og trykker på navnet. Da får du listet opp alle bøttene i altinn-data-prod prosjektet. Til slutt trykker du på bøtta som Team SUV har opprettet for undersøkelsen4 og klikker Select til nederst på siden. Trykk deretter Next step for å gå videre.\nUnder Choose a destination så skal du velge hvor du skal kopiere data til. Dette steget er ganske likt som forrige, men du skal nå velge ditt eget projekt og kildebøtta der. Trykk på Browse. I vinduet som dukker opp trykker du på 🔻-ikonet ved siden av Project ID. I søkevinduet som dukker opp søker du opp prod-&lt;ditt teamnavn&gt; og trykker på navnet. Da får du listet opp alle bøttene i ditt team sitt prosjekt. Velg kildebøtta som har navnet ssb-prod-&lt;teamnavn&gt;-data-kilde. Hvis du ønsker å kopiere data til en undermappe i bøtta, så trykker du på &gt;-ikonet ved bøttenavnet og velger ønsket undermappe5. Til slutt trykker du på Select til nederst på siden. Trykk deretter Next step for å gå videre.\nUnder Choose how and when to run job velger du Run with custom frequency og Starting now i Batch-modus. Hvis du ønsker å overføre så ofte som mulig, så velger du Custom frequency og Repeat every 1 Hours. Til slutt trykker du Next Step nederst på siden.\nUnder Choose Settings så legger du til en kort beskrivelse av jobben du har opprettet. Under de andre valgene kan du gjøre følgende:\n\nUnder Advanced transfer Options trenger du ikke gjøre noen endringer.\nUnder When to overwrite kan du velge If different.\nUnder When to delete kan du velge Never.\nUnder Manifest trenger du ikke huke av.\nUnder Logging options velger du samme som vist i Figur 2.\n\n\n\n\n\nFigur 2: Valg av opsjoner for logging i Transfer Service\n\n\nTil slutt trykker du Create for å aktivere tjenesten. Den vil da sjekke Team SUV sin bøtte hver time og kopiere over alle filer som ikke allerede eksisterer statistikkteamets kildebøtte."
  },
  {
    "objectID": "statistikkere/altinn3.html#automatiseringstjeneste-for-kildedata",
    "href": "statistikkere/altinn3.html#automatiseringstjeneste-for-kildedata",
    "title": "Altinn 3",
    "section": "",
    "text": "Når du har satt opp Transfer Service til å kopiere over filer fra Team SUV sin bøtte til statistikkteamets kildebøtte, så vil det potensielt komme inn nye skjemaer hver time. Disse kan du lese inn i Jupyter og jobbe med hvis du er kildedata-ansvarlig i teamet. Hvis du ikke er det så må du vente til dataene er tilgjengeliggjort i produkt-bøtta til teamet.\nSiden få personer innehar rollen som kildedata-ansvarlig så er det laget en automatiseringstjeneste som kan bearbeide alle filer som kommer inn i kildebøtta. Den lar deg kjøre et python-script på alle filer som kommer inn i kildebøtta.\nLes mer om hvordan du kan bruker tjenesten her."
  },
  {
    "objectID": "statistikkere/altinn3.html#tips-og-triks",
    "href": "statistikkere/altinn3.html#tips-og-triks",
    "title": "Altinn 3",
    "section": "",
    "text": "I denne delen deles noen tips og triks for å jobbe med Altinn3-dataene på Dapla. Fokuset vil være på hvordan du kan lese inn og transformere xml-filer fra skjema-dataene.\n\n\nFor å se innholdet i en mappe gir det mest mening å bruke Google Cloud Console. Her kan du se både filer og mapper i et pek-og-klikk grensesnitt. Men du kan ikke se på innholdet i filene der. Til det må du bruke Jupyter.\nAnta at vi ønsker å liste ut alle som leverte skjema den 10. mars 2023. Da kan vi bruke gcsfs til å gjøre det6:\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/**.xml\")\n\n# Print the list of parquet files\nxml_files\nHer bruker vi fs.glob()-funksjonen fra gcsfs til å loope gjennom alle undermapper av gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/, og legge alle filer som slutter med .xml til listen xml_files. Dermed har vi et Python-objekt som kun brukes til å hente inn de filene vi ønsker. Vi bruker da det vi vet om mappestrukturen i Team SUV sin bøtte som vi så tidligere i Figur 1.\n\n\n\nNoen ganger kan det være nyttig å se en xml-fil direkte. Da kan vi lese den inn i Jupyter og printe den ut. Her er et eksempel på hvordan vi kan gjøre det, samtidig som vi formatterer den med xml.dom.minidom:\nfrom xml.dom.minidom import parseString\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\n# Sett inn filstien din her\nfile = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/form_dc551844cd74.xml\"\n\ndom = parseString(fs.cat_file(file))\npretty_xml = dom.toprettyxml(indent=\"  \")\nprint(pretty_xml)\nVi brukte da fs.cat_file() til å hente inn innholdet i mappen, gjorde det om til xml-object, og formatterer outputen med xml.dom.minidom. Resultatet vil da se ut noe sånt som dette:\n&lt;?xml version=\"1.0\" ?&gt;\n&lt;melding xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" dataFormatProvider=\"SERES\" dataFormatId=\"7079\" dataFormatVersion=\"47315\"&gt;\n  &lt;InternInfo&gt;\n    &lt;raNummer&gt;RA-XXXX&lt;/raNummer&gt;\n    &lt;skjemaVersjon&gt;1.0&lt;/skjemaVersjon&gt;\n    &lt;undersoekelsesNr&gt;XXXXX&lt;/undersoekelsesNr&gt;\n    &lt;visOppgaveByrde&gt;0&lt;/visOppgaveByrde&gt;\n    &lt;visBrukeropplevelse&gt;0&lt;/visBrukeropplevelse&gt;\n    &lt;delregNr&gt;2XXXXX&lt;/delregNr&gt;\n    &lt;periodeFritekst&gt;3. kvartal 2022&lt;/periodeFritekst&gt;\n    &lt;periodeFomDato&gt;2022-07-01&lt;/periodeFomDato&gt;\n    &lt;periodeTomDato&gt;2022-09-30&lt;/periodeTomDato&gt;\n    &lt;periodeType&gt;KVRT&lt;/periodeType&gt;\n    &lt;periodeNummer&gt;3&lt;/periodeNummer&gt;\n    &lt;periodeAAr&gt;2022&lt;/periodeAAr&gt;\n    &lt;periodeDato&gt; &lt;/periodeDato&gt;\n    &lt;enhetsIdent&gt;XXXXXX&lt;/enhetsIdent&gt;\n    &lt;enhetsType&gt;BEDR&lt;/enhetsType&gt;\n    &lt;enhetsOrgNr&gt;XXXXXXXXX&lt;/enhetsOrgNr&gt;\n    &lt;enhetsNavn&gt;STATISTISK SENTRALBYRÅ &lt;/enhetsNavn&gt;\n    &lt;enhetsGateadresse&gt;Akersveien 26&lt;/enhetsGateadresse&gt;\n    &lt;enhetsPostnr&gt;0177&lt;/enhetsPostnr&gt;\n    &lt;enhetsPoststed&gt;OSLO&lt;/enhetsPoststed&gt;\n    &lt;enhetsAvdeling&gt;AVD XXXX&lt;/enhetsAvdeling&gt;\n    &lt;reporteeOrgNr&gt;XXXXXXXXX&lt;/reporteeOrgNr&gt;\n  &lt;/InternInfo&gt;\n  &lt;Kontakt&gt;\n    &lt;kontaktPersonNavn&gt;OLA NORDMANN&lt;/kontaktPersonNavn&gt;\n    &lt;kontaktPersonEpost&gt;ola@tull.no&lt;/kontaktPersonEpost&gt;\n    &lt;kontaktPersonTelefon&gt; &lt;/kontaktPersonTelefon&gt;\n    &lt;kontaktInfoBekreftet&gt;1&lt;/kontaktInfoBekreftet&gt;\n    &lt;kontaktInfoKommentar&gt;Dette er en et eksempel for Dapla-manualen&lt;/kontaktInfoKommentar&gt;\n  &lt;/Kontakt&gt;\n  &lt;Skjemadata&gt;\n    &lt;ledigeStillinger&gt;75&lt;/ledigeStillinger&gt;\n    &lt;datoPrefill&gt; &lt;/datoPrefill&gt;\n  &lt;/Skjemadata&gt;\n&lt;/melding&gt;\n\n\n\nDe færreste ønsker å jobbe direkte med XML-filer. Derfor er det nyttig å kunne transformere XML-filene til et mer brukervennlig format, f.eks. en Pandas Dataframe. Her er et eksempel på hvordan vi kan gjøre det med Pandas:\n\nimport xml.etree.ElementTree as ET\nimport pandas as pd\nfrom dapla import FileClient\n\ndef single_xml_to_dataframe(file_path: str) -&gt; pd.DataFrame:\n    fs = FileClient.get_gcs_file_system()\n\n    with fs.open(file_path, mode=\"r\") as f:\n        single_xml = f.read()\n\n    root = ET.fromstring(single_xml)\n    intern_info = root.find(\"InternInfo\")\n    kontakt = root.find(\"Kontakt\")\n    skjemadata = root.find(\"Skjemadata\")\n\n    data = []\n    all_tags = set()\n\n    for element in intern_info:\n        all_tags.add(element.tag)\n\n    for element in kontakt:\n        all_tags.add(element.tag)\n\n    for element in skjemadata:\n        all_tags.add(element.tag)\n\n    for tag in all_tags:\n        element = intern_info.find(tag)\n        if element is None:\n            element = kontakt.find(tag)\n        if element is None:\n            element = skjemadata.find(tag)\n        if element is not None:\n            value = element.text\n            data.append(value)\n        else:\n            data.append(None)\n\n    # Include the full path to the XML file in the data list\n    data.append(file_path)\n\n    # Create the DataFrame inside the function\n    df = pd.DataFrame([data], columns=list(all_tags) + [\"pathToXmlFile\"])\n    path_to_xml_file_split = df[\"pathToXmlFile\"].str.split(\"/\")\n    return df\n\n# Run the function\nfile_path = \"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/24/28c34dce4232_e7bed8a4-def5-42dc-b1e2-b9fc66beb769/form_28c34dce4232.xml\"\ndf = single_xml_to_dataframe(file_path=file_path)\nI funksjonen så søker vi etter alle elementer under taggene intern_info, kontakt og skjemadata. Dvs. at vi lager en dataframe med en rad, der vi tar med all data fra xml-filen. Til slutt legger vi til en kolonne med filstien til filen. Det kan være nyttig senere hvis man gå tilbake til xml-filen for å sjekke om konverteringen til en dataframe er riktig.\nFunksjonen single_xml_to_dataframe() kan brukes til å loope over en liste med filstier (vi viste hvordan man lager en slik liste tidligere i kapitlet). Men et typisk problem som vil oppstå da er at xml-filene har forskjellig antall elementer i seg, som gir dataframes med ulikt antall kolonner, og derfor vil ikke Pandas vite hvordan man konkatinerer disse. For å fikse dette må du modifisere funksjonen til å ta høyde for dette.\n\n\n\nHvis vi ønsker å kopiere filer manuelt (dvs. ikke med Transfer Service) fra Team SUV sine bøtter til egen kildebøtte, kan vi gjøre det fra Jupyter. Vi har tidligere sett hvordan vi kan kopiere enkeltfiler mellom to bøtter. Under viser vi hvordan man kan kopiere alle filer under en viss filsti med kommandoen fs.copy():\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfrom_path = \"gs://ssb-prod-arbmark-skjema-data-kilde/ledstill/altinn/2022/11/21/\"\nto_path = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/\"\nfs.copy(from_path, to_path, recursive=True)\nI koden over så kopierer vi alle filer under from_path til to_path. Vi bruker recursive=True for å sørge for at vi kopierer alle filer under from_path.\nI eksempelet over så kopierer vi over alt som er from_path og dets undermapper. I tilfellet med Altinn-data så ligger det også pdf-filer av skjemaet som kanskje ikke ønsker å kopiere. I de tilfellene kan vi først søke etter de filene vi ønsker å kopiere og legge de i en liste. Deretter kan vi kopiere over en og en fil fra listen. Denne tilnærmingen er veldig nyttig hvis vi ønsker å filtrere ut filer som ikke er XML-filer, eller vi ønsker en annen mappestruktur en den som ligger i from_path. Her er en måte vi kan gjøre det på:\nfrom dapla import FileClient\n\n# Lager en filsystem-instans av GCS\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/**.xml\")\n\n# Stien du ønsker å kopiere til.\n# Koden under foutsetter at du har med gs:// først\nto_folder = \"gs://ssb-dapla-felles-data-delt-prod/\"\n\n# Kopierer over filene\nfor file in xml_files:\n    from_bucket = file.split(\"/\")[0]\n    to_bucket = to_folder.split(\"/\")[2]\n    to_path = file.replace(\n        from_bucket, to_bucket\n    )\n    fs.cp(file, to_path)\nI koden over så bruker vi fs.glob() og ** til å søke rekursivt etter alle xml-filer under filstien gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10. Deretter kopierer vi over filene til egen kildebøtte med fs.cp(). Når vi skal kopiere over til en ny bøtte må vi bestemme oss for hvor filene skal ligge og hvor mye av den gamle filstien vi skal beholde. Anta at vi skal beholde hele mappestrukturen i Team SUV sin bøtte. Da kan vi egentlig bare erstatte ra0678-01-altinn-data-prod-e17d-ssb-altinn/ med den nye bøtte-navnet, og vi vil få den samme strukturen som i Team SUV sin bøtte."
  },
  {
    "objectID": "statistikkere/altinn3.html#footnotes",
    "href": "statistikkere/altinn3.html#footnotes",
    "title": "Altinn 3",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nForslag til e-post til Team SUV etter at teamet er opprettet:\nVi har opprettet et Dapla-tema som heter &lt;ditt teamnavn&gt; for å jobbe med skjema &lt;RA-XXXX&gt;. Kan dere gi oss tilgang til riktig lagringsområde og også gi vår Transfer Service lesetilgang.↩︎\nEn typisk prosessering som de fleste vil ønske å gjøre er å konvertere fra xml-formatet det kom på, og over til parquet-formatet.↩︎\nDu kan gå inn i Google Cloud Console og søke opp prosjektet til Team SUV som de bruker for å dele data. Det heter altinn-data-prod, og du finner bøttene ved å klikke deg inn på Cloud Storage↩︎\nBøttenavnet starter alltid med RA-nummeret til undersøkelsen.↩︎\nAlternativt oppretter du en mappe direkte vinduet ved å trykke på mappe-ikonet med en +-tegn i seg.↩︎\nFor å jobbe mot datat i GCS som i et “vanlig” filsysten kan vi bruke FileClient.get_gcs_file_system() fra dapla-toolbelt.↩︎"
  },
  {
    "objectID": "statistikkere/administrasjon-av-team.html",
    "href": "statistikkere/administrasjon-av-team.html",
    "title": "Administrasjon av team",
    "section": "",
    "text": "I dette kapitlet viser vi hvordan du kan opprette et nytt team eller gjøre endringer i et eksisterende team. Typiske endringer er å:\n\nLegge til eller fjerne medlemmer i et team\nListe ut medlemmer og tilgangsgrupper i et team\n\n\n\nFor å komme i gang med å opprette et Dapla-team trengs det en oversikt over teamets medlemmer og hvilke tilgangsgrupper medlemmene skal være med i. Det trengs også informasjon om hvilke Dapla-tjenester som er aktuelle for teamet å ta i bruk. Derfor har det blitt opprettet en egen veileder for dette kalt Dapla Start.\n\n\n\n\n\n\nGå til Dapla Start for starte bestilling av et nytt Dapla-team.\n\n\n\nNår teamet er opprettet får alle medlemmene tilgang til sitt eget prosjekt i Google Cloud Platform (GCP), som er SSBs leverandør av skytjenester. Videre får hvert prosjekt et sett med tjenester og tilganger som knyttes til teamet. Det opprettes også datalagringsområder (ofte kalt bøtter) som bare kan aksesseres av brukere som er med i teamets tilgangsgrupper.\nDapla-teamet vil også få sin egen gruppe i SSBs Active Directory slik at medlemskapet i gruppen kan administreres av Kundeservice.\n\n\n\nFor å legge til eller fjerne medlemmer i et team må du foreløpig opprette en Kundeservice-sak. Send en e-post til Kundeservice eller registrer en sak i portalen deres med følgende innhold:\n\nNavnet på Dapla-teamet du ønsker tilgang\nHvilken tilgang du søker om. Typisk er det enten data-admins, developers eller consumers). Les mer om hva de ulike tilgangene innebærer her.\nBeskrivelse av formålet med tilgangen.\n\nEndringer i team må godkjennes av søker seksjonsleder før de blir effektuert.\nFor fjerning av tilganger kan man sende en tilsvarende henvendelse, men man trenger ikke inkludere forklaring av formål.\n\n\n\n\n\n\nMidlertidig løsning\n\n\n\nAt endringer i team må gjøres via Kundeservice er midlertidig. Det jobbes med å lage et eget verktøy for dette.\n\n\n\n\n\nFor å se hvem som har de ulike tilgangsrollene i et team, så kan man bruke pakken dapla-team-cli fra Jupyter. Pakken er installert for alle og du kan liste ut medlemmer av team ved å gjøre følgende:\n\nLogg deg inn på Dapla.\nÅpne en ny terminal\nSkriv inn dpteam groups list-members og trykk Enter\nI prompten som dukker opp skriver du inn team-navn og trykker Enter.\n\nHvis du ikke har lagret Personal Access Token i Jupyter så blir du spurt om GitHub-bruker og passord etter punkt 4. Da oppgir du du bare din GitHub-bruker og token som er autentisert mot statisticsnorway.\nFor de som ikke har mulighet til å bruke Jupyter så kan man også sende inn en forespørsel til Kundeservice om å få en oversikt."
  },
  {
    "objectID": "statistikkere/administrasjon-av-team.html#opprette-dapla-team",
    "href": "statistikkere/administrasjon-av-team.html#opprette-dapla-team",
    "title": "Administrasjon av team",
    "section": "",
    "text": "For å komme i gang med å opprette et Dapla-team trengs det en oversikt over teamets medlemmer og hvilke tilgangsgrupper medlemmene skal være med i. Det trengs også informasjon om hvilke Dapla-tjenester som er aktuelle for teamet å ta i bruk. Derfor har det blitt opprettet en egen veileder for dette kalt Dapla Start.\n\n\n\n\n\n\nGå til Dapla Start for starte bestilling av et nytt Dapla-team.\n\n\n\nNår teamet er opprettet får alle medlemmene tilgang til sitt eget prosjekt i Google Cloud Platform (GCP), som er SSBs leverandør av skytjenester. Videre får hvert prosjekt et sett med tjenester og tilganger som knyttes til teamet. Det opprettes også datalagringsområder (ofte kalt bøtter) som bare kan aksesseres av brukere som er med i teamets tilgangsgrupper.\nDapla-teamet vil også få sin egen gruppe i SSBs Active Directory slik at medlemskapet i gruppen kan administreres av Kundeservice."
  },
  {
    "objectID": "statistikkere/administrasjon-av-team.html#legge-til-eller-fjerne-medlemmer-i-et-team",
    "href": "statistikkere/administrasjon-av-team.html#legge-til-eller-fjerne-medlemmer-i-et-team",
    "title": "Administrasjon av team",
    "section": "",
    "text": "For å legge til eller fjerne medlemmer i et team må du foreløpig opprette en Kundeservice-sak. Send en e-post til Kundeservice eller registrer en sak i portalen deres med følgende innhold:\n\nNavnet på Dapla-teamet du ønsker tilgang\nHvilken tilgang du søker om. Typisk er det enten data-admins, developers eller consumers). Les mer om hva de ulike tilgangene innebærer her.\nBeskrivelse av formålet med tilgangen.\n\nEndringer i team må godkjennes av søker seksjonsleder før de blir effektuert.\nFor fjerning av tilganger kan man sende en tilsvarende henvendelse, men man trenger ikke inkludere forklaring av formål.\n\n\n\n\n\n\nMidlertidig løsning\n\n\n\nAt endringer i team må gjøres via Kundeservice er midlertidig. Det jobbes med å lage et eget verktøy for dette."
  },
  {
    "objectID": "statistikkere/administrasjon-av-team.html#se-medlemmer-og-roller-i-et-team",
    "href": "statistikkere/administrasjon-av-team.html#se-medlemmer-og-roller-i-et-team",
    "title": "Administrasjon av team",
    "section": "",
    "text": "For å se hvem som har de ulike tilgangsrollene i et team, så kan man bruke pakken dapla-team-cli fra Jupyter. Pakken er installert for alle og du kan liste ut medlemmer av team ved å gjøre følgende:\n\nLogg deg inn på Dapla.\nÅpne en ny terminal\nSkriv inn dpteam groups list-members og trykk Enter\nI prompten som dukker opp skriver du inn team-navn og trykker Enter.\n\nHvis du ikke har lagret Personal Access Token i Jupyter så blir du spurt om GitHub-bruker og passord etter punkt 4. Da oppgir du du bare din GitHub-bruker og token som er autentisert mot statisticsnorway.\nFor de som ikke har mulighet til å bruke Jupyter så kan man også sende inn en forespørsel til Kundeservice om å få en oversikt."
  },
  {
    "objectID": "statistikkere/nytt-ssbproject.html",
    "href": "statistikkere/nytt-ssbproject.html",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "I dette kapittelet forklarer vi hvordan du oppretter et ssb-project og hva det innebærer. ssb-project er et CLI1 for å raskt komme i gang med koding på Dapla, hvor en del SSB-spesifikke beste-prakiser er ivaretatt. Kode som naturlig hører sammen, f.eks. koden til et produksjonsløp for en statistikk, er målgruppen for dette programmet. Kort fortalt kan du kjøre denne kommandoen i en terminal\n\n\nterminal\n\nssb-project create stat-testprod\n\nog du vil få en mappe som heter stat-testprod med følgende innhold:\n\nStandard mappestruktur En standard mappestruktur gjør det lettere å dele og samarbeide om kode, som igjen reduserer sårbarheten knyttet til at få personer kjenner koden.\nVirtuelt miljø Virtuelle miljøer isolerer og lagrer informasjon knyttet til kode. For eksempel hvilken versjon av Python du bruker og tilhørende pakkeversjoner. Det er viktig for at publiserte tall skal være reproduserbare. Verktøyet for å lage virtuelt miljø er Poetry.\nVersjonshåndtering med Git Initierer versjonshåndtering med Git og legger til SSBs anbefalte .gitignore og .gitattributes. Det sikrer at du ikke versjonhåndterer filer/informasjon som ikke skal versjonshåndteres.\n\nI tillegg lar ssb-project deg opprette et GitHub-repo hvis du ønsker. Les mer om hvordan du kan ta i bruk dette verktøyet under.\n\n\n\n\n\n\nNote\n\n\n\nDokumentasjonen for ssb-project finnes her: https://statisticsnorway.github.io/ssb-project-cli/. Det oppdateres hver gang en ny versjon av ssb-project slippes.\n\n\n\n\nFør du kan ta i bruk ssb-project så er det et par ting som må være på plass:\n\nDu må ha opprettet en git-bruker og git-epost lokalt der du skal kalle på programmet (les mer om hvordan her).\nHvis du ønsker at ssb-project også skal opprette et GitHub-repo for deg må du også følgende være på plass:\n\nDu må ha en GitHub-bruker (les hvordan her)\nSkru på 2-faktor autentisering for GitHub-brukeren din (les hvordan her)\nVære koblet mot SSBs organisasjon statisticsnorway på GitHub (les hvordan her)\nOpprette Personal Access Token (PAT) og godkjenne det for bruk mot statisticsnorway (les hvordan her)\n\n\nDet er også å anbefale at du lagrer PAT lokalt slik at du ikke trenger å forholde deg til det når jobber med Git og GitHub. Hvis du har alt dette på plass så kan du bare fortsette å følge de neste kapitlene.\n\n\n\n\n\n\n\n\n\nHar du Github bruker? Noe funksjonalitet i ssb-project krever det. Finn ut hvordan ved å lese forrige kapittel.\n\n\n\nssb-project lar deg opprette en prosjekt-mappe med og uten GitHub-repo. La oss ta for oss hver av alternativene.\n\n\nFor å opprette et nytt ssb-project uten GitHub-repo gjør du følgende:\n\nÅpne en terminal. De fleste vil gjøre dette i Jupyterlab på bakke eller sky og da kan de bare trykke på det blå ➕-tegnet i Jupyterlab og velge Terminal.\nFør vi kjører programmet må vi være obs på at ssb-project vil opprette en ny mappe der vi står. Gå derfor til den mappen du ønsker å ha den nye prosjektmappen. For å opprette et prosjekt som heter stat-testprod så skriver du følgende i terminalen:\n\n\n\nterminal\n\nssb-project create stat-testprod\n\n\n\nHvis du stod i hjemmemappen din på når du skrev inn kommandoen over i terminalen, så har du fått mappestrukturen som vises i Figur 1. 2. Den inneholder følgende :\n\n.git-mappe som blir opprettet for å versjonshåndtere med Git.\nsrc-mappe som skal inneholde all koden som utgjør produksjonsløpet.\ntests-mappe som inneholder tester du skriver for koden din.\nLICENCE-fil som skal benyttes for public-repos i SSB.\npoetry.lock-fil som inneholder alle versjoner av Python-pakker som blir brukt.\nREADME.md-fil som brukes for tekstlig innhold på GitHub-siden for prosjektet.\n\n\n\n\n\n\nFigur 1: Mappen som ble opprettet av ssb-project.\n\n\n\n\n\n\n\nOver så opprettet vi et ssb-project uten å opprette et GitHub-repo. Hvis du ønsker å opprette et GitHub-repo også må du endre kommandoen over til:\n\n\nterminal\n\nssb-project create stat-testprod --github --github-token='blablabla'\n\nKommandoen over oppretter en mappestruktur slik vi så tidligere, men også et ssb-project som heter stat-testprod med et GitHub-repo med samme navn. Som du ser så må vi da sende med opsjonen --github og PAT med opsjonen --github-token='blablabla'. Repoet i GitHub ser da ut som i Figur 2.\n\n\n\nFigur 2: GitHub-repo som er opprettet av ssb-project\n\n\n\n\n\n\n\n\nNår du har opprettet et nytt ssb-project, eller bygget et eksisterende prosjekt, så kan det ta rundt 30 sekunder før kernelen viser seg i Jupterlab-launcher. Vær tålmodig!\n\n\n\n\n\n\n\nNår du har opprettet et ssb-project så kan du installere de python-pakkene du trenger fra PyPI. Hvis du for eksempel ønsker å installere Pandas, et populært data wrangling bibliotek, så kan du gjøre følgende:\n\nÅpne en terminal i Jupyterlab.\nGå inn i prosjektmappen din ved å skrive\n\n\n\nterminal\n\ncd &lt;sti til prosjektmappe&gt;\n\n\nLag en branch/utviklingsg som f.eks. heter install-pandas:\n\n\n\nterminal\n\ngit checkout -b install-pandas\n\n\nInstaller Pandas ved å skrive følgende\n\n\n\nterminal\n\npoetry add pandas\n\n\n\n\nFigur 3: Installasjon av Pandas med ssb-project\n\n\nFigur 3 viser hvordan dette vil se ut i en Jupyterlab-terminal. Kommandoen for å installere noe er poetry add etterfulgt av pakkenavnet. Vi ser også at den automatisk legger til Pandas-versjonen i filen poetry.lock. Les mer om hvordan man installerer pakker her.\n\n\n\nNår du nå har installert en pakke så har filen poetry.lock endret seg. La oss for eksempelets skyld anta at du ønsker å bruke Git til å dokumentere denne hendelsen, og dele det med en kollega via GitHub. Hvis vi har opprettet et ssb-project med et GitHub-repo så kan vi gjøre akkurat dette:\n\nVi kan stage alle endringer med følgende kommando i terminalen når vi står i prosjektmappen:\n\n\n\nterminal\n\ngit add -A\n\n\nVidere kan commit en endring, dvs. ta et stillbilde av koden i dette øyeblikket, ved å skrive følgende:\n\n\n\nterminal\n\ngit commit -m \"Installert pandas\"\n\n\nPush det opp til GitHub3. Anta at vi gjorde dette i branchen install-pandas som ble opprettet tidligere. Da kan vi skrive følgende:\n\n\n\nterminal\n\ngit push --set-upstream origin install-pandas\n\nMer kommer her.\n\n\n\nNår vi skal samarbeide med andre om kode så gjør vi dette via GitHub. Når du pusher koden din til GitHub, så kan samarbeidspartnere pulle ned koden og jobbe videre med den. Men når de henter ned koden så vil de bare hente ned selve koden, ikke pakker og Python-versjonen som ble brukt. De må installere alt som du hadde installert. I tillegg trenger de en kernel hvis de skal jobbe i Jupyterlab. ssb-project gjør det svært enkelt å bygge opp det du trenger, siden det virtuelle miljøet har all informasjon om hva som trengs.\nFor at samarbeidspartneren din skal kunne bygge miljøet på nytt, må de ha gjort en minimal konfigurering av Git. Les mer om hvordan du frem for å gjøre dette her.\nFor å bygge opp et eksisterende miljø gjør du følgende:\n\nFørst må du kopiere prosjektet ned lokalt, eller klone repoet med git-terminologi\n\n\n\nterminal\n\ngit clone https://github.com/statisticsnorway/&lt;prosjektnavn&gt;\n\n\nGå inn i mappen du klonet\n\n\n\nterminal\n\ncd &lt;prosjektnavn&gt;\n\n\nSkape et virtuelt miljø og installere en tilsvarende Jupyter kernel med\n\n\n\nterminal\n\nssb-project build\n\n\n\n\nDet vil være tilfeller hvor man ønsker å slette et ssb-project, enten fordi man ikke trenger koden lenger eller fordi man bare testet litt.\n\n\nHvis man jobber med flere prosjekter så kan det fort bli mange Jupyter kerneler hengende igjen. Derfor er det også mulighet å kjøre\n\n\nterminal\n\nssb-project clean stat-testprod\n\nsom sletter Jupyter-kernelen og de installerte pakkene i prosjektet. Hvis du også ønsker å slette selve mappen med kode må du gjøre det manuelt4:\n\n\nterminal\n\nrm -rf ~/stat-testprod/\n\nProsjektmappen over lå direkte i hjemmemappen min og hjemmemappen på Linux kan alltid referes til med et tilda-tegn ~.\n\n\n\nGitHub-repoer som er opprettet under SSB sin organinasjons statisticsnorway på GitHub kan ikke slettes, bare arkiveres. Grunnen er at hvis man oppdager en sårbarhet senere så er det viktig å kunne se repoet for å forstå hva som har skjedd.\nHvis du ikke trenger et GitHub-repo lenger kan man arkivere repoet. Det gjør du på følgende måte:\n\nGi inn i repoet Settings slik som vist med rød pil i Figur 4.\n\n\n\n\nFigur 4: Settings for repoet.\n\n\n\nUnder General scroller du deg ned til delen som heter Danger Zone og velger Archive this repository, slik som vist på Figur 5.\n\n\n\n\nFigur 5: Arkivering av et repo.\n\n\n\nI dialogboksen som dukker opp fyller du inn reponavnet som beskrevet og trykker på I understand the consequences, archive this repository, som vist i Figur 6.\n\n\n\n\nFigur 6: Bekreftelse av arkiveringen.\n\n\nNår det er gjort så er repoet lesbart, men man kan ikke jobbe med det. Men som vi ser av @#fig-github-repo-settings-archive-warning kan man omgjøre arkiveringen senere hvis det skulle være ønskelig.\n\n\n\n\nVi har foreløpig ikke integret R i ssb-project. Grunnen er at det mest populære virtuelle miljø-verktøet for R, renv, kun tilbyr å passe på versjoner av R-pakker og ikke selve R-installasjonen. Det er en svakhet som trolig gjør det vanskeligere enn nødvendig å gjenskape tidligere publiserte resultater med ssb-project. I tillegg klarer den ikke å gjenkjenne pakker som blir brukt i ipynb-filer.\nPlanen er å finne et annet verktøy enn renv som kan også reprodusere R-versjonen. Team Statistikktjenester ser nærmere på hvilke alternativer som finnes og vil tilby noe i fremtiden.\nI mellomtiden kan man bruke renv slik det er beskrevet her for skymiljøet, og med denne modifiseringen for bakkemiljøet."
  },
  {
    "objectID": "statistikkere/nytt-ssbproject.html#forberedelser",
    "href": "statistikkere/nytt-ssbproject.html#forberedelser",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "Før du kan ta i bruk ssb-project så er det et par ting som må være på plass:\n\nDu må ha opprettet en git-bruker og git-epost lokalt der du skal kalle på programmet (les mer om hvordan her).\nHvis du ønsker at ssb-project også skal opprette et GitHub-repo for deg må du også følgende være på plass:\n\nDu må ha en GitHub-bruker (les hvordan her)\nSkru på 2-faktor autentisering for GitHub-brukeren din (les hvordan her)\nVære koblet mot SSBs organisasjon statisticsnorway på GitHub (les hvordan her)\nOpprette Personal Access Token (PAT) og godkjenne det for bruk mot statisticsnorway (les hvordan her)\n\n\nDet er også å anbefale at du lagrer PAT lokalt slik at du ikke trenger å forholde deg til det når jobber med Git og GitHub. Hvis du har alt dette på plass så kan du bare fortsette å følge de neste kapitlene."
  },
  {
    "objectID": "statistikkere/nytt-ssbproject.html#opprett-ssb-project",
    "href": "statistikkere/nytt-ssbproject.html#opprett-ssb-project",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "Har du Github bruker? Noe funksjonalitet i ssb-project krever det. Finn ut hvordan ved å lese forrige kapittel.\n\n\n\nssb-project lar deg opprette en prosjekt-mappe med og uten GitHub-repo. La oss ta for oss hver av alternativene.\n\n\nFor å opprette et nytt ssb-project uten GitHub-repo gjør du følgende:\n\nÅpne en terminal. De fleste vil gjøre dette i Jupyterlab på bakke eller sky og da kan de bare trykke på det blå ➕-tegnet i Jupyterlab og velge Terminal.\nFør vi kjører programmet må vi være obs på at ssb-project vil opprette en ny mappe der vi står. Gå derfor til den mappen du ønsker å ha den nye prosjektmappen. For å opprette et prosjekt som heter stat-testprod så skriver du følgende i terminalen:\n\n\n\nterminal\n\nssb-project create stat-testprod\n\n\n\nHvis du stod i hjemmemappen din på når du skrev inn kommandoen over i terminalen, så har du fått mappestrukturen som vises i Figur 1. 2. Den inneholder følgende :\n\n.git-mappe som blir opprettet for å versjonshåndtere med Git.\nsrc-mappe som skal inneholde all koden som utgjør produksjonsløpet.\ntests-mappe som inneholder tester du skriver for koden din.\nLICENCE-fil som skal benyttes for public-repos i SSB.\npoetry.lock-fil som inneholder alle versjoner av Python-pakker som blir brukt.\nREADME.md-fil som brukes for tekstlig innhold på GitHub-siden for prosjektet.\n\n\n\n\n\n\nFigur 1: Mappen som ble opprettet av ssb-project.\n\n\n\n\n\n\n\nOver så opprettet vi et ssb-project uten å opprette et GitHub-repo. Hvis du ønsker å opprette et GitHub-repo også må du endre kommandoen over til:\n\n\nterminal\n\nssb-project create stat-testprod --github --github-token='blablabla'\n\nKommandoen over oppretter en mappestruktur slik vi så tidligere, men også et ssb-project som heter stat-testprod med et GitHub-repo med samme navn. Som du ser så må vi da sende med opsjonen --github og PAT med opsjonen --github-token='blablabla'. Repoet i GitHub ser da ut som i Figur 2.\n\n\n\nFigur 2: GitHub-repo som er opprettet av ssb-project\n\n\n\n\n\n\n\n\nNår du har opprettet et nytt ssb-project, eller bygget et eksisterende prosjekt, så kan det ta rundt 30 sekunder før kernelen viser seg i Jupterlab-launcher. Vær tålmodig!"
  },
  {
    "objectID": "statistikkere/nytt-ssbproject.html#installere-pakker",
    "href": "statistikkere/nytt-ssbproject.html#installere-pakker",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "Når du har opprettet et ssb-project så kan du installere de python-pakkene du trenger fra PyPI. Hvis du for eksempel ønsker å installere Pandas, et populært data wrangling bibliotek, så kan du gjøre følgende:\n\nÅpne en terminal i Jupyterlab.\nGå inn i prosjektmappen din ved å skrive\n\n\n\nterminal\n\ncd &lt;sti til prosjektmappe&gt;\n\n\nLag en branch/utviklingsg som f.eks. heter install-pandas:\n\n\n\nterminal\n\ngit checkout -b install-pandas\n\n\nInstaller Pandas ved å skrive følgende\n\n\n\nterminal\n\npoetry add pandas\n\n\n\n\nFigur 3: Installasjon av Pandas med ssb-project\n\n\nFigur 3 viser hvordan dette vil se ut i en Jupyterlab-terminal. Kommandoen for å installere noe er poetry add etterfulgt av pakkenavnet. Vi ser også at den automatisk legger til Pandas-versjonen i filen poetry.lock. Les mer om hvordan man installerer pakker her."
  },
  {
    "objectID": "statistikkere/nytt-ssbproject.html#push-til-github",
    "href": "statistikkere/nytt-ssbproject.html#push-til-github",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "Når du nå har installert en pakke så har filen poetry.lock endret seg. La oss for eksempelets skyld anta at du ønsker å bruke Git til å dokumentere denne hendelsen, og dele det med en kollega via GitHub. Hvis vi har opprettet et ssb-project med et GitHub-repo så kan vi gjøre akkurat dette:\n\nVi kan stage alle endringer med følgende kommando i terminalen når vi står i prosjektmappen:\n\n\n\nterminal\n\ngit add -A\n\n\nVidere kan commit en endring, dvs. ta et stillbilde av koden i dette øyeblikket, ved å skrive følgende:\n\n\n\nterminal\n\ngit commit -m \"Installert pandas\"\n\n\nPush det opp til GitHub3. Anta at vi gjorde dette i branchen install-pandas som ble opprettet tidligere. Da kan vi skrive følgende:\n\n\n\nterminal\n\ngit push --set-upstream origin install-pandas\n\nMer kommer her."
  },
  {
    "objectID": "statistikkere/nytt-ssbproject.html#bygg-eksisterende-ssb-project",
    "href": "statistikkere/nytt-ssbproject.html#bygg-eksisterende-ssb-project",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "Når vi skal samarbeide med andre om kode så gjør vi dette via GitHub. Når du pusher koden din til GitHub, så kan samarbeidspartnere pulle ned koden og jobbe videre med den. Men når de henter ned koden så vil de bare hente ned selve koden, ikke pakker og Python-versjonen som ble brukt. De må installere alt som du hadde installert. I tillegg trenger de en kernel hvis de skal jobbe i Jupyterlab. ssb-project gjør det svært enkelt å bygge opp det du trenger, siden det virtuelle miljøet har all informasjon om hva som trengs.\nFor at samarbeidspartneren din skal kunne bygge miljøet på nytt, må de ha gjort en minimal konfigurering av Git. Les mer om hvordan du frem for å gjøre dette her.\nFor å bygge opp et eksisterende miljø gjør du følgende:\n\nFørst må du kopiere prosjektet ned lokalt, eller klone repoet med git-terminologi\n\n\n\nterminal\n\ngit clone https://github.com/statisticsnorway/&lt;prosjektnavn&gt;\n\n\nGå inn i mappen du klonet\n\n\n\nterminal\n\ncd &lt;prosjektnavn&gt;\n\n\nSkape et virtuelt miljø og installere en tilsvarende Jupyter kernel med\n\n\n\nterminal\n\nssb-project build"
  },
  {
    "objectID": "statistikkere/nytt-ssbproject.html#rydd-opp-etter-deg",
    "href": "statistikkere/nytt-ssbproject.html#rydd-opp-etter-deg",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "Det vil være tilfeller hvor man ønsker å slette et ssb-project, enten fordi man ikke trenger koden lenger eller fordi man bare testet litt.\n\n\nHvis man jobber med flere prosjekter så kan det fort bli mange Jupyter kerneler hengende igjen. Derfor er det også mulighet å kjøre\n\n\nterminal\n\nssb-project clean stat-testprod\n\nsom sletter Jupyter-kernelen og de installerte pakkene i prosjektet. Hvis du også ønsker å slette selve mappen med kode må du gjøre det manuelt4:\n\n\nterminal\n\nrm -rf ~/stat-testprod/\n\nProsjektmappen over lå direkte i hjemmemappen min og hjemmemappen på Linux kan alltid referes til med et tilda-tegn ~.\n\n\n\nGitHub-repoer som er opprettet under SSB sin organinasjons statisticsnorway på GitHub kan ikke slettes, bare arkiveres. Grunnen er at hvis man oppdager en sårbarhet senere så er det viktig å kunne se repoet for å forstå hva som har skjedd.\nHvis du ikke trenger et GitHub-repo lenger kan man arkivere repoet. Det gjør du på følgende måte:\n\nGi inn i repoet Settings slik som vist med rød pil i Figur 4.\n\n\n\n\nFigur 4: Settings for repoet.\n\n\n\nUnder General scroller du deg ned til delen som heter Danger Zone og velger Archive this repository, slik som vist på Figur 5.\n\n\n\n\nFigur 5: Arkivering av et repo.\n\n\n\nI dialogboksen som dukker opp fyller du inn reponavnet som beskrevet og trykker på I understand the consequences, archive this repository, som vist i Figur 6.\n\n\n\n\nFigur 6: Bekreftelse av arkiveringen.\n\n\nNår det er gjort så er repoet lesbart, men man kan ikke jobbe med det. Men som vi ser av @#fig-github-repo-settings-archive-warning kan man omgjøre arkiveringen senere hvis det skulle være ønskelig."
  },
  {
    "objectID": "statistikkere/nytt-ssbproject.html#hva-med-r",
    "href": "statistikkere/nytt-ssbproject.html#hva-med-r",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "Vi har foreløpig ikke integret R i ssb-project. Grunnen er at det mest populære virtuelle miljø-verktøet for R, renv, kun tilbyr å passe på versjoner av R-pakker og ikke selve R-installasjonen. Det er en svakhet som trolig gjør det vanskeligere enn nødvendig å gjenskape tidligere publiserte resultater med ssb-project. I tillegg klarer den ikke å gjenkjenne pakker som blir brukt i ipynb-filer.\nPlanen er å finne et annet verktøy enn renv som kan også reprodusere R-versjonen. Team Statistikktjenester ser nærmere på hvilke alternativer som finnes og vil tilby noe i fremtiden.\nI mellomtiden kan man bruke renv slik det er beskrevet her for skymiljøet, og med denne modifiseringen for bakkemiljøet."
  },
  {
    "objectID": "statistikkere/nytt-ssbproject.html#footnotes",
    "href": "statistikkere/nytt-ssbproject.html#footnotes",
    "title": "Nytt ssb-project",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nCLI = Command-Line-Interface. Dvs. et program som er skrevet for å brukes terminalen ved hjelp av enkle kommandoer.↩︎\nFiler og mapper som starter med punktum er skjulte med mindre man ber om å se dem. I Jupyterlab kan disse vises i filutforskeren ved å velge View fra menylinjen, og deretter velge Show hidden files. I en terminal skriver man ls -a for å se de.↩︎\nÅ pushe til GitHub uten å sende ved Personal Access Token fordrer at du har lagret det lokalt så Git kan finne det. Her et eksempel på hvordan det kan gjøres.↩︎\nDette kan også gjøres ved å høyreklikke på mappen i Jupyterlab sin filutforsker og velge Delete.↩︎"
  },
  {
    "objectID": "statistikkere/dapla-team.html",
    "href": "statistikkere/dapla-team.html",
    "title": "Dapla Team",
    "section": "",
    "text": "Dapla Team\nI forrige del beskrev vi noen grunnleggende ting rundt Dapla. I denne delen tar vi for oss hvordan du kan begynne å jobbe med skarpe data på plattformen.\nKapittelet som beskriver hvordan man logger seg inn på Dapla vil fungere uten at du må gjøre noen forberedelser. Er man koblet på SSB sitt nettverk så vil alle SSB-ansatte kunne gå inn på plattformen og kode i Python og R. Men du får ikke tilgang til SSBs område for datalagring på plattformen. I praksis vil det si at man kan generere data med kode, men man kan ikke jobbe med skarpe data.\nFor å få muligheten til å jobbe med skarpe data MÅ du først opprette et dapla-team. Dette er det første naturlige steget å ta når man skal begynne å jobbe med statistikkproduksjon på Dapla. I dette kapittelet vil vi forklare det du trenger å vite om det å opprette og jobbe innenfor et team."
  },
  {
    "objectID": "statistikkere/spark.html",
    "href": "statistikkere/spark.html",
    "title": "Apache Spark",
    "section": "",
    "text": "Koding i R og Python har historisk sett foregått på en enkelt maskin og vært begrenset av minnet (RAM) og prosessorkraften på maskinen. For bearbeiding av små og mellomstore datasett er det sjelden et problem på kjøre på en enkelt maskin. Populære pakker som dplyr for R, og pandas for Python, blir ofte brukt i denne typen databehandling. I senere år har det også kommet pakker som er optimalisert for å kjøre kode parallelt på flere kjerner på en enkelt maskin, skrevet i minne-effektive språk som Rust og C++.\nMen selv om man kommer langt med å kjøre kode på en enkelt maskin, vil enkelte oppgaver kreve mer minne og prosessorkraft enn dette. For større datasett, eller store beregninger, kan det være nyttig å bruke et rammeverk som kan kjøre kode parallelt på flere maskiner. Et slikt rammeverk er Apache Spark.\nApache Spark er et rammeverk for å kjøre kode parallelt på flere maskiner. Det er bygget for å håndtere store datasett og store beregninger. Det er derfor et nyttig verktøy for å løse problemer som er for store for å kjøre på en enkelt maskin. Men det finnes også andre bruksområder som er nyttige for Apache Spark. Her er noen eksempler:\nDette er noen av bruksområdene der Spark kan løse problemer som er for store for å kjøre på en enkelt maskin med for eksempel Pandas eller dplyr."
  },
  {
    "objectID": "statistikkere/spark.html#spark-på-dapla",
    "href": "statistikkere/spark.html#spark-på-dapla",
    "title": "Apache Spark",
    "section": "Spark på Dapla",
    "text": "Spark på Dapla\nDapla kjører på et Kubernetes-kluster og er derfor er et svært egnet sted for å kjøre kode parallelt på flere maskiner. Jupyter på Dapla har også en flere klargjorte kernels for å kjøre kode i Apache Spark. Denne koden vil kjøre på et eget kluster av maskiner som er dedikert til Apache Spark, slik som vist i Figur 1.\n\n\n\n\n\n\n\n(a) PySpark på kubernetes\n\n\n\n\n\n\n\n(b) PySpark på 1 maskin\n\n\n\n\n\n\n\n\n\n(c) SparkR på kubernetes\n\n\n\n\n\n\n\n(d) SparkR på 1 maskin\n\n\n\n\nFigur 1: Ferdigkonfigurerte kernels for Spark på Dapla.\n\n\nFigur 1 (a) og Figur 1 (c) kan velges hvis du ønsker å bruke Spark for å kjøre store jobber på flere maskiner, for henholdsvis Python- og R-grensesnittene for Spark.\nFigur 1 (b) og Figur 1 (d) bør du velge hvis du ønsker å bruke Spark av andre grunner enn å kjøre store jobber på flere maskiner. For eksempel hvis du ønsker å bruke en av de mange pakker som er bygget på Spark, eller hvis du ønsker å bruke Spark til å lese og skrive data fra Dapla.\nHvis du ønsker å sette opp et eget virtuelt miljø for å kjøre Spark, så kan du bruke ssb-project. Se ssb-project for mer informasjon."
  },
  {
    "objectID": "statistikkere/spark.html#spark-i-fa-brands-r-project-og-python-fa-brands-python",
    "href": "statistikkere/spark.html#spark-i-fa-brands-r-project-og-python-fa-brands-python",
    "title": "Apache Spark",
    "section": "Spark i  og Python",
    "text": "Spark i  og Python\nSpark er implementert i programmeringsspråket Scala. Men det tilbys også mange grensesnitt for å bruke Spark fra andre språk. De mest populære grensesnittene er PySpark for Python og SparkR for R. Disse grensesnittene er bygget på Spark, og gir tilgang til Spark-funksjonalitet fra Python og R.\n\nPySpark\nPySpark er et Python-grensesnitt for Apache Spark. Det er en Python-pakke som gir tilgang til Spark-funksjonalitet fra Python. Det er enkelt å bruke, og har mange av de samme funksjonene som Pandas.\nUnder ser du datasettet som benyttes for i vedlagt notebook pyspark-intro.ipynb. Den viser hvordan man kan gjøre vanlige databehandling med PySpark. I eksempelet brukes kernel som vist i Figur 1 (b).\n\n\n\nCode\n# Legger til row index til DataFrame før join med dates_df\ndata_df = data_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n\n# Joiner de to datasettene\ndf = (\n    dates_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n    .join(data_df, \"row_index\")\n    .drop(\"row_index\")\n)\n\n# Legger til år, kvartal og mnd\ndf = df.withColumn(\"Year\", date_format(df.Date, \"yyyy\"))\ndf = df.withColumn(\"Quarter\", expr(\"quarter(Date)\"))\ndf = df.withColumn(\"Month\", date_format(df.Date, \"MM\"))\n\ndf.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n\nSource: pyspark-intro.ipynb\nDet finnes også et Pandas API/grensesnitt mot Spark. Målet med en er å gjøre overgangen fra Pandas til Spark lettere for nybegynneren. Men hvis man skal gjøre litt mer avansert databehandling anbefales det at man bruker PySpark direkte og ikke Pandas API-et.\n\n\nSparkR\nSparkR er et R-grensesnitt for Apache Spark. Det er en R-pakke som gir tilgang til Spark-funksjonalitet fra R. Det er enkelt å bruke, og har mange av de samme funksjonene som dplyr. Se eksempel i notebook under:\n\n\nselectedColumns &lt;- select(file, \"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\")\nshowDF(selectedColumns, numRows = 5)\n\nSource: sparkr-intro.ipynb"
  },
  {
    "objectID": "statistikkere/spark.html#lakehouse-arkitektur",
    "href": "statistikkere/spark.html#lakehouse-arkitektur",
    "title": "Apache Spark",
    "section": "Lakehouse-arkitektur",
    "text": "Lakehouse-arkitektur\n\n\n\n\n\n\nWarning\n\n\n\nI denne delen viser vi hvordan funksjonalitet som kan bli relevant for SSB å benytte seg av i fremtiden. Men det er fortsatt under testing og ta det i betraktning før man eventuelt implementerer dette i produksjon.\n\n\nEn av utvidelsene som er laget rundt Apache Spark er den såkalte Lakehouse-arkitekturen. Kort fortalt kan den dekke behovene som et klassisk datavarehus har tatt seg av tidligere. I kontekst av SSB sine teknologier kan det også benyttes som et databaselag over Parquet-filer i bøtter. Det finnes flere open source løsninger for dette, men mest aktuelle er:\n\nDelta Lake\nApache Hudi\nApache Iceberg\n\nI det følgende omtaler vi hovedsakelig egenskapene til Delta Lake, men alle rammeverkene har mange av de samme egenskapene. Delta Lake kan også benyttes på Dapla nå.\nSentrale egenskaper ved Delta Lake er:\n\nACID-transactions som sikrer data-integritet og stabilitet, også når det skjer feil.\nMetadata som bli håndtert akkurat som all annen data og er veldig skalebar. Den støtter også egendefinert metadata.\nSchema Enforcement and Evolution sikrer at skjemaet til dataene blir håndhevet, og den tillater også den kan endres over tid.\nTime travel sikrer at alle versjoner av dataene er blitt lagret, og at du kan gå tilbake til tidligere versjoner av en fil.\nAudit history sikrer at du kan få full oversikt over hvilke operasjoner som utført på dataene.\nInserts, updates and deletes betyr at du lettere kan manipulere data enn hva som er tilfellet med vanlige Parquet-filer.\nIndexing er støttes for forbedre spørringer mot store datamengder.\n\nI vedlagt notebook deltalake-intro.ipynb finner du blant annet eksempler på hvordan du legger til følgende metadata i spesifikk versjon av en fil:\n\n\n\nCode\n# Lagrer egendefinert metadata i en json-fil\nimport json\n\nmetadata = {\n    \"comment\": \"Kontaktet oppgavegiver og kranglet!\",\n    \"manueltEditert\": \"True\",\n    \"maskineltEditert\": \"False\",\n}\nmetadata\n\n\nSource: deltalake-intro.ipynb"
  },
  {
    "objectID": "statistikkere/automatisering-avansert.html",
    "href": "statistikkere/automatisering-avansert.html",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "Figur 1: Infrastruktur\n\n\n\n\n\nSom utgangspunkt, får hver prosesseringsinstans standard ressurstildeling for Cloud Run, dvs. 1 CPU kjerne og 512MB minne. Dette kan være for lite i noen tilfeller, særlig for større kildedatafiler. Det er mulig å konfigurere dette vha. config.yaml.\n\n\nMan setter en verdi memory_size i config.yaml til en integer mellom 0 og 32 (GiB). Bak kulissene konfigurerer det også antall CPU kjerner i henhold til begrensningene som Google setter. Fasiten for de presise verdiene som blir satt finnes her.\n\n\n\nTatt fra rundstykker eksempelet, hvis man hadde behov for 10GiB minne, kunne man konfigurere det slikt:\nfolder_prefix: rundstykker\nmemory_size: 10  # GiB\n\n\n\nEndringer i config.yaml krever at man skriver atlantis apply i en PR for å effektuere endringene.\n\n\n\n\nEn liste over Python pakker man kan benytte seg av i process_source_data.py finnes her. Ta gjerne kontakt med Dapla Kundeservice hvis man har behov for ytterligere.\n\n\n\nHver kilde kan skalere opp med parallele prosesseringsinstanser. Det gjør det kjappere å prosessere mange filer. I utgangspunktet er dette begrenset til 5 parallele instanser, men det kan økes ved behov.\n\n\n\nÅ benytte både prod og staging miljøer er en god praksis for å sikre at nye funksjoner og endringer fungerer som forventet før de rulles ut i produksjon. Staging-miljøet gir mulighet til å validere endringer i en mer kontrollert og isolert setting før de blir lansert i produksjonsmiljøet med skarpe data.\nAutomatiseringsløsningen støtter oppsett av kilder i både staging og prod miljø. For oppsett av kilder i staging-miljøet, må man legge til kilden i en egen undermappe kalt staging. Dette kan gjøres på samme måte som beskrevet her, bortsett fra at kilden legges i en undermappe som heter staging.\nHer er et eksempel på konfigurasjon av både staging og produksjonsmiljø:\n...\n├── automation\n│   └── source_data\n│       ├── prod\n│       │   └── kilde1\n│       │       ├── config.yaml\n│       │       └── process_source_data.py\n│       └── staging\n│           └── kilde1\n│               ├── config.yaml\n│               └── process_source_data.py\n...\n\n\n\n\n\n\nInnholdet i config.yaml for både staging og produksjonsmiljø er som hovedregel identisk, men bøttene som benyttes vil være forskjellige. Stagingmiljøet har en egen kildedatabøtte kalt gs://ssb-staging-my-project-data-kilde.\n\n\n\n\n\n\nFor å gi raskt tilbakemelding på noen mulige feilsituasjoner, så kjøres det enkel validering på kilde config og process_source_data.py når en PR er opprettet.\nHvis valideringen feiler, så må feilen rettes før PRen merges.\nTestene feiler hvis:\n\nKildemappen ikke har et python script kalt process_source_data.py med metodesignaturen, som beskrevet her.\nKildemappen ikke har en yaml fil og en gyldig folder_prefix definert, som i dette eksempelet.\nPython scriptet ikke kan importeres av tjenesten. Tjenesten støtter kun disse tredjeparts pakkene.\nHvis Pyflakes finner feil med kildens Python script.\n\n\n\nHvis validerings testene feiler kan det være nyttig å se på loggene for å finne frem til feilen.\n\nFinn frem til testen som feiler, i bildet feiler valideringstestene for kilde1. Trykk så på lenken “Details” som vist i bilde under. \nPå siden du nå har kommet til skal det være en tabell som heter “Build Information”, trykk på lenken i Build kolonnen. \nDu har nå kommet frem til loggene, se etter indikasjoner på feil. I eksemplet under ser vi at testen test_main_accepts_expected_number_of_args feiler fordi process_source_data.py mangler en main funksjon. \nFiks feilen og push endingen til samme branch, testen vil da starte på nytt.\n\n\n\n\n\nEndringer til process_source_data.py blir automatisk rullet ut når en PR er merget til main branchen. Utrullingsprosessen tar noe tid, ca. 2-3 minutter fra branchen er merget til tjenesten er oppdatert, for å bekrefte at tjenesten er rullet ut kan du følge stegene i neste avsnitt.\n\n\nStegene under viser hvordan man går frem for å finne resultat av utrullingen av kilden “ledstill” for teamet “arbmark-skjema”. Og forutsetter at koden er pushet til main branchen.\n\nNaviger til GitHub.\nI søkefeltet oppe i venstre hjørne skriv arbmark-skjema og klikk “Jump to” arbmark-skjema-iac. Som i bilde under. \nNår utrullingen er ferdig vil en av disse ikonene vises, grønn hake betyr at tjeneste er rullet ut med koden som ligger i main og at nye filer blir behandlet med koden som ligger der. . Rødt kryss indikerer at utrullig har feilet.  Se etter symbolene der hvor den røde pilen i bilde under peker. I eksempel er utrulligen vellykket. \n\n\n\n\n\nMan får en oversikt over kildene man har konfigurert prosessering for og statusen på dem ved hjelp av konsollet på GCP. Der navigerer man til siden for Cloud Run (se Figur 2) som er kjøremiljøet som kildedata prosessering benyttes av. Eksempel URl er: https://console.cloud.google.com/run?project=&lt;teamets-prosjekt-id&gt;\nHer får man en oversikt av ressursbruk og loggene til prosesseringen.\n\n\n\nFigur 2: Cloud Run dashboard\n\n\n\n\nEtter du har valgt kilden kan du se logger ved å velge fanen “LOGS”. Her ligger alle logger for den spesifikke kilden. For å få bedre oversikt over eventulle feil kan man sette severity til error. Dette vil uten ekstra konfigurasjon gi oversikt over alle uhåndterte exceptions. \n\n\n\nHvis en fil blir mottatt av tjenesten, men ikke lar seg behandle blir det skrevet til loggen. Man kan få en oversik over hvilke filer som ikke har blitt prosessert ved å søke etter: Could not process object. \n\n\n\n\nNoen ganger vil det være nødvendig å trigge kjøring av en kilde uten at de tilhørende filene i kildebøtta er oppdatert f.eks. etter en endring i prosseseringsskriptet. For å gjøre dette kan man benytte seg av dapla toolbelt.\nFor å trigge en ny kjøring må man være data-admin i teamet og ha denne informasjonen tilgjengelig:\n\nproject_id(prosjekt id) for kilden, den finner man ved å følge beskrivelsen her.\nfolder_prefix beskriver stien til filene som skal behandles og fungerer på samme måte som i config.yaml\nsource_name finner man ved å se på navnet til mappen hvor kilden konfigureres, i eksempelet her ser vi at team smaabakst har to kilder boller og rundstykker.\n\n\n\nDette eksemplet viser hvordan man går frem for å manuelt trigge kilden boller for team smaabakst.\nTeam smaabakst ønsker å re-prosessere alle filer i kilden boller. Ved å bruke samme folder_prefix som i config.yaml vil alle filer som tilhører kilden bli prosessert på nytt.\nfrom dapla import trigger_source_data_processing\n\nproject_id = \"prod-smaabakst-b69d\"\nsource_name = \"boller\"\nfolder_prefix = \"boller\"\n\ntrigger_source_data_processing(project_id, source_name, folder_prefix)"
  },
  {
    "objectID": "statistikkere/automatisering-avansert.html#infrastruktur-oversikt",
    "href": "statistikkere/automatisering-avansert.html#infrastruktur-oversikt",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "Figur 1: Infrastruktur"
  },
  {
    "objectID": "statistikkere/automatisering-avansert.html#ressurser",
    "href": "statistikkere/automatisering-avansert.html#ressurser",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "Som utgangspunkt, får hver prosesseringsinstans standard ressurstildeling for Cloud Run, dvs. 1 CPU kjerne og 512MB minne. Dette kan være for lite i noen tilfeller, særlig for større kildedatafiler. Det er mulig å konfigurere dette vha. config.yaml.\n\n\nMan setter en verdi memory_size i config.yaml til en integer mellom 0 og 32 (GiB). Bak kulissene konfigurerer det også antall CPU kjerner i henhold til begrensningene som Google setter. Fasiten for de presise verdiene som blir satt finnes her.\n\n\n\nTatt fra rundstykker eksempelet, hvis man hadde behov for 10GiB minne, kunne man konfigurere det slikt:\nfolder_prefix: rundstykker\nmemory_size: 10  # GiB\n\n\n\nEndringer i config.yaml krever at man skriver atlantis apply i en PR for å effektuere endringene."
  },
  {
    "objectID": "statistikkere/automatisering-avansert.html#tilgjengelige-pakker",
    "href": "statistikkere/automatisering-avansert.html#tilgjengelige-pakker",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "En liste over Python pakker man kan benytte seg av i process_source_data.py finnes her. Ta gjerne kontakt med Dapla Kundeservice hvis man har behov for ytterligere."
  },
  {
    "objectID": "statistikkere/automatisering-avansert.html#skalering",
    "href": "statistikkere/automatisering-avansert.html#skalering",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "Hver kilde kan skalere opp med parallele prosesseringsinstanser. Det gjør det kjappere å prosessere mange filer. I utgangspunktet er dette begrenset til 5 parallele instanser, men det kan økes ved behov."
  },
  {
    "objectID": "statistikkere/automatisering-avansert.html#testing-i-staging-miljø",
    "href": "statistikkere/automatisering-avansert.html#testing-i-staging-miljø",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "Å benytte både prod og staging miljøer er en god praksis for å sikre at nye funksjoner og endringer fungerer som forventet før de rulles ut i produksjon. Staging-miljøet gir mulighet til å validere endringer i en mer kontrollert og isolert setting før de blir lansert i produksjonsmiljøet med skarpe data.\nAutomatiseringsløsningen støtter oppsett av kilder i både staging og prod miljø. For oppsett av kilder i staging-miljøet, må man legge til kilden i en egen undermappe kalt staging. Dette kan gjøres på samme måte som beskrevet her, bortsett fra at kilden legges i en undermappe som heter staging.\nHer er et eksempel på konfigurasjon av både staging og produksjonsmiljø:\n...\n├── automation\n│   └── source_data\n│       ├── prod\n│       │   └── kilde1\n│       │       ├── config.yaml\n│       │       └── process_source_data.py\n│       └── staging\n│           └── kilde1\n│               ├── config.yaml\n│               └── process_source_data.py\n...\n\n\n\n\n\n\nInnholdet i config.yaml for både staging og produksjonsmiljø er som hovedregel identisk, men bøttene som benyttes vil være forskjellige. Stagingmiljøet har en egen kildedatabøtte kalt gs://ssb-staging-my-project-data-kilde."
  },
  {
    "objectID": "statistikkere/automatisering-avansert.html#validering",
    "href": "statistikkere/automatisering-avansert.html#validering",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "For å gi raskt tilbakemelding på noen mulige feilsituasjoner, så kjøres det enkel validering på kilde config og process_source_data.py når en PR er opprettet.\nHvis valideringen feiler, så må feilen rettes før PRen merges.\nTestene feiler hvis:\n\nKildemappen ikke har et python script kalt process_source_data.py med metodesignaturen, som beskrevet her.\nKildemappen ikke har en yaml fil og en gyldig folder_prefix definert, som i dette eksempelet.\nPython scriptet ikke kan importeres av tjenesten. Tjenesten støtter kun disse tredjeparts pakkene.\nHvis Pyflakes finner feil med kildens Python script.\n\n\n\nHvis validerings testene feiler kan det være nyttig å se på loggene for å finne frem til feilen.\n\nFinn frem til testen som feiler, i bildet feiler valideringstestene for kilde1. Trykk så på lenken “Details” som vist i bilde under. \nPå siden du nå har kommet til skal det være en tabell som heter “Build Information”, trykk på lenken i Build kolonnen. \nDu har nå kommet frem til loggene, se etter indikasjoner på feil. I eksemplet under ser vi at testen test_main_accepts_expected_number_of_args feiler fordi process_source_data.py mangler en main funksjon. \nFiks feilen og push endingen til samme branch, testen vil da starte på nytt."
  },
  {
    "objectID": "statistikkere/automatisering-avansert.html#utrulling-1",
    "href": "statistikkere/automatisering-avansert.html#utrulling-1",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "Endringer til process_source_data.py blir automatisk rullet ut når en PR er merget til main branchen. Utrullingsprosessen tar noe tid, ca. 2-3 minutter fra branchen er merget til tjenesten er oppdatert, for å bekrefte at tjenesten er rullet ut kan du følge stegene i neste avsnitt.\n\n\nStegene under viser hvordan man går frem for å finne resultat av utrullingen av kilden “ledstill” for teamet “arbmark-skjema”. Og forutsetter at koden er pushet til main branchen.\n\nNaviger til GitHub.\nI søkefeltet oppe i venstre hjørne skriv arbmark-skjema og klikk “Jump to” arbmark-skjema-iac. Som i bilde under. \nNår utrullingen er ferdig vil en av disse ikonene vises, grønn hake betyr at tjeneste er rullet ut med koden som ligger i main og at nye filer blir behandlet med koden som ligger der. . Rødt kryss indikerer at utrullig har feilet.  Se etter symbolene der hvor den røde pilen i bilde under peker. I eksempel er utrulligen vellykket."
  },
  {
    "objectID": "statistikkere/automatisering-avansert.html#monitorering-av-tjenesten",
    "href": "statistikkere/automatisering-avansert.html#monitorering-av-tjenesten",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "Man får en oversikt over kildene man har konfigurert prosessering for og statusen på dem ved hjelp av konsollet på GCP. Der navigerer man til siden for Cloud Run (se Figur 2) som er kjøremiljøet som kildedata prosessering benyttes av. Eksempel URl er: https://console.cloud.google.com/run?project=&lt;teamets-prosjekt-id&gt;\nHer får man en oversikt av ressursbruk og loggene til prosesseringen.\n\n\n\nFigur 2: Cloud Run dashboard\n\n\n\n\nEtter du har valgt kilden kan du se logger ved å velge fanen “LOGS”. Her ligger alle logger for den spesifikke kilden. For å få bedre oversikt over eventulle feil kan man sette severity til error. Dette vil uten ekstra konfigurasjon gi oversikt over alle uhåndterte exceptions. \n\n\n\nHvis en fil blir mottatt av tjenesten, men ikke lar seg behandle blir det skrevet til loggen. Man kan få en oversik over hvilke filer som ikke har blitt prosessert ved å søke etter: Could not process object."
  },
  {
    "objectID": "statistikkere/automatisering-avansert.html#trigge-tjenesten-manuelt",
    "href": "statistikkere/automatisering-avansert.html#trigge-tjenesten-manuelt",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "Noen ganger vil det være nødvendig å trigge kjøring av en kilde uten at de tilhørende filene i kildebøtta er oppdatert f.eks. etter en endring i prosseseringsskriptet. For å gjøre dette kan man benytte seg av dapla toolbelt.\nFor å trigge en ny kjøring må man være data-admin i teamet og ha denne informasjonen tilgjengelig:\n\nproject_id(prosjekt id) for kilden, den finner man ved å følge beskrivelsen her.\nfolder_prefix beskriver stien til filene som skal behandles og fungerer på samme måte som i config.yaml\nsource_name finner man ved å se på navnet til mappen hvor kilden konfigureres, i eksempelet her ser vi at team smaabakst har to kilder boller og rundstykker.\n\n\n\nDette eksemplet viser hvordan man går frem for å manuelt trigge kilden boller for team smaabakst.\nTeam smaabakst ønsker å re-prosessere alle filer i kilden boller. Ved å bruke samme folder_prefix som i config.yaml vil alle filer som tilhører kilden bli prosessert på nytt.\nfrom dapla import trigger_source_data_processing\n\nproject_id = \"prod-smaabakst-b69d\"\nsource_name = \"boller\"\nfolder_prefix = \"boller\"\n\ntrigger_source_data_processing(project_id, source_name, folder_prefix)"
  },
  {
    "objectID": "statistikkere/dashboard.html",
    "href": "statistikkere/dashboard.html",
    "title": "Dash og dashboard",
    "section": "",
    "text": "Hvis en ønsker å lage ett dashbord som et brukergrensesnitt, så kan pakken Dash være et godt alternativ. Dash er ett rammeverk hvor man selv kan bygge opp applikasjoner i form av dashbord på en enklere måte, og det bygges oppå javascript pakker som plotly.js og react.js. Det er et produkt ved siden av og helintegrert med plotly, som også er en annen pakke i Python som gir oss interaktive grafer. Dash er et godt verktøy hvis en ønsker et dashbord som brukergrensesnitt for interaktiv visualisering av data. Dash kan kodes i Python og R, men også Julia og F#.\n\n\nI SSB kan man lage dashbord i virtuelle miljøer, gjerne satt opp med ssb-project, men man trenger helt spesifikke pakker for å få det oppe å gå. Mer info om å sette opp et eget miljø med ssb-project finner du her. Tabell under viser navn på pakkene, og gjeldende versjoner som er benyttet i skrivende stund av eksempel i neste avsnitt. Vel og merke kan nyere versjoner også fungere fint, noe man må prøve ut selv, men følgende versjoner fungerer fint.\n\n\n\nPakke\nVersjon (i skrivende stund)\n\n\n\n\ndash\n2.8.1\n\n\njupyter-dash\n0.4.2\n\n\njupyter-server-proxy\n3.2.2\n\n\njupyterlab-dash\n0.1.0a3\n\n\nipykernel\n6.21.3\n\n\ndash-bootstrap-components\n1.3.0 (hvis ønskelig)\n\n\n\nFor mer om håndtering av pakker i ett virtuelt miljø satt opp med ssb-project kan man se nærmere her. For å legge til disse pakkene kan man gjøre følgende i terminalen:\n\n\nterminal\n\npoetry add dash jupyter-dash jupyter-server-proxy jupyterlab-dash ipykernel\n\nOg hvis en ønsker Dash-Bootstrap-Components:\n\n\nterminal\n\npoetry add dash-bootstrap-components\n\nVel og merke så vil ikke denne pakken fungere uten at tilhørende filer er med i selve repoet. Dette fordi i hovedsak leter denne etter filer den trenger på internett. Pakken i seg selv har en fordel i at det er lettere å bygge opp utseende(layout) i dashbordet ditt, samt andre komponenter som ikke ligger i standard dash pakken.\n\n\n\nNoen ting er viktig å huske på at kommer i korrekt rekkefølge når en koder opp i JupyterLab. Her kommer ett veldig enkelt eksempel.\nFørste celle importerer vi alle nødvendige pakker\n\n\nnotebook\n\nfrom dash import html\nfrom jupyter_dash import JupyterDash\nfrom jupyter_dash.comms import _send_jupyter_config_comm_request\n_send_jupyter_config_comm_request()\n\nI Andre celle må følgende kjøres, men her er det VELDIG VIKTIG at man avventer 2-3 sekunder etter at forrige celle ble kjørt.\n\n\nnotebook\n\nJupyterDash.infer_jupyter_proxy_config()\n\nDeretter så er vi klare for å bygge opp selve dashbordet. så i Tredje celle kan en enkel kode for eksempel se slik ut:\n\n\nnotebook\n\napp = JupyterDash(__name__)\nporten = 8642 # Valgfritt fire sifret nummer\n\napp.layout = html.Div([\n    html.H1(\"Eget dashbord med Dash i SSB\")\n])\n\nif __name__ == \"__main__\":\n    app.run_server(debug=True, port=porten, mode=\"external\")\n\nDenne koden vil starte opp dashbordet i eget vindu i browseren ettersom mode=“external”. Dersom man vil ha et eget vindu i JupyterLab kan man sette denne til “jupyterlab”, eller hvis du vil at dashbordet skal dukke opp under cellen i notebooken din, så kan man sette denne til “inline”.\n\n\n\nDiverse som er verdt å se nærmere på når en bygger dashbord applikasjon med Dash. Det følger med mange gode eksempler for bruk av diverse komponenter i dokumentasjonene under her.\n\nStartsiden til Dash\nLage interaktive grafer i Python med Plotly\nDash Core Components\nDash HTML Components\nDash Bootstrap Components\n\nVel og merke henter denne diverse materialer fra internett, og vil ikke fungere uten tilgang. Det er heller anbefalt å ha de nødvendige filene lagret lokalt for bruk av denne pakken."
  },
  {
    "objectID": "statistikkere/dashboard.html#anbefalte-nødvendige-pakker",
    "href": "statistikkere/dashboard.html#anbefalte-nødvendige-pakker",
    "title": "Dash og dashboard",
    "section": "",
    "text": "I SSB kan man lage dashbord i virtuelle miljøer, gjerne satt opp med ssb-project, men man trenger helt spesifikke pakker for å få det oppe å gå. Mer info om å sette opp et eget miljø med ssb-project finner du her. Tabell under viser navn på pakkene, og gjeldende versjoner som er benyttet i skrivende stund av eksempel i neste avsnitt. Vel og merke kan nyere versjoner også fungere fint, noe man må prøve ut selv, men følgende versjoner fungerer fint.\n\n\n\nPakke\nVersjon (i skrivende stund)\n\n\n\n\ndash\n2.8.1\n\n\njupyter-dash\n0.4.2\n\n\njupyter-server-proxy\n3.2.2\n\n\njupyterlab-dash\n0.1.0a3\n\n\nipykernel\n6.21.3\n\n\ndash-bootstrap-components\n1.3.0 (hvis ønskelig)\n\n\n\nFor mer om håndtering av pakker i ett virtuelt miljø satt opp med ssb-project kan man se nærmere her. For å legge til disse pakkene kan man gjøre følgende i terminalen:\n\n\nterminal\n\npoetry add dash jupyter-dash jupyter-server-proxy jupyterlab-dash ipykernel\n\nOg hvis en ønsker Dash-Bootstrap-Components:\n\n\nterminal\n\npoetry add dash-bootstrap-components\n\nVel og merke så vil ikke denne pakken fungere uten at tilhørende filer er med i selve repoet. Dette fordi i hovedsak leter denne etter filer den trenger på internett. Pakken i seg selv har en fordel i at det er lettere å bygge opp utseende(layout) i dashbordet ditt, samt andre komponenter som ikke ligger i standard dash pakken."
  },
  {
    "objectID": "statistikkere/dashboard.html#eksempel-kode-i-jupyterlab",
    "href": "statistikkere/dashboard.html#eksempel-kode-i-jupyterlab",
    "title": "Dash og dashboard",
    "section": "",
    "text": "Noen ting er viktig å huske på at kommer i korrekt rekkefølge når en koder opp i JupyterLab. Her kommer ett veldig enkelt eksempel.\nFørste celle importerer vi alle nødvendige pakker\n\n\nnotebook\n\nfrom dash import html\nfrom jupyter_dash import JupyterDash\nfrom jupyter_dash.comms import _send_jupyter_config_comm_request\n_send_jupyter_config_comm_request()\n\nI Andre celle må følgende kjøres, men her er det VELDIG VIKTIG at man avventer 2-3 sekunder etter at forrige celle ble kjørt.\n\n\nnotebook\n\nJupyterDash.infer_jupyter_proxy_config()\n\nDeretter så er vi klare for å bygge opp selve dashbordet. så i Tredje celle kan en enkel kode for eksempel se slik ut:\n\n\nnotebook\n\napp = JupyterDash(__name__)\nporten = 8642 # Valgfritt fire sifret nummer\n\napp.layout = html.Div([\n    html.H1(\"Eget dashbord med Dash i SSB\")\n])\n\nif __name__ == \"__main__\":\n    app.run_server(debug=True, port=porten, mode=\"external\")\n\nDenne koden vil starte opp dashbordet i eget vindu i browseren ettersom mode=“external”. Dersom man vil ha et eget vindu i JupyterLab kan man sette denne til “jupyterlab”, eller hvis du vil at dashbordet skal dukke opp under cellen i notebooken din, så kan man sette denne til “inline”."
  },
  {
    "objectID": "statistikkere/dashboard.html#aktuell-dokumentasjon",
    "href": "statistikkere/dashboard.html#aktuell-dokumentasjon",
    "title": "Dash og dashboard",
    "section": "",
    "text": "Diverse som er verdt å se nærmere på når en bygger dashbord applikasjon med Dash. Det følger med mange gode eksempler for bruk av diverse komponenter i dokumentasjonene under her.\n\nStartsiden til Dash\nLage interaktive grafer i Python med Plotly\nDash Core Components\nDash HTML Components\nDash Bootstrap Components\n\nVel og merke henter denne diverse materialer fra internett, og vil ikke fungere uten tilgang. Det er heller anbefalt å ha de nødvendige filene lagret lokalt for bruk av denne pakken."
  },
  {
    "objectID": "statistikkere/gcc.html",
    "href": "statistikkere/gcc.html",
    "title": "Google Cloud Console",
    "section": "",
    "text": "Google Cloud er SSBs leverandør av skytjenester som Dapla er bygget på.\nGoogle Cloud Console er et web-basert grensesnitt for å administrere ressurser og tjenester på Google Cloud. For å bruke denne må man ha en Google-konto. Alle i SSB har en konto knyttet opp mot Google.\n\n\n\n\n\n\nGå til Google Cloud Console og logg på med din SSB-bruker."
  },
  {
    "objectID": "statistikkere/gcc.html#prosjektvelger",
    "href": "statistikkere/gcc.html#prosjektvelger",
    "title": "Google Cloud Console",
    "section": "Prosjektvelger",
    "text": "Prosjektvelger\nØverst på siden, til høyre for teksten Google Cloud finnes det en prosjektvelger. Her er det viktig å velge ditt teams Google prosjekt, ettersom teamets ressurser kun er tilgjengelige innenfor prosjektet. Hvis du trykker på prosjektvelgeren vil det åpnes opp et nytt vindu. Sjekk at det står SSB.NO øverst i dette vinduet.(Figur 1).\n\n\n\nFigur 1: Prosjektvelgeren i Google Cloud Console\n\n\n\nVelg prosjekt\nHer vises det hvordan man velger et prosjekt I GCC. Eksempelet benytter Dapla teamet demo stat b og fortsetter fra (Figur 1).\n\nSkrive teamnavn i søkefeltet, resultatene burde se ut som i (Figur 2).\nTrykk på lenken prod-demo-stat-b, som markert med rød pil i (Figur 2).\n\n\n\n\n\n\n\nI ID kolonnen ser man prosjektets ID (Figur 2).\n\n\n\n\n\n\nFigur 2: Søk i Prosjektvelgeren\n\n\nHar man gjort alle stegene rett vil det i venstre hjørne se ut som i (Figur 3).\n\n\n\nFigur 3: Aktivt prosjekt i GCC"
  },
  {
    "objectID": "statistikkere/gjenopprette-data.html",
    "href": "statistikkere/gjenopprette-data.html",
    "title": "Gjenopprette data fra bøtter",
    "section": "",
    "text": "Alle bøtter har automatisk versjonering. Dette gjør det mulig å tilbakeføre filer til en tidligere versjon eller gjenopprette filer som er slettet ved et uhell.\nLogg inn på Google Cloud Console og søk opp “Cloud Storage” i søkefeltet. Klikk på den bøtten hvor filen er lagret under “Buckets”.\n\n\nFra Cloud Storage skjermbildet kan man navigere seg frem til den mappen hvor filen tidligere er lagret og skru på radioknappen “Show deleted data” (Figur 1)\n\n\n\nFigur 1: Skru på visning av slettede filer\n\n\nNå vil man kunne se slettede filer i kursiv med teksten (Deleted) på slutten. Kolonnen “Version history” vil også vise hvor mange tidligere versjoner som finnes av denne filen. Trykk på filnavnet du ønsker å gjenopprette og velg deretter fanen “Version history”. I listen av versjoner til denne filen har man mulighet til å gjenopprette til en tidligere versjon ved å klikke på “Restore” (Figur 2).\n\n\n\nFigur 2: Gjenoppretting av en slettet fil\n\n\n\n\n\nFra Cloud Storage skjermbildet kan man navigere seg frem til den mappen hvor filen er lagret, og trykke på filnavnet. Velg deretter fanen “Version history”. I listen av versjoner til denne filen har man mulighet til å gjenopprette til en tidligere versjon ved å klikke på “Restore” (Figur 3).\n\n\n\nFigur 3: Versjonshistorikk til en fil"
  },
  {
    "objectID": "statistikkere/gjenopprette-data.html#gjenopprette-en-slettet-fil",
    "href": "statistikkere/gjenopprette-data.html#gjenopprette-en-slettet-fil",
    "title": "Gjenopprette data fra bøtter",
    "section": "",
    "text": "Fra Cloud Storage skjermbildet kan man navigere seg frem til den mappen hvor filen tidligere er lagret og skru på radioknappen “Show deleted data” (Figur 1)\n\n\n\nFigur 1: Skru på visning av slettede filer\n\n\nNå vil man kunne se slettede filer i kursiv med teksten (Deleted) på slutten. Kolonnen “Version history” vil også vise hvor mange tidligere versjoner som finnes av denne filen. Trykk på filnavnet du ønsker å gjenopprette og velg deretter fanen “Version history”. I listen av versjoner til denne filen har man mulighet til å gjenopprette til en tidligere versjon ved å klikke på “Restore” (Figur 2).\n\n\n\nFigur 2: Gjenoppretting av en slettet fil"
  },
  {
    "objectID": "statistikkere/gjenopprette-data.html#gjenopprette-en-fil-til-en-tidligere-versjon",
    "href": "statistikkere/gjenopprette-data.html#gjenopprette-en-fil-til-en-tidligere-versjon",
    "title": "Gjenopprette data fra bøtter",
    "section": "",
    "text": "Fra Cloud Storage skjermbildet kan man navigere seg frem til den mappen hvor filen er lagret, og trykke på filnavnet. Velg deretter fanen “Version history”. I listen av versjoner til denne filen har man mulighet til å gjenopprette til en tidligere versjon ved å klikke på “Restore” (Figur 3).\n\n\n\nFigur 3: Versjonshistorikk til en fil"
  },
  {
    "objectID": "statistikkere/daplalab-overview.html",
    "href": "statistikkere/daplalab-overview.html",
    "title": "DaplaLab",
    "section": "",
    "text": "DaplaLab"
  },
  {
    "objectID": "statistikkere/datatjenester-overview.html",
    "href": "statistikkere/datatjenester-overview.html",
    "title": "Datatjenester",
    "section": "",
    "text": "Datatjenester"
  },
  {
    "objectID": "statistikkere/git-og-github.html",
    "href": "statistikkere/git-og-github.html",
    "title": "Git og Github",
    "section": "",
    "text": "I SSB anbefales det man versjonhåndterer koden sin med Git og deler koden via GitHub. For å lære seg å bruke disse verktøyene på en god måte er det derfor viktig å forstå forskjellen mellom Git og Github. Helt overordnet er forskjellen følgende:\n\nGit er programvare som er installert på maskinen du jobber på og som sporer endringer i koden din.\nGitHub er et slags felles mappesystem på internett som lar deg dele og samarbeide med andre om kode.\n\nAv definisjonene over så skjønner vi at det er Git som gir oss all funksjonalitet for å lagre versjoner av koden vår. GitHub er mer som et valg av mappesystem. Men måten kodemiljøene våre er satt opp på Dapla så har vi ingen fellesmappe som alle kan kjøre koden fra. Man utvikler kode i sin egen hjemmemappe, som bare du har tilgang til, og når du skal samarbeide med andre, så må du sende koden til GitHub. De du samarbeider med må deretter hente ned denne koden før de kan kjøre den.\nI dette kapittelet ser vi nærmere på Git og Github og hvordan de er implementert i SSB. Selv om SSB har laget programmet ssb-project for å gjøre det lettere å bl.a. forholde seg til Git og GitHub, så vil vi dette kapittelet forklare nærmere hvordan det funker uten dette hjelpemiddelet. Forhåpentligvis vil det gjøre det lettere å håndtere mer kompliserte situasjoner som oppstår i arbeidshverdagen som statistikker.\n\n\nGit er terminalprogram som installert på maskinen du jobber. Hvis man ikke liker å bruke terminalen finnes det mange pek-og-klikk versjoner av Git, blant annet i Jupyterlab, SAS EG og RStudio. Men typisk vil det en eller annen gang oppstå situasjoner der det ikke finnes løsninger i pek-og-klikk versjonen, og man må ordne opp i terminalen. Av den grunn velger vi her å fokusere på hvordan Git fungerer fra terminalen. Vi vil også fokusere på hvordan Git fungerer fra terminalen i Jupyterlab på Dapla.\n\n\nGit er en programvare for distribuert versjonshåndtering av filer. Det vil si at den tar vare på historien til koden din. At Git er distribuert betyr at alle som jobber med koden har en kopi av koden hos seg selv. Når man ønsker å dele koden med andre, så laster man opp koden til et felles kodelager som GitHub. Typisk vil Git brukes til å passe på historien til rene tekstfiler, f.eks. kode-script, hvor det gir mening å se på forskjeller mellom filen på ulike tidspunkter. Men når det er sagt, så kan Git også brukes til å følge med på endringer i andre filtyper, f.eks. binære filer som bilder, PDF-filer, etc.. Men binære filer er ikke så vanlig inkludere i Git-prosjekter siden forskjellene i historikken til filene er vanskelig å forstå for mennesker.\nMan aktiverer Git på en mappe i filsystemet sitt med kommandoen git init når man står i mappen som skal versjonshånderes. Da vil Git versjonshåndtere alle filer som er i den mappen og i eventuelle undermapper. Når du så gjør endringer på en fil i mappen, så vil Git registrere endringer. Ønsker du at endringen skal bli et punkt i historikken til prosjektet, så må du først legge til filen i Git med kommandoen git add filnavn. Når du har gjort dette, så kan du lagre endringen med kommandoen git commit -m \"Din melding her\". Når du har gjort dette, så vil endringen være lagret i Git. Når du har gjort mange endringer, så kan du sende endringene til GitHub med kommandoen git push. Når du har gjort dette, så vil endringene være synlige for alle som har tilgang til GitHub-prosjektet.\nI SSB anbefaler vi at du starter et nytt Git-prosjekt ved å benytte ssb-project. Da vil du ikke bare aktivere Git, men du kan også få implementert en del andre gode praksiser for å holde koden din ryddig, oversiktlig og sikker.\nMen før vi kan begynne å bruke Git må vi konfigurere vår egen bruker for Git, som er forklart i neste seksjon.\nLes mer om Git på https://git-scm.com/.\n\n\n\n\n\n\n\n\n\nBruk SSB sin konfigurasjon!\n\n\n\nAlle som bruker GitHub i SSB bør bruke SSB sin Git-konfigurasjon. Hvis ikke kan man bl.a. risikere at output fra Notebooks blir pushet til GitHub. Du kan bruke SSBs konfigurasjon ved å kjøre ssb-gitconfig.py i terminalen og svare på spørsmålene som dukker opp.\n\n\nFor å jobbe med Git så må man konfigurere brukeren sin slik at Git vet hvem som gjør endringer i koden. I praksis betyr det at du må ha filen .gitconfig på hjemmeområdet ditt (f.eks. /home/jovyan/.gitconfig på Dapla) med noe grunnleggende informasjon:\n\n\n.gitconfig\n\n# /home/jovyan/.gitconfig på Dapla\n[user]\n        name = Ola Nordmann\n        email = &lt;ini&gt;@ssb.no\n\nMed denne konfigurasjonen så kan man bruke Git lokalt. Men skal man også bruke GitHub i SSB, dvs. dele kode med andre, så må man også legge til mer informasjon i .gitconfig. Heldigvis er det skrevet et script som gjør dette for deg. For å få anbefalt konfigurasjon for Git så kan du kjøre følgende kommando i terminalen:\n\n\nterminal\n\nssb-gitconfig.py\n\nDette scriptet vil spørre deg om ditt brukernavn i SSB, og så vil det opprette en fil som heter .gitconfig i hjemmeområdet ditt. Denne filen vil inneholde informasjon om brukernavn, e-post og tilgang til GitHub. I tillegg vil den sørge for at ditt ssb-project ikke sender output fra notebooks til GitHub.\n\n\n\nPå Dapla er det Jupyterlab som er utviklingsmiljøet for kode. I Jupyterlab vil man ofte jobbe i notebooks, og det har noen konsekvenser for versjonshåndtering. En notebook er en ipynb-fil som inneholder både tekst og kode. Åpner vi disse filene i Jupyterlab så ser vi det kjente notebook-grensesnittet med kode- og markdown-celler. Men egentlig er det bare et grensesnitt bygd over en json-fil1. Denne json-filen inneholder metadata og masse annet som gjør det vanskelig å se forskjellen på en fil over tid. Dette er derfor noe som å fikses før Git blir et nyttig verktøy for disse filene.\nI SSB sin anbefalte konfigurasjon setter opp nbdime for å få leselige differ mellom notebooks. Har du satt opp Git med ssb-gitconfig.py så vil dette være automatisk satt opp for deg.\nDet finnes også alternativer til å bruke nbdime. På Dapla er Jupytext installert for de som ikke ønsker å versjonshåndtere ipynb-filer, men heller vil bruke rene script-filer som .py- eller .R-filer. Måten Jupytext gjør dette på er konvertere f.eks. en ipynb-fil py-fil for deg, Den kan gjøre det automatisk når du lagrer, eller du kan gjøre det manuelt. Med denne tilnærmingen så kan du be Git ignorere alle ipynb-filer og bare versjonshåndtere de rene script.filene. Oppsett er Jupytext er ikke en del av SSB sin standard-konfigurasjon og er noe du må sett opp selv.\n\n\n\nGit er veldig sterkt verktøy med mange muligheter. Det er ikke tenkt at alt skal adresseres i dette kapitlet. Men det er noen vanlig operasjoner som er så vanlige at alle som jobber med kode i SSB bør kjenne dem.\nVi har tidligere nevnt at kommandoen for å aktivere versjonshåndtering med Git på en mappe, er git init. Dette gjøres også automatisk når man oppretter et nytt ssb-project.\nHva skjer hvis man gjør en endring i en fil i mappa? For det første kan du kjøre kommandoen git status for å se hvilke endringer Git har oppdaget. Men det betyr ikke at endringene er under versjonskontroll enda. For å fortelle Git om at disse endringene skal registreres så må du kjøre git add &lt;filnavn&gt;. Men fortsatt er ikke endringene et punkt i historien til koden din. For å gjøre det må du kjøre git commit -m \"Din melding her\". Ved å gjøre det så har du laget et unikt punkt i historien til koden din, med en egen ID, som du kan gå tilbake til senere hvis du ønsker.\nNår man utvikler kode så gjør man det fra såkalte branches2. Hvis vi tenker oss at din eksisterende kodebase er stammen på et tre (ofte kalt master eller main), så legger Git opp til at man gjør endringer på denne koden via branches eller grener av treet. Med andre ord holder vi stammen/master urørt helt til vi vet at endringen fungerer som den skal. Til slutt merger vi den inn i master. Vi kan opprette en ny branch og gå inn i den ved å skrive git checkout -b &lt;branch navn&gt;. Da står du i en branch og kan bruke kommandoer som git add og git commit som vist tidligere.\nVi kan merge endringene fra vår branch inn i main ved å først gå inn i main, git switch main, og deretter skrive git merge &lt;branch navn&gt;. Men typisk vil ikke dette være fremgangsmåten i SSB. Når man er fornøyd med endringene i en branch, så vil man pushe den opp til GitHub, slik at en kollega kan vurdere om den skal merges inn i main. Dermed gjøres selve mergen i GitHub-grensenittet. Vi skal se nærmere på GitHub i neste kapittel.\n\n\n\n\nGitHub er et nettsted som bl.a. fungerer som vårt felles mappesystem for deling av kode. SSB har sin egen organisasjonskonto med navn statisticsnorway. Men selv om SSB har en organisasjonskonto på GitHub må alle ansatte opprette sin egen brukerprofil, og knytte den mot SSB sin organisasjonskonto. Under forklarer vi hvordan du gjør dette.\n\n\nDette kapittelet er bare relevant hvis man ikke har en GitHub-brukerkonto fra før. For å bruke ssb-project-programmet til å generere et remote repo på GitHub må du ha en konto. Derfor starter vi med å gjøre dette. Det er en engangsjobb og du trenger aldri gjøre det igjen.\n\n\n\n\n\n\nSSB har valgt å ikke sette opp SSB-brukerne til de ansatte som GitHub-brukere. En viktig årsak er at er en GitHub-konto ofte regnes som en del av den ansattes CV. For de som aldri har brukt GitHub før kan det virke fremmed, men det er nok en fordel på sikt når alle blir godt kjent med denne arbeidsformen.\n\n\n\nSlik gjør du det:\n\nGå til https://github.com/\nTrykk Sign up øverst i høyre hjørne\nI dialogboksen som åpnes, se Figur 1, skriver du inn din e-postkonto og lager et passord. Dette trenger ikke være din SSB-bruker og e-post. Vi anbefaler at du bruker en personlig e-postkonto og velger ditt eget passord. Det samme gjelder brukernavn også.\n\n\n\n\nFigur 1: Dialogboks for opprettelse av GitHub-bruker.\n\n\nDu har nå laget en egen GitHub-bruker. I neste steg skal vi knytte denne kontoen til din SSB-bruker.\n\n\n\nHvis du har fullført forrige steg så har du nå en GitHub-konto. Hvis du står på din profil-side så ser den ut som i Figur 2.\n\n\n\nFigur 2: Et eksempel på hjemmeområdet til en GitHub-bruker\n\n\nDet neste vi må gjøre er å aktivere 2-faktor autentisering, siden det er dette som benyttes i SSB. Hvis du står på siden i bildet over, så gjør du følgende for å aktivere 2-faktor autentisering mot GitHub:\n\n\n\nTrykk på den lille pilen øverst til høyre og velg Settings(se Figur 3).\nDeretter velger du Password and authentification i menyen til venstre.\nUnder Two-factor authentication trykker du på Enable.\n\n\n\n\n\n\nFigur 3: Åpne settings for din GitHub-bruker.\n\n\n\n\n\nFigur 4 viser dialogboksen som vises. Velg Enable two-factor authentification.\n\n\n\n\nFigur 4: Dialogboks som åpnes når 2FA skrus på første gang.\n\n\n\nFigur 5 viser dialogboksen som vises for å velge hvordan man skal autentisere seg. Her anbefales det å velge Set up using an app, slik at du kan bruke Microsoft Authenticator-appen på din mobil.\n\n\n\n\nFigur 5: Dialogboks for å velge hvordan man skal autentisere seg med 2FA.\n\n\nFigur 6 viser QR-koden som vises. Denne skal vi bruke i neste steg.\n\n\n\nFigur 6: QR-kode som skannes av Microsoft Authenticator.\n\n\n\n\n\nStrekkoden over skal skannes i din Microsoft Authenticator-app på mobilen, som vist i Figur 7. Åpne appen, trykk på Bekreftede ID-er, og til slutt trykk på Skann QR-kode. Deretter skanner du QR-koden fra punkt 5.\nNår koden er skannet har du fått opp følgende bilde på appens hovedside (se bilde til høyre). Skriv inn den 6-siffer koden på GitHub-siden med QR-koden.\nTil slutt lagrer du Recovery-codes et trygt sted som bare du har tilgang til.\n\n\n\n\n\n\nFigur 7: Mobilappen Microsoft authenticator\n\n\n\n\nNå har vi aktivert 2-faktor autentisering for GitHub og er klare til å knytte vår personlige konto til vår SSB-bruker på SSBs “Github organisation” statisticsnorway.\n\n\n\nI forrige steg aktiverte vi 2-faktor autentisering for GitHub. Det neste vi må gjøre er å koble oss til Single Sign On (SSO) for SSB sin organisasjon på GitHub:\n\nTrykk på lenken https://github.com/orgs/statisticsnorway/sso\nI dialogboksen som dukker opp trykker du på Continue, slik som vist i Figur 8.\n\n\n\n\nFigur 8: Single Sign on (SSO) for SSB sin organisasjon på GitHub\n\n\nNår du har gjennomført dette så har du tilgang til statisticsnorway på GitHub. Går du inn på denne lenken så skal du nå kunne lese både Public, Private og Internal repoer, slik som vist i Figur 9.\n\n\n\nFigur 9: Medlemsvisning for SSB sin GitHub-organisasjon.\n\n\n\n\n\nNår vi skal jobbe med SSB-kode som ligger lagret hos statistcsnorway på GitHub, så må vi autentisere oss. Måten vi gjøre det på er ved å generere et Personal Access Token (ofte forkortet PAT) som vi oppgir når vi vil hente eller oppdatere kode på GitHub. Da sender vi med PAT for å autentisere oss for GitHub.\n\n\nFor å lage en PAT som er godkjent mot statisticsnorway så gjør man følgende:\n\nGå til din profilside på GitHub og åpne Settings slik som ble vist Seksjon 1.2.2.\nVelg Developer Settings i menyen til venstre.\nI menyen til venstre velger du Personal Access Token, og deretter Tokens (classic).\nVelg Generate new token og deretter Generate new token (classic).\nUnder Note kan du gi PAT’en et navn. Velg et navn som er intuitivt for deg. Hvis du skal bruke PAT til å jobbe mot Dapla, så ville jeg ganske enkelt kalt den dapla. Hvis du skal bruke den mot bakkemiljøet ville jeg kalt den prodsone eller noe annet som gjør det lett for det skjønne innholdet i ettertid.\nUnder Expiration velger du hvor lang tid som skal gå før PAT blir ugyldig. Dette er en avvening mellom sikkerhet og hva som er praktisk. Det anbefales at du velger 365 dager. Når PAT utløper må du gjenta stegene i dette kapittelet.\nUnder Select scopes velger du repo og workflow slik som vist i Figur 10.\n\n\n\n\nFigur 10: Gi token et kort og beskrivende navn\n\n\n\nTrykk på Generate token nederst på siden og du får noe lignende det du ser i Figur 11.\n\n\n\n\nFigur 11: Token som ble generert.\n\n\n\nKopier deretter PAT til en midlertidig fil. Grunnen er at du aldri vil se det igjen her etter at vi har gjennomført neste steg.\nDeretter trykker du på Configure SSO og velger Authorize ved siden statisticsnorway, slik som vist i Figur 12. Svar deretter på spørsmålene som dukker opp.\n\n\n\n\nFigur 12: Autorisering av Token mot SSBs GiHub-organisasjon.\n\n\nVi har nå opprettet en PAT som er godkjent for bruk mot SSB sin kode på GitHub. Det betyr at hvis vi vil jobbe med Git på SSB sine maskiner i sky eller på bakken, så må vi sendte med dette tokenet for å få lov til å jobbe med koden som ligger på statisticsnorway på GitHub.\n\n\n\nDet er ganske upraktisk å måtte sende med tokenet hver gang vi skal jobbe med GitHub. Vi bør derfor lagre det lokalt der vi jobber, slik at Git automatisk finner det. Det finnes mange måter å gjøre dette på og det er ikke bestemt hva som skal være beste-praksis i SSB. Men en måte å gjøre det er via en .netrc-fil. Vi oppretter da en fil som heter .netrc på vårt hjemmeområde, og legger følgende informasjon på en (hvilken som helst) linje i filen:\n\n\n~/.netrc\n\nmachine github.com login &lt;github-bruker&gt; password &lt;Personal Access Token&gt;\n\nGitHub-bruker er da din personlige bruker og IKKE brukernavnet ditt i SSB. Personal Access Token er det vi lagde i forrige kapittelet.\nEn veldig enkel måte å lagre dette er som følger. Anta at min personlige GitHub-bruker er SSB-Chad og at min Personal Access Token er blablabla. Da kan jeg gjøre følgende for å lagre det i .netrc:\n\nGå inn i Jupyterlab og åpne en Python-notebook.\nI den første kodecellen skriver du:\n\n\n\nterminal\n\n!echo \"machine github.com login SSB-Chad password blablabla\" &gt;&gt; ~/.netrc\n\nAlternativt kan du droppe det utropstegnet og kjøre det direkte i en terminal. Det vil gi samme resultat. Koden over legger til en linje med teksten\nmachine github.com login SSB-Chad password blablabla\ni en .netrc-fil på din hjemmeområdet, uanvhengig av om du har en fra før eller ikke. Hvis du har en fil fra før som allerede har et token fra GitHub, ville jeg nok slettet det før jeg legger en et nytt token.\nHver gang du jobber mot GitHub vil Git sjekke om informasjon om autentisering ligger i denne filen, og bruke den hvis den ligger der.\n\n\n\nI eksempelet over lagde vi en PAT som var gyldig i 90 dager. Dermed vil du ikke kunne jobbe mot GitHub med dette tokenet etter 90 dager. For å oppdatere tokenet gjør du følgende:\n\nLag et nytt PAT ved å repetere Seksjon 1.2.4.1.\nI miljøet der du skal jobbe med Git og GitHub går du inn i din .netrc og bytter ut token med det nye.\n\nOg med det er du klar til å jobbe mot statisticsnorway på GitHub."
  },
  {
    "objectID": "statistikkere/git-og-github.html#git-fa-brands-git-alt",
    "href": "statistikkere/git-og-github.html#git-fa-brands-git-alt",
    "title": "Git og Github",
    "section": "",
    "text": "Git er terminalprogram som installert på maskinen du jobber. Hvis man ikke liker å bruke terminalen finnes det mange pek-og-klikk versjoner av Git, blant annet i Jupyterlab, SAS EG og RStudio. Men typisk vil det en eller annen gang oppstå situasjoner der det ikke finnes løsninger i pek-og-klikk versjonen, og man må ordne opp i terminalen. Av den grunn velger vi her å fokusere på hvordan Git fungerer fra terminalen. Vi vil også fokusere på hvordan Git fungerer fra terminalen i Jupyterlab på Dapla.\n\n\nGit er en programvare for distribuert versjonshåndtering av filer. Det vil si at den tar vare på historien til koden din. At Git er distribuert betyr at alle som jobber med koden har en kopi av koden hos seg selv. Når man ønsker å dele koden med andre, så laster man opp koden til et felles kodelager som GitHub. Typisk vil Git brukes til å passe på historien til rene tekstfiler, f.eks. kode-script, hvor det gir mening å se på forskjeller mellom filen på ulike tidspunkter. Men når det er sagt, så kan Git også brukes til å følge med på endringer i andre filtyper, f.eks. binære filer som bilder, PDF-filer, etc.. Men binære filer er ikke så vanlig inkludere i Git-prosjekter siden forskjellene i historikken til filene er vanskelig å forstå for mennesker.\nMan aktiverer Git på en mappe i filsystemet sitt med kommandoen git init når man står i mappen som skal versjonshånderes. Da vil Git versjonshåndtere alle filer som er i den mappen og i eventuelle undermapper. Når du så gjør endringer på en fil i mappen, så vil Git registrere endringer. Ønsker du at endringen skal bli et punkt i historikken til prosjektet, så må du først legge til filen i Git med kommandoen git add filnavn. Når du har gjort dette, så kan du lagre endringen med kommandoen git commit -m \"Din melding her\". Når du har gjort dette, så vil endringen være lagret i Git. Når du har gjort mange endringer, så kan du sende endringene til GitHub med kommandoen git push. Når du har gjort dette, så vil endringene være synlige for alle som har tilgang til GitHub-prosjektet.\nI SSB anbefaler vi at du starter et nytt Git-prosjekt ved å benytte ssb-project. Da vil du ikke bare aktivere Git, men du kan også få implementert en del andre gode praksiser for å holde koden din ryddig, oversiktlig og sikker.\nMen før vi kan begynne å bruke Git må vi konfigurere vår egen bruker for Git, som er forklart i neste seksjon.\nLes mer om Git på https://git-scm.com/.\n\n\n\n\n\n\n\n\n\nBruk SSB sin konfigurasjon!\n\n\n\nAlle som bruker GitHub i SSB bør bruke SSB sin Git-konfigurasjon. Hvis ikke kan man bl.a. risikere at output fra Notebooks blir pushet til GitHub. Du kan bruke SSBs konfigurasjon ved å kjøre ssb-gitconfig.py i terminalen og svare på spørsmålene som dukker opp.\n\n\nFor å jobbe med Git så må man konfigurere brukeren sin slik at Git vet hvem som gjør endringer i koden. I praksis betyr det at du må ha filen .gitconfig på hjemmeområdet ditt (f.eks. /home/jovyan/.gitconfig på Dapla) med noe grunnleggende informasjon:\n\n\n.gitconfig\n\n# /home/jovyan/.gitconfig på Dapla\n[user]\n        name = Ola Nordmann\n        email = &lt;ini&gt;@ssb.no\n\nMed denne konfigurasjonen så kan man bruke Git lokalt. Men skal man også bruke GitHub i SSB, dvs. dele kode med andre, så må man også legge til mer informasjon i .gitconfig. Heldigvis er det skrevet et script som gjør dette for deg. For å få anbefalt konfigurasjon for Git så kan du kjøre følgende kommando i terminalen:\n\n\nterminal\n\nssb-gitconfig.py\n\nDette scriptet vil spørre deg om ditt brukernavn i SSB, og så vil det opprette en fil som heter .gitconfig i hjemmeområdet ditt. Denne filen vil inneholde informasjon om brukernavn, e-post og tilgang til GitHub. I tillegg vil den sørge for at ditt ssb-project ikke sender output fra notebooks til GitHub.\n\n\n\nPå Dapla er det Jupyterlab som er utviklingsmiljøet for kode. I Jupyterlab vil man ofte jobbe i notebooks, og det har noen konsekvenser for versjonshåndtering. En notebook er en ipynb-fil som inneholder både tekst og kode. Åpner vi disse filene i Jupyterlab så ser vi det kjente notebook-grensesnittet med kode- og markdown-celler. Men egentlig er det bare et grensesnitt bygd over en json-fil1. Denne json-filen inneholder metadata og masse annet som gjør det vanskelig å se forskjellen på en fil over tid. Dette er derfor noe som å fikses før Git blir et nyttig verktøy for disse filene.\nI SSB sin anbefalte konfigurasjon setter opp nbdime for å få leselige differ mellom notebooks. Har du satt opp Git med ssb-gitconfig.py så vil dette være automatisk satt opp for deg.\nDet finnes også alternativer til å bruke nbdime. På Dapla er Jupytext installert for de som ikke ønsker å versjonshåndtere ipynb-filer, men heller vil bruke rene script-filer som .py- eller .R-filer. Måten Jupytext gjør dette på er konvertere f.eks. en ipynb-fil py-fil for deg, Den kan gjøre det automatisk når du lagrer, eller du kan gjøre det manuelt. Med denne tilnærmingen så kan du be Git ignorere alle ipynb-filer og bare versjonshåndtere de rene script.filene. Oppsett er Jupytext er ikke en del av SSB sin standard-konfigurasjon og er noe du må sett opp selv.\n\n\n\nGit er veldig sterkt verktøy med mange muligheter. Det er ikke tenkt at alt skal adresseres i dette kapitlet. Men det er noen vanlig operasjoner som er så vanlige at alle som jobber med kode i SSB bør kjenne dem.\nVi har tidligere nevnt at kommandoen for å aktivere versjonshåndtering med Git på en mappe, er git init. Dette gjøres også automatisk når man oppretter et nytt ssb-project.\nHva skjer hvis man gjør en endring i en fil i mappa? For det første kan du kjøre kommandoen git status for å se hvilke endringer Git har oppdaget. Men det betyr ikke at endringene er under versjonskontroll enda. For å fortelle Git om at disse endringene skal registreres så må du kjøre git add &lt;filnavn&gt;. Men fortsatt er ikke endringene et punkt i historien til koden din. For å gjøre det må du kjøre git commit -m \"Din melding her\". Ved å gjøre det så har du laget et unikt punkt i historien til koden din, med en egen ID, som du kan gå tilbake til senere hvis du ønsker.\nNår man utvikler kode så gjør man det fra såkalte branches2. Hvis vi tenker oss at din eksisterende kodebase er stammen på et tre (ofte kalt master eller main), så legger Git opp til at man gjør endringer på denne koden via branches eller grener av treet. Med andre ord holder vi stammen/master urørt helt til vi vet at endringen fungerer som den skal. Til slutt merger vi den inn i master. Vi kan opprette en ny branch og gå inn i den ved å skrive git checkout -b &lt;branch navn&gt;. Da står du i en branch og kan bruke kommandoer som git add og git commit som vist tidligere.\nVi kan merge endringene fra vår branch inn i main ved å først gå inn i main, git switch main, og deretter skrive git merge &lt;branch navn&gt;. Men typisk vil ikke dette være fremgangsmåten i SSB. Når man er fornøyd med endringene i en branch, så vil man pushe den opp til GitHub, slik at en kollega kan vurdere om den skal merges inn i main. Dermed gjøres selve mergen i GitHub-grensenittet. Vi skal se nærmere på GitHub i neste kapittel."
  },
  {
    "objectID": "statistikkere/git-og-github.html#github-fa-brands-github",
    "href": "statistikkere/git-og-github.html#github-fa-brands-github",
    "title": "Git og Github",
    "section": "",
    "text": "GitHub er et nettsted som bl.a. fungerer som vårt felles mappesystem for deling av kode. SSB har sin egen organisasjonskonto med navn statisticsnorway. Men selv om SSB har en organisasjonskonto på GitHub må alle ansatte opprette sin egen brukerprofil, og knytte den mot SSB sin organisasjonskonto. Under forklarer vi hvordan du gjør dette.\n\n\nDette kapittelet er bare relevant hvis man ikke har en GitHub-brukerkonto fra før. For å bruke ssb-project-programmet til å generere et remote repo på GitHub må du ha en konto. Derfor starter vi med å gjøre dette. Det er en engangsjobb og du trenger aldri gjøre det igjen.\n\n\n\n\n\n\nSSB har valgt å ikke sette opp SSB-brukerne til de ansatte som GitHub-brukere. En viktig årsak er at er en GitHub-konto ofte regnes som en del av den ansattes CV. For de som aldri har brukt GitHub før kan det virke fremmed, men det er nok en fordel på sikt når alle blir godt kjent med denne arbeidsformen.\n\n\n\nSlik gjør du det:\n\nGå til https://github.com/\nTrykk Sign up øverst i høyre hjørne\nI dialogboksen som åpnes, se Figur 1, skriver du inn din e-postkonto og lager et passord. Dette trenger ikke være din SSB-bruker og e-post. Vi anbefaler at du bruker en personlig e-postkonto og velger ditt eget passord. Det samme gjelder brukernavn også.\n\n\n\n\nFigur 1: Dialogboks for opprettelse av GitHub-bruker.\n\n\nDu har nå laget en egen GitHub-bruker. I neste steg skal vi knytte denne kontoen til din SSB-bruker.\n\n\n\nHvis du har fullført forrige steg så har du nå en GitHub-konto. Hvis du står på din profil-side så ser den ut som i Figur 2.\n\n\n\nFigur 2: Et eksempel på hjemmeområdet til en GitHub-bruker\n\n\nDet neste vi må gjøre er å aktivere 2-faktor autentisering, siden det er dette som benyttes i SSB. Hvis du står på siden i bildet over, så gjør du følgende for å aktivere 2-faktor autentisering mot GitHub:\n\n\n\nTrykk på den lille pilen øverst til høyre og velg Settings(se Figur 3).\nDeretter velger du Password and authentification i menyen til venstre.\nUnder Two-factor authentication trykker du på Enable.\n\n\n\n\n\n\nFigur 3: Åpne settings for din GitHub-bruker.\n\n\n\n\n\nFigur 4 viser dialogboksen som vises. Velg Enable two-factor authentification.\n\n\n\n\nFigur 4: Dialogboks som åpnes når 2FA skrus på første gang.\n\n\n\nFigur 5 viser dialogboksen som vises for å velge hvordan man skal autentisere seg. Her anbefales det å velge Set up using an app, slik at du kan bruke Microsoft Authenticator-appen på din mobil.\n\n\n\n\nFigur 5: Dialogboks for å velge hvordan man skal autentisere seg med 2FA.\n\n\nFigur 6 viser QR-koden som vises. Denne skal vi bruke i neste steg.\n\n\n\nFigur 6: QR-kode som skannes av Microsoft Authenticator.\n\n\n\n\n\nStrekkoden over skal skannes i din Microsoft Authenticator-app på mobilen, som vist i Figur 7. Åpne appen, trykk på Bekreftede ID-er, og til slutt trykk på Skann QR-kode. Deretter skanner du QR-koden fra punkt 5.\nNår koden er skannet har du fått opp følgende bilde på appens hovedside (se bilde til høyre). Skriv inn den 6-siffer koden på GitHub-siden med QR-koden.\nTil slutt lagrer du Recovery-codes et trygt sted som bare du har tilgang til.\n\n\n\n\n\n\nFigur 7: Mobilappen Microsoft authenticator\n\n\n\n\nNå har vi aktivert 2-faktor autentisering for GitHub og er klare til å knytte vår personlige konto til vår SSB-bruker på SSBs “Github organisation” statisticsnorway.\n\n\n\nI forrige steg aktiverte vi 2-faktor autentisering for GitHub. Det neste vi må gjøre er å koble oss til Single Sign On (SSO) for SSB sin organisasjon på GitHub:\n\nTrykk på lenken https://github.com/orgs/statisticsnorway/sso\nI dialogboksen som dukker opp trykker du på Continue, slik som vist i Figur 8.\n\n\n\n\nFigur 8: Single Sign on (SSO) for SSB sin organisasjon på GitHub\n\n\nNår du har gjennomført dette så har du tilgang til statisticsnorway på GitHub. Går du inn på denne lenken så skal du nå kunne lese både Public, Private og Internal repoer, slik som vist i Figur 9.\n\n\n\nFigur 9: Medlemsvisning for SSB sin GitHub-organisasjon.\n\n\n\n\n\nNår vi skal jobbe med SSB-kode som ligger lagret hos statistcsnorway på GitHub, så må vi autentisere oss. Måten vi gjøre det på er ved å generere et Personal Access Token (ofte forkortet PAT) som vi oppgir når vi vil hente eller oppdatere kode på GitHub. Da sender vi med PAT for å autentisere oss for GitHub.\n\n\nFor å lage en PAT som er godkjent mot statisticsnorway så gjør man følgende:\n\nGå til din profilside på GitHub og åpne Settings slik som ble vist Seksjon 1.2.2.\nVelg Developer Settings i menyen til venstre.\nI menyen til venstre velger du Personal Access Token, og deretter Tokens (classic).\nVelg Generate new token og deretter Generate new token (classic).\nUnder Note kan du gi PAT’en et navn. Velg et navn som er intuitivt for deg. Hvis du skal bruke PAT til å jobbe mot Dapla, så ville jeg ganske enkelt kalt den dapla. Hvis du skal bruke den mot bakkemiljøet ville jeg kalt den prodsone eller noe annet som gjør det lett for det skjønne innholdet i ettertid.\nUnder Expiration velger du hvor lang tid som skal gå før PAT blir ugyldig. Dette er en avvening mellom sikkerhet og hva som er praktisk. Det anbefales at du velger 365 dager. Når PAT utløper må du gjenta stegene i dette kapittelet.\nUnder Select scopes velger du repo og workflow slik som vist i Figur 10.\n\n\n\n\nFigur 10: Gi token et kort og beskrivende navn\n\n\n\nTrykk på Generate token nederst på siden og du får noe lignende det du ser i Figur 11.\n\n\n\n\nFigur 11: Token som ble generert.\n\n\n\nKopier deretter PAT til en midlertidig fil. Grunnen er at du aldri vil se det igjen her etter at vi har gjennomført neste steg.\nDeretter trykker du på Configure SSO og velger Authorize ved siden statisticsnorway, slik som vist i Figur 12. Svar deretter på spørsmålene som dukker opp.\n\n\n\n\nFigur 12: Autorisering av Token mot SSBs GiHub-organisasjon.\n\n\nVi har nå opprettet en PAT som er godkjent for bruk mot SSB sin kode på GitHub. Det betyr at hvis vi vil jobbe med Git på SSB sine maskiner i sky eller på bakken, så må vi sendte med dette tokenet for å få lov til å jobbe med koden som ligger på statisticsnorway på GitHub.\n\n\n\nDet er ganske upraktisk å måtte sende med tokenet hver gang vi skal jobbe med GitHub. Vi bør derfor lagre det lokalt der vi jobber, slik at Git automatisk finner det. Det finnes mange måter å gjøre dette på og det er ikke bestemt hva som skal være beste-praksis i SSB. Men en måte å gjøre det er via en .netrc-fil. Vi oppretter da en fil som heter .netrc på vårt hjemmeområde, og legger følgende informasjon på en (hvilken som helst) linje i filen:\n\n\n~/.netrc\n\nmachine github.com login &lt;github-bruker&gt; password &lt;Personal Access Token&gt;\n\nGitHub-bruker er da din personlige bruker og IKKE brukernavnet ditt i SSB. Personal Access Token er det vi lagde i forrige kapittelet.\nEn veldig enkel måte å lagre dette er som følger. Anta at min personlige GitHub-bruker er SSB-Chad og at min Personal Access Token er blablabla. Da kan jeg gjøre følgende for å lagre det i .netrc:\n\nGå inn i Jupyterlab og åpne en Python-notebook.\nI den første kodecellen skriver du:\n\n\n\nterminal\n\n!echo \"machine github.com login SSB-Chad password blablabla\" &gt;&gt; ~/.netrc\n\nAlternativt kan du droppe det utropstegnet og kjøre det direkte i en terminal. Det vil gi samme resultat. Koden over legger til en linje med teksten\nmachine github.com login SSB-Chad password blablabla\ni en .netrc-fil på din hjemmeområdet, uanvhengig av om du har en fra før eller ikke. Hvis du har en fil fra før som allerede har et token fra GitHub, ville jeg nok slettet det før jeg legger en et nytt token.\nHver gang du jobber mot GitHub vil Git sjekke om informasjon om autentisering ligger i denne filen, og bruke den hvis den ligger der.\n\n\n\nI eksempelet over lagde vi en PAT som var gyldig i 90 dager. Dermed vil du ikke kunne jobbe mot GitHub med dette tokenet etter 90 dager. For å oppdatere tokenet gjør du følgende:\n\nLag et nytt PAT ved å repetere Seksjon 1.2.4.1.\nI miljøet der du skal jobbe med Git og GitHub går du inn i din .netrc og bytter ut token med det nye.\n\nOg med det er du klar til å jobbe mot statisticsnorway på GitHub."
  },
  {
    "objectID": "statistikkere/git-og-github.html#footnotes",
    "href": "statistikkere/git-og-github.html#footnotes",
    "title": "Git og Github",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPrøv selv å åpne en ipynb som json ved høreklikke på fila i Jupyterlab, velge Open with, og velg json. Da vil du se den underliggende json-filen↩︎\nBranches kan oversettes til grener på norsk. Men i denne boken velger vi å bruke det engelske ordet branches. Grunnen er at det erfaringsmessig er lettere forholde seg til det engelske ordet når man skal søke etter informasjon i annen dokumentasjon↩︎"
  },
  {
    "objectID": "statistikkere/features.html",
    "href": "statistikkere/features.html",
    "title": "Features",
    "section": "",
    "text": "En feature er en GCP-tjeneste som som er satt opp og konfigurert slik at Dapla-team kan ta det i bruk på en enkel og selvbetjent måte. Når man tar i bruk en feature kan man være sikker på at sikkerhet og beste-praksis i SSB er ivaretatt. Et viktig poeng med features er at teamene selv skal kunne skru av og på features etter behov.\nForeløpig er det tilgjengeliggjort følgende features på Dapla:\n\ndapla-buckets\ndapla-buckets er en feature som gir deg Google Cloud Storage bøttene som statistikkteam skal bruke for å lagre data i Dapla. Dvs. en bøtte for kildedata, en bøtte for produkt-data, og en bøtte for delt data.\nsource-data-automation\nsource-data-automation er en feature som gir deg tilgang til Kildomaten. Den lar deg automatisere prosessering av data fra kildedata til inndata ved hjelp av Cloud Run.\ntransfer-service\ntransfer-service er en feature som gir deg tilgang til å overføre data mellom lagringstjenester i Dapla. Den lar deg overføre data mellom bøtter, og mellom bakke- og skyplattformen i SSB. Den er bygget på GCP-tjenesten Google Transfer Service.\n\n\n\n\n\n\n\n\n\nSkru på en feature om gangen.\n\n\n\nHvis du ønsker å skru på flere features samtidig, så må du gjøre det i flere PR-er. Atlantis vil ikke klare å håndtere flere features i samme PR. Følg oppskriften under for hver feature du ønsker å skru på.\n\n\nAktivering av en feature for Dapla-teamet ditt er veldig enkelt. Endringen er så liten at vi anbefaler å gjøre endringen direkte i GitHubs grensesnitt, uten å klone repoet først. Slik går du frem:\n\nSøk opp ditt teams sitt IaC-repoet under https://github.com/statisticsnorway/.\n\n\n\n\nI repoet åpner du fila ./infra/projects.yaml. Se eksempel for teamet play-obr til høyre.\nFinn ut om du ønsker å skru på en feature i test eller prod. Hvis du ønsker å gjøre det i prod, så skal du legge til en linje under features der env: prod. Hvis du ønsker å gjøre det i test, så skal du legge til en linje under features der env: test.\nDu kan legge til linjen ved å trykke på -ikonet øverst til høyre i fila, endre teksten, og trykke på Commit changes. Velg deretter hvilket navn du ønsker på branchen, og trykk Create pull request.\nI PR-en venter du og ser at atlantis/plan1 har blitt kjørt og får en  til venstre for hver kjøring, slik som vist i Figur 1.\n\n\n\n\n./infra/projects.yaml\n\nteam_uniform_name: play-obr\n\nprojects:\n  - project_name: play-obr\n    env: test\n    features:\n      - dapla-buckets\n      - source-data-automation\n\n  - project_name: play-obr\n    env: prod\n    features:\n      - dapla-buckets\n\n\n\n\n\n\nFigur 1: Atlantis plan planlegger endringen og sjekker om den er klar til gjennomføring.\n\n\n\nHvis alt er i orden så ber du en kollega om å se over endringen og godkjenne hvis alt ser riktig ut. Når den er godkjent vil du se et bilde som ligner det du ser i Figur 2.\n\n\n\n\nFigur 2: Sjekker av PR-er i teamets IaC-repo.\n\n\n\nNår PR er godkjent av en kollega, og atlantis plan ikke viser noen feilmeldinger, så kan du effektuere endringene ved å atlantis apply i kommentarfeltet til PR-en. Kommentareren vil trigge en kjøring som effektuerer alle endringer på plattformen.\nEtter at atlantis apply er kjørt, så kan du merge inn branchen til main.\nTil slutt kan du slette branchen.\n\nNår dette er gjort så endringen effektuert på Dapla og klar til bruk. For eksempel, hvis vi hadde aktivert transfer-service i prod for projects.yaml over, så ville den etter endringen sett slik ut:\n\n\n./infra/projects.yaml\n\nteam_uniform_name: play-obr\n\nprojects:\n  - project_name: play-obr\n    env: test\n    features:\n      - dapla-buckets\n      - source-data-automation\n\n  - project_name: play-obr\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service\n\n\n\n\nFor å deaktivere en feature som ikke lenger i bruk, så følger du bare beskrivelsen for Aktivere tjeneste over, med det unntak at du nå fjerner en linje istedenfor å legge til."
  },
  {
    "objectID": "statistikkere/features.html#aktivere-feature",
    "href": "statistikkere/features.html#aktivere-feature",
    "title": "Features",
    "section": "",
    "text": "Skru på en feature om gangen.\n\n\n\nHvis du ønsker å skru på flere features samtidig, så må du gjøre det i flere PR-er. Atlantis vil ikke klare å håndtere flere features i samme PR. Følg oppskriften under for hver feature du ønsker å skru på.\n\n\nAktivering av en feature for Dapla-teamet ditt er veldig enkelt. Endringen er så liten at vi anbefaler å gjøre endringen direkte i GitHubs grensesnitt, uten å klone repoet først. Slik går du frem:\n\nSøk opp ditt teams sitt IaC-repoet under https://github.com/statisticsnorway/.\n\n\n\n\nI repoet åpner du fila ./infra/projects.yaml. Se eksempel for teamet play-obr til høyre.\nFinn ut om du ønsker å skru på en feature i test eller prod. Hvis du ønsker å gjøre det i prod, så skal du legge til en linje under features der env: prod. Hvis du ønsker å gjøre det i test, så skal du legge til en linje under features der env: test.\nDu kan legge til linjen ved å trykke på -ikonet øverst til høyre i fila, endre teksten, og trykke på Commit changes. Velg deretter hvilket navn du ønsker på branchen, og trykk Create pull request.\nI PR-en venter du og ser at atlantis/plan1 har blitt kjørt og får en  til venstre for hver kjøring, slik som vist i Figur 1.\n\n\n\n\n./infra/projects.yaml\n\nteam_uniform_name: play-obr\n\nprojects:\n  - project_name: play-obr\n    env: test\n    features:\n      - dapla-buckets\n      - source-data-automation\n\n  - project_name: play-obr\n    env: prod\n    features:\n      - dapla-buckets\n\n\n\n\n\n\nFigur 1: Atlantis plan planlegger endringen og sjekker om den er klar til gjennomføring.\n\n\n\nHvis alt er i orden så ber du en kollega om å se over endringen og godkjenne hvis alt ser riktig ut. Når den er godkjent vil du se et bilde som ligner det du ser i Figur 2.\n\n\n\n\nFigur 2: Sjekker av PR-er i teamets IaC-repo.\n\n\n\nNår PR er godkjent av en kollega, og atlantis plan ikke viser noen feilmeldinger, så kan du effektuere endringene ved å atlantis apply i kommentarfeltet til PR-en. Kommentareren vil trigge en kjøring som effektuerer alle endringer på plattformen.\nEtter at atlantis apply er kjørt, så kan du merge inn branchen til main.\nTil slutt kan du slette branchen.\n\nNår dette er gjort så endringen effektuert på Dapla og klar til bruk. For eksempel, hvis vi hadde aktivert transfer-service i prod for projects.yaml over, så ville den etter endringen sett slik ut:\n\n\n./infra/projects.yaml\n\nteam_uniform_name: play-obr\n\nprojects:\n  - project_name: play-obr\n    env: test\n    features:\n      - dapla-buckets\n      - source-data-automation\n\n  - project_name: play-obr\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service"
  },
  {
    "objectID": "statistikkere/features.html#deaktivere-en-feature",
    "href": "statistikkere/features.html#deaktivere-en-feature",
    "title": "Features",
    "section": "",
    "text": "For å deaktivere en feature som ikke lenger i bruk, så følger du bare beskrivelsen for Aktivere tjeneste over, med det unntak at du nå fjerner en linje istedenfor å legge til."
  },
  {
    "objectID": "statistikkere/features.html#footnotes",
    "href": "statistikkere/features.html#footnotes",
    "title": "Features",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nDet som skjer når atlantis plan kjøres er at det genereres en detaljert beskrivelse av hvilke endringer som må skje på plattformen for at teamets feature skal aktiveres. Derfor må eventuelle feilmeldinger fra atlantis plan fikses før man faktiske kan effektuere endringene med atlantis apply. ↩︎"
  },
  {
    "objectID": "utviklere/iac.html",
    "href": "utviklere/iac.html",
    "title": "Infrastructure-as-Code (IaC)",
    "section": "",
    "text": "IaC-repo\n\ndapla-example-iac\n│\n├── automation\n│   │\n│   └── source_data\n│       │\n│       ├── dapla-example-test\n│       │\n│       └── dapla-example-prod\n│\n├── data\n│\n├── docs\n│\n├── infra\n│   │\n│   ├── stack\n│   │\n│   └── projects\n│       │\n│       ├── dapla-example-test\n│       │   │\n│       │   ├── project.yaml\n│       │   │\n│       │   ├── ....tf\n│       │\n│       └── dapla-example-prod\n│           │\n│           ├── project.yaml\n│           │\n│           ├── ....tf\n│\n└── atlantis.yaml\n\n\n\n\nMappe for skript som blir brukt av automatiseringstjenesten.\n\n\n\nMappe for å ha metadata som teamet selv ønsker.\n\n\n\n\n\n\nViktig\n\n\n\nData mappen er ikke ment for å lagre data til bruk i statistikkproduksjon.\n\n\n\n\n\nMappe for å samle dokumentasjon.\n\n\n\nMappe for teamets infrastruktur.\n\n\nDette er en fil som teamet selv kan spesifisere prosjekter og features som er ønsket at et prosjekt skal ha. Når man lager en PR etter å lagt inn f.eks et nytt prosjekt, så trigger det en Github Action som genererer Terraform koden som lager prosjektet. project_name feltet skal ikke inneholde miljøtypen eller andre prefiks eller suffikser, disse blir automatisk lagt på.\nEksemplet under vil lage 3 prosjekter i Google Cloud som vil få følgende prosjekt navn dapla-example-p, dapla-example-t, og dapla-example-d, prosjekt ID’ene vil være det samme bare med 2 tilfeldige karakterer på slutten.\n\n\nteam_uniform_name: dapla-example\n\nprojects:\n  - project_name: dapla-example\n    env: test\n    features:\n      - dapla-buckets\n\n  - project_name: dapla-example\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service\n  \n  - project_name: dapla-example\n    env: dev\n\n\n\n\n\n\n\n\n\n\n\nName\nDescription\nInteracts with\n\n\n\n\ndapla-buckets\nOppretter “standard” Dapla-bøttene (kilde, produkt, delt) samt et kildeprosjekt.\ntransfer-service,disable-default-bucket-iam,disable-default-project-iam\n\n\ntransfer-service\nOppretter bindinger for Transfer Service service kontoer\ndapla-buckets\n\n\nsource-data-automation\nIkke helt klar ennå :)\nnone\n\n\ndisable-default-project-iam\nDeaktiverer standard IAM-bindinger for standard Dapla-prosjekter\ndapla-buckets\n\n\ndisable-default-bucket-iam\nDeaktiverer standard IAM-bindinger for standard Dapla-bøttene\ndapla-buckets\n\n\n\n\n\n\n\nHer er det en mappe for hvert prosjekt teamet har i Google Cloud. Hvis man bruker infra/projects.yaml for å opprette prosjekter så blir det automatisk opprettet en egen mappe under her som inneholder all Terraform kode for det prosjektet.\n\n\n\nHvis teamet har noen gjenbrukbare moduler som skal brukes i alle prosjekter så kan de opprettes her.\n\n\n\n\nKonfigurasjon av prosjektene Atlantis automatisk kjører atlantis plan for. Hvis et prosjekt er opprettet via infra/projects.yaml så blir det automatisk lagt til i denne filen.\n\n\nversion: 3\nparallel_plan: true\nparallel_apply: true\nprojects:\n- dir: ./infra/projects/dapla-example-prod\n  autoplan:\n    when_modified: [\"*.tf\", \"terraform.tfvars\", \"*.yaml\"]\n- dir: ./infra/projects/dapla-example-test\n  autoplan:\n    when_modified: [\"*.tf\", \"terraform.tfvars\", \"*.yaml\"]\n- dir: ./infra/projects/dapla-example-dev\n  autoplan:\n    when_modified: [\"*.tf\", \"terraform.tfvars\", \"*.yaml\"]"
  },
  {
    "objectID": "utviklere/iac.html#struktur",
    "href": "utviklere/iac.html#struktur",
    "title": "Infrastructure-as-Code (IaC)",
    "section": "",
    "text": "IaC-repo\n\ndapla-example-iac\n│\n├── automation\n│   │\n│   └── source_data\n│       │\n│       ├── dapla-example-test\n│       │\n│       └── dapla-example-prod\n│\n├── data\n│\n├── docs\n│\n├── infra\n│   │\n│   ├── stack\n│   │\n│   └── projects\n│       │\n│       ├── dapla-example-test\n│       │   │\n│       │   ├── project.yaml\n│       │   │\n│       │   ├── ....tf\n│       │\n│       └── dapla-example-prod\n│           │\n│           ├── project.yaml\n│           │\n│           ├── ....tf\n│\n└── atlantis.yaml\n\n\n\n\nMappe for skript som blir brukt av automatiseringstjenesten.\n\n\n\nMappe for å ha metadata som teamet selv ønsker.\n\n\n\n\n\n\nViktig\n\n\n\nData mappen er ikke ment for å lagre data til bruk i statistikkproduksjon.\n\n\n\n\n\nMappe for å samle dokumentasjon.\n\n\n\nMappe for teamets infrastruktur.\n\n\nDette er en fil som teamet selv kan spesifisere prosjekter og features som er ønsket at et prosjekt skal ha. Når man lager en PR etter å lagt inn f.eks et nytt prosjekt, så trigger det en Github Action som genererer Terraform koden som lager prosjektet. project_name feltet skal ikke inneholde miljøtypen eller andre prefiks eller suffikser, disse blir automatisk lagt på.\nEksemplet under vil lage 3 prosjekter i Google Cloud som vil få følgende prosjekt navn dapla-example-p, dapla-example-t, og dapla-example-d, prosjekt ID’ene vil være det samme bare med 2 tilfeldige karakterer på slutten.\n\n\nteam_uniform_name: dapla-example\n\nprojects:\n  - project_name: dapla-example\n    env: test\n    features:\n      - dapla-buckets\n\n  - project_name: dapla-example\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service\n  \n  - project_name: dapla-example\n    env: dev\n\n\n\n\n\n\n\n\n\n\n\nName\nDescription\nInteracts with\n\n\n\n\ndapla-buckets\nOppretter “standard” Dapla-bøttene (kilde, produkt, delt) samt et kildeprosjekt.\ntransfer-service,disable-default-bucket-iam,disable-default-project-iam\n\n\ntransfer-service\nOppretter bindinger for Transfer Service service kontoer\ndapla-buckets\n\n\nsource-data-automation\nIkke helt klar ennå :)\nnone\n\n\ndisable-default-project-iam\nDeaktiverer standard IAM-bindinger for standard Dapla-prosjekter\ndapla-buckets\n\n\ndisable-default-bucket-iam\nDeaktiverer standard IAM-bindinger for standard Dapla-bøttene\ndapla-buckets\n\n\n\n\n\n\n\nHer er det en mappe for hvert prosjekt teamet har i Google Cloud. Hvis man bruker infra/projects.yaml for å opprette prosjekter så blir det automatisk opprettet en egen mappe under her som inneholder all Terraform kode for det prosjektet.\n\n\n\nHvis teamet har noen gjenbrukbare moduler som skal brukes i alle prosjekter så kan de opprettes her.\n\n\n\n\nKonfigurasjon av prosjektene Atlantis automatisk kjører atlantis plan for. Hvis et prosjekt er opprettet via infra/projects.yaml så blir det automatisk lagt til i denne filen.\n\n\nversion: 3\nparallel_plan: true\nparallel_apply: true\nprojects:\n- dir: ./infra/projects/dapla-example-prod\n  autoplan:\n    when_modified: [\"*.tf\", \"terraform.tfvars\", \"*.yaml\"]\n- dir: ./infra/projects/dapla-example-test\n  autoplan:\n    when_modified: [\"*.tf\", \"terraform.tfvars\", \"*.yaml\"]\n- dir: ./infra/projects/dapla-example-dev\n  autoplan:\n    when_modified: [\"*.tf\", \"terraform.tfvars\", \"*.yaml\"]"
  },
  {
    "objectID": "utviklere/appendix.html",
    "href": "utviklere/appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "Appendix\ndksfjkldsjf"
  },
  {
    "objectID": "utviklere/tldr/ci-cd.html",
    "href": "utviklere/tldr/ci-cd.html",
    "title": "CI/CD",
    "section": "",
    "text": "CI/CD\n\nMÅ anvende GitHub Actions som CI/CD verktøy."
  },
  {
    "objectID": "utviklere/tldr/dokumentasjon.html",
    "href": "utviklere/tldr/dokumentasjon.html",
    "title": "Dokumentasjon",
    "section": "",
    "text": "Dokumentasjon"
  },
  {
    "objectID": "utviklere/tldr/overvaakning.html",
    "href": "utviklere/tldr/overvaakning.html",
    "title": "Overvåkning",
    "section": "",
    "text": "Overvåkning\n\nMÅ eksponere readiness og liveness-endepunkter fra langtlevenede tjenester.\nMÅ eksponere metrikk/admin/service-endepunkter på egne porter.\nBØR strukturere logger i JSON-format.\nBØR skrive logger til stdout."
  },
  {
    "objectID": "utviklere/tldr/tldr.html",
    "href": "utviklere/tldr/tldr.html",
    "title": "TL;DR",
    "section": "",
    "text": "Her finnes det ikke lange beskrivelser, bare akkurat det du trenger å vite som utvikler i SSB.\n\n\nHver TL;DR:\n\nMÅ begynne med et av ordene beskrevet i Begrepsforklaring.\nMÅ være på norsk.\nMÅ være en leselig setning.\nBØR være mulig å iverksette i ethver utviklers arbeid.\nKAN lenke til mer utfyllende informasjon.\n\n\n\n\nBetydningen defineres av RFC 2119, med tilhørende oversettelse til norsk.\n\nMÅ (IKKE)\nBØR (IKKE)\nKAN"
  },
  {
    "objectID": "utviklere/tldr/tldr.html#meta",
    "href": "utviklere/tldr/tldr.html#meta",
    "title": "TL;DR",
    "section": "",
    "text": "Hver TL;DR:\n\nMÅ begynne med et av ordene beskrevet i Begrepsforklaring.\nMÅ være på norsk.\nMÅ være en leselig setning.\nBØR være mulig å iverksette i ethver utviklers arbeid.\nKAN lenke til mer utfyllende informasjon."
  },
  {
    "objectID": "utviklere/tldr/tldr.html#begrepsforklaring",
    "href": "utviklere/tldr/tldr.html#begrepsforklaring",
    "title": "TL;DR",
    "section": "",
    "text": "Betydningen defineres av RFC 2119, med tilhørende oversettelse til norsk.\n\nMÅ (IKKE)\nBØR (IKKE)\nKAN"
  },
  {
    "objectID": "utviklere/tldr/kvalitet.html",
    "href": "utviklere/tldr/kvalitet.html",
    "title": "Kvalitet",
    "section": "",
    "text": "Kvalitet"
  },
  {
    "objectID": "om-dapla.html",
    "href": "om-dapla.html",
    "title": "Om Dapla",
    "section": "",
    "text": "Om Dapla\nDapla står for dataplattform, og er en skybasert løsning for statistikkproduksjon og forskning.\n\n\nHvorfor Dapla?\nSom dataplattform skal Dapla stimulerere til økt kvalitet på statistikk og forskning, samtidig som den gjør organisasjonen mer tilpasningsdyktig i møte med fremtiden.\n\n\nDen nye skybaserte dataplattformen (Dapla) skal bli viktig for å effektivisere arbeids-og produksjonsprosesser, den skal sikre effektiv lagring og gjenfinning av data og metadata, og støtte opp under deling av data på tvers av statistikkområder.\n\nKilde: Langtidsplan for SSB (2022-2024)\n\n\n\nMålet med Dapla er å tilby tjenester og verktøy som lar statistikkprodusenter og forskere produsere resultater på en sikker og effektiv måte."
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Eksempler",
    "section": "",
    "text": "Eksempler\nlsdkjflkdsjfklj"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html",
    "href": "notebooks/spark/deltalake-intro.html",
    "title": "Introduksjon til Delta Lake",
    "section": "",
    "text": "Delta Lake er et databaselag som kan legges over parquet-filer i bøtter. Det kan gi oss mye av den funksjonaliteten vi har vært vant til i relasjonelle databaser og datavarehus. I denne notebooken vises hvordan man kan ta det i bruk på Dapla."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#oppsett",
    "href": "notebooks/spark/deltalake-intro.html#oppsett",
    "title": "Introduksjon til Delta Lake",
    "section": "Oppsett",
    "text": "Oppsett\nHvis du logger deg inn på https://jupyter.dapla.ssb.no/ kan du ta i bruk Delta Lake via Spark. Men for å gjøre det må du installere delta-spark. For å installere pakken må du jobbe i et ssb-project. I tillegg må du installere delta-spark-versjon som er kompatibel med Spark-versjonen som er installert på Dapla. Gjør derfor følgende:\n\nOpprett et ssb-project med kommandoen:\nssb-project create test-delta-lake\nI terminalen skriver du følgende for å sjekke Spark-versjonen som er installert:\nspark-shell --version\nSjekk hvilken delta-spark-versjon som er kompatibel med din Spark-versjon og installer den med kommandoen1:\npoetry add delta-spark@2.3\nÅpne en ny notebook og velg kernel test-delta-lake.\n\nNå har du satt opp et virtuelt miljø med en PySpark-kernel som kjører en maskin (såkalt Pyspark local kernel), der du har installert delta-spark. Vi kan nå importere de bibliotekene vi trenger og sette igang en Spark-session.\n\n# Importerer biblioteker\nimport pyspark\nfrom delta import *\n\nDeretter initialiserer jeg en Spark-session. %%capture_output er med for å forhindre at det ikke blir printet ut sensitiv informasjon i en notebook som skal inn i Dapla-manualen.\n\n%%capture captured_output\n%run /usr/local/share/jupyter/kernels/pyspark_local/init.py"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#genererer-noe-data",
    "href": "notebooks/spark/deltalake-intro.html#genererer-noe-data",
    "title": "Introduksjon til Delta Lake",
    "section": "Genererer noe data",
    "text": "Genererer noe data\n\n# Genererer noe data med Spark\ndata = spark.range(10, 15)\ndata.show()"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#skriver-ut-en-delta-lake-table",
    "href": "notebooks/spark/deltalake-intro.html#skriver-ut-en-delta-lake-table",
    "title": "Introduksjon til Delta Lake",
    "section": "Skriver ut en Delta Lake Table",
    "text": "Skriver ut en Delta Lake Table\nLa oss skrive ut Spark DataFrame som en Delta Lake Table og bli kjent med strukturen i objektet:\n\n%%time\ndata.write.format(\"delta\").mode(\"overwrite\").save(\n    \"gs://ssb-dapla-felles-data-delt-prod/temp4\"\n)\n\nVi kan deretter printe ut hva som ble opprettet når vi skrev ut en Delta Lake Table:\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/temp4/**\")\n\n\nDelta Lake Tabellstruktur\nMappenstrukturen du ser over er typisk for en Delta Lake-tabell. La oss bryte ned komponentene:\n\n_delta_log/:\n\nDette er transaksjonsloggmappen for Delta Lake-tabellen. Den inneholder en serie med JSON-filer som registrerer alle transaksjoner gjort på tabellen.\nTransaksjonsloggen er avgjørende for Delta Lakes ACID-transaksjonsegenskaper, som muliggjør funksjoner som atomiske forpliktelser, tilbakerullinger og tid-reise-forespørsler.\n\n_delta_log/00000000000000000000.json:\n\nDette er en JSON-fil innenfor transaksjonsloggkatalogen. Hver JSON-fil i denne mappen tilsvarer en transaksjon (eller en batch av transaksjoner) gjort på tabellen.\nFilnavnet er null-fylt og representerer transaksjonsversjonen. I dette tilfellet representerer 00000000000000000000.json den aller første versjonen (versjon 0) av tabellen. Etter hvert som flere transaksjoner blir gjort, vil nye filer bli lagt til med økende versjonsnumre.\n\npart-00000-450715bd-6b0c-4827-bb8a-b0265505ca72-c000.snappy.parquet og part-00001-977ed55f-5ce5-469f-8a1d-9eafb143c215-c000.snappy.parquet:\n\nDette er de faktiske datafilene til Delta Lake-tabellen, lagret i Parquet-format.\n.snappy.parquet-utvidelsen indikerer at dataene er komprimert ved hjelp av Snappy-komprimeringsalgoritmen.\nFilnavnene er typiske for Sparks navngivningskonvensjon for datadeler. Prefiksene part-00000 og part-00001 indikerer partisjonsnumre. De lange UUID-lignende strengene er unike identifikatorer for datafilene. Suffikset c000 indikerer Spark-oppgaven som skrev filen.\n\n\nMappen representerer en Delta Lake-tabell med data lagret i Parquet-format og en transaksjonslogg som sporer endringer i tabellen. Tilstedeværelsen av _delta_log-mappen og innholdet er det som skiller en Delta Lake-tabell fra et vanlig Parquet-datasett."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#lese-inn-tabell",
    "href": "notebooks/spark/deltalake-intro.html#lese-inn-tabell",
    "title": "Introduksjon til Delta Lake",
    "section": "Lese inn tabell",
    "text": "Lese inn tabell\n\ndeltaTable = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-delt-prod/temp4\")\ndeltaTable.toDF().show()"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#modifisere-datasettet",
    "href": "notebooks/spark/deltalake-intro.html#modifisere-datasettet",
    "title": "Introduksjon til Delta Lake",
    "section": "Modifisere datasettet",
    "text": "Modifisere datasettet\nLa oss modifisere datasettet ved å bytte ut verdien 13 med 15 i id-kolonnen:\n\nfrom pyspark.sql.functions import col, lit\n\n# Update the cell with value 13 to 15\ndeltaTable.update(condition=(col(\"id\") == 13), set={\"id\": lit(15)})\n\nEt viktig poeng å få med seg her er at vi nå oppdaterte Delta Lake Table objektet både i minnet og på disk. La oss bevise det ved å lese inn fra disk:\n\ndeltaTable2 = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-delt-prod/temp4\")\ndeltaTable2.toDF().show()\n\nOg deretter ved å printe ut det opprinnelige objektet vi leste inn:\n\ndeltaTable.toDF().show()"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#append-data",
    "href": "notebooks/spark/deltalake-intro.html#append-data",
    "title": "Introduksjon til Delta Lake",
    "section": "Append data",
    "text": "Append data\nLa oss legge til verdiene 20 og 21 til datasettet. Først lager vi en Spark dataframe:\n\nnew_data = [(20,), (21,)]\nnew_df = spark.createDataFrame(new_data, [\"id\"])\nnew_df.show()\n\nDeretter kan vi appendere det til vår opprinnelige dataframe:\n\nnew_df.write.format(\"delta\").mode(\"append\").save(\n    \"gs://ssb-dapla-felles-data-delt-prod/temp4\"\n)\n\n\ndeltaTable.toDF().show()"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#historien-og-metadata-til-filen",
    "href": "notebooks/spark/deltalake-intro.html#historien-og-metadata-til-filen",
    "title": "Introduksjon til Delta Lake",
    "section": "Historien og metadata til filen",
    "text": "Historien og metadata til filen\nNå som vi har gjort noen endringer kan vi se på historien til filen:\n\n# Lister ut filene i bøtta\nfs = FileClient.get_gcs_file_system()\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/temp4/**\")\n\nVi ser av transaksjonsloggen i _delta_log-mappen at det nå har vært 3 transaksjoner på datasettet. vi ser også av navnene på parquet-filene, part-00000 og part-00001, at det finnes to versjoner av filen. Hvis vi ønsker å bli bedre kjent med hva slags informasjon som blir lagret fra transaksjonene, så kan vi printe ut den siste transaksjonen som heter 00000000000000000002.json:\n\nimport json\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\n# Filsti\npath = \"gs://ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000002.json\"\n\nwith fs.open(path, \"r\") as f:\n    for line in f:\n        data = json.loads(line)\n        pretty_content = json.dumps(data, indent=4)\n        print(pretty_content)\n        print(\"-\" * 50)  # Print separator for clarity\n\nHer ser vi at vi får masse informasjon om endringen, både metadata om transaksjonen i commitInfo, og informasjon om nye data-filer i add-delen. Vi ser at det er en veldig rik beskrivelse av endringer, men det kan være vanskeig å lese innholdet direkte. La oss heller bruke history()-funksjonen som kommer med delta:\n\nhistory = deltaTable.history()\nhistory.show()\n\nSiden det blit trangt i tabellen over så kan vi velge hvilke variabler vi ønsker å se på:\n\n# Oversikt over alle kolonner som finnes i historien\nhistory.columns\n\n\n# Velger de kolonnene jeg er interessert i\nselected_history = deltaTable.history().select(\n    \"version\", \"timestamp\", \"operation\", \"operationParameters\"\n)\n\n\n# Display the selected columns\nselected_history.show(truncate=50)"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#egendefinert-metadata",
    "href": "notebooks/spark/deltalake-intro.html#egendefinert-metadata",
    "title": "Introduksjon til Delta Lake",
    "section": "Egendefinert metadata",
    "text": "Egendefinert metadata\nDelta Lake støtter også egendefinert metadata. Det kan for eksempel være nyttig hvis man ønsker å bruke Delta Lake som en backend for et GUI som lar brukeren oppdatere en tabell fra GUI-et. Da ønsker man typisk å lagre hvem som gjorde endringer og når det ble gjort. La oss legge på noe slik metadata:\n\n# Leser inn filen\ndf = spark.read.format(\"delta\").load(\"gs://ssb-dapla-felles-data-delt-prod/temp4\")\n\n\n\nCode\n# Lagrer egendefinert metadata i en json-fil\nimport json\n\nmetadata = {\n    \"comment\": \"Kontaktet oppgavegiver og kranglet!\",\n    \"manueltEditert\": \"True\",\n    \"maskineltEditert\": \"False\",\n}\nmetadata\n\n\nVi kan deretter appende dette til den siste versjonen av fila.\n\n(\n    df.write.format(\"delta\")\n    .mode(\"append\")\n    .option(\"userMetadata\", json.dumps(metadata))  # Serialize metadata to a string\n    .save(\"gs://ssb-dapla-felles-data-delt-prod/temp4\")\n)\n\n\n# Laster inn tabellen\ndeltaTable = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-delt-prod/temp4\")\n\n# Henter ut historien\nhistory_df = deltaTable.history()\n\n\n# Show the operation details, including metadata\nhistory_df.select(\n    \"version\", \"timestamp\", \"operation\", \"operationParameters\", \"userMetadata\"\n).show(truncate=10)\n\n\nhistory_df.select(\"version\", \"userMetadata\").show(truncate=50)\n\nVi ser at vi la til vår egen metadata i versjon 3 av fila. Vi kan printe ut den rå transaksjonsloggen som tidligere, men nå er vi på transaksjon 3 00000000000000000003.json:\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\n# Filsti\npath = \"gs://ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000003.json\"\n\nwith fs.open(path, \"r\") as f:\n    for line in f:\n        data = json.loads(line)\n        pretty_content = json.dumps(data, indent=4)\n        print(pretty_content)\n        print(\"-\" * 50)  # Print separator for clarity\n\nVi er da at metadataene ligger som forventet i metadata-delen."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#footnotes",
    "href": "notebooks/spark/deltalake-intro.html#footnotes",
    "title": "Introduksjon til Delta Lake",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI eksempelet er det Spark V3.3.1 som er installert og jeg installerer derfor delta-spark v2.3↩︎"
  }
]