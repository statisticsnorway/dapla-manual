[
  {
    "objectID": "statistikkere/filinnsamling-moveit.html",
    "href": "statistikkere/filinnsamling-moveit.html",
    "title": "Filinnsamling via MoveIT",
    "section": "",
    "text": "Her finner du fremgangsmÃ¥ten beskrevet for hvordan ditt team kan fÃ¥ pÃ¥ plass automatisert overfÃ¸ring av filer samlet inn via Moveit pÃ¥ bakken til teamets kildedatabÃ¸tte pÃ¥ Dapla.\nFremgangsmÃ¥ten beskrevet her innebÃ¦rer en begrensning pÃ¥ at overflytting av filer til Dapla ikke kan utfÃ¸res oftere enn en gang i timen. For de aller fleste er ikke dette en utfordring. Dersom dette er en utfordring for ditt team, ta kontakt pÃ¥ arkitektur@ssb.no.\nTransfer Service er en tjeneste som brukes til Ã¥ flytte filer mellom bakke og sky. NÃ¥r du skal ta i bruk tjenesten for Ã¥ overfÃ¸re data fra bakken til en kildedata-bÃ¸tte i Dapla-teamet ditt, sÃ¥ fÃ¸lger du denne beskrivelsen pÃ¥ hvordan man setter opp overfÃ¸ringsjobber. FÃ¸r du kan kan ta i bruk Transfer Service, mÃ¥ filene samlet inn via Moveit fÃ¸rst flyttes til filomrÃ¥det pÃ¥ â€œkildeâ€ som Transfer Service benytter (/ssb/cloud_sync/â€daplateamnavnâ€/tilsky).\nDette mÃ¥ settes opp av s782, og henvendelsen mÃ¥ gÃ¥ via Kundeservice. Henvendelsen mÃ¥ inneholde:\nNÃ¥r flyttingen av filene fra Moveit til Cloud_sync-omrÃ¥det er etablert, sÃ¥ mÃ¥ du sette opp tjenesten for Ã¥ overfÃ¸re filene til en kildedata-bÃ¸tte i Dapla-teamet ditt. Da fÃ¸lger du denne beskrivelsen pÃ¥ hvordan man setter opp slike overfÃ¸ringsjobber.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Filinnsamling via MoveIT"
    ]
  },
  {
    "objectID": "statistikkere/filinnsamling-moveit.html#sette-opp-en-helt-ny-innsamling-i-moveit",
    "href": "statistikkere/filinnsamling-moveit.html#sette-opp-en-helt-ny-innsamling-i-moveit",
    "title": "Filinnsamling via MoveIT",
    "section": "Sette opp en helt ny innsamling i Moveit",
    "text": "Sette opp en helt ny innsamling i Moveit\nDersom det er en ny innsamling i Moveit, sÃ¥ mÃ¥ det ogsÃ¥ opprettes en filsluse-konto for den eksterne leverandÃ¸ren av datafilene. Dette mÃ¥ settes opp av s782, og henvendelsen mÃ¥ gÃ¥ via Kundeservice (Kundeservice@ssb.no) med fÃ¸lgende informasjon:\n\nEn kort beskrivelse av formÃ¥let med innsamlingen og hva slags data vil det gjelde for.\n\n\nPersonlig informasjon pÃ¥ ekstern leverandÃ¸r:\n\nFullt navn\nE-postadresse\nTelefonnummer\nFirmanavn\nInformasjon om datafilene\n\n\n\nInformasjon for automatisk overfÃ¸ring av datafilene via MoveIT Automation:\n\nDaplateamets navn (teamet som datafilene skal overfÃ¸res til pÃ¥ Dapla)\nÃ˜nsket frekvens/evt tidspunkt for overfÃ¸ringen\nEventuelle spesifikke krav eller preferanser for overfÃ¸ringen\n\nNÃ¥r bÃ¥de kontoen til den eksterne leverandÃ¸ren er pÃ¥ plass, og flyttingen av filene fra Moveit til Cloud_sync-omrÃ¥det er etablert, sÃ¥ mÃ¥ du sette opp tjenesten for Ã¥ overfÃ¸re filene til en kildedata-bÃ¸tte i Dapla-teamet ditt. Da fÃ¸lger du denne beskrivelsen pÃ¥ hvordan man setter opp slike overfÃ¸ringsjobber.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Filinnsamling via MoveIT"
    ]
  },
  {
    "objectID": "statistikkere/altinn-sfu.html",
    "href": "statistikkere/altinn-sfu.html",
    "title": "System for utvalgsadministrasjon (SFU)",
    "section": "",
    "text": "Denne kapitlet beskriver hvordan man kan bruke dapla-suv-tools pakken for Ã¥ hente utvalg og enhetsinformasjon fra SFU i Dapla-miljÃ¸et.\nDet finnes fortsatt avhengigheter til bakkesystemene for Ã¥ kunne kjÃ¸re et fullstendig produksjonslÃ¸p pÃ¥ Dapla. SU-V har laget integrasjoner mellom bakke- og skylÃ¸sninger der det er behov for dette. Administrasjon av utvalg og enheter skjer fortsatt fra SFU pÃ¥ bakke.",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "SFU"
    ]
  },
  {
    "objectID": "statistikkere/altinn-sfu.html#forberedelser",
    "href": "statistikkere/altinn-sfu.html#forberedelser",
    "title": "System for utvalgsadministrasjon (SFU)",
    "section": "Forberedelser",
    "text": "Forberedelser\nFor Ã¥ kunne hente enhetsinformasjon fra SFU mÃ¥ brukeren din ha tilgang til delregisteret i riktig miljÃ¸ (DB1T/DB1P). Ta kontakt med Kundeservice dersom du opplever problemer med tilganger.\nI tillegg mÃ¥ dapla-suv-tools vÃ¦re installert i en tjeneste pÃ¥ Dapla Lab:\n\n\nterminal\n\npoetry add dapla-suv-tools",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "SFU"
    ]
  },
  {
    "objectID": "statistikkere/altinn-sfu.html#funksjonalitet",
    "href": "statistikkere/altinn-sfu.html#funksjonalitet",
    "title": "System for utvalgsadministrasjon (SFU)",
    "section": "Funksjonalitet",
    "text": "Funksjonalitet\nUnder finner du eksempler pÃ¥ funksjonalitet som tilbys i dapla-suv-tools for Ã¥ jobbe med SFU fra Dapla Lab.\n\nHente utvalg fra SFU\nFor Ã¥ hente utvalg fra SFU, bruk metoden get_utvalg_from_sfu i SuvClient. SÃ¸rg for at du oppgir riktig delregisternummer og RA-nummer for utvalget. Dersom utvalget er delt inn i puljer kan du oppgi pulje som parameter.\n\n\nnotebook\n\nclient = SuvClient()\n\nresponse = client.get_utvalg_from_sfu(\n    delreg_nr=49430224,\n    ra_nummer='RA-0666A3',\n    pulje='2'\n)\n\nprint(json.dumps(response, indent=4))\n\n\n\nHente enhetsinformasjon fra SFU\nFor Ã¥ hente enhetsinformajon fra SFU, bruk metoden get_enhet_from_sfu i SuvClient. SÃ¸rg for at du oppgir riktig delregisternummer og organisajonsnummer.\n\n\nnotebook\n\nclient = SuvClient()\n\nresponse = client.get_enhet_from_sfu(\n    delreg_nr=49430224,\n    orgnr='123456789' \n)\n\nprint(json.dumps(response, indent=4))\n\n\n\n\n\n\n\nVis output\n\n\n\n\n\n{\n    \"delreg_nr\": 49430224,\n    \"ident_nr\": \"A3TF0019\",\n    \"orgnr\": \"123456789\",\n    \"enhets_type\": \"FRTK\",\n    \"foretak\": \"A3TF0019\",\n    \"orgnr_foretak\": \"123456789\",\n    \"flv\": \"0\",\n    \"navn1\": \"MITT REGNSKAP\",\n    \"navn2\": null,\n    \"navn3\": null,\n    \"f_adresse1\": \"Testvegen 19\",\n    \"f_adresse2\": null,\n    \"f_adresse3\": null,\n    \"f_postnr\": \"0019\",\n    \"f_poststed\": \"OSLO\",\n    \"maalform\": null,\n    \"kontaktperson\": \"OLA NORDMANN\",\n    \"kont_telefon\": \"12121212\",\n    \"kont_mobiltlf\": null,\n    \"kont_epost\": null,\n    \"h_var1_n\": null,\n    \"h_var2_n\": null,\n    \"h_var3_n\": null,\n    \"h_var1_a\": null,\n    \"h_var2_a\": null,\n    \"h_var3_a\": null,\n    \"utvalgsstatus\": null,\n    \"pulje_nr\": 2,\n    \"vedtak_tvmulkt\": \"N\",\n    \"sendt_si\": \"N\",\n    \"status\": null,\n    \"org_form\": null,\n    \"sn07_1\": \"17.120\",\n    \"str_kode\": null,\n    \"viktig_enhet\": null,\n    \"kommentar_int\": null,\n    \"kommentar_ekst\": null,\n    \"test_pulje\": null,\n    \"prosedyre\": null,\n    \"felles_oppgave\": null\n}",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "SFU"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html",
    "href": "statistikkere/jupyter.html",
    "title": "Jupyter",
    "section": "",
    "text": "Jupyter er en tjeneste pÃ¥ Dapla Lab som lar brukerne kode i Jupyterlab. Tjenesten kommer med R og Python og noen vanlige Jupyterlab-extensions ferdig installert. MÃ¥lgruppen for tjenesten er brukere som skal skrive produksjonskode i Jupyterlab.\nSiden tjenesten er ment for produksjonskode sÃ¥ er det veldig fÃ¥ R- og Python-pakker som er forhÃ¥ndsinstallert. Antagelsen er at brukeren/teamet heller bÃ¸r installere de pakkene de selv trenger, framfor at det ligger ferdiginstallerte pakker som skal dekke behovet til alle brukere/team i SSB. Det reduserer kompleksitet i tjenesten og dermed sannsynligheten for feilsituasjoner.\nFor uerfarne brukere finnes det en egen tjeneste som heter Jupyter-playground. Her er mange av de vanlige R- og Python-pakkene installert og det er opprettet en ferdig kernel som lar brukerne komme i gang fort med koding. Denne tjenesten er ikke tenkt for bruk i produksjonskode.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#forberedelser",
    "href": "statistikkere/jupyter.html#forberedelser",
    "title": "Jupyter",
    "section": "Forberedelser",
    "text": "Forberedelser\nFÃ¸r man starter Jupyter-tjenesten bÃ¸r man ha lest kapitlet om Dapla Lab og satt opp Git- og GitHub-konfigurasjonen under Min konto. Deretter gjÃ¸r du fÃ¸lgende:\n\nLogg deg inn pÃ¥ Dapla Lab\nUnder Tjenestekatalog trykker du pÃ¥ Start-knappen for Jupyter\nGi tjenesten et navn\nÃ…pne Jupyter konfigurasjoner",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#konfigurasjon",
    "href": "statistikkere/jupyter.html#konfigurasjon",
    "title": "Jupyter",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nFÃ¸r man Ã¥pner en tjeneste kan man konfigurere hvor mye ressurser man Ã¸nsker, hvilket team man skal representere, om et GitHub-repo skal klones ved oppstart, og mange andre ting. Valgene man gjÃ¸r kan ogsÃ¥ lagres slik at man Ã¥ slipper Ã¥ gjÃ¸re samme jobb senere. FigurÂ 1 viser Tjeneste-delen i konfigurasjonen for Jupyter hvor man kan velge hvilken versjon av Jupyter man vil bruke.\n\n\n\n\n\n\nFigurÂ 1: Jupyter-versjon i Dapla Lab\n\n\n\n\nData\nUnder Data-menyen kan man velge hvilket team og tilgangsgruppe man skal representere, som igjen bestemmer hvilke data man fÃ¥r tilgang til. Man gjÃ¸r dette ved Ã¥ velge navnet pÃ¥ tilgangsgruppen, og denne er alltid pÃ¥ formen &lt;teamnavn&gt;-&lt;tilgangsgruppe&gt;. FigurÂ 2 viser at brukeren har valgt tilgangsgruppen dapla-felles-developers, dvs. at de representerer tilgangsgruppen developers for teamet dapla-felles.\n\n\n\n\n\n\nFigurÂ 2: Detaljert tjenestekonfigurasjon for bÃ¸ttetilgang i Dapla Lab\n\n\n\nUnder Team og tilgangsgruppe kan brukeren ogsÃ¥ velge Ã¥ representere tilgangsgruppen data-admins for et team. I de tilfellene er det et krav om brukeren oppgir en skriftlig begrunnelse for hvorfor tilgangen er nÃ¸dvendig. I tillegg mÃ¥ kan de maksimalt aktivere tilgangen i 8 timer.\nFigurÂ 3 viser en bruker som aktiverer sin data-admins tilgang for team dapla-felles. Hvis brukeren ikke oppgir en begrunnelse vil de fÃ¥ en feilmelding ved oppstart av tjenesten.\n\n\n\n\n\n\nFigurÂ 3: Aktivere tilgang til kildedata for data-admins.\n\n\n\nNÃ¥r man Ã¥pner tjeneste, og representerer et team, sÃ¥ tilgjengeliggjÃ¸res det teamets bÃ¸tter inne i tjenesten under filstien /buckets/. Men et team kan ogsÃ¥ ha tilgang til andre sine delt-bÃ¸tter og Ã¸nske Ã¥ tilgjengliggjÃ¸re disse ogsÃ¥. FigurÂ 4 viser hvordan man spesifiserer hvilke delt-bÃ¸tter man Ã¸nsker Ã¥ tilgjengeliggjÃ¸re inne i tjenesten. Man gjÃ¸r det ved Ã¥ spesifisere det tekniske teamnavnet til teamet som eier dataene, og spesifiserer kortnavnet for delt-bÃ¸tta1.\n\n\n\n\n\n\nFigurÂ 4: Konfigurer hvilke delt-bÃ¸tter fra andre som skal tilgjengeliggjÃ¸res i tjenesten.\n\n\n\n\n\n\n\n\n\nHvor finner jeg filene inne i tjenesten?\n\n\n\n\n\nBÃ¸tter som tilgjengeliggjÃ¸res inne tjenesten finner du alltid under filstien /buckets/ i tjenesten. Under er et eksempel som vil vÃ¦re vanlig for et statistikkteam:\n\n\nFilsystem\n\n/buckets/\nâ”œâ”€â”€ produkt/\nâ”‚   â”œâ”€â”€ inndata/\nâ”‚   â””â”€â”€ klargjorte data/\nâ”œâ”€â”€ frasky/\nâ”œâ”€â”€ tilsky/\nâ”œâ”€â”€ delt-ledstill/\nâ”œâ”€â”€ delt-freg/\nâ””â”€â”€ shared/\n    â”œâ”€â”€ arbmark-register/\n    â”‚   â””â”€â”€ ameld/\n    â””â”€â”€ vof/\n        â””â”€â”€ rollebasen/\n\nBÃ¸ttene som eies av teamet (produkt, frasky, tilsky, delt-ledstill og delt-freg) tilgjengeliggjÃ¸res rett under filstien /buckets/, mens andre team sine delt-bÃ¸tter tilgjengeliggjÃ¸res under /buckets/shared/&lt;teamnavn&gt;/&lt;kortnavn for bÃ¸tte&gt;. Teamets egne delt-bÃ¸tter fÃ¥r et delt-prefiks slik at en delt-bÃ¸tte med kortnavn ledstill blir tilgjengeliggjort som delt-ledstill.\n\n\n\nMan kan ogsÃ¥ velge Ã¥ jobbe direkte mot bÃ¸ttene, og da trenger man ikke Ã¥ tilgjengeliggjÃ¸re bÃ¸ttene i filsystemet. Under tjenestekonfigurasjonen Avansert kan man skru av tilgjengeliggjÃ¸ringen av bÃ¸tter i filsystemet.\n\n\nGit/GitHub\nUnder menyen Git/GitHub kan man konfigurere Git og GitHub slik at det blir lettere Ã¥ jobbe med inne i tjenesten. Som standard arves informasjonen som er lagret under Min konto-Git i Dapla Lab. Informasjonen under tjenestekonfigurasjonen blir tilgjengeliggjort som miljÃ¸variabler i tjenesten. Informasjonen blir ogsÃ¥ lagt i $HOME/.netrc slik at man kan benytte ikke trenger Ã¥ gjÃ¸re noe mer for Ã¥ jobbe mot GitHub fra tjenesten.\n\n\n\n\n\n\nFigurÂ 5: Konfigurasjon av Git og GitHub for Jupyter-tjenesten i Dapla Lab\n\n\n\nFigurÂ 5 viser at brukeren som standard fÃ¥r aktivert Aktiver Git. Dette innebÃ¦rer at Git-brukernavn, Git e-post og GitHub-token arves fra brukerkonfigurasjonen. I tillegg sÃ¥ opprettes SSBs standard Git-konfigurasjon i ~/.gitconfig.\nMan kan ogsÃ¥ velge at ssb-project build skal kjÃ¸res pÃ¥ repoet under oppstart av tjenesten. Det fÃ¸rer til litt lengre oppstartstid, men er alle pakker installert og en kernel opprettet nÃ¥r tjenesten er klar.\n\n\nPython/R\nUnder menyen Python/R kan man velge hvilke versjon av R og Python man Ã¸nsker Ã¥ kjÃ¸re. Man kan velge mellom alle tidligere tilbudte kombinasjoner av R og Python.\nI FigurÂ 6 ser vi av navnet r4.4.0-py311-v55-2024.10.31 at tjenesten som default vil startes versjon 4.4.0 av R og 3.11 for Python. Etterhvert som nye versjoner av R og Python kommer kan disse tilgjengeliggjÃ¸res i tjenesten, men brukeren kan velge Ã¥ starte en eldre versjon av tjenesten.\n\n\n\n\n\n\nFigurÂ 6: Konfigurasjon av Git og GitHub for Jupyter-tjenesten i Dapla Lab\n\n\n\n\n\nRessurser\nUnder menyen Resources kan man velge hvor mye CPU og RAM man Ã¸nsker i tjenesten, slik som vist i FigurÂ 7. Velg sÃ¥ lite som trengs for Ã¥ gjÃ¸re jobben du skal gjÃ¸re.\n\n\n\n\n\n\nFigurÂ 7: Konfigurasjon av ressurser for Jupyter-tjenesten i Dapla Lab\n\n\n\n\n\nDiskplass\nSom default fÃ¥r alle som starter en instans av Jupyter-tjenesten en lokal disk pÃ¥ 10GB inne i tjenesten. Under Diskplass-menyen kan man velge Ã¥ Ã¸ke stÃ¸rrelsen pÃ¥ disken eller ikke noe disk i det hele tatt. Siden lokal disk i tjenesten hovedsakelig skal benyttes til Ã¥ lagre en lokal kopi av koden som lagres pÃ¥ GitHub mens man gjÃ¸r endringer bÃ¸r ikke stÃ¸rrelsen pÃ¥ disken vÃ¦re stor. FigurÂ 8 viser valgene som kan gjÃ¸res under Diskplass-fanen.\n\n\n\n\n\n\nFigurÂ 8: Konfigurasjon av lokal disk for Jupyter-tjenesten i Dapla Lab\n\n\n\n\n\nAvansert\nUnder Avansert kan man velge Ã¥ ikke tilgjengeliggjÃ¸re bÃ¸tter som filsystem inne i tjenesten. Konsekvensen av dette er at man mÃ¥ lese og skrive filer ved Ã¥ referere til bÃ¸ttene direkte.\n\n\n\n\n\n\nFigurÂ 9: Avansert konfigurasjon for Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#datatilgang",
    "href": "statistikkere/jupyter.html#datatilgang",
    "title": "Jupyter",
    "section": "Datatilgang",
    "text": "Datatilgang\nSlik kan man inspisere dataene fra en terminal inne i tjenesten:\n\nÃ…pne en instans av Jupyter med data fra bÃ¸tter\nÃ…pne en terminal inne i Jupyter\nGÃ¥ til mappen med bÃ¸ttene ved Ã¥ kjÃ¸re dette fra terminalen cd /buckets\nKjÃ¸r ls -ahl i teminalen for Ã¥ se pÃ¥ hvilke bÃ¸tter som er montert.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#installere-pakker",
    "href": "statistikkere/jupyter.html#installere-pakker",
    "title": "Jupyter",
    "section": "Installere pakker",
    "text": "Installere pakker\nSiden det nesten ikke er installert noen pakker i tjenesten sÃ¥ kan brukeren opprette et ssb-project og installere pakker som vanlig.\nFor Ã¥ bygge et eksisterende ssb-project sÃ¥ kan brukeren ogsÃ¥ bruke ssb-project.\nFor Ã¥ installere R-pakker fÃ¸lger man beskrivelsen for renv.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#slette-tjenesten",
    "href": "statistikkere/jupyter.html#slette-tjenesten",
    "title": "Jupyter",
    "section": "Slette tjenesten",
    "text": "Slette tjenesten\nFor Ã¥ slette tjenesten kan man trykke pÃ¥ Slette-knappen i Dapla Lab under Mine tjenester. NÃ¥r man sletter en tjeneste sÃ¥ sletter man hele disken inne i tjenesten og frigjÃ¸r alle ressurser som er reservert. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#pause-tjenesten",
    "href": "statistikkere/jupyter.html#pause-tjenesten",
    "title": "Jupyter",
    "section": "Pause tjenesten",
    "text": "Pause tjenesten\nMan kan pause tjenesten ved Ã¥ trykke pÃ¥ Pause-knappen i Dapla Lab under Mine tjenester. NÃ¥r man pauser sÃ¥ slettes alt pÃ¥den lokale disken som ikke er lagret under $HOME/work. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#monitorering",
    "href": "statistikkere/jupyter.html#monitorering",
    "title": "Jupyter",
    "section": "Monitorering",
    "text": "Monitorering\n\n\nMan kan moniterere en instans av Jupyter ved Ã¥ trykke pÃ¥ Jupyter-teksten under Mine tjenester i Dapla Lab, slik som vist i FigurÂ 10.\nDenne funksjonaliteten er under arbeid og mer informasjon kommer snart.\n\n\n\n\n\n\n\n\n\nFigurÂ 10: Monitorering av Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#footnotes",
    "href": "statistikkere/jupyter.html#footnotes",
    "title": "Jupyter",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nKortnavnet til en delt-bÃ¸tte kan leses ut av Dapla Ctrl eller hentes fra selve bÃ¸ttenavnet. F.eks. er det kortnavnet til delt-bÃ¸tta ssb-vof-data-delt-rollebase-fnr-prod bare rollebase-fnr. Det tekniske teamnavnet er vof.â†©ï¸",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/altinn-oversikt.html",
    "href": "statistikkere/altinn-oversikt.html",
    "title": "Oversikt",
    "section": "",
    "text": "Frem mot sommeren 2026 skal alle skjema-undersÃ¸kelser i SSB som gjennomfÃ¸res pÃ¥ Altinn 2 flyttes over til Altinn 3. Skjemaer som flyttes til Altinn 3 vil motta sine data pÃ¥ Dapla, og ikke pÃ¥ bakken som tidligere. Datafangsten hÃ¥ndteres av Team SU-V, mens statistikkseksjonene henter sine data fra Team SU-V sitt lagringsomrÃ¥de pÃ¥ Dapla. I dette kapitlet beskriver vi nÃ¦rmere hvordan statistikkseksjonene kan jobbe med Altinn3-data pÃ¥ Dapla. Kort oppsummert bestÃ¥r det av disse stegene:\nI resten av kapitlet gis en oversikt over hvordan statistikkteam kan jobbe med data fra Altinn 3.",
    "crumbs": [
      "Manual",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn-oversikt.html#ansvarsfordeling",
    "href": "statistikkere/altinn-oversikt.html#ansvarsfordeling",
    "title": "Oversikt",
    "section": "Ansvarsfordeling",
    "text": "Ansvarsfordeling\nTeam SU-V har ansvaret for datafangst fra Altinn3 til SSB. Deretter tilgjengeliggjÃ¸r de dette for statistikkteamet som skal jobbe videre med dataene for Ã¥ produsere statistikk.\nDet er statistikteamet som lagrer dataene som sin kildedata som er ansvarlig for dataene og at disse hÃ¥ndteres pÃ¥ riktig mÃ¥te. Av den grunn er det statistikteamet som setter opp jobben for Ã¥ synkronisere data fra bÃ¸tta til Team SU-V til sin kildebÃ¸tte, slik at de kan organisere dataene som de Ã¸nsker. I tillegg vil backup av data bli hÃ¥ndtert i kildebÃ¸tta til statistikkteamet.",
    "crumbs": [
      "Manual",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn-oversikt.html#tilgangsstyring",
    "href": "statistikkere/altinn-oversikt.html#tilgangsstyring",
    "title": "Oversikt",
    "section": "Tilgangsstyring",
    "text": "Tilgangsstyring\nSkjemaer fra Altinn 3 hentes inn til SSB av team SU-V og lagres i delt-bÃ¸tter som SU-V administrerer. SU-V kan deretter gi tilgang til statistikkteam slik at de hente data fra bÃ¸ttene og lagre det i sine kildebÃ¸tter. Siden tilgangsstyring mellom prod- og test-miljÃ¸et fungerer ulikt sÃ¥ forklarer vi hver av de under.\n\nProd\nTilgang til bÃ¸tter med skjemadata i prod-miljÃ¸et gis kun til Transfer Service for kildeprosjektet til statistikkteamet, og aldri direkte til brukere. Dvs. at team SU-V gir tilgang til Transfer Service til statistikkteamet, og deretter kan statistikkteamet sette opp en automatisk jobb med Transfer Service som synkroniserer data fra SU-V sin delt-bÃ¸tte og over til statistikkteamets kildebÃ¸tte.\nSiden tilgangsstyringen til data i prod er ganske restriktiv, sÃ¥ anbefaler vi at statistikere gjÃ¸r seg kjent med sine skjemadata i test-miljÃ¸et (se under) fÃ¸r de starter arbeidet i prod.\n\n\nTest\nTilgang til bÃ¸tter med skjemadata i test-miljÃ¸et er mindre restriktive enn for prod-miljÃ¸et, siden disse dataene er fiktive. Derfor anbefales det at statistikere fÃ¸rst jobber i test-miljÃ¸et nÃ¥r de skal gjÃ¸re seg kjent med dataene fra Altinn 3. I test-miljÃ¸et kan bÃ¥de brukere og Transfer Service fÃ¥ tilgang til SU-V sine delt-data.",
    "crumbs": [
      "Manual",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn-oversikt.html#data",
    "href": "statistikkere/altinn-oversikt.html#data",
    "title": "Oversikt",
    "section": "Data",
    "text": "Data\nDataene som hentes inn av Team SU-V lagres som xml-filer i en bÃ¸tte. Statistikkteamet som skal hente inn dataene synkroniseres deretter dataene til sin delt-bÃ¸tte med Transfer Service. Det finnes bÃ¥de data og metadata om hvert skjema som sendes inn og under forklares innholdet i hver av de.\n\nSkjemadata\nHvert skjema som leveres inn av en oppgavegiver blir lagret som en separat xml-fil med et unikt filnavn2. Under ser du et eksempel pÃ¥ hvordan et skjema kan se ut.\n\n\n\n\n\n\nEksempel pÃ¥ xml-fil\n\n\n\n\n\n&lt;?xml version=\"1.0\" ?&gt;\n&lt;melding xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" dataFormatProvider=\"SERES\" dataFormatId=\"7079\" dataFormatVersion=\"47315\"&gt;\n  &lt;InternInfo&gt;\n    &lt;raNummer&gt;RA-XXXX&lt;/raNummer&gt;\n    &lt;skjemaVersjon&gt;1.0&lt;/skjemaVersjon&gt;\n    &lt;undersoekelsesNr&gt;XXXXX&lt;/undersoekelsesNr&gt;\n    &lt;visOppgaveByrde&gt;0&lt;/visOppgaveByrde&gt;\n    &lt;visBrukeropplevelse&gt;0&lt;/visBrukeropplevelse&gt;\n    &lt;delregNr&gt;2XXXXX&lt;/delregNr&gt;\n    &lt;periodeFritekst&gt;3. kvartal 2022&lt;/periodeFritekst&gt;\n    &lt;periodeFomDato&gt;2022-07-01&lt;/periodeFomDato&gt;\n    &lt;periodeTomDato&gt;2022-09-30&lt;/periodeTomDato&gt;\n    &lt;periodeType&gt;KVRT&lt;/periodeType&gt;\n    &lt;periodeNummer&gt;3&lt;/periodeNummer&gt;\n    &lt;periodeAAr&gt;2022&lt;/periodeAAr&gt;\n    &lt;periodeDato&gt; &lt;/periodeDato&gt;\n    &lt;enhetsIdent&gt;XXXXXX&lt;/enhetsIdent&gt;\n    &lt;enhetsType&gt;BEDR&lt;/enhetsType&gt;\n    &lt;enhetsOrgNr&gt;XXXXXXXXX&lt;/enhetsOrgNr&gt;\n    &lt;enhetsNavn&gt;STATISTISK SENTRALBYRÃ… &lt;/enhetsNavn&gt;\n    &lt;enhetsGateadresse&gt;Akersveien 26&lt;/enhetsGateadresse&gt;\n    &lt;enhetsPostnr&gt;0177&lt;/enhetsPostnr&gt;\n    &lt;enhetsPoststed&gt;OSLO&lt;/enhetsPoststed&gt;\n    &lt;enhetsAvdeling&gt;AVD XXXX&lt;/enhetsAvdeling&gt;\n    &lt;reporteeOrgNr&gt;XXXXXXXXX&lt;/reporteeOrgNr&gt;\n  &lt;/InternInfo&gt;\n  &lt;Kontakt&gt;\n    &lt;kontaktPersonNavn&gt;OLA NORDMANN&lt;/kontaktPersonNavn&gt;\n    &lt;kontaktPersonEpost&gt;ola@tull.no&lt;/kontaktPersonEpost&gt;\n    &lt;kontaktPersonTelefon&gt; &lt;/kontaktPersonTelefon&gt;\n    &lt;kontaktInfoBekreftet&gt;1&lt;/kontaktInfoBekreftet&gt;\n    &lt;kontaktInfoKommentar&gt;Dette er en et eksempel for Dapla-manualen&lt;/kontaktInfoKommentar&gt;\n  &lt;/Kontakt&gt;\n  &lt;Skjemadata&gt;\n    &lt;ledigeStillinger&gt;75&lt;/ledigeStillinger&gt;\n    &lt;datoPrefill&gt; &lt;/datoPrefill&gt;\n  &lt;/Skjemadata&gt;\n&lt;/melding&gt;\n\n\n\nFilstien til filene i team SUV-V sine delt-bÃ¸tter fÃ¸lger en streng navnestandard. FigurÂ 1 viser et eksempel pÃ¥ hvordan filstien til et fiktivt skjema kan se ut i bÃ¸tta til team SU-V. Filstien har egenskapen at den er globalt unik og inneholder informasjon om tidspunktet skjemaet ble innkvittert i SSB sine systemer.\n\n\n\n\n\n\nFigurÂ 1: Typisk filsti for et Altinn3-skjema.\n\n\n\nSkjemanavnet du ser i FigurÂ 1 er unike. Dvs. at hvis en oppgavegiver sender inn flere skjemaer pÃ¥ samme dag, sÃ¥ er fortsatt skjemanavnet unikt. Det er viktig Ã¥ vÃ¦re klar over nÃ¥r man bruker automatiseringstjenesten for kildedata senere. Siden tjenesten trigges per fil, og man er avhengig av unike navn for Ã¥ ikke skrive over filer, sÃ¥ er det nyttig Ã¥ vite at man kan viderefÃ¸re skjemanavnet i overgangen fra kildedata til inndata.\n\n\nMetadata\nFor hver ny innsending fra Altinn3 i test- og prod-miljÃ¸et, sÃ¥ tilrettelegger team SU-V en json-fil med metadata om innsedningen. Filen inneholder forelÃ¸pig to variabler:\n\nReferansenummeret som oppgavegiver fÃ¥r ved innsending.\nTidspunkt for nÃ¥r skjema er levert i Altinn3.\n\nBrukerbehovet er i hovedsak dublettkontroll og svartjeneste. For dette trenger man et eksakt tidspunkt for nÃ¥r skjema faktisk ble sendt inn (trykket pÃ¥ knappen i Altinn). Merk at tidspunkt i fila er UTC.\nFilen ligger i bÃ¸ttene sammen med xml/pdf (og eventuelle vedlegg). Team T-Rex vil se videre pÃ¥ Ã¥ integrere dette inn i sin Python-pakke.\n{\n    \"altinnReferanse\": \"f23415ca6b2f\", \n    \"altinnTidspunktLevert\": \"2024-04-29T07:16:10.5080448Z\"\n}\n\n\n\n\n\n\nFigurÂ 2: Metadata fra Altinn",
    "crumbs": [
      "Manual",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn-oversikt.html#synkronisering-av-data",
    "href": "statistikkere/altinn-oversikt.html#synkronisering-av-data",
    "title": "Oversikt",
    "section": "Synkronisering av data",
    "text": "Synkronisering av data\nNÃ¥r et statistikkteam har fÃ¥tt beskjed av team SU-V at data er tilgjengelig deres delt-bÃ¸tte og at statistikkteamets Transfer Service har fÃ¥tt tilgang, sÃ¥ kan man sette opp synkronisering av data fra team SU-V til statistikkteamets kildebÃ¸tte.\nNÃ¥r man synkroniserer data fra team SU-V til egen kildebÃ¸tte er det anbefalt Ã¥ gjÃ¸re fÃ¸lgende:\nLag en mappe per skjema\nFÃ¸r man setter opp synkroniseringen bÃ¸r man opprette en mappe per datakilde som teamet har. Dette inkluderer bÃ¥de Altinn-kilder og andre kilder. Hvis teamet har flere skjemaer fra Altinn sÃ¥ kan man f.eks. bruke skjemanummer som Ã¸verste mappenivÃ¥ i kildebÃ¸tta, og synkronisere hvert skjema til sin egen undermappe.\n\n\n\n\n\n\nEksempel pÃ¥ mappestruktur\n\n\n\n\n\nssb-dapla-felles-data-kilde-prod\nâ”œâ”€â”€ altinn\nâ”‚   â”œâ”€â”€ ra0678\nâ”‚   â””â”€â”€ ra0778\nâ””â”€â”€ andrekilder\n\n\n\nBehold mappestrukturen til team SU-V\nNÃ¥r vi bruker Transfer Service til Ã¥ synkronisere innholdet i Team SU-V sitt lagringsomrÃ¥de til Dapla-teamet sin kildebÃ¸tte, sÃ¥ er det anbefalt Ã¥ fortsette Ã¥ bruke mappe-strukturen som Team SU-V har for Ã¥ sikre at ingen filer blir overskrevet pÃ¥ grunn av at de har identiske navn.\n\n\n\n\n\n\nEksempel pÃ¥ mappestruktur for ra0678\n\n\n\n\n\nssb-dapla-felles-data-kilde-prod\nâ”œâ”€â”€ altinn\nâ”‚   â”œâ”€â”€ ra0678\nâ”‚   â”‚   â””â”€â”€ 2026\nâ”‚   â”‚       â””â”€â”€ 3\nâ”‚   â”‚           â””â”€â”€ 28\nâ”‚   â”‚               â””â”€â”€ b66abe1880cc_a35bceb7-950d-4e9b-a4a0-caea736ab270\nâ”‚   â”‚                   â””â”€â”€ form_b66abe1880cc.xml\nâ”‚   â””â”€â”€ ra0778\nâ””â”€â”€ andrekilder\n\n\n\n\nTransfer Service\nNÃ¥r vi skal overfÃ¸re filer fra Team SU-V sin bÃ¸tte til vÃ¥r kildebÃ¸tte, sÃ¥ kan vi gjÃ¸re det manuelt fra Jupyter som forklart her.. Men det er en bedre lÃ¸sning Ã¥ bruke en tjeneste som gjÃ¸r dette for deg. Transfer Service er en tjeneste som kan brukes til Ã¥ synkronisere innholdet mellom bÃ¸tter pÃ¥ Dapla, samt mellom bakke og sky. NÃ¥r du skal ta i bruk tjenesten for Ã¥ overfÃ¸re data mellom en bÃ¸tte fra Team SU-V sitt prosjekt suv-altinn-data-p, til en kildedata-bÃ¸tte i Dapla-teamet ditt, sÃ¥ gjÃ¸r du fÃ¸lgende:\n\nFÃ¸lg denne beskrivelsen hvordan man setter opp overfÃ¸ringsjobber.\nEtter at du har trykket pÃ¥ Create Transfer Job velger du Google Cloud Storage pÃ¥ begge alternativene under Get Started. Deretter gÃ¥r du videre ved Ã¥ klikke pÃ¥ Next Step.\nUnder Choose a source sÃ¥ skal du velge hvor du skal kopiere data fra. Trykk pÃ¥ Browse. I vinduet som dukker opp trykker du pÃ¥ ğŸ”»-ikonet ved siden av Project ID. I sÃ¸kevinduet som dukker opp sÃ¸ker du opp suv-altinn-data-p og trykker pÃ¥ navnet. Da fÃ¥r du listet opp alle bÃ¸ttene i suv-altinn-data-p prosjektet. Til slutt trykker du pÃ¥ bÃ¸tta som Team SU-V har opprettet for undersÃ¸kelsen3 og klikker Select til nederst pÃ¥ siden. Trykk deretter Next step for Ã¥ gÃ¥ videre.\nUnder Choose a destination sÃ¥ skal du velge hvor du skal kopiere data til. Dette steget er ganske likt som forrige, men du skal nÃ¥ velge ditt eget projekt og kildebÃ¸tta der. Trykk pÃ¥ Browse. I vinduet som dukker opp trykker du pÃ¥ ğŸ”»-ikonet ved siden av Project ID. I sÃ¸kevinduet som dukker opp sÃ¸ker du opp &lt;ditt teamnavn&gt;-kilde-&lt;miljÃ¸&gt; og trykker pÃ¥ navnet. Da fÃ¥r du listet opp alle bÃ¸ttene i ditt team sitt prosjekt. Velg kildebÃ¸tta som har navnet ssb-&lt;teamnavn&gt;-kilde-&lt;miljÃ¸&gt;. Hvis du Ã¸nsker Ã¥ kopiere data til en undermappe i bÃ¸tta, sÃ¥ trykker du pÃ¥ &gt;-ikonet ved bÃ¸ttenavnet og velger Ã¸nsket undermappe4. Til slutt trykker du pÃ¥ Select til nederst pÃ¥ siden. Trykk deretter Next step for Ã¥ gÃ¥ videre.\nUnder Choose how and when to run job velger du Run with custom frequency og Starting now i Batch-modus. Hvis du Ã¸nsker Ã¥ overfÃ¸re sÃ¥ ofte som mulig, sÃ¥ velger du Custom frequency og Repeat every 1 Hours. Til slutt trykker du Next Step nederst pÃ¥ siden.\nUnder Choose Settings sÃ¥ legger du til en kort beskrivelse av jobben du har opprettet. Under de andre valgene kan du gjÃ¸re fÃ¸lgende:\n\nUnder Advanced transfer Options trenger du ikke gjÃ¸re noen endringer.\nUnder When to overwrite kan du velge If different.\nUnder When to delete kan du velge Never.\nUnder Manifest trenger du ikke huke av.\nUnder Logging options velger du samme som vist i FigurÂ 3.\n\n\n\n\n\n\n\n\nFigurÂ 3: Valg av opsjoner for logging i Transfer Service\n\n\n\nTil slutt trykker du Create for Ã¥ aktivere tjenesten. Den vil da sjekke Team SU-V sin bÃ¸tte hver time og kopiere over alle filer som ikke allerede eksisterer statistikkteamets kildebÃ¸tte.",
    "crumbs": [
      "Manual",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn-oversikt.html#kildomaten",
    "href": "statistikkere/altinn-oversikt.html#kildomaten",
    "title": "Oversikt",
    "section": "Kildomaten",
    "text": "Kildomaten\nNÃ¥r du har satt opp Transfer Service til Ã¥ kopiere over filer fra Team SU-V sin bÃ¸tte til statistikkteamets kildebÃ¸tte, sÃ¥ vil det potensielt komme inn nye skjemaer hver time. Siden ingen pÃ¥ statistikkteamet har tilgang til kildebÃ¸tta som standard5, sÃ¥ er neste steg Ã¥ prosessere dataene med Kildomaten og lagre dataene i produktbÃ¸tta hvot alle i teamet har tilgang.\nLes mer om hvordan du kan bruker tjenesten her.",
    "crumbs": [
      "Manual",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn-oversikt.html#altinn3-til-isee",
    "href": "statistikkere/altinn-oversikt.html#altinn3-til-isee",
    "title": "Oversikt",
    "section": "Altinn3 til ISEE",
    "text": "Altinn3 til ISEE\nEnkelte Dapla-team mÃ¥ flytte sine skjemaer over til Altinn 3, men Ã¸nsker Ã¥ bruke ISEE videre i produksjonsprosessen. Det er fullt mulig og dokumentert her.\nDet er ogsÃ¥ utviklet en Altinn3-pakke i Python som flater ut XML-filer fra Altinn 3 og lager en csv-fil som er pÃ¥ ISEE-format av innholdet. ssb-altinn-python er tilgjengelig i Kildomaten, og kan benyttes for Ã¥ automatisk flate ut Altinn3-skjema mellom kilde- og produktbÃ¸tte.\nDet er ogsÃ¥ skrevet et blogginnlegg som beskriver i mer detaljer hvordan man inkludere Altinn 3, Dapla og ISEE i et produksjonslÃ¸p.",
    "crumbs": [
      "Manual",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn-oversikt.html#footnotes",
    "href": "statistikkere/altinn-oversikt.html#footnotes",
    "title": "Oversikt",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nEn typisk prosessering som de fleste vil Ã¸nske Ã¥ gjÃ¸re er Ã¥ konvertere fra xml-formatet det kom pÃ¥, og over til parquet-formatet.â†©ï¸\nTeknisk sett er hele filstien det samme som et filnavn i en bÃ¸tte. Men det omtales heretter som filnavn og filsti for Ã¥ kunne skille mellom hele stien, og det som tradisjonelt er oppfattet som filnavn.â†©ï¸\nBÃ¸ttenavnet starter alltid med RA-nummeret til undersÃ¸kelsen.â†©ï¸\nAlternativt oppretter du en mappe direkte vinduet ved Ã¥ trykke pÃ¥ mappe-ikonet med en +-tegn i seg.â†©ï¸\nKun data-admins i teamet kan aktivere tilgang ved behovâ†©ï¸",
    "crumbs": [
      "Manual",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html",
    "href": "statistikkere/jupyter-pyspark.html",
    "title": "Jupyter-pyspark",
    "section": "",
    "text": "Jupyter-pyspark er en tjeneste pÃ¥ Dapla Lab som lar brukerne kode i Jupyterlab. Tjenesten kommer med Python, Pyspark og noen vanlige Jupyterlab-extensions ferdig installert. MÃ¥lgruppen for tjenesten er brukere som skal skrive produksjonskode med Pyspark i Jupyterlab.\nSiden tjenesten er ment for produksjonskode sÃ¥ er det veldig fÃ¥ Python-pakker som er forhÃ¥ndsinstallert. Antagelsen er at brukeren/teamet heller bÃ¸r installere de pakkene de selv trenger, framfor at det ligger ferdiginstallerte pakker som skal dekke behovet til alle brukere/team i SSB. Det reduserer kompleksitet i tjenesten og dermed sannsynligheten for feilsituasjoner.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#forberedelser",
    "href": "statistikkere/jupyter-pyspark.html#forberedelser",
    "title": "Jupyter-pyspark",
    "section": "Forberedelser",
    "text": "Forberedelser\nFÃ¸r man starter Jupyter-pyspark bÃ¸r man ha lest kapitlet om Dapla Lab og satt opp Git- og GitHub-konfigurasjonen under Min konto. Deretter gjÃ¸r du fÃ¸lgende:\n\nLogg deg inn pÃ¥ Dapla Lab\nUnder Tjenestekatalog trykker du pÃ¥ Start-knappen for Jupyter-pyspark\nGi tjenesten et navn\nÃ…pne Jupyter-pyspark konfigurasjoner",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#konfigurasjon",
    "href": "statistikkere/jupyter-pyspark.html#konfigurasjon",
    "title": "Jupyter-pyspark",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nFÃ¸r man Ã¥pner en tjeneste kan man konfigurere hvor mye ressurser man Ã¸nsker, hvilket team man skal representere, om et GitHub-repo skal klones ved oppstart, og mange andre ting. Valgene man gjÃ¸r kan ogsÃ¥ lagres slik at man Ã¥ slipper Ã¥ gjÃ¸re samme jobb senere. FigurÂ 1 viser Tjeneste-delen i konfigurasjonen for Jupyter hvor man kan velge hvilken versjon av Jupyter man vil bruke.\n\n\n\n\n\n\nFigurÂ 1: Jupyter-pyspark versjon i Dapla Lab\n\n\n\n\nData\nUnder Data-menyen kan man velge hvilket team og tilgangsgruppe man skal representere. Man gjÃ¸r dette ved Ã¥ velge navnet pÃ¥ tilgangsgruppen, og denne er alltid pÃ¥ formen &lt;teamnavn&gt;-&lt;tilgangsgruppe&gt;. FigurÂ 2 viser at brukeren har valgt tilgangsgruppen dapla-felles-developers, dvs. at de representerer tilgangsgruppen developers for teamet dapla-felles.\n\n\n\n\n\n\nFigurÂ 2: Detaljert tjenestekonfigurasjon for bÃ¸ttetilgang i Dapla Lab\n\n\n\nUnder Team og tilgangsgruppe kan brukeren ogsÃ¥ velge Ã¥ representere tilgangsgruppen data-admins for et team. I de tilfellene er det et krav om brukeren oppgir en skriftlig begrunnelse for hvorfor tilgangen er nÃ¸dvendig. I tillegg mÃ¥ kan de maksimalt aktivere tilgangen i 8 timer.\nFigurÂ 3 viser en bruker som aktiverer sin data-admins tilgang for team dapla-felles. Hvis brukeren ikke oppgir en begrunnelse vil de fÃ¥ en feilmelding ved oppstart av tjenesten.\n\n\n\n\n\n\nFigurÂ 3: Aktivere tilgang til kildedata for data-admins.\n\n\n\n\n\nGit/GitHub\nUnder menyen Git/GitHub kan man konfigurere Git og GitHub slik at det blir lettere Ã¥ jobbe med inne i tjenesten. Som standard arves informasjonen som er lagret under Min konto-Git i Dapla Lab. Informasjonen under tjenestekonfigurasjonen blir tilgjengeliggjort som miljÃ¸variabler i tjenesten. Informasjonen blir ogsÃ¥ lagt i $HOME/.netrc slik at man kan benytte ikke trenger Ã¥ gjÃ¸re noe mer for Ã¥ jobbe mot GitHub fra tjenesten.\n\n\n\n\n\n\nFigurÂ 4: Konfigurasjon av Git og GitHub for Jupyter-pyspark tjenesten i Dapla Lab\n\n\n\nFigurÂ 4 viser at brukeren som standard fÃ¥r aktivert Aktiver Git. Dette innebÃ¦rer at Git-brukernavn, Git e-post og GitHub-token arves fra brukerkonfigurasjonen. I tillegg sÃ¥ opprettes SSBs standard Git-konfigurasjon i ~/.gitconfig.\n\n\nPython/R\nUnder menyen Python/R kan man velge hvilke versjon av R og Python man Ã¸nsker Ã¥ kjÃ¸re. Man kan velge mellom alle tidligere tilbudte kombinasjoner av R og Python.\nI FigurÂ 5 ser vi av navnet py311-spark3.5.3-v4-2024.11.21 at tjenesten som default vil startes versjon 3.11 av Python og og 3.5.3 av Spark. Etter hvert som nye versjoner av Python og Spark kommer, kan disse tilgjengeliggjÃ¸res i tjenesten, men brukeren kan velge Ã¥ starte en eldre versjon av tjenesten.\n\n\n\n\n\n\nFigurÂ 5: Konfigurasjon av Git og GitHub for Jupyter-pyspark tjenesten i Dapla Lab\n\n\n\n\n\nRessurser\nUnder menyen Resources kan man velge hvor mye CPU og RAM man Ã¸nsker i tjenesten, slik som vist i FigurÂ 6. Velg sÃ¥ lite treng for Ã¥s gjÃ¸re jobben du skal gjÃ¸re.\n\n\n\n\n\n\nFigurÂ 6: Konfigurasjon av ressurser for Jupyter-pyspark tjenesten i Dapla Lab\n\n\n\n\n\nDiskplass\nSom default fÃ¥r alle som starter en instans av tjenesten en lokal disk pÃ¥ 10GB inne i tjenesten. Under Diskplass-menyen kan man velge Ã¥ Ã¸ke stÃ¸rrelsen pÃ¥ disken eller ikke noe disk i det hele tatt. Siden lokal disk i tjenesten hovedsakelig skal benyttes til Ã¥ lagre en lokal kopi av koden som lagres pÃ¥ GitHub mens man gjÃ¸r endringer bÃ¸r ikke stÃ¸rrelsen pÃ¥ disken vÃ¦re stor. FigurÂ 7 viser valgene som kan gjÃ¸res under Diskplass-fanen.\n\n\n\n\n\n\nFigurÂ 7: Konfigurasjon av lokal disk for Jupyter-pyspark tjenesten i Dapla Lab\n\n\n\n\n\nAvansert\nUnder Avansert kan man velge Ã¥ ikke tilgjengeliggjÃ¸re bÃ¸tter som filsystem inne i tjenesten. Konsekvensen av dette er at man mÃ¥ lese og skrive filer ved Ã¥ referere til bÃ¸ttene direkte.\n\n\n\n\n\n\nFigurÂ 8: Avansert konfigurasjon for Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#tilgjengelige-jar-filer-og-bruk-i-pyspark",
    "href": "statistikkere/jupyter-pyspark.html#tilgjengelige-jar-filer-og-bruk-i-pyspark",
    "title": "Jupyter-pyspark",
    "section": "Tilgjengelige JAR-filer og bruk i PySpark",
    "text": "Tilgjengelige JAR-filer og bruk i PySpark\nI /jupyter/lib-mappen i Jupyter-pyspark-miljÃ¸et er flere nyttige JAR-filer tilgjengelige for bruk med PySpark, inkludert stÃ¸tte for Google Cloud Storage, BigQuery, Avro og Delta Lake. Disse JAR-filene kan inkluderes i PySpark-konfigurasjonen for Ã¥ fÃ¥ tilgang til og arbeide med data fra disse kildene.\n\nTilgjengelige JAR-filer:\n\ngcs-connector-hadoop.jar: Kobler PySpark til Google Cloud Storage.\nspark-bigquery-with-dependencies_2.12.jar: Kobler PySpark til Google BigQuery.\nspark-avro_2.12.jar: StÃ¸tte for Ã¥ lese og skrive Avro-data.\ndelta-storage.jar og delta-core_2.12.jar: StÃ¸tte for Delta Lake, som muliggjÃ¸r ACID-transaksjoner og data versjonering.\n\nLegge til JAR-filer i PySpark:\n\nFor Ã¥ bruke disse JAR-filene, konfigurer PySpark med stien til hver fil:\nspark = SparkSession.builder \\\n    .appName(\"Jupyter-pyspark-konfig\") \\\n    .config(\"spark.jars\", \"/jupyter/lib/gcs-connector-hadoop.jar,\"\n                          \"/jupyter/lib/spark-bigquery-with-dependencies_2.12.jar,\"\n                          \"/jupyter/lib/spark-avro_2.12.jar,\"\n                          \"/jupyter/lib/delta-storage.jar,\"\n                          \"/jupyter/lib/delta-core_2.12.jar\") \\\n    .getOrCreate()\n\nEksempler pÃ¥ bruk av tilkoblingene:\n\nGoogle Cloud Storage (GCS):\ndf = spark.read.format(\"parquet\").load(\"gs://ditt-bucket-navn/path/to/data\")\nGoogle BigQuery:\ndf = spark.read.format(\"bigquery\") \\\n    .option(\"table\", \"prosjekt_id.dataset_id.tabell_id\") \\\n    .load()\nAvro:\ndf = spark.read.format(\"avro\").load(\"/path/to/avro/files\")\nDelta Lake:\n\nFor Ã¥ skrive til en Delta-tabell:\ndf.write.format(\"delta\").save(\"/path/to/delta-table\")\nFor Ã¥ lese fra en Delta-tabell:\ndelta_df = spark.read.format(\"delta\").load(\"/path/to/delta-table\")\n\n\n\nMed disse instruksjonene kan brukerne effektivt konfigurere Jupyter-PySpark til Ã¥ jobbe med eksterne datakilder og forskjellige dataformater i sitt PySpark-miljÃ¸.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#hvordan-spark-lokalt-fungerer-og-arbeidsfordeling-pÃ¥-kjerner",
    "href": "statistikkere/jupyter-pyspark.html#hvordan-spark-lokalt-fungerer-og-arbeidsfordeling-pÃ¥-kjerner",
    "title": "Jupyter-pyspark",
    "section": "Hvordan Spark Lokalt Fungerer og Arbeidsfordeling pÃ¥ Kjerner",
    "text": "Hvordan Spark Lokalt Fungerer og Arbeidsfordeling pÃ¥ Kjerner\nNÃ¥r Spark kjÃ¸res lokalt, starter det en SparkSession som kjÃ¸rer pÃ¥ en enkelt node (tjenesten din) uten Ã¥ involvere en distribuert klynge. Lokalt i Spark kan du kontrollere ressursbruken og fordele arbeidsmengden pÃ¥ tilgjengelige CPU-kjerner for Ã¥ optimalisere ytelsen.\n\nKjÃ¸ring i Lokal Modus\nNÃ¥r Spark konfigureres til Ã¥ kjÃ¸re i lokal modus, spesifiseres dette med \"local[N]\", der N representerer antall kjerner som Spark skal bruke. For eksempel: - \"local[*]\": Bruk alle tilgjengelige kjerner pÃ¥ maskinen. - \"local[2]\": Bruk 2 kjerner, uavhengig av hvor mange som er tilgjengelige.\nEksempel pÃ¥ konfigurasjon:\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"Lokal Spark\") \\\n    .master(\"local[*]\") \\  # Bruker alle tilgjengelige kjerner\n    .getOrCreate()",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#datatilgang",
    "href": "statistikkere/jupyter-pyspark.html#datatilgang",
    "title": "Jupyter-pyspark",
    "section": "Datatilgang",
    "text": "Datatilgang\nMan inspisere dataene fra en terminal inne i tjenesten:\n\nÃ…pne en instans av Jupyter-pyspark med data fra bÃ¸tter\nÃ…pne en terminal inne i Jupyter\nGÃ¥ til mappen med bÃ¸ttene ved Ã¥ kjÃ¸re dette fra terminalen cd /buckets\nKjÃ¸r ls -ahl i terminalen for Ã¥ se pÃ¥ hvilke bÃ¸tter som er montert.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#installere-pakker",
    "href": "statistikkere/jupyter-pyspark.html#installere-pakker",
    "title": "Jupyter-pyspark",
    "section": "Installere pakker",
    "text": "Installere pakker\nSiden det nesten ikke er installert noen pakker i tjenesten sÃ¥ kan brukeren opprette et ssb-project og installere pakker som vanlig.\nFor Ã¥ bygge et eksisterende ssb-project kan brukeren ogsÃ¥ bruke ssb-project.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#slette-tjenesten",
    "href": "statistikkere/jupyter-pyspark.html#slette-tjenesten",
    "title": "Jupyter-pyspark",
    "section": "Slette tjenesten",
    "text": "Slette tjenesten\nFor Ã¥ slette tjenesten kan man trykke pÃ¥ Slette-knappen i Dapla Lab under Mine tjenester. NÃ¥r man sletter en tjeneste sÃ¥ slettes hele disken inne i tjenesten, og alle ressurser frigjÃ¸res. Vi anbefaler at man avslutter heller enn pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#pause-tjenesten",
    "href": "statistikkere/jupyter-pyspark.html#pause-tjenesten",
    "title": "Jupyter-pyspark",
    "section": "Pause tjenesten",
    "text": "Pause tjenesten\nMan kan pause tjenesten ved Ã¥ trykke pÃ¥ Pause-knappen i Dapla Lab under Mine tjenester. NÃ¥r man pauser, slettes alt pÃ¥ den lokale disken som ikke er lagret under $HOME/work. Vi anbefaler at man avslutter heller enn pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#monitorering",
    "href": "statistikkere/jupyter-pyspark.html#monitorering",
    "title": "Jupyter-pyspark",
    "section": "Monitorering",
    "text": "Monitorering\n\n\nMan kan moniterere en instans av Jupyter-pyspark ved Ã¥ trykke pÃ¥ Jupyter-pyspark-teksten under Mine tjenester i Dapla Lab, slik som vist i FigurÂ 9.\nDenne funksjonaliteten er under arbeid og mer informasjon kommer snart.\n\n\n\n\n\n\n\n\n\nFigurÂ 9: Monitorering av Jupyter-pyspark i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/vardef-toolbelt.html",
    "href": "statistikkere/vardef-toolbelt.html",
    "title": "Vardef",
    "section": "",
    "text": "Warning\n\n\n\nVardef er forelÃ¸pig kun tilgjengelig i test-miljÃ¸et og er ikke i produksjon.\nVardef er SSBs system for dokumentasjon av variabler. Vardef bestÃ¥r av et sentralt datalager som man kan interagere med via et API. Statistikere og forskere i SSB kan interagere med systemet gjennom Vardef-delen av Python-pakken dapla-toolbelt-metadata.\nBrukerdokumentasjonen for Vardef er skrevet i notebooks som er ferdig installert i tjenesten Vardef-forvaltning i Dapla Lab. Det er opprette egne notebooks for typiske arbeidsoppgaver man Ã¸nsker Ã¥ gjÃ¸re i Vardef, og brukeren kan kjÃ¸re disse uten Ã¥ skrive kode selv. I notebooks-ene finner man ferdigskrevet kode som bruker dapla-toolbelt-metadata for Ã¥ jobbe med Vardef. Hvis man Ã¸nsker Ã¥ se innholdet i notebooks uten Ã¥ starte tjenesten Vardef-forvaltning, sÃ¥ kan gÃ¥ inn pÃ¥ lenkene til hÃ¸yre pÃ¥ denne siden.\nPÃ¥ denne siden dokumenteres hvordan notebooksâ€™ene i Vardef-forvaltning kan benyttes for Ã¥ gjÃ¸re ulike arbeidsoppgaver knyttet til Vardef.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata",
      "Vardef"
    ]
  },
  {
    "objectID": "statistikkere/vardef-toolbelt.html#forberedelser",
    "href": "statistikkere/vardef-toolbelt.html#forberedelser",
    "title": "Vardef",
    "section": "Forberedelser",
    "text": "Forberedelser\nFor Ã¥ jobbe med Vardef i Vardef-forvaltning mÃ¥ man fÃ¸rst gjÃ¸re fÃ¸lgende:\n\nLogge deg inn Dapla Lab\nÃ…pne Tjenestekatalog og trykke Start for Vardef-forvaltning\nUnder Data velger du hvilket team og tilgangsgruppe du skal representere i tjenesten.\nTrykk Start\n\nSiden det er Dapla-team og tilgangsgrupper i teamene som forvalter variabeldefinisjoner i Vardef, sÃ¥ er det svÃ¦rt viktig at man velger riktig under punkt 3. Hvis man skal opprette en ny definisjon sÃ¥ er det teamet man logger seg inn med i Vardef-forvaltning som blir automatisk satt som eier av definisjonen. Tilsvarende er tilfellet for endringer; man mÃ¥ logge seg inn som det teamet som forvalter en variabeldefinisjon for Ã¥ kunne gjÃ¸re endringer.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata",
      "Vardef"
    ]
  },
  {
    "objectID": "statistikkere/vardef-toolbelt.html#funksjonalitet",
    "href": "statistikkere/vardef-toolbelt.html#funksjonalitet",
    "title": "Vardef",
    "section": "Funksjonalitet",
    "text": "Funksjonalitet\nUnder finner dere beskrivelser av hvordan man gjÃ¸r typiske operasjoner mot Vardef.\n\nMigrere fra Vardok\nÃ˜nsker man Ã¥ migrere en variabeldefinisjon sÃ¥ kan man kjÃ¸re notebooken migrere_variabel_definisjon.ipynb.\nKjÃ¸r gjennom notebookâ€™en og skriv inn variabeldefinisjonens ID i Vardok nÃ¥r man blir bedt om det. Alle seksjonsledere har oversikt over hvilke IDâ€™er fra Vardok som deres seksjon er ansvarlig for.\nNÃ¥r notebookâ€™en er kjÃ¸rt gjennom sÃ¥ er variabeldefinisjonen migrert fra Vardok til status Utkast i Vardef. I denne prosessen oppdaterer dapla-toolbelt-metadata all informasjon som kan utledes automatisk, og deretter mÃ¥ brukeren oppdatere gjenvÃ¦rende informasjon fÃ¸r de publiserer den internt eller eksternt.\n\nOppdatere utkast\nEt utkast kan oppdateres ved Ã¥ kjÃ¸re notebooken redigere_utkast.ipynb. Den skriver variabeldefinisjonen ned til en fil som brukeren kan gjÃ¸re endringer i. NÃ¥r endringene er gjort, sÃ¥ kan man oppdatere utkastet i slutten av notebooken.\n\n\nPublisere utkast\nNÃ¥r brukeren er fornÃ¸yd med utkastet og Ã¸nsker Ã¥ publisere, sÃ¥ kan man velge Ã¥ publisere internt eller eksternt. Ved migrering av tidligere eksternt publiserte definisjoner fra Vardok, sÃ¥ er hovedregelen at ogsÃ¥ man publiserer variabeldefinisjonen eksternt i Vardef.\nMan kan publisere en variabeldefinisjon eksternt ved Ã¥ kjÃ¸re notebookâ€™en publisere_eksternt.ipynb.\nMan kan publisere en variabeldefinisjon internt ved Ã¥ kjÃ¸re notebookâ€™en publisere_internt.ipynb.\n\n\n\nNy variabeldefinisjon\nFor Ã¥ opprette en ny variabeldefinisjon mÃ¥ man fÃ¸rst opprette et utkast, deretter redigere utkastet og til slutt publisere. Da vil man bruke fÃ¸lgende notebooks:\n\nopprette_utkast.ipynb\nredigere_utkast.ipynb\npublisere_internt.ipynb eller publisere_eksternt.ipynb\n\n\n\nNy gyldighetsperiode\nNy gyldighetsperiode i Vardef betyr at man Ã¸nsker Ã¥ gjÃ¸re endringer i en eksisterende variabeldefinisjon som gjÃ¸r at den fÃ¥r en ny betydning. Dette betyr at definisjonsteksten endres, og denne endringen krever at den fÃ¥r en ny gyldighetsperiode. Dette kan kun gjÃ¸res pÃ¥ variabeldefinisjoner som allerede er publisert internt eller eksternt.\nFor opprette en ny gyldighetsperiode kan man kjÃ¸re gjennom notebooken opprette_gyldighetsperiode.ipynb.\nHer blir man bedt om Ã¥ oppgi kortnavnet til variabeldefinisjonen som skal ha ny gyldighetsperiode, deretter skrives den til en fil hvor endringer kan gjÃ¸res. Til slutt oppdateres variabeldefinisjonen i Vardef.\n\n\nPatch\nEn patch i Vardef er en mindre endring som ikke endrer betydningen av variabeldefinisjonen. Man kan opprette en patch ved Ã¥ kjÃ¸re notebookâ€™en redigere_publisert_variabel.ipynb.\n\n\nLese ut informasjon\nFor Ã¥ hente ut informasjon fra Vardef finnes det ulike notebooks avhengig av hva man Ã¸nsker Ã¥ hente ut.\n\nEn variabeldefinisjon\nNotebooken lese_en_variabeldefinisjon.ipynb lar deg hente ut informasjon om en konkret variabeldefinisjon. Man kan hente ut all informasjon om variabeldefinisjonen, eller hente ut noen fÃ¥ informasjonselementer.\n\n\nAlle variabeldefinisjoner\nNotebooken lese_variabel_definisjoner.ipynb lar deg hente ut informasjon om alle variabeldefinisjoner og filtrere disse basert pÃ¥ ulike kriterier.\n\n\nLedige kortnavn\nNotebooken sjekk_om_kortnavn_er_ledig.ipynb lar deg sjekke om et kortnavn er tatt eller ikke. Kortnavn i Vardef mÃ¥ vÃ¦re unikem, sÃ¥ det kan vÃ¦re lurt Ã¥ sjekke om Ã¸nsket kortnavn er ledig fÃ¸r man prÃ¸ver Ã¥ publisere.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata",
      "Vardef"
    ]
  },
  {
    "objectID": "statistikkere/vardef-toolbelt.html#arbeide-med-yaml-filer",
    "href": "statistikkere/vardef-toolbelt.html#arbeide-med-yaml-filer",
    "title": "Vardef",
    "section": "Arbeide med YAML-filer",
    "text": "Arbeide med YAML-filer\n\n\n\n\n\n\nCaution\n\n\n\nFargene i YAML-eksemplene er kun veiledende og kan variere fra verktÃ¸y til verktÃ¸y, ettersom de settes av automatisk syntaksutheving.\n\n\n\nYAML-filstruktur\nYAML-filene fÃ¸lger samme grunnstruktur, med smÃ¥ variasjoner avhengig av bruk.\n\nMal\nDenne filen genereres nÃ¥r du vil opprette en ny variabeldefinisjon.\n# --- Variabeldefinisjon mal ---\n\n\n\n\n\n\nFilnavn mal\n\n\n\nLagres som variable_definition_template_&lt;timestamp&gt;.yaml\n\n\n\n\n\n\n\n\nNB! Innholder eksempelverdier som mÃ¥ endres\n\n\n\ncontact:\n    title:\n        nb: |-\n            generert tittel\n        nn:\n        en:\n\n\n\n\nVariabeldefinisjon\nDette er en YAML-fil med alle lagrede verdier for en variabeldefinisjon.\n# --- Variabeldefinisjon ---\n\n\n\n\n\n\nFilnavn variabeldefinisjon\n\n\n\nLagres som variable_definition_&lt;kortnavn&gt;_&lt;id&gt;_&lt;timestamp&gt;.yaml\n\n\n\n\nFeltblokker\nHvert felt i YAML-filen har sin egen kommentarseksjon rett over seg.\n# ! Obligatorisk felt !\n# Variabelens navn. Dette skal ikke vÃ¦re en mer â€œtekniskâ€ forkortelse, men et navn som er forstÃ¥elig for mennesker.\n# -------------------------\n# &gt;&gt;&gt; EKSEMPEL:\n# name:\n#   nb: |-\n#       LÃ¸nnsinntekter\nname:\nFelt markert med ~ Valgfritt felt ~ trenger ikke ha en verdi. De kan utelates dersom de ikke er relevante.\n# ~ Valgfritt felt ~\nFelt markert med ! Obligatorisk felt ! mÃ¥ alltid ha en gyldig verdi.\nI noen tilfeller settes verdien automatisk av systemet. For eksempel:\n# ! Obligatorisk felt !\n# Livssyklus for variabelen.\nvariable_status: DRAFT\n\n\n\nRedigere YAML-filen\n\nEnkle verdier\n\nType: DoubleQuotedScalar\nVerdien skrives pÃ¥ samme linje som feltnavn (nÃ¸kkel)\nBruk doble fnutter \"\"\n\n\n\n\n\n\n\nFelt med enkle verdier\n\n\n\n\n\n\nclassification_reference\nmeasurement_type\nexternal_reference_uri\nowner.team\n\n\n\n\n\n\n\n\n\n\nKorrekt plassering og format\n\n\n\nshort_name: \"landbak\"\n\nmeasurement_type: \"03\"\n\n\n\nVanlige feil enkle verdier\n\n\n\n\n\n\nFeil plassering av verdi\n\n\n\nmeasurement_type: \n\"03\"\n\n\n\n\n\n\n\n\nFeil innrykk mellom nÃ¸kkel og verdi\n\n\n\nclassification_reference:\"91\"\n\n\n\n\n\n\n\n\nMangler kolon\n\n\n\nshort_name\"landbak\"\n\n\n\n\n\n\n\n\nFeil verdi type\n\n\n\n# Hvis fnuttene fjernes vil verdien tolkes som et tall og validering vil feile.\nmeasurement_type: 03\n\n\n\n\n\nFlersprÃ¥klige felt\n\nType: LiteralScalar.\nEt blokk symbol |- fÃ¸lger nÃ¸kkel.\nTeksten mÃ¥ starte rett under blokk-symbolet\nRiktig innrykk er viktig\nFormattering (avsnitt/linjeskift) lagres.\n\n\n\n\n\n\n\nFlersprÃ¥klige felt\n\n\n\n\n\n\nname\ndefinition\ncomment\ncontact.title\n\n\n\n\n\n\n\n\n\n\nKorrekt plassering av tekst under blokk-symbol\n\n\n\n\n\ncontact:\n    title:\n        nb: |-\n            Her starter teksten akkurat under blokk symbolet og vil lagres uten feil.\n        nn:\n        en:\n    email: generert@ssb.no\n\n\n\n\n\n\n\n\n\nKorrekt lagring av avsnitt\n\n\n\n\n\ncomment:\n    nb: |-\n        Her er det en god plan, fÃ¸rst er det en beskrivelse som innledning.\n\n        Deretter bevisst en blank linje:\n          - Liste-element 1\n          - Liste-element 2\n    nn:\n    en:\n\n\n\n\n\n\n\n\n\nBruk av spesialtegn\n\n\n\n\n\nname:\n    nb: |-\n        Her er kan vi lagre spesialtegn som : men det kan fÃ¸re til feil syntaksutheving.\n    nn:\n    en:\n\n\n\n\nVanlige feil flersprÃ¥klige felt\n\n\n\n\n\n\nUÃ¸nsket tekstoppsett (linjeskift midt i ord)\n\n\n\n\n\nname:\n    nb: |-\n        Her deles teksten opp\n        plutselig. Og den vil lagres akku-\n        rat slik du ser den.\n    nn:\n    en:\n\n\n\n\n\n\n\n\n\nFeil indentering (mangler korrekt innrykk)\n\n\n\n\n\nname:\n    nb: |-\nHer er teksten ikke indentert og vil fÃ¸re til feil nÃ¥r du lagrer.\n    nn:\n    en:\n\n\n\n\n\n\n\n\n\nMangler blokk-symbolet\n\n\n\n\n\ncomment:\n    nb: Her er blokk symbolet fjernet og tekst med ':' vil feile.\n    nn:\n    en: Also text with paragraphs\n    will not be saved correctly.\n\n\n\n\n\n\nLister\n\nType: DoubleQuotedScalar\nVerdien skrives pÃ¥ samme linje som liste-symbol -\nBruk doble fnutter \"\"\n\n\n\n\n\n\n\nFelt med lister\n\n\n\n\n\n\nunit_types\nsubject_fields\nrelated_variable_definition_uris\nowner.groups\n\n\n\n\n\n\n\n\n\n\nKorrekt listeformat og indentering\n\n\n\nunit_types:\n    - \"20\"\n\n\n\nVanlige feil lister\n\n\n\n\n\n\nFeil type i listeelementer\n\n\n\nunit_types:\n    - 01\n    - 02\n\n\n\n\n\n\n\n\nFeil indentering pÃ¥ liste-symbol\n\n\n\nunit_types:\n- \"20\"\n\n\n\n\n\n\n\n\nFeil indentering ved listeverdi\n\n\n\nsubject_fields:\n  -\"bf\"\n\n\n\n\n\nAndre typer\n\n\n\n\n\n\nDato\n\n\n\nvalid_from: 2003-01-01\n\n\n\n\n\n\n\n\nBoolean\n\n\n\ncontains_special_categories_of_personal_data: true\n\n\n\n\n\n\n\n\nVariabel status\n\n\n\n\nvariable_status: DRAFT\n\nvariable_status: PUBLISHED_INTERNAL\n\nvariable_status: PUBLISHED_EXTERNAL",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata",
      "Vardef"
    ]
  },
  {
    "objectID": "statistikkere/klass.html",
    "href": "statistikkere/klass.html",
    "title": "Klass",
    "section": "",
    "text": "Klass er SSBs system for dokumentasjon av kodeverk1. Hensikten er at alle kodeverk skal dokumenteres og oppdateres ett sted (av ansvarlig seksjon), og gjenbrukes av alle som har behov for dem. Ansvarlig seksjon vil da sikre at alle fÃ¥r tak i gyldig versjon (eller en tidligere versjon dersom det er det brukeren trenger).\nAlle2 kodeverk som brukes i SSBs statistikkproduksjon skal dokumenteres i Klass. Hvert kodeverk har en eierseksjon, og det er eierseksjonen som har ansvar for Ã¥ dokumentere, og senere oppdatere, kodeverket. Klass viser hvilke kategorier de ulike kodeverkene inneholder pÃ¥ en gitt dato, eller i lÃ¸pet av en gitt tidsperiode, hvordan kategoriene endrer seg over tid (f.eks. endring i kommuneinndelingen Standard for kommuneinndeling). I tillegg kan Klass vise mappingen mellom to ulike kodeverk (f.eks mellom politidistrikt og kommuner Standard for politidistrikt). En kan ogsÃ¥ lage en variant av et kodeverk, der en f.eks. aggregerer ulike koder i det opprinnelige kodeverket (se f.eks. Standard for nÃ¦ringsgruppering (SN) som er en variant der kodene i Standard for nÃ¦ringsgruppering er blitt aggregert). Kodeverkene kan gjenbrukes av alle som trenger dem, bÃ¥de SSB-ansatte og eksterne brukere. Mange av kodeverkene i Klass, f.eks. Standard for kommuneinndeling og Standard for nÃ¦ringsgruppering, gjenbrukes i stor grad bÃ¥de av interne og eksterne brukere.\nDet kan refereres til Klass bÃ¥de fra Vardef og Datadoc.\nKodeverkene kan hentes ut via et API eller lastes ned i csv-format. API-er det mest fleksible Ã¥ bruke. Her kan en f.eks. hente ut kodeverket slik det var pÃ¥ en bestemt dato, i et bestemt tidsrom, hvilke endringer som har skjedd innen et visst tidsrom og korrespondansetabeller som viser mappingen mellom to ulike kodeverk. Det finnes ogsÃ¥ veiledning til bruk av Klass i statistikkproduksjonen (sas, R og Python) her: Bruk av Klass i statistikkproduksjonen.",
    "crumbs": [
      "Manual",
      "Metadata",
      "Klass"
    ]
  },
  {
    "objectID": "statistikkere/klass.html#footnotes",
    "href": "statistikkere/klass.html#footnotes",
    "title": "Klass",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nKodeverk er en fellesbetegnelse for klassifikasjoner og kodelister. En klassifikasjon er et mer Â«formeltÂ» kodeverk som oppfyller flere betingelser, bl.a. at den er normativ og uttÃ¸mmende. Alle klassifikasjoner i SSB har navn som begynner med Standard for, f.eks. Standard for nÃ¦ringsgruppering eller Standard for kommuneinndeling. Ei kodeliste er ofte skreddersydd for en bestemt statistikk, er ikke normativ og trenger ikke vÃ¦re uttÃ¸mmende.â†©ï¸\nDet finnes kodeverk som ikke kan legges inn i Klass fordi de ikke oppfyller kravene som stilles i den internasjonale modellen som Klass bygger pÃ¥.â†©ï¸",
    "crumbs": [
      "Manual",
      "Metadata",
      "Klass"
    ]
  },
  {
    "objectID": "statistikkere/spark.html",
    "href": "statistikkere/spark.html",
    "title": "Apache Spark",
    "section": "",
    "text": "Koding i R og Python har historisk sett foregÃ¥tt pÃ¥ en enkelt maskin og vÃ¦rt begrenset av minnet (RAM) og prosessorkraften pÃ¥ maskinen. For bearbeiding av smÃ¥ og mellomstore datasett er det sjelden et problem pÃ¥ kjÃ¸re pÃ¥ en enkelt maskin. PopulÃ¦re pakker som dplyr for R, og pandas for Python, blir ofte brukt i denne typen databehandling. I senere Ã¥r har det ogsÃ¥ kommet pakker som er optimalisert for Ã¥ kjÃ¸re kode parallelt pÃ¥ flere kjerner pÃ¥ en enkelt maskin, skrevet i minne-effektive sprÃ¥k som Rust og C++.\nMen selv om man kommer langt med Ã¥ kjÃ¸re kode pÃ¥ en enkelt maskin, vil enkelte oppgaver kreve mer minne og prosessorkraft enn dette. For stÃ¸rre datasett, eller store beregninger, kan det vÃ¦re nyttig Ã¥ bruke et rammeverk som kan kjÃ¸re kode parallelt pÃ¥ flere maskiner. Et slikt rammeverk er Apache Spark.\nApache Spark er et rammeverk for Ã¥ kjÃ¸re kode parallelt pÃ¥ flere maskiner. Det er bygget for Ã¥ hÃ¥ndtere store datasett og store beregninger. Det er derfor et nyttig verktÃ¸y for Ã¥ lÃ¸se problemer som er for store for Ã¥ kjÃ¸re pÃ¥ en enkelt maskin. Men det finnes ogsÃ¥ andre bruksomrÃ¥der som er nyttige for Apache Spark. Her er noen eksempler:\nDette er noen av bruksomrÃ¥dene der Spark kan lÃ¸se problemer som er for store for Ã¥ kjÃ¸re pÃ¥ en enkelt maskin med for eksempel Pandas eller dplyr.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Apache Spark"
    ]
  },
  {
    "objectID": "statistikkere/spark.html#spark-pÃ¥-dapla",
    "href": "statistikkere/spark.html#spark-pÃ¥-dapla",
    "title": "Apache Spark",
    "section": "Spark pÃ¥ Dapla",
    "text": "Spark pÃ¥ Dapla\nDapla kjÃ¸rer pÃ¥ et Kubernetes-kluster og er derfor er et svÃ¦rt egnet sted for Ã¥ kjÃ¸re kode parallelt pÃ¥ flere maskiner. Jupyter pÃ¥ Dapla har ogsÃ¥ en flere klargjorte kernels for Ã¥ kjÃ¸re kode i Apache Spark. Denne koden vil kjÃ¸re pÃ¥ et eget kluster av maskiner som er dedikert til Apache Spark, slik som vist i FigurÂ 1.\n\n\n\n\n\n\n\n\n\n\n\n(a) PySpark pÃ¥ kubernetes\n\n\n\n\n\n\n\n\n\n\n\n(b) PySpark pÃ¥ 1 maskin\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) SparkR pÃ¥ kubernetes\n\n\n\n\n\n\n\n\n\n\n\n(d) SparkR pÃ¥ 1 maskin\n\n\n\n\n\n\n\nFigurÂ 1: Ferdigkonfigurerte kernels for Spark pÃ¥ Dapla.\n\n\n\nFigurÂ 1 (a) og FigurÂ 1 (c) kan velges hvis du Ã¸nsker Ã¥ bruke Spark for Ã¥ kjÃ¸re store jobber pÃ¥ flere maskiner, for henholdsvis Python- og R-grensesnittene for Spark.\nFigurÂ 1 (b) og FigurÂ 1 (d) bÃ¸r du velge hvis du Ã¸nsker Ã¥ bruke Spark av andre grunner enn Ã¥ kjÃ¸re store jobber pÃ¥ flere maskiner. For eksempel hvis du Ã¸nsker Ã¥ bruke en av de mange pakker som er bygget pÃ¥ Spark, eller hvis du Ã¸nsker Ã¥ bruke Spark til Ã¥ lese og skrive data fra Dapla.\nHvis du Ã¸nsker Ã¥ sette opp et eget virtuelt miljÃ¸ for Ã¥ kjÃ¸re Spark, sÃ¥ kan du bruke ssb-project. Se ssb-project for mer informasjon.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Apache Spark"
    ]
  },
  {
    "objectID": "statistikkere/spark.html#spark-i-og-python",
    "href": "statistikkere/spark.html#spark-i-og-python",
    "title": "Apache Spark",
    "section": "Spark i  og Python",
    "text": "Spark i  og Python\nSpark er implementert i programmeringssprÃ¥ket Scala. Men det tilbys ogsÃ¥ mange grensesnitt for Ã¥ bruke Spark fra andre sprÃ¥k. De mest populÃ¦re grensesnittene er PySpark for Python og SparkR for R. Disse grensesnittene er bygget pÃ¥ Spark, og gir tilgang til Spark-funksjonalitet fra Python og R.\n\nPySpark\nPySpark er et Python-grensesnitt for Apache Spark. Det er en Python-pakke som gir tilgang til Spark-funksjonalitet fra Python. Det er enkelt Ã¥ bruke, og har mange av de samme funksjonene som Pandas.\nUnder ser du datasettet som benyttes for i vedlagt notebook pyspark-intro.ipynb. Den viser hvordan man kan gjÃ¸re vanlige databehandling med PySpark. I eksempelet brukes kernel som vist i FigurÂ 1 (b).\n\n\n\nCode\n# Legger til row index til DataFrame fÃ¸r join med dates_df\ndata_df = data_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n\n# Joiner de to datasettene\ndf = (\n    dates_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n    .join(data_df, \"row_index\")\n    .drop(\"row_index\")\n)\n\n# Legger til Ã¥r, kvartal og mnd\ndf = df.withColumn(\"Year\", date_format(df.Date, \"yyyy\"))\ndf = df.withColumn(\"Quarter\", expr(\"quarter(Date)\"))\ndf = df.withColumn(\"Month\", date_format(df.Date, \"MM\"))\n\ndf.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows\n\n\n\nSource: Introduksjon til PySpark\nDet finnes ogsÃ¥ et Pandas API/grensesnitt mot Spark. MÃ¥let med en er Ã¥ gjÃ¸re overgangen fra Pandas til Spark lettere for nybegynneren. Men hvis man skal gjÃ¸re litt mer avansert databehandling anbefales det at man bruker PySpark direkte og ikke Pandas API-et.\n\n\nSparkR\nSparkR er et R-grensesnitt for Apache Spark. Det er en R-pakke som gir tilgang til Spark-funksjonalitet fra R. Det er enkelt Ã¥ bruke, og har mange av de samme funksjonene som dplyr. Se eksempel i notebook under:\n\n\nselectedColumns &lt;- select(file, \"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\")\nshowDF(selectedColumns, numRows = 5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows\n\n\nSource: Introduksjon til SparkR",
    "crumbs": [
      "Manual",
      "Appendix",
      "Apache Spark"
    ]
  },
  {
    "objectID": "statistikkere/spark.html#lakehouse-arkitektur",
    "href": "statistikkere/spark.html#lakehouse-arkitektur",
    "title": "Apache Spark",
    "section": "Lakehouse-arkitektur",
    "text": "Lakehouse-arkitektur\n\n\n\n\n\n\nWarning\n\n\n\nI denne delen viser vi hvordan funksjonalitet som kan bli relevant for SSB Ã¥ benytte seg av i fremtiden. Men det er fortsatt under testing og ta det i betraktning fÃ¸r man eventuelt implementerer dette i produksjon.\n\n\nEn av utvidelsene som er laget rundt Apache Spark er den sÃ¥kalte Lakehouse-arkitekturen. Kort fortalt kan den dekke behovene som et klassisk datavarehus har tatt seg av tidligere. I kontekst av SSB sine teknologier kan det ogsÃ¥ benyttes som et databaselag over Parquet-filer i bÃ¸tter. Det finnes flere open source lÃ¸sninger for dette, men mest aktuelle er:\n\nDelta Lake\nApache Hudi\nApache Iceberg\n\nI det fÃ¸lgende omtaler vi hovedsakelig egenskapene til Delta Lake, men alle rammeverkene har mange av de samme egenskapene. Delta Lake kan ogsÃ¥ benyttes pÃ¥ Dapla nÃ¥.\nSentrale egenskaper ved Delta Lake er:\n\nACID-transactions som sikrer data-integritet og stabilitet, ogsÃ¥ nÃ¥r det skjer feil.\nMetadata som bli hÃ¥ndtert akkurat som all annen data og er veldig skalebar. Den stÃ¸tter ogsÃ¥ egendefinert metadata.\nSchema Enforcement and Evolution sikrer at skjemaet til dataene blir hÃ¥ndhevet, og den tillater ogsÃ¥ den kan endres over tid.\nTime travel sikrer at alle versjoner av dataene er blitt lagret, og at du kan gÃ¥ tilbake til tidligere versjoner av en fil.\nAudit history sikrer at du kan fÃ¥ full oversikt over hvilke operasjoner som utfÃ¸rt pÃ¥ dataene.\nInserts, updates and deletes betyr at du lettere kan manipulere data enn hva som er tilfellet med vanlige Parquet-filer.\nIndexing er stÃ¸ttes for forbedre spÃ¸rringer mot store datamengder.\n\nI vedlagt notebook deltalake-intro.ipynb finner du blant annet eksempler pÃ¥ hvordan du legger til fÃ¸lgende metadata i spesifikk versjon av en fil:\n\n\n\nCode\n# Lagrer egendefinert metadata i en json-fil\nimport json\n\nmetadata = {\n    \"comment\": \"Kontaktet oppgavegiver og kranglet!\",\n    \"manueltEditert\": \"True\",\n    \"maskineltEditert\": \"False\",\n}\nmetadata\n\n\n{'comment': 'Kontaktet oppgavegiver og kranglet!',\n 'manueltEditert': 'True',\n 'maskineltEditert': 'False'}\n\n\nSource: Introduksjon til Delta Lake",
    "crumbs": [
      "Manual",
      "Appendix",
      "Apache Spark"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-data.html",
    "href": "statistikkere/jobbe-med-data.html",
    "title": "Jobbe med data",
    "section": "",
    "text": "NÃ¥r man oppretter et dapla-team sÃ¥ fÃ¥r vi tildelt et eget omrÃ¥det for lagring av data. For Ã¥ kunne lese og skrive data fra Jupyter til disse omrÃ¥dene mÃ¥ vi autentisere oss, siden Jupyter og lagringsomrÃ¥det er to separate sikkerhetsoner.\nFigurÂ 1 viser dette klarer skillet mellom hvor vi koder og hvor dataene ligger pÃ¥ Dapla1. I dette kapitlet beskriver vi nÃ¦rmere hvordan du kan jobbe med dataene dine pÃ¥ Dapla.",
    "crumbs": [
      "Manual",
      "Data"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-data.html#ssb-biblioteker",
    "href": "statistikkere/jobbe-med-data.html#ssb-biblioteker",
    "title": "Jobbe med data",
    "section": "SSB-biblioteker",
    "text": "SSB-biblioteker\nFor Ã¥ gjÃ¸re det enklere Ã¥ jobbe data pÃ¥ tvers av Jupyter og lagringsomrÃ¥det er det laget noen egne SSB-utviklede biblioteker for Ã¥ gjÃ¸re vanlige operasjoner mot lagringsomrÃ¥det. Siden bÃ¥de R og Python skal brukes pÃ¥ Dapla, sÃ¥ er det laget to biblioteker, en for hver av disse sprÃ¥kene. fellesr er biblioteket for R, og dapla-toolbelt er biblioteket for Python.\n\ndapla-toolbelt\ndapla-toolbelt er en en pakke som lar deg enkelt lese og skrive til lagringsomrÃ¥det uten Ã¥ mÃ¥tte autentifisere deg manuelt. Den har en Pandas-aktig syntaks som forhÃ¥pentlig er gjenkjennbar for de fleste. Pakken er installert i alle Python-kernels pÃ¥ Dapla, sÃ¥ du trenger ikke Ã¥ installere den selv hvis du Ã¥pner en notebook med Python3 for eksempel. For Ã¥ importere hele biblioteket i en notebook skriver du bare\n\n\nnotebook\n\nimport dapla as dp\n\ndapla-toolbelt bruker en pakke som heter gcsfs for Ã¥ kommunisere med lagringsomrÃ¥det. gcsfs er en pakke som lar deg bruke Google Cloud Storage (GCS) som om det var en filsystem. Det betyr at du kan bruke samme syntaks som du bruker for Ã¥ lese og skrive til filer pÃ¥ din egen maskin. Du kan lese mulighetene i gcsfs her. Et eksempel pÃ¥ hvordan de to pakkene kan brukes sammen ser du her:\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Example of how you can use gcsfs and dapla-toolbelt together\nfs.touch(\"gs://my-bucket/my-folder/\")\n\nI koden over brukte jeg kommandoen touch fra gcsfs og FileClient fra dapla-toolbelt for Ã¥ opprette en mappe i lagringsomrÃ¥det.\nI kapitlene under finner du konkrete eksempler pÃ¥ hvordan du kan bruke dapla-toolbelt til Ã¥ jobbe med data i SSBs lagringsomrÃ¥det.\n\n\nfellesr\nR-pakken fellesr er under utvikling og gir mye av den samme funksjonaliteten som dapla-toolbelt gir for Python. I tillegg til Ã¥ kunne lese og skrive til lagringsomrÃ¥det pÃ¥ Dapla, sÃ¥ har fellesr ogsÃ¥ funksjoner for Ã¥ jobbe med metadata pÃ¥ Dapla.\nPÃ¥ Dapla Lab kan man bruke vanlige R-funksjoner for mappene du har tilgjengelig i filsystemet, dvs. bÃ¸tten du har tilgang til via Dapla-teamet ditt. Det betyr at funksjonene for Ã¥ lese (Â´read_SSBÂ´) og skrive (Â´write_SSBÂ´) data i Â´fellesrÂ´ ikke lenger er nÃ¸dvendige og vil ikke lenger vedlikeholdes.\nfellesr er installert pÃ¥ Dapla og funksjoner kan benyttes ved:\n\n\nnotebook\n\nlibrary(fellesr)\n\nHvis du benytter et renv-miljÃ¸, mÃ¥ pakken installeres fÃ¸r bruk. Dette kan gjÃ¸res ved:\n\n\nnotebook\n\nrenv::install(\"statisticsnorway/ssb-fellesr\")",
    "crumbs": [
      "Manual",
      "Data"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-data.html#vanlige-operasjoner",
    "href": "statistikkere/jobbe-med-data.html#vanlige-operasjoner",
    "title": "Jobbe med data",
    "section": "Vanlige operasjoner",
    "text": "Vanlige operasjoner\nI denne delen viser vi hvordan man gjÃ¸r veldig vanlige operasjoner nÃ¥r man koder et produksonslÃ¸p for en statistikk. Flere eksempler pÃ¥ nyttige systemkommandoer finner du her.\n\nListe ut innhold i mappe\n\n\n\n\n\n\nEksempeldata i Dapla Felles\n\n\n\nDapla Felles er et team der alle i SSB er med i developers-gruppa. Dvs. at alle har lese- og skrivetilgang til fÃ¸lgende omrÃ¥der:\ngs://ssb-dapla-felles-data-produkt-prod/ i prod-miljÃ¸et pÃ¥ Dapla, og\ngs://ssb-dapla-felles-data-produkt-test/ i test-miljÃ¸et. Eksemplene under bruker fÃ¸rstnevnte i koden, slik at alle kan kjÃ¸re koden selv.\nKode-eksemplene finnes for bÃ¥de R og Python, og du kan velge hvilken du skal se ved Ã¥ trykke pÃ¥ den arkfanen du er interessert i.\n\n\nÃ… liste ut innhold i et gitt mappe pÃ¥ Dapla er ganske enkelt. Under ser du hvordan du kan liste ut innholdet i fÃ¸lgende mappe:\ngs://ssb-dapla-felles-data-produkt-prod/datadoc/brukertest/10/sykefratot/klargjorte_data\n\nPython \n\n\nVi bruker modulen FileClient fra dapla-toolbelt for Ã¥ liste ut innholdet i en mappe.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Set path to folder\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/datadoc/brukertest/10/sykefratot/klargjorte_data\"\n\nFileClient.ls(file_path)\n\nMed kommandoen over fÃ¥r du listet ut alle filnavn i mappen. Hvis du vil ha mer informasjon om filene sÃ¥ kan du bruke ls-kommandoen med detail = True, som under:\n\n\nnotebook\n\nFileClient.ls(file_path, detail = True)\n\nSyntaksen med ls er veldig lik det man kjenner fra en Linux-terminal. Men nÃ¥r vi bruker detail = True blir metadata om filene returnert som en Python-liste med dictionaries. Det kan vÃ¦re svÃ¦rt nyttig nÃ¥r du f.eks. trenger Ã¥ vite dato og tidspunkt for nÃ¥r en fil ble opprettet, eller nÃ¥r den sist ble oppdatert.\n\n\n\n\nnotebook\n\n# Path to folder\nfile_path = \"/buckets/produkt/datadoc/brukertest/10/sykefratot/klargjorte_data\"\n\n# List files in folder \nlist.files(file_path)\n\n\n\n\n\n\nSkrive ut filer\nÃ… skrive filer til et lagringsomrÃ¥de pÃ¥ Dapla er ogsÃ¥ ganske enkelt. Det ligner mye pÃ¥ den syntaksen vi er kjent med fra vanlige R- og Python-pakker, med noen smÃ¥ unntak.\n\nParquet\nUnder lager vi en dataframe i en notebook og skriver den ut til en parquet-fil.\n\nPython \n\n\nNÃ¥r vi leser en Parquet-fil med dapla-toolbelt sÃ¥ bruker den pyarrow i bakgrunnen. Dette er en av de raskeste mÃ¥tene Ã¥ lese og skrive Parquet-filer pÃ¥.\n\n\nnotebook\n\nimport dapla as dp\nimport pandas as pd\nimport numpy as np\n\n# Set path to folder\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-manual-examples\"\n\n# Create pandas dataframe\npurchases = pd.DataFrame(np.random.randn(10, 5),\n                        columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\n# Write pandas dataframe as parquet to GCS bucket\ndp.write_pandas(df = purchases,\n                gcs_path = f\"{file_path}/data.parquet\",\n                file_format = \"parquet\",)\n\nNÃ¥r vi kalte write_pandas over sÃ¥ spesifiserte vi at filformatet skulle vÃ¦re parquet. Dette er default, sÃ¥ vi kunne ogsÃ¥ ha skrevet det slik:\n\n\nnotebook\n\ndp.write_pandas(df = purchases,\n                gcs_path = f\"{file_path}/data.parquet\")\n\nMen for de andre filformatene mÃ¥ vi altsÃ¥ spesifisere dette.\n\n\nNÃ¥r vi jobber med Parquet-fil i R, bruker vi pakken arrow.\n\n\nnotebook\n\n# Set stien til hvor data skal lagres\nfile_path = \"/buckets/produkt/dapla-manual-examples/purchases.parquet\"\n\n# Lag eksempeldata\npurchases = data.frame(A = runif(10), B= runif(10), C=runif(10))\n\n# Skrive data til bÃ¸tte som parquet\narrow::write_parquet(purchases, file_path)\n\n\n\n\n\n\nTekstfiler\nNoen ganger Ã¸nsker vi Ã¥ lagre data i andre formatter slik som CSV, JSON og XML.\n\nPython \n\n\ndapla-toolbelt kan skrive ut json, csv og posisjonsfiler (fixed-width-files/fwf) til lagringsomrÃ¥det. MÃ¥ten den gjÃ¸r det pÃ¥ er Ã¥ bruke Pandas sine funksjoner read_json, read_csv, read_fwf under panseret. Dette kan vÃ¦re nyttig Ã¥ vite for skjÃ¸nne hvordan dapla-toolbelt hÃ¥ndterer ulike strukturer i (spesielt hierarkiske) tekstfiler. Under ser du hvordan du kan skrive ut en dataframe til en json-fil.\n\n\nnotebook\n\nimport numpy as np\nimport pandas as pd\nfrom dapla import FileClient\n\n# Set path to folder\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-manual-examples\"\n\n# Create a dataframe with Pandas\ndf = pd.DataFrame(np.random.randn(10, 5), columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\n# Save dataframe as json with dapla-toolbelt\ndp.write_pandas(df = df,\n                gcs_path = f\"{file_path}/test.json\",\n                file_format = \"json\")\n\nSom vi ser at syntaksen over sÃ¥ kunne vi skrevet ut til noe annet enn json ved Ã¥ endre verdien i argumentet file_format.\n\n\nFunksjonen write.csv2 kan brukes til Ã¥ skrive csv-filer til bÃ¸tter.\n\n\nnotebook\n\n# Sett stien til hvor data skal lagres\nfile_path = \"/buckets/produkt/dapla-manual-examples/purchases.csv\"\n\n# Lag eksempeldata\npurchases = data.frame(A = runif(10), B= runif(10), C=runif(10))\n\n# Skriv til csv\nwrite.csv2(purchases, \n           file_path, \n           row.names = FALSE)\n\n\n\n\n\n\nxlsx\nDet er ikke anbefalt Ã¥ bruke xlsx-formatet, men her ser du hvordan det kan skrives ut. Mer kommer.\n\nPython \n\n\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import AuthClient\n\n# Henter token for Ã¥ kunne lese fra Dapla\ntoken = AuthClient.fetch_google_credentials()\n\n# Filsti\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-manual-examples\"\n\ndf.to_excel(f\"{file_path}/test.xlsx\",\n           storage_options={\"token\": token})\n\n\n\nFunksjonen write.xlsx fra pakken openxlsx kan brukes til Ã¥ skrive .xlsx-filer til bÃ¸tter i R.\n\n\nnotebook\n\n\n# Sett stien til hvor data skal lagres\nfile_path = \"/buckets/produkt/dapla-manual-examples/purchases.xlsx\"\n\n# Lag eksempeldata\npurchases = data.frame(A = runif(10), B= runif(10), C=runif(10))\n\n# Skriv til xlsx\nopenxlsx::write.xlsx(purchases, \n                     file = file_path,\n                     rowNames = FALSE,\n                     showNA = FALSE,\n                     overwrite=T)\n\n\n\n\n\n\n\nLese inn filer\nUnder finner du eksempler pÃ¥ hvordan du kan lese inn data til en Jupyter Notebooks pÃ¥ Dapla.\n\nParquet\n\nPython \n\n\n\n\nnotebook\n\nimport dapla as dp\n\n# Set path to folder\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/datadoc/brukertest/10/sykefratot/klargjorte_data/person_testdata_p2021_v1.parquet\"\n\n# Read path into pandas dataframe \ndp.read_pandas(gcs_path= file_path,\n               file_format = \"parquet\",\n               columns = None,)\n\nSom vi sÃ¥ med write_pandas sÃ¥ er file_format default satt til parquet, og default for columns = None, sÃ¥ vi kunne ogsÃ¥ ha skrevet det slik:\ndp.read_pandas(gcs_path= file_path)\ncolumns-argumentet er en liste med kolonnenavn som vi Ã¸nsker Ã¥ lese inn. Hvis vi ikke spesifiserer noen kolonner sÃ¥ vil alle kolonnene leses inn.\n\n\nFunksjonen read_parquet fra pakken arrow kan brukes til Ã¥ lese inn parquet-filer i R.\nHer er et eksempel av Ã¥ lese inn parquet-filen â€œperson_testdata_p2021_v1.parquetâ€:\n\n\nnotebook\n\nlibrary(arrow)\n\nfile_path = \"/buckets/produkt/datadoc/brukertest/10/sykefratot/klargjorte_data/person_testdata_p2021_v1.parquet\"\n\nperson_testdata &lt;- arrow::read_parquet(file_path)\n\nVi kan ogsÃ¥ filtrere hvilke variabler vi Ã¸nsker Ã¥ lese inn ved Ã¥ spesifisere parameter col_select. For eksempel:\nperson_testdata &lt;- arrow::read_parquet(file_path,\n                                       col_select = c(\"fnr\", \"sivilstand\"))\n\n\n\nKartdata lagret som .parquet kan leses inn ved Ã¥ kombinere funksjonen [open_dataset](https://arrow.apache.org/docs/r/reference/open_dataset.html) fra pakken [arrow](https://cran.r-project.org/web/packages/arrow/index.html) og [read_sf_dataset](https://rdrr.io/cran/sfarrow/man/read_sf_dataset.html) fra pakken [sfarrow](https://wcjochem.github.io/sfarrow/articles/example_sfarrow.html).\n\n```r\nlibrary(arrow)\nlibrary(sfarrow)\nlibrary(tidyverse)\n\ndata &lt;- arrow::open_dataset(\"/buckets/produkt/GIS/Kart/2023/ABAS_grunnkrets_flate_2023/ABAS_grunnkrets_flate_2023.parquet\") %&gt;%\n  dplyr::filter(KOMMUNENR == \"0301\") %&gt;%\n  sfarrow::read_sf_dataset()\n\n\n\n\n\nTekstfiler\nKommer mer snart. Python-koden under bygger pÃ¥ eksempelet over.\n\nPython \n\n\n\n\nnotebook\n\nimport dapla as dp\n\n# Path to write to\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-manual-examples/test.json\"\n\n# Read in json-file from dapla-storage\ndf = dp.read_pandas(gcs_path = file_path,\n               file_format = \"json\")\n\n\n\n\n\nnotebook\n\n# Filsti\nfile_path = \"/buckets/produkt/dapla-manual-examples/purchases.csv\"\n\n# Lese inn CSV-fil\ndt_1987 &lt;- read.csv2(file_path)\n\nFor Ã¥ lese inn en json-fil kan skrive fÃ¸lgende:\n\n\nnotebook\n\nlibrary(jsonlite)\n\n# Filsti\nfile_path = \"/buckets/produkt/dapla-manual-examples/test.json\"\n\n# Lese inn JSON-fil\ndata &lt;- jsonlite::fromJSON(file_path)\n\n\n\n\n\n\nxlsx\n\nPython \n\n\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import AuthClient\n\n# Hent token\ntoken = AuthClient.fetch_google_credentials()\n\n# Les inn fil\ndf = pd.read_excel(\"gs://ssb-dapla-felles-data-produkt-prod/dapla-manual-examples/test.xlsx\",\n    storage_options={\"token\": token})\n\n\n\nXLSX-filer kan lese inn med funksjonen read.xlsx fra pakken openxlsx.\n\n\nnotebook\n\nlibrary(openxlsx)\n\nfile_path = \"/buckets/produkt/dapla-manual-examples/purchases.xlsx\"\n\ndata &lt;- openxlsx::read.xlsx(file_path)\n\n\n\n\n\n\nSAS\nHer er et eksempel pÃ¥ hvordan man leser inn en sas7bdat-fil pÃ¥ Dapla som har blitt generert i prodsonen.\n\nPython \n\n\n\n\nnotebook\n\nimport dapla as dp\n\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-manual-examples/statbank_ledstill.sas7bdat\"\n\ndp.read_pandas(file_path, file_format=\"sas7bdat\", encoding=\"latin1\")\n\n\n\nFor Ã¥ lese sas7bdat-filer i R kan man bruke funksjonen read_sas fra pakken haven (som ligger i Â´tidyverseÂ´).\n\n\nnotebook\n\nlibrary(tidyverse)\n\n# Filsti\nfile_path = \"/buckets/produkt/dapla-manual-examples/statbank_ledstill.sas7bdat\"\n\ndata &lt;- haven::read_sas(file_path)\n\n\n\n\n\n\n\nSlette filer\nÃ… slette filer fra lagringsomrÃ¥det kan gjÃ¸res pÃ¥ flere mÃ¥ter. I kapitlet om sletting av data viste vi hvordan man gjÃ¸r det med pek-og-klikk i Google Cloud Console. Under ser du hvordan du kan slette filer med dapla-toolbelt og gcsfs eller fellesr.\n\nPython \n\n\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Skriv inn full filsti til filen som skal slettes\nfile_path = \"\"\n\nfs.rm(file_path)\n\n\n\nFunksjonen file.remove kan brukes til Ã¥ slette data pÃ¥ lagringsomrÃ¥det.\n\n\nnotebook\n\n# Skriv inn full filsti til filen som skal slettes\nfile_path = \"\"\n\nfile.remove(file_path)\n\n\n\n\n\n\nKopiere filer\nÃ… kopiere filer mellom mapper pÃ¥ et Linux-filsystem innebÃ¦rer som regel bruke cp-kommandoen. PÃ¥ Dapla er det ikke sÃ¥ mye forskjell. Vi bruker en ligende tilnÃ¦rming nÃ¥ vi skal kopiere mellom bÃ¸tter eller mapper pÃ¥ lagringsomrÃ¥det til SSB. Under ser du hvordan du kan kopiere en fil fra en mappe til en annen.\n\nPython \n\n\nLa oss begynne med et eksempel der vi kopierer en fil fra en mappe til en annen i samme bÃ¸tte.\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Path to folders\nbucket = \"gs://ssb-dapla-felles-data-produkt-prod\"\nfrom_folder = \"felles/veiledning/python/eksempler/purchases\"\nto_folder = \"felles/veiledning/python/eksempler\"\n\n# Copy file\nfs.cp(f\"{bucket}/{from_folder}/data.parquet\",\n      f\"{bucket}/{to_folder}/data_copy.parquet\")\n\nDet fungerer ogsÃ¥ for Ã¥ kopiere filer mellom bÃ¸tter.\nEt annet scenario vi ofte vil stÃ¸te pÃ¥ er at vi Ã¸nsker Ã¥ kopiere en fil fra vÃ¥rt Jupyter-filsystem til en mappe pÃ¥ lagringsomrÃ¥det. Her kan vi bruke fs.put-metoden.\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Create a new file in your home directory called test.txt\nwith open('/home/jovyan/test.txt', 'w') as f:\n    f.write('Create a new text file!')\n\n#Path to folder\nbucket = \"gs://ssb-dapla-felles-data-produkt-prod\"\nfolder = \"felles/veiledning/python/eksempler\"\n\n# Copy file from local to remote file system\nfs.put(lpath=f\"/home/jovyan/test.txt\", rpath=f\"{bucket}/{folder}/test.txt\")\n\nÃ˜nsker vi Ã¥ kopiere en hel mappe fra lagringsomrÃ¥det til Jupyter-filsystemet, kan vi bruke fs.get-metoden, med opsjonen recursive=True.\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Copy file\nfs.get(&lt;from_bucket&gt;,\n      \"/home/jovyan/sesongjustering/\",\n      recursive=True)\n\n\n\nKommer snart\n\n\n\n\n\nFlytte filer\n\nPython \n\n\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\nbucket = \"gs://ssb-dapla-felles-data-produkt-prod\"\nfrom_folder = \"felles/veiledning/python/eksempler/purchases\"\nto_folder = \"felles/veiledning/python/eksempler\"\n\nfs.mv(f\"{bucket}/{from_folder}/data.parquet\", f\"{bucket}/{to_folder}/data.parquet\")\n\n\n\nKommer snart\n\n\n\n\n\nOpprette mapper\nSelv om bÃ¸tter ikke har mapper med en hierarkisk struktur slik man er kjent med fra klassike filsystemer, sÃ¥ kan man opprette det som ser ut som mapper i objektnavnet. I realiteten blir bare / oppfattet som en del av navnet pÃ¥ objektet. Skulle du likevel Ã¸nske Ã¥ opprette dette sÃ¥ kan du gjÃ¸re det fÃ¸lgende mÃ¥te:\n\nPython \n\n\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n#Path to folder\nbucket = \"gs://ssb-dapla-felles-data-produkt-prod\"\nfolder = \"felles/veiledning/python/eksempler\"\n\n# Create folder\nfs.touch(f\"{bucket}/{folder}/testmappe/\")\n\n\n\nKommer snart",
    "crumbs": [
      "Manual",
      "Data"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-data.html#footnotes",
    "href": "statistikkere/jobbe-med-data.html#footnotes",
    "title": "Jobbe med data",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI de tidligere systemene pÃ¥ bakken sÃ¥ var det ikke nÃ¸dvendig med autentisering mellom kodemiljÃ¸ og datalagringenâ†©ï¸",
    "crumbs": [
      "Manual",
      "Data"
    ]
  },
  {
    "objectID": "statistikkere/vardef-forvaltning.html",
    "href": "statistikkere/vardef-forvaltning.html",
    "title": "Vardef-forvaltning",
    "section": "",
    "text": "Warning\n\n\n\nDenne siden er under arbeid frem til Vardef er i produksjon.\nVardef-forvaltning er en tjeneste pÃ¥ Dapla Lab som lar brukere forvalte informasjon i Vardef med ferdiglagde notebooks i en instans av Jupyterlab. Tjenesten kommer med Python, dapla-toolbelt-metadata og tilhÃ¸rende avhengigheter ferdig installert. I tillegg er det opprettet en kernel med navn variable_definitions som lar brukerne jobbe med Vardef uten noen ytterligere oppsett.\nNotebooksâ€™ene som ligger inne i tjenesten inneholder dokumentasjon for vanlige brukerflyter i Vardef. Les mer notebooksâ€™ene under Vardef i dapla-toolbelt-metadata.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vardef-forvaltning"
    ]
  },
  {
    "objectID": "statistikkere/vardef-forvaltning.html#forberedelser",
    "href": "statistikkere/vardef-forvaltning.html#forberedelser",
    "title": "Vardef-forvaltning",
    "section": "Forberedelser",
    "text": "Forberedelser\nFÃ¸r man starter Vardef-forvaltning bÃ¸r man ha lest kapitlet om Dapla Lab og gjort seg godt kjent med dokumentasjonen til Vardef. Deretter gjÃ¸r du fÃ¸lgende:\n\nLogg deg inn pÃ¥ Dapla Lab\nUnder Tjenestekatalog trykker du pÃ¥ Start-knappen for Vardef-forvaltning\nGi tjenesten et navn\nDefiner Ã¸nsket konfigurasjon\nTrykk Start nÃ¥r du er klar for Ã¥ starte tjenesten",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vardef-forvaltning"
    ]
  },
  {
    "objectID": "statistikkere/vardef-forvaltning.html#konfigurasjon",
    "href": "statistikkere/vardef-forvaltning.html#konfigurasjon",
    "title": "Vardef-forvaltning",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nKonfigurasjonen av Vardef-forvaltning er identisk som for Jupyter-tjenesten. Men siden variabeldefinisjoner forvaltes av team, sÃ¥ mÃ¥ man velge riktig team og tilgangsgruppe under Data i tjenestekonfigurasjonen fÃ¸r man starter tjenesten. F.eks. hvis dapla-felles-developers forvalter variabeldefinisjonen fnr, sÃ¥ mÃ¥ jeg velge dapla-felles-developers under Data fÃ¸r jeg starter tjenesten. Og tilsvarende hvis jeg Ã¸nsker Ã¥ opprette en ny variabeldefinisjon, sÃ¥ mÃ¥ jeg representere det teamet jeg Ã¸nsker at skal forvalte variabeldefinisjonen fÃ¸r jeg oppretter den.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vardef-forvaltning"
    ]
  },
  {
    "objectID": "statistikkere/vardef-forvaltning.html#slette-tjenesten",
    "href": "statistikkere/vardef-forvaltning.html#slette-tjenesten",
    "title": "Vardef-forvaltning",
    "section": "Slette tjenesten",
    "text": "Slette tjenesten\nFor Ã¥ slette tjenesten kan man trykke pÃ¥ Slette-knappen i Dapla Lab under Mine tjenester. NÃ¥r man sletter en tjeneste sÃ¥ sletter man hele disken inne i tjenesten og frigjÃ¸r alle ressurser som er reservert. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vardef-forvaltning"
    ]
  },
  {
    "objectID": "statistikkere/vardef-forvaltning.html#pause-tjenesten",
    "href": "statistikkere/vardef-forvaltning.html#pause-tjenesten",
    "title": "Vardef-forvaltning",
    "section": "Pause tjenesten",
    "text": "Pause tjenesten\nMan kan pause tjenesten ved Ã¥ trykke pÃ¥ Pause-knappen i Dapla Lab under Mine tjenester. NÃ¥r man pauser sÃ¥ slettes alt pÃ¥den lokale disken som ikke er lagret under $HOME/work. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vardef-forvaltning"
    ]
  },
  {
    "objectID": "statistikkere/vardef-forvaltning.html#monitorering",
    "href": "statistikkere/vardef-forvaltning.html#monitorering",
    "title": "Vardef-forvaltning",
    "section": "Monitorering",
    "text": "Monitorering\n\n\nMan kan moniterere en instans av Jupyter ved Ã¥ trykke pÃ¥ Jupyter-teksten under Mine tjenester i Dapla Lab, slik som vist i FigurÂ 1.\nDenne funksjonaliteten er under arbeid og mer informasjon kommer snart.\n\n\n\n\n\n\n\n\n\nFigurÂ 1: Monitorering av Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vardef-forvaltning"
    ]
  },
  {
    "objectID": "statistikkere/index.html",
    "href": "statistikkere/index.html",
    "title": "Hurtigstart",
    "section": "",
    "text": "Velkommen til hurtigstart-siden! Her presenterer vi manualen og setter deg i gang pÃ¥ 1-2-3!\nDenne manualen tar sikte pÃ¥ Ã¥ gi SSB-ansatte mulighet til Ã¥ ta i bruk Dapla uten hjelp fra eksperter. Manualen inneholder artikler om blant annet kodesprÃ¥kene Python og R, Git, Dapla Lab, Metadata og mye mer. I tillegg har manualen en blogg og en nyhetsside.\nUnder hjelp-fanen Ã¸verst pÃ¥ nettsiden finner du blant annet siden FAQ hvor ofte stilte spÃ¸rsmÃ¥l besvares. I denne boken omtaler vi den gamle produksjonssonen, ofte kalt prodsonen, som bakke, og det nye skymiljÃ¸et Google Cloud som sky. Det er ikke helt presist men duger for formÃ¥lene i denne boken.\n\nLes omâ€¦\n\nDapla Nyheter - Hva skjer pÃ¥ Dapla om dagen?\nVi har vÃ¥r egen nyhetsside Dapla Nyheter. Bruk den for Ã¥ holde deg oppdatert pÃ¥ hva som skjer pÃ¥ Dapla!\n\n\nHva er Dapla Team?\nStatistikkproduksjon pÃ¥ Dapla foregÃ¥r med Dapla-team som midtpunktet. Vi anbefaler at alle begynner med Ã¥ lese artikkelen Hva er Dapla-team!\n\n\nHvor er dataene vÃ¥re? I bÃ¸tter!\nDataene vÃ¥re lagres pÃ¥ Google Cloud platform i det som kalles bÃ¸tter. BÃ¸ttene er basert pÃ¥ de obligatoriske datatilstandene. Les artikkelen Hva er bÃ¸tter? og artikkelen om datatilstander\n\n\nDapla lab - arbeidsbenken vÃ¥r\nDapla lab er der vi finner verktÃ¸y og tjenester som Jupyter, RStudio og Datadoc-editor. Les hovedartikkelen om Dapla-lab. Det finnes ogsÃ¥ artikler for hver tjeneste vi har i Dapla lab.\n\n\nGit, GitHub og malen vÃ¥r SSB-project\nGit og GitHub brukes for Ã¥ lagre (og versjonshÃ¥ndtere) koden vÃ¥r. SSB-project brukes som mal for GitHub-repoer og for Ã¥ hÃ¥ndtere Pythonpakker. Her finner du artikler om Git og GitHub, SSB-project (oversikt) og SSB-project for pakkehÃ¥ndtering i Pyton. For pakkehÃ¥ndtering i R brukes renv.\n\n\n\n\n\n\nVi trenger bidragsytere!\n\n\n\nDapla er i konstant utvikling og det er manualen og! Derfor trenger vi flere bidragsytere til Ã¥ fjerne utdatert informasjon, forbedre eksisterende artikler og skrive nye.\nKunne du tenkt deg Ã¥ bidra? Les om hvordan du kan bidra i denne artikkelen i appendiksen. Har du lyst til Ã¥ bidra, men er ikke helt sikker pÃ¥ hva du kan bidra med? Ta en titt pÃ¥ issues i GitHub-repoet.\n\n\nKommentarer og Ã¸nsker vedrÃ¸rende boken tas imot med Ã¥pne armer. Dette kan gjÃ¸res ved Ã¥ lage en issue i GitHub-repoet.\nGod fornÃ¸yelseğŸ˜",
    "crumbs": [
      "Manual",
      "Hurtigstart"
    ]
  },
  {
    "objectID": "statistikkere/metodebibliotek.html",
    "href": "statistikkere/metodebibliotek.html",
    "title": "Metodebiblioteket",
    "section": "",
    "text": "Metodebiblioteket er SSBs bibliotek for statistiske metode funksjoner. Biblioteket finner du her : https://statisticsnorway.github.io/ssb-metodebiblioteket/.\nMetodene er organisert som en liste av funksjoner skrevet i R eller Python. Funksjonene kan kjÃ¸res pÃ¥ bÃ¥de pÃ¥ Dapla og i bakke-miljÃ¸ene. Alle funksjoner er testet av Seksjon for Metode for bruk i produksjon av offisiell statistikk, og alle er brukt i minst ett produksjonslÃ¸p eller i SSBs interne metodekurs. Funksjonene er bÃ¥de utviklet internt og hentet fra godkjente eksterne pakker.\nMetodebiblioteket kan sÃ¸kes i enten via en generell liste eller gjennom steg i prosessmodellen.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Metodebiblioteket"
    ]
  },
  {
    "objectID": "statistikkere/jit.html",
    "href": "statistikkere/jit.html",
    "title": "Just-in-Time Access",
    "section": "",
    "text": "Just-in-Time Access (JIT) er en applikasjon der data-admins kan gi seg selv midlertidig tilgang til kildedata. PÃ¥ Dapla benyttes for dette for Ã¥ unngÃ¥ at data-admins har permanent tilgang til sensitive data, siden all kildedata er definert som sensitiv informasjon. data-admins mÃ¥ bruke JIT-applikasjonen for Ã¥ gi seg selv korte, begrunnede tilganger til kildedata ved behov. Den som er ansvarlig for teamet kan monitere i hvilken grad teamets data-admins benytter seg av denne muligheten.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Just-in-Time Access"
    ]
  },
  {
    "objectID": "statistikkere/jit.html#hvordan-bruke-jit",
    "href": "statistikkere/jit.html#hvordan-bruke-jit",
    "title": "Just-in-Time Access",
    "section": "Hvordan bruke JIT",
    "text": "Hvordan bruke JIT\nFor Ã¥ bruke JIT mÃ¥ du vÃ¦re data-admin for teamet som eier kildedataene du Ã¸nsker tilgang til. Du aktiverer tilganger i JIT-appen ved gjÃ¸re fÃ¸lgende:\n\nGÃ¥ inn pÃ¥ nettsiden https://jitaccess.dapla.ssb.no/1\nOppgi Prosjekt-id for prosjektet der kildedataene du Ã¸nsker tilgang til er lagret.\nHuk av hvilke tilgangsroller du Ã¸nsker aktivert. Se beskrivelse av roller i TabellÂ 1.\nVelg hvor lenge tilgangen du Ã¸nsker at tilgangen skal vare. Den kan maksimalt vare i 8 timer.\nOppgi en begrunnelse for at aktiverer tilgangen.\nTil slutt trykker du pÃ¥ Request access.\n\nTilgangen vil deretter vÃ¦re aktiv ila. noen minutter.\n\n\n\n\n\n\nHvordan bÃ¸r begrunnelsen skrives?\n\n\n\nPrÃ¸v Ã¥ gjÃ¸r begrunnelsen forstÃ¥elig for andre enn deg selv pÃ¥ det tidspunktet. Den som er ansvarlig for teamet skal kunne forstÃ¥ begrunnelsen nÃ¥r de ser pÃ¥ loggene. I tillegg vil sentralt i SSB monitere i hvilken grad Dapla-teamene benytter seg av tilgangene.\n\n\n\n\n\nTabellÂ 1: De mest relevante tilgangene som kan aktiveres i JIT-applikasjonen\n\n\n\n\n\n\n\n\n\nRoller\nHva?\n\n\n\n\nssb.buckets.list\nListe ut bÃ¸ttene i kildeprosjektet.\n\n\nstoragetransfer.admin\nSette opp jobb med Transfer Service i kildeprosjektet.\n\n\nssb.bucket.write\nLese og skrive til kildebÃ¸tta.\n\n\n\n\n\n\nSkal du sette opp en Transfer Service overfÃ¸ring med kildedata sÃ¥ mÃ¥ du aktivere ssb.bucket.write og storagetransfer.admin. SKal du skrive og lese fra et miljÃ¸ som Jupyter, sÃ¥ mÃ¥ du aktivere rollene ssb.bucket.write og ssb.buckets.list.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Just-in-Time Access"
    ]
  },
  {
    "objectID": "statistikkere/jit.html#footnotes",
    "href": "statistikkere/jit.html#footnotes",
    "title": "Just-in-Time Access",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nHvis du jobber fra hjemmekontor sÃ¥ mÃ¥ du vÃ¦re pÃ¥ VPN for Ã¥ fÃ¥ tilgang til JIT-appen.â†©ï¸",
    "crumbs": [
      "Manual",
      "Appendix",
      "Just-in-Time Access"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-model.html",
    "href": "statistikkere/datadoc-model.html",
    "title": "Datamodell",
    "section": "",
    "text": "Important\n\n\n\nDenne siden er under arbeid.\nI dette kapitlet beskrives informasjonselementene i Datadoc. Siden noen gjelder for datasett og andre gjelder for variabler, sÃ¥ er kapitlet delt inn etter disse.\nFor hvert informasjonselement angis det om informasjonen er obligatoriskâœ… for gitte datatilstander. Siden inndata ikke er en obligatorisk datatilstand, og kildedata ikke er mulig Ã¥ dokumentere enda, sÃ¥ angis de ikke. Men hvis man lagrer inndata i en statistikkproduksjon, sÃ¥ er de samme feltene obligatorisk som for klargjorte data.\nDet angis ogsÃ¥ hva det internasjonaleğŸŒ navnet er, som er navnet som faktisk benyttes i Datadoc-filene som genereres.\nNoen informasjonselementer er kun relevant for noen typer av data, og det er derfor kun obligatorisk hvis man har denne typen data. Dette angis med âš ï¸.\nâœ… = obligatorisk, âš ï¸ = hvis relevant, ğŸŒ = internasjonalt",
    "crumbs": [
      "Manual",
      "Metadata",
      "Datadoc",
      "Datamodell"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-model.html#datasett",
    "href": "statistikkere/datadoc-model.html#datasett",
    "title": "Datamodell",
    "section": "Datasett",
    "text": "Datasett\n\nNavn\nğŸŒ name\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nNavnet pÃ¥ datasettet. Navnet skal vÃ¦re forstÃ¥elig for mennesker (ikke kun forkortelser) slik at det er sÃ¸kbart. Navnet skal fylles ut pÃ¥ bokmÃ¥l eller nynorsk. Det er valgfritt om en ogsÃ¥ vil fylle ut pÃ¥ den andre norske mÃ¥lformen og engelsk.\n\n\n\nBeskrivelse\nğŸŒ description\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nEn kort beskrivelse av hva datasettet inneholder, f.eks.:\n\nInneholder data om bruk av helsetjenester (LevekÃ¥rsundersÃ¸kelsen).\n\n\n\n\nInneholder personopplysninger\nğŸŒ contains_personal_data\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nVelges hvis datasettet inneholder identifiserende personopplysninger, dvs. alle opplysninger som entydig kan knyttes til en fysisk person (f.eks. fÃ¸dselsnummer, ulike adresser og bankkontonummer).\nSe flere eksempler pÃ¥ personopplysninger i liste laget i PAPIS-prosjektet:\nPAPIS-Variabelliste.docx\nNÃ¦ringsdata og enkeltpersonforetak (ENK) blir ikke regnet som personopplysninger.\n\n\n\nVerdivurdering\nğŸŒ assessment\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nUtledes automatisk basert pÃ¥ datasettets datatilstand, og informasjon om datatilstand hentes fra filstien (se Navnestandarden). Verdivurdering settes med fÃ¸lgende regler:\n\nsensitiv -&gt; kildedata\nskjermet -&gt; inndata, klargjorte data og statistikkdata\nÃ¥pen -&gt; utdata\n\n\n\n\nBruksrestriksjon\nğŸŒ use_restriction\nklargjorte data âœ… | statistikkdata âœ…\nFylles ut dersom datasettet har bruksrestriksjoner. Datasett uten brukrestriksjoner trenger ikke Ã¥ fylle ut feltet. FÃ¸lgende verdier er tillatt for datasett med bruksrestriksjoner:\n\nSletting/anonymisering\nğŸŒ DELETION_ANONYMIZATION\n\n\nBehandlingsbegrensninger\nğŸŒ PROCESS_LIMITATIONS\n\n\nSekundÃ¦rbruksrestriksjoner\nğŸŒ SECONDARY_USE_RESTRICTIONS\n\n\n\n\nBruksrestriksjonsdato\nğŸŒ use_restriction_date\nklargjorte data âœ… | statistikkdata âœ…\nFylles kun ut dersom det er knyttet bruksrestriksjoner til datasettet, og det finnes en â€œtiltaksdatoâ€. En tiltaksdato kan f.eks. vÃ¦re at et datasett skal slettes eller anonymiseres pÃ¥ en gitt dato. Noen datasett med Bruksrestriksjon vil ikke ha en slik dato, f.eks. vil en behandlingsbegrensning normalt vÃ¦re permanent/tidsuavhengig, og da skal ikke dette feltet fylles ut.\nDato fylles ut pÃ¥ ISO 8601 Date and time format. F.eks. 2024-12-31 eller 2022-09-27 18:00:00.000.\n\n\n\nDatatilstand\nğŸŒ dataset_state\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nUtledes automatisk fra filstien hvis datasettet som dokumenteres er lagret iht Navnestandarden, men kan overstyres. Datasettets datatilstand er en av fÃ¸lgende:\n\nkildedata\ninndata\nklargjorte data\nstatistikk\nutdata\n\n\n\n\nStatus\nğŸŒ dataset_status\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nAngir hvor metadataene er i livssyklusen. FÃ¸lgende kategorier kan velges:\n\nUtkast\nğŸŒ DRAFT\nArbeid med data og metadata pÃ¥gÃ¥r, dvs. ikke delt internt eller eksternt.\n\n\nIntern\nğŸŒ INTERNAL\nMetadata er godkjent for intern bruk, men data med verdivurdering â€œskjermetâ€ kan kun brukes/deles med ansatte med tjenstlig behov for tilgang\n\n\nEkstern\nğŸŒ EXTERNAL\nMetadata er godkjent for bÃ¥de intern og eksternt bruk, men data kan kun deles med alle eksterne hvis datatilstanden er â€œutdataâ€ (verdivurdering=Ã¥pen). Andre datatilstander kan deles f.eks. med forskere etter sÃ¸knad og godkjenning.\n\n\nUtgÃ¥tt\nğŸŒ DEPRECATED\nUtgÃ¥tt, avsluttet eller erstattet av noe annet.\n\n\n\n\nEnhetstype\nğŸŒ unit_type\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nHer fyller en inn enhetstypen (Â«objektetÂ») som datasettet inneholder informasjon om. Eksempler pÃ¥ enhetstyper er Â«personÂ», Â«foretakÂ» og Â«bruksenhetÂ».\nKodeliste i Klass for tillatte verdier.\n\n\n\n\n\n\nFremtidig funksjonalitet i Datadoc-editor\n\n\n\nDersom datasettet inneholder informasjon om flere enhetstyper, kan disse enhetstypene dokumenteres for de aktuelle variablene under Variabler-fanen. Vi anbefaler da at den hyppigst forekommende enhetstypen velges i dette feltet. I noen tilfeller vil dataene vÃ¦re aggregerte allerede nÃ¥r de kommer til SSB. Da vil det ikke vÃ¦re logisk Ã¥ snakke om enhetstyper, og en kan velge kategorien Â«aggregertÂ» som verdi i Enhetstype-feltet. Denne kategorien vil ogsÃ¥ ofte vÃ¦re det aktuelle valget for datatilstandene statistikk og utdata da disse ofte vil bestÃ¥ av aggregerte data. Et unntak her vil vÃ¦re Kostra, som ogsÃ¥ har aggregerte data, men der disse er aggregert pÃ¥ kommune- eller fylkes nivÃ¥, og det vil da vÃ¦re naturlig Ã¥ bruke enhetstypen Â«kommune(forvaltning)Â» eller Â«fylke(forvaltning)Â».\n\n\n\n\n\nPopulasjon\nğŸŒ population_description\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nHer skal en skrive inn populasjonen som datasettet dekker(fritekst). En populasjon sier hvilken enhetstype(er) som finnes i datasettet, geografisk dekningsomrÃ¥de og tidsperiode, f.eks. alle (bosatte) personer i Norge per 31.12.2024 eller alle foretak i Oslo per 01.03.2025.\nDersom noen av variablene i datasettet har en annen populasjon, kan dette dokumenteres for de aktuelle variablene under Variabler-fanen.\n\n\n\nVersjon\nğŸŒ version\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nDersom datasettet fÃ¸lger navnestandarden, settes versjonsnummeret automatisk. Dersom datasettet ikke fÃ¸lger navnestandarden (det er f.eks. ikke et krav at kildedata skal gjÃ¸re det), settes det manuelt. Les mer om versjoner i Dapla-manualen.\n\n\n\nVersjonsbeskrivelse\nğŸŒ version_description\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nHer beskriver man kort Ã¥rsaken til at en ny versjon ble laget. For versjon 1 kan en bare skrive Â«Opprinnelig versjonÂ».\n\n\n\nInneholder data f.o.m.\nğŸŒ contains_data_from\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nDersom datasettet fÃ¸lger navnestandarden, settes denne datoen automatisk. Dersom datasettet ikke fÃ¸lger navnestandarden (det er f.eks. ikke et krav at kildedata skal gjÃ¸re det), settes denne datoen manuelt. Dersom variablene i datasettet inneholder data med ulike startdatoer, settes den eldste datoen her. Under variabler-fanen kan en sette korrekt Â«Inneholder data f.o.m.Â» for variabler som avviker fra datoen som settes her.\n\n\n\nInneholder data t.o.m.\nğŸŒ contains_data_until\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nDersom datasettet fÃ¸lger navnestandarden, settes denne datoen automatisk. Dersom datasettet ikke fÃ¸lger navnestandarden (det er f.eks. ikke et krav at kildedata skal gjÃ¸re det), settes denne datoen manuelt. Dersom variablene i datasettet inneholder data med ulike sluttdatoer, settes den nyeste datoen her. Under variabler-fanen kan en sette korrekt Â«Inneholder data t.o.m.Â» for variabler som avviker fra datoen som settes her.\n\n\n\nDatakilde\nğŸŒ data_source\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nHer oppgis kilden til dataene (pÃ¥ etats-/organisasjonsnivÃ¥). Denne velges fra menyen. Dersom variablene i datasettet har ulike datakilder, kan disse presiseres for de aktuelle variablene under Variabler-fanen. I dette tilfellet velges den hyppigst forekommende datakilden her.\nKlass-kodelist for tillatte verdier.\n\n\n\nTemporalitetstype\nğŸŒ TEMPORALITY_TYPE\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nTemporalitetstypen sier noe om tidsdimensjonen i datasettet. FÃ¸lgende verdier er tillatt:\n\nFast\nğŸŒ FIXED\nData med verdier som ikke endres over tid, f.eks. fÃ¸dselsdato.\n\n\nTverrsnitt\nğŸŒ STATUS\nÂ«TverrsnittÂ» er data som er mÃ¥lt pÃ¥ et bestemt tidspunkt.\n\n\nAkkumulert\nğŸŒ ACCUMULATED\ner data som er samlet over en viss tidsperiode, f.eks. inntekt gjennom et Ã¥r.\n\n\nHendelse/forlÃ¸p\nğŸŒ EVENT\nÂ«Hendelse/forlÃ¸pÂ» registrerer tidspunkt og tidsperiode for ulike hendelser/tilstander, f.eks. (skifte av) bosted.\nDersom variablene i datasettet har ulike temporalitetstyper, kan disse presiseres for de aktuelle variablene under Variabler-fanen. I dette tilfellet, velges den hyppigst forekommende temporalitetstypen her.\n\n\n\n\nStatistikkomrÃ¥det\nğŸŒ subject_field\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nHer skal det primÃ¦re statistikkomrÃ¥det som datasettet tilhÃ¸rer, velges fra menyen.\n\n\n\nNÃ¸kkelord\nğŸŒ keyword\nklargjorte data âš ï¸ | statistikkdata âš ï¸ | utdata âš ï¸\nHer kan en legge inn nÃ¸kkelord som beskriver datasettet, og som dermed kan brukes i sÃ¸k. NÃ¸kkelordene mÃ¥ legges inn som kommaseparert streng, f.eks. Â«befolkning, skatt, arbeidsledighetÂ».\n\n\n\nGeografisk dekningsomrÃ¥de\nğŸŒ spatial_coverage_description\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nOppgi datasettets geografiske dekningsomrÃ¥de. Norge er default.",
    "crumbs": [
      "Manual",
      "Metadata",
      "Datadoc",
      "Datamodell"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-model.html#variabler",
    "href": "statistikkere/datadoc-model.html#variabler",
    "title": "Datamodell",
    "section": "Variabler",
    "text": "Variabler\n\nNavn\nğŸŒ name\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nNavnet pÃ¥ variabelen. Navnet skal vÃ¦re forstÃ¥elig for mennesker (ikke kortnavn/teknisk navn) slik at det er sÃ¸kbart. Navnet skal fylles ut pÃ¥ bokmÃ¥l eller nynorsk.\nEt eksempel pÃ¥ navn er Personidentifikator. Det tilhÃ¸rende kortnavnet (navnet i datasettet) vil vÃ¦re pers_id.\nDe fleste variabler skal dokumenters i Vardef, og dermed kunne pekes til fra Datadoc-editor. Men hvis datasettet inneholder noen variabler som kun brukes i dette datasettet, og dermed ikke skal gjenbrukes, trenger de ikke Ã¥ dokumenteres i Vardef . Det kan f.eks. dreie seg om variabler som brukes i en spesiell beregning eller kontroll. Disse dokumenteres da i Datadoc-editor. Navnet skal dokumenteres i dette feltet, mens definisjonen og eventuelt tilhÃ¸rende kodeverk skal dokumenteres hhv. i feltet Kommentar og Kodeverkets URI. (NÃ¥r en fyller ut Â«Definisjons URIÂ»-feltet, vil en ogsÃ¥ fÃ¥ lenke til ev. kodeverk via Vardef-variabelen).\n\n\n\n\n\n\nSammenkobling med Vardef\n\n\n\nNÃ¥r Vardef kommer i produksjon mÃ¥ Navn-feltet bare fylles ut dersom Definisjons-URI-feltet ikke er utfylt. Dersom det er utfylt, vil navnefeltet automatisk fylles med navnet til Vardef-variabelen som det lenkes til. Det er ogsÃ¥ mulig Ã¥ endre Vardef-navnet i Datadoc-editor dersom det er relevant.\n\n\n\n\n\nDefinisjons-URI\nğŸŒ definition_uri\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nHer oppgis lenken til tilhÃ¸rende variabel i Vardef.\n\n\n\n\n\n\nSammenkobling med Vardef\n\n\n\nNÃ¥r Vardef kommer i produksjon vil Datadoc-editor bruke kortnavnet i datasettet til Ã¥ sÃ¸ke etter variabler med samme kortnavn i Vardef, og brukeren kan velge Ã¥ benytte denne informasjonen eller ikke for sine data.\n\n\n\n\n\nEr personopplysning\nğŸŒ is_personal_data\nklargjorte data âœ…\nAngir om variabelen er en personopplysning eller ikke, og hvis det er det, er den anonymisert eller ikke. FÃ¸lgende verdier er tillatt:\n\nIkke personopplysning (default)\nğŸŒ NOT_PERSONAL_DATA\nVariabelforekomsten er ikke en personopplysning\n\n\nPseudonymisert/kryptert personopplysning\nğŸŒ PSEUDONYMISED_ENCRYPTED_PERSONAL_DATA\nVariabelforekomsten er en personopplysning som er pseudonymisert/kryptert. Eksempelvis pseudonymisert fÃ¸dselsnummer eller kryptert person-navn\n\n\nIkke-pseudonymisert/ikke-kryptert personopplysning\nğŸŒ NON_PSEUDONYMISED_ENCRYPTED_PERSONAL_DATA\nVariabelforekomsten er en personopplysning som ikke er pseudonymisert/kryptert. Eksempelvis fÃ¸dselsnummer eller navn i klartekst.\nPersonopplysninger er alle opplysninger som entydig kan knyttes til en fysisk person (f.eks. fÃ¸dselsnummer, ulike adresser og bankkontonummer). Se flere eksempler pÃ¥ personopplysninger i liste laget av PAPIS-prosjektet:\nPAPIS - Variabelliste.docx\nNÃ¦ringsdata og enkeltpersonforetak (ENK) blir ikke regnet som personopplysninger.\n\n\n\n\nMÃ¥leenhet\nğŸŒ measurement_unit\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nDersom variabelen er kvantitativ, skal den ha en mÃ¥leenhet, f.eks. Â«kronerÂ» eller Â«tonnÂ».\nKlass-kodeliste for tillatte mÃ¥leeneheter.\n\n\n\nMultiplikasjonsfaktor\nğŸŒ multiplication_factor\nMultiplikasjonsfaktor er den numeriske verdien som multipliseres med mÃ¥leenheten, f.eks. hvis det er store tall i datasettet. En kan f.eks. velge multiplikasjonsfaktor 1000, og mÃ¥leenhet Â«kronerÂ» slik at verdiene vises i 1000 kroner.\n\n\n\nVariabelens rolle\nğŸŒ variable_role\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nHer skal en oppgi variabelens rolle i datasettet (velge fra menyen). De ulike rollene er Â«identifikatorÂ» (identifiserer de ulike enhetene i et datasett, f.eks. fÃ¸dselsnummer som identifiserer personer og organisasjonsnummer som identifiserer foretak), Â«mÃ¥levariablerÂ» som beskriver det vi mÃ¥ler, dvs. egenskaper ( sivilstand eller omsetning), Â«startdatoÂ» (beskriver startdato for hendelser som har et forlÃ¸p, eller mÃ¥letidspunkt for tverrsnittdata), Â«stoppdatoÂ» (beskriver stoppdato for hendelser som har et forlÃ¸p) og Â«attributtÂ» (som kan brukes dersom en Ã¸nsker Ã¥ utvide datasettet med informasjon knyttet til gitte variabler, f.eks. vedrÃ¸rende datakvalitet eller editering).\n\n\n\nKodeverkets URI\nğŸŒ classification_uri\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nLenke (URI) til gyldig kodeverk (klassifikasjon eller kodeliste) i Klass. Dette feltet vil sjelden benyttes siden variabelen i all hovedsak vil knyttes til tilhÃ¸rende kodeverk via relevant variabeldefinisjon i Vardef. Unntaksvis kan den imidlertid knyttes direkte til Klass via dette feltet i tilfeller der variabelen ikke defineres i Vardef (se nÃ¦rmere info under feltet Â«NavnÂ»). Via dette feltet kan en ogsÃ¥ lenke til en variant av et kodeverk dersom en Ã¸nsker Ã¥ spesifisere kodeverket som er knyttet til variabeldefinisjonen i Vardef (eksempelvis variabelen â€œnÃ¦ringâ€ som i Vardef er knyttet til Standard for nÃ¦ringsgruppering (SN). For variabelen i datasettet kan en da f.eks. lenke til en variant av SN som kun inkluderer nÃ¦ringene som er aktuelle for egen statistikk, hvis dette er Ã¸nskelig).\n\n\n\nKommentar\nğŸŒ comment\nKommentar-feltet kan brukes pÃ¥ to mÃ¥ter. Den fÃ¸rste er at en kan legge inn ytterligere informasjon om variabelen, f.eks. hvis en Ã¸nsker Ã¥ utdype noe i definisjonen fra Vardef. Den andre mÃ¥ten er knyttet til de sjeldne tilfellene der en variabel ikke mÃ¥ dokumenteres i Vardef. En kan nemlig unnlate Ã¥ dokumentere en variabel i Vardef dersom den kun brukes i ett datasettet, dvs. at den aldri gjenbrukes (se mer info under feltet Â«NavnÂ»). I et slikt tilfelle mÃ¥ variabelen defineres i Kommentar-feltet. (NB variabler som kun brukes i ett datasett, kan godt dokumenters i Vardef i stedet for i kommentar-feltet dersom en Ã¸nsker det).\n\n\n\nDatakilde\nğŸŒ data_source\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nDatakilde settes vanligvis pÃ¥ datasettnivÃ¥, men dersom datasettet bestÃ¥r av variabler med ulike datakilder, kan datakilde pÃ¥ variabelnivÃ¥ velges i dette feltet (pÃ¥ etats-og organisasjonsnivÃ¥, velges fra meny).\n\n\n\nTemporalitetstype\nğŸŒ temporality_type\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nTemporalitetstype settes vanligvis pÃ¥ datasettnivÃ¥, men dersom datasettet bestÃ¥r av variabler med ulike temporalitetstyper, kan temporalitetstype pÃ¥ variabelnivÃ¥ velges i dette feltet. Temporalitetstypen sier noe om tidsdimensjonen til variabelen. FÃ¸lgende er tillatte verdier:\n\nFast\nğŸŒ FIXED\nData med verdier som ikke endres over tid, f.eks. fÃ¸dselsdato.\n\n\nTverrsnitt\nğŸŒ STATUS\nÂ«TverrsnittÂ» er data som er mÃ¥lt pÃ¥ et bestemt tidspunkt.\n\n\nAkkumulert\nğŸŒ ACCUMULATED\nData som er samlet over en viss tidsperiode, f.eks. inntekt gjennom et Ã¥r.\n\n\nHendelse/forlÃ¸p\nğŸŒ EVENT\nÂ«Hendelse/forlÃ¸pÂ» registrerer tidspunkt og tidsperiode for ulike hendelser/tilstander, f.eks. (skifte av) bosted.\n\n\n\n\nPopulasjon\nğŸŒ population_description\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nPopulasjon settes vanligvis pÃ¥ datasettnivÃ¥, men dersom datasettet bestÃ¥r av variabler med ulike populasjoner, kan populasjonen dokumenteres pÃ¥ variabelnivÃ¥ i dette feltet.\nEn populasjon sier hvilken enhetstype(er) som finnes i datasettet, geografisk dekningsomrÃ¥de og tidsperiode, f.eks. alle (bosatte) personer i Norge per 31.12.2024 eller alle foretak i Oslo per 01.03.2025.\n\n\n\nFormat\nğŸŒ format\nDette feltet kan benyttes som en ytterligere presisering av datatype i tilfellene der det er relevant. En kan fylle inn verdienes format (fysisk format eller regulÃ¦rt uttrykk) i maskinlesbar form i forbindelse med validering, f.eks. ISO 8601 som datoformat.\n\n\n\nInneholder data f.o.m.\nğŸŒ contains_data_from\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nDenne datoen settes vanligvis pÃ¥ datasettenivÃ¥, men dersom variablene i datasettet inneholder data med ulike startdatoer, kan startdato dokumenteres pÃ¥ variabelnivÃ¥ i dette feltet.\n\n\n\nInneholder data t.o.m.\nğŸŒ contains_data_until\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nDenne datoen settes vanligvis pÃ¥ datasettenivÃ¥, men dersom variablene i datasettet inneholder data med ulike sluttdatoer, kan sluttdato dokumenteres pÃ¥ variabelnivÃ¥ i dette feltet.\n\n\n\nDatatype\nğŸŒ data_type\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nHer dokumenterer en datatype ved Ã¥ velge en av fÃ¸lgende kategorier i menyen: Â«tekstÂ», Â«heltallÂ», Â«desimaltallÂ»,Â» datotidÂ» eller Â«boolskÂ» (sant/usant). Dersom variabelen er knyttet til et kodeverk i Klass, benyttes verdien Â«tekstÂ».\n\n\n\nDataelementsti\nğŸŒ data_element_path\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nDenne skal kun brukes for hierarkiske datasett (JSON) hvor det er nÃ¸dvendig Ã¥ oppgi sti til dataelementet. Bruk â€œdot-notasjonâ€ (JsonPath- lignende syntaks) til Ã¥ peke til variabelen (dataelementet). Dette er en generisk/teknologinÃ¸ytral mÃ¥te Ã¥ peke til elementer i bÃ¥de JSON, XML og andre hierarkiske datastrukturer.\n\n\n\nUgyldige verdier\nğŸŒ invalid_value_description\nDette feltet er et fritekstfelt, og brukes til Ã¥ beskrive ugyldige verdier som inngÃ¥r i variabelen. Et eksempel kan vÃ¦re variabelen Â«organisasjonsnummerÂ» hvis en vet at noen av verdiene knyttet til enpersonsforetak mangler et siffer i fÃ¸dselsnummeret eller egentlig er passnummer.\n\n\n\nKortnavn\nğŸŒ short_name\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nDette er navnet som brukes pÃ¥ variabelen i datasettet, denne maskingenereres i Datadoc-editor.\n\n\n\nId\nğŸŒ id\nklargjorte data âœ… | statistikkdata âœ… | utdata âœ…\nDette er en unik SSB-identifikator for variabelen i datasettet. Denne maskingenereres i Datadoc.",
    "crumbs": [
      "Manual",
      "Metadata",
      "Datadoc",
      "Datamodell"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-model.html#pseudonymisering",
    "href": "statistikkere/datadoc-model.html#pseudonymisering",
    "title": "Datamodell",
    "section": "Pseudonymisering",
    "text": "Pseudonymisering\nKommer snart.",
    "crumbs": [
      "Manual",
      "Metadata",
      "Datadoc",
      "Datamodell"
    ]
  },
  {
    "objectID": "statistikkere/renv.html",
    "href": "statistikkere/renv.html",
    "title": "PakkehÃ¥ndtering i R - renv",
    "section": "",
    "text": "R-pakken, renv, er et verktÃ¸y som lar oss opprette et miljÃ¸ for R og installere pakker. Det er anbefalt Ã¥ bruke renv for Ã¥ sikre at alle som jobber med prosjektet har samme versjon av pakkene. I tillegg er det enkelt Ã¥ dele prosjektet med andre.\nOppretting av renv-miljÃ¸ og installering av pakker mÃ¥ gjÃ¸res fra terminal for Ã¥ fÃ¥ riktig oppsett. For Ã¥ starte R i terminalen i Jupyter/DAPLA:\n\nÃ…pne en terminal fra Launcher\nStÃ¥ i mappen der du vil aktivere det virtuelle miljÃ¸et/installere pakker, dvs prosjekt mappen.\nStarte R ved Ã¥ skrive in R\n\n\n\n\n\n\n\nStarte renv i et eksisterende prosjekt\nFor Ã¥ installere dine egne R-pakker mÃ¥ du opprette et renv-miljÃ¸ med renv. Dette kan gjÃ¸res ved Ã¥ starte R i terminalen (se over) og skrive:\n\n\nterminal\n\nrenv::init()\n\n\n\n\n\n\nKommandoen aktiverer et renv-miljÃ¸ i mappen du stÃ¥r i. Rent praktisk vil det si at du fikk fÃ¸lgende filer/mapper i mappen din:\n\nrenv.lock\nEn fil som inneholder versjoner av alle pakker du benytter i koden din.\n.Rprofile En fil som inneholder informasjon om oppsetting av miljÃ¸.\nrenv\nMappe som inneholder alle pakkene du installerer.\nrenv/activate.R En fil som aktiverer renv miljÃ¸et for et prosjekt.\n\n\n\nKobler en .R eller notebook fil til et renv-miljÃ¸\nFor Ã¥ ta i bruk et renv-miljÃ¸ mÃ¥ det aktiveres ved starten av koden som skal kjÃ¸res. I JupyterLab nÃ¥r du Ã¥pner en .R-fil som en notebook mÃ¥ renv-miljÃ¸et aktiveres ved:\n\n\nnotebook\n\nrenv::autoload()\n\nDeretter kan du benytte pakker som er installerte (se neste avsnitt) og funksjoner ved library() osv. Funkjsonen renv::autoload() vil lete etter filene i prosjektet for Ã¥ aktivere miljÃ¸et, bÃ¥de i prosjekt-mappen og i foreldre-mapper.\n\n\n\n\n\n\n\nInstallering av pakker\nPakker kan installeres fra R pÃ¥ terminalen eller i en notebook/.R fil. Vi installerer pakker med funksjonen renv::install(). For eksempel, for Ã¥ installere pakken PxWebApiData:\n\n\nnotebook\n\nrenv::install(\"PxWebApiData\")\n\n\n\n\n\n\nFor Ã¥ installere R-pakker som ligger pÃ¥ â€˜statisticsnorwayâ€™ omrÃ¥det pÃ¥ github mÃ¥ det spesifiseres foran pakkenavnet:\n\n\nnotebook\n\nrenv::install(\"statisticsnorway/ssb-klassr\")\n\n\n\n\n\n\nFor Ã¥ installere en spesifikk versjon av en pakke kan du spesifisere dette med installering med @ og versjonsnummer. For eksempel Ã¥ installere PxWEbApiData versjon 0.4.0:\n\n\nterminal\n\nrenv::install(\"PxWebApiData@0.4.0\")\n\nFor Ã¥ lagre versjonsnummer av de nye pakkene som er installerte til renv.lock filen, kjÃ¸r:\n\n\nnotebook\n\n`renv::snapshot()`\n\n\n\nDele prosjektet og renv-miljÃ¸et med andre\nFÃ¸r du deler prosjektet forsikre det om at renv.lock-filen er oppdatert. Dette kan gjÃ¸res ved Ã¥ kjÃ¸re:\n\n\nnotebook\n\nrenv::snapshot()\n\nFor at en pakke skal lagres i renv.lock mÃ¥ pakken benyttes ved library() et sted i prosjektet (pÃ¥ en .R eller .ipynb fil).\nFor Ã¥ dele renv-miljÃ¸et som en del av prosjektet skal fÃ¸lgende filene vÃ¦re pÃ¥ github: renv.lock, .Rprofile og renv/activate.R\n\n\nTa i bruk et prosjekt og renv-miljÃ¸ fra andre\nHvis prosjektet er opprettet av noen andre, og har blitt delt med deg, kan alle pakkene i prosjektet installeres samtidig. Clone repository og start deretter R i terminalen. KjÃ¸r fÃ¸lgende for Ã¥ installere alle nÃ¸dvendige pakker:\n\n\nterminal\n\nrenv:restore()\n\n\n\n\n\n\n\n\nAvinstallering\nIndividuelle pakker kan fjernes fra renv-miljÃ¸et ved renv::remove()-funksjonen. For eksempel:\n\n\nterminal\n\nrenv::remove(\"PxWebApiData\")\n\nFor Ã¥ fjerne fra renv.lock-filen ogsÃ¥ mÃ¥ du ta en snapshot() etterpÃ¥.\n\n\nterminal\n\nrenv::snapshot()\n\nEn annen nyttig funksjon er renv::clean(). Dette fjerner alle pakker fra library som ikke er i bruk\n\n\nterminal\n\nrenv::clean()\n\nIgjen mÃ¥ du ta en snapshot() for at endringer skal lagres pÃ¥ renv.lock-filen\n\n\nOppgradere pakker\nFor Ã¥ oppgradere en pakke kan du bruke renv::update(). For eksempel, for Ã¥ oppgradere PxWebApiData skriv:\n\n\nterminal\n\nrenv::update(\"PxWebApiData\")\n\nHusk Ã¥ ta en snapshot() etterpÃ¥ for Ã¥ lagre endringer til renv.lock-filen. Det betyr at du og andre kan gjenskape miljÃ¸et pÃ¥ nytt.\n\n\nterminal\n\nrenv::snapshot()\n\n\n\nOppgradering av R\nI Jupyterlab pÃ¥ Dapla og i produksjonssone vil versjonen av R oppgraderes jevnlig. Dette er fordi operativsystemet og programmer som R er avhengig skal holdes oppdatert. For at R skal fungere optimalt mÃ¥ det oppgraderes ofte. Dette skaper noen utfordringer for renv-miljÃ¸er som er avhengig av en spesifikk versjon av R.\nHvis du plutselig fÃ¥r en feilmelding ved oppstart eller nÃ¥r du kjÃ¸re renv::autoload(), om at R-versjonen er forskjellig fra det den som e r oppgitt lock-filen, har det trolig vÃ¦rt en oppgradering av R siden sist noen jobbet med koden. FÃ¸lg denne oppskriften for Ã¥ lÃ¸se opp i problemene:\nBenytt prosjekt-biblioteket ved Ã¥ kjÃ¸re:\n\nStart R i terminal.\nOppgrader versjon av renv ved:\n\n\n\nterminal\n\nrenv::upgrade()\n\n\nOppgrader alle pakkene ved:\n\n\n\nterminal\n\nrenv::hydrate(update = \"all\")\n\n\nLagre renv.lock filen:\n\n\n\nterminal\n\nrenv::snapshot()\n\n\n\n\n\n\nFor mer informasjon kan du lese denne artikkelen om oppdatering av renv her\n\n\nTa det lang tid til Ã¥ starte et renv-miljÃ¸?\nVed Ã¥ aktivere et renv-miljÃ¸ ved renv::autoload() sÃ¸ker R i alle programfiler for Ã¥ sjekke at alle pakker som benyttes er installerte. Hvis du har en veldig stor repository kan dette ta en lang tid. En lÃ¸sning er Ã¥ opprette en .renvignore fil som spesifisere hvilke filer skal ignoreres. For eksempel om alle R kode ligger i .R filer kan .renvignore inneholder *.ipynb (ignorere all notebook filer).\n\n\n\n\n\n\n\nFlytting mellom jupyter pÃ¥ bakken og Dapla\nPakker installeres fra ulike sted nÃ¥r vi jobber pÃ¥ Dapla vs jupyter pÃ¥ bakken. For Ã¥ bruke et renv-miljÃ¸ i en repo som var laget pÃ¥ bakken, pÃ¥ Dapla, mÃ¥ vi endre adressen hvor pakkene skal installeres fra i renv.lock. Addressen skal se slik ut:\n\n\nrenv.lock\n\n\"Repositories\": [\n      {\n        \"Name\": \"CRAN\",\n        \"URL\": \"https://packagemanager.posit.co/cran/latest\"\n      }\n    ]\n\n\n\nPakke installering med renv pÃ¥ bakken\nProsessen med Ã¥ installere pakker for R pÃ¥ bakken er det samme som pÃ¥ Dapla. Noen pakker (for eksempel devtools) kan forelÃ¸pig ikke installeres pÃ¥ bakken pÃ¥ egenhÃ¥nd pga 3. parti avhengigheter. Vi jobber med Ã¥ finne en lÃ¸sning til dette.\nFor Ã¥ installere arrow, kopier og kjÃ¸r fÃ¸lgende kommando i R:\n\n\nterminal\n\ninstall.packages(\"/ssb/bruker/felles/R_pakker/arrow_11.0.0.2.tar.gz\",\n                repos = NULL,\n                type = \"source\")",
    "crumbs": [
      "Manual",
      "Kode",
      "PakkehÃ¥ndtering i R - renv"
    ]
  },
  {
    "objectID": "statistikkere/metadata.html",
    "href": "statistikkere/metadata.html",
    "title": "Metadata",
    "section": "",
    "text": "Metadata er informasjonen som trengs for Ã¥ produsere, finne, forstÃ¥ og gjenbruke data. For Ã¥ bruke et datasett riktig mÃ¥ vi forstÃ¥ hva det inneholder. Vi mÃ¥ vite hva de ulike variablene representerer, for eksempel om variabelen inntekt i datasett A er den samme som inntekt i datasett B og derfor kan sammenliknes. Vi trenger ogsÃ¥ innsikt i hva kodene i datasettet betyr, som for eksempel hva sivilstandskode 2 representerer. I tillegg er det viktig Ã¥ vite om datasettet inneholder personopplysninger, slik at vi kan oppfylle krav fra Datatilsynet.\nAlt dette er eksempler pÃ¥ metadata som er tett knyttet til selve dataene. I tillegg har SSB mange andre typer metadata, som kvalitetsindikatorer og prosessbeskrivelser. Her velger vi imidlertid Ã¥ fokusere pÃ¥ de metadataene som er nÃ¸dvendige for Ã¥ kunne bruke et datasett riktig.\nSSB har i flere Ã¥r hatt systemer for Ã¥ dokumentere Â«datanÃ¦reÂ» metadata. Vardok dokumenterer variabler, som definisjoner og eierseksjoner. Klass hÃ¥ndterer kodeverk, inkludert klassifikasjoner og kodelister. Datadok fokuserer pÃ¥ dokumentasjon av datasett, for eksempel variabler, variabeltyper og antall desimaler.\nSSB valgte Ã¥ dokumentere metadata i flere separate systemer, bÃ¥de fordi andre statistikkbyrÃ¥er hadde erfart at ett samlet system ble for stort og komplekst, og fordi SSB allerede hadde en eksisterende lÃ¸sning, Datadok, som skulle viderefÃ¸res. I stedet for Ã¥ lage ett enkelt system, ble strategien Ã¥ utvikle flere systemer som skulle fungere sammen som en helhet. Dette fÃ¸rte til integrasjoner mellom systemene, som for eksempel en kobling mellom Vardok og Klass. En variabel som NÃ¦ring i Vardok har en lenke til Standard for nÃ¦ringsgruppering i Klass. I tillegg er Vardok koblet til Statistikkbanken og MetaDB (NUDB og FD-Trygd) for utveksling av variabeldefinisjoner. Datadok er ogsÃ¥ koblet til Vardok, slik at variablene i Datadok kan referere til tilhÃ¸rende variabeldefinisjoner fra Vardok.\nVerken Datadok eller Vardok kan brukes pÃ¥ Dapla. Derfor er en ny lÃ¸sning for dokumentasjon av datasett, Datadoc, allerede implementert pÃ¥ Dapla. En ny lÃ¸sning for dokumentasjon av variabeldefinisjoner, Vardef, er under utvikling. NÃ¥r Vardef er ferdigstilt, skal variabler fra Vardok flyttes dit, og alle fremtidige oppdateringer vil skje i Vardef. Klass kan derimot brukes fra Dapla, og det er derfor ennÃ¥ ikke besluttet om og nÃ¥r denne lÃ¸sningen skal flyttes eller eventuelt implementeres pÃ¥ nytt pÃ¥ Dapla.\nPÃ¥ Dapla skal det etter hvert ogsÃ¥ implementeres en datakatalog som skal vÃ¦re en portal inn til metadatalÃ¸sningene. Via datakatalogen skal en kunne sÃ¸ke i alle disse metadataene.\n\n\n\n\n\n\nFigurÂ 1: Metadata pÃ¥ Dapla",
    "crumbs": [
      "Manual",
      "Metadata"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html",
    "href": "statistikkere/jobbe-med-kode.html",
    "title": "Jobbe med kode",
    "section": "",
    "text": "PÃ¥ Dapla jobber vi med utvikling av Python- og R-kode i et Jupyter-miljÃ¸. For de som Ã¸nsker det, er det mulig Ã¥ enkelt Ã¥pne en notebook med en av vÃ¥re forhÃ¥ndskonfigurerte kernels1. Man kan umiddelbart begynne Ã¥ skrive kode og deretter lagre den i det lokale filsystemet. Dette er ideelt for enkel datautforskning eller for pedagogiske formÃ¥l.\nNÃ¥r koden skal settes i produksjon, er det essensielt Ã¥ ta hensyn til fÃ¸lgende:\n\nResultater bÃ¸r vÃ¦re reproduserbare.\nKoden mÃ¥ kunne deles med andre.\nKoden bÃ¸r vÃ¦re organisert slik at den er gjenkjennelig for kollegaer.\n\nFor Ã¥ lette etterlevelsen av beste praksis for kodeutvikling pÃ¥ Dapla, har vi utviklet et verktÃ¸y kalt ssb-project. Dette er et CLI-verktÃ¸y2 som enkelt lar deg opprette et prosjekt med en standard mappestruktur, et virtuelt miljÃ¸ og integrasjon med Git for versjonshÃ¥ndtering. Som en bonus kan det ogsÃ¥ opprette et GitHub-repositorium for deg ved behov.\nI dette kapitlet vil vi veilede deg gjennom bruken av ssb-project. Du vil lÃ¦re Ã¥ opprette et nytt prosjekt, installere pakker, hÃ¥ndtere versjoner med Git, bygge et eksisterende prosjekt og vedlikeholde prosjektet over tid.\n\n\n\n\n\n\nSSB-project stÃ¸tter ikke R enda\n\n\n\nPer nÃ¥ stÃ¸tter SSB-project kun prosjekter skrevet i Python. Dette skyldes begrensninger ved det populÃ¦re virtuelle miljÃ¸-verktÃ¸yet for R, renv. Mens renv effektivt hÃ¥ndterer versjoner av R-pakker, har det ikke kapasitet til Ã¥ ta vare pÃ¥ spesifikke R-installasjonsversjoner. Dette kan potensielt gjÃ¸re det mer utfordrende Ã¥ reprodusere tidligere publiserte resultater ved bruk av ssb-project. Vi arbeider mot en lÃ¸sning for Ã¥ inkludere stÃ¸tte for R i fremtiden."
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#forberedelser",
    "href": "statistikkere/jobbe-med-kode.html#forberedelser",
    "title": "Jobbe med kode",
    "section": "Forberedelser",
    "text": "Forberedelser\nFÃ¸r du kan ta i bruk ssb-project sÃ¥ er det et par ting som mÃ¥ vÃ¦re pÃ¥ plass:\n\nDu mÃ¥ ha konfigurert Git etter SSB sin standard (les mer om hvordan her).\nHvis du Ã¸nsker at ssb-project ogsÃ¥ skal opprette et GitHub-repo for deg mÃ¥ du ogsÃ¥ fÃ¸lgende vÃ¦re pÃ¥ plass:\n\nDu mÃ¥ ha en GitHub-bruker (les hvordan her)\nSkru pÃ¥ 2-faktor autentifisering for GitHub-brukeren din (les hvordan her)\nVÃ¦re koblet mot SSBs organisasjon statisticsnorway pÃ¥ GitHub (les hvordan her)\nOpprette Personal Access Token (PAT) og godkjenne det for bruk mot statisticsnorway (les hvordan her)\n\n\nDet er ogsÃ¥ Ã¥ anbefale at du lagrer PAT lokalt slik at du ikke trenger Ã¥ forholde deg til det nÃ¥r jobber med Git og GitHub. Hvis du har alt dette pÃ¥ plass sÃ¥ kan du bare fortsette Ã¥ fÃ¸lge de neste kapitlene."
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#opprett-ssb-project",
    "href": "statistikkere/jobbe-med-kode.html#opprett-ssb-project",
    "title": "Jobbe med kode",
    "section": "Opprett ssb-project",
    "text": "Opprett ssb-project\n\n\n\n\n\n\nHar du Github bruker? Noe funksjonalitet i ssb-project krever det. Finn ut hvordan ved Ã¥ lese forrige kapittel.\n\n\n\nssb-project lar deg opprette en prosjekt-mappe med og uten GitHub-repo. La oss ta for oss hver av alternativene.\n\nUten GitHub-repo\nFor Ã¥ opprette et nytt ssb-project uten GitHub-repo gjÃ¸r du fÃ¸lgende:\n\nÃ…pne en terminal. De fleste vil gjÃ¸re dette i Jupyterlab pÃ¥ bakke eller sky og da kan de bare trykke pÃ¥ det blÃ¥ â•-tegnet i Jupyterlab og velge Terminal.\nFÃ¸r vi kjÃ¸rer programmet mÃ¥ vi vÃ¦re obs pÃ¥ at ssb-project vil opprette en ny mappe der vi stÃ¥r. GÃ¥ derfor til den mappen du Ã¸nsker Ã¥ ha den nye prosjektmappen. For Ã¥ opprette et prosjekt som heter stat-testprod sÃ¥ skriver du fÃ¸lgende i terminalen:\n\n\n\nterminal\n\nssb-project create stat-testprod\n\n\n\nHvis du stod i hjemmemappen din pÃ¥ nÃ¥r du skrev inn kommandoen over i terminalen, sÃ¥ har du fÃ¥tt mappestrukturen som vises i FigurÂ 1. 3. Den inneholder fÃ¸lgende :\n\n.git-mappe som blir opprettet for Ã¥ versjonshÃ¥ndtere med Git.\nsrc-mappe som skal inneholde all koden som utgjÃ¸r produksjonslÃ¸pet.\ntests-mappe som inneholder tester du skriver for koden din.\nLICENCE-fil som skal benyttes for public-repos i SSB.\npoetry.lock-fil som inneholder alle versjoner av Python-pakker som blir brukt.\npyproject.toml-fil som inneholder informasjon om prosjektet og hvilke pakker som blir brukt.\nREADME.md-fil som brukes for tekstlig innhold pÃ¥ GitHub-siden for prosjektet.\n\n\n\n\n\n\n\n\n\nFigurÂ 1: Mappen som ble opprettet av ssb-project.\n\n\n\n\n\n\n\nMed Github-repo\nOver sÃ¥ opprettet vi et ssb-project uten Ã¥ opprette et GitHub-repo. Hvis du Ã¸nsker Ã¥ opprette et GitHub-repo ogsÃ¥ mÃ¥ du endre kommandoen over til:\n\n\nterminal\n\nssb-project create stat-testprod --github --github-token='blablabla'\n\nKommandoen over oppretter en mappestruktur slik vi sÃ¥ tidligere, men ogsÃ¥ et ssb-project som heter stat-testprod med et GitHub-repo med samme navn. Som du ser sÃ¥ mÃ¥ vi da sende med opsjonen --github og PAT med opsjonen --github-token='blablabla'. Repoet i GitHub ser da ut som i FigurÂ 2. Hvis du Ã¸nsker Ã¥ slippe mÃ¥tte forholde deg til PAT hver gang interagerer med GitHub, kan du fÃ¸lge denne beskrivelsen for Ã¥ lagre den lokalt. Da kan droppe --github-token='blablabla' fra kommandoen over.\n\n\n\n\n\n\nFigurÂ 2: GitHub-repo som er opprettet av ssb-project\n\n\n\n\n\n\n\n\n\nNÃ¥r du har opprettet et nytt ssb-project, eller bygget et eksisterende prosjekt, sÃ¥ kan det ta rundt 30 sekunder fÃ¸r kernelen viser seg i Jupterlab-launcher. VÃ¦r tÃ¥lmodig!"
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#installere-pakker",
    "href": "statistikkere/jobbe-med-kode.html#installere-pakker",
    "title": "Jobbe med kode",
    "section": "Installere pakker",
    "text": "Installere pakker\nNÃ¥r du har opprettet et ssb-project sÃ¥ kan du installere de python-pakkene du trenger fra PyPI. Men fÃ¸r du installerer en pakke bÃ¸r gjÃ¸re fÃ¸lgende for Ã¥ sikre deg at du ikke installerer en pakke med skadelig kode:\n\nSÃ¸k opp pakken pÃ¥ PyPI.\nSjekk om pakken er et populÃ¦rt/velkjent prosjekt ved Ã¥ besÃ¸ke repoet der koden ligger. Antall Stars og Forks pÃ¥ gitHub er en grei indikasjon pÃ¥ dette.\nHvis du er i tvil om pakken er trygg Ã¥ installere, sÃ¥ kan du spÃ¸rre kollegaer om de har erfaring med den, eller spÃ¸rre pÃ¥ en egnet Yammer-kanal i SSB.\nHvis du fortsatt Ã¸nsker Ã¥ installere pakken sÃ¥ anbefaler vi Ã¥ copy-paste navnet fra PyPi, ikke skrive det inn manuelt nÃ¥r du installerer.\n\nSelve installeringen av pakken gjÃ¸res enkelt pÃ¥ fÃ¸lgende mÃ¥te:\n\nÃ…pne en terminal i Jupyterlab.\nGÃ¥ inn i prosjektmappen din ved Ã¥ skrive:\n\n\n\nterminal\n\ncd &lt;sti til prosjektmappe&gt;\n\n\nLag en branch/utviklingsbranch som f.eks. heter install-pandas:\n\n\n\nterminal\n\ngit checkout -b install-pandas\n\n\nInstaller, f.eks. Pandas, ved Ã¥ skrive fÃ¸lgende:\n\n\n\nterminal\n\npoetry add pandas\n\n\n\n\n\n\n\nFigurÂ 3: Installasjon av Pandas med ssb-project\n\n\n\nFigurÂ 3 viser hvordan dette vil se ut i en Jupyterlab-terminal. Kommandoen for Ã¥ installere noe er poetry add etterfulgt av pakkenavnet. Vi ser ogsÃ¥ at den automatisk legger til Pandas-versjonen i filen poetry.lock.\nDu kan ogsÃ¥ spesifisere en konkret versjon av pakken som skal installeres med fÃ¸lgende kommando:\n\n\nterminal\n\npoetry add pandas@1.2.3\n\n\nAvinstallere pakker\nOfte eksperimenterer man med nye pakker, og alle blir ikke med videre i produksjonskoden. Det er god praksis Ã¥ fjerne pakker som ikke brukes, blant annet for Ã¥ unngÃ¥ at de blir en sikkerhetsrisiko. Det gjÃ¸r du enkelt ved Ã¥ skrive fÃ¸lgende i terminalen:\n\n\nterminal\n\npoetry remove pandas\n\n\n\nOppdatere pakker\nHvis det kommer en ny versjon av en pakke du bruker, sÃ¥ kan du oppdatere den med fÃ¸lgende kommando:\n\n\nterminal\n\npoetry update pandas\n\nhvis du kjÃ¸rer poetry update uten noe pakkenavn, sÃ¥ vil alle pakkene dine oppdateres til siste versjon, med mindre du har spesifisert versjonsbegrensninger i pyproject.toml-filen.\n\n\nUndersÃ¸k avhengigheter\nHvis du lurer pÃ¥ hvilke pakker som har hvilke avhengigheter, sÃ¥ kan du lett liste ut dette i terminalen med fÃ¸lgende kommando:\n\n\nterminal\n\npoetry show --tree\n\nDet vil gi en grafisk fremstilling av avhengighetene som vist i FigurÂ 4.\n\n\n\n\n\n\nFigurÂ 4: Visning av pakke-avhengigheter i ssb-project"
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#push-til-github",
    "href": "statistikkere/jobbe-med-kode.html#push-til-github",
    "title": "Jobbe med kode",
    "section": "Push til GitHub",
    "text": "Push til GitHub\nNÃ¥r du nÃ¥ har installert en pakke sÃ¥ har filen poetry.lock endret seg. For at dine samarbeidspartnere skal fÃ¥ tilgang til denne endringen i et SSB-project, sÃ¥ mÃ¥ du pushe en ny versjon av poetry.lock-filen opp Github, og kollegaene mÃ¥ pulle ned og bygge prosjektet pÃ¥ nytt. Du kan gjÃ¸re dette pÃ¥ fÃ¸lgende mÃ¥te etter at du har installert en ny pakke:\n\nVi kan stage alle endringer med fÃ¸lgende kommando i terminalen nÃ¥r vi stÃ¥r i prosjektmappen:\n\n\n\nterminal\n\ngit add -A\n\n\nDeretter commite en endring, dvs. ta et stillbilde av koden i dette Ã¸yeblikket, ved Ã¥ skrive fÃ¸lgende:\n\n\n\nterminal\n\ngit commit -m \"Installert pandas\"\n\n\nPush det opp til GitHub4. Anta at vi gjorde dette i branchen install-pandas som ble opprettet tidligere. Da kan vi skrive fÃ¸lgende :\n\n\n\nterminal\n\ngit push --set-upstream origin install-pandas\n\nDeretter kan kollegaene dine pulle ned endringene og bygge prosjektet pÃ¥ nytt. Vi forklarer hvordan man kan bygge prosjektet pÃ¥ nytt senere i kapitlet."
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#dependabot",
    "href": "statistikkere/jobbe-med-kode.html#dependabot",
    "title": "Jobbe med kode",
    "section": "Dependabot",
    "text": "Dependabot\nNÃ¥r man installerer pakker sÃ¥ vil det etter hvert utvikle seg et sett av potensielt kompliserte avhengigheter mellom disse pakkene. Dette skyldes at en pakke kan benytte seg av funksjonalitet i andre pakker, som igjen benytter seg av funksjonalitet i andre pakker, osv.. Hvis noen finner en sikkerhetssÃ¥rbarhet i en pakke sÃ¥ kan det fikses ved at en ny versjon av den pakken slippes, som igjen kan fÃ¥ konsekvenser for pakker som er avhengig av denne.\nI SSB er det tilrettelagt for at alle som versjonshÃ¥ndterer koden sin pÃ¥ GitHub kan skanne pakkene sine for sÃ¥rbarheter og nye versjoner av pakker med Dependabot. Dependabot hjelper oss med Ã¥ finne og fikse sÃ¥rbarheter og gamle pakkeversjoner. Dette er spesielt viktig nÃ¥r man installerer sine egne pakker.\nDependabot sjekker med jevne mellomrom om det finnes oppdateringer i pakkene som er listet i din pyproject.toml-fil, og den tilhÃ¸rende poetry.lock. Hvis det finnes oppdateringer sÃ¥ vil den lage en pull request som du kan godkjenne. NÃ¥r du godkjenner den sÃ¥ vil den oppdatere poetry.lock-filen og lage en ny commit som du kan pushe til GitHub. Dependabot gir ogsÃ¥ en sikkerhetsvarslinger hvis det finnes kjente sÃ¥rbarheter i pakkene du bruker.\nDet er anbefalt at alle som installerer sine egne pakker i SSB skrur pÃ¥ Dependabot i sine GitHub-repoer.\n\nAktivere Dependabot\nDu kan aktivere Dependabot ved Ã¥ gi inn i GitHub-repoet ditt og gjÃ¸re fÃ¸lgende:\n\nGÃ¥ inn repoet\nTrykk pÃ¥ Settings for det repoet som vist pÃ¥ FigurÂ 5.\n\n\n\n\n\n\n\nFigurÂ 5: Ã…pne Settings for et GitHub-repo.\n\n\n\n\nI menyen til venstre velger du Code security and analysis\nUnder seksjonen Dependabot velger Enable pÃ¥ minst Dependabot alerts og Dependabot security updates, slik som vist i FigurÂ 6.\n\n\n\n\n\n\n\nFigurÂ 6: Skru pÃ¥ Dependabot i GitHub.\n\n\n\nNÃ¥r du har gjort dette vil GitHub varsle deg hvis det finnes en kjent sÃ¥rbarhet i pakkene som benyttes.\n\n\nOppdatere pakker\nHvis en av pakkene du bruker kommer med en oppdatering, sÃ¥ vil Dependabot lage en pull request (PR) i GitHub som du kan godkjenne. Dependabot sjekker ogsÃ¥ om oppdateringen er i konflikt med andre pakker du bruker. Hvis det er tilfellet sÃ¥ vil den lage en pull request som oppdaterer alle pakkene som er i konflikt.\nHvis en av pakkene du bruker har en kjent sikkerhetssÃ¥rbarhet, sÃ¥ vil Dependabot varsle deg om dette under Security-fanen i GitHub-repoet ditt. Hvis du trykker pÃ¥ View Dependabot alerts sÃ¥ vil du fÃ¥ en oversikt over alle sÃ¥rbarhetene som er funnet, og hvilken alvorlighetsgrad den har. Hvis du trykker pÃ¥ en av sÃ¥rbarhetene sÃ¥ vil du fÃ¥ mer informasjon om den, og du kan trykke pÃ¥ Create pull request for Ã¥ oppdatere pakken.\nSom nevnt over kan du enkelt oppdatere pakker fra GitHub ved hjelp av Dependabot. Men det finnes tilfeller der du vil teste om en oppdatering gjÃ¸r at deler av koden din ikke fungerer lenger. Anta at du bruker en Python-pakken Pandas i koden din, og at du fÃ¥r en pull request fra Dependabot om Ã¥ oppdatere den fra versjon 1.5 til 2.0. Hvis du Ã¸nsker Ã¥ teste om koden din fortsatt fungerer med den nye versjonen av Pandas, sÃ¥ kan du gjÃ¸re dette i Jupyterlab ved Ã¥ fÃ¸lge ved Ã¥ lage en branch som f.eks. heter update-pandas. Deretter kan du installere den nye versjonen av Pandas med fÃ¸lgende kommando fra terminalen:\n\n\nterminal\n\npoetry update pandas@2.0\n\nHvis du nÃ¥ kjÃ¸rer koden din kan du teste om den fortsatt fungerer som forventet. GjÃ¸r den ikke det kan du tilpasse koden din og pushe endringene til Github. Deretter kan du godkjenne pull requesten fra Dependabot og merge den. Etter dette kan du slette den PR-en som Dependabot lagde for deg."
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#bygg-eksisterende-ssb-project",
    "href": "statistikkere/jobbe-med-kode.html#bygg-eksisterende-ssb-project",
    "title": "Jobbe med kode",
    "section": "Bygg eksisterende ssb-project",
    "text": "Bygg eksisterende ssb-project\nNÃ¥r vi skal samarbeide med andre om kode sÃ¥ gjÃ¸r vi dette via GitHub. NÃ¥r du pusher koden din til GitHub, sÃ¥ kan samarbeidspartnere pulle ned koden og jobbe videre med den. Men nÃ¥r de henter ned koden sÃ¥ vil de bare hente ned selve koden, ikke pakker og Python-versjonen som ble brukt. De mÃ¥ installere alt som du hadde installert. I tillegg trenger de en kernel hvis de skal jobbe i Jupyterlab. ssb-project gjÃ¸r det svÃ¦rt enkelt Ã¥ bygge opp det du trenger, siden det virtuelle miljÃ¸et har all informasjon om hva som trengs.\nFor at samarbeidspartneren din skal kunne bygge miljÃ¸et pÃ¥ nytt, mÃ¥ de ha gjort en minimal konfigurering av Git. Les mer om hvordan du frem for Ã¥ gjÃ¸re dette her.\nFor Ã¥ bygge opp et eksisterende miljÃ¸ gjÃ¸r du fÃ¸lgende:\n\nFÃ¸rst mÃ¥ du kopiere prosjektet ned lokalt, eller klone repoet med git-terminologi\n\n\n\nterminal\n\ngit clone https://github.com/statisticsnorway/&lt;prosjektnavn&gt;\n\n\nGÃ¥ inn i mappen du klonet\n\n\n\nterminal\n\ncd &lt;prosjektnavn&gt;\n\n\nSkape et virtuelt miljÃ¸ og installere en tilsvarende Jupyter kernel med\n\n\n\nterminal\n\nssb-project build"
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#slette-ssb-project",
    "href": "statistikkere/jobbe-med-kode.html#slette-ssb-project",
    "title": "Jobbe med kode",
    "section": "Slette ssb-project",
    "text": "Slette ssb-project\nDet vil vÃ¦re tilfeller hvor man Ã¸nsker Ã¥ slette et ssb-project, enten fordi man ikke trenger koden lenger eller fordi man bare testet litt.\n\nLokalt\nHvis man jobber med flere prosjekter sÃ¥ kan det fort bli mange Jupyter kerneler hengende igjen. Derfor er det ogsÃ¥ mulighet Ã¥ kjÃ¸re\n\n\nterminal\n\nssb-project clean stat-testprod\n\nsom sletter Jupyter-kernelen og de installerte pakkene i prosjektet. Hvis du ogsÃ¥ Ã¸nsker Ã¥ slette selve mappen med kode mÃ¥ du gjÃ¸re det manuelt5:\n\n\nterminal\n\nrm -rf ~/stat-testprod/\n\nProsjektmappen over lÃ¥ direkte i hjemmemappen min og hjemmemappen pÃ¥ Linux kan alltid referes til med et tilda-tegn ~.\n\n\nArkiver GitHub-repo\nGitHub-repoer som er opprettet under SSB sin organinasjons statisticsnorway pÃ¥ GitHub kan ikke slettes, bare arkiveres. Grunnen er at hvis man oppdager en sÃ¥rbarhet senere sÃ¥ er det viktig Ã¥ kunne se repoet for Ã¥ forstÃ¥ hva som har skjedd.\nHvis du ikke trenger et GitHub-repo lenger kan man arkivere repoet. Det gjÃ¸r du pÃ¥ fÃ¸lgende mÃ¥te:\n\nGi inn i repoet Settings slik som vist med rÃ¸d pil i FigurÂ 7.\n\n\n\n\n\n\n\nFigurÂ 7: Settings for repoet.\n\n\n\n\nUnder General scroller du deg ned til delen som heter Danger Zone og velger Archive this repository, slik som vist pÃ¥ FigurÂ 8.\n\n\n\n\n\n\n\nFigurÂ 8: Arkivering av et repo.\n\n\n\n\nI dialogboksen som dukker opp fyller du inn reponavnet som beskrevet og trykker pÃ¥ I understand the consequences, archive this repository.\n\nNÃ¥r det er gjort sÃ¥ er repoet lesbart, men man kan ikke jobbe med det. Men som vi ser av @#fig-github-repo-settings-archive-warning kan man omgjÃ¸re arkiveringen senere hvis det skulle vÃ¦re Ã¸nskelig."
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#spark-i-ssb-project",
    "href": "statistikkere/jobbe-med-kode.html#spark-i-ssb-project",
    "title": "Jobbe med kode",
    "section": "Spark i ssb-project",
    "text": "Spark i ssb-project\nFor Ã¥ kunne bruke Spark i et ssb-project mÃ¥ man fÃ¸rst installere pyspark. Det gjÃ¸r du ved Ã¥ skrive fÃ¸lgende i en terminal:\n\n\nterminal\n\npoetry add pyspark==$(pip show pyspark | grep Version | egrep -o \"([0-9]{1,}\\.)+[0-9]{1,}\") --no-dev\n\nHer installerer du samme versjon av pyspark som pÃ¥ Jupyterlab.\nVidere kan vi konfigurere Spark til Ã¥ enten kjÃ¸re pÃ¥ lokal maskin eller pÃ¥ flere maskiner (sÃ¥kalte clusters). Under beskriver vi begge variantene.\n\nLokal maskin\nOppsettet for Pyspark pÃ¥ lokal maskin er det enkleste Ã¥ sette opp siden Pyspark vil ha direkte tilgang til det lokale filsystemet. Man kan bruke miljÃ¸variabelen PYSPARK_PYTHON til Ã¥ peke pÃ¥ det virtuelle miljÃ¸et, og dermed vil Pyspark ogsÃ¥ ha tilgang til alle pakkene som er installert der. I en notebook vil dette kunne settes opp slik:\n\n\nnotebook\n\nimport os\nimport subprocess\n\n# Finner filstien til det virtuelle miljÃ¸et\npython_path = subprocess.run(['poetry', 'run', 'which', 'python'],\n                             capture_output=True, text=True).stdout.rstrip('\\n')\n\nos.environ[\"PYSPARK_PYTHON\"] = python_path\nos.environ[\"PYSPARK_SUBMIT_ARGS\"] = os.environ[\"PYSPARK_LOCAL_SUBMIT_ARGS\"]\n\nNÃ¥r du oppretter en Notebook og bruker den kernelen du har laget sÃ¥ mÃ¥ du alltid ha denne pÃ¥ toppen av notebooken. Det initialiserer Spark slik at det kan brukes i notebooken.\n\n\nnotebook\n\n%run /usr/local/share/jupyter/kernels/pyspark_local/init.py\n\nDette scriptet vil sette et spark objekt som brukes for Ã¥ kalle APIâ€™et til pyspark.\n\n\nCluster\nHvis man vil kjÃ¸re Pyspark i et cluster (dvs. pÃ¥ flere maskiner) sÃ¥ vil databehandlingen foregÃ¥ pÃ¥ andre maskiner som ikke har tilgang til det lokale filsystemet. Man mÃ¥ dermed lage en â€œpakkeâ€ av det virtuelle miljÃ¸et pÃ¥ lokal maskin og tilgjengeliggjÃ¸re dette for alle maskinene i clusteret. For Ã¥ lage en slik â€œpakkeâ€ kan man bruke et bibliotek som heter venv-pack. Dette kan kjÃ¸res fra et terminalvindu slik:\n\n\nterminal\n\nvenv-pack -p .venv -o pyspark_venv.tar.gz\n\nMerk at kommandoen over mÃ¥ kjÃ¸res fra rot-mappen i prosjektet ditt. Her er pyspark_venv.tar.gz et tilfeldig valgt filnavn, men dette filnavnet skal brukes videre i notebooken.\n\n\nnotebook\n\nimport os\nimport subprocess\n\n# MiljÃ¸variabel som peker pÃ¥ en utpakket versjon av det virtuelle miljÃ¸et\nos.environ[\"PYSPARK_PYTHON\"] = \"./environment/bin/python\"\n\n# Legg til et flagg, --archives, som peker pÃ¥ \"pakken\" med det virtuelle miljÃ¸et\nconf = os.environ[\"PYSPARK_K8S_SUBMIT_ARGS\"].split(' ')\nlast_index = conf.index('pyspark-shell')\nconf[last_index:last_index] = ['--archives', 'pyspark_venv.tar.gz#environment']\nos.environ[\"PYSPARK_SUBMIT_ARGS\"] = ' '.join(conf)\n\nNÃ¥r du oppretter en Notebook og bruker den kernelen du har laget sÃ¥ mÃ¥ du alltid ha denne pÃ¥ toppen av notebooken. Det initialiserer Spark slik at det kan brukes i notebooken.\n\n\nnotebook\n\n%run /usr/local/share/jupyter/kernels/pyspark_k8s/init.py\n\nDette scriptet vil sette et spark objekt som brukes for Ã¥ kalle APIâ€™et til pyspark."
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#tips-og-triks",
    "href": "statistikkere/jobbe-med-kode.html#tips-og-triks",
    "title": "Jobbe med kode",
    "section": "Tips og triks",
    "text": "Tips og triks\nI denne delen av kapitlet vil vi gi deg noen tips og triks som kan vÃ¦re nyttige nÃ¥r du jobber med ssb-project.\n\nPoetry\nssb-project bruker Poetry for Ã¥ hÃ¥ndtere virtuelle miljÃ¸er. Poetry er et verktÃ¸y som gjÃ¸r det enkelt Ã¥ installere pakker og hÃ¥ndtere versjoner av disse. Det er ogsÃ¥ Poetry som hÃ¥ndterer Jupyter-kernelen for deg.\nHvis du etterlyser funksjonalitet i et ssb-project sÃ¥ kan det vÃ¦re nyttig Ã¥ lese dokumentasjonen til Poetry for Ã¥ se om det er mulig Ã¥ fÃ¥ til det du Ã¸nsker.\n\n\nFull disk pÃ¥ Dapla\nDet â€œlokaleâ€ filsystemet pÃ¥ Dapla har kun 10GB diskplass. Har du mange virtuelle miljÃ¸er pÃ¥ denne disken kan det fort bli fullt, siden alle pakker blir installert her. Vanligvis er det 2 grunner til at disken blir full:\n\nFor mange virtuelle miljÃ¸er (ssb-projects) lagret lokalt.\nDette vil ofte kunne lÃ¸ses ved Ã¥ slette virtuelle miljÃ¸er som ikke lenger er i bruk. Hvis du har 5 virtuelle miljÃ¸er som hver bruker 1GB, og du kun jobber pÃ¥ en av de nÃ¥, sÃ¥ vil du frigjÃ¸re 40% av disken ved Ã¥ slette 4 av dem. Husk at det permanente lagringsstedet for kode er pÃ¥ GitHub, og du kan alltid klone ned et prosjekt senere og bygge det hvis det trengs.\n/home/jovyan/.cache/ har blitt for stort.\nDette er en mappe som brukes av applikasjoner til Ã¥ lagre midlertidig data slik at de kan kjÃ¸re raskere. Denne kan bli ganske stor etter hvert. Ofte kan man frigjÃ¸re flere GB ved Ã¥ slette denne. Du sletter denne mappen ved Ã¥ skrive fÃ¸lgende i en terminal:\n\n\n\nterminal\n\nrm -rf /home/jovyan/.cache/\n\nHvis du opplever at disken er full, sÃ¥ kan det anbefales Ã¥ undersÃ¸ke hvilke mapper som tar stÃ¸rst plass med fÃ¸lgende kommando i terminalen:\n\n\nterminal\n\ncd ~ && du -h --max-depth=5 | sort -rh | head -n 10 && cd -\n\nKommandoen over sjekker, fra hjemmemappen din, hvilke mapper og undermapper som tar mest plass. Den viser de 10 stÃ¸rste mappene. Hvis du Ã¸nsker Ã¥ se flere mapper sÃ¥ kan du endre tallet etter head -n. Hvis du Ã¸nsker Ã¥ se alle mapper sÃ¥ kan du fjerne head -n. --max-depth=5 betyr at den kun sjekker mapper som er 5 mapper dype fra hjemmemappen din.\nNÃ¥r du har gjort det kan selv vurdere hvilke som kan slettes for Ã¥ frigjÃ¸re plass.\n\n\nHold prosjektet oppdatert\nHvis du sitter med en lokal kopi av prosjektet ditt, og flere andre jobber med den samme kodebasen, sÃ¥ er det viktig at du holder din lokale kopi oppdatert. Hvis du jobber i en branch pÃ¥ en lokal kopi, bÃ¸r du holde denne oppdatert med main-branchen pÃ¥ GitHub. Det er vanlig Git-praksis. NÃ¥r man ogsÃ¥ bruker ssb-project, sÃ¥ man huske Ã¥ ogsÃ¥ bygge prosjektet pÃ¥ nytt hver gang det er endringer som er gjort av andre i poetry.lock.-filen."
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#footnotes",
    "href": "statistikkere/jobbe-med-kode.html#footnotes",
    "title": "Jobbe med kode",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nEn kernel refererer til en Python- eller R-installasjon som er optimalisert for bruk med Jupyterlab Notebooks.â†©ï¸\nCLI = Command-Line-Interface, som betyr et program designet for bruk i terminalen med kommandoer.â†©ï¸\nFiler og mapper som starter med punktum er skjulte med mindre man ber om Ã¥ se dem. I Jupyterlab kan disse vises i filutforskeren ved Ã¥ velge View fra menylinjen, og deretter velge Show hidden files. I en terminal skriver man ls -a for Ã¥ se de.â†©ï¸\nÃ… pushe til GitHub uten Ã¥ sende ved Personal Access Token fordrer at du har lagret det lokalt sÃ¥ Git kan finne det. Her et eksempel pÃ¥ hvordan det kan gjÃ¸res.â†©ï¸\nDette kan ogsÃ¥ gjÃ¸res ved Ã¥ hÃ¸yreklikke pÃ¥ mappen i Jupyterlab sin filutforsker og velge Delete.â†©ï¸"
  },
  {
    "objectID": "statistikkere/transfer-service.html",
    "href": "statistikkere/transfer-service.html",
    "title": "Transfer Service",
    "section": "",
    "text": "Storage Transfer Service1 er en Google-tjeneste for Ã¥ flytte data mellom lagringsomrÃ¥der. I SSB bruker vi hovedsakelig tjenesten til Ã¥:\nTjenesten stÃ¸tter bÃ¥de automatiserte og ad-hoc overfÃ¸ringer, og den inkluderer et brukergrensesnitt for Ã¥ sette opp og administrere overfÃ¸ringene i Google Cloud Console (GCC).",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Transfer Service"
    ]
  },
  {
    "objectID": "statistikkere/transfer-service.html#tilgangsstyring",
    "href": "statistikkere/transfer-service.html#tilgangsstyring",
    "title": "Transfer Service",
    "section": "Tilgangsstyring",
    "text": "Tilgangsstyring\nTilgangsstyringen til data gjelder ogsÃ¥ for overfÃ¸ringer av data med Transfer Service. Det betyr at du mÃ¥ ha tilgang til dataene du skal sette opp overfÃ¸ringsjobber for. Ved bruk av Transfer Service for overfÃ¸ring av data mellom bakke og sky sÃ¥ er det satt opp en dedikerte mapper for dette i prodsonen. OgsÃ¥ her fÃ¸lges tilgangsstyringen til dataene, med unntak av at data-admins har permanent tilgang til kildedata som er synkronisert ned til bakken, mens man pÃ¥ Dapla mÃ¥ de gi seg selv korte, begrunnede tilganger ved behov.\n\n\nPÃ¥ Dapla sÃ¥ er det opprettet dedikerte bÃ¸tter for overfÃ¸ring av data mellom bakke og sky. Disse heter tilsky og frasky. Tanken med disse â€œmellomstasjoneneâ€ for overfÃ¸ring av data er at de skal beskytte Dapla-team fra Ã¥ overskrive data ved en feil. Ved Ã¥ ha egne bÃ¸tter som data blir synkronisert gjennom, sÃ¥ legges det opp til at man deretter manuelt3 flytter dataene til riktig bÃ¸tte.\nMen det er ikke lagt noen sperrer for synkronisere direkte til en annen bÃ¸tte man har tilgang til. Systembrukeren (se forklaringsboks) som kjÃ¸rer Transfer Service har tilgang til alle bÃ¸ttene i prosjektet. Det betyr at en data-admin kan velge Ã¥ synkronisere data direkte inn i kildebÃ¸tta hvis man mener at det er hensiktsmessig. Det samme gjelder for developers som setter opp dataoverfÃ¸ringer i standardprosjektet. Men da er det som sagt viktig Ã¥ vÃ¦re bevisst pÃ¥ hvordan man setter opp reglene for overskriving av data hvis filene har like navn. Disse opsjonene forklares nÃ¦rmere senere i kapitlet.\n\n\n\n\n\n\n\n\n\nPersonlig bruker vs systembruker\n\n\n\nNÃ¥r du setter opp en overfÃ¸ringsjobb med Transfer Service sÃ¥ setter du opp en jobb som kjÃ¸res av en systembruker4 og ikke din egen personlige bruker. Dette er spesielt viktig Ã¥ vÃ¦re klar over nÃ¥r man setter opp automatiserte overfÃ¸ringsjobber. En konsekves av dette er at automatiske overfÃ¸ringsjobber vil fortsette Ã¥ kjÃ¸re selv om din tilgang til dataene er midlertidig, siden det er en systembruker som faktisk kjÃ¸rer jobben.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Transfer Service"
    ]
  },
  {
    "objectID": "statistikkere/transfer-service.html#forberedelser",
    "href": "statistikkere/transfer-service.html#forberedelser",
    "title": "Transfer Service",
    "section": "Forberedelser",
    "text": "Forberedelser\nFÃ¸rste gang du bruker Transfer Service mÃ¥ du sjekke at tjenesten er aktivert for teamet. Transfer Service er en sÃ¥kalt feature som teamet kan skru av og pÃ¥ selv. For Ã¥ sjekke om den er skrudd pÃ¥ gÃ¥r du inn i teamets IaC-repo5 og sjekker filen ./infra/projects.yaml.\n\n\ndapla-example-iac/infra/projects.yaml\n\nteam_uniform_name: dapla-example\n\nprojects:\n  - project_name: dapla-example\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service\n\nI filen over ser du at teamet har skrudd pÃ¥ tjenesten i prod-miljÃ¸et, siden den transfer-service er listet under features. Hvis tjenesten ikke er skrudd pÃ¥ kan du lese om hvordan du skrur den pÃ¥ i feature-dokumentasjonen.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Transfer Service"
    ]
  },
  {
    "objectID": "statistikkere/transfer-service.html#overfÃ¸ring-av-data",
    "href": "statistikkere/transfer-service.html#overfÃ¸ring-av-data",
    "title": "Transfer Service",
    "section": "OverfÃ¸ring av data",
    "text": "OverfÃ¸ring av data\n\n\n\n\n\n\nOverfÃ¸ring av kildedata\n\n\n\nOverfÃ¸ring av kildedata mÃ¥ gjÃ¸res av en data-admin i teamet som har aktivert sin forhÃ¥ndsgodkjente tilgang til kildedata. Tilgangen aktiveres ved Ã¥ gÃ¥ inn i JIT-applikasjonen og velge prosjekt-id. Deretter velger du rollene ssb.bucket.write, ssb.buckets.list og storagetransfer.admin, og hvor lenge du Ã¸nsker tilgangen. Til slutt oppgir du en begrunnelse for hvorfor du trenger tilgangentilgangen og trykker Request access. NÃ¥r du har gjort dette vil du fÃ¥ en bekreftelse pÃ¥ at tilgangen er aktivert, og det tar ca 1 minutt fÃ¸r den aktiverte tilgangen er synlig i GCC.\n\n\nGrensesnittet for Ã¥ sette opp overfÃ¸ringsjobber i Transfer Service er tilgjengelig i Google Cloud Console (GCC).\n\n\n\nGÃ¥ inn pÃ¥ Google Cloud Console i en nettleser.\nSjekk, Ã¸verst i hÃ¸yre hjÃ¸rne, at du er logget inn med din SSB-konto (xxx@ssb.no).\nVelg prosjektet6 som overfÃ¸ringen skal settes opp under.\nEtter at du har valgt prosjekt kan du sÃ¸ke etter Storage Transfer i sÃ¸kefeltet Ã¸verst pÃ¥ siden, og gÃ¥ inn pÃ¥ siden.\n\n\n\n\n\n\n\n\n\n\nHva er mitt prosjektnavn?\n\n\n\nNÃ¥r det opprettes et Dapla-team, sÃ¥ opprettes det flere Google-prosjekter for teamet. NÃ¥r du skal velge hvilket prosjekt du skal jobbe pÃ¥ i GCC, sÃ¥ fÃ¸lger de en fast navnestruktur. For eksempel sÃ¥ vil et team med navnet dapla-example fÃ¥ et standardprosjekt som heter dapla-example-p. Det blir ogsÃ¥ opprettet et kildeprosjekt som heter dapla-example-kilde-p.\n\n\n\n\nFÃ¸rste gang du bruker Storage Transfer mÃ¥ man gjÃ¸re en engangsjobb for Ã¥ bruke tjenesten. Dette gjÃ¸res kun fÃ¸rste gang din bruker setter opp en jobb, og deretter trenger du ikke Ã¥ gjÃ¸re det flere ganger.\nNÃ¥r du kommer inn pÃ¥ siden til Storage Transfer sÃ¥ trykker du pÃ¥ Set Up Connection. NÃ¥r du trykker pÃ¥ denne vil det dukke opp et nytt felt hvor du fÃ¥r valget Create Pub-Sub Resources. Trykk pÃ¥ den blÃ¥ Create-knappen, og deretter trykk pÃ¥ Close lenger nede. Da er engangsjobben gjort, og du kan begynne Ã¥ sette opp overfÃ¸ringsjobber.\n\nI navigasjonsmenyen til venstre trykk Transfer jobs, og deretter trykk pÃ¥ + Create transfer job Ã¸verst pÃ¥ siden for Ã¥ opprette en ny overfÃ¸ringsjobb. Da fÃ¥r du opp bildet som vist i FigurÂ 1.\n\n\n\n\n\n\n\nFigurÂ 1: Opprett overfÃ¸ringsjobb i Google Cloud Console.\n\n\n\nVidere vil det variere om man skal overfÃ¸re data mellom bÃ¸tter eller mellom Dapla og prodsonen. Under forklarer vi begge fremgangsmÃ¥tene.\n\nProdsonen og Dapla\nOverfÃ¸ring mellom bakke og sky er en overfÃ¸ring av data mellom en bÃ¸tte pÃ¥ Dapla og en mappe i prodsonen. Siden tilgangsstyring til kildedata er strengere enn tilgangsstyring til annen data, sÃ¥ er det det to litt fremgangsmÃ¥ter for Ã¥ sette opp overfÃ¸ringsjobber for disse.\nSiden stegene er litt forskjellig avhengig av om man skal flytte kildedata eller annen data, sÃ¥ deler vi denne delen i to. FigurÂ 2 viser hvordan dette er satt opp. Kildeprosjektet pÃ¥ Dapla har en tilsky-bÃ¸tte for Ã¥ flytting av data fra prodsonen til Dapla, og den har en frasky-bÃ¸tte for Ã¥ flytte data fra Dapla til prodsonen. Standardprosjektet pÃ¥ Dapla har ogsÃ¥ en tilsky-bÃ¸tte for Ã¥ flytte data fra prodsonen til Dapla, og den har en frasky-bÃ¸tte for Ã¥ flytte data fra Dapla til prodsonen.\n\n\n\n\n\n\nFigurÂ 2: OverfÃ¸ring av data mellom prodsonen og Dapla.\n\n\n\nVidere viser vi hvordan man overfÃ¸rer fra Dapla til prodsonen. OverfÃ¸ring motsatt vei innebÃ¦rer bare at man bytter om pÃ¥ Source type og Destination type.\n\nI fanen Get started velger du:\n\nSource type: Google Cloud Storage\nDestination type: POSIX filesystem\n\nI fanen Choose a source trykker du pÃ¥ Browse, velger hvilken bÃ¸tte eller â€œundermappeâ€ i en bÃ¸tte du skal overfÃ¸re fra, og trykker Select7.\nI fanen Choose a destination velger du transfer_service_default under Agent pool. Under Destination directory path velger du hvilken undermappe av som filen skal overfÃ¸res til. Tjenesten vet allerede om du er i kilde- eller standardprosjektet, sÃ¥ du trenger kun Ã¥ skrive inn frasky/ eller tilsky/ her, og evt. undermappenavn hvis det er aktuelt (f.eks. frasky/data/8). Trykk Next step.\nI fanen Choose when to run job velger du hvor ofte og hvordan jobber skal kjÃ¸re. TabellÂ 1 viser hvilke valg du kan ta. Trykk Next step.\n\n\n\n\nTabellÂ 1: Valg under Choose when to run job\n\n\n\n\n\n\n\n\n\nValg\nFrekvens\n\n\n\n\nRun once\nEngangoverfÃ¸ringer\n\n\nRun every day\nSynkroniser hver dag\n\n\nRun every week\nSynkroniser hver uke\n\n\nRun with custom frequency\nSynkroniser inntill hver time\n\n\nRun on demand\nSynkroniserer nÃ¥r du manuelt trigger jobben\n\n\n\n\n\n\n\nI fanen Choose settings kan du velge hvordan detaljer knyttet til overfÃ¸ringen skal hÃ¥ndteres. TabellÂ 2 viser hvilke valg du kan ta.\n\n\n\n\nTabellÂ 2: Valg under Choose settings\n\n\n\n\n\n\n\n\n\n\nValg\nUndervalg\nHandling\n\n\n\n\nIdentify your job\n\nBeskriv jobben kort.\n\n\nManifest file\n\nIkke relevant. Bruk default valg.\n\n\nChoose how to handle your data\nMetadata options\nIkke relevant. Bruk default valg.\n\n\n\nWhen to overwrite\nTenk nÃ¸ye gjennom hva du velger her.\n\n\n\nWhen to delete\nTenk nÃ¸ye gjennom hva du velger her.\n\n\nChoose how to keep track of transfer progress\nLogging options\nSkru pÃ¥ logging.\n\n\n\n\n\n\nValgene When to overwrite og When to delete er det viktig at tenkes nÃ¸ye gjennom, spesielt ved automatiske synkroniseringer. When to overwrite er spesielt siden det kan fÃ¸re til data blir overskrevet eller tapt.\n\nTrykk pÃ¥ den blÃ¥ Create-knappen for Ã¥ opprette overfÃ¸ringsjobben. Du vil kunne se kjÃ¸rende jobber under menyen Transfer jobs.\n\n\nMappestrukturen i prodsonen\nMappestrukturen for overfÃ¸ringer med Transfer Service mellom bakke og sky har en fast struktur som er likt for alle team. Hvis du logger deg inn i terminalen pÃ¥ en av Linux-serverne i prodsonen, Ã¥pner du mappen ved Ã¥ skrive cd /ssb/cloud_sync. Under denne mappen finner du en mappe for hvert team som har aktivert Transfer Service. Hvis et team for eksempel heter dapla-example sÃ¥ vil det vÃ¦re en mappe som heter dapla-example. Her kan teamet hente og levere data som skal synkroniseres mellom bakke og sky. Videre er det undermapper for kilde- og standardprosjektet til teamet. Det er kun data-admins som har tilgang til kildeprosjektet, og det er kun developers som har tilgang til standardprosjektet. Under finner du en oversikt over hvordan mappene ser ut for et team som heter dapla-example.\n\n\n/ssb/cloud_sync/dapla-example/\n\ndapla-example\nâ”‚\nâ”œâ”€â”€ kilde\nâ”‚   â”‚\nâ”‚   â”‚â”€â”€ tilsky\nâ”‚   â”‚\nâ”‚   â””â”€â”€ frasky\nâ”‚\nâ””â”€â”€ standard\n    â”‚\n    â”‚â”€â”€ tilsky\n    â”‚\n    â””â”€â”€ frasky\n\n\n\n\nBÃ¸tte til bÃ¸tte\nOverfÃ¸ring mellom bÃ¸tter er en overfÃ¸ring av data mellom to bÃ¸tter pÃ¥ Dapla. FremgangsmÃ¥ten er helt likt som beskrevet tidligere, men at du nÃ¥ velger Google Cloud Storage som bÃ¥de kilde og destinasjon. Igjen er vi avhengig av at systembrukeren som utfÃ¸rer jobben har tilgang til begge bÃ¸ttene som er involvert i overfÃ¸ringen. Default er at et team kan overfÃ¸re mellom bÃ¸tter i kildeprosjektet, og at de kan overfÃ¸re mellom bÃ¸tter i standardprosjektet, men aldri mellom de to. Hvis du Ã¸nsker Ã¥ overfÃ¸re mellom bÃ¸tter i ditt prosjekt og et annet teams prosjekt, sÃ¥ mÃ¥ du be det andre teamet om Ã¥ gi din systembruker tilgang til dette.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Transfer Service"
    ]
  },
  {
    "objectID": "statistikkere/transfer-service.html#footnotes",
    "href": "statistikkere/transfer-service.html#footnotes",
    "title": "Transfer Service",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI SSB kaller vi tjenesten for Transfer Service, men du kan oppleve at Google kaller den litt forskjellige ting. Den blir omtalt som Storage Transfer Service noen steder, mens i Google Cloud Console blir den omtalt som Data Transfer eller Storage Transferâ†©ï¸\nFlytting av data mellom bÃ¸tter krever at prosjektets Transfer Service har tilgang til begge bÃ¸ttene.â†©ï¸\nMed manuelt menes her at man gÃ¥r inn og flytter filer fra en bÃ¸tte til en annen. Men det kan ogsÃ¥ bety at man flytter data til riktig bÃ¸tte som en del produksjonskoden sin, som igjen kan kjÃ¸res automatisk.â†©ï¸\nSystembrukere heter Service Accounts pÃ¥ engelsk og blir ofte referert til som SA-er i dagligtale.â†©ï¸\nDu finner teamets IaC-repo ved Ã¥ gÃ¥ inn pÃ¥ https://github.com/orgs/statisticsnorway/repositories og sÃ¸ke etter ditt teamnavn og Ã¥pne den som har navnestrukturen teamnavn-iac. For eksempel vil et team som heter dapla-example har et IaC-repo som heter dapla-example-iac.â†©ï¸\nDu kan velge prosjekt Ã¸verst pÃ¥ siden, til hÃ¸yre for teksten Google Cloud. I bildet under ser du at hvordan det ser ut nÃ¥r prosjektet dapla-felles-p er valgt.â†©ï¸\nNÃ¥r du skal velge en undermappe i en bÃ¸tte sÃ¥ er grensesnittet litt lite intuitivt. Du kan ikke trykke pÃ¥ navnet, men du pÃ¥ trykke pÃ¥ -tegnet for Ã¥ se undermappene.â†©ï¸\nNÃ¥r du skal synkronisere fra Dapla til en undermappe i prodsonen, sÃ¥ mÃ¥ mappen i prodsonen allerede eksisterere. Hvis den ikke gjÃ¸r det vil jobben feile. Ved synkronsiering fra prodsonen til Dapla trenger ikke undermappen eksistere, siden bÃ¸tter egentlig ikke har undermapper og filstien fra prodsonen bare blir til filnavnet i bÃ¸tta.â†©ï¸",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Transfer Service"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html",
    "href": "statistikkere/vscode-python.html",
    "title": "Vscode-python",
    "section": "",
    "text": "Vscode-python er en tjeneste pÃ¥ Dapla Lab for utvikling av kode i Python1. MÃ¥lgruppen for tjenesten er brukere som skal skrive produksjonskode i Python.\nSiden tjenesten er ment for produksjonskode sÃ¥ er det veldig fÃ¥ forhÃ¥ndsinstallerte Python-pakker som er installert. Antagelsen er at brukerene/teamet heller bÃ¸r installere de pakkene de trenger, framfor at alle pakker som alle brukere/team er forhÃ¥ndsinstallert og skal dekke behovet til alle. Det reduserer kompleksitet i tjenesten og dermed sannsynligheten for feilsituasjoner.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#forberedelser",
    "href": "statistikkere/vscode-python.html#forberedelser",
    "title": "Vscode-python",
    "section": "Forberedelser",
    "text": "Forberedelser\nFÃ¸r man starter Vscode-python bÃ¸r man ha lest kapitlet om Dapla Lab og satt opp Git- og GitHub-konfigurasjonen under Min konto. Deretter gjÃ¸r du fÃ¸lgende:\n\nLogg deg inn pÃ¥ Dapla Lab\nUnder Tjenestekatalog trykker du pÃ¥ Start-knappen for Vscode-python\nGi tjenesten et navn\nÃ…pne Vscode-python konfigurasjoner",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#konfigurasjon",
    "href": "statistikkere/vscode-python.html#konfigurasjon",
    "title": "Vscode-python",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nKonfigurasjonen av VSCode er nÃ¦r identisk Jupyter-tjenesten sin konfigurasjon. Se dokumentasjonen for konfigurasjon av Jupyter.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#datatilgang",
    "href": "statistikkere/vscode-python.html#datatilgang",
    "title": "Vscode-python",
    "section": "Datatilgang",
    "text": "Datatilgang\nSlik kan man inspisere dataene fra en terminal inne i tjenesten:\n\nÃ…pne en instans av Vscode-python med data fra bÃ¸tter\nÃ…pne en terminal inne i Vscode-python\nGÃ¥ til mappen med bÃ¸ttene ved Ã¥ kjÃ¸re dette fra terminalen cd /buckets\nKjÃ¸r ls -ahl i teminalen for Ã¥ se pÃ¥ hvilke bÃ¸tter som er montert.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#installere-pakker",
    "href": "statistikkere/vscode-python.html#installere-pakker",
    "title": "Vscode-python",
    "section": "Installere pakker",
    "text": "Installere pakker\nSiden det nesten ikke er installert noen pakker i tjenesten sÃ¥ kan brukeren opprette et ssb-project og installere pakker som vanlig.\nFor Ã¥ bygge et eksisterende ssb-project sÃ¥ kan brukeren ogsÃ¥ bruke ssb-project.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#slette-tjenesten",
    "href": "statistikkere/vscode-python.html#slette-tjenesten",
    "title": "Vscode-python",
    "section": "Slette tjenesten",
    "text": "Slette tjenesten\nFor Ã¥ slette tjenesten kan man trykke pÃ¥ Slette-knappen i Dapla Lab under Mine tjenester. NÃ¥r man sletter en tjeneste sÃ¥ sletter man hele disken inne i tjenesten og frigjÃ¸r alle ressurser som er reservert. Siden pakkene som er installert ogsÃ¥ ligger lagret pÃ¥ disken, betyr dette at pakkene mÃ¥ installeres pÃ¥ nytt etter at en tjeneste er slettet. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#pause-tjenesten",
    "href": "statistikkere/vscode-python.html#pause-tjenesten",
    "title": "Vscode-python",
    "section": "Pause tjenesten",
    "text": "Pause tjenesten\nMan kan pause tjenesten ved Ã¥ trykke pÃ¥ Pause-knappen i Dapla Lab under Mine tjenester. NÃ¥r man pauser sÃ¥ slettes alt pÃ¥den lokale disken som ikke er lagret under $HOME/work. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#monitorering",
    "href": "statistikkere/vscode-python.html#monitorering",
    "title": "Vscode-python",
    "section": "Monitorering",
    "text": "Monitorering\n\n\nMan kan moniterere en instans av Vscode-python ved Ã¥ trykke pÃ¥ Vscode-python-teksten under Mine tjenester i Dapla Lab, slik som vist i Figur 1.\n\n\n\n\n\n\n\n\n\nFigurÂ 1: Monitorering av Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#extensions",
    "href": "statistikkere/vscode-python.html#extensions",
    "title": "Vscode-python",
    "section": "Extensions",
    "text": "Extensions\nVSCode fÃ¸lger med et sett med extensions ferdig installert. Disse kan per nÃ¥ ikke installeres av brukeren selv.\n\nJupytext\nJupytext-filer kan jobbes med som notebooks i Jupyter. For Ã¥ gjÃ¸re dette, mÃ¥ man legge til Jupytext som en Python-avhengighet i ditt Python-prosjekt:\npoetry add --group dev \"jupytext &gt;=1\"\n..og deretter velge din pakkes Python-versjon som Ã¥ vÃ¦re interpreter. Dette gjÃ¸r man ved Ã¥ trykke pÃ¥ den rÃ¸de boksen pÃ¥ bildet, og velge interpreteren pÃ¥ filstien &lt;PAKKENAVN&gt;/.venv/bin/python.\n\n\n\n\n\n\nFigurÂ 2: Monitorering av Jupyter-tjenesten i Dapla Lab\n\n\n\nDeretter kan man hÃ¸yreklikke pÃ¥ filen og trykke â€œOpen as Jupyter Notebookâ€.\n\n\n\n\n\n\nFigurÂ 3: Konfigurasjon av Git for Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#footnotes",
    "href": "statistikkere/vscode-python.html#footnotes",
    "title": "Vscode-python",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nVscode-python er web-versjonen av VS Code og er ikke helt identisk med desktop-versjonen av VS Code mange er kjent med. Blant annet er det kun extensions fra Open VSX Registry som kan installeres.â†©ï¸",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/arkivering.html",
    "href": "statistikkere/arkivering.html",
    "title": "Arkivering",
    "section": "",
    "text": "Alle som flytter produksjon til Dapla mÃ¥ fortsatt arkivere dataene i bakkemiljÃ¸et. Grunnen til dette er at det enda ikke er bestemt hvordan arkivering skal foregÃ¥ pÃ¥ Dapla. Inntill videre mÃ¥ derfor statistikkteam arkivere i de gamle systemene.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Arkivering"
    ]
  },
  {
    "objectID": "statistikkere/arkivering.html#skrive-fil",
    "href": "statistikkere/arkivering.html#skrive-fil",
    "title": "Arkivering",
    "section": "Skrive fil",
    "text": "Skrive fil\nFÃ¸r man kan arkivere data mÃ¥ det skrives ut slik det er definert i Datadok. Mens man tidligere gjorde dette fra SAS, skal man pÃ¥ Dapla gjÃ¸re det fra R eller Python.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Arkivering"
    ]
  },
  {
    "objectID": "statistikkere/arkivering.html#overfÃ¸re-fil",
    "href": "statistikkere/arkivering.html#overfÃ¸re-fil",
    "title": "Arkivering",
    "section": "OverfÃ¸re fil",
    "text": "OverfÃ¸re fil\nEtter at filen er skrevet mÃ¥ den flyttes fra Dapla til bakkemiljÃ¸et, og til slutt inn i riktig arkiv-mappe. OverfÃ¸ring av filer mellom bakke og sky gjÃ¸res med Transfer Service. NÃ¥r filen er flyttet til bakkemiljÃ¸et, mÃ¥ brukeren selv flytte filen til arkiv-mappen. Ã˜nsker man Ã¥ automatisere flyttingen, sÃ¥ kan man sende en forespÃ¸rsel til Kundeservice.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Arkivering"
    ]
  },
  {
    "objectID": "statistikkere/tilgangsstyring.html",
    "href": "statistikkere/tilgangsstyring.html",
    "title": "Tilgangsstyring",
    "section": "",
    "text": "Hvert Dapla-team har sine egne lagringsomrÃ¥der for data som ingen andre har tilgang til, med mindre teamet eksplisitt velger Ã¥ dele data med andre team. I tillegg har teamet tilgang til egne ressurser for Ã¥ behandle dataene.\nDet er tilgangsgruppen managers som bestemmer hvilke personer som skal ha hvilke roller i et team, og dermed hvilke data de ulike team-medlemmene fÃ¥r tilgang til. Den som jobber med data kan bli plassert i tilgangsgruppene data-admins eller developers. Sistnevnte fÃ¥r tilgang til alle datatilstander utenom kildedata, mens data-admins er forhÃ¥ndsgodkjent til Ã¥ ogsÃ¥ Ã¥ aksessere kildedata ved behov. Dermed er data-admins en priveligert rolle pÃ¥ teamet som er forbeholdt noen fÃ¥ personer.\n\n\n\n\n\n\nFigurÂ 1: Datatilstander som et team sitt medlemmer har ilgang til.\n\n\n\nFigurÂ 1 viser hvem som har tilgang til hvilke datatilstander. Som nevnt er data-admins ansett som forhÃ¥ndsgodkjent til Ã¥ aksessere kildedata ved behov. MÃ¥ten dette er implementert pÃ¥ er at data-admins mÃ¥ aktivere denne tilgangen selv, ved Ã¥ bruke et JIT-grensesnitt (Just-In-Time Access). Tilgangen krever en begrunnelse og bruken kan lÃ¸pende monitoreres av managers for teamet.",
    "crumbs": [
      "Manual",
      "Dapla-team",
      "Tilgangsstyring"
    ]
  },
  {
    "objectID": "statistikkere/contribution.html",
    "href": "statistikkere/contribution.html",
    "title": "Bidra til Dapla-manualen",
    "section": "",
    "text": "Alle i SSB kan bidra til denne manualen. Endringer mÃ¥ godkjennes av noen i Team Statistikktjenester eller Alex Crozier (akc@ssb.no). Si gjerne i fra at det ligger en PR Ã¥ se pÃ¥. Vi trenger bidrag med alt fra sprÃ¥kvask, nye artikler og andre gode initiativer! Har du lyst til Ã¥ bidra, men er ikke helt sikker pÃ¥ hva du kan bidra med? Ta en titt pÃ¥ issues i GitHub-repoet.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Bidra til Dapla-manualen"
    ]
  },
  {
    "objectID": "statistikkere/contribution.html#forutsetninger",
    "href": "statistikkere/contribution.html#forutsetninger",
    "title": "Bidra til Dapla-manualen",
    "section": "Forutsetninger",
    "text": "Forutsetninger\n\nMan trenger basis git kompetanse, det ligger en fin beskrivelse av det pÃ¥ Beste Praksis siden fra KVAKK.\nMan trenger en konto pÃ¥ Github, det kan man opprette ved Ã¥ fÃ¸lge instruksjonene her.\nMan kan lÃ¦re seg Markdown (enkel tekst formatering), en fin oversikt finnes her.\nVerktÃ¸yet Quarto burde installeres for Ã¥ kunne se endringene slik som de ser ut pÃ¥ nettsiden. Installasjon instruksjoner finnes her.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Bidra til Dapla-manualen"
    ]
  },
  {
    "objectID": "statistikkere/contribution.html#fremgangsmÃ¥ten",
    "href": "statistikkere/contribution.html#fremgangsmÃ¥ten",
    "title": "Bidra til Dapla-manualen",
    "section": "FremgangsmÃ¥ten",
    "text": "FremgangsmÃ¥ten\n\nKlone repoet\n\n\n\nterminal\n\ngit clone https://github.com/statisticsnorway/dapla-manual.git\n\n\nLage en ny gren\nGjÃ¸re endringen\nKjÃ¸r fÃ¸lgende og fÃ¸lge lenken for Ã¥ sjekke at alt ser bra ut pÃ¥ nettsiden:\n\n\n\nterminal\n\nquarto preview dapla-manual\n\n\nÃ…pne en PR\nBe noen Ã¥ gjennomgÃ¥ endringen og eventuelt godkjenne den\nMerge PRen\nEtter et par minutter skal din endring vÃ¦re synlig!\n\n\nTittelblock og nyhetsinnlegg\nI dapla-manualen har vi noen konvensjoner vi fÃ¸lger. En av de er Ã¥ lage en title block som det kalles pÃ¥ quarto sin nettside.\nVi ber vÃ¥re bidragsytere om Ã¥ lage en lik block med overskrift og datoen artikkelen ble endret. Ã…pner man filen til denne siden ser det pÃ¥ dette tidspunktet slik ut:\n\n\n\n\n\n\nFigurÂ 1: Tittelblock for denne siden.\n\n\n\nManualens egen nyhetssiden skal oppdaters med et innlegg nÃ¥r det kommer nye artikler. Det er ikke nÃ¸dvendig Ã¥ lage en sak om at en side har blitt oppdatert, med mindre endringene er omfattende. Nyhetssiden er i Quarto sitt blog-format. FremgangsmÃ¥ten er enkel og beskrives i quarto sin artikkel om blogger. Ellers anbefales det Ã¥ ta en titt pÃ¥ hvordan det gjÃ¸res i dapla-manualen. Alex Crozier (akc@ssb.no) er ansvarlig for nyhetssiden og kan kontaktes dersom man trenger hjelp.\n\n\nEmbedded notebooks\nQuarto tilbyr Ã¥ legge ved (embed) notebooks inn i nettsiden. Dette er en fin mÃ¥te Ã¥ dele kode og output pÃ¥. Men det krever at vi tenker gjennom hvor outputen genereres. Siden Dapla-manualen renderes med GitHub-action, sÃ¥ Ã¸nsker vi ikke Ã¥ introdusere kompleksiteten det innebÃ¦rer Ã¥ generere output fra kode her. I tillegg er det mange miljÃ¸-spesifikke konfigurasjoner som bestemmer hvordan output kan genereres. Derfor anbefaler vi fÃ¸lgende tilnÃ¦rming nÃ¥r man skal legge ved en notebook til et kapittel:\n\nLogg deg inn i miljÃ¸et du Ã¸nsker Ã¥ bruke.\nKlone ned repoet til Dapla-manualen.\nOpprett en notebook i mappen ./dapla-manual/notebooks/notebook-navn.ipynb\nSkriv kode og generer output som du Ã¸nsker. Husk Ã¥ bare bruke data som alle har tilgang til eller de kan generere selv.\nEmbed output i kapitlet du Ã¸nsker iht til denne beskrivelsen\nPÃ¥ toppen av notebooken lager du en raw-celle der du skriver:\n\n\n\nnotebook\n\n---\nfreeze: true\n---\n\n\nKjÃ¸r denne notebooken fra terminalen med kommandoen:\n\n\n\nterminal\n\nquarto render ./notebooks/&lt;notebook-navn.ipynb&gt; --execute\n\nDette sikrer at vi kan se output etter at boken bygges i GitHub Actions. Husk at notebooks ikke blir strippet for output pÃ¥ vanlig mÃ¥te, slik at kun Ã¥pne data skal benyttes.\nSpÃ¸r Team Statistikktjenester eller Alex (akc@ssb.no) om du lurer pÃ¥ noe.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Bidra til Dapla-manualen"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html",
    "href": "statistikkere/gcc.html",
    "title": "Google Cloud Console (GCC)",
    "section": "",
    "text": "Google Cloud Console (GCC) er et web-basert grensesnitt for Ã¥ administrere ressurser og tjenester pÃ¥ Google Cloud Platform (GCP). Alle i SSB kan logge seg inn i GCC med sin SSB-bruker. Dapla-team har sjelden mulighet til Ã¥ opprette nye ressurser fra dette grensesnittet, siden vi Ã¸nsker at det skal gjÃ¸res med kode. Men det er likevel et nyttig verktÃ¸y for Ã¥ se pÃ¥ ressurser og gjÃ¸re endringer pÃ¥ eksisterende ressurser. I SSB bruker bruker vi GCC hovedsakelig til fÃ¸lgende:",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Google Cloud Console (GCC)"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html#innlogging",
    "href": "statistikkere/gcc.html#innlogging",
    "title": "Google Cloud Console (GCC)",
    "section": "Innlogging",
    "text": "Innlogging\nFor Ã¥ logge inn i GCC sÃ¥ gjÃ¸r du fÃ¸lgende:\n\nÃ…pne Google Cloud Console i en nettleser.\nLogg in med din SSB-bruker.\n\nHvis du ogsÃ¥ har en privat Google-konto som benyttes i samme nettleser, mÃ¥ du noen ganger passe pÃ¥ at du er logget inn med riktig konto. Dette kan du sjekke ved Ã¥ trykke pÃ¥ profilbildet ditt Ã¸verst til hÃ¸yre i GCC. Hvis du ikke er logget inn med riktig konto, sÃ¥ trykker du pÃ¥ Logg ut og logger inn pÃ¥ nytt med riktig konto.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Google Cloud Console (GCC)"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html#prosjektvelger",
    "href": "statistikkere/gcc.html#prosjektvelger",
    "title": "Google Cloud Console (GCC)",
    "section": "Prosjektvelger",
    "text": "Prosjektvelger\nEtter at du har logget deg pÃ¥ med din SSB-bruker, sÃ¥ mÃ¥ du velge hvilket av ditt teams prosjekter du Ã¸nsker Ã¥ jobbe med. Dette gjÃ¸r du ved Ã¥ trykke pÃ¥ prosjektvelgeren Ã¸verst til venstre pÃ¥ siden. Vidoen under viser hvordan du velger et prosjekt og lister ut hvilke bÃ¸tter som finnes i prosjektet.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Google Cloud Console (GCC)"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html#filutforsker",
    "href": "statistikkere/gcc.html#filutforsker",
    "title": "Google Cloud Console (GCC)",
    "section": "Filutforsker",
    "text": "Filutforsker\nFor Ã¥ utforske bÃ¸tter og filer i et Dapla-team sitt Google-prosjekt sÃ¥ kan man bruke Cloud Storage-grensesnittet i GCC. For Ã¥ bruke denne funksjonaliteten gjÃ¸r du fÃ¸lgende:\n\nBruk prosjektvelgeren til Ã¥ velge Ã¸nsket prosjekt.\nDeretter sÃ¸ker du opp Google Storage i sÃ¸kefeltet Ã¸verst pÃ¥ siden.\n\nDa fÃ¥r du en oversikt over alle bÃ¸ttene i prosjektet. Velg Ã¸nsker bÃ¸tte for Ã¥ utforske innholdet i bÃ¸tta.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Google Cloud Console (GCC)"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html#logger",
    "href": "statistikkere/gcc.html#logger",
    "title": "Google Cloud Console (GCC)",
    "section": "Logger",
    "text": "Logger\nKommer snart.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Google Cloud Console (GCC)"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html#transfer-service",
    "href": "statistikkere/gcc.html#transfer-service",
    "title": "Google Cloud Console (GCC)",
    "section": "Transfer Service",
    "text": "Transfer Service\nLes mer om hvordan man bruker Transfer Service her.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Google Cloud Console (GCC)"
    ]
  },
  {
    "objectID": "statistikkere/altinn-bygge-prefill.html",
    "href": "statistikkere/altinn-bygge-prefill.html",
    "title": "Bygge skjemaprefill",
    "section": "",
    "text": "Denne siden forklarer hvordan du gÃ¥r fram for Ã¥ bygge din egen skjemaprefill. Med skjemaprefill menes prefill som gÃ¥r ut over det som er standard prefill i skjemaet.",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Prefill"
    ]
  },
  {
    "objectID": "statistikkere/altinn-bygge-prefill.html#skjema-prefill-meta",
    "href": "statistikkere/altinn-bygge-prefill.html#skjema-prefill-meta",
    "title": "Bygge skjemaprefill",
    "section": "Skjema prefill meta",
    "text": "Skjema prefill meta\nFor hver skjemaversjon eksisterer det en skjema prefill meta tabell. Tabellen gir informasjon om hvilke skjemaspesifikke prefill-felter som skjemaet kan inneholde.\n\n\n\n\n\n\nFigurÂ 1: Eksempel pÃ¥ skjema prefill meta tabell\n\n\n\n\n\n\n\n\n\nHva hvis skjemaprefill meta mangler?\n\n\n\nTa kontakt med planleggeren for skjemaet pÃ¥ seksjon 821 dersom skjemaet ditt skal inneholde skjemaprefill og meta-tabellen ikke inneholder data.\n\n\n\nMetadata beskrivelse\nTabellÂ 1 beskriver feltene i skjemaprefill meta-tabellen.\n\n\n\nTabellÂ 1: Prefill Meta\n\n\n\n\n\n\n\n\n\nMeta\nForklaring\n\n\n\n\nKomplett sti\nRefererer til hele stien som brukes for Ã¥ navigere fra SkjemaData til en spesifikk verdi. Den beskriver nÃ¸yaktig plasseringen av dataene i hierarkiet og kan bestÃ¥ av flere nivÃ¥er.\n\n\nType\nBeskriver hvilken datatype feltet kan inneholde.\n\n\nMin\nBegrensing (minimum) pÃ¥ datatypen.\n\n\nMaks\nBegrensing (makimum) pÃ¥ datatypen.\n\n\nObligatorisk\nIndikerer om feltet er pÃ¥krevd for prefill.\n\n\nDublett-sjekk\nAngir om feltet skal inneholde unike verdier.\n\n\nStatistikk navn\nKan brukes for mapping til statistikkteamenes interne systemer.\n\n\nBeskrivelse\nEventuell beskrivelse av feltet.\n\n\nKommentar\nEventuelle kommentarer om feltet\n\n\n\n\n\n\n\n\nHente prefill meta med kode\nFor Ã¥ hente ut skjema prefill meta i Python, kan du bruke metoden get_prefill_meta_by_skjema_def i SuvClient. SÃ¸rg for at du oppgir riktig RA-nummer, versjon og undersÃ¸kelsesnummer.\n\n\nnotebook\n\nclient = SuvClient()\n\noutput = client.get_prefill_meta_by_skjema_def(\n            ra_nummer = 'RA-0666A3',\n            versjon = 1,\n            undersokelse_nr = '1060'\n        )\n\nprint(json.dumps(output, indent=4))",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Prefill"
    ]
  },
  {
    "objectID": "statistikkere/altinn-bygge-prefill.html#struktur-pÃ¥-skjemaprefill",
    "href": "statistikkere/altinn-bygge-prefill.html#struktur-pÃ¥-skjemaprefill",
    "title": "Bygge skjemaprefill",
    "section": "Struktur pÃ¥ skjemaprefill",
    "text": "Struktur pÃ¥ skjemaprefill\nSkjemaprefill kan representeres som en hierarkisk struktur basert pÃ¥ sti_navn. Dette gir en oversikt over hvordan dataene er organisert. FÃ¸lgende kode viser hvordan du kan vise strukturen med valgfri inkludering av metadata.\n\n\nnotebook\n\nclient = SuvClient()\n\nresultat = client.get_prefill_meta_by_skjema_def(\n                ra_nummer = 'RA-0666A3',\n                versjon = 1,\n                undersokelse_nr = '1060'            \n)\n\ndef build_structure(data, include_metadata=False):\n    structure = {}\n\n    for item in data:\n        path = item['sti_navn'].split('.') if item['sti_navn'] else [item['navn']]\n        current_level = structure\n\n        for part in path:\n            if part not in current_level:\n                current_level[part] = {\"metadata\": {}} if include_metadata else {}\n            current_level = current_level[part]\n\n        if include_metadata:\n            current_level[\"metadata\"][\"type\"] = item.get(\"type\")\n            current_level[\"metadata\"][\"kontroll\"] = item.get(\"kontroll\")\n            current_level[\"metadata\"][\"stat_navn\"] = item.get(\"stat_navn\")\n            current_level[\"metadata\"][\"kommentar\"] = item.get(\"kommentar\")\n\n    return structure          \n\nresult_without_metadata = build_structure(resultat, include_metadata=False)\nprint(json.dumps(result_without_metadata, indent=4))\n\nEksempel pÃ¥ output for hierarkisk struktur uten metadata:\n{\n    \"innkjoptElektriskKraft\": {},\n    \"innkjoptElektriskKost\": {},\n    \"GassOgPetroleumKost\": {\n        \"produktTypeBruktId\": {},\n        \"produktTypeBrukt\": {},\n        \"produktEnhet\": {}\n    },\n    \"EgneprodEnergiProd\": {\n        \"egenprodEnergiProdTypeID\": {},\n        \"egenprodEnergiProdType\": {},\n        \"egenprodEnergiProdEnhet\": {}\n    }\n}",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Prefill"
    ]
  },
  {
    "objectID": "statistikkere/altinn-bygge-prefill.html#lagre-prefill-data",
    "href": "statistikkere/altinn-bygge-prefill.html#lagre-prefill-data",
    "title": "Bygge skjemaprefill",
    "section": "Lagre prefill data",
    "text": "Lagre prefill data\nFor Ã¥ lagre prefill for en enhet bruker du metoden save_prefill_for_enhet i SuvClient. Metoden lagrer skjemaprefill for en spesifikk enhet i utvalget. Beskrivelse av hvordan du henter utvalg finner du pÃ¥ siden Utvalg fra SFU\n\n\nnotebook\n\nclient = SuvClient()\n\noutput = client.save_prefill_for_enhet(\n     ra_nummer = 'RA-0666A3',\n     versjon = 1,\n     periode_aar = 2024,\n     periode_type = 'KVRT',\n     periode_nr = 2,\n     enhetsident = 'A3TF0018',\n     enhetstype = 'FRTK',\n     prefill = {\n        \"innkjoptElektriskKost\": 10, \n        \"innkjoptElektriskKraft\": 20, \n        \"GassOgPetroleumKost\": [\n            {\"produktTypeBruktId\": \"1\"},\n            {\"produktTypeBruktId\": \"2\"}\n        ]\n    }                  \n )\nprint(output)    \n\nTabellÂ 2 viser en beskrivelse av de ulike parameterne i save_prefill_for_enhet\n\n\n\nTabellÂ 2: Beskrivelse av parametere\n\n\n\n\n\n\n\n\n\nParameter\nForklaring\n\n\n\n\nRA-nummer\nRA-nummer for skjemaet.\n\n\nversjon\nSkjemaversjon.\n\n\nperiode_aar\nÃ…rstall for perioden.\n\n\nperiode_type\nPeriode type. Gyldige verdier er AAR, MND, KVRT, UKE\n\n\nperiode_nr\nPeriode nummer\n\n\nenhetsident\nIdentifikator for enheten\n\n\nenhetstype\nType enhet. Gyldige verdier er FRTK, BEDR, PERS\n\n\nprefill\nPrefill-data i gyldig JSON-format.",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Prefill"
    ]
  },
  {
    "objectID": "statistikkere/altinn-bygge-prefill.html#validering-av-prefill-data",
    "href": "statistikkere/altinn-bygge-prefill.html#validering-av-prefill-data",
    "title": "Bygge skjemaprefill",
    "section": "Validering av prefill data",
    "text": "Validering av prefill data\nValidering av skjemaprefill sikrer at dataene oppfyller kravene til struktur og innhold. Dette gjÃ¸res automatisk i save_prefill_for_enhet. Feil i valideringen vil fÃ¸re til at dataene ikke lagres, og det gis en feilmelding. Dersom du Ã¸nsker Ã¥ sjekke om prefill er gyldig fÃ¸r lagring er dette mulig ved hjelp av metoden validate_skjemadata.\n\n\nnotebook\n\nclient = SuvClient()\n\nclient.validate_skjemadata(      \n        ra_nummer = 'RA-0678A3',\n        versjon = 2,\n        skjemadata = {\"antallAnsattePrefill\": \"10\"}\n)   \n\ndapla-suv-tools pakken inneholder ogsÃ¥ metoder for Ã¥ hente ut lagret prefill og slette prefill. Det er mulig Ã¥ slette prefill enten pÃ¥ enhetsnivÃ¥ eller skjemanivÃ¥.\nValideringen er basert pÃ¥ JSON Schema.\n\n\n\n\n\n\nWarning\n\n\n\nDersom skjemaet blir instansiert med ugyldige prefill data vil skjemaet feile ved instansiering hos Altinn. Skjemaet vil i dette tilfellet bli slettet fra innboksen til oppdragsgiver.",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Prefill"
    ]
  },
  {
    "objectID": "statistikkere/altinn-bygge-prefill.html#eksempelkode",
    "href": "statistikkere/altinn-bygge-prefill.html#eksempelkode",
    "title": "Bygge skjemaprefill",
    "section": "Eksempelkode",
    "text": "Eksempelkode\nNoe demokode ligger i repoet, og kan vÃ¦re ett godt utgangspunkt Ã¥ kopiere og endre fra.\n\nRA-0678 (Ledige stillinger)\nDette eksemplet viser hvordan du kan bygge skjemaprefill for RA-0678 (Ledige stillinger). Nedenfor ser du hvordan innholdet i SkjemaData blokka er strukturert.\n{\n    \"SkjemaData\": {\n        \"antallAnsattePrefill\": {\n            \"type\": \"string\"\n        },\n        \"datoPrefill\": {\n            \"type\": \"string\"\n        }\n    }\n}\nSkjemaet inneholder to felter som skal forhÃ¥ndsutfylles. Det er antallAnsattePrefill og datoPrefill.\nTabellÂ 3 beskriver feltene som kan forhÃ¥ndsutfylles.\n\n\n\nTabellÂ 3: Prefill felter\n\n\n\n\n\n\n\n\n\nFelt\nSti\n\n\n\n\nantallAnsattePrefill\n-\n\n\ndatoPrefill\n-\n\n\n\n\n\n\nEksempelet leser prefill-data fra en tekstfil og bygger den nÃ¸dvendige strukturen for Ã¥ forhÃ¥ndsutfylle skjemaet for utvalgte enheter.\n\n\nRA-0692 (Utenrikshandel med tjenester)\nDette eksemplet viser hvordan du kan bygge skjemaprefill for RA-0692 (Utenrikshandel med tjenester). Nedenfor ser du hvordan innholdet i SkjemaData blokka er strukturert.\n{\n   \"SkjemaData\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"leveransetype\": {\n                \"type\": \"string\",\n                \"enum\": [\n                    \"0\",\n                    \"1\",\n                    \"2\"\n                  ]\n            },\n            \"Eksport\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"$ref\": \"#/$defs/Eksport\"                \n              }\n            },\n            \"Import\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"$ref\": \"#/$defs/Import\"\n            }\n        }\n    },   \n    \"Eksport\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"cpaLevel1Eksport\": {\n                \"type\": \"string\"        \n            },\n            \"cpaEksport\": {\n                \"type\": \"string\"\n            },\n            \"PostEksport\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"$ref\": \"#/$defs/PostEksport\"\n            }\n        }\n    },\n    \"Import\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"cpaLevel1Import\": {\n                \"type\": \"string\",          \n            },\n            \"cpaImport\": {\n                \"type\": \"string\"\n            },\n            \"PostImport\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"$ref\": \"#/$defs/PostImport\"\n            }\n        } \n    },\n    \"PostEksport\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"landkodeEksport\": {\n                \"type\": \"string\"\n            },\n            \"forrigeKvartalKrEksport\": {\n                \"type\": \"integer\"\n            },\n            \"forrigeKonsernIntKrEksport\": {\n                \"type\": \"integer\"\n            },\n        }\n    },\n    \"PostImport\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"landkodeImport\": {\n                \"type\": \"string\",\n            },\n            \"forrigeKvartalKrImport\": {\n                \"type\": \"integer\"\n            },\n            \"forrigeKonsernIntKrImport\": {\n                \"type\": \"integer\"\n            },\n        }\n    }\n}\nMerk at bÃ¥de Eksport/Import og PostEksport/PostImport er repeterende. Den ytterste gruppa er CPA-verdien, mens den innerste gruppa inneholder landskodene og kronebelÃ¸pene.\nTabellÂ 4 beskriver feltene som kan forhÃ¥ndsutfylles.\n\n\n\nTabellÂ 4: Prefill felter\n\n\n\n\n\n\n\n\n\nFelt\nSti\n\n\n\n\nleveransetype\n-\n\n\ncpaLevel1Eksport\nEksport\n\n\ncpaEksport\nEksport\n\n\nlandkodeEksport\nEksport.PostEksport\n\n\nforrigeKvartalKrEksport\nEksport.PostEksport\n\n\nforrigeKonsernIntKrEksport\nEksport.PostEksport\n\n\ncpaLevel1Import\nImport\n\n\ncpaImport\nImport\n\n\nlandkodeImport\nImport.PostImport\n\n\nforrigeKvartalKrImport\nImport.PostImport\n\n\nforrigeKonsernIntKrImport\nImport.PostImport\n\n\n\n\n\n\nEksempelet leser prefill-data fra flere tekstfiler og bygger den nÃ¸dvendige strukturen for Ã¥ forhÃ¥ndsutfylle skjemaet for utvalgte enheter.",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Prefill"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-editor.html",
    "href": "statistikkere/datadoc-editor.html",
    "title": "Datadoc-editor",
    "section": "",
    "text": "Datadoc editor er et grafisk grensesnitt for Ã¥ dokumentere datasett og variablene som utgjÃ¸r datasettet.\nFormÃ¥let med tjenesten er Ã¥ tilby et lett-Ã¥-bruke grensesnitt som hovedsakelig vil benyttes fÃ¸rste gang man dokumenterer en type datasett.\nSiden lÃ¸pende statistikkproduksjon ofte innebÃ¦rer at nye data legges til data fra tidligere perioder, uten at strukturen i datasett endres, sÃ¥ tilbys det ogsÃ¥ et annet verktÃ¸y som lar brukeren programmatisk gjenbruke metadata fra en tidligere periode. Les mer om Python-pakken dapla-toolbelt-metadata.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Datadoc-editor"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-editor.html#forberedelser",
    "href": "statistikkere/datadoc-editor.html#forberedelser",
    "title": "Datadoc-editor",
    "section": "Forberedelser",
    "text": "Forberedelser\nFÃ¸r man starter Datadoc-editor-tjenesten bÃ¸r man ha lest kapitlet om Dapla Lab. Deretter gjÃ¸r du fÃ¸lgende:\n\nLogg deg inn pÃ¥ Dapla Lab\nUnder Tjenestekatalog trykker du pÃ¥ Start-knappen for Datadoc-editor\nGi tjenesten et navn\nÃ…pne Datadoc-editor konfigurasjoner og gjÃ¸r Ã¸nskede konfigurasjoner (se neste kapittel).\nTrykk Start igjen for Ã¥ Ã¥pne tjenesten.\n\nDatadoc editor bruker ca. 1 minutt pÃ¥ starte og etter det klart for dokumentere datasett.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Datadoc-editor"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-editor.html#konfigurasjon",
    "href": "statistikkere/datadoc-editor.html#konfigurasjon",
    "title": "Datadoc-editor",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nFÃ¸r man starter Datadoc editor bÃ¸r man konfigurere tjenesten. Dette er spesielt viktig siden du bare kan representere et Dapla-team for hver Datadoc editor man starter. I tjenestekonfigurasjonen til Datadoc er det to nedtrekksmenyer: Data og Tjeneste.\n\nData\nUnder Data kan man velge Team og tilgangsgruppe. I denne menyen fÃ¥r du listet alle team og tilgangsgrupper du er med i. Listen vises pÃ¥ formen &lt;daplateam&gt;-&lt;tilgangsgruppe&gt;.\nFigurÂ 1 viser tilfellet der det er valgt Ã¥ representere tilgangsgruppen developers i teamet Dapla Felles, derav dapla-felles-developers. Dette er standardvalget.\n\n\n\n\n\n\nFigurÂ 1: Data-menyen i tjenestekonfigurasjonen for Datadoc editor.\n\n\n\nDatadoc editor stÃ¸tter for Ã¸yeblikket ikke kildedata selv om man kan velge begrunnelse og tilgangvarighetÂ fra konfigurasjons fanen.\n\n\nTjeneste\nUnder menyen Tjeneste kan man velge versjon av tjenesten. Det vil vÃ¦re svÃ¦rt sjelden at brukere trenger Ã¥ endre pÃ¥ noe her. Som standard Ã¥pnes alltid siste versjon av tjenesten.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Datadoc-editor"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-editor.html#datatilgang",
    "href": "statistikkere/datadoc-editor.html#datatilgang",
    "title": "Datadoc-editor",
    "section": "Datatilgang",
    "text": "Datatilgang\nNÃ¥r man starter en Datadoc-editor tjeneste mÃ¥ man pÃ¥ forhÃ¥nd velge hvilket team og tilgangsgruppe man skal representere, som forklart i forrige del.\n\n\n\n\n\n\ndata-admins ikke tilgjengelig enda\n\n\n\nDet er ikke mulig Ã¥ velge andre tilgangsgrupper enn developers for Ã¸yeblikket. Av den grunn kan man ikke bruke Datadoc editor til Ã¥ dokumentere kildedata enda.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Datadoc-editor"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-editor.html#funksjonalitet",
    "href": "statistikkere/datadoc-editor.html#funksjonalitet",
    "title": "Datadoc-editor",
    "section": "Funksjonalitet",
    "text": "Funksjonalitet\n\nÃ…pne datasett\nFÃ¸r man kan benytte Datadoc editor, mÃ¥ man Ã¥pne et Datasett. Det gjÃ¸res enkelt ved Ã¥ lime inn stien til datasettet i Filsti tekstboksen (Punkt 1 i FigurÂ 2) og trykke pÃ¥ Ã…pne fil knappen (Punkt 2 i FigurÂ 2).\nDatadoc editor benytter brukerens innloggingsopplysninger for Ã¥ aksessere data. Det betyr at man i utgangspunktet har tilgang til de samme filene som ellers pÃ¥ Dapla.\n\n\n\n\n\n\nWarning\n\n\n\nMan mÃ¥ inkludere gs:// pÃ¥ begynnelsen av stien nÃ¥r man jobber med et datasett i en bÃ¸tte.\n\n\n\n\n\n\n\n\nNote\n\n\n\nMan kan finne filstien gjennom Google konsollet eller ved Ã¥ benytte Dapla toolbelt\n\n\n\n\n\n\n\n\nFigurÂ 2: Input-feltet for Ã¥ oppgi filsti i Datadoc editor.\n\n\n\n\nVellykket Ã¥pning\nEtter at man har trykket pÃ¥ Ã…pne fil knappen bÃ¸r man se meldingen vist i FigurÂ 3\n\n\n\n\n\n\nFigurÂ 3: Meldingen at det var velykket Ã¥ Ã¥pne datasettet.\n\n\n\n\n\nVellykket Ã¥pning med advarsel\nHvis man Ã¥pner et datasett som ikke fÃ¸lger navnestandarden, vil det komme en advarsel (FigurÂ 4). Det er fortsatt fullt mulig Ã¥ bruke Datadoc editor for Ã¥ dokumentere datasettet, men ikke like mye metadata kan utledes automatisk (TODO: lenke til seksjonen om utledning).\n\n\n\n\n\n\nNote\n\n\n\nDette kan vÃ¦re en fin anledning til Ã¥ justere pÃ¥ navngivning og strukturen i teamets bÃ¸tter slik at alt fÃ¸lger navnestandarden. Det er en lenke til navnestandarden i meldingen.\n\n\n\n\n\n\n\n\nFigurÂ 4: Meldingen at datasettet ikke fÃ¸lger navnestandarden.\n\n\n\n\n\nFeil ved Ã¥pning\nHvis Datadoc editor ikke klarer Ã¥ Ã¥pne datasettet vises en rÃ¸d error melding (FigurÂ 5). Som oftest forÃ¥rsakes dette av at filen ikke finnes (skrivefeil) eller fordi man ikke har tilgang til filen.\n\n\n\n\n\n\nFigurÂ 5: Meldingen at det var en feil ved Ã¥pning av datasettet.\n\n\n\n\n\nÃ…pne et datasett nÃ¥r metadatadokument eksisterer\nHvis et metadatadokument eksisterer, er det denne informasjonen som lastes inn. Det utledes ingenting fra datasettet.\n\n\n\nUtledet informasjon\nInformasjon som kan utledes vil bli fylt inn nÃ¥r du Ã¥pner datasettet. Informasjonen hentes enten fra filstien eller settes inn som en default verdi (*). Det er mulig Ã¥ korrigere informasjonen i ettertid. FÃ¸lgende felter blir forsÃ¸kt utledet:\nDatasett:\n\nVerdivurdering\nStatus (*)\nDatatilstand\nVersjon\nStatistikkomrÃ¥de\nInneholder data f.o.m.\nInneholder data t.o.m.\nGeografisk dekningsomrÃ¥de (*)\n\nVariabler:\n\nKortnavn\nDatatype\n\n\n\nDokumentere datasett-metadata\nDokumentasjon av datasettet som helthet gjÃ¸res i datasettfanen i Datadoc editor.\nAlle felter har en ordforklaring  du kan trykke pÃ¥. Her vil du fÃ¥ en kort forklaring til hva som skal stÃ¥ i feltet.\nFlere felter har verdilister hvor mange er hentet fra KLASS, mens noen er fritekstfelter. For noen av fritekstfeltene gjÃ¸res det en sjekk av innholdet og du vil fÃ¥ en feilmelding hvis kriteriene ikke er oppfylt.\n\nObligatorisk\nAlt som stÃ¥r under obligatorisk mÃ¥ fylles inn.\n\n\nAnbefalt\nAnbefalte felter er frivillig Ã¥ fylle ut.\n\n\nMaskingenerert\nFeltene her genereres automatisk og kan ikke redigeres. De er kun med til informasjon.\n\n\n\nDokumentere variabelforekomst-metadata\nDokumentasjon av variabelforekomster for et datasett kan gjÃ¸res i variabelfanen i Datadoc editor. Her vil man se en liste av alle kortnavnene til variabelforekomstene i datasettet. Ved Ã¥ trykke seg inn pÃ¥ et av kortnavnene kan man dokumentere de obligatoriske og anbefalte feltene for en variabelforekomst.\n\nArv mellom datasett og variabelforekomst fanen\nFor Ã¥ forenkle dokumentasjonen av variabelforekomster vil noen felt arve verdiene som blir satt i datasettfanen. Dette gjelder fÃ¸lgende felter:\n\nDatakilde\nPopulasjon\nTemporalitetstype\nInneholder data f.o.m\nInneholder data t.o.m\n\nDet er mulig Ã¥ redigere vediene i variabelforekomst fanen etter en verdi er satt i datasettfanen. Hvis disse feltene blir endret i datasettfanen senere, vil de alltid overskrive det som er satt i variabelforekomst fanen.\n\n\nSÃ¸k i variabelforekomster\nDet er mulig Ã¥ sÃ¸ke gjennom variabelforekomstene sine kortnavn. Dette filtrerer pÃ¥ listen over variabelforekomster.\n\n\n\n\n\n\nFigurÂ 6: SÃ¸k gjennom kortnavn til variabelforekomster\n\n\n\n\n\n\nLagre metadata\n\n\n\n\n\n\nViktig informasjon\n\n\n\nDatadoc editor mellomlagrer ikke utfylt metadata.\nPass pÃ¥ Ã¥ lagre metadataene ofte ved Ã¥ trykke lagre og legg merke til om du fÃ¥r en bekreftelse pÃ¥ at metadataene er lagret.\n\n\n\nVed lagring\nNÃ¥r du trykker Lagre metadata knappen vil du fÃ¥ en bekreftelse pÃ¥ vellykket lagring. \nHvis ikke alle obligatoriske felt er utfylt vil du fÃ¥ opp en advarsel for datasett og variabelforekomstene. Advarselen for datasett viser en liste over hvilke felt som mangler. For variabelforekomster vises bÃ¥de variabelens kortnavn og manglende felt.\nNÃ¥r du fyller ut de manglende obligatoriske feltene mÃ¥ du lagre pÃ¥ nytt og advarslene vil forsvinne nÃ¥r alle obligatoriske felt er fylt ut.\nVed lagring gjÃ¸res det ogsÃ¥ en sjekk pÃ¥ om variabel kortnavene avvikerÂ for navnestandardenÂ for variabelnavn. Om det finnes avvik vil disse kortnavene vises i en gul advarsel boks. Navnestandarden for variabelkortnavn er som fÃ¸lger:\n\nAlfanumerisk begrenset til a-z (kun smÃ¥ bokstaver)\n0-9\n_ (understrek).\n\n\n\nMetadata filen\nNÃ¥r du trykker pÃ¥ Lagre metadata knappen i Datadoc editor skrives alle metadata til en fil i samme mappe (katalog) som datafilen. Dette er en JSON-fil med nesten samme navn som datafilen. Navnekonvensjonen for metadatafilen er\n&lt;navn pÃ¥ datasettfilen uten endelse&gt;__DOC.json\nEksempelvis vil Datadoc lagre metadata i filen skattedata_p2022_v1__DOC.json hvis datafilen har navnet skattedata_p2022_v1.parquet.\nFordelen med Ã¥ benytte en JSON-fil til Ã¥ lagre metadata er at denne filen kan kopieres og flyttes like enkelt som selve datafilen. JSON-filer er strukturerte tekstfiler som kan leses av bÃ¥de maskiner (Python/R) og av mennesker (Ã¥pnes i en tekst-editor).\nSe et eksempel pÃ¥ JSON metadata-fil lagret av DataDoc.\n\n\n\nModifisere metadata\nÃ˜nsker du Ã¥ endre eller legge til metadata, Ã¥pner du et datasett slik som beskrevet i Ã…pne et datasett. Da vil innholdet fra metadata-filen leses inn i Datadoc editor og kan redigeres videre. Endringene blir lagret nÃ¥r man trykker Lagre metadata.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Datadoc-editor"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-editor.html#kildekode",
    "href": "statistikkere/datadoc-editor.html#kildekode",
    "title": "Datadoc-editor",
    "section": "Kildekode",
    "text": "Kildekode\nKildekoden til Datadoc editor er offentlig tilgjengelig pÃ¥ Github: https://github.com/statisticsnorway/datadoc",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Datadoc-editor"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-botter.html",
    "href": "statistikkere/hva-er-botter.html",
    "title": "Hva er bÃ¸tter?",
    "section": "",
    "text": "PÃ¥ Dapla er det Google Cloud Storage (GCS) som benyttes til Ã¥ lagre data og filer. FÃ¸lgelig er det GCS som erstatter det vi kjente som Linux-stammene i prodsonen tidligere. I SSB har vi vÃ¦rt vant til Ã¥ jobbe med data lagret pÃ¥ filsystemer i et Linux-miljÃ¸1. GCS-bÃ¸ttene skiller seg fra klassiske filsystemer pÃ¥ flere mÃ¥ter, og det er viktig Ã¥ vÃ¦re klar over disse forskjellene. I denne kapitlet vil vi gÃ¥ gjennom noen av de viktigste forskjellene og hvordan man gjÃ¸r vanlige operasjoner mot bÃ¸tter i GCS.\n\n\nI et Linux- eller Windows-filsystem, som vi har vÃ¦rt vant til tidligere, sÃ¥ er filer og mapper organisert i en hierarkisk struktur pÃ¥ et operativsystem (OS). I SSB har OS-ene vÃ¦rt installert pÃ¥ fysiske maskiner som vi vedlikeholder selv.\nEn bÃ¸tte i GCS er derimot en kjÃ¸pt tjeneste som lar brukeren lagre alle typer objekter i en container. Man trenger altsÃ¥ ikke Ã¥ tenke pÃ¥ om filene ligger i et hierarki, hvilket operativsystem det kjÃ¸rer pÃ¥, eller hvor mye diskplass som er tilgjengelig.\nAt data er lagret som objekter i en bÃ¸tte har noen praktiske implikasjoner for hvordan vi jobber med data i SSB:\n\nMange er vant til Ã¥ jobbe direkte med filer i en Linux-terminal eller via systemkall fra sprÃ¥k som SAS, Pyton eller R. For Ã¥ gjÃ¸re det samme i Jupyter mot en bÃ¸tte, sÃ¥ kan vi bruke Python-pakken gcsfs. Se eksempler under.\nNÃ¥r vi bruker Python- eller R-pakker for lese eller skrive data fra bÃ¸tter, sÃ¥ er vi avhengig av at pakkene tilbyr integrasjon mot bÃ¸tter. Mange pakker gjÃ¸r det, men ikke alle. For de som ikke gjÃ¸r det kan vi bruke ofte bruke gcsfs til Ã¥ gi oss et python-objekt, som igjen kan brukes av de fleste pakker.\nDet finnes egentlig ikke noen mapper i bÃ¸tter. I motsetning til et vanlig filsystem sÃ¥ er det ikke en hierarkisk mappestruktur i en bÃ¸tte. Det vil si at alt ligger et sted, men de kan ha lange navn med /-tegn slik at det ligner pÃ¥ et klassisk filsystem. Bruker du / i objekt-navnet sÃ¥ vil ogsÃ¥ Google Cloud Console vise det som mapper, men det er bare for Ã¥ gjÃ¸re det enklere Ã¥ forholde seg til. En praktisk konsekvens av dette er f.eks. at man ikke trenger Ã¥ opprette en mappe fÃ¸r man legger et fil/objekt i den. Det er bare en tekststreng som er en del av objekt-navnet.\n\nUnder kommer det en del eksempler pÃ¥ hvordan man kan jobbe med objekter i bÃ¸tter pÃ¥ samme mÃ¥te som filer i et filsystem.\n\n\n\nPÃ¥ Dapla skal data lagres i bÃ¸tter. Men nÃ¥r du Ã¥pner Jupyterlab sÃ¥ fÃ¥r du ogsÃ¥ et â€œlokaltâ€ eller klassisk filsystem, slik vi definerte det i forrige kapittel. Det er dette filsystemet du ser i filutforskeren i Jupyterlab, slik som vist til venstre i FigurÂ 1. Det er ogsÃ¥ dette filsystemet du ser nÃ¥r du f.eks. bruker ls-kommandoen i en terminal i Jupyterlab.\n\n\n\n\n\n\nFigurÂ 1: Til venstre i bildet ser du filutforskeren i Jupyterlab\n\n\n\nDette filsystemet er ment for Ã¥ lagre kode midlertidig mens du jobber med dem. Det er ikke ment for Ã¥ lagre data. Det er heller ikke ment som et permanent lagringssted for kode. Permanent lagring av kode skal gjÃ¸res pÃ¥ GitHub. Selv om filene du lagrer der fortsetter Ã¥ eksistere for hver gang du logger deg inn i Jupyterlab, sÃ¥ bÃ¸r kode du Ã¸nsker Ã¥ bevare pushes til GitHub fÃ¸r du avslutter en sesjon i Jupyterlab. Se en mer teknisk beskrivelse av hvordan dette fungerer i boksen under.\n\n\n\n\n\n\nHvem er egentlig denne Jovyan?\n\n\n\nNÃ¥r du logger deg inn i Jupyterlab pÃ¥ Dapla, sÃ¥ ser du at brukeren din pÃ¥ det lokale filsystemet heter jovyan. Grunnen er at det er en Linux-container som kjÃ¸rer et Jupyterlab Docker-image, og alle bruker denne containeren. Det er derfor vi kaller det et â€œlokaltâ€ filsystem, fordi det er lokalt i containeren.\nI tillegg er det satt opp persistent volum (PV) og en persistent volume claim (PVC) som er knyttet til denne containeren. Det er dette som gjÃ¸r at filene du lagrer i Jupyterlab blir lagret mellom hver gang du logger deg inn. I tillegg tas det en backup av filsystemene hver dag, slik at vi kan gjenopprette filene dine hvis noe skulle skje med PV-en.\nLagringsplassen pÃ¥ PV-en er begrenset til 10 GB per bruker. Grunnen til at vi ikke tilbyr mer lagringsplass er fordi det kun er kode skal lagres her, og at denne lagringsplassen er relativt dyrt sammenlignet med GCS-bÃ¸tter. Hvis du jobber i virtuelle miljÃ¸er og lagrer mange miljÃ¸er lokalt, kan du oppleve at disken blir full. Les mer her om hvordan man bÃ¸r hÃ¥ndere dette.\n\n\n\n\n\nTidligere har vi diskutert forskjellene mellom bÃ¸tter og filsystemer. Mange kjenner hvordan man gjÃ¸r systemkommandoer2 i klassiske filsystemer fra en terminal eller fra sprÃ¥k som Python, R og SAS. I dette kapitlet viser vi hvordan dette kan gjÃ¸res mot bÃ¸tter fra Jupyterlab.\ngcsfs er en Python-pakke som lar deg jobbe med objekter i bÃ¸tter pÃ¥ nesten samme mÃ¥te som filer i et filsystem. For Ã¥ kunne gjÃ¸re det mÃ¥ vi fÃ¸rst sette opp en filsystem-instans som lar oss bruke en bÃ¸tte som et filsystem. Pakken dapla-toolbelt lar oss gjÃ¸re det ganske enkelt:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs er nÃ¥ et filsystem-versjon av bÃ¸ttene vi har tilgang til pÃ¥ GCS. Vi kan nÃ¥ bruk fs til Ã¥ gjÃ¸re typiske operasjoner vi har vÃ¦rt vant til Ã¥ gjÃ¸re i en Linux-terminal. Se en fullstendig liste over kommandoer her.\nUnder vises noen flere eksempler pÃ¥ nyttige funksjoner enn det som ble vist i kapitlet tidligere i boken\n\n\n\n\n\n\nWarning\n\n\n\nSelv om ordet mapper blir brukt i eksemplene under, sÃ¥ er det viktig Ã¥ huske at det ikke finnes noen mapper i bÃ¸tter. Det er bare en tekststreng som er en del av objekt-navnet. Men siden vi her jobber med et filsystem-representasjon av bÃ¸tten, sÃ¥ tillater vi oss Ã¥ gjÃ¸re det for Ã¥ gjÃ¸re det enklere Ã¥ lese.\n\n\n\n\nfs.glob() lar oss sÃ¸ke etter filer i bÃ¸tten. Vi kan bruke *, **, ? og [..] som wildcard for Ã¥ finne det vi trenger. Det som returneres er enten en liste eller dictionary avhengig av hva vi velger Ã¥ gjÃ¸re.\nHent en liste over alle filer i en undermappe R_smoke_test i bÃ¸tta gs://ssb-dapla-felles-data-delt-prod:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/R_smoke_test/*\")\n\nNÃ¥r vi legger til * pÃ¥ slutten av filstien sÃ¥ returnerer den alle filer i den eksakte undermappen. Men hvis vi Ã¸nsker Ã¥ Ã¥ fÃ¥ alle filer i alle undermapper, sÃ¥ kan vi bruke ** pÃ¥ denne mÃ¥ten:\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/R_smoke_test/**\").\nVi kan ogsÃ¥ sÃ¸ke mer avansert ved ved Ã¥ bruke ?. ?-tegnet sier at en enkeltkarakter kan vÃ¦re hva som helst. F.eks. kunne vi skrevet\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/R_smoke_test/**/R_smoke_test_??.csv\")\nfor Ã¥ rekursivt liste ut alle filer som starter med R_smoke_test_, etterfulgt av to karakterer av hvilken som helst type, og slutter med .csv.\nHvis vi viste at de to karakterene ikke skulle vÃ¦re av hvilken som helst karakterer, men det var en liten bokstav, etterfulgt av et tall mellom 2-6, sÃ¥ kunne vi brukt [a-z] og [2-6] for Ã¥ spesifisere dette. F.eks. kunne vi skrevet:\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/R_smoke_test/**/R_smoke_test_[a-z][2-6].csv\")\nSom vi ser er fs.glob() et verktÃ¸y som git oss mye den funksjonaliteten vi er vant til fra Linux-terminalen. Men i tillegg kan vi bruke den til Ã¥ hente inn metadataene til de filene/objektene vi fÃ¥r treff pÃ¥, ved Ã¥ bruke argumentet detail=True. Her er et eksempel:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\n    \"gs://ssb-dapla-felles-data-produkt-prod/R_smoke_test/**/R_smoke_test_/[a-z][2-6].csv\",\n    detail=True,\n)\n\nMetadataene gir deg da informasjon om filstÃ¸rrelse, tidspunkt for opprettelse, tidspunkt for siste endring, og hvem som har opprettet og endret filen.\n\n\n\nI noen tilfeller kan det vÃ¦re nyttig Ã¥ sjekke om en fil eksisterer i bÃ¸tten. Det kan vi gjÃ¸re med fs.exists():\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.exists(\"gs://ssb-dapla-felles-data-produkt-prod/altinn3/form_dc551844cd74.xml\")\n\nKoden returnerer enten True eller False avhengig av om filen eksisterer eller ikke.\n\n\n\nHvis du lurer pÃ¥ hvor mange GB data du har i en bÃ¸tte, sÃ¥ kan du bruke fs.du():\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss pÃ¥ bÃ¸ttene\nfs = FileClient.get_gcs_file_system()\n\ntotal_bytes = fs.du(\n    \"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/\",\n)\n\ntotal_size_gb = total_bytes / (1024**3)\nprint(f\"Total size: {total_size_gb:.3f} GB\")\n\n\n\n\nHvert objekt i bÃ¸tta har metadata knyttet til seg som kan hentes inn i Jupyter og benyttes.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss pÃ¥ bÃ¸ttene\nfs = FileClient.get_gcs_file_system()\n\nfile = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/form_dc551844cd74.xml\"\n\nfs.info(file)\n\nDette gir deg blant annet informasjon om filstÃ¸rrelse, tidspunkt for opprettelse, og tidspunkt for siste endring. Hvis du Ã¸nsker dette for flere filer sÃ¥ kan man ogsÃ¥ bruke fs.glob(&lt;pattern&gt;, details=True) som vi sÃ¥ pÃ¥ tidligere.\n\n\n\nfs.ls() brukes for Ã¥ gi en liste av filer i et omrÃ¥de. Det kan brukes for bÃ¥de bÃ¸tter og mapper.\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.ls(\"gs://ssb-dapla-felles-data-produkt-prod/altinn3\")\n\nKoden returnerer en liste.\n\n\n\nfs.open() lar oss Ã¥pne en fil i bÃ¸tta for lesing og skriving med vanlige Python-funksjoner. Funksjonen returnerer et fil-aktig objekt som kan brukes med vanlige som Python-biblioteker som Pandas og NumPy.\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import FileClient\n\n# Kobler oss pÃ¥ bÃ¸ttene\nfs = FileClient.get_gcs_file_system()\n\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-metrics/number-of-teams.csv\"\n\n# Bruker fs.open() til Ã¥ Ã¥pne fil, og deretter leses den med Pandas\nwith fs.open(file_path, \"r\") as file:\n    df = pd.read_csv(file)\ndf\n\nDu kan ogsÃ¥ bruke fs.open() til Ã¥ skrive til en fil i bÃ¸tta. Her er et eksempel pÃ¥ hvordan man skriver en parquet-fil med Pandas og PyArrow:\n\n\nnotebook\n\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom dapla import FileClient\n\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-metrics/test.parquet\"\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\n# Lager eksempeldata\ndata = {\"A\": [1, 2, 3], \"B\": [4, 5, 6]}\ndf = pd.DataFrame(data)\n\n# Skriver parquet-fil\nwith fs.open(file_path, \"wb\") as file:\n    pq.write_table(pa.Table.from_pandas(df), file)\n\nOver brukte vi wb for Ã¥ Ã¥pne den binÃ¦re filen for skriving. Hvis du Ã¸nsker Ã¥ lese fra en binÃ¦r fil sÃ¥ bruker du rb. Skulle du jobbet en ren tekstfil, sÃ¥ hadde man brukt w til Ã¥ skrive og r til Ã¥ lese.\n\n\n\nfs.touch() lar deg opprette en tom fil i bÃ¸tta, eller oppdatere metadataene til objektet for nÃ¥r den sist ble modifisert.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nfs.touch(\"gs://ssb-dapla-felles-data-produkt-prod/dapla-metrics/test.parquet\")\n\n\n\n\nfs.put() lar deg laste opp en fil fra Jupyter-filsystemet3 til bÃ¸tta. Husk at filstien til ditt hjemmeomrÃ¥de pÃ¥ Jupyter er /home/jovyan/. Her er et eksempel pÃ¥ hvordan man kan bruke det pÃ¥ enkelt-filer:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"/home/jovyan/custom.scss\"\ndestination = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/test2.csv\"\n\nfs.put(source, destination)\n\nDu kan ogsÃ¥ kopiere hele mapper mellom jovyan og bÃ¸ttene:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"/home/jovyan/sesongjustering/\"\ndestination = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/sesongjustering/\"\n\nfs.put(source, destination, recursive=True)\n\nEt brukstilfellet for Ã¥ kopiere mellom bÃ¸tter og jovyan er ved sesongjustering med Jdemetra+ og JWSACruncher. Siden Jdemetra+ ikke kan lese/skrive fra bÃ¸tter, sÃ¥ mÃ¥ vi midlertidig kopiere dataene til jovyan med fs.put() fÃ¸r vi kan kjÃ¸re sesongjusteringen. NÃ¥r vi er ferdige med kjÃ¸ringen kopierer vi dataene tilbake til bÃ¸tta med fs.get().\n\n\n\nfs.get() gjÃ¸r det samme som fs.put(), bare motsatt vei. Den kopierer fra en bÃ¸tte til jovyan.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/sesongjustering/\"\ndestination = \"/home/jovyan/sesongjustering/\"\n\nplaceholder = fs.get(source, destination, recursive=True)\n\nfs.get() returnerer potensielt mye output. Hvis du ikke vil ha det, sÃ¥ kan du bare definere det som et objekt. F.eks. placeholder som i eksempelet over.\n\n\n\nfs.mv() lar deg flytte filer og mapper mellom bÃ¸tter, samt Ã¥ gi objekter nye navn.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss pÃ¥ bÃ¸ttene\nfs = FileClient.get_gcs_file_system()\n\nsource = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-metrics/number-of-teams.csv\"\ndestination = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/number-of-teams.csv\"\nfs.mv(source, destination)\n\n\n\n\nfs.copy() lar deg kopiere filer og mapper mellom bÃ¸tter. I eksempelet under kopierer vi rekursivt:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfrom_path = \"gs://ssb-dapla-felles-data-produkt-prod/ledstill/altinn/2022/11/21/\"\nto_path = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/\"\nfs.copy(from_path, to_path, recursive=True)\n\n\n\n\nfs.rm() lar deg slette filer og mapper i bÃ¸tta. Her sletter vi en enkeltfil:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfs.rm(\"gs://ssb-dapla-felles-data-produkt-prod/altinn3/number-of-teams.csv\")\n\nOgsÃ¥ denne funksjonen tar et recursive-argument hvis du Ã¸nsker Ã¥ slette en hel mappe.",
    "crumbs": [
      "Manual",
      "Data",
      "Hva er bÃ¸tter?"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-botter.html#bÃ¸tter-vs-filsystemer",
    "href": "statistikkere/hva-er-botter.html#bÃ¸tter-vs-filsystemer",
    "title": "Hva er bÃ¸tter?",
    "section": "",
    "text": "I et Linux- eller Windows-filsystem, som vi har vÃ¦rt vant til tidligere, sÃ¥ er filer og mapper organisert i en hierarkisk struktur pÃ¥ et operativsystem (OS). I SSB har OS-ene vÃ¦rt installert pÃ¥ fysiske maskiner som vi vedlikeholder selv.\nEn bÃ¸tte i GCS er derimot en kjÃ¸pt tjeneste som lar brukeren lagre alle typer objekter i en container. Man trenger altsÃ¥ ikke Ã¥ tenke pÃ¥ om filene ligger i et hierarki, hvilket operativsystem det kjÃ¸rer pÃ¥, eller hvor mye diskplass som er tilgjengelig.\nAt data er lagret som objekter i en bÃ¸tte har noen praktiske implikasjoner for hvordan vi jobber med data i SSB:\n\nMange er vant til Ã¥ jobbe direkte med filer i en Linux-terminal eller via systemkall fra sprÃ¥k som SAS, Pyton eller R. For Ã¥ gjÃ¸re det samme i Jupyter mot en bÃ¸tte, sÃ¥ kan vi bruke Python-pakken gcsfs. Se eksempler under.\nNÃ¥r vi bruker Python- eller R-pakker for lese eller skrive data fra bÃ¸tter, sÃ¥ er vi avhengig av at pakkene tilbyr integrasjon mot bÃ¸tter. Mange pakker gjÃ¸r det, men ikke alle. For de som ikke gjÃ¸r det kan vi bruke ofte bruke gcsfs til Ã¥ gi oss et python-objekt, som igjen kan brukes av de fleste pakker.\nDet finnes egentlig ikke noen mapper i bÃ¸tter. I motsetning til et vanlig filsystem sÃ¥ er det ikke en hierarkisk mappestruktur i en bÃ¸tte. Det vil si at alt ligger et sted, men de kan ha lange navn med /-tegn slik at det ligner pÃ¥ et klassisk filsystem. Bruker du / i objekt-navnet sÃ¥ vil ogsÃ¥ Google Cloud Console vise det som mapper, men det er bare for Ã¥ gjÃ¸re det enklere Ã¥ forholde seg til. En praktisk konsekvens av dette er f.eks. at man ikke trenger Ã¥ opprette en mappe fÃ¸r man legger et fil/objekt i den. Det er bare en tekststreng som er en del av objekt-navnet.\n\nUnder kommer det en del eksempler pÃ¥ hvordan man kan jobbe med objekter i bÃ¸tter pÃ¥ samme mÃ¥te som filer i et filsystem.",
    "crumbs": [
      "Manual",
      "Data",
      "Hva er bÃ¸tter?"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-botter.html#lokalt-filsystem-pÃ¥-dapla",
    "href": "statistikkere/hva-er-botter.html#lokalt-filsystem-pÃ¥-dapla",
    "title": "Hva er bÃ¸tter?",
    "section": "",
    "text": "PÃ¥ Dapla skal data lagres i bÃ¸tter. Men nÃ¥r du Ã¥pner Jupyterlab sÃ¥ fÃ¥r du ogsÃ¥ et â€œlokaltâ€ eller klassisk filsystem, slik vi definerte det i forrige kapittel. Det er dette filsystemet du ser i filutforskeren i Jupyterlab, slik som vist til venstre i FigurÂ 1. Det er ogsÃ¥ dette filsystemet du ser nÃ¥r du f.eks. bruker ls-kommandoen i en terminal i Jupyterlab.\n\n\n\n\n\n\nFigurÂ 1: Til venstre i bildet ser du filutforskeren i Jupyterlab\n\n\n\nDette filsystemet er ment for Ã¥ lagre kode midlertidig mens du jobber med dem. Det er ikke ment for Ã¥ lagre data. Det er heller ikke ment som et permanent lagringssted for kode. Permanent lagring av kode skal gjÃ¸res pÃ¥ GitHub. Selv om filene du lagrer der fortsetter Ã¥ eksistere for hver gang du logger deg inn i Jupyterlab, sÃ¥ bÃ¸r kode du Ã¸nsker Ã¥ bevare pushes til GitHub fÃ¸r du avslutter en sesjon i Jupyterlab. Se en mer teknisk beskrivelse av hvordan dette fungerer i boksen under.\n\n\n\n\n\n\nHvem er egentlig denne Jovyan?\n\n\n\nNÃ¥r du logger deg inn i Jupyterlab pÃ¥ Dapla, sÃ¥ ser du at brukeren din pÃ¥ det lokale filsystemet heter jovyan. Grunnen er at det er en Linux-container som kjÃ¸rer et Jupyterlab Docker-image, og alle bruker denne containeren. Det er derfor vi kaller det et â€œlokaltâ€ filsystem, fordi det er lokalt i containeren.\nI tillegg er det satt opp persistent volum (PV) og en persistent volume claim (PVC) som er knyttet til denne containeren. Det er dette som gjÃ¸r at filene du lagrer i Jupyterlab blir lagret mellom hver gang du logger deg inn. I tillegg tas det en backup av filsystemene hver dag, slik at vi kan gjenopprette filene dine hvis noe skulle skje med PV-en.\nLagringsplassen pÃ¥ PV-en er begrenset til 10 GB per bruker. Grunnen til at vi ikke tilbyr mer lagringsplass er fordi det kun er kode skal lagres her, og at denne lagringsplassen er relativt dyrt sammenlignet med GCS-bÃ¸tter. Hvis du jobber i virtuelle miljÃ¸er og lagrer mange miljÃ¸er lokalt, kan du oppleve at disken blir full. Les mer her om hvordan man bÃ¸r hÃ¥ndere dette.",
    "crumbs": [
      "Manual",
      "Data",
      "Hva er bÃ¸tter?"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-botter.html#systemkommandoer-mot-bÃ¸ttter",
    "href": "statistikkere/hva-er-botter.html#systemkommandoer-mot-bÃ¸ttter",
    "title": "Hva er bÃ¸tter?",
    "section": "",
    "text": "Tidligere har vi diskutert forskjellene mellom bÃ¸tter og filsystemer. Mange kjenner hvordan man gjÃ¸r systemkommandoer2 i klassiske filsystemer fra en terminal eller fra sprÃ¥k som Python, R og SAS. I dette kapitlet viser vi hvordan dette kan gjÃ¸res mot bÃ¸tter fra Jupyterlab.\ngcsfs er en Python-pakke som lar deg jobbe med objekter i bÃ¸tter pÃ¥ nesten samme mÃ¥te som filer i et filsystem. For Ã¥ kunne gjÃ¸re det mÃ¥ vi fÃ¸rst sette opp en filsystem-instans som lar oss bruke en bÃ¸tte som et filsystem. Pakken dapla-toolbelt lar oss gjÃ¸re det ganske enkelt:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs er nÃ¥ et filsystem-versjon av bÃ¸ttene vi har tilgang til pÃ¥ GCS. Vi kan nÃ¥ bruk fs til Ã¥ gjÃ¸re typiske operasjoner vi har vÃ¦rt vant til Ã¥ gjÃ¸re i en Linux-terminal. Se en fullstendig liste over kommandoer her.\nUnder vises noen flere eksempler pÃ¥ nyttige funksjoner enn det som ble vist i kapitlet tidligere i boken\n\n\n\n\n\n\nWarning\n\n\n\nSelv om ordet mapper blir brukt i eksemplene under, sÃ¥ er det viktig Ã¥ huske at det ikke finnes noen mapper i bÃ¸tter. Det er bare en tekststreng som er en del av objekt-navnet. Men siden vi her jobber med et filsystem-representasjon av bÃ¸tten, sÃ¥ tillater vi oss Ã¥ gjÃ¸re det for Ã¥ gjÃ¸re det enklere Ã¥ lese.\n\n\n\n\nfs.glob() lar oss sÃ¸ke etter filer i bÃ¸tten. Vi kan bruke *, **, ? og [..] som wildcard for Ã¥ finne det vi trenger. Det som returneres er enten en liste eller dictionary avhengig av hva vi velger Ã¥ gjÃ¸re.\nHent en liste over alle filer i en undermappe R_smoke_test i bÃ¸tta gs://ssb-dapla-felles-data-delt-prod:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/R_smoke_test/*\")\n\nNÃ¥r vi legger til * pÃ¥ slutten av filstien sÃ¥ returnerer den alle filer i den eksakte undermappen. Men hvis vi Ã¸nsker Ã¥ Ã¥ fÃ¥ alle filer i alle undermapper, sÃ¥ kan vi bruke ** pÃ¥ denne mÃ¥ten:\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/R_smoke_test/**\").\nVi kan ogsÃ¥ sÃ¸ke mer avansert ved ved Ã¥ bruke ?. ?-tegnet sier at en enkeltkarakter kan vÃ¦re hva som helst. F.eks. kunne vi skrevet\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/R_smoke_test/**/R_smoke_test_??.csv\")\nfor Ã¥ rekursivt liste ut alle filer som starter med R_smoke_test_, etterfulgt av to karakterer av hvilken som helst type, og slutter med .csv.\nHvis vi viste at de to karakterene ikke skulle vÃ¦re av hvilken som helst karakterer, men det var en liten bokstav, etterfulgt av et tall mellom 2-6, sÃ¥ kunne vi brukt [a-z] og [2-6] for Ã¥ spesifisere dette. F.eks. kunne vi skrevet:\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/R_smoke_test/**/R_smoke_test_[a-z][2-6].csv\")\nSom vi ser er fs.glob() et verktÃ¸y som git oss mye den funksjonaliteten vi er vant til fra Linux-terminalen. Men i tillegg kan vi bruke den til Ã¥ hente inn metadataene til de filene/objektene vi fÃ¥r treff pÃ¥, ved Ã¥ bruke argumentet detail=True. Her er et eksempel:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\n    \"gs://ssb-dapla-felles-data-produkt-prod/R_smoke_test/**/R_smoke_test_/[a-z][2-6].csv\",\n    detail=True,\n)\n\nMetadataene gir deg da informasjon om filstÃ¸rrelse, tidspunkt for opprettelse, tidspunkt for siste endring, og hvem som har opprettet og endret filen.\n\n\n\nI noen tilfeller kan det vÃ¦re nyttig Ã¥ sjekke om en fil eksisterer i bÃ¸tten. Det kan vi gjÃ¸re med fs.exists():\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.exists(\"gs://ssb-dapla-felles-data-produkt-prod/altinn3/form_dc551844cd74.xml\")\n\nKoden returnerer enten True eller False avhengig av om filen eksisterer eller ikke.\n\n\n\nHvis du lurer pÃ¥ hvor mange GB data du har i en bÃ¸tte, sÃ¥ kan du bruke fs.du():\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss pÃ¥ bÃ¸ttene\nfs = FileClient.get_gcs_file_system()\n\ntotal_bytes = fs.du(\n    \"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/\",\n)\n\ntotal_size_gb = total_bytes / (1024**3)\nprint(f\"Total size: {total_size_gb:.3f} GB\")\n\n\n\n\nHvert objekt i bÃ¸tta har metadata knyttet til seg som kan hentes inn i Jupyter og benyttes.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss pÃ¥ bÃ¸ttene\nfs = FileClient.get_gcs_file_system()\n\nfile = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/form_dc551844cd74.xml\"\n\nfs.info(file)\n\nDette gir deg blant annet informasjon om filstÃ¸rrelse, tidspunkt for opprettelse, og tidspunkt for siste endring. Hvis du Ã¸nsker dette for flere filer sÃ¥ kan man ogsÃ¥ bruke fs.glob(&lt;pattern&gt;, details=True) som vi sÃ¥ pÃ¥ tidligere.\n\n\n\nfs.ls() brukes for Ã¥ gi en liste av filer i et omrÃ¥de. Det kan brukes for bÃ¥de bÃ¸tter og mapper.\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.ls(\"gs://ssb-dapla-felles-data-produkt-prod/altinn3\")\n\nKoden returnerer en liste.\n\n\n\nfs.open() lar oss Ã¥pne en fil i bÃ¸tta for lesing og skriving med vanlige Python-funksjoner. Funksjonen returnerer et fil-aktig objekt som kan brukes med vanlige som Python-biblioteker som Pandas og NumPy.\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import FileClient\n\n# Kobler oss pÃ¥ bÃ¸ttene\nfs = FileClient.get_gcs_file_system()\n\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-metrics/number-of-teams.csv\"\n\n# Bruker fs.open() til Ã¥ Ã¥pne fil, og deretter leses den med Pandas\nwith fs.open(file_path, \"r\") as file:\n    df = pd.read_csv(file)\ndf\n\nDu kan ogsÃ¥ bruke fs.open() til Ã¥ skrive til en fil i bÃ¸tta. Her er et eksempel pÃ¥ hvordan man skriver en parquet-fil med Pandas og PyArrow:\n\n\nnotebook\n\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom dapla import FileClient\n\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-metrics/test.parquet\"\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\n# Lager eksempeldata\ndata = {\"A\": [1, 2, 3], \"B\": [4, 5, 6]}\ndf = pd.DataFrame(data)\n\n# Skriver parquet-fil\nwith fs.open(file_path, \"wb\") as file:\n    pq.write_table(pa.Table.from_pandas(df), file)\n\nOver brukte vi wb for Ã¥ Ã¥pne den binÃ¦re filen for skriving. Hvis du Ã¸nsker Ã¥ lese fra en binÃ¦r fil sÃ¥ bruker du rb. Skulle du jobbet en ren tekstfil, sÃ¥ hadde man brukt w til Ã¥ skrive og r til Ã¥ lese.\n\n\n\nfs.touch() lar deg opprette en tom fil i bÃ¸tta, eller oppdatere metadataene til objektet for nÃ¥r den sist ble modifisert.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nfs.touch(\"gs://ssb-dapla-felles-data-produkt-prod/dapla-metrics/test.parquet\")\n\n\n\n\nfs.put() lar deg laste opp en fil fra Jupyter-filsystemet3 til bÃ¸tta. Husk at filstien til ditt hjemmeomrÃ¥de pÃ¥ Jupyter er /home/jovyan/. Her er et eksempel pÃ¥ hvordan man kan bruke det pÃ¥ enkelt-filer:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"/home/jovyan/custom.scss\"\ndestination = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/test2.csv\"\n\nfs.put(source, destination)\n\nDu kan ogsÃ¥ kopiere hele mapper mellom jovyan og bÃ¸ttene:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"/home/jovyan/sesongjustering/\"\ndestination = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/sesongjustering/\"\n\nfs.put(source, destination, recursive=True)\n\nEt brukstilfellet for Ã¥ kopiere mellom bÃ¸tter og jovyan er ved sesongjustering med Jdemetra+ og JWSACruncher. Siden Jdemetra+ ikke kan lese/skrive fra bÃ¸tter, sÃ¥ mÃ¥ vi midlertidig kopiere dataene til jovyan med fs.put() fÃ¸r vi kan kjÃ¸re sesongjusteringen. NÃ¥r vi er ferdige med kjÃ¸ringen kopierer vi dataene tilbake til bÃ¸tta med fs.get().\n\n\n\nfs.get() gjÃ¸r det samme som fs.put(), bare motsatt vei. Den kopierer fra en bÃ¸tte til jovyan.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/sesongjustering/\"\ndestination = \"/home/jovyan/sesongjustering/\"\n\nplaceholder = fs.get(source, destination, recursive=True)\n\nfs.get() returnerer potensielt mye output. Hvis du ikke vil ha det, sÃ¥ kan du bare definere det som et objekt. F.eks. placeholder som i eksempelet over.\n\n\n\nfs.mv() lar deg flytte filer og mapper mellom bÃ¸tter, samt Ã¥ gi objekter nye navn.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss pÃ¥ bÃ¸ttene\nfs = FileClient.get_gcs_file_system()\n\nsource = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-metrics/number-of-teams.csv\"\ndestination = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/number-of-teams.csv\"\nfs.mv(source, destination)\n\n\n\n\nfs.copy() lar deg kopiere filer og mapper mellom bÃ¸tter. I eksempelet under kopierer vi rekursivt:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfrom_path = \"gs://ssb-dapla-felles-data-produkt-prod/ledstill/altinn/2022/11/21/\"\nto_path = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/\"\nfs.copy(from_path, to_path, recursive=True)\n\n\n\n\nfs.rm() lar deg slette filer og mapper i bÃ¸tta. Her sletter vi en enkeltfil:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfs.rm(\"gs://ssb-dapla-felles-data-produkt-prod/altinn3/number-of-teams.csv\")\n\nOgsÃ¥ denne funksjonen tar et recursive-argument hvis du Ã¸nsker Ã¥ slette en hel mappe.",
    "crumbs": [
      "Manual",
      "Data",
      "Hva er bÃ¸tter?"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-botter.html#footnotes",
    "href": "statistikkere/hva-er-botter.html#footnotes",
    "title": "Hva er bÃ¸tter?",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nEgentlig har vi jobbet med data-filer pÃ¥ bÃ¥de Linux- og Windows-filsystemer. Men Linux-stammene har vÃ¦rt det anbefalte stedet Ã¥ lagre datafiler.â†©ï¸\nMed systemkommandoer sÃ¥ mener vi bash-kommandoer som ls og mv, eller implementasjoner av disse kommandoene i Python, R eller SAS.â†©ï¸\nJupyter-miljÃ¸et har sitt eget filsystem, ofte kalt jovyan. Det er som et vanlig Linux-filsystem, og vil vÃ¦re det vi omtaler som â€œlokaltâ€ pÃ¥ maskinen din i Jupyter.â†©ï¸",
    "crumbs": [
      "Manual",
      "Data",
      "Hva er bÃ¸tter?"
    ]
  },
  {
    "objectID": "statistikkere/dapla-pseudo.html",
    "href": "statistikkere/dapla-pseudo.html",
    "title": "dapla-toolbelt-pseudo",
    "section": "",
    "text": "dapla-toolbelt-pseudo er en python-pakke som har som sitt hovedformÃ¥l Ã¥ gi Dapla-brukere muligheten til Ã¥ pseudonymisere, de-pseudonymisere og re-pseudonymisere data. Det skal sikre at brukerne av Dapla har verktÃ¸yene de trenger for Ã¥ jobbe med direkte identifiserende opplysninger i henhold til lovverk og SSBs tolkninger av disse.\nSiden tilgang til direkte identifiserende opplysninger er underlagt strenge regler, sÃ¥ krever bruken av dapla-pseudo-toolbelt at man forholder seg til vedtatte standarder som datatilstander og systemer som Kildomaten. I tillegg er det en streng tilgangsstyring til hvor man kan kalle funksjonaliteten fra. Tjenestene er satt opp pÃ¥ en slik mÃ¥te at Dapla-team skal vÃ¦re selvbetjent i bruken av funksjonaliteten, samtidig som regler, prosesser og standarder etterleves pÃ¥ enklest mulig mÃ¥te.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-pseudo"
    ]
  },
  {
    "objectID": "statistikkere/dapla-pseudo.html#forberedelser",
    "href": "statistikkere/dapla-pseudo.html#forberedelser",
    "title": "dapla-toolbelt-pseudo",
    "section": "Forberedelser",
    "text": "Forberedelser\nFÃ¸r man tar i bruk funksjonaliteten er det viktig at man kjenner godt til tilgangstyring i Dapla-team og Kildomaten, og har diskutert med seksjonen hvordan man skal behandle direkte identifiserende opplysninger i de aktuelle dataene.\nFor at et Dapla-team skal kunne bruke dapla-toolbelt-pseudo mÃ¥ Kildomaten vÃ¦re skrudd pÃ¥ for miljÃ¸et1 man Ã¸nsker Ã¥ jobbe fra. Som standard fÃ¥r alle statistikkteam skrudd pÃ¥ Kildomaten i prod-miljÃ¸et og ikke i test-miljÃ¸et. Ã˜nsker du Ã¥ aktivere Kildomaten i test-miljÃ¸et kan dette gjÃ¸res selvbetjent som en feature.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-pseudo"
    ]
  },
  {
    "objectID": "statistikkere/dapla-pseudo.html#tilgangsstyring",
    "href": "statistikkere/dapla-pseudo.html#tilgangsstyring",
    "title": "dapla-toolbelt-pseudo",
    "section": "Tilgangsstyring",
    "text": "Tilgangsstyring\nTilgang til Ã¥ funksjonalitet i dapla-toolbelt-pseudo kan regnes som sensitivt i seg selv, og derfor er det en streng tilgangsstyring for bruk av tjenesten. I prod-miljÃ¸et kan man kun ta i bruk funksjonaliteten ved Ã¥ prosessere dataene i Kildomaten, og det er bare tilgangsgruppen data-admins som har tilgang til Ã¥ godkjenne slike automatiske prosesseringer. I test-miljÃ¸et derimot kan alle pÃ¥ teamet benytte seg av all funksjonalitet, siden det aldri skal forekomme ekte data her.\n\n\n\nTabellÂ 1: Tilgangsstyring til dapla-pseudo-toolbelt\n\n\n\n\n\n(a) Test-miljÃ¸\n\n\n\n\n\n\n\n\n\n\n\n\nAktÃ¸r\nValidator\nPseudonymize()\nDepseudonymize()\nRepseudonymize()\n\n\n\n\nKildomaten\nâœ…\nâœ…\nâœ…\nâœ…\n\n\ndata-admins (interaktivt)\nâœ…\nâœ…\nâœ…\nâœ…\n\n\ndevelopers (interaktivt)\nâœ…\nâœ…\nâœ…\nâœ…\n\n\n\n\n\n\n\n\n\n(b) Prod-miljÃ¸\n\n\n\n\n\n\n\n\n\n\n\n\nAktÃ¸r\nValidator\nPseudonymize()\nDepseudonymize()\nRepseudonymize()\n\n\n\n\nKildomaten\nâœ…\nâœ…\nğŸš«\nğŸš«\n\n\ndata-admins (interaktivt)\nğŸš«\nğŸš«\nğŸš«\nğŸš«\n\n\ndevelopers (interaktivt)\nğŸš«\nğŸš«\nğŸš«\nğŸš«\n\n\n\n\n\n\n\n\n\nI TabellÂ 1 ser vi fra TabellÂ 1 (a) at man i test-miljÃ¸et har full tilgang til funksjonaliteten i dapla-toolbelt-pseudo, bÃ¥de fra Kildomaten og nÃ¥r man jobber interaktivt2 i Jupyterlab. TabellÂ 1 (b) viser at det kun er tilgang til pseudonymize() og validator() fra Kildomaten i prod-miljÃ¸et, og man kan aldri interaktivt kan kalle pÃ¥ funksjoner som potensielt avslÃ¸rer et pseudonym. Av den grunn er det alltid anbefalt Ã¥ teste ut koden sin i test-miljÃ¸et fÃ¸r den produksjonssettes i i prod-miljÃ¸et med Kildomaten.\n\n\n\n\n\n\nUlike pseudonymer i prod og test\n\n\n\nSelv om man har videre rettigheter til Ã¥ bruke funksjonaliteten i dapla-toolbelt-pseudo fra test-miljÃ¸et sammenlignet med prod-miljÃ¸et, sÃ¥ betyr ikke det at samme input i de to miljÃ¸ene vil samme output. NÃ¥r funksjonaliteten kalles fra test-miljÃ¸et sÃ¥ benyttes det automatisk en annen krypteringsnÃ¸kkel enn den som benyttes i prod. PÃ¥ den mÃ¥ten vil et pseudonym produsert fra prod-miljÃ¸et aldri vÃ¦re likt det som produseres fra prod-miljÃ¸et selv om input skulle vÃ¦re den samme.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-pseudo"
    ]
  },
  {
    "objectID": "statistikkere/dapla-pseudo.html#funksjonalitet",
    "href": "statistikkere/dapla-pseudo.html#funksjonalitet",
    "title": "dapla-toolbelt-pseudo",
    "section": "Funksjonalitet",
    "text": "Funksjonalitet\nI denne delen viser vi hvilken funksjonalitet som tilbys gjennom dapla-toolbelt-pseudo. Eksempelkoden under viser hvordan man ville kjÃ¸rt det fra en notebook i test-miljÃ¸et til teamet, og ikke hvordan koden mÃ¥ skrives nÃ¥r det skal kjÃ¸res i Kildomaten3.\n\nInstallering\ndapla-toolbelt-pseudo er ferdig installert i Kildomaten. Men Ã¸nsker du Ã¥ bruke den i test-miljÃ¸et til teamet sÃ¥ kan du installere det i et ssb-project fra PyPI med denne kommandoen:\n\n\nTerminal\n\npoetry add dapla-toolbelt-pseudo\n\n\n\nDataformater\ndapla-toolbelt-pseudo stÃ¸tter innlesing av fÃ¸lgende dataformater:\n\nCSV\nJSON\nPandas dataframes\nPolars dataframes\n\nEksemplene under viser hovedskelig innlesing av dataframes fra minnet4, men du kan lese mer om filbasert prosessering lenger ned i kapitlet.\n\n\nPseudonymisering\nPseudonymisering tilbys via Pseudonymize-metoden i dapla-toolbelt-pseudo. Den fÃ¸lger et builder-pattern der man spesifiserer hva og i hvilken rekkefÃ¸lge operasjonene skal gjÃ¸res. Anta at det finnes en Polars dataframe i minnet som heter df hvor kolonnen fnr skal pseudonymiseres. Da vil koden se slik ut:\n\n\nNotebook\n\nfrom dapla_pseudo import Pseudonymize\n\nresult_df = (\n    Pseudonymize.from_polars(df) \n    .on_fields(\"fnr\")\n    .with_default_encryption()                     \n    .run()                                \n    .to_polars()                                   \n)\n\nI koden over sÃ¥ angir from_polars(df) at kolonnen vi Ã¸nsker Ã¥ pseudonymisere ligger i en Polars dataframe i minnet og heter df. Deretter spesifiserer vi at kolonnen fnr er den som skal behandles med funksjonen on_fields(\"fnr\"). Videre angir with_default_encryption() at fnr skal pseudonymiseres med dapla-toolbelt-speudo sin standard-algoritme5. Til slutt ber vi om at det ovennevnte blir kjÃ¸rt med funksjonen run(), og at dataene skal returneres som en Polars dataframe med funksjonen to_polars().\nSe flere eksempler i dokumentasjonen.\n\n\nDe-pseudonymisering\nDe-pseudonymisering tilbys via Depseudonymize-metoden i dapla-toolbelt-pseudo. Den fÃ¸lger et builder-pattern der man spesifiserer hva og i hvilken rekkefÃ¸lge operasjonene skal gjÃ¸res. Anta at det finnes i en Polars dataframe i minnet som heter df hvor kolonnen fnr skal de-pseudonymiseres. Da vil koden se slik ut:\n\n\nNotebook\n\nresult_df = (\n    Depseudonymize.from_polars(df)            \n    .on_fields(\"fnr\")                          \n    .with_default_encryption()                     \n    .run()                                         \n    .to_polars()                                   \n)\n\nOppbygningen av koden med Depseudonymize() er helt lik som for Pseudonomize(). Les beskrivelsen der for Ã¥ se hva de ulike funksjonskallene gjÃ¸r. Se flere eksempler i dokumentasjonen.\nDe-pseudonymisering er ogsÃ¥ stÃ¸ttet for informasjon som fÃ¸rst er transformert til stabil ID og deretter pseudonymisert med Papis-nÃ¸kkelen. I disse tilfellene kan det ogsÃ¥ vÃ¦re behov for Ã¥ spesifisere hvilken versjon av snr-katalogen man Ã¸nsker Ã¥ benytte for Ã¥ erstatte snr med fÃ¸dselsnummer:\n\n\nNotebook\n\nfrom dapla_pseudo import Depseudonymize\n\nresult_df = (\n    Depseudonymize.from_pandas(df)            \n    .on_fields(\"fnr\")                          \n    .with_stable_id(\n      sid_snapshot_date=\"2023-05-29\")                    \n    .run()                                         \n    .to_pandas()                                   \n)\n\nI eksempelet over spesifiserer vi datoen 2023-05-29 og da benyttes snr-katalogen som ligger nÃ¦rmest i tid til denne datoen. Hvis sid_snapshot_date ikke oppgis benyttes siste tilgjengelige versjon av katalogen.\n\n\n\n\n\n\nDe-pseudonymisering ikke tilgjengelig i prod-miljÃ¸\n\n\n\nForelÃ¸pig er det kun tilgang til Ã¥ pseudonymisere i test-miljÃ¸et med test-data. Ta kontakt med Dapla-teamene dersom det dukker opp behov for Ã¥ kunne de-pseudonymisere i prod-miljÃ¸et.\n\n\n\n\nRe-pseudonymisering\nUnder utvikling.\n\n\nStabil ID\nI statistikkproduksjon og forskning er det viktig Ã¥ kunne fÃ¸lge de samme personene over tid. Derfor har fÃ¸dselsnummer ofte blitt oversatt til en mer stabilt identifikator, ofte kalt SNR eller stabil ID6. Funksjonene with_stable_id() og validator() bruker stabilID-katalogen til Ã¥ henholdsvis bytte ut fÃ¸dselsnummer med stabil ID, og for Ã¥ validere om fÃ¸dselsnummer er gyldige(se under).\nDu kan selv spesifisere hvilken versjon av SNR-katalogen du Ã¸nsker Ã¥ bruke. Det gjÃ¸r du ved Ã¥ oppgi en gyldighetsdato og sÃ¥ finner dapla-toolbelt-pseudo hvilken versjon av katalogen som ligger nÃ¦rmest i tid.\n\n\nPapis-pseudonym\nDapla tilbyr samme pseudonym som Papis-prosjektet7. Denne kan benyttes pÃ¥ 2 mÃ¥ter:\n\nPseudonymisere hvilken som helst informasjon med samme nÃ¸kkel som Papis.\nTransformere fÃ¸dselsnummer til snr-nummer og deretter pseudonymisere med samme nÃ¸kkel som Papis.\n\nPunkt 1 er nyttig for de som har pseudonymisert informasjon pÃ¥ bakken tidligere og vil ha samme pseudonym pÃ¥ Dapla8. Dette kan gjelde hvilken som helst informasjon, ogsÃ¥ direkte pseudonymisering av fÃ¸dselsnummer, uten at det er gÃ¥tt via snr-nummer. Her er et eksempel pÃ¥ hvordan man pseudonymiserer pÃ¥ denne mÃ¥ten:\n\n\nNotebook\n\nresult = (\n    Pseudonymize.from_pandas(df)\n    .on_fields(\"fornavn\")                      \n    .with_papis_compatible_encryption()         \n    .run()                               \n    .to_pandas()                                  \n)\n\nPunkt 2 over er nok den som benyttes mest i SSB siden den sikrer at pseudonymisert fÃ¸dselsnummer kan kobles med data som er pseudonymisert pÃ¥ bakken. Her er et eksempel pÃ¥ hvordan man pseudonymiserer snr med Papis-nÃ¸kkelen:\n\n\nNotebook\n\nresult = (\n    Pseudonymize.from_pandas(df)\n    .on_fields(\"fnr\")                      \n    .with_stable_id()         \n    .run()                               \n    .to_pandas()                                  \n)\n\n\n\n\n\n\n\nHva betyr det Ã¥ tilby samme pseudonym?\n\n\n\nAt Papis og Dapla tilbyr samme pseudonum betyr egentlig at vi bruker samme krypteringsalgoritme, og en felles krypteringsnÃ¸kkel. Krypteringsalgoritmen som benyttes er formatpreserverende (FPE) og biblioteket som brukes er Tink FPE Python. En begrensning med algorimen er at kun karakterer som finnes i et forhÃ¥ndsdefinert karaktersett (tall, store og smÃ¥ bokstaver fra a-z) blir vurdert. Andre karakterer f.Â eks Ã¦Ã¸Ã¥ blir ikke kryptert. Papis-nÃ¸kkelen (som brukes f.Â eks for fnr og snr) benytter en SKIP-strategi for karakterer som faller utenom, som betyr at algoritmen simpelthen â€œhopper overâ€ disse. FPE-algoritmen er ogsÃ¥ avhengig av stÃ¸rrelsen pÃ¥ det predefinterte karaktersettet for Ã¥ avgjÃ¸re minimumslengden pÃ¥ teksten som pseudonymiseres. For Papis-nÃ¸kkelen betyr det at teksten minst mÃ¥ vÃ¦re 4 karakterer lang. Kortere tekster blir ikke kryptert.\n\n\n\n\nValidere fÃ¸dselsnummer\nValidator-metoden kan benyttes til Ã¥ sjekke om fÃ¸dselsnummer finnes i SNR-katalogen (se over), og returnerer de ugyldige fÃ¸dselsnummerne tilbake. Her kan man ogsÃ¥ spesifisere hvilken versjon av SNR-katalogen man Ã¸nsker Ã¥ bruke. Standard, hvis ingenting velges, er at siste tilgjengelige versjon velges. Under er et eksempel pÃ¥ hvordan man validerer fÃ¸dselsnummer for en gitt gyldighetsdato:\n\n\nNotebook\n\nfrom dapla_pseudo import Validator\n\nresult = (\n    Validator.from_polars(df)\n    .on_field(\"fnr\")\n    .validate_map_to_stable_id(\n        sid_snapshot_date=\"2023-08-29\"\n    )\n)\n# Vis hvilken versjon av SNR-katalogen som er benyttet\nresult.metadata\n# Vis fÃ¸dselsnummer som ikke fikk treff i SNR-katalogen som en Polars dataframe\nresult.to_polars()\n\n\n\nFilbasert prosessering\nSelv om mange av eksemplene i dette kapitlet viser hvordan man bruker dapla-toolbelt-pseudo ved Ã¥ gi funksjonene et datasett fra minnet, sÃ¥ stÃ¸tter den ogsÃ¥ Ã¥ prosessere filer som er lagret pÃ¥ disk/bÃ¸tter. Dette kan vÃ¦re en fordel hvis dataene er for store for Kildomaten (&gt;32GB RAM) eller de har en dyp hierarkisk struktur (f.eks. json-filer).\ndapla-toolbelt-pseudo stÃ¸tter fÃ¸lgende filtyper:\n\ncsv\njson\n\nHer er et eksempel pÃ¥ hvordan man pseudonymiserer en fil som ligger lagret pÃ¥ disk:\n\n\nNotebook\n\nfrom dapla_pseudo import Pseudonymize\nfrom dapla import AuthClient\n\n\nfile_path = \"gs://bucket/pseudo-examples/andeby_personer.csv\"\n\noptions = {\n    \"dtypes\": {\"fnr\": pl.Utf8, \"fornavn\": pl.Utf8, \"etternavn\": pl.Utf8, \"kjonn\": pl.Categorical, \"fodselsdato\": pl.Utf8}\n}\n\nresult_df = (\n    Pseudonymize.from_file(file_path)\n    .on_fields(\"fnr\")\n    .with_default_encryption()\n    .run()\n    .to_polars(**options)\n)\n\nSe flere eksempler i dokumentasjonen.\n\nWildcards\nKommer snart.\n\n\n\nDataminimering\nKommer snart.\n\n\nAlgoritmer\nKommer snart.\n\n\nMetadata\nDet genereres to typer av metadata nÃ¥r man pseudonymiserer:\n\nDatadoc\nMetadata\n\nDe to typene av metadata returneres til brukeren i to forskjellige objekter.\n\nDatadoc\n\n\nDatadoc-metadata er pÃ¥ et format som er planlagt integrert i Datadoc9 pÃ¥ et senere tidspunkt. I koden til hÃ¸yre sÃ¥ printes metadataene fra et kall til Pseudonomize ved Ã¥ skrive print(result.datadoc). Da printer man objektet interaktivt i f.eks. Jupyterlab, noe som kun er mulig i test-miljÃ¸et. Skal man kjÃ¸re dette i Kildomaten sÃ¥ er det lettere Ã¥ skrive filen direkte til riktig json-format med to_file-funksjonen. Da fÃ¥r fÃ¥r filen endelsen __DOC pÃ¥ slutten av filnavnet, og man slipper Ã¥ tenke pÃ¥ om filen skrives med riktig formattering, osv..\n\n\n\n\n\nNotebook\n\nresult = (\n    Pseudonymize.from_polars(df)    \n    .on_fields(\"fnr\")           \n    .with_stable_id()\n    .run()                      \n)\n\n1print(result.datadoc)\n2result.to_file(\"gs://bucket/test.parquet\")\n\n\n1\n\nPrinter metadata i en Notebook.\n\n2\n\nSkriver metadata direkte til bÃ¸tte.\n\n\n\n\nNÃ¥r man kjÃ¸rer pseudonymisering fra Kildomaten er det viktig Ã¥ tenke pÃ¥ at felter som er pseudonymisert ikke mÃ¥ endres uten at metadataene ogsÃ¥ endrer. Da kan man risikere at metadatene ikke lenger beskriver riktig informasjon.\nUnder ser man hvilken informasjon som genereres fra pseudonymiseringen.\n\n\nDatadoc\n\n{\n  \"document_version\": \"0.0.1\",\n  \"datadoc\": null,\n  \"pseudonymization\": {\n    \"document_version\": \"0.1.0\",\n    \"pseudo_dataset\": null,\n    \"pseudo_variables\": [\n      {\n        \"short_name\": \"fnr\",   \n        \"data_element_path\": \"fnr\",\n        \"data_element_pattern\": \"**\",\n        \"stable_identifier_type\": \"FREG_SNR\",\n        \"stable_identifier_version\": \"2023-08-31\",\n        \"encryption_algorithm\": \"TINK-FPE\",\n        \"encryption_key_reference\": \"papis-common-key-1\",\n        \"encryption_algorithm_parameters\": [\n          {\n            \"keyId\": \"papis-common-key-1\"\n          },\n          {\n            \"strategy\": \"SKIP\"\n          }\n        ],\n        \"source_variable\": null,\n        \"source_variable_datatype\": null\n      }\n    ]\n  }\n}\n\nAv metadatene kan vi se fra pseudo_variables at det bare var feltet fnr som ble pseudonymisert. Vi ser ogsÃ¥ av stable_identifier_type ser vi at fnr ble oversatt til snr, og at versjonen av SNR-katalogen var fra 2023-08-31. encryption_algorithm angir at det var det var den formatpreserverende algoritmen TINK-FPE som ble benyttet. keyID: \"papis-common-key-1\" angir hvilken nÃ¸kkel-id som ble benyttet. strategy: \"SKIP\" refererer til at den format-preserverende algoritmen skal â€œhoppe overâ€ ugyldige karakterer og la de vÃ¦re som de er.\nDenne informasjonen vil vÃ¦re svÃ¦rt nyttig i SSB hvis man senere skal kunne de-pseudonymisere eller re-pseudonymisere data.\n\n\nMetadata\nDen andre typen av metadata kan hentes ut etter et kall til Pseudonymize med kommandoen result.metadata. Den returnerer en python dictionary. Den inneholder hovedsaklig logginformasjon og metrikker forelÃ¸pig. For de som pseudonymiserer med with_stable_id() kan output se slik ut:\n\n\nMetadata\n\n{\n    'fnr': {\n        'logs': [\n            'No SID-mapping found for fnr 999999*****',\n            'No SID-mapping found for fnr XX',\n            'No SID-mapping found for fnr X8b7k2*'\n        ],\n        'metrics': [\n            {'MAPPED_SID': 10},\n            {'MISSING_SID': 3}\n        ]\n    }\n}\n\nI Kildomaten kan det vÃ¦rt nyttig Ã¥ printe denne informasjonen til loggene. Av eksempelet over ser vi at verdier som er over 6 karakterer lange blir maskert.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-pseudo"
    ]
  },
  {
    "objectID": "statistikkere/dapla-pseudo.html#brukerveiledning",
    "href": "statistikkere/dapla-pseudo.html#brukerveiledning",
    "title": "dapla-toolbelt-pseudo",
    "section": "Brukerveiledning",
    "text": "Brukerveiledning\nPÃ¥ grunn av den strenge tilgangsstyringen til dapla-pseudo-toolbelt og kildedata er det anbefalt Ã¥ utvikle kode for overgangen fra kildedata til inndata i test-miljÃ¸et til teamet. I denne delen viser vi hvordan denne arbeidsflyten kan se ut, fra testing til en automatisert produksjon med ekte data, med et helt konkret eksempel.\n\nInteraktiv utvikling\nFor Ã¥ kunne kjÃ¸re pseudonymiseringen interaktivt i f.eks. en notebook i Jupyter, sÃ¥ mÃ¥ man jobbe i test-miljÃ¸et til teamet.\n\n\nKildomaten i test\nKommer snart.\n\n\nKildomaten i prod\nKommer snart.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-pseudo"
    ]
  },
  {
    "objectID": "statistikkere/dapla-pseudo.html#footnotes",
    "href": "statistikkere/dapla-pseudo.html#footnotes",
    "title": "dapla-toolbelt-pseudo",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nEt Dapla-team har bÃ¥de et test- og et prod-miljÃ¸. Kildomaten mÃ¥ vÃ¦re skrudd pÃ¥ i det miljÃ¸et du Ã¸nkser Ã¥ benytte dapla-toolbelt-pseudo fra.â†©ï¸\nMed interaktiv jobbing menes at man skriver og kode og fÃ¥r tilbake output i samme verktÃ¸y. F.eks. er Jupyterlab et eksempel pÃ¥ et verktÃ¸y som lar deg jobbe interaktivt med data.â†©ï¸\nI Kildomaten mÃ¥ koden blant annet pakke inn i main()-funksjon.â†©ï¸\nPandas og Polars dataframes er eksempler pÃ¥ dataformater som lever i minnet, og mÃ¥ konverteres fÃ¸r de skrives til et lagringsommrÃ¥de. I praksis vil det ofte si at man jobber med dataframes nÃ¥r man jobber i verktÃ¸y som Jupyterlab, mens man skriver til lagringsomrÃ¥de nÃ¥r man er ferdig i Jupyterlab.â†©ï¸\nStandardalgoritmen i dapla-toolbelt-pseudo er den deterministiske krypteringsalgoritmen Deterministic Authenticated Encryption with Associated Data, eller DAEAD-algoritmen.â†©ï¸\nSNR-katalogen eies og tilbys av Team Register pÃ¥ Dapla.â†©ï¸\nPapis var et prosjekt med fokus pÃ¥ bakkesystemene i SSB som skulle bringe SSBs behandling av personopplysninger, som brukes i statistikkproduksjon, i samsvar med GDPR gjennom en enhetlig pseudonymiseringslÃ¸sning.â†©ï¸\nGenerelt sett er det ikke Ã¥ anbefale Ã¥ benytte denne nÃ¸kkelen pÃ¥ annen informasjon enn fÃ¸dselsnummer. Grunnen er at den er svakere enn andre algoritmer, der blant annet tekst som er kortere enn 4 karakter lang ikke blir pseudonymisert.â†©ï¸\nDatadoc er det nye systemet for dokumentasjon av datasett i SSB. Systemet er fortsatt under utvikling.â†©ï¸",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-pseudo"
    ]
  },
  {
    "objectID": "statistikkere/kvakk.html",
    "href": "statistikkere/kvakk.html",
    "title": "Kvalitet i kode og koding (KVAKK)",
    "section": "",
    "text": "KVAKK er en tverrfaglig gruppe som utarbeider regler, anbefalinger og veiledninger for god prakis for jobbing med kode og koding. Gruppa har mandat fra DM og bestÃ¥r av representanter fra alle statistikkavdelinger, forskning, metode og IT.\nGruppa jobber temabasert og publiserer regler, anbefalinger og veiledninger for hvert tema under beste praksis omrÃ¥det pÃ¥ Confluence. Regler skal fÃ¸lges, med mindre man har en dokumentert begrunnelse pÃ¥ hvorfor regelen avvikes. FÃ¸lgende regler er fastsatt i SSB:\n\nAll produksjonskode skal vÃ¦re under versjonskontroll i GitHub\nKildekode i GitHub skal ikke inneholde ukrypterte passord eller hemmeligheter\nGit-klienter skal konfigureres slik at resultat fra kjÃ¸ringer i Jupyter Notebooks ikke lagres pÃ¥ GitHub\nAlle biblioteker skal ha en eier\nBruk SSB-mal for PyPI-biblioteker nÃ¥r du skal lage et python-bibliotek\nGrensesnittet til biblioteket skal vÃ¦re dokumentert\nAlle biblioteker skal ha tilhÃ¸rende tester\nAll produksjonskode skal lagres i et format som stÃ¸tter kodeanalyse\n\nAnbefalinger er god praksis som de fleste bÃ¸r fÃ¸lge. Disse finner du pÃ¥ KVAKKs sider om regler, anbefalinger og veiledninger.\nDu finner mer informasjon om KVAKK, hvem som er med og arbeidet deres pÃ¥ KVAKK-siden.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Kvalitet i kode og koding (KVAKK)"
    ]
  },
  {
    "objectID": "statistikkere/kartdata.html",
    "href": "statistikkere/kartdata.html",
    "title": "Kartdata",
    "section": "",
    "text": "Det er tilrettelagt kartdata i Dapla til analyse, statistikkproduksjon og visualisering. Det er de samme dataene som er i GEODB i vanlig produksjonssone. Kartdataene er lagret pÃ¥ ssb-kart-data-delt-geo-prod. De er lagret som parquetfiler i standardprojeksjonen vi bruker i SSB (UTM sone 33N) hvis ikke annet er angitt. Det er ogsÃ¥ SSBs standard-rutenett i ulike stÃ¸rrelser samt Eurostats rutenett over Norge.\nI tillegg ligger det noe testdata i fellesbÃ¸tta her: ssb-dapla-felles-data-produkt-prod/GIS/testdata\n\n\n\n\nGeopandas er et Python-bibliotek som integrerer Pandas med GIS-analysering og kartlegging. Geopandas er bygd opp rundt GeoDataFrames, som er en pandas DataFrame med geografisk informasjon. Disse kan lastes inn direkte hvis man har geografiske filer, eller man kan konvertere fra DataFrames med koordinat-kolonner (se eksempel under).\nGeopandas kan blant annet brukes til Ã¥ kartlegge dataene, beregne avstander og labe variabler for nÃ¦rmiljÃ¸ ved Ã¥ koble datasett sammen basert pÃ¥ geografisk overlapp.\nFor lesing og skriving av geodata i Dapla, kan man bruke SSB-pakken sgis. Med denne kan man blant annet ogsÃ¥ beregne reisetider langs veg og lage publiseringsverdige kart.\nPakkene kan installeres i Dapla sÃ¥nn her:\n\n\nterminal\n\npoetry add geopandas\npoetry add ssb-sgis\n\nOg sÃ¥ importeres det i Python pÃ¥ vanlig vis.\n\n\nnotebook\n\nimport geopandas as gpd\nimport sgis as sg\n\nEksempel pÃ¥ lesing/skriving av kartdata\nMan kan lese/skrive geodata i Dapla med sgis-funksjonene read_geopandas og write_geopandas. StÃ¸ttede formater er blant annet (geo)parquet, geojson og gml. En rekke kartdata for hele landet til analyse- og visualiseringsformÃ¥l ligger i bÃ¸tta â€œkartâ€. For eksempel kommuneflater til analyse eller statistikk:\n\n\nnotebook\n\nfilsti = \"ssb-kart-data-delt-geo-prod/analyse_data/klargjorte-data/2024/ABAS_kommune_flate_p2024_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\n\nEksempel pÃ¥ lesing av kommuneflater for visualisering. Disse dataene er generalisert slik at de egner seg for Ã¥ lage kart, men blir unÃ¸yaktige til statistikk:\n\n\nnotebook\n\nfilsti = \"ssb-kart-data-delt-geo-prod/visualisering_data/klargjorte-data/2024/parquet/N5000_kommune_flate_p2024_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\n\nTilsvarende for skriving til parquet eller annet geodataformat:\n\n\nnotebook\n\nsg.write_geopandas(kommuner, filsti_ny) \n\nMan kan ogsÃ¥ gjÃ¸re vanlige tabell-filer geografiske hvis man har koordinat-kolonner. For eksempel situasjonsuttak fra Virksomhets- og foretaksregisteret (VoF):\n\n\nnotebook\n\nimport dapla as dp\n\nVOFSTI = \"ssb-vof-data-delt-stedfesting-prod/klargjorte-data/parquet\"\nvof_df = dp.read_pandas(\n    f\"{VOFSTI}/stedfesting-situasjonsuttak_p2023-01_v1.parquet\"\n)\nvof_gdf = gpd.GeoDataFrame(\n    vof_df, \n    geometry=gpd.points_from_xy(\n        vof_df[\"y_koordinat\"],\n        vof_df[\"x_koordinat\"],\n    ),\n    crs=25833,\n)\nvof_gdf\n\n\n\n\nI tillegg til kartdataene, ligger det testdata i fellesbÃ¸tta som man kan leke seg med. Testdataene finnes for Oslo og Kongsvinger, og kan leses sÃ¥nn her:\n\n\nnotebook\n\ntestdatasti = \"ssb-dapla-felles-data-produkt-prod/GIS/testdata\"\nveger = sg.read_geopandas(f\"{testdatasti}/veger_oslo.parquet\")\n\nUnder fÃ¸lger noen eksempler pÃ¥ GIS-prosessering med testdataene.\nEksempel pÃ¥ avstandsberegning\nAvstand i luftlinje finner man enkelt med sjoin_nearest. SÃ¥nn her finner man avstand fra boligbygg (1000 tilfeldige) i Oslo til nÃ¦rmeste butikkbygg.\n\n\nnotebook\n\nboligbygg = sg.read_geopandas(f\"{testdatasti}/noen_boligbygg_oslo.parquet\")\nbutikkbygg = sg.read_geopandas(f\"{testdatasti}/butikkbygg_oslo.parquet\")\n\nboligbygg_med_butikkavstand = boligbygg.sjoin_nearest(butikkbygg, distance_col=\"meter_til_butikk\")\n\nFor Ã¥ finne avstand eller reisetid langs veier, kan man gjÃ¸re nettverksanalyse med sgis. Man mÃ¥ fÃ¸rst klargjÃ¸re vegnettet og bestemme regler for beregningen(e):\n\n\nnotebook\n\nveger_klargjort = (\n    sg.get_connected_components(veger)\n    .loc[lambda x: x[\"connected\"] == 1]\n    .pipe(sg.make_directed_network_norway, dropnegative=True)\n)\n\nregler = sg.NetworkAnalysisRules(directed=True, weight=\"minutes\")\n\nnwa = sg.NetworkAnalysis(network=veger_klargjort, rules=regler)\n\nSÃ¥ kan man beregne reisetider fra boligbygg til butikkbygg:\n\n\nnotebook\n\navstander = nwa.od_cost_matrix(boligbygg, butikkbygg)\n\nKorteste reisetid per bolig kan kobles pÃ¥ som kolonne i boligdataene sÃ¥nn her:\n\n\nnotebook\n\nboligbygg[\"minutter_til_butikk\"] = avstander.groupby(\"origin\")[\"minutes\"].min()\n\nUndersÃ¸k resultatene i interaktivt kart:\n\n\nnotebook\n\nsg.explore(boligbygg, butikkbygg, \"minutter_til_butikk\")\n\nSe mer informasjon og eksempler i dokumentasjonen.\nOg flere nettverksanalyse-eksempler her.\nEksempel pÃ¥ geografisk kobling\nDatasett kan kobles basert pÃ¥ geografisk overlapp med geopandas.sjoin. Denne fungerer akkurat som pandas.merge, bare at det kobles basert pÃ¥ geometrien.\nKodesnutten under returnerer Ã©n kommuneflate for hvert bygg som overlapper, med kolonner fra begge datasettene. Inner-join er default, men med left-join beholder man ogsÃ¥ geometriene som ikke overlapper.\n\n\nnotebook\n\nkommuner = sg.read_geopandas(f\"{testdatasti}/enkle_kommuner.parquet\")\nkommuner_med_boliginfo = kommuner.sjoin(boligbygg, how=\"left\")\n\nMed motsatt rekkefÃ¸lge, fÃ¥r man ett boligbygg per kommune den overlapper med. Siden byggene er punkter, vil hvert bygg havne i maksimalt Ã©n kommune.\n\n\nnotebook\n\nboligbygg_med_kommuneinfo = boligbygg.sjoin(kommuner, how=\"left\")\n\nEksempel pÃ¥ Ã¥ lage kart\nMan kan lage tematiske kart med sgis-klassen ThematicMap. Her er et enkelt eksempel pÃ¥ et kart over arealet i kommuner.\n\n\nnotebook\n\nkommuner[\"Kvadratmeter\"] = kommuner.area\n\nm = sg.ThematicMap(kommuner, column=\"Kvadratmeter\", size=15)\nm.title = \"Areal i kommunene\"\nm.plot()\n\nSe flere kart-eksempler her.\nKonvertere DataFrame til GeoDataFrame\nEn pandas.DataFrame med koordinat-kolonner, kan konverteres til GeoDataFrame sÃ¥nn her:\n\n\nnotebook\n\nimport pandas as pd\ndf = pd.DataFrame({\"x\": [10.8, 10.7, 10.9], \"y\": [59.9, 60, 59.85]})\n\ngdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.x, df.y), crs=4326)\n\nFor Ã¥ beregne avtand i meter og kunne koble med annen geodata i Dapla, mÃ¥ man ha UTM-koordinater (hvis man ikke hadde det fra fÃ¸r):\n\n\nnotebook\n\ngdf = gdf.to_crs(25833)\n\nSe ogsÃ¥ geopandasâ€™ dokumentasjon for mer utfyllende informasjon.\n\n\n\n\nDen viktigste GIS-pakken i R er sf. Pakken er bygd opp rundt sf-objekter, som er data.frames med geografisk informasjon. Man kan gjÃ¸re standard tidyverse-opersjoner pÃ¥ sf-objektene, pluss GIS-operasjoner.\nSindre Mikael Haugen har lagd funksjoner for Ã¥ lese og skrive blant annet geodata i Dapla. For Ã¥ fÃ¥ geodata, setter man parametret â€˜sfâ€™ til TRUE:\n\n\nnotebook\n\nlibrary(fellesr)\n\nveger = read_SSB(\"ssb-dapla-felles-data-produkt-prod/GIS/testdata/veger_oslo.parquet\", sf = TRUE)\n\nHaugen har ogsÃ¥ lagd en pakke for Ã¥ gjÃ¸re nettverksanalyse, som ogsÃ¥ lar deg geokode adresser, altsÃ¥ Ã¥ finne adressenes koordinater.\n\n\nnotebook\n\nlibrary(GISSB)\n\nLite eksempel pÃ¥ kobling basert geografisk sammenfall (enkle kommuner er egentlig til visualisering). Her fÃ¥r man ett bygg per kommune som overlapper (som maksimalt er Ã©n kommune siden dette er bygningspunkter):\n\n\nnotebook\n\nlibrary(sf)\n\nboligbygg = read_SSB(\"ssb-dapla-felles-data-produkt-prod/GIS/testdata/noen_boligbygg_oslo.parquet\", sf = TRUE)\nenkle_kommuner = read_SSB(\"ssb-dapla-felles-data-produkt-prod/GIS/testdata/enkle_kommuner.parquet\", sf = TRUE)\n\nboligbygg_med_kommuneinfo = st_join(boligbygg, enkle_kommuner)\n\nMed motsatt rekkefÃ¸lge, fÃ¥r man Ã©n kommuneflate per bolig som overlapper:\n\n\nnotebook\n\nkommuner_med_boliginfo = st_join(enkle_kommuner, boligbygg)",
    "crumbs": [
      "Manual",
      "Appendix",
      "Kartdata"
    ]
  },
  {
    "objectID": "statistikkere/kartdata.html#python",
    "href": "statistikkere/kartdata.html#python",
    "title": "Kartdata",
    "section": "",
    "text": "Geopandas er et Python-bibliotek som integrerer Pandas med GIS-analysering og kartlegging. Geopandas er bygd opp rundt GeoDataFrames, som er en pandas DataFrame med geografisk informasjon. Disse kan lastes inn direkte hvis man har geografiske filer, eller man kan konvertere fra DataFrames med koordinat-kolonner (se eksempel under).\nGeopandas kan blant annet brukes til Ã¥ kartlegge dataene, beregne avstander og labe variabler for nÃ¦rmiljÃ¸ ved Ã¥ koble datasett sammen basert pÃ¥ geografisk overlapp.\nFor lesing og skriving av geodata i Dapla, kan man bruke SSB-pakken sgis. Med denne kan man blant annet ogsÃ¥ beregne reisetider langs veg og lage publiseringsverdige kart.\nPakkene kan installeres i Dapla sÃ¥nn her:\n\n\nterminal\n\npoetry add geopandas\npoetry add ssb-sgis\n\nOg sÃ¥ importeres det i Python pÃ¥ vanlig vis.\n\n\nnotebook\n\nimport geopandas as gpd\nimport sgis as sg\n\nEksempel pÃ¥ lesing/skriving av kartdata\nMan kan lese/skrive geodata i Dapla med sgis-funksjonene read_geopandas og write_geopandas. StÃ¸ttede formater er blant annet (geo)parquet, geojson og gml. En rekke kartdata for hele landet til analyse- og visualiseringsformÃ¥l ligger i bÃ¸tta â€œkartâ€. For eksempel kommuneflater til analyse eller statistikk:\n\n\nnotebook\n\nfilsti = \"ssb-kart-data-delt-geo-prod/analyse_data/klargjorte-data/2024/ABAS_kommune_flate_p2024_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\n\nEksempel pÃ¥ lesing av kommuneflater for visualisering. Disse dataene er generalisert slik at de egner seg for Ã¥ lage kart, men blir unÃ¸yaktige til statistikk:\n\n\nnotebook\n\nfilsti = \"ssb-kart-data-delt-geo-prod/visualisering_data/klargjorte-data/2024/parquet/N5000_kommune_flate_p2024_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\n\nTilsvarende for skriving til parquet eller annet geodataformat:\n\n\nnotebook\n\nsg.write_geopandas(kommuner, filsti_ny) \n\nMan kan ogsÃ¥ gjÃ¸re vanlige tabell-filer geografiske hvis man har koordinat-kolonner. For eksempel situasjonsuttak fra Virksomhets- og foretaksregisteret (VoF):\n\n\nnotebook\n\nimport dapla as dp\n\nVOFSTI = \"ssb-vof-data-delt-stedfesting-prod/klargjorte-data/parquet\"\nvof_df = dp.read_pandas(\n    f\"{VOFSTI}/stedfesting-situasjonsuttak_p2023-01_v1.parquet\"\n)\nvof_gdf = gpd.GeoDataFrame(\n    vof_df, \n    geometry=gpd.points_from_xy(\n        vof_df[\"y_koordinat\"],\n        vof_df[\"x_koordinat\"],\n    ),\n    crs=25833,\n)\nvof_gdf\n\n\n\n\nI tillegg til kartdataene, ligger det testdata i fellesbÃ¸tta som man kan leke seg med. Testdataene finnes for Oslo og Kongsvinger, og kan leses sÃ¥nn her:\n\n\nnotebook\n\ntestdatasti = \"ssb-dapla-felles-data-produkt-prod/GIS/testdata\"\nveger = sg.read_geopandas(f\"{testdatasti}/veger_oslo.parquet\")\n\nUnder fÃ¸lger noen eksempler pÃ¥ GIS-prosessering med testdataene.\nEksempel pÃ¥ avstandsberegning\nAvstand i luftlinje finner man enkelt med sjoin_nearest. SÃ¥nn her finner man avstand fra boligbygg (1000 tilfeldige) i Oslo til nÃ¦rmeste butikkbygg.\n\n\nnotebook\n\nboligbygg = sg.read_geopandas(f\"{testdatasti}/noen_boligbygg_oslo.parquet\")\nbutikkbygg = sg.read_geopandas(f\"{testdatasti}/butikkbygg_oslo.parquet\")\n\nboligbygg_med_butikkavstand = boligbygg.sjoin_nearest(butikkbygg, distance_col=\"meter_til_butikk\")\n\nFor Ã¥ finne avstand eller reisetid langs veier, kan man gjÃ¸re nettverksanalyse med sgis. Man mÃ¥ fÃ¸rst klargjÃ¸re vegnettet og bestemme regler for beregningen(e):\n\n\nnotebook\n\nveger_klargjort = (\n    sg.get_connected_components(veger)\n    .loc[lambda x: x[\"connected\"] == 1]\n    .pipe(sg.make_directed_network_norway, dropnegative=True)\n)\n\nregler = sg.NetworkAnalysisRules(directed=True, weight=\"minutes\")\n\nnwa = sg.NetworkAnalysis(network=veger_klargjort, rules=regler)\n\nSÃ¥ kan man beregne reisetider fra boligbygg til butikkbygg:\n\n\nnotebook\n\navstander = nwa.od_cost_matrix(boligbygg, butikkbygg)\n\nKorteste reisetid per bolig kan kobles pÃ¥ som kolonne i boligdataene sÃ¥nn her:\n\n\nnotebook\n\nboligbygg[\"minutter_til_butikk\"] = avstander.groupby(\"origin\")[\"minutes\"].min()\n\nUndersÃ¸k resultatene i interaktivt kart:\n\n\nnotebook\n\nsg.explore(boligbygg, butikkbygg, \"minutter_til_butikk\")\n\nSe mer informasjon og eksempler i dokumentasjonen.\nOg flere nettverksanalyse-eksempler her.\nEksempel pÃ¥ geografisk kobling\nDatasett kan kobles basert pÃ¥ geografisk overlapp med geopandas.sjoin. Denne fungerer akkurat som pandas.merge, bare at det kobles basert pÃ¥ geometrien.\nKodesnutten under returnerer Ã©n kommuneflate for hvert bygg som overlapper, med kolonner fra begge datasettene. Inner-join er default, men med left-join beholder man ogsÃ¥ geometriene som ikke overlapper.\n\n\nnotebook\n\nkommuner = sg.read_geopandas(f\"{testdatasti}/enkle_kommuner.parquet\")\nkommuner_med_boliginfo = kommuner.sjoin(boligbygg, how=\"left\")\n\nMed motsatt rekkefÃ¸lge, fÃ¥r man ett boligbygg per kommune den overlapper med. Siden byggene er punkter, vil hvert bygg havne i maksimalt Ã©n kommune.\n\n\nnotebook\n\nboligbygg_med_kommuneinfo = boligbygg.sjoin(kommuner, how=\"left\")\n\nEksempel pÃ¥ Ã¥ lage kart\nMan kan lage tematiske kart med sgis-klassen ThematicMap. Her er et enkelt eksempel pÃ¥ et kart over arealet i kommuner.\n\n\nnotebook\n\nkommuner[\"Kvadratmeter\"] = kommuner.area\n\nm = sg.ThematicMap(kommuner, column=\"Kvadratmeter\", size=15)\nm.title = \"Areal i kommunene\"\nm.plot()\n\nSe flere kart-eksempler her.\nKonvertere DataFrame til GeoDataFrame\nEn pandas.DataFrame med koordinat-kolonner, kan konverteres til GeoDataFrame sÃ¥nn her:\n\n\nnotebook\n\nimport pandas as pd\ndf = pd.DataFrame({\"x\": [10.8, 10.7, 10.9], \"y\": [59.9, 60, 59.85]})\n\ngdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.x, df.y), crs=4326)\n\nFor Ã¥ beregne avtand i meter og kunne koble med annen geodata i Dapla, mÃ¥ man ha UTM-koordinater (hvis man ikke hadde det fra fÃ¸r):\n\n\nnotebook\n\ngdf = gdf.to_crs(25833)\n\nSe ogsÃ¥ geopandasâ€™ dokumentasjon for mer utfyllende informasjon.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Kartdata"
    ]
  },
  {
    "objectID": "statistikkere/kartdata.html#r",
    "href": "statistikkere/kartdata.html#r",
    "title": "Kartdata",
    "section": "",
    "text": "Den viktigste GIS-pakken i R er sf. Pakken er bygd opp rundt sf-objekter, som er data.frames med geografisk informasjon. Man kan gjÃ¸re standard tidyverse-opersjoner pÃ¥ sf-objektene, pluss GIS-operasjoner.\nSindre Mikael Haugen har lagd funksjoner for Ã¥ lese og skrive blant annet geodata i Dapla. For Ã¥ fÃ¥ geodata, setter man parametret â€˜sfâ€™ til TRUE:\n\n\nnotebook\n\nlibrary(fellesr)\n\nveger = read_SSB(\"ssb-dapla-felles-data-produkt-prod/GIS/testdata/veger_oslo.parquet\", sf = TRUE)\n\nHaugen har ogsÃ¥ lagd en pakke for Ã¥ gjÃ¸re nettverksanalyse, som ogsÃ¥ lar deg geokode adresser, altsÃ¥ Ã¥ finne adressenes koordinater.\n\n\nnotebook\n\nlibrary(GISSB)\n\nLite eksempel pÃ¥ kobling basert geografisk sammenfall (enkle kommuner er egentlig til visualisering). Her fÃ¥r man ett bygg per kommune som overlapper (som maksimalt er Ã©n kommune siden dette er bygningspunkter):\n\n\nnotebook\n\nlibrary(sf)\n\nboligbygg = read_SSB(\"ssb-dapla-felles-data-produkt-prod/GIS/testdata/noen_boligbygg_oslo.parquet\", sf = TRUE)\nenkle_kommuner = read_SSB(\"ssb-dapla-felles-data-produkt-prod/GIS/testdata/enkle_kommuner.parquet\", sf = TRUE)\n\nboligbygg_med_kommuneinfo = st_join(boligbygg, enkle_kommuner)\n\nMed motsatt rekkefÃ¸lge, fÃ¥r man Ã©n kommuneflate per bolig som overlapper:\n\n\nnotebook\n\nkommuner_med_boliginfo = st_join(enkle_kommuner, boligbygg)",
    "crumbs": [
      "Manual",
      "Appendix",
      "Kartdata"
    ]
  },
  {
    "objectID": "om-dapla.html",
    "href": "om-dapla.html",
    "title": "Om Dapla",
    "section": "",
    "text": "Om Dapla\nDapla stÃ¥r for dataplattform, og er en skybasert lÃ¸sning for statistikkproduksjon og forskning.\n\n\nHvorfor Dapla?\nSom dataplattform skal Dapla stimulerere til Ã¸kt kvalitet pÃ¥ statistikk og forskning, samtidig som den gjÃ¸r organisasjonen mer tilpasningsdyktig i mÃ¸te med fremtiden.\n\n\nDen nye skybaserte dataplattformen (Dapla) skal bli viktig for Ã¥ effektivisere arbeids-og produksjonsprosesser, den skal sikre effektiv lagring og gjenfinning av data og metadata, og stÃ¸tte opp under deling av data pÃ¥ tvers av statistikkomrÃ¥der.\n\nKilde: Langtidsplan for SSB (2022-2024)\n\n\n\nMÃ¥let med Dapla er Ã¥ tilby tjenester og verktÃ¸y som lar statistikkprodusenter og forskere produsere resultater pÃ¥ en sikker og effektiv mÃ¥te."
  },
  {
    "objectID": "news/posts/2025-04-07-altinn-to-isee/index.html",
    "href": "news/posts/2025-04-07-altinn-to-isee/index.html",
    "title": "Mer dokumentasjon om Altinn 3 i Dapla-manualen",
    "section": "",
    "text": "Team SU-V har skrevet flere nye kapitler med brukerdokumentasjon for brukere som jobber med Altinn 3 pÃ¥ Dapla. Kapitlene er samlet i en egen del av manualen, i tillegg sÃ¥ Python-pakken dapla-suv-tools dokumentert under Datatjenester.\nMagnus Theodor Engh har ogsÃ¥ skrevet et blogginnlegg som gir en detaljert beskrivelse av hvordan man kan integrere et datafangst med Altinn 3 og ISEE."
  },
  {
    "objectID": "news/posts/2025-04-25-update-dapla-ctrl/index.html",
    "href": "news/posts/2025-04-25-update-dapla-ctrl/index.html",
    "title": "Dapla Ctrl viser nÃ¥ type delt-bÃ¸tte",
    "section": "",
    "text": "Ifm. innfÃ¸ring av en ny type delt-bÃ¸tte pÃ¥ Dapla, sÃ¥ har Dapla Ctrl blitt oppdatert slik at den ogsÃ¥ viser hvilken type en delt-bÃ¸tte det er. Dette viser bÃ¥de pÃ¥ Oversiktssiden for teamets bÃ¸tter slik som vist i FigurÂ 1, og pÃ¥ siden for en enkelt delt-bÃ¸tte som vist i FigurÂ 2.\n\n\n\n\n\n\nFigurÂ 1: Oversiktssiden som viser alle delt-bÃ¸ttene til et team\n\n\n\n\n\n\n\n\n\nFigurÂ 2: Siden som viser hvem som har tilgang til en gitt delt-bÃ¸tte"
  },
  {
    "objectID": "news/posts/2024-12-11-nais-platform/index.html",
    "href": "news/posts/2024-12-11-nais-platform/index.html",
    "title": "Tar i bruk Navs applikasjonsplattform",
    "section": "",
    "text": "I 2025 skal SSB for fullt ta i bruk Navs applikasjonsplattform, NAIS.\n\nMed Nais er mange gjennomtenkte valg allerede tatt. Det blir lettere for applikasjonsutviklere i SSB Ã¥ gÃ¥ fra idÃ© til implementering, samtidig som vi slipper utvikling og vedlikehold som ellers mÃ¥tte gjÃ¸re selv.\n\nLes mer i denne SSB-interne saken."
  },
  {
    "objectID": "news/posts/2024-12-03-refresh-buckets/index.html",
    "href": "news/posts/2024-12-03-refresh-buckets/index.html",
    "title": "refresh-buckets kommando i Dapla Lab",
    "section": "",
    "text": "NÃ¥ kan man kjÃ¸re kommandoen refresh-buckets fra terminalen i en tjeneste pÃ¥ Dapla Lab for Ã¥ oppdatere visningen av en bÃ¸ttene i filsystemet. Dette kan vÃ¦re nyttig hvis det blir opprettet mapper med dapla-toolbelt eller Kildomaten, og ikke via buckets-mappen i filsystemet. Les mer i kapitlet om Dapla Lab."
  },
  {
    "objectID": "news/posts/2024-11-08-env-vars/index.html",
    "href": "news/posts/2024-11-08-env-vars/index.html",
    "title": "Faste miljÃ¸variabler i Dapla Lab",
    "section": "",
    "text": "Vi har oppdatert listen over miljÃ¸variabler som brukere av programmeringsmiljÃ¸ene (Jupyter, VS Code og RStudio) i Dapla Lab kan forvente Ã¥ finne. FormÃ¥let med disse er hovedsakelig Ã¥ tilby de som lager Python- eller R-biblioteker en enkel Ã¥ sjekke hvilket miljÃ¸ koden kjÃ¸res i. Vi har ogsÃ¥ noen miljÃ¸variabler som gjÃ¸r at de som lager pakker slipper Ã¥ hardkode url-er i koden sin."
  },
  {
    "objectID": "news/posts/2025-03-24-dapla-lab-tmux/index.html",
    "href": "news/posts/2025-03-24-dapla-lab-tmux/index.html",
    "title": "StÃ¸tte for langtkjÃ¸rende jobber pÃ¥ Dapla Lab",
    "section": "",
    "text": "Brukere som har behov for Ã¥ kjÃ¸re jobber pÃ¥ Dapla Lab som tar mer enn 12 timer, har opplevd at kjÃ¸ringene avbrytes etter noen timer uten noen Ã¥penbar grunn. Feilen har ogsÃ¥ oppstÃ¥tt nÃ¥r brukeren har lukket nettleser-fanen eller maskinen til brukeren har gÃ¥tt i dvale, selv om dette ikke i teorien skulle pÃ¥virke kjÃ¸ringen. Ã…rsaken virker Ã¥ vÃ¦re IDE-ene (Jupyter, VS Code og RStudio) pauser jobbene nÃ¥r de blir pauset.\nFor Ã¥ unngÃ¥ denne oppfÃ¸rselen er terminal-verktÃ¸yet tmux installert i Jupyter, VS Code og RStudio. tmux er terminal multiplexer som lar brukeren kjÃ¸re og organisere mange terminal-sesjoner i et vindu. En av fordelene med tmux i denne sammenhengen er at den lar brukeren kjÃ¸re prosesser selv om terminalen lukkes. Derfor anbefaler vi at brukere som skal kjÃ¸re kode over lang tid (&gt;12h) kjÃ¸rer koden i en tmux-session.\nDu kan kjÃ¸re ditt Python-script ved Ã¥ kjÃ¸re fÃ¸lgende:\n\n\nTerminal\n\ntmux new -s minsesjon\n\nI koden over startes en session med navn minsesjon. For Ã¥ aktivere et shell for ditt ssb-project, slik at din Python-installasjon kan kalles pÃ¥ med python-kommandoen i en terminal, sÃ¥ kan du kjÃ¸re kommandoen:\n\n\nTerminal\n\npoetry shell\n\nNÃ¥ kan du kjÃ¸re scriptet ditt pÃ¥ fÃ¸lgende mÃ¥te:\n\n\nTerminal\n\npython mittskript.py\n\nDeretter kan lukke PC-en, eller nettleser-fanen, og vÃ¦re trygg pÃ¥ at skriptet ditt vil kjÃ¸re til det er ferdig. Husk at alle tjenester pauses kl 22 hver kveld, sÃ¥ hvis du ikke Ã¸nsker det mÃ¥ du ogsÃ¥ fÃ¸lge denne oppskriften.\ntmux operer med konsepter som session, window og panes. Over startet vi en session med navnet minsesjon. Dette er et komplett arbeidsmiljÃ¸ hvor man kan ha flere windows og splitte de opp i panes.\nEn av styrkene til tmux er at man kan kjÃ¸re prosesser i bakgrunnen. Derfor er det ogsÃ¥ viktig Ã¥ kjenne til at en session kan vÃ¦re attached, at brukeren kan se sesjonen i terminalen, og detached, som betyr at den kan kjÃ¸re i bakgrunnen mens brukeren jobber med noe annet. Du kan liste ut dine sesjoner ved Ã¥ skrive fÃ¸lgende i en terminal:\n\n\nTerminal\n\ntmux ls\n\nI eksempelet over sÃ¥ kan vi koble til (attach) til vÃ¥r sesjon ved Ã¥ skrive:\n\n\nTerminal\n\ntmux attach -t minsesjon\n\nDu vet at din sesjon er attached ved at du ser en grÃ¸nn statusbar nederst med navnet til sesjonen, slik som vist FigurÂ 1.\n\n\n\n\n\n\nFigurÂ 1: Statusbar for en tmux-sesjon\n\n\n\nDu kan koble fra en sesjon (detach) ved Ã¥ skrive CTRL B og deretter d, eller skrive fÃ¸lgende i terminalen:\n\n\nTerminal\n\ntmux detach -s minsesjon\n\nProsessen vil deretter kjÃ¸re i sesjonen, samtidig som du kan jobbe med andre ting i tjenesten."
  },
  {
    "objectID": "news/posts/2025-01-16-manualen-omstrukturert/index.html",
    "href": "news/posts/2025-01-16-manualen-omstrukturert/index.html",
    "title": "Omstrukturering av manualen!",
    "section": "",
    "text": "Finner du ikke favorittartiklene dine? Det er fordi vi har omstrukturert manualen. Vi har delt opp det som tidligere var jobbe-med-kode inn i 3 forksjellige artikler: SSB-project, PakkehÃ¥ndtering i Python og PakkehÃ¥ndtering i R."
  },
  {
    "objectID": "news/posts/2025-04-02-ssb-project-dapla-lab/index.html",
    "href": "news/posts/2025-04-02-ssb-project-dapla-lab/index.html",
    "title": "Enklere med ssb-project i Dapla Lab",
    "section": "",
    "text": "Mange brukere har etterspurt muligheten for Ã¥ kunne angi et repo og at ssb-project build skal kjÃ¸res ved oppstart av nye Dapla Lab tjenester. Vi har nÃ¥ lagt til stÃ¸tte for dette i tjenestekonfigurasjonen. Brukere kan be om at ssb-project build kjÃ¸res ved oppstart, og brukeren vil da komme inn i en ny tjeneste med ferdiginstallerte biblioteker og opprettet kernel. Oppstartstiden for tjenesten blir litt lengre for de som benytter seg av dette.\nEn stor gevinst med denne endringen er at det vil gjÃ¸r det lettere Ã¥ slette tjenester jevnlig istedenfor Ã¥ pause de, siden man kan lagre konfigurasjonen og komme til â€œdekka bordâ€ neste gang man starter en ny tjeneste. Se eksempel pÃ¥ dette i videoen under.\nDokumentasjon finner du her."
  },
  {
    "objectID": "news/posts/2025-04-28-function-use/index.html",
    "href": "news/posts/2025-04-28-function-use/index.html",
    "title": "Blogginnlegg om bruk av funksjoner",
    "section": "",
    "text": "Arne SÃ¸rli har skrevet et blogginnlegg som oppfordrer til mer bruk av funksjoner i statistikkproduksjonskode. Innlegget sier litt om hvorfor, hvordan man kan gjÃ¸re det og kommer med noen eksempler. Det har ogsÃ¥ et skript du kan bruke til Ã¥ sjekke hvor mye bruk av funksjoner det er i ditt repo.\nLes innlegget her."
  },
  {
    "objectID": "news/posts/2025-04-01-validate-standards/index.html",
    "href": "news/posts/2025-04-01-validate-standards/index.html",
    "title": "Nye funksjoner for Ã¥ validere navnestandard",
    "section": "",
    "text": "Team Metadata har nÃ¥ lansert funksjonalitet i dapla-toolbelt-metadata for Ã¥ sjekke om filene i bÃ¸ttene fÃ¸lger navnestandarden. Metoden check_naming_standard kan validere alt fra en enkel filsti til en hel bÃ¸tte og returnerer lister med eventuelle brudd pÃ¥ navnestandarden. I tillegg er det mulig Ã¥ hente ut en enkel oppsummering med generate_validation_report. Det er laget en notebook med eksempler https://github.com/statisticsnorway/dapla-toolbelt-metadata/blob/main/demo/standards/navnestandard.ipynb."
  },
  {
    "objectID": "news/posts/2025-04-24-update-shared-buckets/index.html",
    "href": "news/posts/2025-04-24-update-shared-buckets/index.html",
    "title": "Endring i opprettelse og administrasjon av delt-bÃ¸tter",
    "section": "",
    "text": "Vi ruller nÃ¥ ut en endring i hvordan brukere kan opprette og gi tilganger til delt-bÃ¸tter. Den nye fremgangsmÃ¥ten er dokumentert i Dapla-manualen. Endringen vil skje gradvis de neste dagene, og vil fullfÃ¸res ila et par dager.\nÃ…rsaken til endringen er at Dapla nÃ¥ skal stÃ¸tte en ny type delt-bÃ¸tte kalt Delomat. Denne bÃ¸ttetypen er spesielt konfigurert for en ny tjeneste for datadeling som utvikles med navnet Delomaten.\nEn direkte konsekvens av endringen er at brukere ikke lenger trenger Ã¥ forholde seg iam.yaml for Ã¥ gi tilganger slik som tidligere. BÃ¥de opprettelse av bÃ¸tter og tilgangstyring skjer nÃ¥ i samme fil shared-buckets.yaml. FigurÂ 1 viser hvordan den nye strukturen ser ut.\nUtrullingen av den nye funksjonaliteten medfÃ¸rer ikke at brukere trenger Ã¥ gjÃ¸re noe, siden alle tidligere shared-buckets.yaml og iam.yaml blir migrert inn til det nye formatet. Hvis noen skulle oppleve endringer i tilganger til delt-bÃ¸tter, sÃ¥ kan man kontakte Kundeservice for Ã¥ fÃ¥ hjelp.\n\n\n\n\n\n\nFigurÂ 1: Bilde av det nye oppsettet for delt-bÃ¸tter."
  },
  {
    "objectID": "news/posts/2025-03-03-dapla-lab-nosuspend/index.html",
    "href": "news/posts/2025-03-03-dapla-lab-nosuspend/index.html",
    "title": "Skru av automatisk pausing for en tjeneste",
    "section": "",
    "text": "Alle tjenester pÃ¥ Dapla Lab pauses hver kveld kl. 22:00, slik som forklart i dokumentasjonen. NÃ¥ er det mulig Ã¥ skru av pausing for enkelttjenester ved Ã¥ skrive [nosuspend] pÃ¥ slutten av visningsnavnet til tjenesten, slik som vist i videoen under:\n\nDenne funksjonaliteten kan skrus av og pÃ¥ etter at tjenesten er startet."
  },
  {
    "objectID": "news/posts/2025-05-01-dapla-lab-groups/index.html",
    "href": "news/posts/2025-05-01-dapla-lab-groups/index.html",
    "title": "Ny informasjon under Mine tjenester i Dapla Lab",
    "section": "",
    "text": "I dag er det lagt til visning av hvilken tilgangsgruppe som er valgt for Ã¥pnede tjenester i Dapla Lab. Det vil gjÃ¸re det lettere Ã¥ vite hvilke data man tilgang til i ulike tjenester. Du finner under informasjonen under Group i tjenesteikonet under Mine tjenester. Informasjonen vises bÃ¥de nÃ¥r tjenesten er aktiv og nÃ¥r den er pauset."
  },
  {
    "objectID": "news/posts/2025-04-10-datadoc-update/index.html",
    "href": "news/posts/2025-04-10-datadoc-update/index.html",
    "title": "Oppdatert dokumentasjon for Datadoc",
    "section": "",
    "text": "Dokumentasjonen for Datadoc-delen av dapla-toolbelt-metadata er skrevet om og inkluderer nÃ¥ flere eksempler pÃ¥ hvordan man jobbe programmatisk med metadata.\nLes kapitlet her."
  },
  {
    "objectID": "news/posts/2024-12-16-blogpost-pandera/index.html",
    "href": "news/posts/2024-12-16-blogpost-pandera/index.html",
    "title": "Nytt blogginnlegg om Pandera",
    "section": "",
    "text": "Manualens blogg har blitt oppdatert med en post om hvordan validere data i kode ved hjelp av Python pakken Pandera. Les mer her."
  },
  {
    "objectID": "news/posts/2025-02-14-cost-in-dapla-lab/index.html",
    "href": "news/posts/2025-02-14-cost-in-dapla-lab/index.html",
    "title": "Estimerte kostnader i Dapla Lab",
    "section": "",
    "text": "Du kan nÃ¥ se et estimat for hvor mye din tjeneste koster under Ressurser-fanen i tjenestekonfigurasjonen pÃ¥ Dapla Lab. Dette er ikke et nÃ¸yaktig estimat, men kan gi brukeren en indikasjon pÃ¥ hvor mye ressursene vil koste ila en arbeidsdag."
  },
  {
    "objectID": "news/posts/2025-03-20-stabilitet-dapla-lab/index.html",
    "href": "news/posts/2025-03-20-stabilitet-dapla-lab/index.html",
    "title": "Endringer for Ã¸kt stabilitet pÃ¥ Dapla Lab",
    "section": "",
    "text": "Etter overgangen til Dapla Lab februar 2025 har vi gjort erfaringer som tilsa at vi burde endre hvordan tjenestene er satt opp. Dette burde resultere i at brukere opplever fÃ¦rre avbrudd nÃ¥r de jobber i en tjeneste som Jupyter, og at det blir fÃ¦rre problemer knyttet til konkurerende minnebruk. Ofte er disse vurderingene en avveining mellom opplevd stabilitet og fleksibilitet vs kostnader for SSB. Det optimalet oppsettet er derfor noe vi mÃ¥ kontinuerlig justere etter hvert som vi fÃ¥r erfaring."
  },
  {
    "objectID": "news/posts/2025-03-20-stabilitet-dapla-lab/index.html#fÃ¦rre-avbrudd",
    "href": "news/posts/2025-03-20-stabilitet-dapla-lab/index.html#fÃ¦rre-avbrudd",
    "title": "Endringer for Ã¸kt stabilitet pÃ¥ Dapla Lab",
    "section": "FÃ¦rre avbrudd",
    "text": "FÃ¦rre avbrudd\nBrukere som har jobbet i Jupyter, VS Code eller RStudio har rapportert at de plutselig kan oppleve avbrudd og at tjenesten ikke kan brukes pÃ¥ noen minutter. En refresh av nettsiden etter noen minutter har fikset problemet, men det oppleves ustabilt for brukeren. Vi har nÃ¥ gjort endringer i den underliggende plattformen slik at dette ikke skal skje. Ã…rsaken til avbruddene har vÃ¦rt at brukere har blitt omplassert pÃ¥ nye maskiner for Ã¥ spare ressurser. Vi har nÃ¥ Ã¸kt toleransen for at brukere skal bli omplassert pÃ¥ ny maskin og dermed bÃ¸r problemet forsvinne. For de som er interessert i mer detaljer sÃ¥ kan de lese neste avsnitt.\nNÃ¥r en tjeneste startes i Dapla Lab bestemmer Kubernetes1 hvilken underliggende maskin (ogsÃ¥ kalt node) tjenesten skal starte pÃ¥. Hvis det er for lite kapasitet vil Kubernetes starte en ny maskin. Denne skaleringen er grunnen til at oppstart av en tjeneste noen ganger kan ta ekstra tid. Skulle det vise seg at vi har flere maskiner enn nÃ¸dvendig vil de overflÃ¸dige maskinene bli skrudd av. Ved Ã¥ kun kjÃ¸re det minste nÃ¸dvendige antallet med maskiner fÃ¥r vi en mer kostnadseffektiv drift. En konsekvens av dette har vÃ¦rt at tjenester plutselig kan oppleve et midlertidig avbrudd mens tjenesten blir flyttet fra en maskin til en annen. Etter tilbakemelding fra brukerne har vi endret denne praksisen, og lar nÃ¥ tjenestene kjÃ¸re videre pÃ¥ samme maskin, selv om det er ledig kapasitet pÃ¥ andre maskiner."
  },
  {
    "objectID": "news/posts/2025-03-20-stabilitet-dapla-lab/index.html#mindre-konkurrerende-minnebruk",
    "href": "news/posts/2025-03-20-stabilitet-dapla-lab/index.html#mindre-konkurrerende-minnebruk",
    "title": "Endringer for Ã¸kt stabilitet pÃ¥ Dapla Lab",
    "section": "Mindre konkurrerende minnebruk",
    "text": "Mindre konkurrerende minnebruk\n\nEnkelte brukere har opplevd at tjenesten i Dapla Lab krÃ¦sjer pga. minnebruk selv om minnebruken er mindre enn det som ble forespurt av brukeren. Dette skyldes at det er forskjell pÃ¥ forespurte og tilgjengelige ressurser i Kubernetes. Vi har nÃ¥ endret oppsettet i Dapla Lab slik at brukerne alltid fÃ¥r det som forespÃ¸rres, og derfor vil mangel pÃ¥ ressurser ikke forekomme lenger. Resultatet for brukeren er en mer stabil tjeneste og bedre brukeropplevelse. De som er interessert i flere detaljer rundt dette, kan lese avsnittet under.\nNÃ¥r man som bruker i Dapla Lab konfigurerer en tjeneste med CPU og Minne/RAM, sÃ¥ blir det satt fÃ¸ringer som mÃ¥ overholdes av Kubernetes. Den ene fÃ¸ringen er en forespÃ¸rsel - mengden CPU/minne man er garantert Ã¥ fÃ¥ - og den andre er grensen/taket. Hvis man gÃ¥r over taket vil Kubernetes â€œdrepeâ€ tjenesten og starte den pÃ¥ nytt.\nI â€œtradisjonelleâ€ applikasjoner (tenk f.eks. Klass sitt API) gir forespÃ¸rsler og grenser mening, grunnet naturlig variasjon i belastning og trafikk, og at applikasjonene er designet for Ã¥ kunne feile, starte pÃ¥ nytt ofte og kjÃ¸re flere instanser i parallel. Hvis den om og om igjen gÃ¥r over grensen sin tyder dette pÃ¥ en feil i applikasjonen (f.eks. minnelekkasje).\nKubernetes benytter seg av det forespurte minnet nÃ¥r den velger hvilken maskin tjenesten eller applikasjonen skal kjÃ¸re pÃ¥. Det vil si at det er mulig Ã¥ totalt ha en hÃ¸yere minnegrense enn hva som er tilgjengelig pÃ¥ maskinen. Et eksempel pÃ¥ dette er hvis tjeneste A har minneforespÃ¸rsel/-grense pÃ¥ 100GB/150GB, og tjeneste B har tilsvarende, men maskinen bare har 256GB med minne tilgjengelig. Da er vi i et scenario der maskinen ikke har nok minne hvis bÃ¥de tjeneste A og tjeneste B bruker opp til minnegrensen sin. I dette tilfellet vil en tjeneste pÃ¥ maskinen bli drept og startet pÃ¥ nytt. Merk at dette kan vÃ¦re en tilfeldig tjeneste pÃ¥ maskinen, ikke nÃ¸dvendigvis den som bruker mest minne.\nVi har sett at det for tjenester pÃ¥ Dapla Lab ikke gir mening Ã¥ differensiere pÃ¥ forespÃ¸rsel og grense, som i tradisjonelle applikasjoner. Derfor er det nÃ¥ innfÃ¸rt en endring der forespÃ¸rselen er lik grensen.Dette hÃ¥per vi gjÃ¸r det mer forutsigbart for brukeren hva de kan forvente av tilgjengelig minne, samtidig som stabiliteten for alle brukerne ved at â€œminnetungeâ€ brukere ikke pÃ¥virker andre sine tjenester."
  },
  {
    "objectID": "news/posts/2025-03-20-stabilitet-dapla-lab/index.html#nye-maskiner",
    "href": "news/posts/2025-03-20-stabilitet-dapla-lab/index.html#nye-maskiner",
    "title": "Endringer for Ã¸kt stabilitet pÃ¥ Dapla Lab",
    "section": "Nye maskiner",
    "text": "Nye maskiner\nDet Ã¥ velge korrekte maskiner er ikke nÃ¸dvendigvis en enkel oppgave, da generell bruk av maskinen mÃ¥ veies opp mot kostnaden ved den. Kostnadsdriveren for maskinene Dapla Lab og tjenestene kjÃ¸rer pÃ¥ er minne. Den nysgjerrige kan ta en titt pÃ¥ pris-oversikten her:\nhttps://cloud.google.com/compute/all-pricing?hl=nb.\nPÃ¥ Dapla Lab benyttes maskintype n2 (standard og highmem). Ved Ã¥ monitorere og observere bruken av ressurser gjÃ¸r vi nÃ¥ ogsÃ¥ endringer i hvilke underliggende maskiner som kan kjÃ¸res opp. Fremover vil vi kjÃ¸re opp fÃ¦rre store maskiner enn flere smÃ¥ (fordi det finnes en basiskostnad for hver ekstra maskin), samtidig som vi nÃ¥ innfÃ¸rer en maskintype som ikke har like mye minne som andre (n2-standard-80). Det vil vÃ¦re kostnadseffektivt for dem som trenger mye CPU men ikke nÃ¸dvendigvis masse minne.\nDette vil vÃ¦re noe brukerne ikke merker eller vil ha et forhold til."
  },
  {
    "objectID": "news/posts/2025-03-20-stabilitet-dapla-lab/index.html#footnotes",
    "href": "news/posts/2025-03-20-stabilitet-dapla-lab/index.html#footnotes",
    "title": "Endringer for Ã¸kt stabilitet pÃ¥ Dapla Lab",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nKubernetes er systemet som brukes under panseret for Ã¥ skalere opp og ned ressurser i Dapla Lab. Les mer om Kubernetes her.â†©ï¸"
  },
  {
    "objectID": "news/posts/2025-01-08-metodebib-chapter/index.html",
    "href": "news/posts/2025-01-08-metodebib-chapter/index.html",
    "title": "Nytt kapittel om Metodebiblioteket",
    "section": "",
    "text": "Vi har skrevet et kapittel i Dapla-manualen om metodebiblioteket. Sjekk det ut her."
  },
  {
    "objectID": "news/posts/2025-04-11-bug-dapla-lab-admins/index.html",
    "href": "news/posts/2025-04-11-bug-dapla-lab-admins/index.html",
    "title": "Forbedring av kildedatatilgang i Dapla Lab",
    "section": "",
    "text": "NÃ¥r man har startet en tjeneste pÃ¥ Dapla Lab som data-admins, sÃ¥ har er tilgangen til data alltid tidsbegrenset. Tidligere har man ikke kunne starte tjenesten igjen nÃ¥r tilgangen har gÃ¥tt ut. Dette har skapt utfordringer for brukere som ikke har pushet koden til GitHub.\nNÃ¥ vi gjort endriner i Dapla Lab slik at tjenesten ogsÃ¥ kan startes opp etter at datatilgangen har utgÃ¥tt. FormÃ¥let med dette er Ã¥ la brukere pushe kode til GitHub, og det er Ã¸nskelig at brukere sletter disse tjenestene etter at det er gjort."
  },
  {
    "objectID": "news/posts/2025-04-24-config-files-in-template/index.html",
    "href": "news/posts/2025-04-24-config-files-in-template/index.html",
    "title": "Mal for statistikk-repoer oppdatert med konfigurasjonsfiler",
    "section": "",
    "text": "Det er lurt Ã¥ samle felles innstillinger og verdier som koden din trenger i en konfigurasjonsfil, i stedet for at de er spredt rundt omkring i koden.\nMalen for statistikk-repoer er nÃ¥ oppdatert med eksempel og kode for hvordan man kan bruke slike konfigurasjonsfiler. Konfigurasjonsfilen er en tekstfil som lagres i TOML-format, og som leses inn med biblioteket DynaConf. Se README-filen for ytterligere beskrivelse og konkrete eksempler.\nDere kan oppdatere deres egne repoer med de nye endringene i malen ved Ã¥ fÃ¸lge denne beskrivelsen. Ta kontakt med tech-coacher eller stÃ¸tteteam hvis dere Ã¸nsker hjelp til oppdateringen."
  },
  {
    "objectID": "news/posts/2025-04-06-update-statbank-client/index.html",
    "href": "news/posts/2025-04-06-update-statbank-client/index.html",
    "title": "Dokumentasjon av lasting til Statistikkbanken er oppdatert",
    "section": "",
    "text": "Kapitlet om dapla-statbank-client i Dapla-manualen er skrevet om og url-en er endret til: https://manual.dapla.ssb.no/statistikkere/dapla-statbank-client.html.\nI tillegg har Carl Corneil skrevet et blogginnlegg hvor han beskriver en optimal fremgangsmÃ¥te for Ã¥ laste til flere statistikkbanktabeller i et produksjonslÃ¸p."
  },
  {
    "objectID": "news/posts/2025-04-26-kildomaten-docs/index.html",
    "href": "news/posts/2025-04-26-kildomaten-docs/index.html",
    "title": "Oppdatert beskrivelse av manuell trigging av Kildomaten-jobber",
    "section": "",
    "text": "Dapla-manualen er nÃ¥ oppdatert med en bedre beskrivelse av hvordan man kan trigge Kildomaten manuelt. Brukere mÃ¥ logge seg inn som developers for Ã¥ manuelt trigge en Kildomaten-jobb. Les mer i dokumentasjonen."
  },
  {
    "objectID": "news/posts/2025-01-14-dapla-ctrl-ny-url/index.html",
    "href": "news/posts/2025-01-14-dapla-ctrl-ny-url/index.html",
    "title": "Dapla Ctrl har fÃ¥tt ny nettadresse",
    "section": "",
    "text": "Nettadressen til Dapla Ctrl er nÃ¥ endret fra https://ctrl.dapla.ssb.no/ til https://dapla-ctrl.intern.ssb.no/. Dette er gjort som en del av overgangen til NAIS-plattformen."
  },
  {
    "objectID": "news/posts/2024-11-29-kvakk-chapter/index.html",
    "href": "news/posts/2024-11-29-kvakk-chapter/index.html",
    "title": "Nytt kapittel om Kvalitet i kode og koding (KVAKK)",
    "section": "",
    "text": "Vi har skrevet et kapittel i Dapla-manualen om Kvalitet i kode og koding (KVAKK). Sjekk det ut her.\nKapittelet er forelÃ¸pig kort og henviser til confluence. Vi har planer om Ã¥ skrive mer utfyllende om KVAKK i manualen."
  },
  {
    "objectID": "news/posts/2025-04-04-dapla-toolbelt-meta-python/index.html",
    "href": "news/posts/2025-04-04-dapla-toolbelt-meta-python/index.html",
    "title": "dapla-toolbelt-metadata stÃ¸tter igjen Python 3.10",
    "section": "",
    "text": "I dapla-toolbelt-metadata v0.5.0 ble stÃ¸tte for Python 3.10 fjernet. Det gjorde at pakken ikke var kompatibel med mange prosjekter skapt vha. ssb-project. Endringen skapte problemer for flere prosjekter og derfor er stÃ¸tte for Python 3.10 nÃ¥ stÃ¸ttet i dapla-toolbelt-metadata v0.6.2."
  },
  {
    "objectID": "news/posts/2025-03-25-dapla-lab-pausing-time/index.html",
    "href": "news/posts/2025-03-25-dapla-lab-pausing-time/index.html",
    "title": "Nytt tidspunkt for automatisk pausing pÃ¥ Dapla Lab",
    "section": "",
    "text": "Alle tjenester pÃ¥ Dapla Lab blir pauset kl 22.00 hver dag. I forbindelse med at vi nÃ¥ tilbyr brukere Ã¥ melde seg ut av den automatiske pausingen ved behov, sÃ¥ endrer vi nÃ¥ tidspunktet for pausing fra 22.00 til 17.00 fra og med torsdag 27. mars.\n\n\n\n\n\n\nFigurÂ 1: Bilde av tjeneste som er pauset."
  },
  {
    "objectID": "news/posts/2024-11-11-jupyter-pyspark-chapter/index.html",
    "href": "news/posts/2024-11-11-jupyter-pyspark-chapter/index.html",
    "title": "Nytt kapittel om Jupyter-pyspark",
    "section": "",
    "text": "Vi har skrevet et kapittel i Dapla-manualen om hvordan man bruker Jupyter-pyspark i Dapla Lab. Sjekk det ut her."
  },
  {
    "objectID": "news/posts/2024-11-08-collector-chapter/index.html",
    "href": "news/posts/2024-11-08-collector-chapter/index.html",
    "title": "Nytt kapittel om Data Collector",
    "section": "",
    "text": "Vi har skrevet et kapittel i Dapla-manualen som beskriver hvordan man bruker Data Collector. Sjekk det ut her."
  },
  {
    "objectID": "news/posts/2025-04-01-daplanytt-mar25/index.html",
    "href": "news/posts/2025-04-01-daplanytt-mar25/index.html",
    "title": "DaplaNytt-mÃ¸te 1.4.2025",
    "section": "",
    "text": "DaplaNytt for mars 2025 ble avholdt som Teams-mÃ¸te 1. april 2025.\nSe opptaket her (intern lenke).\nPresentasjonen finner du her.\n\n\n\nSkjembilde fra Teams-mÃ¸tet DaplaNytt."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html",
    "href": "notebooks/spark/deltalake-intro.out.html",
    "title": "Introduksjon til Delta Lake",
    "section": "",
    "text": "Delta Lake er et databaselag som kan legges over parquet-filer i bÃ¸tter. Det kan gi oss mye av den funksjonaliteten vi har vÃ¦rt vant til i relasjonelle databaser og datavarehus. I denne notebooken vises hvordan man kan ta det i bruk pÃ¥ Dapla."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#oppsett",
    "href": "notebooks/spark/deltalake-intro.out.html#oppsett",
    "title": "Introduksjon til Delta Lake",
    "section": "Oppsett",
    "text": "Oppsett\nHvis du logger deg inn pÃ¥ https://jupyter.dapla.ssb.no/ kan du ta i bruk Delta Lake via Spark. Men for Ã¥ gjÃ¸re det mÃ¥ du installere delta-spark. For Ã¥ installere pakken mÃ¥ du jobbe i et ssb-project. I tillegg mÃ¥ du installere delta-spark-versjon som er kompatibel med Spark-versjonen som er installert pÃ¥ Dapla. GjÃ¸r derfor fÃ¸lgende:\n\nOpprett et ssb-project med kommandoen:\nssb-project create test-delta-lake\nI terminalen skriver du fÃ¸lgende for Ã¥ sjekke Spark-versjonen som er installert:\nspark-shell --version\nSjekk hvilken delta-spark-versjon som er kompatibel med din Spark-versjon og installer den med kommandoen[1]:\npoetry add delta-spark@2.3\nÃ…pne en ny notebook og velg kernel test-delta-lake.\n\nNÃ¥ har du satt opp et virtuelt miljÃ¸ med en PySpark-kernel som kjÃ¸rer en maskin (sÃ¥kalt Pyspark local kernel), der du har installert delta-spark. Vi kan nÃ¥ importere de bibliotekene vi trenger og sette igang en Spark-session.\n[1] I eksempelet er det Spark V3.3.1 som er installert og jeg installerer derfor delta-spark v2.3\n\n# Importerer biblioteker\nimport pyspark\nfrom delta import *\n\nDeretter initialiserer jeg en Spark-session. %%capture_output er med for Ã¥ forhindre at det ikke blir printet ut sensitiv informasjon i en notebook som skal inn i Dapla-manualen.\n\n%%capture captured_output\n%run /usr/local/share/jupyter/kernels/pyspark_local/init.py"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#genererer-noe-data",
    "href": "notebooks/spark/deltalake-intro.out.html#genererer-noe-data",
    "title": "Introduksjon til Delta Lake",
    "section": "Genererer noe data",
    "text": "Genererer noe data\n\n# Genererer noe data med Spark\ndata = spark.range(10, 15)\ndata.show()\n\n+---+\n| id|\n+---+\n| 10|\n| 11|\n| 12|\n| 13|\n| 14|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#skriver-ut-en-delta-lake-table",
    "href": "notebooks/spark/deltalake-intro.out.html#skriver-ut-en-delta-lake-table",
    "title": "Introduksjon til Delta Lake",
    "section": "Skriver ut en Delta Lake Table",
    "text": "Skriver ut en Delta Lake Table\nLa oss skrive ut Spark DataFrame som en Delta Lake Table og bli kjent med strukturen i objektet:\n\n%%time\ndata.write.format(\"delta\").mode(\"overwrite\").save(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp4\"\n)\n\nCPU times: user 3.38 ms, sys: 1.7 ms, total: 5.08 ms\nWall time: 5.59 s\n\n\nVi kan deretter printe ut hva som ble opprettet nÃ¥r vi skrev ut en Delta Lake Table:\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/temp4/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp4/_delta_log',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000000.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-9b3b81a9-2771-4fb4-9f0f-659fd160d643-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-0f2f8ba5-3161-41e8-b5d1-2084128a5bed-c000.snappy.parquet']\n\n\n\nDelta Lake Tabellstruktur\nMappenstrukturen du ser over er typisk for en Delta Lake-tabell. La oss bryte ned komponentene:\n\n_delta_log/:\n\nDette er transaksjonsloggmappen for Delta Lake-tabellen. Den inneholder en serie med JSON-filer som registrerer alle transaksjoner gjort pÃ¥ tabellen.\nTransaksjonsloggen er avgjÃ¸rende for Delta Lakes ACID-transaksjonsegenskaper, som muliggjÃ¸r funksjoner som atomiske forpliktelser, tilbakerullinger og tid-reise-forespÃ¸rsler.\n\n_delta_log/00000000000000000000.json:\n\nDette er en JSON-fil innenfor transaksjonsloggkatalogen. Hver JSON-fil i denne mappen tilsvarer en transaksjon (eller en batch av transaksjoner) gjort pÃ¥ tabellen.\nFilnavnet er null-fylt og representerer transaksjonsversjonen. I dette tilfellet representerer 00000000000000000000.json den aller fÃ¸rste versjonen (versjon 0) av tabellen. Etter hvert som flere transaksjoner blir gjort, vil nye filer bli lagt til med Ã¸kende versjonsnumre.\n\npart-00000-450715bd-6b0c-4827-bb8a-b0265505ca72-c000.snappy.parquet og part-00001-977ed55f-5ce5-469f-8a1d-9eafb143c215-c000.snappy.parquet:\n\nDette er de faktiske datafilene til Delta Lake-tabellen, lagret i Parquet-format.\n.snappy.parquet-utvidelsen indikerer at dataene er komprimert ved hjelp av Snappy-komprimeringsalgoritmen.\nFilnavnene er typiske for Sparks navngivningskonvensjon for datadeler. Prefiksene part-00000 og part-00001 indikerer partisjonsnumre. De lange UUID-lignende strengene er unike identifikatorer for datafilene. Suffikset c000 indikerer Spark-oppgaven som skrev filen.\n\n\nMappen representerer en Delta Lake-tabell med data lagret i Parquet-format og en transaksjonslogg som sporer endringer i tabellen. TilstedevÃ¦relsen av _delta_log-mappen og innholdet er det som skiller en Delta Lake-tabell fra et vanlig Parquet-datasett."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#lese-inn-tabell",
    "href": "notebooks/spark/deltalake-intro.out.html#lese-inn-tabell",
    "title": "Introduksjon til Delta Lake",
    "section": "Lese inn tabell",
    "text": "Lese inn tabell\n\ndeltaTable = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 13|\n| 14|\n| 10|\n| 11|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#modifisere-datasettet",
    "href": "notebooks/spark/deltalake-intro.out.html#modifisere-datasettet",
    "title": "Introduksjon til Delta Lake",
    "section": "Modifisere datasettet",
    "text": "Modifisere datasettet\nLa oss modifisere datasettet ved Ã¥ bytte ut verdien 13 med 15 i id-kolonnen:\n\nfrom pyspark.sql.functions import col, lit\n\n# Update the cell with value 13 to 15\ndeltaTable.update(condition=(col(\"id\") == 13), set={\"id\": lit(15)})\n\nEt viktig poeng Ã¥ fÃ¥ med seg her er at vi nÃ¥ oppdaterte Delta Lake Table objektet bÃ¥de i minnet og pÃ¥ disk. La oss bevise det ved Ã¥ lese inn fra disk:\n\ndeltaTable2 = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\ndeltaTable2.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n+---+\n\n\nOg deretter ved Ã¥ printe ut det opprinnelige objektet vi leste inn:\n\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#append-data",
    "href": "notebooks/spark/deltalake-intro.out.html#append-data",
    "title": "Introduksjon til Delta Lake",
    "section": "Append data",
    "text": "Append data\nLa oss legge til verdiene 20 og 21 til datasettet. FÃ¸rst lager vi en Spark dataframe:\n\nnew_data = [(20,), (21,)]\nnew_df = spark.createDataFrame(new_data, [\"id\"])\nnew_df.show()\n\n+---+\n| id|\n+---+\n| 20|\n| 21|\n+---+\n\n\nDeretter kan vi appendere det til vÃ¥r opprinnelige dataframe:\n\nnew_df.write.format(\"delta\").mode(\"append\").save(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp4\"\n)\n\n\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n| 21|\n| 20|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#historien-og-metadata-til-filen",
    "href": "notebooks/spark/deltalake-intro.out.html#historien-og-metadata-til-filen",
    "title": "Introduksjon til Delta Lake",
    "section": "Historien og metadata til filen",
    "text": "Historien og metadata til filen\nNÃ¥ som vi har gjort noen endringer kan vi se pÃ¥ historien til filen:\n\n# Lister ut filene i bÃ¸tta\nfs = FileClient.get_gcs_file_system()\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/temp4/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp4/_delta_log',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000000.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000001.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000002.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-73e5052f-1b82-48da-ab37-2cbc01bb46c1-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-9b3b81a9-2771-4fb4-9f0f-659fd160d643-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-d04d0ca2-8e8b-42e9-a8a3-0fed9a0e4e41-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-0f2f8ba5-3161-41e8-b5d1-2084128a5bed-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-30d707e4-dd9a-4bfd-a4c7-7fbb1933e9ae-c000.snappy.parquet']\n\n\nVi ser av transaksjonsloggen i _delta_log-mappen at det nÃ¥ har vÃ¦rt 3 transaksjoner pÃ¥ datasettet. vi ser ogsÃ¥ av navnene pÃ¥ parquet-filene, part-00000 og part-00001, at det finnes to versjoner av filen. Hvis vi Ã¸nsker Ã¥ bli bedre kjent med hva slags informasjon som blir lagret fra transaksjonene, sÃ¥ kan vi printe ut den siste transaksjonen som heter 00000000000000000002.json:\n\nimport json\n\nfrom dapla import FileClient\n\n# Kobler oss pÃ¥ bÃ¸ttene\nfs = FileClient.get_gcs_file_system()\n\n# Filsti\npath = \"gs://ssb-dapla-felles-data-produkt-prod/temp4/_delta_log/00000000000000000002.json\"\n\nwith fs.open(path, \"r\") as f:\n    for line in f:\n        data = json.loads(line)\n        pretty_content = json.dumps(data, indent=4)\n        print(pretty_content)\n        print(\"-\" * 50)  # Print separator for clarity\n\n{\n    \"commitInfo\": {\n        \"timestamp\": 1696942544879,\n        \"operation\": \"WRITE\",\n        \"operationParameters\": {\n            \"mode\": \"Append\",\n            \"partitionBy\": \"[]\"\n        },\n        \"readVersion\": 1,\n        \"isolationLevel\": \"Serializable\",\n        \"isBlindAppend\": true,\n        \"operationMetrics\": {\n            \"numFiles\": \"2\",\n            \"numOutputRows\": \"2\",\n            \"numOutputBytes\": \"956\"\n        },\n        \"engineInfo\": \"Apache-Spark/3.3.1 Delta-Lake/2.3.0\",\n        \"txnId\": \"a3dcd582-8362-4fc2-a8ce-57613d2eb2b8\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00000-73e5052f-1b82-48da-ab37-2cbc01bb46c1-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 478,\n        \"modificationTime\": 1696942544755,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":20},\\\"maxValues\\\":{\\\"id\\\":20},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00001-30d707e4-dd9a-4bfd-a4c7-7fbb1933e9ae-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 478,\n        \"modificationTime\": 1696942544833,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":21},\\\"maxValues\\\":{\\\"id\\\":21},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n\n\nHer ser vi at vi fÃ¥r masse informasjon om endringen, bÃ¥de metadata om transaksjonen i commitInfo, og informasjon om nye data-filer i add-delen. Vi ser at det er en veldig rik beskrivelse av endringer, men det kan vÃ¦re vanskeig Ã¥ lese innholdet direkte. La oss heller bruke history()-funksjonen som kommer med delta:\n\nhistory = deltaTable.history()\nhistory.show()\n\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n|      2|2023-10-10 12:55:...|  null|    null|    WRITE|{mode -&gt; Append, ...|null|    null|     null|          1|  Serializable|         true|{numFiles -&gt; 2, n...|        null|Apache-Spark/3.3....|\n|      1|2023-10-10 12:55:...|  null|    null|   UPDATE|{predicate -&gt; (id...|null|    null|     null|          0|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n|      0|2023-10-10 12:55:...|  null|    null|    WRITE|{mode -&gt; Overwrit...|null|    null|     null|       null|  Serializable|        false|{numFiles -&gt; 2, n...|        null|Apache-Spark/3.3....|\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n\n\nSiden det blit trangt i tabellen over sÃ¥ kan vi velge hvilke variabler vi Ã¸nsker Ã¥ se pÃ¥:\n\n# Oversikt over alle kolonner som finnes i historien\nhistory.columns\n\n['version',\n 'timestamp',\n 'userId',\n 'userName',\n 'operation',\n 'operationParameters',\n 'job',\n 'notebook',\n 'clusterId',\n 'readVersion',\n 'isolationLevel',\n 'isBlindAppend',\n 'operationMetrics',\n 'userMetadata',\n 'engineInfo']\n\n\n\n# Velger de kolonnene jeg er interessert i\nselected_history = deltaTable.history().select(\n    \"version\", \"timestamp\", \"operation\", \"operationParameters\"\n)\n\n\n# Display the selected columns\nselected_history.show(truncate=50)\n\n+-------+-----------------------+---------+--------------------------------------+\n|version|              timestamp|operation|                   operationParameters|\n+-------+-----------------------+---------+--------------------------------------+\n|      2|2023-10-10 12:55:45.014|    WRITE|   {mode -&gt; Append, partitionBy -&gt; []}|\n|      1|2023-10-10 12:55:37.054|   UPDATE|        {predicate -&gt; (id#4452L = 13)}|\n|      0|2023-10-10 12:55:29.048|    WRITE|{mode -&gt; Overwrite, partitionBy -&gt; []}|\n+-------+-----------------------+---------+--------------------------------------+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#egendefinert-metadata",
    "href": "notebooks/spark/deltalake-intro.out.html#egendefinert-metadata",
    "title": "Introduksjon til Delta Lake",
    "section": "Egendefinert metadata",
    "text": "Egendefinert metadata\nDelta Lake stÃ¸tter ogsÃ¥ egendefinert metadata. Det kan for eksempel vÃ¦re nyttig hvis man Ã¸nsker Ã¥ bruke Delta Lake som en backend for et GUI som lar brukeren oppdatere en tabell fra GUI-et. Da Ã¸nsker man typisk Ã¥ lagre hvem som gjorde endringer og nÃ¥r det ble gjort. La oss legge pÃ¥ noe slik metadata:\n\n# Leser inn filen\ndf = spark.read.format(\"delta\").load(\"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\n\n\n# Lagrer egendefinert metadata i en json-fil\nimport json\n\nmetadata = {\n    \"comment\": \"Kontaktet oppgavegiver og kranglet!\",\n    \"manueltEditert\": \"True\",\n    \"maskineltEditert\": \"False\",\n}\nmetadata\n\n{'comment': 'Kontaktet oppgavegiver og kranglet!',\n 'manueltEditert': 'True',\n 'maskineltEditert': 'False'}\n\n\nVi kan deretter appende dette til den siste versjonen av fila.\n\n(\n    df.write.format(\"delta\")\n    .mode(\"append\")\n    .option(\"userMetadata\", json.dumps(metadata))  # Serialize metadata to a string\n    .save(\"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\n)\n\n\n# Laster inn tabellen\ndeltaTable = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\n\n# Henter ut historien\nhistory_df = deltaTable.history()\n\n\n# Show the operation details, including metadata\nhistory_df.select(\n    \"version\", \"timestamp\", \"operation\", \"operationParameters\", \"userMetadata\"\n).show(truncate=10)\n\n+-------+----------+---------+-------------------+------------+\n|version| timestamp|operation|operationParameters|userMetadata|\n+-------+----------+---------+-------------------+------------+\n|      3|2023-10...|    WRITE|         {mode -...|  {\"comme...|\n|      2|2023-10...|    WRITE|         {mode -...|        null|\n|      1|2023-10...|   UPDATE|         {predic...|        null|\n|      0|2023-10...|    WRITE|         {mode -...|        null|\n+-------+----------+---------+-------------------+------------+\n\n\n\nhistory_df.select(\"version\", \"userMetadata\").show(truncate=50)\n\n+-------+--------------------------------------------------+\n|version|                                      userMetadata|\n+-------+--------------------------------------------------+\n|      3|{\"comment\": \"Kontaktet oppgavegiver og kranglet...|\n|      2|                                              null|\n|      1|                                              null|\n|      0|                                              null|\n+-------+--------------------------------------------------+\n\n\nVi ser at vi la til vÃ¥r egen metadata i versjon 3 av fila. Vi kan printe ut den rÃ¥ transaksjonsloggen som tidligere, men nÃ¥ er vi pÃ¥ transaksjon 3 00000000000000000003.json:\n\nfrom dapla import FileClient\n\n# Kobler oss pÃ¥ bÃ¸ttene\nfs = FileClient.get_gcs_file_system()\n\n# Filsti\npath = \"gs://ssb-dapla-felles-data-produkt-prod/temp4/_delta_log/00000000000000000003.json\"\n\nwith fs.open(path, \"r\") as f:\n    for line in f:\n        data = json.loads(line)\n        pretty_content = json.dumps(data, indent=4)\n        print(pretty_content)\n        print(\"-\" * 50)  # Print separator for clarity\n\n{\n    \"commitInfo\": {\n        \"timestamp\": 1696942553907,\n        \"operation\": \"WRITE\",\n        \"operationParameters\": {\n            \"mode\": \"Append\",\n            \"partitionBy\": \"[]\"\n        },\n        \"readVersion\": 2,\n        \"isolationLevel\": \"Serializable\",\n        \"isBlindAppend\": false,\n        \"operationMetrics\": {\n            \"numFiles\": \"2\",\n            \"numOutputRows\": \"7\",\n            \"numOutputBytes\": \"989\"\n        },\n        \"userMetadata\": \"{\\\"comment\\\": \\\"Kontaktet oppgavegiver og kranglet!\\\", \\\"manueltEditert\\\": \\\"True\\\", \\\"maskineltEditert\\\": \\\"False\\\"}\",\n        \"engineInfo\": \"Apache-Spark/3.3.1 Delta-Lake/2.3.0\",\n        \"txnId\": \"e7de92bf-b0f9-4341-8bbb-9b382f2f3eb6\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00000-96369f3d-fe4a-4365-a0df-00c813027399-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 503,\n        \"modificationTime\": 1696942553856,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":5,\\\"minValues\\\":{\\\"id\\\":10},\\\"maxValues\\\":{\\\"id\\\":15},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00001-0f1bc8e6-093b-49a9-ad0b-78d5a148cfb6-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 486,\n        \"modificationTime\": 1696942553853,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":2,\\\"minValues\\\":{\\\"id\\\":20},\\\"maxValues\\\":{\\\"id\\\":21},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n\n\nVi er da at metadataene ligger som forventet i metadata-delen."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html",
    "href": "notebooks/spark/deltalake-intro.html",
    "title": "Introduksjon til Delta Lake",
    "section": "",
    "text": "Delta Lake er et databaselag som kan legges over parquet-filer i bÃ¸tter. Det kan gi oss mye av den funksjonaliteten vi har vÃ¦rt vant til i relasjonelle databaser og datavarehus. I denne notebooken vises hvordan man kan ta det i bruk pÃ¥ Dapla."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#oppsett",
    "href": "notebooks/spark/deltalake-intro.html#oppsett",
    "title": "Introduksjon til Delta Lake",
    "section": "Oppsett",
    "text": "Oppsett\nHvis du logger deg inn pÃ¥ https://jupyter.dapla.ssb.no/ kan du ta i bruk Delta Lake via Spark. Men for Ã¥ gjÃ¸re det mÃ¥ du installere delta-spark. For Ã¥ installere pakken mÃ¥ du jobbe i et ssb-project. I tillegg mÃ¥ du installere delta-spark-versjon som er kompatibel med Spark-versjonen som er installert pÃ¥ Dapla. GjÃ¸r derfor fÃ¸lgende:\n\nOpprett et ssb-project med kommandoen:\nssb-project create test-delta-lake\nI terminalen skriver du fÃ¸lgende for Ã¥ sjekke Spark-versjonen som er installert:\nspark-shell --version\nSjekk hvilken delta-spark-versjon som er kompatibel med din Spark-versjon og installer den med kommandoen1:\npoetry add delta-spark@2.3\nÃ…pne en ny notebook og velg kernel test-delta-lake.\n\nNÃ¥ har du satt opp et virtuelt miljÃ¸ med en PySpark-kernel som kjÃ¸rer en maskin (sÃ¥kalt Pyspark local kernel), der du har installert delta-spark. Vi kan nÃ¥ importere de bibliotekene vi trenger og sette igang en Spark-session.\n\n# Importerer biblioteker\nimport pyspark\nfrom delta import *\n\nDeretter initialiserer jeg en Spark-session. %%capture_output er med for Ã¥ forhindre at det ikke blir printet ut sensitiv informasjon i en notebook som skal inn i Dapla-manualen.\n\n%%capture captured_output\n%run /usr/local/share/jupyter/kernels/pyspark_local/init.py"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#genererer-noe-data",
    "href": "notebooks/spark/deltalake-intro.html#genererer-noe-data",
    "title": "Introduksjon til Delta Lake",
    "section": "Genererer noe data",
    "text": "Genererer noe data\n\n# Genererer noe data med Spark\ndata = spark.range(10, 15)\ndata.show()\n\n+---+\n| id|\n+---+\n| 10|\n| 11|\n| 12|\n| 13|\n| 14|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#skriver-ut-en-delta-lake-table",
    "href": "notebooks/spark/deltalake-intro.html#skriver-ut-en-delta-lake-table",
    "title": "Introduksjon til Delta Lake",
    "section": "Skriver ut en Delta Lake Table",
    "text": "Skriver ut en Delta Lake Table\nLa oss skrive ut Spark DataFrame som en Delta Lake Table og bli kjent med strukturen i objektet:\n\n%%time\ndata.write.format(\"delta\").mode(\"overwrite\").save(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp4\"\n)\n\nCPU times: user 3.38 ms, sys: 1.7 ms, total: 5.08 ms\nWall time: 5.59 s\n\n\nVi kan deretter printe ut hva som ble opprettet nÃ¥r vi skrev ut en Delta Lake Table:\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/temp4/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp4/_delta_log',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000000.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-9b3b81a9-2771-4fb4-9f0f-659fd160d643-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-0f2f8ba5-3161-41e8-b5d1-2084128a5bed-c000.snappy.parquet']\n\n\n\nDelta Lake Tabellstruktur\nMappenstrukturen du ser over er typisk for en Delta Lake-tabell. La oss bryte ned komponentene:\n\n_delta_log/:\n\nDette er transaksjonsloggmappen for Delta Lake-tabellen. Den inneholder en serie med JSON-filer som registrerer alle transaksjoner gjort pÃ¥ tabellen.\nTransaksjonsloggen er avgjÃ¸rende for Delta Lakes ACID-transaksjonsegenskaper, som muliggjÃ¸r funksjoner som atomiske forpliktelser, tilbakerullinger og tid-reise-forespÃ¸rsler.\n\n_delta_log/00000000000000000000.json:\n\nDette er en JSON-fil innenfor transaksjonsloggkatalogen. Hver JSON-fil i denne mappen tilsvarer en transaksjon (eller en batch av transaksjoner) gjort pÃ¥ tabellen.\nFilnavnet er null-fylt og representerer transaksjonsversjonen. I dette tilfellet representerer 00000000000000000000.json den aller fÃ¸rste versjonen (versjon 0) av tabellen. Etter hvert som flere transaksjoner blir gjort, vil nye filer bli lagt til med Ã¸kende versjonsnumre.\n\npart-00000-450715bd-6b0c-4827-bb8a-b0265505ca72-c000.snappy.parquet og part-00001-977ed55f-5ce5-469f-8a1d-9eafb143c215-c000.snappy.parquet:\n\nDette er de faktiske datafilene til Delta Lake-tabellen, lagret i Parquet-format.\n.snappy.parquet-utvidelsen indikerer at dataene er komprimert ved hjelp av Snappy-komprimeringsalgoritmen.\nFilnavnene er typiske for Sparks navngivningskonvensjon for datadeler. Prefiksene part-00000 og part-00001 indikerer partisjonsnumre. De lange UUID-lignende strengene er unike identifikatorer for datafilene. Suffikset c000 indikerer Spark-oppgaven som skrev filen.\n\n\nMappen representerer en Delta Lake-tabell med data lagret i Parquet-format og en transaksjonslogg som sporer endringer i tabellen. TilstedevÃ¦relsen av _delta_log-mappen og innholdet er det som skiller en Delta Lake-tabell fra et vanlig Parquet-datasett."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#lese-inn-tabell",
    "href": "notebooks/spark/deltalake-intro.html#lese-inn-tabell",
    "title": "Introduksjon til Delta Lake",
    "section": "Lese inn tabell",
    "text": "Lese inn tabell\n\ndeltaTable = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 13|\n| 14|\n| 10|\n| 11|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#modifisere-datasettet",
    "href": "notebooks/spark/deltalake-intro.html#modifisere-datasettet",
    "title": "Introduksjon til Delta Lake",
    "section": "Modifisere datasettet",
    "text": "Modifisere datasettet\nLa oss modifisere datasettet ved Ã¥ bytte ut verdien 13 med 15 i id-kolonnen:\n\nfrom pyspark.sql.functions import col, lit\n\n# Update the cell with value 13 to 15\ndeltaTable.update(condition=(col(\"id\") == 13), set={\"id\": lit(15)})\n\nEt viktig poeng Ã¥ fÃ¥ med seg her er at vi nÃ¥ oppdaterte Delta Lake Table objektet bÃ¥de i minnet og pÃ¥ disk. La oss bevise det ved Ã¥ lese inn fra disk:\n\ndeltaTable2 = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\ndeltaTable2.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n+---+\n\n\n\nOg deretter ved Ã¥ printe ut det opprinnelige objektet vi leste inn:\n\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#append-data",
    "href": "notebooks/spark/deltalake-intro.html#append-data",
    "title": "Introduksjon til Delta Lake",
    "section": "Append data",
    "text": "Append data\nLa oss legge til verdiene 20 og 21 til datasettet. FÃ¸rst lager vi en Spark dataframe:\n\nnew_data = [(20,), (21,)]\nnew_df = spark.createDataFrame(new_data, [\"id\"])\nnew_df.show()\n\n+---+\n| id|\n+---+\n| 20|\n| 21|\n+---+\n\n\n\nDeretter kan vi appendere det til vÃ¥r opprinnelige dataframe:\n\nnew_df.write.format(\"delta\").mode(\"append\").save(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp4\"\n)\n\n\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n| 21|\n| 20|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#historien-og-metadata-til-filen",
    "href": "notebooks/spark/deltalake-intro.html#historien-og-metadata-til-filen",
    "title": "Introduksjon til Delta Lake",
    "section": "Historien og metadata til filen",
    "text": "Historien og metadata til filen\nNÃ¥ som vi har gjort noen endringer kan vi se pÃ¥ historien til filen:\n\n# Lister ut filene i bÃ¸tta\nfs = FileClient.get_gcs_file_system()\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/temp4/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp4/_delta_log',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000000.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000001.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000002.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-73e5052f-1b82-48da-ab37-2cbc01bb46c1-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-9b3b81a9-2771-4fb4-9f0f-659fd160d643-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-d04d0ca2-8e8b-42e9-a8a3-0fed9a0e4e41-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-0f2f8ba5-3161-41e8-b5d1-2084128a5bed-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-30d707e4-dd9a-4bfd-a4c7-7fbb1933e9ae-c000.snappy.parquet']\n\n\nVi ser av transaksjonsloggen i _delta_log-mappen at det nÃ¥ har vÃ¦rt 3 transaksjoner pÃ¥ datasettet. vi ser ogsÃ¥ av navnene pÃ¥ parquet-filene, part-00000 og part-00001, at det finnes to versjoner av filen. Hvis vi Ã¸nsker Ã¥ bli bedre kjent med hva slags informasjon som blir lagret fra transaksjonene, sÃ¥ kan vi printe ut den siste transaksjonen som heter 00000000000000000002.json:\n\nimport json\n\nfrom dapla import FileClient\n\n# Kobler oss pÃ¥ bÃ¸ttene\nfs = FileClient.get_gcs_file_system()\n\n# Filsti\npath = \"gs://ssb-dapla-felles-data-produkt-prod/temp4/_delta_log/00000000000000000002.json\"\n\nwith fs.open(path, \"r\") as f:\n    for line in f:\n        data = json.loads(line)\n        pretty_content = json.dumps(data, indent=4)\n        print(pretty_content)\n        print(\"-\" * 50)  # Print separator for clarity\n\n{\n    \"commitInfo\": {\n        \"timestamp\": 1696942544879,\n        \"operation\": \"WRITE\",\n        \"operationParameters\": {\n            \"mode\": \"Append\",\n            \"partitionBy\": \"[]\"\n        },\n        \"readVersion\": 1,\n        \"isolationLevel\": \"Serializable\",\n        \"isBlindAppend\": true,\n        \"operationMetrics\": {\n            \"numFiles\": \"2\",\n            \"numOutputRows\": \"2\",\n            \"numOutputBytes\": \"956\"\n        },\n        \"engineInfo\": \"Apache-Spark/3.3.1 Delta-Lake/2.3.0\",\n        \"txnId\": \"a3dcd582-8362-4fc2-a8ce-57613d2eb2b8\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00000-73e5052f-1b82-48da-ab37-2cbc01bb46c1-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 478,\n        \"modificationTime\": 1696942544755,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":20},\\\"maxValues\\\":{\\\"id\\\":20},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00001-30d707e4-dd9a-4bfd-a4c7-7fbb1933e9ae-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 478,\n        \"modificationTime\": 1696942544833,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":21},\\\"maxValues\\\":{\\\"id\\\":21},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n\n\nHer ser vi at vi fÃ¥r masse informasjon om endringen, bÃ¥de metadata om transaksjonen i commitInfo, og informasjon om nye data-filer i add-delen. Vi ser at det er en veldig rik beskrivelse av endringer, men det kan vÃ¦re vanskeig Ã¥ lese innholdet direkte. La oss heller bruke history()-funksjonen som kommer med delta:\n\nhistory = deltaTable.history()\nhistory.show()\n\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n|      2|2023-10-10 12:55:...|  null|    null|    WRITE|{mode -&gt; Append, ...|null|    null|     null|          1|  Serializable|         true|{numFiles -&gt; 2, n...|        null|Apache-Spark/3.3....|\n|      1|2023-10-10 12:55:...|  null|    null|   UPDATE|{predicate -&gt; (id...|null|    null|     null|          0|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n|      0|2023-10-10 12:55:...|  null|    null|    WRITE|{mode -&gt; Overwrit...|null|    null|     null|       null|  Serializable|        false|{numFiles -&gt; 2, n...|        null|Apache-Spark/3.3....|\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n\n\n\nSiden det blit trangt i tabellen over sÃ¥ kan vi velge hvilke variabler vi Ã¸nsker Ã¥ se pÃ¥:\n\n# Oversikt over alle kolonner som finnes i historien\nhistory.columns\n\n['version',\n 'timestamp',\n 'userId',\n 'userName',\n 'operation',\n 'operationParameters',\n 'job',\n 'notebook',\n 'clusterId',\n 'readVersion',\n 'isolationLevel',\n 'isBlindAppend',\n 'operationMetrics',\n 'userMetadata',\n 'engineInfo']\n\n\n\n# Velger de kolonnene jeg er interessert i\nselected_history = deltaTable.history().select(\n    \"version\", \"timestamp\", \"operation\", \"operationParameters\"\n)\n\n\n# Display the selected columns\nselected_history.show(truncate=50)\n\n+-------+-----------------------+---------+--------------------------------------+\n|version|              timestamp|operation|                   operationParameters|\n+-------+-----------------------+---------+--------------------------------------+\n|      2|2023-10-10 12:55:45.014|    WRITE|   {mode -&gt; Append, partitionBy -&gt; []}|\n|      1|2023-10-10 12:55:37.054|   UPDATE|        {predicate -&gt; (id#4452L = 13)}|\n|      0|2023-10-10 12:55:29.048|    WRITE|{mode -&gt; Overwrite, partitionBy -&gt; []}|\n+-------+-----------------------+---------+--------------------------------------+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#egendefinert-metadata",
    "href": "notebooks/spark/deltalake-intro.html#egendefinert-metadata",
    "title": "Introduksjon til Delta Lake",
    "section": "Egendefinert metadata",
    "text": "Egendefinert metadata\nDelta Lake stÃ¸tter ogsÃ¥ egendefinert metadata. Det kan for eksempel vÃ¦re nyttig hvis man Ã¸nsker Ã¥ bruke Delta Lake som en backend for et GUI som lar brukeren oppdatere en tabell fra GUI-et. Da Ã¸nsker man typisk Ã¥ lagre hvem som gjorde endringer og nÃ¥r det ble gjort. La oss legge pÃ¥ noe slik metadata:\n\n# Leser inn filen\ndf = spark.read.format(\"delta\").load(\"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\n\n\n\nCode\n# Lagrer egendefinert metadata i en json-fil\nimport json\n\nmetadata = {\n    \"comment\": \"Kontaktet oppgavegiver og kranglet!\",\n    \"manueltEditert\": \"True\",\n    \"maskineltEditert\": \"False\",\n}\nmetadata\n\n\n{'comment': 'Kontaktet oppgavegiver og kranglet!',\n 'manueltEditert': 'True',\n 'maskineltEditert': 'False'}\n\n\nVi kan deretter appende dette til den siste versjonen av fila.\n\n(\n    df.write.format(\"delta\")\n    .mode(\"append\")\n    .option(\"userMetadata\", json.dumps(metadata))  # Serialize metadata to a string\n    .save(\"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\n)\n\n\n# Laster inn tabellen\ndeltaTable = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\n\n# Henter ut historien\nhistory_df = deltaTable.history()\n\n\n# Show the operation details, including metadata\nhistory_df.select(\n    \"version\", \"timestamp\", \"operation\", \"operationParameters\", \"userMetadata\"\n).show(truncate=10)\n\n+-------+----------+---------+-------------------+------------+\n|version| timestamp|operation|operationParameters|userMetadata|\n+-------+----------+---------+-------------------+------------+\n|      3|2023-10...|    WRITE|         {mode -...|  {\"comme...|\n|      2|2023-10...|    WRITE|         {mode -...|        null|\n|      1|2023-10...|   UPDATE|         {predic...|        null|\n|      0|2023-10...|    WRITE|         {mode -...|        null|\n+-------+----------+---------+-------------------+------------+\n\n\n\n\nhistory_df.select(\"version\", \"userMetadata\").show(truncate=50)\n\n+-------+--------------------------------------------------+\n|version|                                      userMetadata|\n+-------+--------------------------------------------------+\n|      3|{\"comment\": \"Kontaktet oppgavegiver og kranglet...|\n|      2|                                              null|\n|      1|                                              null|\n|      0|                                              null|\n+-------+--------------------------------------------------+\n\n\n\nVi ser at vi la til vÃ¥r egen metadata i versjon 3 av fila. Vi kan printe ut den rÃ¥ transaksjonsloggen som tidligere, men nÃ¥ er vi pÃ¥ transaksjon 3 00000000000000000003.json:\n\nfrom dapla import FileClient\n\n# Kobler oss pÃ¥ bÃ¸ttene\nfs = FileClient.get_gcs_file_system()\n\n# Filsti\npath = \"gs://ssb-dapla-felles-data-produkt-prod/temp4/_delta_log/00000000000000000003.json\"\n\nwith fs.open(path, \"r\") as f:\n    for line in f:\n        data = json.loads(line)\n        pretty_content = json.dumps(data, indent=4)\n        print(pretty_content)\n        print(\"-\" * 50)  # Print separator for clarity\n\n{\n    \"commitInfo\": {\n        \"timestamp\": 1696942553907,\n        \"operation\": \"WRITE\",\n        \"operationParameters\": {\n            \"mode\": \"Append\",\n            \"partitionBy\": \"[]\"\n        },\n        \"readVersion\": 2,\n        \"isolationLevel\": \"Serializable\",\n        \"isBlindAppend\": false,\n        \"operationMetrics\": {\n            \"numFiles\": \"2\",\n            \"numOutputRows\": \"7\",\n            \"numOutputBytes\": \"989\"\n        },\n        \"userMetadata\": \"{\\\"comment\\\": \\\"Kontaktet oppgavegiver og kranglet!\\\", \\\"manueltEditert\\\": \\\"True\\\", \\\"maskineltEditert\\\": \\\"False\\\"}\",\n        \"engineInfo\": \"Apache-Spark/3.3.1 Delta-Lake/2.3.0\",\n        \"txnId\": \"e7de92bf-b0f9-4341-8bbb-9b382f2f3eb6\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00000-96369f3d-fe4a-4365-a0df-00c813027399-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 503,\n        \"modificationTime\": 1696942553856,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":5,\\\"minValues\\\":{\\\"id\\\":10},\\\"maxValues\\\":{\\\"id\\\":15},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00001-0f1bc8e6-093b-49a9-ad0b-78d5a148cfb6-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 486,\n        \"modificationTime\": 1696942553853,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":2,\\\"minValues\\\":{\\\"id\\\":20},\\\"maxValues\\\":{\\\"id\\\":21},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n\n\nVi er da at metadataene ligger som forventet i metadata-delen."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#footnotes",
    "href": "notebooks/spark/deltalake-intro.html#footnotes",
    "title": "Introduksjon til Delta Lake",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI eksempelet er det Spark V3.3.1 som er installert og jeg installerer derfor delta-spark v2.3â†©ï¸"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html",
    "href": "notebooks/spark/pyspark-intro.html",
    "title": "Introduksjon til PySpark",
    "section": "",
    "text": "Apache Spark er et sterkt verktÃ¸y som utvider mulighetsrommet for databehandling med R og Python. Kjernefunksjonaliteten ligger i at den lar deg kjÃ¸re en jobb pÃ¥ flere maskiner samtidig, noe som ikke er mulig med klassiske rammeverk som Pandas og Tidyverse. FÃ¸lgelig er det et rammeverk som blant annet er veldig egnet for Ã¥ prosessere store datamengder eller gjÃ¸re store beregninger. Les om mer om Apache Spark i Dapla-manualen\nI denne notebooken vises noen enkle eksempler pÃ¥ hvordan du kan jobbe med data med PySpark, et Python-grensesnitt mot Spark."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#oppsett",
    "href": "notebooks/spark/pyspark-intro.html#oppsett",
    "title": "Introduksjon til PySpark",
    "section": "Oppsett",
    "text": "Oppsett\nNÃ¥r du logger deg inn pÃ¥ Dapla kan du velge mellom 2 ferdigoppsatte kernels for Ã¥ jobbe med PySpark:\n\nPyspark (local)\nPyspark (k8s cluster)\n\nDen fÃ¸rste lar deg bruke Spark pÃ¥ en enkeltmaskin, mens den andre lar deg distribuere kjÃ¸ringen pÃ¥ mange maskiner avhengig av hvor store jobbene er. I eksemplene under brukes Pyspark (local).\n\n# Importer biblioteker\nimport numpy as np\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import date_format, explode, expr, sequence, sum, avg\nfrom pyspark.sql.types import DateType, DoubleType, StructField, StructType\n\n# Initialierer en SparkSession\nspark = (\n    SparkSession.builder.master(\"local[2]\")\n    .appName(\"Dapla-manual-example\")\n    .getOrCreate()\n)\n\nI koden over importerer vi de bibliotekene vi skal bruke under. Grunnen til at vi importerer pyspark.sql er at dette er at Spark SQL er Apache Spark sin modul for Ã¥ jobbe med strukturerte data. Og som navnet tilsier vil det si at vi kan blande Python og SQL i koden vÃ¥r. Vi skal nÃ¦rmere pÃ¥ hvordan man bruke SQL fra PySpark-notebook senere.\nSpark tilbyr ogsÃ¥ et eget grensesnitt, Spark UI, for Ã¥ monitorere hva som skjer under en SparkSession. Vi kan bruke fÃ¸lgende kommando for Ã¥ fÃ¥ opp en lenke til Spark UI i notebooken vÃ¥r:\n\nspark.sparkContext\n\n\n        \n            SparkContext\n\n            Spark UI\n\n            \n              Version\n                v3.3.1\n              Master\n                local[*]\n              AppName\n                pyspark-shell\n            \n        \n        \n\n\nKlikker du pÃ¥ Spark UI-lenken sÃ¥ tar den deg til dashboard som lar deg monitorere, debugge, optimalisere og forstÃ¥ kjÃ¸ringene dine. Det kan vÃ¦re et svÃ¦rt nyttig verktÃ¸y i mange tilfeller."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#generere-data",
    "href": "notebooks/spark/pyspark-intro.html#generere-data",
    "title": "Introduksjon til PySpark",
    "section": "Generere data",
    "text": "Generere data\nVi kan begynne med Ã¥ generere en Spark DataFrame med en kolonne som inneholder mÃ¥nedlige datoer for perioden 2000M1-2023M8.\n\n# Genererer mÃ¥nedlige data\ndates_df = spark.range(1).select(\n    explode(\n        sequence(\n            start=expr(\"date '2000-01-01'\"),\n            stop=expr(\"date '2023-08-01'\"),\n            step=expr(\"interval 1 month\"),\n        )\n    ).alias(\"Date\")\n)\ndates_df.show(5)\n\n+----------+\n|      Date|\n+----------+\n|2000-01-01|\n|2000-02-01|\n|2000-03-01|\n|2000-04-01|\n|2000-05-01|\n+----------+\nonly showing top 5 rows\n\n\n\nEn Spark DataFrame er en distribuert samling av data som er organisert inn i kolonner. Siden Spark lar deg distribuere kjÃ¸ringer pÃ¥ flere maskiner, er DataFrames optimalisert for Ã¥ kunne splittes opp slik at de kan brukes pÃ¥ flere maskiner. Med andre er dette ikke det samme som en Pandas dataframe mange kjenner fra fÃ¸r.\nOver genererte vi en datokolonne. For Ã¥ fÃ¥ litt mer data kan vi ogsÃ¥ generere 100 kolonner med tidsseriedata og sÃ¥ printer vi de 2 fÃ¸rste av disse:\n\n# Genererer random walk data\nschema = StructType(\n    [StructField(f\"serie{i:02d}\", DoubleType(), True) for i in range(100)]\n)\n\ndata = [\n    tuple((10 + np.random.normal(0, 1, 100)).cumsum().tolist())\n    for _ in range(284)  # 284 months from 2000-01 to 2023-08\n]\n\ndata_df = spark.createDataFrame(data, schema=schema)\n\ndata_df.select(\"serie00\", \"serie01\").show(5)\n\n+------------------+------------------+\n|           serie00|           serie01|\n+------------------+------------------+\n|10.410703377184355| 21.06318801110079|\n|10.509249410154466|  19.5674295298024|\n| 9.618310122060274|17.635805093465642|\n| 9.691112692298294|18.593842915949082|\n| 9.903675228685067|20.012215769058564|\n+------------------+------------------+\nonly showing top 5 rows\n\n\n\nTil slutt kan vi joine de to datasettene sammen og lage noen kolonner som viser Ã¥r, kvartal og mÃ¥ned. Deretter printer vi ut noen av kolonnene med kommandoen show().\n\n\nCode\n# Legger til row index til DataFrame fÃ¸r join med dates_df\ndata_df = data_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n\n# Joiner de to datasettene\ndf = (\n    dates_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n    .join(data_df, \"row_index\")\n    .drop(\"row_index\")\n)\n\n# Legger til Ã¥r, kvartal og mnd\ndf = df.withColumn(\"Year\", date_format(df.Date, \"yyyy\"))\ndf = df.withColumn(\"Quarter\", expr(\"quarter(Date)\"))\ndf = df.withColumn(\"Month\", date_format(df.Date, \"MM\"))\n\ndf.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows\n\n\n\nOg med det har vi noe data vi kan jobbe med i resten av notebooken."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#skrive-til-parquet",
    "href": "notebooks/spark/pyspark-intro.html#skrive-til-parquet",
    "title": "Introduksjon til PySpark",
    "section": "Skrive til Parquet",
    "text": "Skrive til Parquet\nPySpark tilbyr mange opsjoner ved skriving til parquet-filer som vi kanskje ikke er vant til Ã¥ forholde oss til med enklere rammeverk som Pandas. Den enkleste mÃ¥ten Ã¥ skrive ut en fil er som fÃ¸lger:\ndf.write.parquet(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp/timeseries.parquet\"\n)\nDette vil fungere hvis filen ikke finnes fra fÃ¸r. Hvis den finnes fra fÃ¸r sÃ¥ vil den feile. Grunnen er at vi ikke har spesifisert hva vi Ã¸nsker at den skal gjÃ¸re. Vi kan velge mellom overwrite, append, ignore eller errorifexists. Sistnevnte er ogsÃ¥ default-oppfÃ¸rsel hvis du ikke ber den gjÃ¸re noe annet.\nUnder bruker vi opsjonen overwrite, det vil si at den skriver over en evt eksisterende fil med samme navn.\n\ndf.write.mode(\"overwrite\").parquet(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp/timeseries.parquet\"\n)\n\nVi kan inspisere hva som ble skrevet ved Ã¥ liste ut innholder i bÃ¸tta.\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/temp/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp/',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/_SUCCESS',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/part-00000-b32e7299-0590-4b31-bcc2-dc3d58725529-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/part-00001-b32e7299-0590-4b31-bcc2-dc3d58725529-c000.snappy.parquet']\n\n\nHvis denne parquet-filen hadde vÃ¦rt partisjonert etter en kolonne, sÃ¥ ville det vÃ¦rt egne undermapper med navnestruktur column_name=value som indikerte hva filen er partisjonert pÃ¥. Siden vi her bruker en maskin og har et lite datasett, valgte Spark Ã¥ ikke partisjonere."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#lese-inn-parquet",
    "href": "notebooks/spark/pyspark-intro.html#lese-inn-parquet",
    "title": "Introduksjon til PySpark",
    "section": "Lese inn Parquet",
    "text": "Lese inn Parquet\nApache Spark kan lese inn flere parquet-filer samtidig. Syntaxen er like enkel som den for Ã¥ skrive ut.\n\ndf_ts = spark.read.parquet(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp/timeseries.parquet\"\n)\ndf_ts.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie66\", \"serie55\").show(5)\n\n+----------+----+-------+-----+-----------------+-----------------+\n|      Date|Year|Quarter|Month|          serie66|          serie55|\n+----------+----+-------+-----+-----------------+-----------------+\n|2000-01-01|2000|      1|   01|670.2679830025959|562.4312808525777|\n|2000-02-01|2000|      1|   02|675.4233411662802|562.5168447360121|\n|2000-03-01|2000|      1|   03|687.3412458214908|568.6203957584232|\n|2000-04-01|2000|      2|   04|673.1128047244955|557.4633871253379|\n|2000-05-01|2000|      2|   05| 667.513406101114|561.7766450346327|\n+----------+----+-------+-----+-----------------+-----------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#pyspark-og-sql",
    "href": "notebooks/spark/pyspark-intro.html#pyspark-og-sql",
    "title": "Introduksjon til PySpark",
    "section": "PySpark og SQL",
    "text": "PySpark og SQL\nDu kan ogsÃ¥ skrive SQL med Spark. For Ã¥ skrive SQL mÃ¥ vi fÃ¸rst lage et temporary view. Under kaller vi viewt for tidsserie.\n\ndf.createOrReplaceTempView(\"tidsserie\")\n\nVi kan deretter skrive en SQL-statement som vi Ã¸nsker Ã¥ kjÃ¸re pÃ¥ viewet:\n\nquery = \"SELECT * FROM tidsserie WHERE Year = 2010\"\n\nDeretter kan vi bruke det til Ã¥ filtrere datasettet:\n\nresult_df = spark.sql(query)\nresult_df.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2010-01-01|2010|      1|   01| 11.26910423907778|21.730128215168268|\n|2010-02-01|2010|      1|   02| 8.722783282690738| 17.46851086792347|\n|2010-03-01|2010|      1|   03|10.376873608348605|20.109386343182802|\n|2010-04-01|2010|      2|   04|11.459959305590926|21.995141825523866|\n|2010-05-01|2010|      2|   05|10.441456792180572| 21.25096473981906|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#aggregering",
    "href": "notebooks/spark/pyspark-intro.html#aggregering",
    "title": "Introduksjon til PySpark",
    "section": "Aggregering",
    "text": "Aggregering\nvi kan aggregere opp enten med PySpark-syntax eller SQL-syntax. Under viser vi begge:\n\nfrom pyspark.sql import functions as F\n\n# Assuming df_ts is your DataFrame\naggregated_df = df_ts.groupBy(\"Quarter\").agg(F.sum(\"serie00\").alias(\"Sum\"),\n                                             F.avg(\"serie00\").alias(\"Average\"),\n                                             F.max(\"serie00\").alias(\"Maximum\"))\n\n# Show the result\naggregated_df.show()\n\n+-------+------------------+------------------+------------------+\n|Quarter|               Sum|           Average|           Maximum|\n+-------+------------------+------------------+------------------+\n|      1|363.95869885234185|10.109963857009497|11.829453550532005|\n|      3|365.68324879453405| 10.15786802207039|12.233378837422391|\n|      4| 342.2334082209804|10.065688477087658|12.210138970053695|\n|      2|  361.991445506568|10.055317930738001|12.276030776082463|\n+-------+------------------+------------------+------------------+\n\n\n\nLa oss gjÃ¸re det samme med SQL, men grupperer etter to variabler og sorterer output etterpÃ¥.\n\n# Assuming df_ts is your DataFrame\ndf_ts.createOrReplaceTempView(\"temp_table\")\n\n# Now you can run a SQL query\nquery = \"\"\"\n    SELECT\n        Year,\n        Quarter,\n        SUM(serie00) AS Sum,\n        AVG(serie00) AS Average,\n        MAX(serie00) AS Maximum\n    FROM \n        temp_table\n    GROUP BY \n        Year, Quarter\n    ORDER BY\n        YEAR, QUARTER\n\"\"\"\n\naggregated_df_sql = spark.sql(query)\n\n# Show the result\naggregated_df_sql.show(10)\n\n+----+-------+------------------+------------------+------------------+\n|Year|Quarter|               Sum|           Average|           Maximum|\n+----+-------+------------------+------------------+------------------+\n|2000|      1|31.323050432219453|10.441016810739818|11.118293927071951|\n|2000|      2|28.911192473027377| 9.637064157675793| 9.900977410685329|\n|2000|      3|33.670797229797415|11.223599076599138|12.233378837422391|\n|2000|      4|28.094793356286914| 9.364931118762305| 10.32000478359274|\n|2001|      1|31.636678535169537|10.545559511723178|11.367822302191831|\n|2001|      2|29.629770128521507| 9.876590042840503|11.135215930381191|\n|2001|      3| 30.75408440118315| 10.25136146706105|10.723803326978505|\n|2001|      4|30.361048932627902|  10.1203496442093|10.368365984482093|\n|2002|      1|31.184163218551227|10.394721072850409|10.550579652234951|\n|2002|      2|29.128978392451202|   9.7096594641504|10.186365745367246|\n+----+-------+------------------+------------------+------------------+\nonly showing top 10 rows"
  },
  {
    "objectID": "blog/posts/2025-04-07-altinn3-til-isee/index.html",
    "href": "blog/posts/2025-04-07-altinn3-til-isee/index.html",
    "title": "Fra Altinn 3 til ISEE",
    "section": "",
    "text": "Her skal vi gÃ¥ gjennom det du trenger for Ã¥ fÃ¥ i gang lasting av Altinn 3 skjemaer til ISEE, steg for steg.\nMÃ¥ten vi gjÃ¸r det ved Ã¥ gÃ¥ gjennom hvordan prosessen er satt opp for hagebruksundersÃ¸kelsen, sÃ¥ har du et konkret eksempel Ã¥ se pÃ¥ underveis. Det kan stort sett kopieres sÃ¥ lenge du endrer filstier og navn fra hagebruk til Ã¥ passe egen undersÃ¸kelse\nVi kommer til Ã¥ vÃ¦re innom Transfer service og Kildomaten, men det er ikke krav om forkunnskaper for Ã¥ komme gjennom denne veiledningen.\nNÃ¥r prosessen er satt opp for en undersÃ¸kelse sÃ¥ vil det i de fleste tilfeller ikke vÃ¦re behov for Ã¥ endre pÃ¥ det, sÃ¥ det er vanligvis en engangsjobb Ã¥ fÃ¥ pÃ¥ plass.\nEksemplene som vises er for Ã¥ laste produksjonsdata, dersom du Ã¸nsker Ã¥ sende ned skjemaer fra test-altinn (tt02) til DB1T kan du se pÃ¥ tegningen av den overordnede flyten.\nProsessen er identisk, men filstiene er ulike. Se skjermbildet for flyten nedenfor for eksempel pÃ¥ oppsettet.\nDet anbefales Ã¥ sette opp test-lÃ¸pet, sÃ¥ du slipper Ã¥ lage rot i DB1P."
  },
  {
    "objectID": "blog/posts/2025-04-07-altinn3-til-isee/index.html#overblikk-over-prosessen.",
    "href": "blog/posts/2025-04-07-altinn3-til-isee/index.html#overblikk-over-prosessen.",
    "title": "Fra Altinn 3 til ISEE",
    "section": "Overblikk over prosessen.",
    "text": "Overblikk over prosessen.\nI prosessen er det en del overfÃ¸ringer som mÃ¥ settes opp, men de kjÃ¸res automatisk nÃ¥r det fÃ¸rst er i gang. Stegene vi skal gjennom er:\n1. Transfer service fra datamottak til din kildebÃ¸tte\n2. Kildomaten\n3. Transfer service fra din produktbÃ¸tte til frasky-bÃ¸tten\n4. Transfer service fra din frasky-bÃ¸tte til bakken.\n5. Opprette mappestruktur pÃ¥ stammen\n6. MoveIT og lasting til ISEE\nTestlÃ¸pet krever at du gjÃ¸r alle stegene dobbelt opp. Ã‰n gang for prod, Ã©n gang for test. Det er viktig Ã¥ ikke blande disse lÃ¸pene i hverandre, og at filstiene viser til en prod-mappe eller test-mappe. Se flytdiagrammet nedenfor for konkrete filstier.\n\n\n\n\n\n\nOverfÃ¸ring mellom suv-altinn-data-t og prod\n\n\n\nFor Ã¥ sette opp testlÃ¸pet kan det forelÃ¸pig vÃ¦re nÃ¸dvendig Ã¥ kontakte kundeservice for Ã¥ Ã¥pne pÃ¥ tilganger, slik at du kan overfÃ¸re filer fra suv-altinn-data-t, datamottaket hvor skjemaer fra test-altinn / tt02 havner, til prod bÃ¸tten din.\nTil vanlig sÃ¥ har du kun tilgang til suv-altinn-data-t fra test-teamet, som fungerer pÃ¥ Dapla, men siden test-teamet ikke kan overfÃ¸re filer til bakken sÃ¥ er det ikke mulig Ã¥ teste innlasting til ISEE med skjemaer fra tt02 uten Ã¥ gÃ¥ via prod bÃ¸tten. Derfor trenger du at det Ã¥pnes for dette.\n\n\n\nFlytdiagram for prosessen\n\n\n\nFlytdiagram for prosessen\n\n\nFor informasjon om hvordan mappestrukturen i bÃ¸ttene dine skal vÃ¦re, se navnestandarden."
  },
  {
    "objectID": "blog/posts/2025-04-07-altinn3-til-isee/index.html#transfer-service-fra-datamottak-til-din-kildebÃ¸tte.",
    "href": "blog/posts/2025-04-07-altinn3-til-isee/index.html#transfer-service-fra-datamottak-til-din-kildebÃ¸tte.",
    "title": "Fra Altinn 3 til ISEE",
    "section": "Transfer service fra datamottak til din kildebÃ¸tte.",
    "text": "Transfer service fra datamottak til din kildebÃ¸tte.\nDet fÃ¸rste du skal gjÃ¸re er Ã¥ sette opp transfer service fra datamottaket til din kildebÃ¸tte. Du kommer til Ã¥ vÃ¦re godt kjent med Ã¥ sette opp transfer service nÃ¥r vi er ferdige med dette, og det er en del detaljer som mÃ¥ stemme. SÃ¥ det er bare Ã¥ holde hodet kaldt, dobbeltsjekke at alt stemmer, prÃ¸ve seg frem og be om hjelp hvis du stÃ¥r fast.\nSiden det fÃ¸rste steget involverer kildebÃ¸tta sÃ¥ trenger du Ã¥ vÃ¦re data-admin pÃ¥ teamet ditt (du kan sjekke rollene dine i dapla ctrl) og sÃ¸ke om midlertidig tilgang via just in time (JIT) tjenesten.\nFor Ã¥ komme seg til vinduet hvor du kan sette opp en transfer service gÃ¥r du fÃ¸rst inn pÃ¥ denne lenken: https://console.cloud.google.com/\nDeretter skal du klikke pÃ¥ en knapp Ã¸verst i skjermbildet pÃ¥ venstre side, til hÃ¸yre for GoogleCloud logoen og fÃ¥ opp menyen hvor du velger hvilket prosjekt du skal jobbe med\nHer skal du velge ditt team, du kan se teamnavnet i dapla-ctrl. Du skal inn pÃ¥ prosjektet som har ditt-teamnavn-kilde-p i navnet sitt. Hvis du ikke ser prosjektet ditt, sjekk at du har valgt ssb.no som organisasjon under â€œSelect a resourceâ€.\n\n\n\nI dette tilfellet er det primaer-j-skjema-kilde-p som er riktig valg.\n\n\nNÃ¥r du er inne pÃ¥ prosjektet ditt kan du klikke i sÃ¸kefeltet pÃ¥ toppen av siden, sÃ¸ke pÃ¥ â€œStorage transferâ€ og velge den.\nNÃ¥ skal vi inn pÃ¥ â€œTransfer jobsâ€ og klikke pÃ¥ â€œCREATE TRANSFER JOBâ€.\nNoter deg hvordan du fant denne menyen, for litt senere i prosessen skal vi inn pÃ¥ et annet prosjekt og sette opp flere transfer jobs.\nDatamottaket ligger i team SUV sitt dapla team som heter â€œsuv-altinn-data-pâ€, og du kan finne skjemaene i bÃ¸tter med navnestrukturen ssb-suv-altinn-raXXXX-01-prod. Merk at 01 viser til versjonen av skjema, sÃ¥ hvis du fÃ¥r en ny versjon av skjemaet ditt vil den ha et annet versjonsnummer.\nHagebruksundersÃ¸kelsen har RA-nummeret 0571, sÃ¥ datamottaket sitt bÃ¸ttenavn vil i dette tilfellet vÃ¦re ssb-suv-altinn-ra0571-01-prod.\nMappestrukturen i datamottaket er slik at det fÃ¸rst er en mappe for Ã¥ret skjemaet ble mottatt, deretter for mÃ¥neden, deretter for dagen, og her finner du hver enkelt innsending. I transfer job henviser vi bare til den Ã¸verste mappen, den fÃ¸rer automatisk med seg alt som ligger lenger â€œnedeâ€ i mappestrukturen.\nSe pÃ¥ bildene for Ã¥ se hvordan filstier og innstillingene for jobben skal se ut.\nDet er lurt Ã¥ skrive tydelig i â€œDescriptionâ€ feltet fÃ¸r du trykker â€œCREATEâ€ hvilken undersÃ¸kelse det gjelder og hva transfer jobben gjÃ¸r. Det sparer andre i teamet (og deg selv) forvirring senere.\nEt lite tips er Ã¥ la denne jobben kjÃ¸re ofte, siden det vil gjÃ¸re at skjemaene behandles fortlÃ¸pende i kildomaten istedenfor Ã¥ komme i store overfÃ¸ringer. Andre overfÃ¸ringer trenger ikke kjÃ¸re mer enn en gang om dagen.\nI denne jobben skal ikke filer slettes.\n\nGet startedChoose a sourceChoose a destinationChoose when to run jobChoose settings"
  },
  {
    "objectID": "blog/posts/2025-04-07-altinn3-til-isee/index.html#kildomaten",
    "href": "blog/posts/2025-04-07-altinn3-til-isee/index.html#kildomaten",
    "title": "Fra Altinn 3 til ISEE",
    "section": "Kildomaten",
    "text": "Kildomaten\nNÃ¥ skal vi gÃ¥ lÃ¸s pÃ¥ den mest tekniske delen av oppsettet. Her blir det nok mye prÃ¸ving og feiling for Ã¥ fÃ¥ alt til Ã¥ fungere, sÃ¥ ikke mist motet.\nKildomaten er en tjeneste som automatiserer bearbeiding av data som kommer inn i kildebÃ¸tta. Etter at den er satt opp sÃ¥ vil den automatisk bearbeide filer som havner i mappen den er stilt inn til Ã¥ fÃ¸lge med pÃ¥. Den vil enkelt forklart bearbeide hvert enkelt skjema hver for seg, og sende videre til produkt bÃ¸tta.\nFor mer informasjon om kildomaten kan du se veiledning for kildomaten her.\nI fÃ¸rste omgang holder vi det enkelt og setter opp rammen som vi senere skal fylle ut.\nDe to filene nedenfor skal opprettes og plasseres i mappen dapla-team-iac/automation/source-data/dapla-team-prod/hagebruk/\n\n\nconfig.yaml\n\nfolder_prefix: hagebruk/altinn\nmemory_size: 1\n\n\n\nprocess_source_data.py\n\nimport dapla as dp\nfrom altinn import isee_transform, create_isee_filename\n\ntags = [\"SkjemaData\", \"Kontakt\", \"Brukeropplevelse\"]\n\nmapping = {\n    \"Altinn3 navn\": \"ISEE navn\"\n}\n\ndef main(source_file):\n    if source_file.endswith(\".xml\"):\n        fil = isee_transform(source_file, mapping, tag_list=tags) # transformerer XML'ene fra kildedatabÃ¸tta til pandas dataframe med riktig ISEE-feltnavn\n        fil[\"FELTNAVN\"] = fil[\"FELTNAVN\"].str[:25].str.upper()\n        if \"altinn/test/\" in source_file:\n            dp.write_pandas(df = fil, gcs_path = f\"ssb-primaer-j-skjema-data-produkt-prod/hagebruk/temp/test/altinn/{create_isee_filename(source_file)}\", file_format=\"csv\", index=False, sep=\";\")\n        elif \"altinn/prod/\" in source_file:\n            dp.write_pandas(df = fil, gcs_path = f\"ssb-primaer-j-skjema-data-produkt-prod/hagebruk/inndata/altinn/{create_isee_filename(source_file)}\", file_format=\"csv\", index=False, sep=\";\")\n        else: \n            raise ValueError (f'source_filen sitt navn inneholder verken altinn/test/ eller altinn/prod/: {source_file}')\n\nFilstiene i process_source_data.py mÃ¥ du endre nÃ¥, mappingen kan du utsette til du er ferdig med Ã¥ sette opp resten av overfÃ¸ringene.\nDet er vanskelig Ã¥ sjekke om mappingen din stemmer uten Ã¥ ha forsÃ¸kt innlasting til ISEE, sÃ¥ det kan vÃ¦re greit Ã¥ ordne resten av lÃ¸pet fÃ¸rst.\nMapping brukes for Ã¥ omkode nytt variabelnavn fra Altinn 3 til ISEE feltnavn og er nÃ¸dvendig for at dataene dine skal bli synlige i ISEE.\nDet er viktig at filen som lages av kildomaten ikke har feltnavn som er lengre enn 25 tegn, er en csv fil og at det er semikolon som er skilletegnet. Dette er tatt hÃ¸yde for i malen over."
  },
  {
    "objectID": "blog/posts/2025-04-07-altinn3-til-isee/index.html#transfer-service-fra-din-produkt-bÃ¸tte-til-frasky-bÃ¸tten",
    "href": "blog/posts/2025-04-07-altinn3-til-isee/index.html#transfer-service-fra-din-produkt-bÃ¸tte-til-frasky-bÃ¸tten",
    "title": "Fra Altinn 3 til ISEE",
    "section": "Transfer service fra din produkt bÃ¸tte til frasky bÃ¸tten",
    "text": "Transfer service fra din produkt bÃ¸tte til frasky bÃ¸tten\nHer skal vi starte med Ã¥ flytte filene som kildomaten vÃ¥r lager til bakken. FÃ¸r filene kan sendes til bakken, sÃ¥ skal de innom frasky-bÃ¸tta. Det betyr at flyttingen ned til bakken involverer to transfer jobs.\nDisse jobbene trenger ikke kjÃ¸re mer enn Ã©n gang om dagen, men pass pÃ¥ at jobben som flytter fra produkt til frasky kjÃ¸rer fÃ¸r jobben som flytter filer til bakken. Hva du kan sette pÃ¥ tidspunkt er vist pÃ¥ â€œChoose when to run jobâ€ bildene for hver jobb.\nFÃ¸rst skal du skifte hvilket prosjekt du jobber i. NÃ¥ skal vi vÃ¦re i teamet som heter ditt-teamnavn-p, altsÃ¥ det som ikke har kilde i navnet sitt. I dette eksempelet er riktig team primaer-j-skjema-p.\nDeretter skal transfer jobs settes opp pÃ¥ samme mÃ¥te, se pÃ¥ skjermbildene for Ã¥ se hvordan det skal se ut.\nOBS! Det er viktig at filene slettes fra kilden etter flytting. Hvis det ikke slettes, sÃ¥ overfÃ¸res samme skjema flere ganger og skaper enormt mange dubletter i ISEE.\n\nGet startedChoose a sourceChoose a destinationChoose when to run jobChoose settings"
  },
  {
    "objectID": "blog/posts/2025-04-07-altinn3-til-isee/index.html#transfer-service-fra-din-frasky-bÃ¸tte-til-bakken",
    "href": "blog/posts/2025-04-07-altinn3-til-isee/index.html#transfer-service-fra-din-frasky-bÃ¸tte-til-bakken",
    "title": "Fra Altinn 3 til ISEE",
    "section": "Transfer service fra din frasky bÃ¸tte til bakken",
    "text": "Transfer service fra din frasky bÃ¸tte til bakken\nI dette steget skal du fortsatt vÃ¦re i â€œditt-teamnavn-pâ€ prosjektet.\nNÃ¥ flytter vi filene ned til bakken. MÃ¥ten mottaket pÃ¥ bakken er lagt opp er at det finnes en mappe pÃ¥ linux for ditt dapla team under omrÃ¥det cloud_sync.\nDet vil ha filstien cloud_sync/team-navn/â€¦\nOBS! Det er viktig at filene slettes fra kilden etter flytting. Hvis det ikke slettes, sÃ¥ overfÃ¸res samme skjema flere ganger og skaper enormt mange dubletter i ISEE.\nMÃ¥ten du skal sette opp transfer service kan du se her:\n\nGet startedChoose a sourceChoose a destinationChoose when to run jobChoose settings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNÃ¥r du har satt opp transfer service sÃ¥ skal du ned pÃ¥ bakken og opprette noen mapper pÃ¥ linux. Samme sted som stammene ligger sÃ¥ finnes ogsÃ¥ et omrÃ¥de som heter cloud_sync, det er dit vi skal nÃ¥.\nDitt dapla team vil ha en egen mappe i cloud_sync som du skal finne.\n\n\nPÃ¥ bakken er det ett mottak for kildedata (mappen som heter kilde) og ett for annen data (mappen som heter standard). Det er â€œstandardâ€ vi skal bruke. Det vil si at vi skal forholde oss til cloud_sync/ditt-teamnavn/standard/frasky.\nI den mappen skal du opprette mappen â€œaltinnâ€ og den skal inneholde to undermapper, â€œprodâ€ og â€œtestâ€.\n\n\n\n\n\n\nMappestruktur pÃ¥ cloud_sync"
  },
  {
    "objectID": "blog/posts/2025-04-07-altinn3-til-isee/index.html#opprette-mappestruktur-pÃ¥-stammen",
    "href": "blog/posts/2025-04-07-altinn3-til-isee/index.html#opprette-mappestruktur-pÃ¥-stammen",
    "title": "Fra Altinn 3 til ISEE",
    "section": "Opprette mappestruktur pÃ¥ stammen",
    "text": "Opprette mappestruktur pÃ¥ stammen\n\n\nFor at innlasting til ISEE skal fungere sÃ¥ trengs det noen spesifikke mapper pÃ¥ stammen.\nDette er nÃ¸dvendig for at den automatiske lastejobben skal fungere, og sÃ¥ lenge du fÃ¸lger dette oppsettet sÃ¥ burde selve innlastingen gÃ¥ fint.\nI skjermbildet vises det oppsett som inkluderer mappene som trengs for test-lÃ¸pet. Dette lÃ¸pet mÃ¥ settes opp separat fra prod-lÃ¸pet.\nSe skjermbildet med overordnet flyt Ã¸verst pÃ¥ siden for Ã¥ se forskjellen pÃ¥ filstiene.\n\n\n\n\n\n\nMappestruktur pÃ¥ stammen"
  },
  {
    "objectID": "blog/posts/2025-04-07-altinn3-til-isee/index.html#moveit-og-lasting-til-isee",
    "href": "blog/posts/2025-04-07-altinn3-til-isee/index.html#moveit-og-lasting-til-isee",
    "title": "Fra Altinn 3 til ISEE",
    "section": "MoveIT og lasting til ISEE",
    "text": "MoveIT og lasting til ISEE\nNÃ¥ som vi har satt opp mappestrukturene som trengs sÃ¥ har vi en positiv overraskelse, det neste steget er enkelt for deg som statistikkansvarlig.\nFÃ¸rst skal du opprette skjemaet i SFU og ISEE. Det skal hete RA-XXXXA3, hvor XXXX er ditt RA-nummer.\nDeretter skal du sende en mail til kundeservice. I den skal du: - si at du trenger en MoveIt overfÃ¸ring fra cloudsync/standard/undersÃ¸kelse/altinn/prod/ og til din stamme sin wk24/datafangst/gSdv/ mappe. - at du trenger opplasting fra datafangst/gSdv mappen din til ISEE.\nDu kan ta utgangspunkt i malen nedenfor. Hvis du ikke setter opp test-lÃ¸pet sÃ¥ kan du fjerne de bitene.\n\nHei!\nTrenger to moveit jobber og innlasting til ISEE fra stammen.\nJeg skal overfÃ¸re RA-XXXXA3 til ISEE, delregisternummer 123456\nMoveIt som flytter fra: /ssb/cloud_sync/primaer-j-skjema/standard/frasky/hagebruk/test/altinn/ Til: /ssb/stamme02/jordbruk/sbhagebruk/test/wk24/datafangst/gSdv\nTest jobben burde kjÃ¸re 20 minutter over hver time sÃ¥ det rekker Ã¥ bli med pÃ¥ lastingen til DB1T.\nMoveIt som flytter fra: /ssb/cloud_sync/primaer-j-skjema/standard/frasky/hagebruk/prod/altinn/ Til: /ssb/stamme02/jordbruk/sbhagebruk/wk24/datafangst/gSdv\nBegge MoveIt jobbene mÃ¥ slette filer fra cloud_sync nÃ¥r det er flyttet.\nTrenger ogsÃ¥ at det opprettes lastejobb fra wk24 og test/wk24 til DB1P og DB1T.\n\nOBS! Det er viktig at filene slettes fra kilden etter flytting. Hvis det ikke slettes, sÃ¥ overfÃ¸res samme skjema flere ganger og skaper enormt mange dubletter i ISEE.\nNÃ¥r disse jobbene er satt opp sÃ¥ fÃ¥r du data inn i ISEE!"
  },
  {
    "objectID": "blog/posts/2025-04-07-altinn3-til-isee/index.html#prosessen-er-satt-opp-hva-nÃ¥",
    "href": "blog/posts/2025-04-07-altinn3-til-isee/index.html#prosessen-er-satt-opp-hva-nÃ¥",
    "title": "Fra Altinn 3 til ISEE",
    "section": "Prosessen er satt opp, hva nÃ¥?",
    "text": "Prosessen er satt opp, hva nÃ¥?\nDu burde nÃ¥ fÃ¥ data lastet inn i tabellene ISEE er bygd pÃ¥, som er et godt stykke men likevel er det en del jobb igjen.\nDet er mange detaljer som skal stemme for at dataene som lastes inn skal dukke opp riktig i ISEE. Hvis mappingen ikke stemmer sÃ¥ vil ikke dataene dine bli synlige i ISEE, selv om de er lastet inn i systemet.\nDu mÃ¥ sjekke at kodelister eksisterer for variablene dine, at disse matcher variabelverdiene som kommer fra Altinn 3 og at eventuelle endringer i Altinn 3 skjemaet er hensyntatt i skjemavisningen i ISEE. Ved behov kan du opprette nye kodelister som passer Altinn 3 dataene dine i klass-forvaltning.\nVÃ¦r oppmerksom pÃ¥ at ofte vil noen kodelister som benyttes i Altinn 3 skjemaet skille seg fra det som ble brukt i altinn2, som vil kreve noen endringer i skjemabyggeren.\nDet er mulig Ã¥ abonnere pÃ¥ rapporter fra ISEE hvis du gÃ¥r inn pÃ¥ Administrasjon -&gt; Oversikt datafangst -&gt; Datafangstrapporter -&gt; Legg inn epostadressen din.\nDu kan ogsÃ¥ fÃ¥ varsler om noe skulle gÃ¥ galt underveis i lastingen til ISEE.\nHer er det noen ressurser som kan vÃ¦re til hjelp med Ã¥ ferdigstille opplegget ditt:\nDokumentasjon for ssb-altinn-python pakken:\n\nhttps://statistics-norway.atlassian.net/wiki/spaces/altinnpython/overview\nhttps://statisticsnorway.github.io/ssb-altinn-python/\n\nISEE dokumentasjon:\n\nhttps://statistics-norway.atlassian.net/wiki/spaces/ISEE/pages/3958932070/Brukerdokumentasjon"
  },
  {
    "objectID": "blog/posts/2024-12-16-data-validering-pandera/index.html",
    "href": "blog/posts/2024-12-16-data-validering-pandera/index.html",
    "title": "Intro til Pandera",
    "section": "",
    "text": "Pandera er en python pakke og rammeverk for testing av data - altsÃ¥ datavalidering.\nBegrepet testing kan fÃ¸re til misforstÃ¥elser mellom statistikkere og utviklere: en statistikker vil ofte tenke pÃ¥ testing av data, og utvikler pÃ¥ testing av kode. Sistnevnte omtales som enhetstesting.\nDet finnes flere rammeverk for testing av kode og datavalidering. NÃ¥r det kommer til Python bruker vi i SSB som oftest Pytest pakken for testing av kode, og Pandera eller Pydantic pakkene for datavalidering. Alle disse pakkene stÃ¥r oppfÃ¸rt pÃ¥ godkjentlista i SSB.\nPandera eller Pydantic? Hvem av dem som bÃ¸r benyttes avhenger mest av strukturen pÃ¥ dataene din. Dersom dataene er semi-strukturert (ofte filformater som json og xml) sÃ¥ vil fort Pydantic vÃ¦re mest aktuell, mens er dataene strukturerte (som en dataframe, eller filformat som csv) sÃ¥ vil Pandera vÃ¦re ett mer naturlig valg. Her vil det gis en intro til Pandera. Vel og merke vil innholdet her dreie seg om grunnleggende bruk, samt forskjellige tips og triks i hvordan det kan brukes, og muligens en bonus til slutt. Mer avanserte temaer, som f.eks. hypotesetesting, er ikke med her.\nMen hvorfor Pandera? Og hvorfor validere data? Siste er enkelt Ã¥ besvare og ligger godt integrert i SSBs samfunnsansvar: Vi skal ha god kvalitet i all statistikk, forskning og analyse.\nI den moderniseringsprosessen SSB er i, overgang til Dapla, er det naturlig at dette integreres i kodene vÃ¥re. Det er en anbefaling fra KVAKK anbefaler ogsÃ¥ Ã¥ kontrollere data for hvert trinn. Da er datavalideringspakker som Pandera hÃ¸yst aktuell. I tillegg er det en annen anbefaling fra KVAKK, og en ADR vedtatt i SSB, om at kildekode skal vÃ¦re offentlig tilgjengelig. En eller annen gang skal altsÃ¥ produksjonskoden vÃ¥r bli offentlig tilgjengelig. Dette er kanskje mine personlige meninger rundt det, men jeg vil tro at det vil foreligge en stor forventning der ute om at SSB validerer data i kode. Selv om Pandera er relativt nytt stÃ¸tter den de aller mest brukte dataframe-rammeverkene som er i bruk i SSB, slik som Pandas, Polars, og PySpark."
  },
  {
    "objectID": "blog/posts/2024-12-16-data-validering-pandera/index.html#import-og-testdata",
    "href": "blog/posts/2024-12-16-data-validering-pandera/index.html#import-og-testdata",
    "title": "Intro til Pandera",
    "section": "Import og testdata",
    "text": "Import og testdata\nFÃ¸rst importerer vi noen biblioteker som vi skal benytte. For Ã¥ benytte Pandera pakken mÃ¥ det lastes inn til ett virtuelt miljÃ¸, som vi i SSB benytter ssb-project for;\n\n\nterminal\n\npoetry add pandera\n\nVersjonen av Pandera som benyttes i introduksjonen her er 0.20.4. FÃ¸lgende pakker fÃ¥r jeg importert deretter;\n\nimport uuid\nfrom typing import Dict\nimport pandas as pd\nimport numpy as np\nimport pandera as pan\nfrom pandera.typing import DataFrame, Series\nfrom pandera.errors import SchemaErrors\n\nJeg lager ogsÃ¥ fÃ¸lgende lekedata vi skal ta for oss i eksemplene;\n\nsize = 6\n\nrandom_data = pd.DataFrame({\n    \"id_nr\": [str(uuid.uuid4()) for _ in range(size)],\n    \"lope_id_nr\": [\"L\" + str(1).zfill(4) for _ in range(size)],\n    \"aar\": np.random.choice(['2023', '2024'], size),\n    \"navn\": np.random.choice(['Ola', 'Kari', 'Per', 'Ida'], size),\n    \"produkt\": np.random.choice(['Eple', 'Gulrot', 'Brokkoli'], size),\n    \"salgsverdi\": np.random.randint(1000, 10000, size),\n    \"vekt\": np.random.randint(500, 5000, size)\n})\n\nrandom_data['kostverdi'] = (\n    random_data['salgsverdi'] * 0.75\n).astype(int)\n\nbad_data = pd.DataFrame({\n    \"id_nr\": [\"random-id1\", \"random-id1\", \"random-id2\",\n              \"random-id2\", \"random-id3\"],\n    \"lope_id_nr\": [\"L0001\", \"L0002\", \"L0001\", \"L0001\", \"0001\"],\n    \"aar\": ['2023', '2023', '2024', '2024', '2024Q1'],\n    \"navn\": ['Ola', 'Ola', 'Per', 'Kari', None],\n    \"produkt\": ['Banan', 'Eple', 'Eple', 'Agurk', 'Eple'],\n    \"salgsverdi\": [5000, 4000, 7000, 3000, 50],\n    \"vekt\": [700, 600, 700, 100, 5],\n    \"kostverdi\": [3500, 2500, 5000, 3100, 55],\n})\n\ndata = pd.concat([random_data, bad_data], ignore_index=True)\ndata\n\n\n\n\n\n\n\n\nid_nr\nlope_id_nr\naar\nnavn\nprodukt\nsalgsverdi\nvekt\nkostverdi\n\n\n\n\n0\n9c2a0f28-830f-4ae3-9400-8d992bcaa4b4\nL0001\n2023\nPer\nEple\n9921\n3698\n7440\n\n\n1\n97a1e3cf-c465-49a9-9d58-28810ff5acd6\nL0001\n2023\nOla\nGulrot\n8806\n994\n6604\n\n\n2\nce06b21d-c416-4a12-9180-71a36a891fb4\nL0001\n2024\nKari\nBrokkoli\n6387\n4145\n4790\n\n\n3\n43903841-4231-4471-b9a5-c8947ca4c985\nL0001\n2023\nKari\nEple\n2813\n4778\n2109\n\n\n4\n76cf98f9-9e6b-42a3-b1d7-d04816b7a1be\nL0001\n2023\nIda\nBrokkoli\n7053\n3606\n5289\n\n\n5\nf78f7396-2554-4f8a-a171-66682628b6db\nL0001\n2024\nKari\nEple\n3221\n2344\n2415\n\n\n6\nrandom-id1\nL0001\n2023\nOla\nBanan\n5000\n700\n3500\n\n\n7\nrandom-id1\nL0002\n2023\nOla\nEple\n4000\n600\n2500\n\n\n8\nrandom-id2\nL0001\n2024\nPer\nEple\n7000\n700\n5000\n\n\n9\nrandom-id2\nL0001\n2024\nKari\nAgurk\n3000\n100\n3100\n\n\n10\nrandom-id3\n0001\n2024Q1\nNone\nEple\n50\n5\n55\n\n\n\n\n\n\n\nDataframen bestÃ¥r av fÃ¸lgende kolonner:\n\nid_nr: identifiseringsnummer\nlope_nr_id: et slags lÃ¸penummerid\naar: perioden i Ã¥r for gjeldende rad\nnavn: navn pÃ¥ enheten (person eller kunde)\nprodukt: produktet det gjelder - la oss si i en frukt og grÃ¸nt butikk\nsalgsverdi: sluttverdien varen ble solgt for\nvekt: sluttvekten som ble solgt\nkostverdi: kostnaden tilknyttet innkjÃ¸p av produktet eller varen.\n\nDet er elementer her som ikke nÃ¸dvendigvis er fullt realistist med virkeligheten, men sammensetningen av disse kolonnene er mest bygd opp for Ã¥ demonstrere mulighetene og fleksibiliteten ved bruk av pandera."
  },
  {
    "objectID": "blog/posts/2024-12-16-data-validering-pandera/index.html#grunnleggende-bruk---schema",
    "href": "blog/posts/2024-12-16-data-validering-pandera/index.html#grunnleggende-bruk---schema",
    "title": "Intro til Pandera",
    "section": "Grunnleggende bruk - schema",
    "text": "Grunnleggende bruk - schema\nFor Ã¥ ta i bruk pandera mÃ¥ vi definere et schema. Schemaer definerer hvordan dataene forventes at skal se ut, spesielt nÃ¥r det kommer til datatyper.\nMed pandera kan du validere bÃ¥de datatyper og innhold. Det er flere mÃ¥ter Ã¥ definere ett schema pÃ¥, men jeg kommer til Ã¥ vise den anbefalte mÃ¥ten Ã¥ gjÃ¸re det pÃ¥. Den er ikke nÃ¸dvendigvis den enkleste, men den er enkel nok, og har likheter til Pydantic.\nEt schema i pandera defineres som fÃ¸lgende;\n\nclass SchemaValidation1(pan.DataFrameModel):\n    \n    id_nr: Series[str] = pan.Field(unique=True)\n    lope_id_nr: Series[str] = pan.Field(\n        str_startswith='L',\n        str_length={'min_value': 5,\n                    'max_value': 5}\n    )\n    aar: Series[str] = pan.Field(\n        str_length={'min_value': 4,\n                    'max_value': 4}\n    )\n    navn: Series[str] = pan.Field(\n        nullable=False # Default\n    )\n    produkt: Series[str] = pan.Field(\n        isin=['Eple', 'Banan', 'Gulrot', 'Brokkoli']\n    )\n    salgsverdi: Series[int] = pan.Field(ge=1000)\n    vekt: Series[int] = pan.Field(ge=500)\n    kostverdi: Series[int] = pan.Field(gt=700)\n\nSÃ¥ hva er det vi har definert her?\nVi har nÃ¥ definert et eget Objekt, en class, kalt SchemaValidation1, som arver egenskapene til Pandera sitt objekt DataFrameModel. Mer avansert fra objekt og class verden trenger du ikke Ã¥ gjÃ¸re eller kunne her egentlig, sÃ¥ ikke bli skremt med det fÃ¸rste. Deretter definerer vi kolonnene som vi forventer i dette schemaet. Pandera er bygget pÃ¥ typing systemet til Python vel og merke, som enkelt forklart vil si at jeg kan bruke typing-pakkens objekter i definisjonen som han vil bruke til Ã¥ validere for, men det gir ogsÃ¥ muligheten til Ã¥ benytte pythons standardobjekter som str og int i definisjonen. Vi har ogsÃ¥ definert regler tilknyttet hver av disse kolonnene som da vil bli validert sammen med datatypene.\n\nid_nr er en Serie (kolonner i pandas dataframe er av datatypen pandas serie) med forventet datatype string (str). Regler som er satt er at innholdet er unikt, altsÃ¥ ingen duplikater i de verdiene som ligger i kolonnen.\nlope_id_nr er ogsÃ¥ forventet datatype string. Den har 2 regler; at alle verdier starter med â€˜Lâ€™, og at teksten er minimum og maksimum 5 karakterer lang.\naar er forventet Ã¥ vÃ¦re string, med regel om at den er 4 tegn lang.\nnavn er forventet Ã¥ vÃ¦re string, med regel om at det ikke skal vÃ¦re noen manglende verdier (missing values). Dette er standard for alle regler, sÃ¥ det er ikke nÃ¸dvendig Ã¥ notere, men for demonstrasjonens skyld sÃ¥ gjorde jeg det her.\nprodukt er forventet Ã¥ vÃ¦re string, med regler om at innholdet er blant verdiene i en gitt liste. I dette tilfellet Eple, Banan, Gulrot, Brokkoli. Kanskje er dette varene butikken selger og har i sortimentet sitt.\nsalgsverdi er forventet Ã¥ vÃ¦re en integer (int). Regel som er satt her er at verdiene er stÃ¸rre eller lik 1000.\nvekt er forventet Ã¥ vÃ¦re en integer. Regel som er satt her er at verdiene er stÃ¸rre eller lik 500.\nkostverdi er forventet Ã¥ vÃ¦re en integer. Regel som er satt her er at verdiene er stÃ¸rre enn 700.\n\nOkei, da har vi definert schemaet. Vi skal bygge videre pÃ¥ dette snart. Det finnes mange flere innebygde valideringsregler enn de vi benytter her, og man mÃ¥ inn i dokumentasjonen til Pandera for Ã¥ se om noe kan passe deg og ditt behov der, men her demonstrerer vi hvertfall noen som sikkert kommer til Ã¥ bli brukt.\nFor Ã¥ utfÃ¸re valideringen gjÃ¸r vi fÃ¸lgende;\n\ntry:\n    valresult = SchemaValidation1.validate(data, lazy=True)\nexcept SchemaErrors as error:\n    # Rapport av feil utslag i dataframe\n    valresult = error.failure_cases\n    # Dataframe som ble sendt inn\n    errdata = error.data\n    # Antall feil utslag\n    num_errors = error.error_counts\n    # Rapportmeilding av feil utslag i dict\n    error_message = error.message\n\nvalresult\n\n\n\n\n\n\n\n\nschema_context\ncolumn\ncheck\ncheck_number\nfailure_case\nindex\n\n\n\n\n0\nColumn\nid_nr\nfield_uniqueness\nNone\nrandom-id1\n6\n\n\n1\nColumn\nid_nr\nfield_uniqueness\nNone\nrandom-id1\n7\n\n\n2\nColumn\nid_nr\nfield_uniqueness\nNone\nrandom-id2\n8\n\n\n3\nColumn\nid_nr\nfield_uniqueness\nNone\nrandom-id2\n9\n\n\n4\nColumn\nlope_id_nr\nstr_length(5, 5)\n0\n0001\n10\n\n\n5\nColumn\nlope_id_nr\nstr_startswith('L')\n1\n0001\n10\n\n\n6\nColumn\naar\nstr_length(4, 4)\n0\n2024Q1\n10\n\n\n7\nColumn\nnavn\nnot_nullable\nNone\nNone\n10\n\n\n8\nColumn\nprodukt\nisin(['Eple', 'Banan', 'Gulrot', 'Brokkoli'])\n0\nAgurk\n9\n\n\n9\nColumn\nsalgsverdi\ngreater_than_or_equal_to(1000)\n0\n50\n10\n\n\n10\nColumn\nvekt\ngreater_than_or_equal_to(500)\n0\n100\n9\n\n\n11\nColumn\nvekt\ngreater_than_or_equal_to(500)\n0\n5\n10\n\n\n12\nColumn\nkostverdi\ngreater_than(700)\n0\n55\n10\n\n\n\n\n\n\n\nObjektet SchemaValidation1 har en metode validate som vi kan sende inn dataframen som skal valideres opp mot schemaet(som vi arvet fra pandera DataFrameModel objektet). Jeg har satt lazy til True her fordi jeg vil at han skal validere alt og ikke stoppe ved fÃ¸rste feil han finner. Dersom valideringen feiler har pandera et error objekt SchemaErrors hvor flere nyttige rapporter blir lagd tilgjengelig for oss. Du kan selv legge med flere av dem, men her tar vi for oss dataframen med alle feilmeldingene som dukker opp. Dersom valideringen gikk bra vil du fÃ¥ dataframen du sendte inn i retur.\nRapporten vi har fÃ¥tt ut nÃ¥ i dataframen valresult har vi flere utslag pÃ¥. Kolonnen id_nr finnes det duplikater blant annet. Kolonnen lope_id_nr er det funnes en som har slÃ¥tt ut i begge definerte reglene som nevnt tidligere. osv, osv. Denne rapporten har vi kanskje et potensial for Ã¥ utnytte videre? Men det fÃ¥r vÃ¦re opp til den enkelte."
  },
  {
    "objectID": "blog/posts/2024-12-16-data-validering-pandera/index.html#behov-for-flere-kontroller",
    "href": "blog/posts/2024-12-16-data-validering-pandera/index.html#behov-for-flere-kontroller",
    "title": "Intro til Pandera",
    "section": "Behov for flere kontroller",
    "text": "Behov for flere kontroller\nDersom de innebygde mulighetene for validering ikke strekker til kan man definere reglene selv ved Ã¥ definere egne metoder med tilhÃ¸rende decorator (alfakrÃ¸ll over metoden). Under her definerer jeg SchemaValidation2 som sett bort ifra de nye metodene er nesten helt identisk med SchemaValidation1. Forskjellen er at nÃ¥ har kolonnen id_nr kun regelen om at den skal ikke ha manglende verdier i stedet for at det skal unike verdier.\n\nclass SchemaValidation2(pan.DataFrameModel):\n    \n    id_nr: Series[str] = pan.Field(nullable=False) # Default\n    lope_id_nr: Series[str] = pan.Field(\n        str_startswith='L',\n        str_length={'min_value': 5,\n                    'max_value': 5}\n    )\n    aar: Series[str] = pan.Field(\n        str_length={'min_value': 4,\n                    'max_value': 4}\n    )\n    navn: Series[str] = pan.Field(nullable=False) # Default\n    produkt: Series[str] = pan.Field(\n        isin=['Eple', 'Banan', 'Gulrot', 'Brokkoli']\n    )\n    salgsverdi: Series[int] = pan.Field(ge=1000)\n    vekt: Series[int] = pan.Field(ge=500)\n    kostverdi: Series[int] = pan.Field(gt=700)\n\n    # Sjekke at kolonne aar er tekst med tall i seg\n    @pan.check(\"aar\",\n               # Valgfritt, men gir eget navn til regelen enn metodenavnet\n               name=\"str_isdigits\",\n               # Valgfritt, men her kan man styre feilmeldingen\n               error=\"str_not_digits\")\n    def check_isdigits(cls, s: Series[str]) -&gt; Series[bool]:\n        return s.str.isdigit()\n\n    # En metode kan sjekke flere kolonner,\n    # her sjekker vi bÃ¥de kostverdi og salgsverdi.\n    # Validerer at Bananer har bÃ¥de hÃ¸yere\n    # salgsverdi og kostverdi enn Epler\n    @pan.check(\"kostverdi\", \"salgsverdi\",\n               groupby=\"produkt\",\n               name=\"check_epler_bananer\")\n    def check_groupby(cls, grouped_value: Dict[str, Series[int]]) -&gt; bool:\n        return grouped_value[\"Eple\"].sum() &lt; grouped_value[\"Banan\"].sum()\n\n    # Trenger du Ã¥ sjekke mer enn bare en kolonne av gangen?\n    # f.eks. at forholde mellom flere kolonner\n    # har en bestemt regel Ã¥ fÃ¸lge?\n    # Her sjekkes at kombinasjonen for kolonnene\n    # id_nr og lope_id_nr er unike\n    @pan.dataframe_check\n    def unique_combo_idnr_lopeidnr(cls, df: pd.DataFrame) -&gt; Series[bool]:\n        df2 = df.copy()\n        df3 = (\n            df2\n            .groupby(['id_nr', 'lope_id_nr'])\n            .agg({'aar': 'count'})\n            .rename(columns={'aar': 'duplikater'}) == 1\n        ).reset_index()\n        df2 = df2.merge(df3,\n                        on=['id_nr', 'lope_id_nr'],\n                        how='left')\n        return df2['duplikater']\n\nSchemaet SchemaValidation2 har som vi ser nÃ¥ 3 metoder;\n\ncheck_isdigits som sjekker at teksten faktisk kun inneholder tall. Her sjekkes kun kolonnen aar.\ncheck_groupby som grupperer verdiene i kolonnen produkt. Det sjekkes her for kolonnene kostverdi og salgsverdi. Den sjekker at summen av bananer er hÃ¸yere enn summen av epler (for Ã¥ gjÃ¸re noe enkelt og irrelevant).\nde 2 fÃ¸rste sjekkene kan kun jobbe med en kolonne av gangen, ev. en groupby pÃ¥ en annen kolonne med fokus pÃ¥ de gjeldende kolonnene en har tenkt Ã¥ sjekke for. Den tredje siste sjekken er litt annerledes, for de andre sjekkene har benyttet decoratoren check, mens den siste har dataframe_check. Dette vil si at hele dataframen sendes inn, og her vil du ha full fleksibilitet til Ã¥ sjekke det du mÃ¥tte Ã¸nske pÃ¥ tvers av alle kolonner. Viktigste er at det returneres en serie(kolonne) av boolske verdier (True/False). I denne siste sjekken unique_combo_idnr_lopeidnr sjekkes det at kombinasjonen av kolonnene id_nr og lope_id_nr er unike i dataframen.\n\nIgjen kan dataene valideres;\n\ntry:\n    valresult = SchemaValidation2.validate(data, lazy=True)\nexcept SchemaErrors as error:\n    valresult = error.failure_cases\n\nvalresult\n\n\n\n\n\n\n\n\nschema_context\ncolumn\ncheck\ncheck_number\nfailure_case\nindex\n\n\n\n\n14\nDataFrameSchema\nlope_id_nr\nunique_combo_idnr_lopeidnr\n0\nL0001\n8\n\n\n15\nDataFrameSchema\nlope_id_nr\nunique_combo_idnr_lopeidnr\n0\nL0001\n9\n\n\n26\nDataFrameSchema\nkostverdi\nunique_combo_idnr_lopeidnr\n0\n5000\n8\n\n\n25\nDataFrameSchema\nvekt\nunique_combo_idnr_lopeidnr\n0\n100\n9\n\n\n24\nDataFrameSchema\nvekt\nunique_combo_idnr_lopeidnr\n0\n700\n8\n\n\n23\nDataFrameSchema\nsalgsverdi\nunique_combo_idnr_lopeidnr\n0\n3000\n9\n\n\n22\nDataFrameSchema\nsalgsverdi\nunique_combo_idnr_lopeidnr\n0\n7000\n8\n\n\n21\nDataFrameSchema\nprodukt\nunique_combo_idnr_lopeidnr\n0\nAgurk\n9\n\n\n20\nDataFrameSchema\nprodukt\nunique_combo_idnr_lopeidnr\n0\nEple\n8\n\n\n19\nDataFrameSchema\nnavn\nunique_combo_idnr_lopeidnr\n0\nKari\n9\n\n\n18\nDataFrameSchema\nnavn\nunique_combo_idnr_lopeidnr\n0\nPer\n8\n\n\n17\nDataFrameSchema\naar\nunique_combo_idnr_lopeidnr\n0\n2024\n9\n\n\n16\nDataFrameSchema\naar\nunique_combo_idnr_lopeidnr\n0\n2024\n8\n\n\n27\nDataFrameSchema\nkostverdi\nunique_combo_idnr_lopeidnr\n0\n3100\n9\n\n\n13\nDataFrameSchema\nid_nr\nunique_combo_idnr_lopeidnr\n0\nrandom-id2\n9\n\n\n12\nDataFrameSchema\nid_nr\nunique_combo_idnr_lopeidnr\n0\nrandom-id2\n8\n\n\n1\nColumn\nlope_id_nr\nstr_startswith('L')\n1\n0001\n10\n\n\n11\nColumn\nkostverdi\ncheck_epler_bananer\n1\nFalse\nNone\n\n\n10\nColumn\nkostverdi\ngreater_than(700)\n0\n55\n10\n\n\n9\nColumn\nvekt\ngreater_than_or_equal_to(500)\n0\n5\n10\n\n\n8\nColumn\nvekt\ngreater_than_or_equal_to(500)\n0\n100\n9\n\n\n7\nColumn\nsalgsverdi\ncheck_epler_bananer\n1\nFalse\nNone\n\n\n6\nColumn\nsalgsverdi\ngreater_than_or_equal_to(1000)\n0\n50\n10\n\n\n5\nColumn\nprodukt\nisin(['Eple', 'Banan', 'Gulrot', 'Brokkoli'])\n0\nAgurk\n9\n\n\n4\nColumn\nnavn\nnot_nullable\nNone\nNone\n10\n\n\n3\nColumn\naar\nstr_not_digits\n1\n2024Q1\n10\n\n\n2\nColumn\naar\nstr_length(4, 4)\n0\n2024Q1\n10\n\n\n0\nColumn\nlope_id_nr\nstr_length(5, 5)\n0\n0001\n10\n\n\n\n\n\n\n\nDesverre vil sjekker som gjelder hele dataframen registrere flere feil ettersom han sjekker alle kolonner for gjeldende rader. Derimot er fleksibiliteten ganske stor!"
  },
  {
    "objectID": "blog/posts/2024-12-16-data-validering-pandera/index.html#bruk-av-validering-i-funksjonene",
    "href": "blog/posts/2024-12-16-data-validering-pandera/index.html#bruk-av-validering-i-funksjonene",
    "title": "Intro til Pandera",
    "section": "Bruk av validering i funksjonene",
    "text": "Bruk av validering i funksjonene\nOver til et eksempel hvor pandera viser seg som veldig nyttig! La oss si at vi har klargjorte data klart, ihht. datatilstandene, og vi er da klare for Ã¥ lage statistikkdata. Det er ikke gitt at lÃ¸pet er helt rett fram mellom disse datatilstandene, men i dette eksempelet er jobben bare Ã¥ fÃ¥ aggregert klargjorte data.\nNedenfor her lager jeg klargjorte data av de dataene som vi har jobbet med, og som er korrekte. Lager et tilhÃ¸rende skjema, som bare arver fra det fÃ¸rste schemaet vi lagde. Valideringen her vil selvsagt gÃ¥ smertefritt igjennom.\n\nklargjort_df = data.head(6)\n\n\nclass KlargjortSchema(SchemaValidation1):\n    pass\n\n\ntry:\n    klargjort_df = KlargjortSchema.validate(klargjort_df, lazy=True)\nexcept SchemaErrors as error:\n    valresult = error.failure_cases\n    raise error\n\nklargjort_df\n\n\n\n\n\n\n\n\nid_nr\nlope_id_nr\naar\nnavn\nprodukt\nsalgsverdi\nvekt\nkostverdi\n\n\n\n\n0\n9c2a0f28-830f-4ae3-9400-8d992bcaa4b4\nL0001\n2023\nPer\nEple\n9921\n3698\n7440\n\n\n1\n97a1e3cf-c465-49a9-9d58-28810ff5acd6\nL0001\n2023\nOla\nGulrot\n8806\n994\n6604\n\n\n2\nce06b21d-c416-4a12-9180-71a36a891fb4\nL0001\n2024\nKari\nBrokkoli\n6387\n4145\n4790\n\n\n3\n43903841-4231-4471-b9a5-c8947ca4c985\nL0001\n2023\nKari\nEple\n2813\n4778\n2109\n\n\n4\n76cf98f9-9e6b-42a3-b1d7-d04816b7a1be\nL0001\n2023\nIda\nBrokkoli\n7053\n3606\n5289\n\n\n5\nf78f7396-2554-4f8a-a171-66682628b6db\nL0001\n2024\nKari\nEple\n3221\n2344\n2415\n\n\n\n\n\n\n\nDeretter definerer vi et eget schema for statistikkdata, med noen tilhÃ¸rende regler og datatyper;\n\nclass StatistikkSchema(pan.DataFrameModel):\n    \n    aar: Series[pd.CategoricalDtype] = pan.Field(\n        coerce=True, # Vil konvertere datatypene for meg\n        str_length={'min_value': 4,\n                    'max_value': 4})\n    produkt: Series[pd.CategoricalDtype] = pan.Field(\n        coerce=True, # Vil konvertere datatypene for meg\n        isin=['Eple', 'Banan', 'Gulrot', 'Brokkoli'])\n    salgsverdi: Series[int] = pan.Field(ge=0)\n\nSÃ¥ over til magien; Pandera schemaene kan innlemmes i hvilken som helst funksjon som har dataframes som input eller output, og det uten at du selv skriver at valideringen skal skje i funksjonen, det skjer automagisk! Og det gjÃ¸res som fÃ¸lgende;\n\n# Lazy for at valideringen skal utfÃ¸res igjennom hele dataframene\n@pan.check_types(lazy=True)\ndef agg_statistikk(\n    df: DataFrame[KlargjortSchema]\n) -&gt; DataFrame[StatistikkSchema]:\n    dff = (\n        df\n        .copy()\n        .groupby(['aar', 'produkt'], as_index=False)\n        .agg({'salgsverdi': 'sum'})\n    )\n    return dff\n\nSÃ¥ nÃ¥ ved Ã¥ bruke funksjonen, vil du ikke fÃ¥ lagd statistikkdata uten at bÃ¥de klargjorte data blir validert og godkjent, og at statistikkdata som er pÃ¥ vei ut av funksjonen er validert og godkjent. I vÃ¥rt tilfelle skal det gÃ¥ fint nÃ¥;\n\nstatistikk_df = agg_statistikk(klargjort_df)\nstatistikk_df\n\n\n\n\n\n\n\n\naar\nprodukt\nsalgsverdi\n\n\n\n\n0\n2023\nBrokkoli\n7053\n\n\n1\n2023\nEple\n12734\n\n\n2\n2023\nGulrot\n8806\n\n\n3\n2024\nBrokkoli\n6387\n\n\n4\n2024\nEple\n3221\n\n\n\n\n\n\n\nMan skal ogsÃ¥ kunne validere flere schemaer samtidig ogsÃ¥ hvis en Ã¸nsker det. AltsÃ¥ at input til funksjonen sjekkes opp mot flere schema samtidig, eller at output blir det. Det er ikke blitt demonstrert her.\nMed det samme kan vi sjekke datatypene, vi hadde satt at Pandera skulle endre datatypene for oss. BÃ¥de fÃ¸r og etter;\n\n\nCode\nfrom IPython.display import HTML, display\n\nkdf = pd.DataFrame(klargjort_df.dtypes, columns=['Datatyper'])\nsdf = pd.DataFrame(statistikk_df.dtypes, columns=['Datatyper'])\n\n# Style dataframes\nstyled_df1 = kdf.style.set_caption(\"Klargjorte-data\")\nstyled_df2 = sdf.style.set_caption(\"Statistikk data\")\n\ndisplay(HTML(\nf\"\"\"\n&lt;div style=\"display: flex; justify-content: space-around;\"&gt;\n&lt;div&gt;{styled_df1.to_html()}&lt;/div&gt;\n&lt;div&gt;{styled_df2.to_html()}&lt;/div&gt;\n&lt;/div&gt;\n\"\"\"\n))\n\n\n\n\n\n\n\n\nTabellÂ 1: Klargjorte-data\n\n\n\n\n\nÂ \nDatatyper\n\n\n\n\nid_nr\nobject\n\n\nlope_id_nr\nobject\n\n\naar\nobject\n\n\nnavn\nobject\n\n\nprodukt\nobject\n\n\nsalgsverdi\nint64\n\n\nvekt\nint64\n\n\nkostverdi\nint64\n\n\n\n\n\n\n\n\n\n\n\n\n\nTabellÂ 2: Statistikk data\n\n\n\n\n\nÂ \nDatatyper\n\n\n\n\naar\ncategory\n\n\nprodukt\ncategory\n\n\nsalgsverdi\nint64"
  },
  {
    "objectID": "blog/posts/2024-12-16-data-validering-pandera/index.html#bonus-auto-transformasjon-av-kolonneverdier",
    "href": "blog/posts/2024-12-16-data-validering-pandera/index.html#bonus-auto-transformasjon-av-kolonneverdier",
    "title": "Intro til Pandera",
    "section": "BONUS: Auto-transformasjon av kolonneverdier",
    "text": "BONUS: Auto-transformasjon av kolonneverdier\nPandera har noe som kalles parsers, som gir oss muligheten til Ã¥ utfÃ¸re preprosesseringer pÃ¥ dataene fÃ¸r validering. Dette kan vÃ¦re flere typer transformasjoner som man bÃ¸r sÃ¸rge for er gjort fÃ¸r valideringen utfÃ¸res, ev. om transformasjonen bare skal gjennomfÃ¸res.\nLa oss ta et eksempel med en liten del av dataene vi har jobbet med til nÃ¥, da med data vi vet det ikke skal bli noe problemer med;\n\ndata['dekningsbidrag'] = data['salgsverdi'] - data['kostverdi']\n\ndf = data.head(3).copy()\ndf\n\n\n\n\n\n\n\n\nid_nr\nlope_id_nr\naar\nnavn\nprodukt\nsalgsverdi\nvekt\nkostverdi\ndekningsbidrag\n\n\n\n\n0\n9c2a0f28-830f-4ae3-9400-8d992bcaa4b4\nL0001\n2023\nPer\nEple\n9921\n3698\n7440\n2481\n\n\n1\n97a1e3cf-c465-49a9-9d58-28810ff5acd6\nL0001\n2023\nOla\nGulrot\n8806\n994\n6604\n2202\n\n\n2\nce06b21d-c416-4a12-9180-71a36a891fb4\nL0001\n2024\nKari\nBrokkoli\n6387\n4145\n4790\n1597\n\n\n\n\n\n\n\nJeg beregner her dekningsbidraget for hver observasjon, som da er differansen mellom salgsverdi og kostverdi. Det er mer eller mindre en funksjon som avhenger av disse to variablene, og mÃ¥ holdes oppdatert.\nOg la oss nÃ¥ si at kostverdien pÃ¥ fÃ¸rste observasjonen ikke skulle vÃ¦re pÃ¥ 75 % av salgsverdi slik vi startet med, men av en eller annen grunn heller skulle vÃ¦re pÃ¥ 85 %. Vi kan editere det inn;\n\ndf.loc[0, ['kostverdi']] = int(round(\n    df.iloc[0]['salgsverdi'] * 0.85, 0)\n                              )\n\ndf\n\n\n\n\n\n\n\n\nid_nr\nlope_id_nr\naar\nnavn\nprodukt\nsalgsverdi\nvekt\nkostverdi\ndekningsbidrag\n\n\n\n\n0\n9c2a0f28-830f-4ae3-9400-8d992bcaa4b4\nL0001\n2023\nPer\nEple\n9921\n3698\n8433\n2481\n\n\n1\n97a1e3cf-c465-49a9-9d58-28810ff5acd6\nL0001\n2023\nOla\nGulrot\n8806\n994\n6604\n2202\n\n\n2\nce06b21d-c416-4a12-9180-71a36a891fb4\nL0001\n2024\nKari\nBrokkoli\n6387\n4145\n4790\n1597\n\n\n\n\n\n\n\nSÃ¥ nÃ¥ har vi fÃ¥tt korrigert kostverdien pÃ¥ fÃ¸rste observasjon, men dekningsbidraget er fortsatt den samme. Dette kan lÃ¸ses som en egen funksjon, men hvorfor ikke innlemme det i data valideringen vÃ¥r, da pandera stÃ¸tter slik transformering. Vi lager fÃ¸rst et tilhÃ¸rende schema;\n\nclass ParserSchema(SchemaValidation1):\n    dekningsbidrag: Series[int]\n\n    @pan.check(\"navn\")\n    def is_uppercase(cls, s: Series[str]) -&gt; Series[bool]:\n        return s.str.isupper()\n\n    # konverterer all tekst i kolonnen til Ã¥ ha kun store bokstaver\n    @pan.parser(\"navn\")\n    def uppercase(cls, s: Series[str]) -&gt; Series[str]:\n        return s.str.upper()\n\n    # SÃ¸rger for at dekningsbidrag blir rekalkulert\n    @pan.dataframe_parser\n    def kalkuler_dekningsbidrag(cls, df: pd.DataFrame) -&gt; pd.DataFrame:\n        df['dekningsbidrag'] = df['salgsverdi'] - df['kostverdi']\n        return df\n\nSÃ¥ her tar jeg i bruk det aller fÃ¸rste schema som vi definerte, men legger pÃ¥ dekningsbidrag som ikke har noen andre valideringer enn datatype. Med dataene vi har nÃ¥ skal det ikke dukke opp noen feil med dette. Jeg legger ved en valideringsregel for navn i dette tilfelle, hvor nÃ¥ alt i kolonnen navn skal vÃ¦re store bokstaver. Vi vet allerede at det ikke er noen store bokstaver der, sÃ¥ vi legger inn en metode som har decorator parser som vil transformere dette. I tillegg legger vi til en egen metode med decorator dataframe_parser for Ã¥ rekalkulere dekningsbidraget.\nSÃ¥ sÃ¥nn sett skulle man kanskje tro at valideringen av kolonnen navn vil kunne slÃ¥ ut i valideringen, men som nevnt sÃ¥ kjÃ¸res transformasjonene fÃ¸rst fÃ¸r valideringen. I tillegg, nÃ¥r valideringen gÃ¥r igjennom, vil du fÃ¥ dataframen du sendte inn i retur ved utfÃ¸relsen av valideringen;\n\ntry:\n    valresult = ParserSchema.validate(df, lazy=True)\nexcept SchemaErrors as error:\n    valresult = error.failure_cases\n\nvalresult\n\n\n\n\n\n\n\n\nid_nr\nlope_id_nr\naar\nnavn\nprodukt\nsalgsverdi\nvekt\nkostverdi\ndekningsbidrag\n\n\n\n\n0\n9c2a0f28-830f-4ae3-9400-8d992bcaa4b4\nL0001\n2023\nPER\nEple\n9921\n3698\n8433\n1488\n\n\n1\n97a1e3cf-c465-49a9-9d58-28810ff5acd6\nL0001\n2023\nOLA\nGulrot\n8806\n994\n6604\n2202\n\n\n2\nce06b21d-c416-4a12-9180-71a36a891fb4\nL0001\n2024\nKARI\nBrokkoli\n6387\n4145\n4790\n1597\n\n\n\n\n\n\n\nSom vi nÃ¥ ser har valideringen gÃ¥tt fint for seg. Vi ser at alle verdier i kolonnen navn har blitt tekst med kun store bokstaver, og vi ser at dekningsbidraget har blitt rekalkulert sÃ¥ det nÃ¥ er korrekt!"
  },
  {
    "objectID": "blog/posts/2024-12-16-data-validering-pandera/index.html#oppsummering",
    "href": "blog/posts/2024-12-16-data-validering-pandera/index.html#oppsummering",
    "title": "Intro til Pandera",
    "section": "Oppsummering",
    "text": "Oppsummering\nFor at vi skal kunne produsere og levere statistikk av hÃ¸y kvalitet er det viktig at vi validerer data lÃ¸pende i produksjonslÃ¸pene vÃ¥re. Store deler av dataene vÃ¥re er strukturerte, ev. tidy om du vil, og da er python pakken Pandera en sterk kandidat Ã¥ benytte inn i kodene vÃ¥re. Hvertfall hvis du programmerer i Python. For R sÃ¥ er pakken Validate aktuell. Her har vi introdusert generell bruk av Pandera for validering av data; hvordan definere schema og valideringsregler, hvordan validere en dataframe med det, og hvordan det kan tas i bruk i blant annet funksjoner. Trenger du hjelp til Ã¥ implementere data validering med Pandera inn i koden din, sÃ¥ er StÃ¸tteteamene mulig Ã¥ spÃ¸rre, ellers kommer man ikke unna dokumentasjonen til Pandera selv."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html",
    "title": "Fra arkiv til parquet",
    "section": "",
    "text": "I arkivet til SSB ligger data lagret som posisjonerte flatfiler, ogsÃ¥ kalt fastbredde-fil eller fixed width file pÃ¥ engelsk. I Datadok ligger det spesifisert hvordan du leser inn disse filene fra dat eller txt i arkivet til sas7bdat-formatet, men ikke hvordan man konverterer til Parquet-formatet. I denne artikkelen deler jeg hvordan jeg gikk frem for Ã¥ konvertere arkivfiler til Parquet."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#hva-er-en-fastbredde-fil",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#hva-er-en-fastbredde-fil",
    "title": "Fra arkiv til parquet",
    "section": "Hva er en fastbredde-fil?",
    "text": "Hva er en fastbredde-fil?\nEn fastbredde-fil er en fil der hver rad har en fast lengde, og hver kolonne har en fast posisjon. Det er ingen komma eller andre tegn som skiller kolonnene, slik som i en CSV-fil. En fastbredde-fil er en ren tekstfil, dvs. at du kan Ã¥pne den opp i teksteditor og kikke pÃ¥ innholdet direkte.\nUnder er et eksempel hvor samme data er lagret bÃ¥de pÃ¥ CSV-formatet og som fastbredde-fil:\n\n\n\n\ncsv\n\n012345;;Ola Nordmann;\n345678;Kvinne;Kari Nordmann;\n\n\n\n\n\n\nfastbredde-fil\n\n012345      Ola Nordmann \n345678KvinneKari Nordmann\n\n\n\nI csv-filen over til venstre ser vi at hver kolonne er separert med et semikolon, og at hver rad er separert med et linjeskift. I fastbredde-filen til hÃ¸yre ser vi at hver kolonne har en fast lengde, den tomme kjÃ¸nnsvariabelen pÃ¥ rad 1 fylles med spaces, hver rad har dermed den samme lengden i antall tegn. I tillegg er det et ekstra mellomrom etter Ola Nordmann ift. Kari Nordmann. Dette er fordi Ola Nordmann er 12 tegn lang, mens Kari Nordmann er 13 tegn lang."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lese-med-pandas",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lese-med-pandas",
    "title": "Fra arkiv til parquet",
    "section": "Lese med Pandas",
    "text": "Lese med Pandas\nVi kan bruke pandas-funksjonen read_fwf() for Ã¥ lese inn en fastbredde-fil. Denne funksjonen tar inn en filsti, og en liste med bredder for hver kolonne. I tillegg kan vi spesifisere navn pÃ¥ kolonnene, og hvilken datatype kolonnene skal ha og hvordan missing-verdier skal representeres.\nVi er helt avhengig av Ã¥ vite bredden pÃ¥ hver kolonne for Ã¥ kunne lese inn en fastbredde-fil. Dette kan vi finne ut ved Ã¥ Ã¥pne filen i en teksteditor og telle/gjette antall tegn i hver kolonne. Alternativt kan vi bruke innlesingsskriptet for SAS som finnes i Datadok, siden breddene er spesifisert der. Under er et ekspempel pÃ¥ hvordan vi kan lese inn en fastbredde-fil fra forrige avsnitt1:\n\nimport pandas as pd\nfrom io import StringIO  # NÃ¸dvendig siden vi sender en streng, ikke en filsti til .read_fwf\ninstring = \"112345      Ola Nordmann \\n345678KvinneKari Nordmann\\n\"\ndf = pd.read_fwf(StringIO(instring),\n                 names=['pers_id', 'kjonn', 'navn'],  # Navngi kolonner\n                 dtype='object',  # Alle kolonnene settes til \"object\"\n                 na_values=['.', ' .'],  # Hvilke karakterer bruker SAS for tom verdi?\n                 widths=[6, 6, 13])  # Tell/regn ut dissa sjÃ¸l\ndf\n\n\n\n\n\n\n\n\npers_id\nkjonn\nnavn\n\n\n\n\n0\n112345\nNaN\nOla Nordmann\n\n\n1\n345678\nKvinne\nKari Nordmann\n\n\n\n\n\n\n\nKoden over returnerer en Pandas Dataframe i minnet. Den kan vi lett lagre til Parquet-formatet. Men innlesingen mÃ¥tte vi spesifisere en masse detaljer manuelt. Hvis vi skal lese inn mange filer med ulik struktur, sÃ¥ er ikke denne fremgangsmÃ¥ten skalerbar. Dette er en fremgangsmÃ¥te for Ã¥ lese inn noen fÃ¥ filer."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#datadok",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#datadok",
    "title": "Fra arkiv til parquet",
    "section": "Datadok",
    "text": "Datadok\nSom nevnt over sÃ¥ finnes det et innlesingsskript for SAS i Datadok. Dette skriptet kan vi bruke til Ã¥ lese inn en fastbredde-fil i Python. Vi kan ogsÃ¥ bruke det til Ã¥ finne breddene pÃ¥ hver kolonne. Et slik skript har denne formen:\n\n\ninnlesingsskript.sas\n\nDATA sas_data;\n   INFILE '/ssb/bruker/felles/flatfileksempel_dapla_manual_blogg.dat' MISSOVER PAD LRECL=36;\n   INPUT\n      @1 pers_id 6.0\n      @7 kjonn $CHAR6.0\n      @13 navn $CHAR13.0\n   ;\nRUN;\n\nVi kunne lest av informasjonen her og omsatt innholdet til argumentene read_fwf() trenger. Men fortsatt innebÃ¦rer dette potensielt en del manuelt arbeid."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lese-med-saspy",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lese-med-saspy",
    "title": "Fra arkiv til parquet",
    "section": "Lese med saspy",
    "text": "Lese med saspy\nEn annen tilnÃ¦rming enn Ã¥ bruke .read_fwf fra Pandas er Ã¥ bruke biblioteket saspy. Dette biblioteket lar oss kjÃ¸re SAS-kode fra Python, pÃ¥ SAS-serverene i prodsonen, og fÃ¥ Dataframes tilbake. Vi kan bruke det til Ã¥ kjÃ¸re sas-skript hentet fra Datadok, konvertere til en pandas dataframe, og til slutt skrive til Parquet. I det fÃ¸lgende antar vi at du jobber i Jupyterlab i prodsonen (sl-jupyter-p), og at du har lagret innlesingsskriptet i en variabel, slik som vist under:\n\n\npython\n\nscript = \"\"\"\nDATA sas_data;\n   INFILE '/ssb/bruker/felles/flatfileksempel_dapla_manual_blogg.dat' MISSOVER PAD LRECL=36;\n   INPUT\n      @1 pers_id 6.0\n      @7 kjonn $CHAR6.0\n      @13 navn $CHAR13.0\n   ;\nRUN;\n\"\"\"\n\nLa oss deretter kjÃ¸re fÃ¸lgende kode fra Jupyterlab:\n\n\npython\n\nfrom fagfunksjoner import saspy_session\n\n# Kobler til sas-serverne\nsas = saspy_session()\n\n# Vi bruker tilkoblingen til Ã¥ sende inn Datadok-skriptet\nresult = sas.submit(script)\n\n# Lagre sas-loggen i en variabel\nlog = result[\"LOG\"]\n\n# Ber om Ã¥ fÃ¥ dataframe tilbake\ndf_frasas = sas.sd2df(\"sas_data\", \"work\")\n\n# Lukker koblingen til sas-serverne\nsas._endsas()\n\n# Printer ut datasettet\ndf_frasas\n\nI koden over har vi brukt en pakke som heter ssb-fagfunksjoner for Ã¥ opprette koblingen til sas-serveren. Pakken inneholder et overbygg over saspy, og koden over forutsetter at du har lagret passordet ditt pÃ¥ en spesiell mÃ¥te2.\n\nDatatyper\nVi har nÃ¥ en pandas dataframe med datatyper pÃ¥fÃ¸rt, men disse er basert pÃ¥ den lave mengden datatyper i SAS. Ofte bÃ¸r det ryddes i datatyper fÃ¸r man skriver til Parquet. Spesielt bÃ¸r du tenke pÃ¥ fÃ¸lgende:\n\nCharacter mappes gjerne til object i pandas, ikke den strengere varianten string eller den mer spesifikke string[pyarrow].\nNumeric mappes stort sett til float64 i pandas, vi fÃ¥r som regel ikke heltall direkte Int64 uten videre behandling.\n\nDu kan la Pandas gjÃ¸re ett nytt forsÃ¸k pÃ¥ Ã¥ gjette datatyper ved Ã¥ kjÃ¸re fÃ¸lgende kode:\n\n\npython\n\ndf_pd_dtypes = df_frasas.convert_dtypes()\ndf_pd_dtypes.dtypes\n\nOm du vil teste min selvskrevne funksjon for Ã¥ gjette pÃ¥ datatyper sÃ¥ ligger den i fellesfunksjons-pakken:\n\n\npython\n\nfrom fagfunksjoner import auto_dtype\ndf_auto = auto_dtype(df_frasas)\ndf_auto.dtypes\n\nSjekk gjerne ut parameteret cardinality_threshold pÃ¥ auto_dtype, om du er interessert i Ã¥ automatisk sette categorical dtypes.\n\n\nSkalering\nHvis du har mange arkivfiler, med mange forskjellige innlesingsskript, sÃ¥ kan du lagre alle skriptene i en mappe, og sÃ¥ hente innholdet programmatisk. Her er koden for Ã©n slik â€œhentingâ€.\n\n\npython\n\nsas_script_path = \"/ssb/bruker/felles/flatfileksempel_dapla_manual_blogg.sas\"\nwith open(sas_script_path, \"r\", encoding=\"latin1\") as sas_script:\n    script = sas_script.read().strip()\n    script = \"DATA \" + script.split(\"DATA \")[1] # Forkort ned scriptet til det vi trenger\nprint(script)\n\nHer henter jeg inn et innlesingsskript fra Datadok som jeg har lagret som en tekstfil i en mappe pÃ¥ linux-serveren i prodsonen. Deretter gjÃ¸r jeg den om til et streng-objekt i minnet som kan sendes til saspy-koden som er vist over. Dermed er det bare Ã¥ finne en logikk som gjÃ¸r at du vet hvilket innlesingskript som skal brukes til hvilke arkivfiler (siste valide datadok-script fÃ¸r datafil oppstod feks), og du kan jobbe veldig effektivt med konvertering. NÃ¥r alt er konvertert kan du f.eks. kjÃ¸re et script som validerer datatypene pÃ¥ tvers av alle Ã¥rganger og filnavn."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lagre-dataframen-til-parquet",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lagre-dataframen-til-parquet",
    "title": "Fra arkiv til parquet",
    "section": "Lagre dataframen til parquet",
    "text": "Lagre dataframen til parquet\nNÃ¥ er det veldig lett Ã¥ skrive filen til Parquet-formatet.\n\n\npython\n\ndf_auto.to_parquet(\n    \"/ssb/bruker/felles/flatfileksempel_dapla_manual_blogg.parquet\"\n    )"
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#nudb",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#nudb",
    "title": "Fra arkiv til parquet",
    "section": "NUDB",
    "text": "NUDB\nI omleggingen av NUDB (Nasjonal utdanningsdatabase), mÃ¥tte vi konvertere hele arkivet vÃ¥rt pÃ¥ 750+ dat-filer.\nDet var Ã¸nskelig Ã¥ slippe Ã¥ lagre til sas7bdat i mellom, for Ã¥ slippe mye dataduplikasjon og arbeidsprosesser. MÃ¥let vÃ¥rt var pseudonymiserte parquetfiler i sky.\nI stor grad kunne dette arbeidet automatiseres (bortsett fra Ã¥ lagre ut innlastingsscript fra gamle datadok). Funksjonene jeg utviklet for dette, ligger stort sett i denne filen:\ngithub.com/utd-nudb/prodsone/konverter_arkiv/archive.py"
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#footnotes",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#footnotes",
    "title": "Fra arkiv til parquet",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\n/n i strengen 112345      Ola Nordmann \\n345678KvinneKari Nordmann\\n betyr linjeskift.â†©ï¸\nHvis du Ã¸nsker kan du bruker ssb-fagfunksjoner til Ã¥ lagre passordet ditt i kryptert form. Da kan du lagre passordet i en fil pÃ¥ din egen maskin, og slipper Ã¥ skrive det inn hver gang du skal koble til SAS. Funksjonen heter fagfunksjoner.prodsone.saspy_ssb.set_password().â†©ï¸"
  },
  {
    "objectID": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html",
    "href": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html",
    "title": "Ideen bak bloggen",
    "section": "",
    "text": "SSB-ere lÃ¸ser hele tiden problemer pÃ¥ nye mÃ¥ter som andre gjerne skulle nyttiggjort seg av. Spesielt nÃ¥r vi gjÃ¸r en sÃ¥ stor overgang i arbeidsform som overgangen til en ny plattform (Dapla), og vi samtidig skifter mange verktÃ¸yene vi har i verktÃ¸ykassen vÃ¥r. Av den grunn har vi opprettet denne bloggen. Her vil vi skrive om hvordan vi lÃ¸ser problemer, hvilke verktÃ¸y vi bruker og hvordan vi bruker dem. Vi vil ogsÃ¥ skrive om hvordan vi jobber med Ã¥ utvikle nye verktÃ¸y og hvordan vi jobber med Ã¥ utvikle Dapla.\nMÃ¥lsetningen med denne bloggen er at alle i SSB som Ã¸nsker Ã¥ dele noe med andre kan skrive en artikkel og dele i bloggen. Mens ByrÃ¥nettet er kanal for Ã¥ dele informasjon med alle i SSB, og Viva Engage en kanal for Ã¥ si det du tenker uten sÃ¦rlig noen formell struktur, er denne bloggen en kanal for Ã¥ dele informasjon med andre som jobber med data og teknologi i SSB.\nFordelen med bloggen er at den er tilpasset hvordan statistikkere, forskere og IT-utviklere jobber til daglig. Artiklene kan skrives samme sted som man utvikler kode, og man inkludere output fra kodekjÃ¸ringer i artikler."
  },
  {
    "objectID": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html#hvordan-skrive-en-artikkel",
    "href": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html#hvordan-skrive-en-artikkel",
    "title": "Ideen bak bloggen",
    "section": "Hvordan skrive en artikkel?",
    "text": "Hvordan skrive en artikkel?\nBloggen er generert med Quarto. Quarto er et rammeverk for Ã¥ skrive artikler i markdown. Det er enkelt Ã¥ komme i gang med Quarto, og det er enkelt Ã¥ skrive artikler i Quarto.\nFor Ã¥ skrive en artikkel gjÃ¸r du fÃ¸lgende:\n\nSkriv artikkelen som en markdown-fil (.qmd-fil) eller en notebook (.ipynb-fil).\nKlon dapla-manual-internal repoet:\ngit clone https://github.com/statisticsnorway/dapla-manual-internal.git\nOpprett en mappe for artikkelen din i mappen ./dapla-manual-internal/blog/posts/. Gi mappen et navn som beskriver artikkelen din.\nInne mappen legger du din .qmd- eller .ipynb-fil. Eventuelle bilder i artikkelen legges ogsÃ¥ i samme mappe.\nOpprette en pull request pÃ¥ repoet og noen vil se over artikkelen din og publisere den."
  },
  {
    "objectID": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html#metadata-om-artikkelen",
    "href": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html#metadata-om-artikkelen",
    "title": "Ideen bak bloggen",
    "section": "Metadata om artikkelen",
    "text": "Metadata om artikkelen\nNÃ¥r du skriver artikkelen sÃ¥ mÃ¥ du starte dokumentet med fÃ¸lgende metadata:\n\n\nindex.qmd\n\n---\ntitle: Ideen bak bloggen\nsubtitle: Hvorfor vi har opprettet denne bloggen? \ncategories:\n  - Quarto\nauthor:\n  - name: Ã˜yvind Bruer-SkarsbÃ¸\n    affiliation: \n      - name: Seksjon for dataplattform (724)\n        email: obr@ssb.no\ndate: \"01/11/2024\"\ndate-modified: \"01/11/2024\"\nimage: ../../../images/dapla-long.png\nimage-alt: \"Bilde av Fame-logoen\"\ndraft: false\n---\n\nHusk Ã¥ fylle ut alle feltene slik at det blir riktig informasjon for din artikkel. Skriver du en ipynb-fil sÃ¥ mÃ¥ metadataene ligge i en celle av typen raw.\nÃ˜nsker du Ã¥ komme fort igang sÃ¥ kan se hvordan denne artikkelen ble skrevet."
  },
  {
    "objectID": "blog/posts/2024-11-17-lonboard/index.html",
    "href": "blog/posts/2024-11-17-lonboard/index.html",
    "title": "Lonboard",
    "section": "",
    "text": "Lonboard1 er et bibliotek for Ã¥ vise kart i Jupyter notebooks. Lonboard er bygget pÃ¥ Deck.gl, et GPU akselerert, hÃ¸ytytende, kartvisualiseringsbibliotek for store data. Lonboard er bygget med Anywidget."
  },
  {
    "objectID": "blog/posts/2024-11-17-lonboard/index.html#hvorfor-lonboard",
    "href": "blog/posts/2024-11-17-lonboard/index.html#hvorfor-lonboard",
    "title": "Lonboard",
    "section": "Hvorfor Lonboard?",
    "text": "Hvorfor Lonboard?\nGeopandasâ€™ explore-metode og explore-funksjonen i ssb-sgis-pakken bruker kartvisningsbiblioteket Folium. Hver gang et kart skal vises med Folium, blir kartdataene konvertert til GeoJSON-formatet, som deretter sendes ukomprimert fra Jupyter-serveren til nettleseren. Dette kan fÃ¸re til lang overfÃ¸ringstid og potensielt hÃ¸yt minneforbruk i nettleseren, spesielt nÃ¥r man forsÃ¸ker Ã¥ vise store datasett, som landsdekkende grunnkretser, tettsteder eller postnummeromrÃ¥der.\nLonboard hÃ¥ndterer store datamengder bedre ved Ã¥ overfÃ¸re data mellom serveren og nettleseren i Parquet-format i stedet for GeoJSON, som sÃ¥ leses av Deck.gl. Siden Parquet tilfeldigvis er SSBs standard lagringsformat, kan denne overfÃ¸ringen skje med minimal datakonvertering.\nIkke alle i SSB jobber med Pandas, og for disse brukerne kan Lonboard visualisere tabeller fra DuckDB og PyArrow, i tillegg til Geopandas-tabeller.\nHvor kommer Anywidget inn? Anywidget er et rammeverk for Ã¥ lage widgets. En widgets lar ta med interaktive komponenter inn i en Jupyter notebook, slik som en datovelger eller en filtrerbar og sorterbar tabell. Litt slik som Dash lar deg gjÃ¸re, men rett i Jupyter notebook. Ã… lage sin egen widget til Juypter er en ganske komplisert afÃ¦re. Det krever at du pakker bÃ¥de Python kode og Javascript kode. Det krever at du mÃ¥ skrive kode for Ã¥ stÃ¸tte alle miljÃ¸er som kan kjÃ¸re Jupyter notbooks, slik som VScode, Jupyterlab eller Google Colab. Anywidget forenkler prosessen veldig, og sÃ¸rger for at widget din virker pÃ¥ alle plattformer. Den forenkler kommunikasjonen mellom Python og Javascript siden, sÃ¥ det blir enklere Ã¥ lage interaktivitet.\nAnywidget er nÃ¥ installert i Jupyter pÃ¥ Dapla Lab, slik at alle kan prÃ¸ve ut bÃ¥de Lonboard, samt andre pakker som bygger pÃ¥ Anywidget. Et eksempel pÃ¥ en slik annen pakke er Vega-altair"
  },
  {
    "objectID": "blog/posts/2024-11-17-lonboard/index.html#eksempel-med-grunnkretser-i-innlandet-fylke",
    "href": "blog/posts/2024-11-17-lonboard/index.html#eksempel-med-grunnkretser-i-innlandet-fylke",
    "title": "Lonboard",
    "section": "Eksempel med grunnkretser i Innlandet fylke",
    "text": "Eksempel med grunnkretser i Innlandet fylke\nUnder er eksempel pÃ¥ hvordan man visualisere grunnkretsene i Innlandet fylke i en Jupyterlab Notebook pÃ¥ Dapla Lab. GjÃ¸r fÃ¸lgende fÃ¸rst:\n\nStart en Jupyter pÃ¥ Dapla Lab.\nOpprett et ssb-project\nInstaller nÃ¸dvendige pakker poetry add geopandas lonboard numpy mapclassify matplotlib libpysal\nÃ…pne en ny notebook med kernelen som ble opprettet av ssb-project.\n\nI den nyopprettede notebooken kan deretter hente inn data om grunngretser i Innlandet fylke og visualisere de med Lonboard.\n\nimport geopandas as gpd\nimport lonboard\nfrom lonboard import basemap\nfrom mapclassify import greedy\nfrom matplotlib import colormaps\nimport numpy as np\n\ngrunnkretser = gpd.read_file(\n    \"https://nedlasting.geonorge.no/geonorge/Basisdata/Grunnkretser/GML/Basisdata_34_Innlandet_25833_Grunnkretser_GML.zip\",\n    layer=\"Grunnkrets\",\n    engine=\"pyogrio\",\n    columns=[\"grunnkretsnummer\", \"grunnkretsnavn\", \"kommunenummer\"],\n)\n\ngrunnkretser.head()\n\ncolor = greedy(grunnkretser, strategy=\"balanced\", balance=\"centroid\").map(\n    colormaps[\"Set2\"].colors.__getitem__\n)\ncolor = (np.stack(color.to_numpy()) * 255).astype(np.uint8)\n\ngrunnkretser_wgs84 = grunnkretser.to_crs(4326)\nlayer = lonboard.PolygonLayer.from_geopandas(\n    grunnkretser_wgs84,\n    opacity=0.2,\n    line_miter_limit=1,\n    line_width_units = \"pixels\",\n    get_fill_color=color,\n    get_line_color=[255,255,255],\n    auto_highlight=True,\n)\n\nkart = lonboard.Map(\n    [layer],\n    basemap_style=basemap.CartoBasemap.DarkMatterNoLabels,\n    _height=500,\n)\n\nkart\n\n\n\n\n\n\n    \n    Lonboard export\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLonboard har i motsetning til Folium muligheten for interaktivitet. Hvis du velger en avgrensningsboks i kartet over, men knappen oppe i hÃ¸yre hjÃ¸rne, sÃ¥ blir det utvalet tilgjengleig i Python som kart.selected_bounds\nif kart.selected_bounds:\n    xmin, ymin, xmax, ymax = kart.selected_bounds\n    utvalg = grunnkretser_wgs84.cx[xmin:xmax, ymin:ymax]\n    print(f\"Det er {len(utvalg)} grunnkretser i utvalget\")\nelse:\n  print(\"Du har ikke gjort et utvalg.\")\nDu kan ogsÃ¥ utvikle din egen widget, men selvom Anywidget forenkler prossessen mye, krever dette fremdeles ferdigheter i bÃ¥de Python og Javascript, sÃ¥ det er kansje ikke for alle."
  },
  {
    "objectID": "blog/posts/2024-11-17-lonboard/index.html#footnotes",
    "href": "blog/posts/2024-11-17-lonboard/index.html#footnotes",
    "title": "Lonboard",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nNavnet er et ordspill. Et Â«longboardÂ», er en type raskt skateboard, et Â«deckÂ» er den delen av skateboardet du stÃ¥r pÃ¥. Â«LonÂ» en mye brukt forkortelse for Â«longitudeÂ», lengdegrad.â†©ï¸"
  },
  {
    "objectID": "utviklere/lenker.html",
    "href": "utviklere/lenker.html",
    "title": "Lenker",
    "section": "",
    "text": "Dokumentasjon for utviklere i SSB er spredt over mange ulike sider. Denne siden er en oversikt over dokumentasjonen som finnes.\n\n\n\nSlack\nGitHub\nConfluence\nByrÃ¥nettet\n\n\n\n\n\n\nhttps://backstage.intern.ssb.no\n\nProgramvarekatalog\nTeamkatalog\nTeknologiradar\n\n\n\n\n\nOffisielle docs fra nav: https://docs.ssb.cloud.nais.io/\nIntern system docs: https://statisticsnorway.github.io/nais-system (krever GitHub innlogging)\nNais console: https://console.ssb.cloud.nais.io/ (krever naisdevice)\n\n\n\n\n\nDapla ctrl (teamkatalog): https://dapla-ctrl.intern.ssb.no/\n\n\n\n\n\nhttps://lab.dapla.ssb.no/\nIntern system docs: https://statisticsnorway.github.io/dapla-lab-system (krever GitHub innlogging)\nBrukerdokumentasjon: https://manual.dapla.ssb.no/statistikkere/dapla-lab.html\n\n\n\n\n\nhttps://docs.bip.ssb.no (krever GitHub innlogging)\nhttps://docs.dapla.ssb.no/ (krever GitHub innlogging)\n\n\n\n\n\n\nArkitekturprinsipper\nArchitecture Decision Records (krever GitHub innlogging)\nTeknologiradar (krever naisdevice)\n\n\n\n\nGenerellt er utviklere i SSB veldig fri til Ã¥ velge sin lokal dev-oppsett utifra preferanser og teknologivalg. FÃ¸lgende lenker gÃ¥r pÃ¥ SSB spesifikk oppsett, sÃ¦rlig relatert til nettverkstilgang og sikkerhet ellers.\n\nKundeservice FAQ\n\nWiFi\nMicrosoft Authenticator\nVPN\nEpost\nPassordbytte\n\nGit/GitHub\nIntelliJ lisenser\nInstaller naisdevice\n\n\n\n\n\n\n\nDependabot\nSnyk\nDetectify\nSonarcloud\nMabl\n\n\n\n\n\n\nOperasjonell modell\nIT Avdelingens leveranse demo (finner sted en gang i mÃ¥neden pÃ¥ fredag)",
    "crumbs": [
      "Utviklere",
      "Lenker"
    ]
  },
  {
    "objectID": "utviklere/lenker.html#samhandling",
    "href": "utviklere/lenker.html#samhandling",
    "title": "Lenker",
    "section": "",
    "text": "Slack\nGitHub\nConfluence\nByrÃ¥nettet",
    "crumbs": [
      "Utviklere",
      "Lenker"
    ]
  },
  {
    "objectID": "utviklere/lenker.html#plattform",
    "href": "utviklere/lenker.html#plattform",
    "title": "Lenker",
    "section": "",
    "text": "https://backstage.intern.ssb.no\n\nProgramvarekatalog\nTeamkatalog\nTeknologiradar\n\n\n\n\n\nOffisielle docs fra nav: https://docs.ssb.cloud.nais.io/\nIntern system docs: https://statisticsnorway.github.io/nais-system (krever GitHub innlogging)\nNais console: https://console.ssb.cloud.nais.io/ (krever naisdevice)\n\n\n\n\n\nDapla ctrl (teamkatalog): https://dapla-ctrl.intern.ssb.no/\n\n\n\n\n\nhttps://lab.dapla.ssb.no/\nIntern system docs: https://statisticsnorway.github.io/dapla-lab-system (krever GitHub innlogging)\nBrukerdokumentasjon: https://manual.dapla.ssb.no/statistikkere/dapla-lab.html\n\n\n\n\n\nhttps://docs.bip.ssb.no (krever GitHub innlogging)\nhttps://docs.dapla.ssb.no/ (krever GitHub innlogging)",
    "crumbs": [
      "Utviklere",
      "Lenker"
    ]
  },
  {
    "objectID": "utviklere/lenker.html#arkitektur",
    "href": "utviklere/lenker.html#arkitektur",
    "title": "Lenker",
    "section": "",
    "text": "Arkitekturprinsipper\nArchitecture Decision Records (krever GitHub innlogging)\nTeknologiradar (krever naisdevice)",
    "crumbs": [
      "Utviklere",
      "Lenker"
    ]
  },
  {
    "objectID": "utviklere/lenker.html#oppsett",
    "href": "utviklere/lenker.html#oppsett",
    "title": "Lenker",
    "section": "",
    "text": "Generellt er utviklere i SSB veldig fri til Ã¥ velge sin lokal dev-oppsett utifra preferanser og teknologivalg. FÃ¸lgende lenker gÃ¥r pÃ¥ SSB spesifikk oppsett, sÃ¦rlig relatert til nettverkstilgang og sikkerhet ellers.\n\nKundeservice FAQ\n\nWiFi\nMicrosoft Authenticator\nVPN\nEpost\nPassordbytte\n\nGit/GitHub\nIntelliJ lisenser\nInstaller naisdevice",
    "crumbs": [
      "Utviklere",
      "Lenker"
    ]
  },
  {
    "objectID": "utviklere/lenker.html#test-og-kvalitet",
    "href": "utviklere/lenker.html#test-og-kvalitet",
    "title": "Lenker",
    "section": "",
    "text": "Dependabot\nSnyk\nDetectify\nSonarcloud\nMabl",
    "crumbs": [
      "Utviklere",
      "Lenker"
    ]
  },
  {
    "objectID": "utviklere/lenker.html#prosess",
    "href": "utviklere/lenker.html#prosess",
    "title": "Lenker",
    "section": "",
    "text": "Operasjonell modell\nIT Avdelingens leveranse demo (finner sted en gang i mÃ¥neden pÃ¥ fredag)",
    "crumbs": [
      "Utviklere",
      "Lenker"
    ]
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "SpÃ¸rsmÃ¥l og svar",
    "section": "",
    "text": "Prosjekt-ID-en til et Google-prosjekt er en unik identifikator som brukes til Ã¥ identifisere prosjektet i Google Cloud Platform. Prosjekt-ID-en er en streng som bestÃ¥r av smÃ¥ bokstaver, tall og bindestrek. Prosjekt-ID-en er ikke det samme som prosjektnavnet, som kan inneholde store bokstaver og mellomrom.\nDu finner prosjekt-ID ved logge deg inn pÃ¥ GCC, Ã¥pne prosjektvelgeren, sÃ¸k opp ditt prosjekt, og sÃ¥ ser du det i hÃ¸yre kolonne, slik som vist i denne sladdete kolonnen i FigurÂ 1.\n\n\n\n\n\n\nFigurÂ 1: Prosjektvelgeren i Google Cloud Console"
  },
  {
    "objectID": "faq.html#footnotes",
    "href": "faq.html#footnotes",
    "title": "SpÃ¸rsmÃ¥l og svar",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nFeaturen dapla-buckets inkluderer produkt- og kildebÃ¸tta i henholdsvis standard- og kildeprosjektet som de fleste statistikkproduserende team fÃ¥r ved opprettelse.â†©ï¸\nI produktbÃ¸tta blir noncurrent versjoner slettet hvis det er mer enn 2 nyere versjoner, mens for kildebÃ¸tta er grensen pÃ¥ 3 nyere versjonerâ†©ï¸"
  },
  {
    "objectID": "utviklere/dokumentere-for-backstage.html",
    "href": "utviklere/dokumentere-for-backstage.html",
    "title": "Hvordan dokumentere systemer for backstage?",
    "section": "",
    "text": "Backstage er en utviklerportal utviklet hos spotify. Vi drifter vÃ¥r egen instans pÃ¥ NAIS og du kan nÃ¥ den fra VPN, eller med naisdevice, pÃ¥ backstage.intern.ssb.no. Tech lead gruppen vedlikeholder techradaren vÃ¥r der, og det er bestemt at alle utviklingsteam hos SSB skal dokumentere sine systemer i backstage sin software katalog.\nÃ… dokumentere et system i backstage er ganske enkelt. Alt du trenger er kunnskap om formatet, sÃ¥ kommer du fort igang. Dokumentasjonen skal ligge sÃ¥ nÃ¦rme koden som mulig sÃ¥ den er enkel Ã¥ vedlikeholde for utviklerne.\nI dette dokumentet gÃ¥r vi gjennom hvordan backstage fungerer ved Ã¥ fÃ¸rst bli kjent med terminologien. Deretter bruker vi et fiktivt microdata-system som eksempel for hvordan vi dokumenterer de forskjellige entitetene. I bunnen av dokumentet finner du ogsÃ¥ SSB-spesifikke retningslinjer for backstage dokumenteringen.\nOm du har noen spÃ¸rsmÃ¥l som ikke blir besvart i lÃ¸pet av dette dokumentet, kontakt gjerne techlead gruppen pÃ¥ slack: #tech-lead-forum.\n\n\nFor Ã¥ dokumentere vÃ¥re systemer i backstage mÃ¥ vi vÃ¦re kjent med entitietene i backstage sin system modell. Her er en kort beskrivelse av hvordan vi bruker disse i SSB:\n\nUser: Er en enkelt ansatt som hentet fra vÃ¥r EntraID\nGroup: Er et team som hentet fra vÃ¥r EntraID f.eks.: microdata-developers\nDomain: Grupperer systemene under domener. Vi har valgt Ã¥ binde domenene til emnene i veikartet:\n\nformidling\ndapla\nfellesfunksjoner\n\nSystem: En samling software og ressurser som sammen utfÃ¸rer en funksjon\nComponent: Et stykke software i et system\nAPI: Et grensesnitt for kommunikasjon mellom komponenter\nResource: Et stykke fysisk eller virituell infrastruktur for som trengs for Ã¥ operere en komponent\n\n\n\n\nFor Ã¥ dokumentere entiteter (systemer, komponenter, apiâ€™er og ressurser) mÃ¥ vi til statisticsnorway sin github. Backstage gÃ¥r nemlig gjennom alle repoene vi eier jevnlig og sjekker om nye backstage-definisjoner har blitt postet. Alt man trenger for at backstage skal legge merke til definisjonene dine er Ã¥:\n\nSette backstage som topic i repoet. Du gjÃ¸r dette ved Ã¥ trykke pÃ¥ tannhjulet ved siden av About pÃ¥ repo siden.\nLegge en backstage.yaml fil i roten av repoet med en gyldig backstage definisjon\n\nLa oss ta for oss et fiktivt microdata-system for Ã¥ forklare hvordan man dokumenterer alle de forskjellige entitetene.\n\n\nSom sagt tidligere er users og groups hentet fra EntraID, og domenene er allerede definert sentralt. NÃ¥r vi starter Ã¥ dokumentere systemet vÃ¥rt er derfor det fÃ¸rste vi mÃ¥ starte med systemet selv.\nEttersom definisjonen av systemet selv ikke hÃ¸rer hjemme i noe spesielt repo, har vi valgt Ã¥ lage et repo statisticsnorway/microdata-docs der vi lagrer dokumentasjon som tilhÃ¸rer microdatasystemet som helhet. Om vi tagger dette repoet med backstage-taggen, kan vi definere systemet vÃ¥rt i roten av repoet med en backstage.yaml slik:\napiVersion: backstage.io/v1alpha1\nkind: System\nmetadata:\n  name: microdata\n  title: microdata\n  description: Tilgang pÃ¥ registerdata uten sÃ¸knadsprosess\n  links:\n    - title: microdata.no\n      url: https://microdata.no\n    - title: data-administrasjon\n      url: https://microdata.no/datastore-admin\n  tags:\n    - on-premises\n    - python\n    - typescript\nspec:\n  owner: microdata-developers\n  domain: formidling\nLa oss se pÃ¥ feltene og hva de betyr:\n\napiVersion: spesifiserer backstage sitt dokumentformat\nkind: Vi Ã¸nsker Ã¥ definere et System\n\n\n\n\nname: Navnet pÃ¥ systemet i kebab-case\ntitle: Menneskeleselig navn pÃ¥ Systemet\ndescription: En kort beskrivelse av systemet\nlinks: En liste med relevante lenker for systemet\ntags: En ustrukturert liste med emnemerker. Se retningslinjene for tags i SSB\n\n\n\n\n\nowner: Gruppen som eier systemet\ndomain: DomenetilhÃ¸rligheten til systemet\n\nEtter noen minutter, kan vi navigere til backstage websiden og se at systemet vÃ¥r har dukket opp.\n\n\n\n\nI microdata teamet publiserer vi et python bibliotek til PyPI. Denne pakken brukes av andre komponenter i systemet. La oss gÃ¥ til github repoet til komponenten statisticsnorway/microdata-tools, markere repoet med backstage som topic, og legge til en backstage.yaml i roten av repoet:\napiVersion: backstage.io/v1alpha1\nkind: Component\nmetadata:\n  name: microdata-tools\n  title: Microdata tools\n  description: |\n    Tools for packaging, encrypting and validating microdata datasets\n  annotations:\n    github.com/project-slug: statisticsnorway/microdata-tools\n  tags:\n    - python\n    - pyarrow\n    - pydantic\nspec:\n  type: library\n  system: microdata\n  owner: microdata-developers\n  lifecycle: production\n  dependencyOf:\n    - component:microdata-job-service\nMange av feltene ligner veldig for Ã¥ dokumentere en komponent. Dette er en komponent av type library. La oss se pÃ¥ hva feltene betyr:\n\n\n\nname: Navnet pÃ¥ komponenten i kebab-case Se retningslinjene for navn i SSB\ntitle: Menneskeleselig navn pÃ¥ komponenten\ndescription: En kort beskrivelse av komponenten\nannotations: Lokasjonen til komponenten pÃ¥ github\ntags: En ustrukturert liste med emnemerker. Se retningslinjene for tags i SSB\n\n\n\n\n\ntype: Hva slags type komponent dette er Se retningslinjene for type i SSB\nsystem: Systemet denne komponenten er en del av\nowner: Gruppen som eier kompoonenten\nlifecycle: MÃ¥ vÃ¦re satt til en av:\n\nexperimental: Om komponenten er under utvikling, men ikke blitt prodsatt\nproduction: Om komponenten er prodsatt og i drift\ndeprecated: Om komponenten er markert for nedleggelse\n\ndependencyOf: Her kan du spesifisere hvilke andre entiteter som er avhengige av denne komponenten. Det er viktig Ã¥ vite at om det finnes noen som bruker dette biblioteket, og marker seg selv som en avhengig av denne, vil det fortsatt registreres av backstage selv om denne avhengigheten ikke er til stedet under dependencyOf-feltet her.\n\n\n\n\n\nVi har et rest-api som kjÃ¸rer on-prem som vi kaller job-service. Job-service eksponerer et rest-api, og er avhengig av microdata-tools som avhengighet. Dette betyr at for Ã¥ representere job-service mÃ¥ vi bruke to komponenter i backstage-modellen. En komponent for Ã¥ beskrive tjenesten selv (type: service) og en api definisjon for Ã¥ beskrive grensesnittet. Vi kan beskrive flere entiteter i samme backstage.yaml ved Ã¥ putter tre bindestreker pÃ¥ en linje mellom definisjonene. La oss gÃ¥ til github repoet til komponenten statisticsnorway/microdata-job-service, markere repoet med backstage som topic, og legge til en backstage.yaml i roten av repoet:\napiVersion: backstage.io/v1alpha1\nkind: Component\nLa oss ta for oss et fiktivt microdata-system for Ã¥ forklare hvordan man dokumenterer alle de forskjellige entitetene.\nmetadata:\n  name: microdata-job-service\n  title:  Job service\n  description: |\n    Lookup service for jobs\n  tags:\n    - python\n    - flask\n    - pymongo\n  annotations:\n    github.com/project-slug: statisticsnorway/microdata-job-service\nspec:\n  type: service\n  system: microdata\n  owner: microdata-developers\n  lifecycle: production\n  providesApis:\n    - job-service-api\n  dependsOn:\n    - component:microdata-tools\n    - resource:microdata-job-db\n---\napiVersion: backstage.io/v1alpha1\nkind: API\nmetadata:\n  name: microdata-job-service-api\n  description: Job service\nspec:\n  type: openapi\n  system: microdata\n  owner: microdata-developers\n  lifecycle: production\n  definition:\n    $text: ./doc/openapi.yaml\nHer er mange av feltene for Component delen like som i sist eksempel, med unntak av:\n\nprovidesApis: Peker pÃ¥ grensesnitt ved en, eller flere, API entiteter denne komponenten eksponerer\ndependsOn: Peker pÃ¥ en eller flere komponenter og ressurser denne tjenesten er avhengig av\n\nFelter man kan ta i bruk som man ikke ser her er ogsÃ¥:\n\nConsumesApis: Peker pÃ¥ grensesnitt ved en, eller flere, API entiteter denne komponenten konsumerer\n\nFor API spesifikasjonen som finnes under ---:\n\n\n\nname: Navnet pÃ¥ APIet i kebab-case Se retningslinjene for navn i SSB\ntitle: Menneskeleselig navn pÃ¥ APIet\ndescription: En kort beskrivelse av APIet\n\n\n\n\n\ntype: Hva slags type grensesnitt dette APIet er. Se retningslinjene for type i SSB\nsystem: Systemet dette APIet er en del av\nowner: Gruppen som eier APIet\nlifecycle: MÃ¥ vÃ¦re satt til en av:\n\nexperimental: Om komponenten er under utvikling, men ikke blitt prodsatt\nproduction: Om komponenten er prodsatt og i drift\ndeprecated: Om komponenten er markert for nedleggelse\n\ndefinition: Om dette er et openapi kan det defineres med openapi formatet i en annen fil. Oppgi path til denne filen i samme repo med $text:\n\nVi fÃ¥r nÃ¥ se avhengigheter tydelig i grafene som backstage generer. Vi kan ogsÃ¥ undersÃ¸ke API definisjonene i backstage websiden ved Ã¥ gÃ¥ til â€œdefinitionâ€ fanen i API ressursen vi har definert.\n\n\n\n\nI microdata.no drifter vi ogsÃ¥ en mongodb som vi sÃ¥ over at job-service var avhengig av. Mongodb er en database-ressurs. Vi har et repo der vi bygger imaget til denne databasen, men her kunne du lagt ved ressursdefinisjonen sammen med applikasjonen eller i iac-repoet til teamen. Backstage definisjonen bÃ¸r vÃ¦re sÃ¥ nÃ¦rme den aktuelle koden som mulig, sÃ¥ tenk pragmatisk pÃ¥ det beste stedet Ã¥ legge den. I dette tilfellet gÃ¥r vi til github repoet til ressursen statisticsnorway/microdata-job-db, markere repoet med backstage som topic, og legge til en backstage.yaml i roten av repoet:\napiVersion: backstage.io/v1alpha1\nkind: Resource\nmetadata:\n  name: microdata-job-db\n  description: |\n    MongoDB that stores jobs and job information in the microdata platform\n  tags:\n    - mongodb\n  annotations:\n    github.com/project-slug: statisticsnorway/microdata-job-db\nspec:\n  type: database\n  owner: microdata-developers\n  lifecycle: production\n  system: microdata\n\n\n\nname: Navnet pÃ¥ ressursen i kebab-case Se retningslinjene for navn i SSB\ntitle: Menneskeleselig navn pÃ¥ ressursen\ndescription: En kort beskrivelse av ressursen\n\n\n\n\n\ntype: Hva slags type ressurs dette er. Se retningslinjene for type i SSB\nsystem: Systemet denne ressursen er en del av\nowner: Gruppen som eier ressursen\nlifecycle: MÃ¥ vÃ¦re satt til en av:\n\nexperimental: Om komponenten er under utvikling, men ikke blitt prodsatt\nproduction: Om komponenten er prodsatt og i drift\ndeprecated: Om komponenten er markert for nedleggelse\n\n\nDa har vi dokumentert alle de forskjellige entitetene vi trenger i backstage. Det er viktig at vi opprettholder denne informasjonen for en bedre utvikleropplevelse i SSB. Om du har noen spÃ¸rsmÃ¥l angÃ¥ende denne guiden, eller lurer pÃ¥ noe angÃ¥ende backstage; ta kontakt med tech lead gruppen pÃ¥ slack under #tech-lead-forum.\n\n\n\n\n\n\n\nFor Ã¥ forsikre sÃ¸kbarhet og god kommunikasjon er det viktig at vi bruker samme sprÃ¥k for Ã¥ beskrive systemene vÃ¥re. Det er anbefalt av backstage at organisasjonen tar stilling til bruk av type-feltet. Vi forsÃ¸ker Ã¥ holde mengden definisjoner til et minimum, og bruker samme terminologi som NAIS nÃ¥r vi har mulighet. Om du mener det mangler en type i listene her, ta gjerne kontakt med techlead-teamet pÃ¥ slack for diskusjon, eller post en pull request med forslaget til denne dokumentasjonen.\n\n\nFor type pÃ¥ komponenter skal kun en av disse brukes:\n\nservice: For langtlevende tjenester\nlibrary: For biblioteker/pakker som eksponeres pÃ¥ maven/pypi/crates el.\njob: For applikasjoner som er ment Ã¥ kjÃ¸res pÃ¥ et skjema, one-shot eller pÃ¥ en trigger\nwebsite: For applikasjoner som skal eksponeres med browser\n\n\n\n\nFor type pÃ¥ APIer skal kun en av disse typene brukes:\n\nopenapi: Dette gjÃ¸r at apiâ€™et kan dokumenteres med openapi dokumentasjon\n\n\n\n\nFor type pÃ¥ ressurser skal kun en av disse brukes:\n\ndatabase: for alle databaser\nbucket: for bÃ¸tter\nqueue: for meldingskÃ¸er som pub/sub og kafka\n\n\n\n\n\nPÃ¥ samme mÃ¥te Ã¸nsker vi at alle tagger sine systemer pÃ¥ en konsistent mÃ¥te.\n\n\nTags for et system skal BARE inneholde:\n\nHvor systemet kjÃ¸rer. f.eks.: on-premises, bip, nais\nProgrammeringssprÃ¥kene brukt i systemet f.eks.: python, kotlin, rust\n\n\n\n\nTags for komponenter skal BARE inneholde:\n\nProgrammeringssprÃ¥kene brukt i systemet f.eks.: python, kotlin, rust\nKjerneteknologier brukt i komponenten f.eks.: micronaut, flask, pyarrow\n\n\n\n\nTags for ressurser kan BARE inneholde:\n\nSpesifisering av teknologi. ex.: postgresql, mongodb, pubsub\n\n\n\n\n\nFor at komponenter og ressurser skal kunne kobles sammen og vises korrekt i avheninghetsgrafen, sÃ¥ er vi avhengig av at det er unike tekniske navn pÃ¥ disse pÃ¥ tvers av Systemer i Backstage. Dette gjÃ¸r vi enklest ved Ã¥ prefikse med Systemet de tilhÃ¸rer: name: &lt;system&gt;-&lt;navn&gt;.\n\n\n\nMan kan validere at Backstage yaml filer er gyldige vha. fÃ¸lgende kommando:\nnpx @roadiehq/backstage-entity-validator backstage.yaml\n\n\n\n\n\nBackstage\nBackstage Docs: The system model\nADR for bruk av backstage i SBB",
    "crumbs": [
      "Utviklere",
      "Hvordan dokumentere systemer for backstage?"
    ]
  },
  {
    "objectID": "utviklere/dokumentere-for-backstage.html#beskrivelse-av-entiteter-i-backstage",
    "href": "utviklere/dokumentere-for-backstage.html#beskrivelse-av-entiteter-i-backstage",
    "title": "Hvordan dokumentere systemer for backstage?",
    "section": "",
    "text": "For Ã¥ dokumentere vÃ¥re systemer i backstage mÃ¥ vi vÃ¦re kjent med entitietene i backstage sin system modell. Her er en kort beskrivelse av hvordan vi bruker disse i SSB:\n\nUser: Er en enkelt ansatt som hentet fra vÃ¥r EntraID\nGroup: Er et team som hentet fra vÃ¥r EntraID f.eks.: microdata-developers\nDomain: Grupperer systemene under domener. Vi har valgt Ã¥ binde domenene til emnene i veikartet:\n\nformidling\ndapla\nfellesfunksjoner\n\nSystem: En samling software og ressurser som sammen utfÃ¸rer en funksjon\nComponent: Et stykke software i et system\nAPI: Et grensesnitt for kommunikasjon mellom komponenter\nResource: Et stykke fysisk eller virituell infrastruktur for som trengs for Ã¥ operere en komponent",
    "crumbs": [
      "Utviklere",
      "Hvordan dokumentere systemer for backstage?"
    ]
  },
  {
    "objectID": "utviklere/dokumentere-for-backstage.html#dokumentasjon",
    "href": "utviklere/dokumentere-for-backstage.html#dokumentasjon",
    "title": "Hvordan dokumentere systemer for backstage?",
    "section": "",
    "text": "For Ã¥ dokumentere entiteter (systemer, komponenter, apiâ€™er og ressurser) mÃ¥ vi til statisticsnorway sin github. Backstage gÃ¥r nemlig gjennom alle repoene vi eier jevnlig og sjekker om nye backstage-definisjoner har blitt postet. Alt man trenger for at backstage skal legge merke til definisjonene dine er Ã¥:\n\nSette backstage som topic i repoet. Du gjÃ¸r dette ved Ã¥ trykke pÃ¥ tannhjulet ved siden av About pÃ¥ repo siden.\nLegge en backstage.yaml fil i roten av repoet med en gyldig backstage definisjon\n\nLa oss ta for oss et fiktivt microdata-system for Ã¥ forklare hvordan man dokumenterer alle de forskjellige entitetene.\n\n\nSom sagt tidligere er users og groups hentet fra EntraID, og domenene er allerede definert sentralt. NÃ¥r vi starter Ã¥ dokumentere systemet vÃ¥rt er derfor det fÃ¸rste vi mÃ¥ starte med systemet selv.\nEttersom definisjonen av systemet selv ikke hÃ¸rer hjemme i noe spesielt repo, har vi valgt Ã¥ lage et repo statisticsnorway/microdata-docs der vi lagrer dokumentasjon som tilhÃ¸rer microdatasystemet som helhet. Om vi tagger dette repoet med backstage-taggen, kan vi definere systemet vÃ¥rt i roten av repoet med en backstage.yaml slik:\napiVersion: backstage.io/v1alpha1\nkind: System\nmetadata:\n  name: microdata\n  title: microdata\n  description: Tilgang pÃ¥ registerdata uten sÃ¸knadsprosess\n  links:\n    - title: microdata.no\n      url: https://microdata.no\n    - title: data-administrasjon\n      url: https://microdata.no/datastore-admin\n  tags:\n    - on-premises\n    - python\n    - typescript\nspec:\n  owner: microdata-developers\n  domain: formidling\nLa oss se pÃ¥ feltene og hva de betyr:\n\napiVersion: spesifiserer backstage sitt dokumentformat\nkind: Vi Ã¸nsker Ã¥ definere et System\n\n\n\n\nname: Navnet pÃ¥ systemet i kebab-case\ntitle: Menneskeleselig navn pÃ¥ Systemet\ndescription: En kort beskrivelse av systemet\nlinks: En liste med relevante lenker for systemet\ntags: En ustrukturert liste med emnemerker. Se retningslinjene for tags i SSB\n\n\n\n\n\nowner: Gruppen som eier systemet\ndomain: DomenetilhÃ¸rligheten til systemet\n\nEtter noen minutter, kan vi navigere til backstage websiden og se at systemet vÃ¥r har dukket opp.\n\n\n\n\nI microdata teamet publiserer vi et python bibliotek til PyPI. Denne pakken brukes av andre komponenter i systemet. La oss gÃ¥ til github repoet til komponenten statisticsnorway/microdata-tools, markere repoet med backstage som topic, og legge til en backstage.yaml i roten av repoet:\napiVersion: backstage.io/v1alpha1\nkind: Component\nmetadata:\n  name: microdata-tools\n  title: Microdata tools\n  description: |\n    Tools for packaging, encrypting and validating microdata datasets\n  annotations:\n    github.com/project-slug: statisticsnorway/microdata-tools\n  tags:\n    - python\n    - pyarrow\n    - pydantic\nspec:\n  type: library\n  system: microdata\n  owner: microdata-developers\n  lifecycle: production\n  dependencyOf:\n    - component:microdata-job-service\nMange av feltene ligner veldig for Ã¥ dokumentere en komponent. Dette er en komponent av type library. La oss se pÃ¥ hva feltene betyr:\n\n\n\nname: Navnet pÃ¥ komponenten i kebab-case Se retningslinjene for navn i SSB\ntitle: Menneskeleselig navn pÃ¥ komponenten\ndescription: En kort beskrivelse av komponenten\nannotations: Lokasjonen til komponenten pÃ¥ github\ntags: En ustrukturert liste med emnemerker. Se retningslinjene for tags i SSB\n\n\n\n\n\ntype: Hva slags type komponent dette er Se retningslinjene for type i SSB\nsystem: Systemet denne komponenten er en del av\nowner: Gruppen som eier kompoonenten\nlifecycle: MÃ¥ vÃ¦re satt til en av:\n\nexperimental: Om komponenten er under utvikling, men ikke blitt prodsatt\nproduction: Om komponenten er prodsatt og i drift\ndeprecated: Om komponenten er markert for nedleggelse\n\ndependencyOf: Her kan du spesifisere hvilke andre entiteter som er avhengige av denne komponenten. Det er viktig Ã¥ vite at om det finnes noen som bruker dette biblioteket, og marker seg selv som en avhengig av denne, vil det fortsatt registreres av backstage selv om denne avhengigheten ikke er til stedet under dependencyOf-feltet her.\n\n\n\n\n\nVi har et rest-api som kjÃ¸rer on-prem som vi kaller job-service. Job-service eksponerer et rest-api, og er avhengig av microdata-tools som avhengighet. Dette betyr at for Ã¥ representere job-service mÃ¥ vi bruke to komponenter i backstage-modellen. En komponent for Ã¥ beskrive tjenesten selv (type: service) og en api definisjon for Ã¥ beskrive grensesnittet. Vi kan beskrive flere entiteter i samme backstage.yaml ved Ã¥ putter tre bindestreker pÃ¥ en linje mellom definisjonene. La oss gÃ¥ til github repoet til komponenten statisticsnorway/microdata-job-service, markere repoet med backstage som topic, og legge til en backstage.yaml i roten av repoet:\napiVersion: backstage.io/v1alpha1\nkind: Component\nLa oss ta for oss et fiktivt microdata-system for Ã¥ forklare hvordan man dokumenterer alle de forskjellige entitetene.\nmetadata:\n  name: microdata-job-service\n  title:  Job service\n  description: |\n    Lookup service for jobs\n  tags:\n    - python\n    - flask\n    - pymongo\n  annotations:\n    github.com/project-slug: statisticsnorway/microdata-job-service\nspec:\n  type: service\n  system: microdata\n  owner: microdata-developers\n  lifecycle: production\n  providesApis:\n    - job-service-api\n  dependsOn:\n    - component:microdata-tools\n    - resource:microdata-job-db\n---\napiVersion: backstage.io/v1alpha1\nkind: API\nmetadata:\n  name: microdata-job-service-api\n  description: Job service\nspec:\n  type: openapi\n  system: microdata\n  owner: microdata-developers\n  lifecycle: production\n  definition:\n    $text: ./doc/openapi.yaml\nHer er mange av feltene for Component delen like som i sist eksempel, med unntak av:\n\nprovidesApis: Peker pÃ¥ grensesnitt ved en, eller flere, API entiteter denne komponenten eksponerer\ndependsOn: Peker pÃ¥ en eller flere komponenter og ressurser denne tjenesten er avhengig av\n\nFelter man kan ta i bruk som man ikke ser her er ogsÃ¥:\n\nConsumesApis: Peker pÃ¥ grensesnitt ved en, eller flere, API entiteter denne komponenten konsumerer\n\nFor API spesifikasjonen som finnes under ---:\n\n\n\nname: Navnet pÃ¥ APIet i kebab-case Se retningslinjene for navn i SSB\ntitle: Menneskeleselig navn pÃ¥ APIet\ndescription: En kort beskrivelse av APIet\n\n\n\n\n\ntype: Hva slags type grensesnitt dette APIet er. Se retningslinjene for type i SSB\nsystem: Systemet dette APIet er en del av\nowner: Gruppen som eier APIet\nlifecycle: MÃ¥ vÃ¦re satt til en av:\n\nexperimental: Om komponenten er under utvikling, men ikke blitt prodsatt\nproduction: Om komponenten er prodsatt og i drift\ndeprecated: Om komponenten er markert for nedleggelse\n\ndefinition: Om dette er et openapi kan det defineres med openapi formatet i en annen fil. Oppgi path til denne filen i samme repo med $text:\n\nVi fÃ¥r nÃ¥ se avhengigheter tydelig i grafene som backstage generer. Vi kan ogsÃ¥ undersÃ¸ke API definisjonene i backstage websiden ved Ã¥ gÃ¥ til â€œdefinitionâ€ fanen i API ressursen vi har definert.\n\n\n\n\nI microdata.no drifter vi ogsÃ¥ en mongodb som vi sÃ¥ over at job-service var avhengig av. Mongodb er en database-ressurs. Vi har et repo der vi bygger imaget til denne databasen, men her kunne du lagt ved ressursdefinisjonen sammen med applikasjonen eller i iac-repoet til teamen. Backstage definisjonen bÃ¸r vÃ¦re sÃ¥ nÃ¦rme den aktuelle koden som mulig, sÃ¥ tenk pragmatisk pÃ¥ det beste stedet Ã¥ legge den. I dette tilfellet gÃ¥r vi til github repoet til ressursen statisticsnorway/microdata-job-db, markere repoet med backstage som topic, og legge til en backstage.yaml i roten av repoet:\napiVersion: backstage.io/v1alpha1\nkind: Resource\nmetadata:\n  name: microdata-job-db\n  description: |\n    MongoDB that stores jobs and job information in the microdata platform\n  tags:\n    - mongodb\n  annotations:\n    github.com/project-slug: statisticsnorway/microdata-job-db\nspec:\n  type: database\n  owner: microdata-developers\n  lifecycle: production\n  system: microdata\n\n\n\nname: Navnet pÃ¥ ressursen i kebab-case Se retningslinjene for navn i SSB\ntitle: Menneskeleselig navn pÃ¥ ressursen\ndescription: En kort beskrivelse av ressursen\n\n\n\n\n\ntype: Hva slags type ressurs dette er. Se retningslinjene for type i SSB\nsystem: Systemet denne ressursen er en del av\nowner: Gruppen som eier ressursen\nlifecycle: MÃ¥ vÃ¦re satt til en av:\n\nexperimental: Om komponenten er under utvikling, men ikke blitt prodsatt\nproduction: Om komponenten er prodsatt og i drift\ndeprecated: Om komponenten er markert for nedleggelse\n\n\nDa har vi dokumentert alle de forskjellige entitetene vi trenger i backstage. Det er viktig at vi opprettholder denne informasjonen for en bedre utvikleropplevelse i SSB. Om du har noen spÃ¸rsmÃ¥l angÃ¥ende denne guiden, eller lurer pÃ¥ noe angÃ¥ende backstage; ta kontakt med tech lead gruppen pÃ¥ slack under #tech-lead-forum.",
    "crumbs": [
      "Utviklere",
      "Hvordan dokumentere systemer for backstage?"
    ]
  },
  {
    "objectID": "utviklere/dokumentere-for-backstage.html#retningslinjer",
    "href": "utviklere/dokumentere-for-backstage.html#retningslinjer",
    "title": "Hvordan dokumentere systemer for backstage?",
    "section": "",
    "text": "For Ã¥ forsikre sÃ¸kbarhet og god kommunikasjon er det viktig at vi bruker samme sprÃ¥k for Ã¥ beskrive systemene vÃ¥re. Det er anbefalt av backstage at organisasjonen tar stilling til bruk av type-feltet. Vi forsÃ¸ker Ã¥ holde mengden definisjoner til et minimum, og bruker samme terminologi som NAIS nÃ¥r vi har mulighet. Om du mener det mangler en type i listene her, ta gjerne kontakt med techlead-teamet pÃ¥ slack for diskusjon, eller post en pull request med forslaget til denne dokumentasjonen.\n\n\nFor type pÃ¥ komponenter skal kun en av disse brukes:\n\nservice: For langtlevende tjenester\nlibrary: For biblioteker/pakker som eksponeres pÃ¥ maven/pypi/crates el.\njob: For applikasjoner som er ment Ã¥ kjÃ¸res pÃ¥ et skjema, one-shot eller pÃ¥ en trigger\nwebsite: For applikasjoner som skal eksponeres med browser\n\n\n\n\nFor type pÃ¥ APIer skal kun en av disse typene brukes:\n\nopenapi: Dette gjÃ¸r at apiâ€™et kan dokumenteres med openapi dokumentasjon\n\n\n\n\nFor type pÃ¥ ressurser skal kun en av disse brukes:\n\ndatabase: for alle databaser\nbucket: for bÃ¸tter\nqueue: for meldingskÃ¸er som pub/sub og kafka\n\n\n\n\n\nPÃ¥ samme mÃ¥te Ã¸nsker vi at alle tagger sine systemer pÃ¥ en konsistent mÃ¥te.\n\n\nTags for et system skal BARE inneholde:\n\nHvor systemet kjÃ¸rer. f.eks.: on-premises, bip, nais\nProgrammeringssprÃ¥kene brukt i systemet f.eks.: python, kotlin, rust\n\n\n\n\nTags for komponenter skal BARE inneholde:\n\nProgrammeringssprÃ¥kene brukt i systemet f.eks.: python, kotlin, rust\nKjerneteknologier brukt i komponenten f.eks.: micronaut, flask, pyarrow\n\n\n\n\nTags for ressurser kan BARE inneholde:\n\nSpesifisering av teknologi. ex.: postgresql, mongodb, pubsub\n\n\n\n\n\nFor at komponenter og ressurser skal kunne kobles sammen og vises korrekt i avheninghetsgrafen, sÃ¥ er vi avhengig av at det er unike tekniske navn pÃ¥ disse pÃ¥ tvers av Systemer i Backstage. Dette gjÃ¸r vi enklest ved Ã¥ prefikse med Systemet de tilhÃ¸rer: name: &lt;system&gt;-&lt;navn&gt;.\n\n\n\nMan kan validere at Backstage yaml filer er gyldige vha. fÃ¸lgende kommando:\nnpx @roadiehq/backstage-entity-validator backstage.yaml",
    "crumbs": [
      "Utviklere",
      "Hvordan dokumentere systemer for backstage?"
    ]
  },
  {
    "objectID": "utviklere/dokumentere-for-backstage.html#lenker",
    "href": "utviklere/dokumentere-for-backstage.html#lenker",
    "title": "Hvordan dokumentere systemer for backstage?",
    "section": "",
    "text": "Backstage\nBackstage Docs: The system model\nADR for bruk av backstage i SBB",
    "crumbs": [
      "Utviklere",
      "Hvordan dokumentere systemer for backstage?"
    ]
  },
  {
    "objectID": "blog/posts/2025-02-07-parquet-viewer/index.html",
    "href": "blog/posts/2025-02-07-parquet-viewer/index.html",
    "title": "Parquet-utforsker i VS Code",
    "section": "",
    "text": "I Dapla Lab tjenesten Vscode-python er nÃ¥ extensionâ€™en vscode-parquet-visualizer installert. Den lar deg Ã¥pne en Parquet-fil uten bruk av Python- eller R-biblioteker. I tillegg lar den deg spÃ¸rre mot datasettet med SQL, filtrere dataettet uten kode, sortere kolonner, gir deg metadata om kolonner og datasett, og gir en forhÃ¥ndsvisning av komplekse celler.\nI videoen under ser man hvordan man Ã¥pner en Parquet-fil i en bÃ¸tte fra et ssb-project. Filen som Ã¥pnes har 4 kolonner, 5 millioner rader og er pÃ¥ 85 megabytes."
  },
  {
    "objectID": "blog/posts/2025-02-07-parquet-viewer/index.html#bruksomrÃ¥de",
    "href": "blog/posts/2025-02-07-parquet-viewer/index.html#bruksomrÃ¥de",
    "title": "Parquet-utforsker i VS Code",
    "section": "BruksomrÃ¥de",
    "text": "BruksomrÃ¥de\nBruksomrÃ¥det for denne funksjonaliteten er Ã¥ utforske Parquet-filer og ikke prosessere data i produksjon. Selv om man kan skrive ut et filtrert datasett med lÃ¸sningen, sÃ¥ skal det ikke benyttes til prosessering som skal vÃ¦re reproduserbar siden det ikke dokumenteres med kode."
  },
  {
    "objectID": "blog/posts/2025-02-07-parquet-viewer/index.html#skrive-sql",
    "href": "blog/posts/2025-02-07-parquet-viewer/index.html#skrive-sql",
    "title": "Parquet-utforsker i VS Code",
    "section": "Skrive SQL",
    "text": "Skrive SQL\nSQL-en som skrives mÃ¥ vÃ¦re duckdbsql siden det er dette extensionâ€™en benytter for Ã¥ hente informasjon fra Parquet-filen."
  },
  {
    "objectID": "blog/posts/2025-04-04-laste-til-statbank/index.html",
    "href": "blog/posts/2025-04-04-laste-til-statbank/index.html",
    "title": "Hvordan laste mange tabeller i Statistikkbanken?",
    "section": "",
    "text": "Filbeskrivelsene kan lagres som json med en metode, og Ã¥pnes med en annen (uten Ã¥ bruke passord). Om man skal jobbe med mange statbanktabeller i ett lÃ¸p, sÃ¥ kan en ok inndeling av notebooks vÃ¦re:\n\nEnkelt notebook for Ã¥ hente alle filbeskrivelser (krever lastepassord)\nEtt notebook per statbanktabell som omformer fra â€œstatistikkfilâ€ til det som skal sendes til statbanken. Kan bruke filbeskrivelsen til avrunding, validering. (krever ikke lastepassord)\nEnkelt notebook for Ã¥ sende alle tabellene til statbanken (krever lastepassord)\n\nMan kan i teorien lagre filbeskrivelses-jsonfilene hvor man vil, de er ikke Ã¥ anse som sensitive, men de kan utlÃ¸pe pÃ¥ dato nÃ¥r metadata i statbanken blir oppdatert. Det kan derfor anbefales at du lagrer dem i Dapla-lab instansen din, dvs. i /work utenfor git-repoet du har klonet. De vil da bli slettet nÃ¥r du sletter tjenesten din, og det kan vÃ¦re Ã¸nskelig sÃ¥ de mÃ¥ hentes pÃ¥ nytt neste gang.\nEtt eksempel pÃ¥ dette ligger i Vgogjen-produksjonslÃ¸pet.\nFÃ¸rst oppretter du en notebook for Ã¥ hente alle filbeskrivelser:\n\n\nnotebook\n\n# Notebook for Ã¥ hente alle filbeskrivelser\nclient = StatbankClient()  # Krever passord\nfor tab_id in config.statbanktabell_ider:\n    filbesk = client.get_description(tab_nr)\n    filbesk.to_json(f\"~/work/filbesk/{tab_nr}.json\")  # SÃ¸rg for at mappe finnes fÃ¸rst kanskje\n\nDeretter kan du opprette en notebook per statistikkbanktabell. Under er et eksempel pÃ¥ hvordan en slik notebook kan se ut:\n\n\nnotebook\n\n# Notebooks for alle statbanktabeller \ntab_nr = \"12958\"\nfilbesk = StatbankClient.read_description_json(f\"filbesk/{tab_nr}.json\")  # Krever ikke passord\ndata = filbesk.transferdata_template(tab_ut)\nfilbesk.validate(data)\ndata = filbesk.round_data(data)\nfor datfil in filbesk.subtables.keys():\n    path = (path_root + \"VG_gjforing/\" + datfil).replace(\".dat\", \".parquet\")\n    data[datfil].to_parquet(path)\n\nTil slutt oppretter du en egen notebook som laster alle tabellene til Statistikkbanken:\n\n\nnotebook\n\n# Notebook for Ã¥ laste alle tabellene \nclient = StatbankClient(date=config.publiseringsdato)  # Krever passord\nfor tab_id in config.statbanktabell_ider:\n    filbesk = client.read_description_json(f\"~/work/filbesk/{tab_nr}.json\")\n    data = {}\n    for datfil in filbesk.subtables.keys():\n        fil = (path_root + \"VG_gjforing/\" + datfil).replace(\".dat\", \".parquet\")\n        data[datfil] = pd.read_parquet(fil)\n        client.transfer(data, tabell)"
  },
  {
    "objectID": "blog/posts/2025-04-25-function-use/index.html",
    "href": "blog/posts/2025-04-25-function-use/index.html",
    "title": "Bruk av funksjoner",
    "section": "",
    "text": "Mange er godt i gang med Ã¥ kode produksjonslÃ¸pene sine i Python og R. Mye virker og er bra, men fortsatt er det mange steder lite bruk av funksjoner. Gruppen for Kvalitet i Kode og Koding (KVAKK) i SSB Ã¸nsker derfor med denne artikkelen Ã¥ oppfordre til Ã¸kt bruk av funksjoner i statistikkproduksjon."
  },
  {
    "objectID": "blog/posts/2025-04-25-function-use/index.html#hvorfor-bruke-funksjoner",
    "href": "blog/posts/2025-04-25-function-use/index.html#hvorfor-bruke-funksjoner",
    "title": "Bruk av funksjoner",
    "section": "Hvorfor bruke funksjoner?",
    "text": "Hvorfor bruke funksjoner?\nProfesjonell kode, bortsett fra i skript, skrives nesten utelukkende innenfor funksjoner1 og klasser. Noen av Ã¥rsakene til dette er:\n\nGjenbruk: Ting som gjÃ¸res flere ganger bÃ¸r skilles ut til funksjoner for Ã¥ unngÃ¥ duplisert kode. Da kan retting og forbedring gjÃ¸res ett sted, i stedet for flere steder.\nMuliggjÃ¸r automatisert testing: Kode utenfor funksjoner er det vanskelig Ã¥ lage automatiserte tester for. Kode du Ã¸nsker Ã¥ teste automatisk bÃ¸r derfor ligge i funksjoner. PÃ¥ sikt bÃ¸r vi erstatte mest mulig av manuell testing med automatiske tester.\nEnklere Ã¥ forstÃ¥ og forklare: NÃ¥r koden er organisert i smÃ¥ funksjoner med tydelige navn, blir det lettere for deg og kollegaene dine Ã¥ forstÃ¥ hva koden gjÃ¸r, og hver bit blir mindre kompleks. Tenk pÃ¥ funksjoner som Â«byggeklosserÂ»: Ã©n funksjon gjÃ¸r Ã©n ting, og navnet pÃ¥ funksjonen forteller tydelig hva denne tingen er. Det blir som Ã¥ skrive en god innholdsfortegnelse i et dokument, slik at du raskt ser hva hvert avsnitt handler om.\nAutomatisert kjÃ¸ring av mange notebooks: Bygger du opp koden din med funksjoner sÃ¥ kan du automatisere kjÃ¸ring av alt sammen, uten Ã¥ bruke Papermill, som har problemer med ytelse."
  },
  {
    "objectID": "blog/posts/2025-04-25-function-use/index.html#hvordan-endre-fra-vanlig-notebook-til-bruk-av-funksjoner",
    "href": "blog/posts/2025-04-25-function-use/index.html#hvordan-endre-fra-vanlig-notebook-til-bruk-av-funksjoner",
    "title": "Bruk av funksjoner",
    "section": "Hvordan endre fra â€œvanligâ€ notebook til bruk av funksjoner?",
    "text": "Hvordan endre fra â€œvanligâ€ notebook til bruk av funksjoner?\nKVAKK har laget en beskrivelse som viser hvordan du kan gÃ¥ fra en â€œvanligâ€ notebook, hvor koden kjÃ¸res fra topp til bunn, uten bruk av funksjoner, og automatisert ved bruk av Papermill, og til en tilsvarende notebook med bruk av funksjoner.\nSe Hvordan automatisere Jupyter notebooks ved bruk av funksjoner."
  },
  {
    "objectID": "blog/posts/2025-04-25-function-use/index.html#verktÃ¸y-for-Ã¥-sjekke-funksjonsbruk",
    "href": "blog/posts/2025-04-25-function-use/index.html#verktÃ¸y-for-Ã¥-sjekke-funksjonsbruk",
    "title": "Bruk av funksjoner",
    "section": "VerktÃ¸y for Ã¥ sjekke funksjonsbruk",
    "text": "VerktÃ¸y for Ã¥ sjekke funksjonsbruk\nTech-coachene pÃ¥ seksjon IT-partner har laget et skript, analyze_function_use.py, du kan bruke til Ã¥ sjekke hvor mye bruk av funksjoner det er i python-koden i et repo. Det gir deg andelen av koden som ligger utenfor funksjoner, samt antall funksjoner.\nSkriptet har ingen avhengigheter, sÃ¥ du kan kjÃ¸re det uten Ã¥ installere noe ekstra.\nSkriptet analyserer python-filer i et clonet GitHub-repo. For hver fil teller den opp antall kodelinjer som er innenfor en funksjonsblokk (eller metode, som er det navnet som brukes pÃ¥ funksjoner som hÃ¸rer til en klasse), og ogsÃ¥ totalt antall kodelinjer i filen. Dette summerer den for alle filer i repoet og beregner andelen av koden som er utenfor funksjonsblokker. Den dropper blanke linjer, kommentarer og filer som ligger tests-katalogen. I tillegg teller den opp antall funksjoner i repoet."
  },
  {
    "objectID": "blog/posts/2025-04-25-function-use/index.html#eksempler",
    "href": "blog/posts/2025-04-25-function-use/index.html#eksempler",
    "title": "Bruk av funksjoner",
    "section": "Eksempler",
    "text": "Eksempler\nEt par eksempler pÃ¥ statistikk-repoer som er gode til Ã¥ bruke funksjoner er stat-bygganlprod og stat-mnd-el pÃ¥ seksjon 422. Ta gjerne en titt pÃ¥ disse. Den fÃ¸rste har kun 4 % av koden utenfor funksjoner og er liten, mens den andre har 634 funksjoner og er en stÃ¸rre statistikk.\nTa gjerne ogsÃ¥ en titt pÃ¥ tech-coach-stat-repoet, som er en eksempelstatistikk som tech-coachâ€™ene har begynt pÃ¥, og som viser mye god kodepraksis."
  },
  {
    "objectID": "blog/posts/2025-04-25-function-use/index.html#footnotes",
    "href": "blog/posts/2025-04-25-function-use/index.html#footnotes",
    "title": "Bruk av funksjoner",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nMed funksjoner menes her ogsÃ¥ metoder i klasser.â†©ï¸"
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html",
    "title": "Fra Fame til Python",
    "section": "",
    "text": "Mange i SSB har data lagret i Fame som de Ã¸nsker Ã¥ bearbeide med Python og R. Dette er spesielt relevant nÃ¥r man skal flytte statistikkproduksjon til Dapla. fython er en Python-pakke som gjÃ¸r dette pÃ¥ en enkel mÃ¥te for deg. Den lar deg eksportere data fra Fame med en enkel funksjon, og kan returnere dataene som enten CSV eller Pandas DataFrame.\nPakken finner du pÃ¥ GitHub."
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#installasjon",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#installasjon",
    "title": "Fra Fame til Python",
    "section": "Installasjon",
    "text": "Installasjon\nPakken er avhengig av at Fame er installert miljÃ¸et der den benyttes. Siden den er installert pÃ¥ sl-fame-1.ssb.no1 sÃ¥ vil de fÃ¦rreste har behov for Ã¥ installere den selv.\nSkulle du likevel Ã¸nske Ã¥ installere pakken selv kan det gjÃ¸res med Poetry pÃ¥ fÃ¸lgende mÃ¥te:\n\n\nterminal\n\npoetry add git+https://github.com/statisticsnorway/ssb-fame-to-python.git"
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#bruk-av-funksjonene",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#bruk-av-funksjonene",
    "title": "Fra Fame til Python",
    "section": "Bruk av funksjonene",
    "text": "Bruk av funksjonene\nfython har to funksjoner: fame_to_csv og fame_to_df. Begge disse funksjonene tar inn de samme argumentene og de er listet opp i TabellÂ 1.\n\n\n\nTabellÂ 1: Forklaring av argumentene i funksjonene til fython\n\n\n\n\n\n\n\n\n\n\n\nArgument\nForklaring\nfame_to_csv()\nfame_to_pandas()\n\n\n\n\ndatabases\nList of Fame databases to access (with full path).\nâœ“\nâœ“\n\n\nfrequency\nFrequency of the data (â€˜aâ€™, â€˜qâ€™, â€˜mâ€™).\nâœ“\nâœ“\n\n\ndate_from\nStart date for the data in Fame syntax (e.g., â€˜2023:1â€™ for quarterly, â€˜2023â€™ for annual).\nâœ“\nâœ“\n\n\ndate_to\nEnd date for the data in Fame syntax (e.g., â€˜2023:1â€™ for quarterly, â€˜2023â€™ for annual).\nâœ“\nâœ“\n\n\nsearch_string\nQuery string for fetching specific data. The search is not case sensitive, and â€œ^â€ and â€œ?â€ are wildcards (for exactly one and any number of characters, respectively)\nâœ“\nâœ“\n\n\ndecimals\nNumber of decimal places in the fetched data (default is 10).\nâœ“\nâœ“\n\n\npath\nPath to write the csv-file.\nâœ“\n\n\n\n\n\n\n\nLa se pÃ¥ noen eksempler.\n\nEksempler\nDersom vi Ã¸nsker Ã¥ hente alt i database1.db og database2.db fra januar 2012 til desember 2022, og fÃ¥ det returnert i en DataFrame, kan vi skrive fÃ¸lgende kode:\n\n\npython\n\nfrom fython import fame_to_pandas\n\ndf = fame_to_pandas(\n  ['sti/til/database1.db', 'sti/til/database2.db', 'm', '2012:1', '2022:12', '?']\n  )\n\nDersom vi i stedet Ã¸nsker Ã¥ hente alle serier som begynner pÃ¥ abc, slutter pÃ¥ d etterfulgt av ett vilkÃ¥rlig tegn, kan vi skrive fÃ¸lgende kode:\n\n\npython\n\nfrom fython import fame_to_pandas\n\ndf = fame_to_pandas(\n  ['sti/til/database1.db', 'sti/til/database2.db', 'm', '2012:1', '2022:12', 'abc?d^']\n  )\n\n? og ^ er altsÃ¥ jokertegn/wildcards som representerer henholdvis et vilkÃ¥rlig antall tegn og nÃ¸yaktig ett tegn.\nDersom vi i stedet vil lagre dataene til en csv-fil kan vi skrive\n\n\npython\n\nfrom fython import fame_to_csv\n\nfame_to_csv(\n  ['sti/til/database1.db', 'sti/til/database2.db', 'm', '2012:1', '2022:12', 'abc?d^', 'sti/til/csv-fil.csv']\n  )\n\n\n\n\n\n\n\nWarning\n\n\n\nDet er viktig Ã¥ pÃ¥peke at enhver serie kun skrives Ã©n gang, og da fra den fÃ¸rste databasen den finnes i (kronologisk iht. til listen med databaser)."
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#kjÃ¸ringer-pÃ¥-serveren",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#kjÃ¸ringer-pÃ¥-serveren",
    "title": "Fra Fame til Python",
    "section": "KjÃ¸ringer pÃ¥ serveren",
    "text": "KjÃ¸ringer pÃ¥ serveren\nNÃ¥r du skal bruke fython sÃ¥ mÃ¥ du ta hensyn til hvilken server Fame er installert pÃ¥, og hvilken server du har tenkt til Ã¥ jobbe pÃ¥. Fame er som sagt installert pÃ¥ sl-fame-1.ssb.no, mens Jupyterlab er installert pÃ¥ sl-jupyter-p.ssb.no. Dvs. at hvis du Ã¸nsker Ã¥ bruke fython i en notebook i Jupyterlab, sÃ¥ mÃ¥ du bruke ssh til Ã¥ koble deg til sl-fame-1.ssb.no, og sÃ¥ kjÃ¸re koden derfra. Koden din kan skrive en fil til Ã¸nsket stammeomrÃ¥det, som du igjen kan lese inn direkte i Jupyterlab."
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#automatiserte-uttrekk",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#automatiserte-uttrekk",
    "title": "Fra Fame til Python",
    "section": "Automatiserte uttrekk",
    "text": "Automatiserte uttrekk\nHvis man Ã¸nsker at utrekk fra Fame skal skje automatisk pÃ¥ gitte tidspunkter eller intervaller, sÃ¥ kan man ta kontakt med Kundeservice. Fordelen med dette er at man ikke trenger Ã¥ bruke ssh slik som beskrevet over. Man kan lese inn direkte fra stammeomrÃ¥det."
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#overfÃ¸re-data-til-dapla",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#overfÃ¸re-data-til-dapla",
    "title": "Fra Fame til Python",
    "section": "OverfÃ¸re data til Dapla",
    "text": "OverfÃ¸re data til Dapla\nHvis man Ã¸nsker Ã¥ overfÃ¸re data fra Fame til Dapla, sÃ¥ kan dette settes opp som en MoveIt-operasjon. For Ã¥ sette opp en MoveIt-jobb mÃ¥ ma kontakte Kundeservice. OverfÃ¸ring til Dapla forutsetter at man har et Dapla-team, og at man setter opp en synkroniseringjobb med Transfer Service."
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#footnotes",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#footnotes",
    "title": "Fra Fame til Python",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPakken er installert i Python-versjon 3.6 pÃ¥ serveren. Du kan Ã¥pne et Python-shell i terminalen pÃ¥ sl-fame-1.ssb.no ved Ã¥ skrive: python3.6.â†©ï¸"
  },
  {
    "objectID": "blog/posts/2024-07-11-pseudo-testdata/index.html",
    "href": "blog/posts/2024-07-11-pseudo-testdata/index.html",
    "title": "Pseudonymisering med testdata",
    "section": "",
    "text": "Den strenge tilgangsstyringen til pseudonymiseringsfunksjonaliteten pÃ¥ Dapla gjÃ¸r at det er vanskelig for brukere Ã¥ bli kjent med funksjonaliteten ved bruk av produksjonsdata. Derfor bÃ¸r alle som jobber med dette starte med Ã¥ bruke testdata og jobbe i test-miljÃ¸et pÃ¥ Dapla."
  },
  {
    "objectID": "blog/posts/2024-07-11-pseudo-testdata/index.html#importere",
    "href": "blog/posts/2024-07-11-pseudo-testdata/index.html#importere",
    "title": "Pseudonymisering med testdata",
    "section": "Importere",
    "text": "Importere\nFÃ¸rst sÃ¥ importerer vi noen biblioteker som vi skal benytte. Du kan benytte standard-kernelen som heter python3, for der er biblioteket som pseudonymiserer, dapla-toolbelt-pseudo, tilgjengelig.\n\nimport json\n\nimport dapla as dp\nimport pandas as pd\nfrom dapla_pseudo import Depseudonymize, Pseudonymize\nfrom dapla_pseudo.constants import MapFailureStrategy\nfrom dapla_pseudo.utils import convert_to_date\nfrom IPython.display import JSON\n\nVersjonen av dapla-toolbelt-pseudo er 2.1.2.\nDataene vi skal bruke syntetiske fÃ¸dselsnummer fra testversjonen SNR-katalogen. PÃ¥ den mÃ¥ten fÃ¥r vi ogsÃ¥ testet pseudonymiseringen via SNR-katalogen som er veldig vanlig i SSB. Denne SNR-katalogen ligger som en fil i en bÃ¸tte som alle i SSB har tilgang til.\n\npath = \"gs://ssb-dapla-felles-data-produkt-test/freg/snr_kat.csv\"\n\ndf = dp.read_pandas(path, file_format=\"csv\", dtype={\"fnr\": str, \"fnr_date\": str})\n\ndf.head(n=5).style.set_table_styles([\n    {'selector': 'th', 'props': [('text-align', 'left')]}\n])\n\n\n\nTabellÂ 1: Syntetisk versjon av SNR-katalogen\n\n\n\n\n\n\n\n\nÂ \nfnr\ncurrent_fnr\nsnr\ncurrent_snr\nfnr_date\ncurrent_fnr_date\n\n\n\n\n0\n16890249063\n16890249063\n026mxd3\n026mxd3\n20201222\n20201222\n\n\n1\n15854996565\n15854996565\n34qm7pt\n34qm7pt\n20201222\n20201222\n\n\n2\n27871547810\n27871547810\n53uxelp\n53uxelp\n20201222\n20201222\n\n\n3\n50889200399\n50889200399\nf35lbnf\nf35lbnf\n20201222\n20201222\n\n\n4\n22919199052\n22919199052\nc2hxvdv\nc2hxvdv\n20201222\n20201222\n\n\n\n\n\n\n\n\nFra TabellÂ 1 ser vi at datasettet inkluderer en del kolonner. For utforsking av pseudonymiseringsfunksjonalitet sÃ¥ trenger vi kun fnr-kolonnen."
  },
  {
    "objectID": "blog/posts/2024-07-11-pseudo-testdata/index.html#forberedelse-av-datasettet",
    "href": "blog/posts/2024-07-11-pseudo-testdata/index.html#forberedelse-av-datasettet",
    "title": "Pseudonymisering med testdata",
    "section": "Forberedelse av datasettet",
    "text": "Forberedelse av datasettet\nLa oss kun beholde fnr-kolonnen og kopiere den en ny kolonne slik at vi enklere kan sammenligne fÃ¸r og etter pseudonymisering. I tillegg kutter jeg antall rader til 10, siden vi ikke trenger noe mer for formÃ¥let her.\n\ndf2 = df.head(n=10)\ndf3 = df2[[\"fnr\"]]\ndf4 = df3.copy()\ndf4['fnr_original'] = df4['fnr']\n\ndf4.head(n=5).style.set_table_styles([\n    {'selector': 'th', 'props': [('text-align', 'left')]}\n])\n\n\n\nTabellÂ 2: Nedstrippet versjon av SNR-katalogen\n\n\n\n\n\n\n\n\nÂ \nfnr\nfnr_original\n\n\n\n\n0\n16890249063\n16890249063\n\n\n1\n15854996565\n15854996565\n\n\n2\n27871547810\n27871547810\n\n\n3\n50889200399\n50889200399\n\n\n4\n22919199052\n22919199052\n\n\n\n\n\n\n\n\nHvis du Ã¸nsker Ã¥ teste hvordan krypteringsalgoritmene fungerer med kolonner som inneholder navn, sÃ¥ kan vi generere noe data med et ogsÃ¥.\n\nfornavn = [\n    \"Jo\",\n    \"Hans-August\",\n    \"Nils\",\n    \"Eva\",\n    \"Lars\",\n    \"Ã˜yvind\",\n    \"Kenneth\",\n    \"Johnny\",\n    \"Rupinder\",\n    \"Nicolas\",\n]\netternavn = [\n    \"Nordman\",\n    \"Karlsen\",\n    \"Nordmann\",\n    \"Karlsen\",\n    \"Carlsen\",\n    \"Nordmann\",\n    \"Carlsen\",\n    \"Nordmann\",\n    \"Karlsen\",\n    \"Normann\",\n]\n\ndf4['fornavn'] = fornavn\ndf4['etternavn'] = etternavn\ndf5 = df4.copy()\n\nTil slutt legger vi pÃ¥ noen ugyldige fÃ¸dselsnummer slik at vi fÃ¥r testet hvordan algoritmene hÃ¥ndterer dette.\n\nnew_row1 = pd.DataFrame(\n    [\n        {\n            \"fnr\": \"99999999999\",\n            \"fnr_original\": \"99999999999\",\n            \"fornavn\": \"Michael\",\n            \"etternavn\": \"Norman\",\n        }\n    ]\n)\ndf6 = pd.concat([df5, new_row1], ignore_index=True)\n\nnew_row2 = pd.DataFrame(\n    [{\"fnr\": \"XX\", \"fnr_original\": \"XX\", \"fornavn\": \"Ola Glenn\", \"etternavn\": \"GÃ¥Ã¥s\"}]\n)\ndf7 = pd.concat([df6, new_row2], ignore_index=True)\n\nnew_row3 = pd.DataFrame(\n    [\n        {\n            \"fnr\": \"X8b7k28\",\n            \"fnr_original\": \"X8b7k28\",\n            \"fornavn\": \"Lars\",\n            \"etternavn\": \"Gaas\",\n        }\n    ]\n)\ndf8 = pd.concat([df7, new_row3], ignore_index=True)\n\ndf8.head(n=5).style.set_table_styles([\n    {'selector': 'th', 'props': [('text-align', 'left')]}\n])\n\n\n\nTabellÂ 3: Datasett for Ã¥ teste pseudonymiseringsfunksjonalitet\n\n\n\n\n\n\n\n\nÂ \nfnr\nfnr_original\nfornavn\netternavn\n\n\n\n\n0\n16890249063\n16890249063\nJo\nNordman\n\n\n1\n15854996565\n15854996565\nHans-August\nKarlsen\n\n\n2\n27871547810\n27871547810\nNils\nNordmann\n\n\n3\n50889200399\n50889200399\nEva\nKarlsen\n\n\n4\n22919199052\n22919199052\nLars\nCarlsen\n\n\n\n\n\n\n\n\nTabellÂ 3 viser datasettet vi skal bruke til Ã¥ teste med."
  },
  {
    "objectID": "blog/posts/2024-07-11-pseudo-testdata/index.html#pseudonymisering",
    "href": "blog/posts/2024-07-11-pseudo-testdata/index.html#pseudonymisering",
    "title": "Pseudonymisering med testdata",
    "section": "Pseudonymisering",
    "text": "Pseudonymisering\nNÃ¥ kan vi begynne Ã¥ leke med dataene. Det fÃ¸rste vi kan gjÃ¸re er Ã¥ pseudonymisere med den mest vanlige algoritmen som benyttes i produksjon: Papis-nÃ¸kkelen.\n\nresult = (\n    Pseudonymize.from_pandas(df8)\n    .on_fields(\"fnr\")\n    .with_stable_id()\n    .run()\n)\nresult.to_pandas()\n\nUnexpected length of metadata: 2\n\n\n\n\nTabellÂ 4: Pseudonymiserer med Papis-algoritmen\n\n\n\n\n\n\n\n\n\n\nfnr\nfnr_original\nfornavn\netternavn\n\n\n\n\n0\nBnQe23u\n16890249063\nJo\nNordman\n\n\n1\nI1mQmBP\n15854996565\nHans-August\nKarlsen\n\n\n2\neVbGYLy\n27871547810\nNils\nNordmann\n\n\n3\nO4jegSM\n50889200399\nEva\nKarlsen\n\n\n4\n6JhhRKi\n22919199052\nLars\nCarlsen\n\n\n5\nvygqpLq\n66821775168\nÃ˜yvind\nNordmann\n\n\n6\nJWzitL8\n13824498614\nKenneth\nCarlsen\n\n\n7\noIhVngD\n46927100797\nJohnny\nNordmann\n\n\n8\n0y8qqUZ\n16907699157\nRupinder\nKarlsen\n\n\n9\nFNnp5AL\n10920998203\nNicolas\nNormann\n\n\n10\n4yI2BlkviaI\n99999999999\nMichael\nNorman\n\n\n11\nXX\nXX\nOla Glenn\nGÃ¥Ã¥s\n\n\n12\ntKHXmUl\nX8b7k28\nLars\nGaas\n\n\n\n\n\n\n\n\n\n\nI TabellÂ 4 ser vi at kolonnen fnr har blitt pseudonymisert. Det er ogsÃ¥ verdt Ã¥ legge merke til at kolonnen ikke endrer navn. Grunnen til at lengden pÃ¥ verdiene som er pseudonymiserte er pÃ¥ 7 tegn for de opprinnelige fÃ¸dselsnummerne, er at det fÃ¸rst skjer en oversetting fra fnr til snr fÃ¸r det pseudonymiseres, og snr-nummerserien er pÃ¥ 7 tegn. Med andre ord sÃ¥ preserverer algoritmen lengden pÃ¥ snr-nummeret siden det er dette som pseudonymiseres.\nDet er ogsÃ¥ verdt Ã¥ merke seg at verdier som er kortere enn 4 i lengde, f.eks. XX i rad 11, ikke blir pseudonymisert i det hele tatt. Verdier som er 4 eller lengre, vil bli pseudonymisert selv om de ikke fikk treff i SNR-katalogen.\n\nMetadata\nDet genereres ogsÃ¥ 2 metadata-objekter ved pseudonymisering. Disse er:\n\nresult.datadoc\nresult.metadata\n\nLa oss se nÃ¦rmere pÃ¥ de:\n\ndata = json.loads(result.datadoc)\ndisplay(data)\n\n{'document_version': '0.0.1',\n 'pseudonymization': {'document_version': '0.1.0',\n  'pseudo_variables': [{'short_name': 'fnr',\n    'data_element_path': 'fnr',\n    'data_element_pattern': '/fnr',\n    'stable_identifier_type': 'FREG_SNR',\n    'stable_identifier_version': '2023-08-31',\n    'encryption_algorithm': 'TINK-FPE',\n    'encryption_key_reference': 'papis-common-key-1',\n    'encryption_algorithm_parameters': [{'keyId': 'papis-common-key-1'},\n     {'strategy': 'skip'}]}]}}\n\n\nDette er metadata som skal integreres i Datadoc etter hvert.\nLa oss se pÃ¥ den andre typen metadata:\n\ndisplay(result.metadata)\n\n{'logs': ['No SID-mapping found for fnr 999********',\n  'No SID-mapping found for fnr X8b****'],\n 'metrics': {'MAPPED_SID': 10, 'FPE_LIMITATION': 1, 'MISSING_SID': 2}}\n\n\nHer ser vo at 10 felt fikk treff i SNR-katalogen, 1 felt var for kort for algoritmen, og 2 felt fikk ikke treff SNR-katalogen. Vi fÃ¥r ogsÃ¥ se litt fÃ¸dselsnummeret til de 2 som ikke fikk treff."
  },
  {
    "objectID": "blog/posts/2024-07-11-pseudo-testdata/index.html#depseudonymisering",
    "href": "blog/posts/2024-07-11-pseudo-testdata/index.html#depseudonymisering",
    "title": "Pseudonymisering med testdata",
    "section": "Depseudonymisering",
    "text": "Depseudonymisering\nLa oss ta vare pÃ¥ den pseudonymiserte kolonnen og sÃ¥ depseudonymisere og se om resultatet blir riktig:\n\nresult2 = result.to_pandas()\nresult2['pseudo_fnr'] = result2['fnr']\n\nresult_df = (\n    Depseudonymize.from_pandas(result2)         \n    .on_fields(\"fnr\")                              \n    .with_stable_id()                              \n    .run()                                         \n    .to_pandas() \n)\n\nresult_df.style.set_table_styles([\n    {'selector': 'th', 'props': [('text-align', 'left')]}\n])\n\n\n\nTabellÂ 5: Depseudonymisering av fÃ¸dselsnummer\n\n\n\n\n\n\n\n\nÂ \nfnr\nfnr_original\nfornavn\netternavn\npseudo_fnr\n\n\n\n\n0\n16890249063\n16890249063\nJo\nNordman\nBnQe23u\n\n\n1\n15854996565\n15854996565\nHans-August\nKarlsen\nI1mQmBP\n\n\n2\n27871547810\n27871547810\nNils\nNordmann\neVbGYLy\n\n\n3\n50889200399\n50889200399\nEva\nKarlsen\nO4jegSM\n\n\n4\n22919199052\n22919199052\nLars\nCarlsen\n6JhhRKi\n\n\n5\n66821775168\n66821775168\nÃ˜yvind\nNordmann\nvygqpLq\n\n\n6\n13824498614\n13824498614\nKenneth\nCarlsen\nJWzitL8\n\n\n7\n46927100797\n46927100797\nJohnny\nNordmann\noIhVngD\n\n\n8\n16907699157\n16907699157\nRupinder\nKarlsen\n0y8qqUZ\n\n\n9\n10920998203\n10920998203\nNicolas\nNormann\nFNnp5AL\n\n\n10\n99999999999\n99999999999\nMichael\nNorman\n4yI2BlkviaI\n\n\n11\nXX\nXX\nOla Glenn\nGÃ¥Ã¥s\nXX\n\n\n12\nX8b7k28\nX8b7k28\nLars\nGaas\ntKHXmUl\n\n\n\n\n\n\n\n\nTabellÂ 5 viser at depseudonymiseringen returnerer de opprinnelige fÃ¸dselsnummerne.\nVidere kan man utforske Ã¥ pseudonymisere navn ved bruk av ulike algoritmer."
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Eksempler",
    "section": "",
    "text": "Eksempler\nSe menyen til venstre for eksempler.",
    "crumbs": [
      "Eksempler"
    ]
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.html",
    "href": "notebooks/spark/sparkr-intro.html",
    "title": "Introduksjon til SparkR",
    "section": "",
    "text": "Akkurat som PySpark sÃ¥ gir SparkR oss et grensesnitt mot Apache Spark fra R. I denne notebooken viser vi noen eksempler hvordan du gjÃ¸re vanlige operasjoner med SparkR."
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.html#oppsett",
    "href": "notebooks/spark/sparkr-intro.html#oppsett",
    "title": "Introduksjon til SparkR",
    "section": "Oppsett",
    "text": "Oppsett\nEksemplene i notebooken bruker SparkR (k8s cluster) pÃ¥ https://jupyter.dapla.ssb.no/. Det vil si at den kan distribuere kjÃ¸ringene pÃ¥ flere maskiner i Kubernetes.\n\nspark\n\nJava ref type org.apache.spark.sql.SparkSession id 1"
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.html#lese-inn-fil",
    "href": "notebooks/spark/sparkr-intro.html#lese-inn-fil",
    "title": "Introduksjon til SparkR",
    "section": "Lese inn fil",
    "text": "Lese inn fil\n\nfile = read.parquet(\"gs://ssb-dapla-felles-data-produkt-prod/temp/timeseries.parquet\")\n\n\nselectedColumns &lt;- select(file, \"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\")\nshowDF(selectedColumns, numRows = 5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html",
    "href": "notebooks/spark/pyspark-intro.out.html",
    "title": "Introduksjon til PySpark",
    "section": "",
    "text": "Apache Spark er et sterkt verktÃ¸y som utvider mulighetsrommet for databehandling med R og Python. Kjernefunksjonaliteten ligger i at den lar deg kjÃ¸re en jobb pÃ¥ flere maskiner samtidig, noe som ikke er mulig med klassiske rammeverk som Pandas og Tidyverse. FÃ¸lgelig er det et rammeverk som blant annet er veldig egnet for Ã¥ prosessere store datamengder eller gjÃ¸re store beregninger. Les om mer om Apache Spark i Dapla-manualen\nI denne notebooken vises noen enkle eksempler pÃ¥ hvordan du kan jobbe med data med PySpark, et Python-grensesnitt mot Spark."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html#oppsett",
    "href": "notebooks/spark/pyspark-intro.out.html#oppsett",
    "title": "Introduksjon til PySpark",
    "section": "Oppsett",
    "text": "Oppsett\nNÃ¥r du logger deg inn pÃ¥ Dapla kan du velge mellom 2 ferdigoppsatte kernels for Ã¥ jobbe med PySpark:\n\nPyspark (local)\nPyspark (k8s cluster)\n\nDen fÃ¸rste lar deg bruke Spark pÃ¥ en enkeltmaskin, mens den andre lar deg distribuere kjÃ¸ringen pÃ¥ mange maskiner avhengig av hvor store jobbene er. I eksemplene under brukes Pyspark (local).\n\n# Importer biblioteker\nimport numpy as np\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import date_format, explode, expr, sequence, sum, avg\nfrom pyspark.sql.types import DateType, DoubleType, StructField, StructType\n\n# Initialierer en SparkSession\nspark = (\n    SparkSession.builder.master(\"local[2]\")\n    .appName(\"Dapla-manual-example\")\n    .getOrCreate()\n)\n\nI koden over importerer vi de bibliotekene vi skal bruke under. Grunnen til at vi importerer pyspark.sql er at dette er at Spark SQL er Apache Spark sin modul for Ã¥ jobbe med strukturerte data. Og som navnet tilsier vil det si at vi kan blande Python og SQL i koden vÃ¥r. Vi skal nÃ¦rmere pÃ¥ hvordan man bruke SQL fra PySpark-notebook senere.\nSpark tilbyr ogsÃ¥ et eget grensesnitt, Spark UI, for Ã¥ monitorere hva som skjer under en SparkSession. Vi kan bruke fÃ¸lgende kommando for Ã¥ fÃ¥ opp en lenke til Spark UI i notebooken vÃ¥r:\n\nspark.sparkContext\n\n\n        \n            SparkContext\n\n            Spark UI\n\n            \n              Version\n                v3.3.1\n              Master\n                local[*]\n              AppName\n                pyspark-shell\n            \n        \n        \n\n\nKlikker du pÃ¥ Spark UI-lenken sÃ¥ tar den deg til dashboard som lar deg monitorere, debugge, optimalisere og forstÃ¥ kjÃ¸ringene dine. Det kan vÃ¦re et svÃ¦rt nyttig verktÃ¸y i mange tilfeller."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html#generere-data",
    "href": "notebooks/spark/pyspark-intro.out.html#generere-data",
    "title": "Introduksjon til PySpark",
    "section": "Generere data",
    "text": "Generere data\nVi kan begynne med Ã¥ generere en Spark DataFrame med en kolonne som inneholder mÃ¥nedlige datoer for perioden 2000M1-2023M8.\n\n# Genererer mÃ¥nedlige data\ndates_df = spark.range(1).select(\n    explode(\n        sequence(\n            start=expr(\"date '2000-01-01'\"),\n            stop=expr(\"date '2023-08-01'\"),\n            step=expr(\"interval 1 month\"),\n        )\n    ).alias(\"Date\")\n)\ndates_df.show(5)\n\n+----------+\n|      Date|\n+----------+\n|2000-01-01|\n|2000-02-01|\n|2000-03-01|\n|2000-04-01|\n|2000-05-01|\n+----------+\nonly showing top 5 rows\n\n\nEn Spark DataFrame er en distribuert samling av data som er organisert inn i kolonner. Siden Spark lar deg distribuere kjÃ¸ringer pÃ¥ flere maskiner, er DataFrames optimalisert for Ã¥ kunne splittes opp slik at de kan brukes pÃ¥ flere maskiner. Med andre er dette ikke det samme som en Pandas dataframe mange kjenner fra fÃ¸r.\nOver genererte vi en datokolonne. For Ã¥ fÃ¥ litt mer data kan vi ogsÃ¥ generere 100 kolonner med tidsseriedata og sÃ¥ printer vi de 2 fÃ¸rste av disse:\n\n# Genererer random walk data\nschema = StructType(\n    [StructField(f\"serie{i:02d}\", DoubleType(), True) for i in range(100)]\n)\n\ndata = [\n    tuple((10 + np.random.normal(0, 1, 100)).cumsum().tolist())\n    for _ in range(284)  # 284 months from 2000-01 to 2023-08\n]\n\ndata_df = spark.createDataFrame(data, schema=schema)\n\ndata_df.select(\"serie00\", \"serie01\").show(5)\n\n+------------------+------------------+\n|           serie00|           serie01|\n+------------------+------------------+\n|10.410703377184355| 21.06318801110079|\n|10.509249410154466|  19.5674295298024|\n| 9.618310122060274|17.635805093465642|\n| 9.691112692298294|18.593842915949082|\n| 9.903675228685067|20.012215769058564|\n+------------------+------------------+\nonly showing top 5 rows\n\n\nTil slutt kan vi joine de to datasettene sammen og lage noen kolonner som viser Ã¥r, kvartal og mÃ¥ned. Deretter printer vi ut noen av kolonnene med kommandoen show().\n\n# Legger til row index til DataFrame fÃ¸r join med dates_df\ndata_df = data_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n\n# Joiner de to datasettene\ndf = (\n    dates_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n    .join(data_df, \"row_index\")\n    .drop(\"row_index\")\n)\n\n# Legger til Ã¥r, kvartal og mnd\ndf = df.withColumn(\"Year\", date_format(df.Date, \"yyyy\"))\ndf = df.withColumn(\"Quarter\", expr(\"quarter(Date)\"))\ndf = df.withColumn(\"Month\", date_format(df.Date, \"MM\"))\n\ndf.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows\n\n\nOg med det har vi noe data vi kan jobbe med i resten av notebooken."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html#skrive-til-parquet",
    "href": "notebooks/spark/pyspark-intro.out.html#skrive-til-parquet",
    "title": "Introduksjon til PySpark",
    "section": "Skrive til Parquet",
    "text": "Skrive til Parquet\nPySpark tilbyr mange opsjoner ved skriving til parquet-filer som vi kanskje ikke er vant til Ã¥ forholde oss til med enklere rammeverk som Pandas. Den enkleste mÃ¥ten Ã¥ skrive ut en fil er som fÃ¸lger:\ndf.write.parquet(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp/timeseries.parquet\"\n)\nDette vil fungere hvis filen ikke finnes fra fÃ¸r. Hvis den finnes fra fÃ¸r sÃ¥ vil den feile. Grunnen er at vi ikke har spesifisert hva vi Ã¸nsker at den skal gjÃ¸re. Vi kan velge mellom overwrite, append, ignore eller errorifexists. Sistnevnte er ogsÃ¥ default-oppfÃ¸rsel hvis du ikke ber den gjÃ¸re noe annet.\nUnder bruker vi opsjonen overwrite, det vil si at den skriver over en evt eksisterende fil med samme navn.\n\ndf.write.mode(\"overwrite\").parquet(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp/timeseries.parquet\"\n)\n\nVi kan inspisere hva som ble skrevet ved Ã¥ liste ut innholder i bÃ¸tta.\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/temp/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp/',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/_SUCCESS',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/part-00000-b32e7299-0590-4b31-bcc2-dc3d58725529-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/part-00001-b32e7299-0590-4b31-bcc2-dc3d58725529-c000.snappy.parquet']\n\n\nHvis denne parquet-filen hadde vÃ¦rt partisjonert etter en kolonne, sÃ¥ ville det vÃ¦rt egne undermapper med navnestruktur column_name=value som indikerte hva filen er partisjonert pÃ¥. Siden vi her bruker en maskin og har et lite datasett, valgte Spark Ã¥ ikke partisjonere."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html#lese-inn-parquet",
    "href": "notebooks/spark/pyspark-intro.out.html#lese-inn-parquet",
    "title": "Introduksjon til PySpark",
    "section": "Lese inn Parquet",
    "text": "Lese inn Parquet\nApache Spark kan lese inn flere parquet-filer samtidig. Syntaxen er like enkel som den for Ã¥ skrive ut.\n\ndf_ts = spark.read.parquet(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp/timeseries.parquet\"\n)\ndf_ts.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie66\", \"serie55\").show(5)\n\n+----------+----+-------+-----+-----------------+-----------------+\n|      Date|Year|Quarter|Month|          serie66|          serie55|\n+----------+----+-------+-----+-----------------+-----------------+\n|2000-01-01|2000|      1|   01|670.2679830025959|562.4312808525777|\n|2000-02-01|2000|      1|   02|675.4233411662802|562.5168447360121|\n|2000-03-01|2000|      1|   03|687.3412458214908|568.6203957584232|\n|2000-04-01|2000|      2|   04|673.1128047244955|557.4633871253379|\n|2000-05-01|2000|      2|   05| 667.513406101114|561.7766450346327|\n+----------+----+-------+-----+-----------------+-----------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html#pyspark-og-sql",
    "href": "notebooks/spark/pyspark-intro.out.html#pyspark-og-sql",
    "title": "Introduksjon til PySpark",
    "section": "PySpark og SQL",
    "text": "PySpark og SQL\nDu kan ogsÃ¥ skrive SQL med Spark. For Ã¥ skrive SQL mÃ¥ vi fÃ¸rst lage et temporary view. Under kaller vi viewt for tidsserie.\n\ndf.createOrReplaceTempView(\"tidsserie\")\n\nVi kan deretter skrive en SQL-statement som vi Ã¸nsker Ã¥ kjÃ¸re pÃ¥ viewet:\n\nquery = \"SELECT * FROM tidsserie WHERE Year = 2010\"\n\nDeretter kan vi bruke det til Ã¥ filtrere datasettet:\n\nresult_df = spark.sql(query)\nresult_df.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2010-01-01|2010|      1|   01| 11.26910423907778|21.730128215168268|\n|2010-02-01|2010|      1|   02| 8.722783282690738| 17.46851086792347|\n|2010-03-01|2010|      1|   03|10.376873608348605|20.109386343182802|\n|2010-04-01|2010|      2|   04|11.459959305590926|21.995141825523866|\n|2010-05-01|2010|      2|   05|10.441456792180572| 21.25096473981906|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html#aggregering",
    "href": "notebooks/spark/pyspark-intro.out.html#aggregering",
    "title": "Introduksjon til PySpark",
    "section": "Aggregering",
    "text": "Aggregering\nvi kan aggregere opp enten med PySpark-syntax eller SQL-syntax. Under viser vi begge:\n\nfrom pyspark.sql import functions as F\n\n# Assuming df_ts is your DataFrame\naggregated_df = df_ts.groupBy(\"Quarter\").agg(F.sum(\"serie00\").alias(\"Sum\"),\n                                             F.avg(\"serie00\").alias(\"Average\"),\n                                             F.max(\"serie00\").alias(\"Maximum\"))\n\n# Show the result\naggregated_df.show()\n\n+-------+------------------+------------------+------------------+\n|Quarter|               Sum|           Average|           Maximum|\n+-------+------------------+------------------+------------------+\n|      1|363.95869885234185|10.109963857009497|11.829453550532005|\n|      3|365.68324879453405| 10.15786802207039|12.233378837422391|\n|      4| 342.2334082209804|10.065688477087658|12.210138970053695|\n|      2|  361.991445506568|10.055317930738001|12.276030776082463|\n+-------+------------------+------------------+------------------+\n\n\nLa oss gjÃ¸re det samme med SQL, men grupperer etter to variabler og sorterer output etterpÃ¥.\n\n# Assuming df_ts is your DataFrame\ndf_ts.createOrReplaceTempView(\"temp_table\")\n\n# Now you can run a SQL query\nquery = \"\"\"\n    SELECT\n        Year,\n        Quarter,\n        SUM(serie00) AS Sum,\n        AVG(serie00) AS Average,\n        MAX(serie00) AS Maximum\n    FROM \n        temp_table\n    GROUP BY \n        Year, Quarter\n    ORDER BY\n        YEAR, QUARTER\n\"\"\"\n\naggregated_df_sql = spark.sql(query)\n\n# Show the result\naggregated_df_sql.show(10)\n\n+----+-------+------------------+------------------+------------------+\n|Year|Quarter|               Sum|           Average|           Maximum|\n+----+-------+------------------+------------------+------------------+\n|2000|      1|31.323050432219453|10.441016810739818|11.118293927071951|\n|2000|      2|28.911192473027377| 9.637064157675793| 9.900977410685329|\n|2000|      3|33.670797229797415|11.223599076599138|12.233378837422391|\n|2000|      4|28.094793356286914| 9.364931118762305| 10.32000478359274|\n|2001|      1|31.636678535169537|10.545559511723178|11.367822302191831|\n|2001|      2|29.629770128521507| 9.876590042840503|11.135215930381191|\n|2001|      3| 30.75408440118315| 10.25136146706105|10.723803326978505|\n|2001|      4|30.361048932627902|  10.1203496442093|10.368365984482093|\n|2002|      1|31.184163218551227|10.394721072850409|10.550579652234951|\n|2002|      2|29.128978392451202|   9.7096594641504|10.186365745367246|\n+----+-------+------------------+------------------+------------------+\nonly showing top 10 rows"
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.out.html",
    "href": "notebooks/spark/sparkr-intro.out.html",
    "title": "Introduksjon til SparkR",
    "section": "",
    "text": "Akkurat som PySpark sÃ¥ gir SparkR oss et grensesnitt mot Apache Spark fra R. I denne notebooken viser vi noen eksempler hvordan du gjÃ¸re vanlige operasjoner med SparkR."
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.out.html#oppsett",
    "href": "notebooks/spark/sparkr-intro.out.html#oppsett",
    "title": "Introduksjon til SparkR",
    "section": "Oppsett",
    "text": "Oppsett\nEksemplene i notebooken bruker SparkR (k8s cluster) pÃ¥ https://jupyter.dapla.ssb.no/. Det vil si at den kan distribuere kjÃ¸ringene pÃ¥ flere maskiner i Kubernetes.\n\nspark\n\nJava ref type org.apache.spark.sql.SparkSession id 1"
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.out.html#lese-inn-fil",
    "href": "notebooks/spark/sparkr-intro.out.html#lese-inn-fil",
    "title": "Introduksjon til SparkR",
    "section": "Lese inn fil",
    "text": "Lese inn fil\n\nfile = read.parquet(\"gs://ssb-dapla-felles-data-produkt-prod/temp/timeseries.parquet\")\n\n\nselectedColumns &lt;- select(file, \"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\")\nshowDF(selectedColumns, numRows = 5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "news/posts/2025-04-24-transfer-service-onprem/index.html",
    "href": "news/posts/2025-04-24-transfer-service-onprem/index.html",
    "title": "Beslutning om Transfer Service fra testmiljÃ¸et pÃ¥ Dapla til bakken",
    "section": "",
    "text": "Mange brukere har bedt om Ã¥ kunne overfÃ¸re data fra et Dapla-team sitt test-miljÃ¸ til bakken. Dette er spesielt relevant for team har datafangst fra Altinn 3 pÃ¥ Dapla, men som skal prosessere dataene pÃ¥ bakken.\nDet er nÃ¥ besluttet at det ikke skal Ã¥pnes for Ã¥ kunne overfÃ¸re data fra test-miljÃ¸ pÃ¥ Dapla til bakken. Ã…rsaken er at det ikke finnes noe test-miljÃ¸et pÃ¥ bakken, og derfor lager dette en Ã¥pning for Ã¥ overfÃ¸re data mellom prod- og test-miljÃ¸ene pÃ¥ Dapla. En slik Ã¥pning er problematisk og derfor er det nÃ¥ besluttet at dette ikke skal vÃ¦re mulig.\nLÃ¸sningen for team som har datafangst med Altinn3, og fÃ¥r test-data fra team SU-V, blir derfor Ã¥ bruke Transfer Service fra prod-miljÃ¸et til teamet for Ã¥ overfÃ¸re til bakken. Team SU-V har nÃ¥ gitt Transfer Service i prod-miljÃ¸et til alle team med datafangst fra Altinn 3 tilgang til test-bÃ¸tta for data, slik som vist i FigurÂ 1.\nTeam som allerede har fÃ¥tt satt opp muligheten til Ã¥ overfÃ¸re data fra test-miljÃ¸et til bakken, vil bli kontaktet av team Skyinfra for endre dette til lÃ¸sningen vist i FigurÂ 1.\n\n\n\n\n\n\n\n\nflowchart LR \n\n    subgraph suv-altinn-data-t\n        id_suv_bucket_t[(ssb-suv-altinn-raXXXX-YY-test)]\n    end\n\n    subgraph ssb-teamnavn-kilde-t\n        id_stat_kilde_bucket_t[(data-kilde)]\n    end\n    id_suv_bucket_t &lt;-- Transfer service read/write --&gt; id_stat_kilde_bucket_t\n\n    subgraph ssb-teamnavn-t\n        id_stat_produkt_bucket_t[(data-produkt)]\n    end\n    id_stat_kilde_bucket_t -- Kildomaten --&gt; id_stat_produkt_bucket_t\n\n    subgraph suv-altinn-data-p\n        id_suv_bucket_p[(ssb-suv-altinn-raXXXX-YY-prod)]\n    end\n\n    subgraph ssb-teamnavn-kilde-p\n        id_stat_kilde_bucket_p[(data-kilde&lt;br&gt;/altinn/prod/)]\n        id_stat_kilde_bucket_pt[(data-kilde&lt;br&gt;/altinn/test/)]\n    end\n    id_suv_bucket_p &lt;-- Transfer service read/write --&gt; id_stat_kilde_bucket_p\n    id_suv_bucket_t -- Transfer service read --&gt; id_stat_kilde_bucket_pt\n\n    subgraph ssb-teamnavn-p\n        id_stat_produkt_bucket_p[(data-produkt&lt;br&gt;/inndata/altinn/)]\n        id_stat_produkt_bucket_pt[(data-produkt&lt;br&gt;/temp/test/altinn/)]\n\n        id_stat_kilde_bucket_p -- Kildomaten --&gt; id_stat_produkt_bucket_p\n        id_stat_kilde_bucket_pt -- Kildomaten --&gt; id_stat_produkt_bucket_pt\n\n        id_stat_frasky_bucket_p[(frasky&lt;br&gt;/altinn/prod)]\n        id_stat_frasky_bucket_pt[(frasky&lt;br&gt;/altinn/test)]\n\n        id_stat_produkt_bucket_p -- Transfer service --&gt; id_stat_frasky_bucket_p\n        id_stat_produkt_bucket_pt -- Transfer service --&gt; id_stat_frasky_bucket_pt\n    end\n\n    subgraph linux on-prem\n        id_cloudsync_p[(cloud_sync&lt;br&gt;/frasky/altinn/prod)]\n        id_cloudsync_pt[(cloud_sync&lt;br&gt;/frasky/altinn/test)]\n\n        id_stat_frasky_bucket_p -- Transfer service --&gt; id_cloudsync_p\n        id_stat_frasky_bucket_pt -- Transfer service --&gt; id_cloudsync_pt\n\n        id_stammekat_p[(stammekatalog&lt;br&gt;/wk24/datafangst/gSdv/)]\n        id_stammekat_pt[(stammekatalog&lt;br&gt;/test/wk24/datafangst/gSdv/)]\n\n        id_cloudsync_p -- MoveIT --&gt; id_stammekat_p \n        id_cloudsync_pt -- MoveIT --&gt; id_stammekat_pt \n    end\n\n    subgraph Oracle DB1P\n        id_isee_p[(ISEE Prod)]\n    end\n    id_stammekat_p -- crontab --&gt; id_isee_p\n\n    subgraph Oracle DB1T\n        id_isee_t[(ISEE Test)]\n    end\n    id_stammekat_pt -- crontab --&gt; id_isee_t\n\n\n\n\n\n\n\n\n\nFigurÂ 1: Figur som viser dataflyt for statistikkproduksjon pÃ¥ Dapla og Dapla/ISEE."
  },
  {
    "objectID": "news/posts/2025-03-18-klass-web-upgrade/index.html",
    "href": "news/posts/2025-03-18-klass-web-upgrade/index.html",
    "title": "Nytt utseende for Klass-siden pÃ¥ ssb.no",
    "section": "",
    "text": "Den 18. mars 2025 ble nettsiden som lar brukere utforske i kodeverk i Klass pÃ¥ www.ssb.no/klass oppdatert. Brukere vil merke at utseende nÃ¥ er oppdatert og mer i trÃ¥d med utseendet pÃ¥ resten av www.ssb.no. I tillegg har selve applikasjonen blitt oppgradert og migrert til Ã¥ kjÃ¸re pÃ¥ SSBs nye applikasjonsplattform NAIS.\nFigurÂ 1 viser hvordan den nye nettsiden ser ut.  \n\n\n\n\n\n\nFigurÂ 1: Den nye nettsiden pÃ¥ ssb.no for Klass."
  },
  {
    "objectID": "news/posts/2025-04-30-challenges-gcsfuse/index.html",
    "href": "news/posts/2025-04-30-challenges-gcsfuse/index.html",
    "title": "Deling av data og /buckets",
    "section": "",
    "text": "Etter noen mÃ¥neder med erfaring med tilgjengeliggjÃ¸ring av bÃ¸tter som filsystem i Dapla Lab, har vi erfart at funksjonaliteten i noen tilfeller har noen utfordringer.\nEn utfordring oppstÃ¥r nÃ¥r team skriver data til delt-bÃ¸tter direkte via Transfer Service, uten Ã¥ logge seg inn pÃ¥ Dapla Lab etterpÃ¥. Hvis det delende teamet aldri Ã¥pner en tjeneste pÃ¥ Dapla Lab etter at dataene er delt, sÃ¥ vil ikke de dataene vises for konsumentene av dataene under /buckets/ i Dapla Lab.\nÃ…rsaken er at opprettelsen av mapper krever skrivetilgang til bÃ¸tta, og det har ikke konsumenten av delte-data. Konsumentene vil kunne se dataene via gs://-stien, men ikke /buckets/-stien. Hvis data derimot deles ved at de produseres direkte fra en tjeneste pÃ¥ Dapla Lab, sÃ¥ vil ikke dette vÃ¦re en problemstilling.\nVi jobber med Ã¥ lÃ¸se dette problemet, og finner forhÃ¥pentligvis en lÃ¸sning innen kort tid."
  },
  {
    "objectID": "news/posts/2025-02-07-parquet-viewer/index.html",
    "href": "news/posts/2025-02-07-parquet-viewer/index.html",
    "title": "Parquet-utforsker i VS Code",
    "section": "",
    "text": "Sjekk ut den nye Parquet-utforskeren i Vscode-python. Skrive SQL, filtrer,inspiser metadata uten bruk av Python eller R. Les mer i denne blogg-artikkelen."
  },
  {
    "objectID": "news/posts/2025-04-14-pausing-dapla-lab/index.html",
    "href": "news/posts/2025-04-14-pausing-dapla-lab/index.html",
    "title": "Nye tidspunkter for pausing pÃ¥ Dapla Lab",
    "section": "",
    "text": "Alle tjenester pÃ¥ Dapla Lab blir automatisk pauset kl 17.00 hver dag, med mindre man har meldt seg ut av pausingen. Basert pÃ¥ erfaring ser vi at det startes en del tjenester etter kl 17, og siden disse ikke pauses automatisk, sÃ¥ kan de stÃ¥ og kjÃ¸re til kl 17 dagen etter hvis brukeren glemmer Ã¥ pause disse. Dette kan medfÃ¸re unÃ¸dvendige ressursbruk og kostnader for SSB.\nDerfor innfÃ¸rer vi en ny automatisk rutine for pausing av tjenester pÃ¥ Dapla Lab. I tillegg til pausingen som skjer kl 17 hver, sÃ¥ vil vi fom. 14. april 2025 pause tjenester ogsÃ¥ hver hele time i tidsintervallet 18.00-05.00. Den nye rutinen vil fortsatt respektere de som meldt seg ut av pausingen. De som har meldt seg ut av pausingen mÃ¥ selv ta ansvar for Ã¥ manuelt pause tjenesten nÃ¥r den ikke er i bruk.\n\n\n\n\n\n\nFigurÂ 1: Bilde av tjeneste som er pauset."
  },
  {
    "objectID": "news/posts/2024-12-23-standarder-chapter/index.html",
    "href": "news/posts/2024-12-23-standarder-chapter/index.html",
    "title": "Nytt kapittel om Standarder og standardutvalget",
    "section": "",
    "text": "Vi har skrevet et kapittel i Dapla-manualen som gir et overblikk over standardutvalget og standardene pÃ¥ Dapla. Sjekk det ut her."
  },
  {
    "objectID": "news/posts/2025-04-07-altinn-part/index.html",
    "href": "news/posts/2025-04-07-altinn-part/index.html",
    "title": "Blogginnlegg om Altinn 3 og ISEE",
    "section": "",
    "text": "Magnus Theodor Engh har skrevet et blogginnlegg som forklarer hvordan man kan integrere datafangst med Altinn 3 pÃ¥ Dapla, med lasting til ISEE i prodsonen. Les innlegget her."
  },
  {
    "objectID": "news/posts/2025-01-22-filinnsamling-moveit/index.html",
    "href": "news/posts/2025-01-22-filinnsamling-moveit/index.html",
    "title": "Nytt kapittel om filinnsamling via MoveIT pÃ¥ Dapla",
    "section": "",
    "text": "Mange statistikker mottar data fra eksterne som filer via MoveIT-systemet pÃ¥ bakken. Disse filene kan automatisk synkroniseres til kildebÃ¸tta pÃ¥ Dapla. Les mer om hvordan dette settes opp i dette kapitlet i Dapla-manualen."
  },
  {
    "objectID": "news/posts/2024-11-25-endring-i-tjenestekonfig/index.html",
    "href": "news/posts/2024-11-25-endring-i-tjenestekonfig/index.html",
    "title": "Endring i tjenestekonfigurasjon pÃ¥ Dapla Lab",
    "section": "",
    "text": "Tjenestekonfigurasjonen i Dapla Lab har endret seg. Dette er altsÃ¥ menyen man fÃ¥r opp nÃ¥r man starter opp en ny tjeneste, for eksempel en ny instans av Jupyter eller VSCode. Parameterne man kan konfigurere er derimot uendret. FÃ¸lgende artikler i manualen har derfor blitt oppdatert: jupyter, jupyter-pyspark, jupyter-playground, VSCode-python, RStudio, Datadoc, JDemetra og artikkelen om Dapla Lab.\n\n\n\n\n\n\nFigurÂ 1: Tjenestekonfigurasjon for jupyter"
  },
  {
    "objectID": "news/posts/2025-03-17-bucket-list/index.html",
    "href": "news/posts/2025-03-17-bucket-list/index.html",
    "title": "Tilgang til Ã¥ liste bÃ¸ttenavn pÃ¥ Dapla",
    "section": "",
    "text": "Fra og med mandag 17. mars 2025 har alle SSB-brukere fÃ¥tt tilgangen bucket.list pÃ¥ alle GCS-bÃ¸tter pÃ¥ Dapla. Det betyr at de kan se navnene til bÃ¸ttene, men ikke innholdet i bÃ¸ttene. FormÃ¥let med Ã¥ tillate dette er at alle skal kunne undersÃ¸ke hvilke bÃ¸tter et team har via Google Cloud Console, noe som kan vÃ¦re nyttige i flere sammenhenger."
  },
  {
    "objectID": "news/posts/2024-11-09-data-admins-developers/index.html",
    "href": "news/posts/2024-11-09-data-admins-developers/index.html",
    "title": "Data-admins i developers-gruppa",
    "section": "",
    "text": "Det er bestemt at medlemmer av tilgangsgruppen data-admins ogsÃ¥ skal vÃ¦re medlem av developers-gruppa. Det er pÃ¥ denne mÃ¥ten at data-admins ogsÃ¥ fÃ¥r tilgang til standardprosjektet til teamet. Dette er en beslutning om arkitekturen rundt tilgangsstyring pÃ¥ Dapla, der medlemskap i de to gruppene gir helt forskjellige tilganger. Medlemskap i data-admins-gruppa vil da vanligvis innebÃ¦re de samme tilgangene som developers-gruppa, med mindre de aktiverer de forhÃ¥ndsgodkjente tilgangene til kildedata."
  },
  {
    "objectID": "news/posts/2024-11-14-kildedata-datadoc/index.html",
    "href": "news/posts/2024-11-14-kildedata-datadoc/index.html",
    "title": "Kildedata og Datadoc",
    "section": "",
    "text": "Datadoc stÃ¸tter ikke dokumentasjon av datasett i datatilstanden kildedata enda. Det vil stÃ¸ttes pÃ¥ sikt, men forelÃ¸pig er det ikke funksjonalitet for dette.\nI Datadoc-applikasjonen i Dapla Lab fÃ¥r man nÃ¥ valget med Ã¥ logge seg inn i tjenesten som data-admins, og dermed en teknisk mulighet til Ã¥ dokumentere kildedata. Siden dette ikke er stÃ¸ttet enda, sÃ¥ ber vi alle om Ã¥ ikke gjÃ¸re dette. Det er ingen grunn til Ã¥ aktivere sin kildedatatilgang nÃ¥r dette ikke stÃ¸ttes.\nLes mer i dokumentasjonen til Datadoc."
  },
  {
    "objectID": "news/posts/2024-11-11-dapla-lab-source-data/index.html",
    "href": "news/posts/2024-11-11-dapla-lab-source-data/index.html",
    "title": "Kildedata tilgjengelig fra Dapla Lab",
    "section": "",
    "text": "Brukere i tilgangsgruppen data-admins kan nÃ¥ aksessere kildedata fra tjenestene pÃ¥ Dapla Lab. Kravene for tilgangen er de samme som fÃ¸r:\nSom tidligere mÃ¥ de da velge hvilket team og tilgangsgruppe de Ã¸nsker Ã¥ starte tjenesten som, og deretter oppgi begrunnelse og lengde pÃ¥ tilgang. Dette gjÃ¸res fra tjenestekonfigurasjonen i Dapla Lab, slik som vist i FigurÂ 1."
  },
  {
    "objectID": "news/posts/2024-11-11-dapla-lab-source-data/index.html#begrunnelse-gis-i-dapla-lab",
    "href": "news/posts/2024-11-11-dapla-lab-source-data/index.html#begrunnelse-gis-i-dapla-lab",
    "title": "Kildedata tilgjengelig fra Dapla Lab",
    "section": "Begrunnelse gis i Dapla Lab",
    "text": "Begrunnelse gis i Dapla Lab\nNÃ¥r en bruker tidligere har aksessesert kildedata fra â€œgamleâ€ Jupyter (https://jupyter.dapla.ssb.no/), sÃ¥ ble en JIT-applikasjon for Ã¥ fÃ¥ midlertidig tilgang. Denne tilnÃ¦rmingen vil fortsette Ã¥ fungere for det â€œgamleâ€ mijÃ¸et inntil det avvikles til fordel for Dapla Lab. Men pÃ¥ Dapla Lab kan begrunnelse og tidspunkt angis direkte i tjenestekonfigurasjonen."
  },
  {
    "objectID": "news/posts/2024-11-11-dapla-lab-source-data/index.html#feil-ved-manglende-begrunnelse",
    "href": "news/posts/2024-11-11-dapla-lab-source-data/index.html#feil-ved-manglende-begrunnelse",
    "title": "Kildedata tilgjengelig fra Dapla Lab",
    "section": "Feil ved manglende begrunnelse",
    "text": "Feil ved manglende begrunnelse\nHvis man prÃ¸ver Ã¥ starte en tjeneste som data-admins uten Ã¥ oppgi en begrunnelse, sÃ¥ vil man fÃ¥ feilmeldingen vist i FigurÂ 2. Denne feilmeldingen er lite forklarende og er noe vi Ã¸nsker Ã¥ forbedre etter hvert.\n\n\n\n\n\n\nFigurÂ 2: Feilmelding ved manglende begrunnelse i Dapla Lab."
  },
  {
    "objectID": "news/posts/2024-11-25-referansegruppe-dapla-lab/index.html",
    "href": "news/posts/2024-11-25-referansegruppe-dapla-lab/index.html",
    "title": "NÃ¥r skrus â€˜gamleâ€™ jupyter av?",
    "section": "",
    "text": "Under forrige DaplaNytt ble 15. januar 2025 nevt som en mulig sluttdato for â€˜gamleâ€™ jupyter, altsÃ¥ https://jupyter.dapla.ssb.no, og dermed en endelig overgang til Dapla Lab.\nDenne datoen er tentativ. Sluttdato for gamle jupyter vil bli endeliggjort etter mÃ¸tet med referansegruppen for statistikktjenester 6. desember 2024. Referansegruppen har representanter fra hver statistikkavdeling.\nI mellomtiden ber vi brukere om Ã¥ henvende seg til sine respektive stÃ¸tteteam, nÃ¦rmeste leder eller medlemmene av referansegruppen for innvendinger eller innspill til nÃ¥r vi kan gjennomfÃ¸re en endelig overgang til Dapla Lab."
  },
  {
    "objectID": "news/posts/2025-03-05-daplanytt-feb25/index.html",
    "href": "news/posts/2025-03-05-daplanytt-feb25/index.html",
    "title": "DaplaNytt-mÃ¸te 5.3.2025",
    "section": "",
    "text": "DaplaNytt for februar 2025 ble avholdt som Teams-mÃ¸te 5. mars 2025.\nSe opptaket her (intern lenke).\nPresentasjonen finner du her.\n\n\n\nSkjembilde fra Teams-mÃ¸tet DaplaNytt."
  },
  {
    "objectID": "news/posts/2025-04-22-dapla-metrics-shared-buckets/index.html",
    "href": "news/posts/2025-04-22-dapla-metrics-shared-buckets/index.html",
    "title": "Statistikk over datadeling pÃ¥ Dapla",
    "section": "",
    "text": "Ett Ã¥r etter at Dapla-team kunne opprette delt-bÃ¸tter selvbetjent pÃ¥ Dapla, sÃ¥ er det tilsammen opprettet 118 bÃ¸tter per 22. April 2025. 83 av disse er i prod-miljÃ¸et til teamene, mens 35 er i test-miljÃ¸et.\nAv de 171 teamene totalt pÃ¥ Dapla, sÃ¥ er det 33 av disse som har opprettet en eller flere delt-bÃ¸tter.\nTabellÂ 1 viser en oversikt over alle delt-bÃ¸tter som er opprettet1, dens kortnavn, hvilket team som er eier den, hvilket miljÃ¸ den finnes i, og hvilken seksjon som er ansvarlig for teamet.\nTabellÂ 1 viser mer informasjon enn hva som er tilgjengelig i dag i Dapla Ctrl. Grunnen er at det i TabellÂ 1 ogsÃ¥ inkluderes delt-bÃ¸tter i test-miljÃ¸et, samt at Dapla Ctrl kun viser delt-bÃ¸tter hvor andre team faktisk har blitt gitt tilgang. I fremtiden vil denne informasjonen kunne inspiseres i en datakatalog, med mye mer detaljert informasjon om dataene som deles.\nFigurÂ 1 viser at det er avdeling 200 som har opprettet flest bÃ¸tter, i alt 52 delt-bÃ¸tter. Deretter fÃ¸lger avdeling 300 med 26 bÃ¸tter.\nAv FigurÂ 2, som viser hvilke 10 team som har flest delt-bÃ¸tter, sÃ¥ ser vi at 48 av de 52 bÃ¸ttene i avdeling 200 kommer fra team Kostra. Vi ser ogsÃ¥ at team arbmark-register har opprettet 9 av totalt 26 bÃ¸tter i avdeling 300.\nFigurÂ 1: Antall delt-bÃ¸tter per avdeling og miljÃ¸\nFigurÂ 2: Antall delt-bÃ¸tter per team og miljÃ¸"
  },
  {
    "objectID": "news/posts/2025-04-22-dapla-metrics-shared-buckets/index.html#footnotes",
    "href": "news/posts/2025-04-22-dapla-metrics-shared-buckets/index.html#footnotes",
    "title": "Statistikk over datadeling pÃ¥ Dapla",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nHelt presist viser tabellen alle delt-bÃ¸tter som er opprettet gjennom shared-buckets-featuren.â†©ï¸"
  },
  {
    "objectID": "news/posts/2025-03-18-dapla-lab-conf-locked-input-fields/index.html",
    "href": "news/posts/2025-03-18-dapla-lab-conf-locked-input-fields/index.html",
    "title": "Forbedring av konfigurasjon i Dapla Lab",
    "section": "",
    "text": "Den 18. mars 2025 rullet vi ut en endring i tjenestekonfigurasjonene til alle tjenester pÃ¥ Dapla Lab. Under Data-fanen er ikke lenger mulig Ã¥ starte en tjeneste som data-admins uten Ã¥ oppgi en begrunnelse. I tillegg vises feltene for Begrunnelse og Varighet nÃ¥ kun hvis man har valgt data-admins under Team og tilgangsgruppe.\nFigurÂ 1 viser hvordan tjenestekonfigurasjonen ser ut etter endringen. FigurÂ 1 (a) viser hvordan Data-fanen ser ut hvis man velger Ã¥ representere developers, mens FigurÂ 1 (b) viser valgene hvis man velger data-admins.\n\n\n\n\n\n\n\n\n\n\n\n(a) developers\n\n\n\n\n\n\n\n\n\n\n\n(b) data-admins\n\n\n\n\n\n\n\nFigurÂ 1: Ulik visning av informasjon basert pÃ¥ om man velger developers eller data-admins"
  },
  {
    "objectID": "news/posts/2025-02-05-daplanytt-jan25/index.html",
    "href": "news/posts/2025-02-05-daplanytt-jan25/index.html",
    "title": "DaplaNytt-mÃ¸te 5.2.2025",
    "section": "",
    "text": "DaplaNytt for januar 2025 ble avholdt som Teams-mÃ¸te 5. februar 2025.\nSe opptaket her (intern lenke).\n\n\n\nSkjembilde fra Teams-mÃ¸te"
  },
  {
    "objectID": "news/posts/2025-01-15-overgang-dapla-lab/index.html",
    "href": "news/posts/2025-01-15-overgang-dapla-lab/index.html",
    "title": "Overgang til Dapla Lab",
    "section": "",
    "text": "Innen 15. februar mÃ¥ alle brukere av dagens Jupyter-miljÃ¸ flytte over til Dapla Lab, SSBs nye arbeidsbenk for staistikkproduksjon og forskning. Innen fristen mÃ¥ kode, filer og kjÃ¸ringer flyttes fra det gamle miljÃ¸et over til Dapla Lab.\nMigreringsguide\nhttps://statistics-norway.atlassian.net/wiki/spaces/DAPLA/pages/4398678345/Migreringsguide+for+Dapla+Lab\nDokumentasjon for Dapla Lab\nhttps://manual.dapla.ssb.no/statistikkere/dapla-lab.html\nDokumentasjon for tjenestene pÃ¥ Dapla Lab\nhttps://manual.dapla.ssb.no/statistikkere/jupyter.html"
  },
  {
    "objectID": "news/posts/2025-03-09-epost-dapla-lab/index.html",
    "href": "news/posts/2025-03-09-epost-dapla-lab/index.html",
    "title": "E-postvarslinger om â€œgamleâ€ tjenester fra Dapla Lab",
    "section": "",
    "text": "Fra og med mandag 10. mars 2025 implementeres automatisk utsending av e-post til brukere som har tjenester som ble startet for mer enn 7 dager siden. FormÃ¥let med varslingen er Ã¥ bevisstgjÃ¸re brukerne pÃ¥ viktigheten av Ã¥ slette tjenester med jevne mellomrom, og starte en ny slik at man kjÃ¸rer pÃ¥ siste versjon.\n\n\n\n\n\n\nEksempel pÃ¥ e-post\n\n\n\nHei Nordmann, Ola\nDu har en eller flere tjenester i Dapla Lab som ble startet for mer enn 7 dager siden. Se oversikt i tabellen under.\nVi anbefaler deg Ã¥ slette tjenester som ble startet for mer enn 7 dager siden. NÃ¥r tjenesten slettes, og du senere starter den opp igjen, fÃ¥r du med deg eventuelle forbedringer som er gjort i tjenesten.\n\n\n\n\n\nTjeneste\n\n\nNavn\n\n\nAntall dager\n\n\n\n\n\n\nvscode-python\n\n\nvscode-etlev\n\n\n25\n\n\n\n\nvscode-python\n\n\nvscode-detlev\n\n\n8\n\n\n\n\nHusk Ã¥ pushe kode til GitHub, og ta vare pÃ¥ andre filer som ligger i tjenestens filsystem, fÃ¸r du sletter. VÃ¦r oppmerksom pÃ¥ alle filer pÃ¥ hjemmeomrÃ¥det ditt slettes nÃ¥r tjenesten slettes.\nLes mer om hvordan man bruker Git og GitHub i SSB."
  },
  {
    "objectID": "news/posts/2024-11-20-daplanytt-nov24/index.html",
    "href": "news/posts/2024-11-20-daplanytt-nov24/index.html",
    "title": "DaplaNytt-mÃ¸te 19.11.2024",
    "section": "",
    "text": "DaplaNytt for november 2024 ble avholdt som Teams-mÃ¸te 19. november 2024.\nSe opptaket her (intern lenke)."
  },
  {
    "objectID": "news/posts/2025-02-09-monter-delt-bÃ¸tter/index.html",
    "href": "news/posts/2025-02-09-monter-delt-bÃ¸tter/index.html",
    "title": "Delt-bÃ¸tter stÃ¸ttes i Dapla Lab",
    "section": "",
    "text": "Man kan nÃ¥ tilgjengeliggjÃ¸re delt-bÃ¸tter fra andre team inne i Jupyter, Vscode-python og Rstudio. Les dokumentasjon her, eller se videoen under:"
  },
  {
    "objectID": "news/posts/2025-04-29-change-data-admins/index.html",
    "href": "news/posts/2025-04-29-change-data-admins/index.html",
    "title": "Endring i tilganger for data-admins",
    "section": "",
    "text": "Ifm med utrulling av en ny type delt-bÃ¸tte ble det oppdaget at noen team hadde en feil i tilgangene for data-admins. Dette ble rettet i sammenheng med utrulling av de nye delt-bÃ¸ttene. I praksis vil data-admins merke dette ved at de ikke lenger har tilgang til produkt-bÃ¸tta nÃ¥r de logger seg som data-admins i Dapla Lab."
  },
  {
    "objectID": "news/posts/2024-11-08-endring-cpu-ram/index.html",
    "href": "news/posts/2024-11-08-endring-cpu-ram/index.html",
    "title": "Endring i default-maskinkraft pÃ¥ Dapla Lab",
    "section": "",
    "text": "FÃ¸rstkommende mandag gjÃ¸r vi en endring i hvor mye RAM og CPU programmeringsmiljÃ¸ene pÃ¥ Dapla Lab har som default. Vi endrer antall millicores (M) CPU fra 2000M til 200M, og mengden RAM fra 8GB til 4GB. Dvs. at brukeren aktivt mÃ¥ Ã¸ke ressursene i tjenestekonfigurasjonen (se FigurÂ 1) fÃ¸r oppstart hvis de Ã¸nsker mer enn dette.\nÃ…rsaken til at vi gjÃ¸r endringen er at vi har observert at flere brukere reserverer ressurser som ikke benyttes, og at dette skaper unÃ¸dvendige kostnader for SSB.\nEndringen gjelder tjenestene Jupyter, Jupyter-playground, Vscode og Rstudio.\n\n\n\n\n\n\nFigurÂ 1: Bilde av Ressurser-fanen under tjenestekonfigurasjonen til Jupyter"
  },
  {
    "objectID": "news/posts/2025-03-31-dapla-lab-metrics-mar25/index.html",
    "href": "news/posts/2025-03-31-dapla-lab-metrics-mar25/index.html",
    "title": "Aktivitet pÃ¥ Dapla Lab i mars 2025",
    "section": "",
    "text": "Fom. 15. februar 2025 har Dapla Lab vÃ¦rt arbeidsbenken for SSB-ere som jobber med data pÃ¥ Dapla. I dette viser vi noen interessante tall om hvor mange som jobber pÃ¥ plattformen.\nFigurÂ 1 viser antall maksimalt antall unike brukere av Dapla Lab ila en dag. Figuren er en oversikt over perioden 25. februar tom. 29. mars 2025.\n\n\n\n\n\n\n\n\n\n\nFigurÂ 1: Maks antall daglige brukere pÃ¥ Dapla Lab\n\n\n\n\n\n\nFigurÂ 2 viser antall unike brukere ila dagen pÃ¥ Dapla Lab. Hver linje i figuren er en dag i perioden 25. februar tom. 29. mars 2025. Dermed gir figuren et bilde av hvilket tidspunkt pÃ¥ dagen brukerne jobber. Den 27. mars 2025 endret vi tidspunktet for automatisk pausing av tjenester fra kl 22.00 til 17.00, og dette er vist figuren ved at dager etter 26. mars vises med grÃ¸nne linjer og resten med grÃ¥ linjer. Av figuren ser vi dermed at det ganske fÃ¥ som jobber pÃ¥ Dapla Lab etter kl 17.00.\n\n\n\n\n\n\n\n\n\n\nFigurÂ 2: Antall brukere gjennom dagen"
  },
  {
    "objectID": "news/posts/2025-01-29-dapla-lab-dagene/index.html",
    "href": "news/posts/2025-01-29-dapla-lab-dagene/index.html",
    "title": "Ã…pen dag om Dapla Lab i Oslo og Kongsvinger",
    "section": "",
    "text": "10. og 11. februar arrangerer vi Ã¥pen dag om Dapla Lab for Ã¥ hjelpe brukere med overgangen til Dapla Lab. Utviklere, tech-coacher og stÃ¸tteteam stiller fysisk for Ã¥ hjelpe deg med Ã¥ flytte kode, filer, sette opp Git- og GitHub-konfigurasjon, eller svare pÃ¥ spÃ¸rsmÃ¥l/innspill man mÃ¥tte ha ifm. overgangen. Vi hÃ¥per at sÃ¥ mange som mulig tar turen innom for Ã¥ fÃ¥ hjelp eller bare slÃ¥ av en prat.\nArrangement i Oslo 10. februar kl. 12 i Auditoriet\nArrangement i Kongsvinger 11. februar kl. 12 i A005"
  },
  {
    "objectID": "news/posts/2024-11-26-statbank-dapla-lab/index.html",
    "href": "news/posts/2024-11-26-statbank-dapla-lab/index.html",
    "title": "Dapla Lab stÃ¸tter Statbank",
    "section": "",
    "text": "Fra og med forrige uke stÃ¸tter Statbank-pakken lasting fra Dapla Lab. Dette er i forbindelse med ny versjon av Python-pakken dapla-statbank-client.\nLes mer om dette i innlegget til Carl Corneil pÃ¥ Viva Engage."
  },
  {
    "objectID": "news/posts/2025-04-11-ssb-altinn-python-fix/index.html",
    "href": "news/posts/2025-04-11-ssb-altinn-python-fix/index.html",
    "title": "Ny versjon av ssb-altinn-python",
    "section": "",
    "text": "Ny release av ssb-altinn-python v0.4.12 er sluppet i Kildomaten. Pakken hÃ¥ndterer nÃ¥ problemet med at UTC-datoformat i feltet ALTINNTIDSPUNKTLEVERT ble trunkert nÃ¥r det sluttet pÃ¥ en eller flere nuller, som igjen forÃ¥rsaket at utflating av XMLâ€™er stoppet/feilet ved konvertering til Oslo-tid."
  },
  {
    "objectID": "statistikkere/jdemetra.html",
    "href": "statistikkere/jdemetra.html",
    "title": "Jdemetra",
    "section": "",
    "text": "Jdemetra er en tjeneste som tilbyr et grafisk grensesnitt (GUI) for sesongjustering og tidsserie-analyse. FormÃ¥let med tjenesten er Ã¥ tilby statistikere i SSB et velkjent verktÃ¸y for opprette nye Jdemetra-workspaces, visuelt inspisere mange tidsserier samtidig, og benytte funksjonalitet som finnes for Ã¥rlige evalueringer av modellene som benyttes.\nJdemetra+ er navnet pÃ¥ en samling programvare for tidsserie-analyse og sesongjustering som er utviklet av Belgias nasjonalbank i samarbeid med Eurostat, Insee og Deutche Bundesbank. I tillegg til GUI-et som tilbys pÃ¥ Dapla Lab, finnes det ogsÃ¥ et CLI for batch-prosessering som heter jwsacruncher, og en R-pakke ved navn RJDemetra. Alle bygger pÃ¥ de samme grunnleggende komponentene. Jwsacruncher er installert i Jupyter og Rstudio pÃ¥ Dapla Lab, mens RJdemetra kan installeres av brukeren selv i de samme tjenestene.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jdemetra"
    ]
  },
  {
    "objectID": "statistikkere/jdemetra.html#forberedelser",
    "href": "statistikkere/jdemetra.html#forberedelser",
    "title": "Jdemetra",
    "section": "Forberedelser",
    "text": "Forberedelser\nFÃ¸r man starter Jdemetra-tjenesten bÃ¸r man ha lest kapitlet om Dapla Lab. Deretter gjÃ¸r du fÃ¸lgende:\n\nLogg deg inn pÃ¥ Dapla Lab\nUnder Tjenestekatalog trykker du pÃ¥ Start-knappen for Jdemetra\nGi tjenesten et navn\nÃ…pne Jdemetra konfigurasjoner",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jdemetra"
    ]
  },
  {
    "objectID": "statistikkere/jdemetra.html#konfigurasjon",
    "href": "statistikkere/jdemetra.html#konfigurasjon",
    "title": "Jdemetra",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nFÃ¸r man Ã¥pner en tjeneste kan man konfigurere hvilket team og tilgangsgruppe man skal representere og dermed hvilke bÃ¸tter man fÃ¥r tilgang til i Jdemetra. Man kan ogsÃ¥ velge hvilken versjon av Jdemetra man Ã¸nsker Ã¥ kjÃ¸re, der default er siste versjon.\n\n\n\n\n\n\nFigurÂ 1: Detaljert tjenestekonfigurasjon i JDemetra-tjenesten: data\n\n\n\nFigurÂ 1 viser hvilke valg man gjÃ¸re under menyen Data. I tillegg viser bildet neddtrekksmenyen for hvilken versjon av Jdemetra man vil bruke. FÃ¸rst kan man velge hvilket team og tilgangsgruppe man Ã¸nsker Ã¥ representere. I tillegg kan man aktivere kildedatatilgang. Alle i SSB er medlem av developers-gruppa i teamet Dapla Felles, derfor kan man velge dette teamet hvis man Ã¸nsker teste ut tjenesten.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jdemetra"
    ]
  },
  {
    "objectID": "statistikkere/jdemetra.html#datatilgang-og-lagring",
    "href": "statistikkere/jdemetra.html#datatilgang-og-lagring",
    "title": "Jdemetra",
    "section": "Datatilgang og lagring",
    "text": "Datatilgang og lagring\nNÃ¥r man Ã¥pner Jdemetra, og velger Ã¥ representere team og tilgangsgruppe, sÃ¥ blir bÃ¸ttene som den tilgangsgruppa har tilgang til, tilgjengeliggjort som et filsystem under /buckets/. Som bruker kan du da lese og skrive til bÃ¸ttene ved benytte denne filstien. F.eks. vil et statistikkteam som Ã¥pner Jdemetra som developers-gruppa ha et filsystem som typisk ser slik ut:\n\n\n/buckets/\n\n/buckets/  \nâ””â”€ produkt/  \n   â”œâ”€â”€ inndata/\n   â”œâ”€â”€ klargjorte-data/\n   â”œâ”€â”€ statistikk/\n   â””â”€â”€ utdata/\nâ””â”€ frasky/  \nâ””â”€ tilsky/                     \n\nI eksempelet over ser vi at bÃ¸ttene produkt, frasky og tilsky ligger under /buckets/.\n\nÃ…pne eksisterende workspace\nHvis jeg velger Ã¥ representere gruppen dapla-felles-developers, sÃ¥ kan jeg Ã¥pne et Jdemetra-workspace som ligger i produktbÃ¸tta til team Dapla Felles ved Ã¥ gjÃ¸re fÃ¸lgende:\n\nVelg File/Open workspace i menyen.\nFinn roten av filsystemet og Ã¥pne mappen /buckets/\nVelg xml-filen som definerer workspacet trykk Open\n\n\n\nOpprette nytt workspace\nFor Ã¥ opprette et nytt workspace sÃ¥ importerer du input-filene pÃ¥ vanlig mÃ¥te under Providers, legger de til i en workspace, og velger /File/Save Workspace As i menyen. Velg en filsti under /buckets/produkt/ for Ã¥ lagre workspacet permanent i en bÃ¸tte.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jdemetra"
    ]
  },
  {
    "objectID": "statistikkere/jdemetra.html#slette-tjenesten",
    "href": "statistikkere/jdemetra.html#slette-tjenesten",
    "title": "Jdemetra",
    "section": "Slette tjenesten",
    "text": "Slette tjenesten\nFor Ã¥ slette tjenesten kan man trykke pÃ¥ Slette-knappen i Dapla Lab under Mine tjenester. NÃ¥r man sletter en tjeneste sÃ¥ sletter man hele disken inne i tjenesten og frigjÃ¸r alle ressurser som er reservert. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jdemetra"
    ]
  },
  {
    "objectID": "statistikkere/jdemetra.html#pause-tjenesten",
    "href": "statistikkere/jdemetra.html#pause-tjenesten",
    "title": "Jdemetra",
    "section": "Pause tjenesten",
    "text": "Pause tjenesten\nMan kan pause tjenesten ved Ã¥ trykke pÃ¥ Pause-knappen i Dapla Lab under Mine tjenester. NÃ¥r man pauser sÃ¥ slettes alt pÃ¥den lokale disken som ikke er lagret under $HOME/work. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jdemetra"
    ]
  },
  {
    "objectID": "statistikkere/jdemetra.html#monitorering",
    "href": "statistikkere/jdemetra.html#monitorering",
    "title": "Jdemetra",
    "section": "Monitorering",
    "text": "Monitorering\n\n\nMan kan moniterere en instans ved Ã¥ trykke pÃ¥ navnet pÃ¥ tjenesten under Mine tjenester i Dapla Lab, slik som vist i FigurÂ 2 med en jupyter-instans.\nDenne funksjonaliteten er under arbeid og mer informasjon kommer snart.\n\n\n\n\n\n\n\n\n\nFigurÂ 2: Monitorering av Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jdemetra"
    ]
  },
  {
    "objectID": "statistikkere/dapla-toolbelt-metadata.html",
    "href": "statistikkere/dapla-toolbelt-metadata.html",
    "title": "dapla-toolbelt-metadata",
    "section": "",
    "text": "dapla-toolbelt-metadata er en Python-pakke for Ã¥ jobbe med metadatasystemene pÃ¥ Dapla. Pakken gir brukeren et Python-grensesnitt for Ã¥ jobbe mot Datadoc og Vardef.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata"
    ]
  },
  {
    "objectID": "statistikkere/dapla-toolbelt-metadata.html#forberedelser",
    "href": "statistikkere/dapla-toolbelt-metadata.html#forberedelser",
    "title": "dapla-toolbelt-metadata",
    "section": "Forberedelser",
    "text": "Forberedelser\ndapla-toolbelt-metadata kan installeres i tjenester pÃ¥ Dapla Lab. Siden det er en Python-pakke sÃ¥ mÃ¥ den installeres i en tjeneste der Python er installert. Deretter gjÃ¸r du fÃ¸lgende:\n\nÃ…pne en tjeneste pÃ¥ Dapla Lab med Python installert. FÃ¸r du Ã¥pner tjenesten mÃ¥ du velge Ã¥ representere team og tilgangsgruppe som har tilgang til dataene som skal dokumenteres.\nInstaller pakken i et ssb-project pÃ¥ fÃ¸lgende mÃ¥te:\n\n\n\nTerminal\n\npoetry add dapla-toolbelt-metadata\n\nEtter det er du klar for bruke funksjonaliteten i dapla-toolbelt-metadata i en notebook.\n\n\n\n\n\n\nDapla Lab og dapla-toolbelt-metadata\n\n\n\nI Dapla Lab velger man hvilket team og tilgangsgruppe man skal representere fÃ¸r man Ã¥pner en tjeneste. Hvis man f.eks. skal dokumentere et datasett i datatilstanden inndata for et team som heter dapla-felles, sÃ¥ mÃ¥ man logge seg inn i en tjeneste som dapla-felles-developers, hvis ikke har man ikke tilgang til datasettet.\nI tillegg sÃ¥ benytter Vardef bl.a. informasjon om hvilket team og tilgangsgruppe du logget deg inn som, for definere hvilket team som blir eier av en nyopprettet variabeldefinisjon. PÃ¥ samme mÃ¥te bruker Vardef denne informasjonen til Ã¥ avgjÃ¸re om en bruker har tilgang til Ã¥ endre en definisjon. F.eks. vil en bruker som logger seg inn som dapla-felles-developers og oppretter en ny definisjon, sÃ¥ vil dapla-felles stÃ¥ som eier av definisjonen og det vil kun vÃ¦re medlemmer av dette teamet som kan gjÃ¸re endringer i definisjonen.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata"
    ]
  },
  {
    "objectID": "statistikkere/datadoc.html",
    "href": "statistikkere/datadoc.html",
    "title": "Datadoc",
    "section": "",
    "text": "Datadoc er SSBs system for dokumentasjon av datasett pÃ¥ Dapla. Hensikten er at alle datasett i de obligatoriske datatilstandene (klargjorte data, statistikk og utdata) skal dokumenteres ett sted, og informasjonen skal knyttes tett til tilhÃ¸rende datasett. Den obligatoriske datatilstanden kildedata skal ogsÃ¥ dokumenters i Datadoc, men her vil det kreves mindre detaljert informasjon enn for de tre andre tilstandene. Kravene til dokumentasjon er stÃ¸rre for klargjorte data, statistikk og utdata siden dette er datatilstander som skal kunne deles med andre1.\nI Datadoc skal bÃ¥de informasjon om alle kolonner, og informasjon om datasettet som helhet, dokumenteres. Informasjon om datasettet kan f.eks. vÃ¦re en beskrivelse av hva datasettet inneholder, hvilket Dapla-team som eier det, om det inneholder personopplysninger, og om det er bruksrestriksjoner knyttet til dataene. I tillegg vil en del felter bli maskingenerert, f.eks. identifikator, filsti og hvilke datoer datasettet inneholder data fra og til. Hele modellen for datasett finnes her: DataDoc - Krav til dokumentasjon av datasett pÃ¥ Dapla - Metadata pÃ¥ DAPLA - Confluence.\nInformasjon om enkeltkolonner (ofte omtalt som variabelforekomster) i datasettet skal ogsÃ¥ dokumenteres. Denne informasjonen skal bl.a. inneholde en beskrivelse av variabelen. Dette skal primÃ¦rt gjÃ¸res ved at en lenker til tilhÃ¸rende variabeldefinisjon i Vardef. I tillegg til beskrivelsen, skal bl.a. mÃ¥leenhet (hvis det er en kvantitativ variabel), datatype og (dersom variabelen er en personopplysning) om verdiene er pseudonymisert, dokumenteres. Hele modellen for variabelforekomster finne her: Variabelforekomst - Metadata pÃ¥ DAPLA - Confluence.\nFigurÂ 1 viser et datasett der en av variabelforekomstene er sivst. Den er lenket opp til definisjonen av sivilstand i Vardef, som igjen er lenket opp til kodeverket som beskriver kategoriene i Â«Standard for sivilstandÂ» i Klass.",
    "crumbs": [
      "Manual",
      "Metadata",
      "Datadoc"
    ]
  },
  {
    "objectID": "statistikkere/datadoc.html#footnotes",
    "href": "statistikkere/datadoc.html#footnotes",
    "title": "Datadoc",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nUtdata er Ã¥pne data, mens klargjorte data og statistikkdata kan deles med brukere som har rett pÃ¥ tilgang.â†©ï¸",
    "crumbs": [
      "Manual",
      "Metadata",
      "Datadoc"
    ]
  },
  {
    "objectID": "statistikkere/dapla-statbank-client.html",
    "href": "statistikkere/dapla-statbank-client.html",
    "title": "dapla-statbank-client",
    "section": "",
    "text": "dapla-statbank-client er en Python-pakke som lar SSB-ere laste inn og hente ut data fra Statistikkbanken fra Dapla Lab1. Pakken tilbyr ulik funksjonalitet som gjÃ¸r det enkelt Ã¥ jobbe mot Statistikkbanken:",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-statbank-client"
    ]
  },
  {
    "objectID": "statistikkere/dapla-statbank-client.html#tilgangsstyring",
    "href": "statistikkere/dapla-statbank-client.html#tilgangsstyring",
    "title": "dapla-statbank-client",
    "section": "Tilgangsstyring",
    "text": "Tilgangsstyring\nTilgang til funksjonalitet i Statistikkbanken styres av brukernavn (â€œlastebrukerâ€) og passord (â€œlastepassordâ€) som brukeren mÃ¥ oppgi. Tilgang til data som skal lastes inn i banken krever at man representerer teamet som har tilgang til dataene nÃ¥r man starter Dapla Lab.\nStatistikkbanken har en prod- og testdatabase. Brukere har tilgang til Ã¥ publisere til begge deler fra Dapla Lab sitt prod-miljÃ¸. Fra Daplas testmiljÃ¸, kan man kun publisere til Statistikkbankens testdatabase.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-statbank-client"
    ]
  },
  {
    "objectID": "statistikkere/dapla-statbank-client.html#forberedelser",
    "href": "statistikkere/dapla-statbank-client.html#forberedelser",
    "title": "dapla-statbank-client",
    "section": "Forberedelser",
    "text": "Forberedelser\nFor Ã¥ bruke dapla-statbank-client i Dapla Lab eller prodsonen mÃ¥ man opprette et ssb-project og installere pakken pÃ¥ fÃ¸lgende mÃ¥te:\n\n\nTerminal\n\npoetry add dapla-statbank-client",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-statbank-client"
    ]
  },
  {
    "objectID": "statistikkere/dapla-statbank-client.html#funksjonalitet",
    "href": "statistikkere/dapla-statbank-client.html#funksjonalitet",
    "title": "dapla-statbank-client",
    "section": "Funksjonalitet",
    "text": "Funksjonalitet\nI denne delen beskrives den viktigste funksjonalitet som tilbys i dapla-statbank-client. Det finnes ogsÃ¥ noe funksjonalitet som kun er beskrevet her.\n\nFilbeskrivelse\nMan kan hente ut filbeskrivelsen til en statistikkbanktabell fra Statistikkbanken med fÃ¸lgende kode:\n\n\nNotebook\n\nfrom statbank import StatbankClient\nstat_client = StatbankClient()  # Ber om lastebruker og lastepassord\n\ntabell = \"06339\"\nfilbeskrivelse = stat_client.get_description(tableid=tabell)\nprint(filbeskrivelse)\n\nI eksempelet over henter vi filbeskrivelsen til statistikkbanktabell 06339 og printer den ut. Beskrivelse-objektet inneholder alle formelle krav til dataene som kan lastes inn i denne tabellen.\n\n\nLaste tabeller\nMan kan laste data inn i Statistikkbankens databaser med dapla-statbank-client. Dataene som skal lastes inn mÃ¥ sendes inn som en dictionary med deltabellnavn som nÃ¸kler, og Pandas dataframes som verdier. Under er et eksempel pÃ¥ at en tabell lastes for publisering 1. januar 2050:\n\n\nNotebook\n\nfrom statbank import StatbankClient\n\nstat_client = StatbankClient(\n    date=\"2050-01-01\",\n    overwrite=True,\n    approve=2\n)\n\nstat_client.transfer({\"deltabellfilnavn.dat\" : df_06399},\n                     \"06339\")\n\nI eksempelet over ser vi ogsÃ¥ at vi sender inn parametere, om vi Ã¸nsker at eksisterende tabeller skal overskrives, og at godkjenning skal skje Just-in-Time. I tillegg har vi spesifiser en publiseringsdato (merk at statbanken kun godtar publiseringer en viss antall mÃ¥neder frem i tid).\nUnder er et eksempel pÃ¥ lasting av flere deltabeller samtidig.\n\n\nNotebook\n\nfrom statbank import StatbankClient\nstat_client = StatbankClient(\n    date=\"2050-01-01\",\n    overwrite=True,\n    approve=2\n)\n\ndata_07495 = {\"kargrs01fylker1.dat\" : df_07495_fylker,\n              \"kargrs01landet1.dat\" : df_07495_landet,}\n\nstat_client.transfer(data_07495, tableid=\"07495\")\n\n\nAvrunding av desimaltall\nMetoden round_data() lar brukeren avrunde desimaltall iht filbeskrivelsen til en statistikkbanktabell. Dette kan vÃ¦re nyttig siden, antall desimaler som skal beholdes ligger markert i filbeskrivelsen, og R og Python hÃ¥ndterer avrunding annerledes enn det mange forventer (se boks under).\nI eksempelet under har vi en dataframe med navn data_07495 som inneholder data som skal lastes i Statistikkbanken. For Ã¥ avrunde denne og lagre endringene, sÃ¥ lagrer vi resultatet tilbake til den opprinnelige dictionaryenâ€™n. Ved dags dato er det smart Ã¥ runde av ETTER man validerer, siden validate() ikke tar hÃ¸yde for at dataene kan vÃ¦re avrundet allerede.\n\n\nNotebook\n\nfrom statbank import StatbankClient\nstat_client = StatbankClient()\n\n# Hente filbeskrivelse\ntabell = \"07495\"\nfilbeskrivelse = stat_client.get_description(tableid=tabell)\n\n# Pakk datasettet i dictionaryen (navnet pÃ¥ deltabellen ligger i filbeskrivelsen)\ndata_07495 = filbeskrivelse.transferdata_template(df_07495)\n\n# For Ã¥ ta vare pÃ¥ endringene, sÃ¥ mÃ¥ du skrive tilbake til datasettet i dictionaryen\ndata_07495 = filbeskrivelse.round_data(data_07495)\n\n\n\n\n\n\n\nAvrunding i Python og R\n\n\n\nAvrunding av desimaltall i Python og R gjÃ¸res mot nÃ¦rmeste partall. Dvs. at 2,5 blir avrundet til 2, og 1,5 blir ogsÃ¥ avrundet til 22. Dette er ulikt hvordan tall blir avrundet i SAS og Excel, der 2,5 blir til 3, og 1,5 blir til 2. Bruker man round_data() i dapla-statbank-client sÃ¥ er det sistnevnte metode som benyttes.\n\n\n\n\nLaste til test\nStatistikkbanken har et test-miljÃ¸ som kan benyttes hvis man Ã¸nsker. Det er Ã¥pnet for Ã¥ laste til Statistikkbankens test-miljÃ¸et fra bÃ¥de Dapla Lab prod og Dapla Lab test.\nHvis man Ã¸nsker Ã¥ laste til Statistikkbankens test-miljÃ¸ fra Dapla Lab prod, sÃ¥ mÃ¥ man eksplisitt angi dette i argumententet til StatbankClient(use_db='TEST'). Det er derimot ikke mulig Ã¥ laste til Statistikkbankens prod-miljÃ¸ fra Dapla Lab test.\n\n\n\nValidering\nMetoden validate() lar brukeren validere en dataframe mot en filbeskrivelsen. Dette er nyttig siden man kan fÃ¥ tilbakemelding om eventuelle feil fÃ¸r man sender over data til Statistikkbanken. Spesielt er dette interessant under utvikling av en statistikkbanklevering i nytt kodesprÃ¥k, hvor validate() kan gi deg tilbakemelding pÃ¥ hva du har gjort feil hittils med formen pÃ¥ dataene.\n\n\nNotebook\n\nfrom statbank import StatbankClient\nstat_client = StatbankClient()\n\n# Hente filbeskrivelse\ntabell = \"07495\"\nfilbeskrivelse = stat_client.get_description(tableid=tabell)\n\n# Pakk deltabeller i en dict fÃ¸r validering\ndata_07495 = filbeskrivelse.transferdata_template(df_07495)\n\n# Vi tar ikke vare pÃ¥ valideringene, men de kan raise errors, eller printes ut.\nfilbeskrivelse.validate(data_07495)\n\n\n\nPubliserte tabeller\nMan kan hente ut publiserte data fra Statistikkbankens Ã¥pne API med dapla-statbank-client. apidata_all() henter all data i tabellen, uten Ã¥ spesifisere noe filter:\n\n\nNotebook\n\nfrom statbank import apidata_all\n\ndf_06339 = apidata_all(\"06339\", include_id=True)\n\nHvis man spesifiserer include_id=True sÃ¥ fletter funksjonen id-kolonner (variabel-koder) for klassifikasjonsvariablene etter â€œlabel-kolonnerâ€.\nMan kan ogsÃ¥ spesifisere akkurat hvilken informasjon man Ã¸nsker fra en tabell, dette â€œqueryâ€ objektet kan man kopiere ut av statstikkbankens nettsider, etter Ã¥ ha filtrert en tabell sÃ¥ ligger det under â€œAPI-spÃ¸rring for denne tabellenâ€ i det andre feltet. Dette kan du copy-paste ut og assigne til variabelen â€œqueryâ€ her. I fÃ¸lgende kodebit henter vi ogsÃ¥ fra intern statbank (upublisert?) ved Ã¥ spesifisere hele adressen, ikke kun numerisk statbank-id.\n\n\nNotebook\n\nfrom statbank import apidata\n\nquery = {'query': [{'code': 'Region', 'selection': {'filter': 'vs:Landet', 'values': ['0']}}, {'code': 'Alder', 'selection': {'filter': 'vs:AldGrupp19', 'values': ['000', '001', '002', '003', '004', '005', '006', '007', '008', '009', '010', '011', '012', '013', '014', '015', '016', '017', '018', '019', '020', '021', '022', '023', '024', '025', '026', '027', '028', '029', '030', '031', '032', '033', '034', '035', '036', '037', '038', '039', '040', '041', '042', '043', '044', '045', '046', '047', '048', '049', '050', '051', '052', '053', '054', '055', '056', '057', '058', '059', '060', '061', '062', '063', '064', '065', '066', '067', '068', '069', '070', '071', '072', '073', '074', '075', '076', '077', '078', '079', '080', '081', '082', '083', '084', '085', '086', '087', '088', '089', '090', '091', '092', '093', '094', '095', '096', '097', '098', '099', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119+']}}, {'code': 'Statsbrgskap', 'selection': {'filter': 'vs:Statsborgerskap', 'values': ['000']}}, {'code': 'Tid', 'selection': {'filter': 'item', 'values': ['2022']}}], 'response': {'format': 'json-stat2'}}\n\ndf_folkemengde = apidata(\"https://i.ssb.no/pxwebi/api/v0/no/prod_24v_intern/START/be/be01/folkemengde/Rd0002Aa\",\n                         query,\n                         include_id = True\n)\n\n\n\nIntern statistikkbank\nSSB-brukere kan hente data fra SSBs interne Statistikkbank:\n\n\nNotebook\n\nfrom statbank import apidata\n\nuri = \"https://i.ssb.no/pxwebi/api/v0/no/prod_24v_intern/START/be/be01/folkemengde/Rd0002Aa\"\n\nquery = {'query': [{'code': 'Region', 'selection': {'filter': 'vs:Landet', 'values': ['0']}}, {'code': 'Alder', 'selection': {'filter': 'vs:AldGrupp19', 'values': ['000', '001', '002', '003', '004', '005', '006', '007', '008', '009', '010', '011', '012', '013', '014', '015', '016', '017', '018', '019', '020', '021', '022', '023', '024', '025', '026', '027', '028', '029', '030', '031', '032', '033', '034', '035', '036', '037', '038', '039', '040', '041', '042', '043', '044', '045', '046', '047', '048', '049', '050', '051', '052', '053', '054', '055', '056', '057', '058', '059', '060', '061', '062', '063', '064', '065', '066', '067', '068', '069', '070', '071', '072', '073', '074', '075', '076', '077', '078', '079', '080', '081', '082', '083', '084', '085', '086', '087', '088', '089', '090', '091', '092', '093', '094', '095', '096', '097', '098', '099', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119+']}}, {'code': 'Statsbrgskap', 'selection': {'filter': 'vs:Statsborgerskap', 'values': ['000']}}, {'code': 'Tid', 'selection': {'filter': 'item', 'values': ['2022']}}], 'response': {'format': 'json-stat2'}}\n\ndf_folkemengde = apidata(uri, query, include_id = True)",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-statbank-client"
    ]
  },
  {
    "objectID": "statistikkere/dapla-statbank-client.html#footnotes",
    "href": "statistikkere/dapla-statbank-client.html#footnotes",
    "title": "dapla-statbank-client",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPakken fungerer ogsÃ¥ pÃ¥ treningsarenaen til Dapla; Jupyter i prodsonenâ†©ï¸\nGrunnen til at R og Python runder av desimaltall nÃ¦rmeste partall er at det reduserer bias i en retning for en kolonne. Dersom alle tall rundes opp vil summen av en kolonne *dras oppover**. â†©ï¸",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-statbank-client"
    ]
  },
  {
    "objectID": "statistikkere/poetry-ssb-project.html",
    "href": "statistikkere/poetry-ssb-project.html",
    "title": "PakkehÃ¥ndtering i Python",
    "section": "",
    "text": "I tillegg til Ã¥ opprette GitHub repoer etter vÃ¥r SSB-mal hjelper SSB-project deg med Ã¥ lage kernels og holde styr pÃ¥ Python-pakkene dine ved bruk av Poetry",
    "crumbs": [
      "Manual",
      "Kode",
      "PakkehÃ¥ndtering i Python"
    ]
  },
  {
    "objectID": "statistikkere/poetry-ssb-project.html#installere-pakker",
    "href": "statistikkere/poetry-ssb-project.html#installere-pakker",
    "title": "PakkehÃ¥ndtering i Python",
    "section": "Installere pakker",
    "text": "Installere pakker\n\n\n\n\n\n\nForsikre deg om at pakken er trygg!\n\n\n\n\n\nFÃ¸r du installerer en pakke bÃ¸r gjÃ¸re fÃ¸lgende for Ã¥ sikre deg at du ikke installerer en pakke med skadelig kode:\n\nSÃ¸k opp pakken pÃ¥ PyPI.\nSjekk om pakken er et populÃ¦rt/velkjent prosjekt ved Ã¥ besÃ¸ke repoet der koden ligger. Antall Stars og Forks pÃ¥ gitHub er en grei indikasjon pÃ¥ dette.\nHvis du er i tvil om pakken er trygg Ã¥ installere, sÃ¥ kan du spÃ¸rre kollegaer om de har erfaring med den, eller spÃ¸rre pÃ¥ en egnet Yammer-kanal i SSB.\nHvis du fortsatt Ã¸nsker Ã¥ installere pakken sÃ¥ anbefaler vi Ã¥ copy-paste navnet fra PyPi, ikke skrive det inn manuelt nÃ¥r du installerer.\n\n\n\n\nNÃ¥r du har opprettet et ssb-project kan du installere de python-pakkene du trenger fra PyPI.\nSelve installeringen av pakken gjÃ¸res enkelt pÃ¥ fÃ¸lgende mÃ¥te:\n\nÃ…pne en terminal i Jupyterlab.\nGÃ¥ inn i prosjektmappen din ved Ã¥ skrive:\n\n\n\nterminal\n\ncd &lt;sti til prosjektmappe&gt;\n\n\nLag en branch/utviklingsbranch som f.eks. heter install-pandas:\n\n\n\nterminal\n\ngit checkout -b install-pandas\n\n\nInstaller, f.eks. Pandas, ved Ã¥ skrive fÃ¸lgende:\n\n\n\nterminal\n\npoetry add pandas\n\n\n\n\n\n\n\nFigurÂ 1: Installasjon av Pandas med ssb-project\n\n\n\nFigurÂ 1 viser hvordan dette vil se ut i en Jupyterlab-terminal. Kommandoen for Ã¥ installere noe er poetry add etterfulgt av pakkenavnet. Vi ser ogsÃ¥ at den automatisk legger til Pandas-versjonen i filen poetry.lock.\nDu kan ogsÃ¥ spesifisere en konkret versjon av pakken som skal installeres med fÃ¸lgende kommando:\n\n\nterminal\n\npoetry add pandas@1.2.3\n\n\nAvinstallere pakker\nOfte eksperimenterer man med nye pakker, og alle blir ikke med videre i produksjonskoden. Det er god praksis Ã¥ fjerne pakker som ikke brukes, blant annet for Ã¥ unngÃ¥ at de blir en sikkerhetsrisiko. Det gjÃ¸r du enkelt ved Ã¥ skrive fÃ¸lgende i terminalen:\n\n\nterminal\n\npoetry remove pandas\n\n\n\nOppdatere pakker\nHvis det kommer en ny versjon av en pakke du bruker, sÃ¥ kan du oppdatere den med fÃ¸lgende kommando:\n\n\nterminal\n\npoetry update pandas\n\nhvis du kjÃ¸rer poetry update uten noe pakkenavn, sÃ¥ vil alle pakkene dine oppdateres til siste versjon, med mindre du har spesifisert versjonsbegrensninger i pyproject.toml-filen.\n\n\nUndersÃ¸k avhengigheter\nHvis du lurer pÃ¥ hvilke pakker som har hvilke avhengigheter, sÃ¥ kan du lett liste ut dette i terminalen med fÃ¸lgende kommando:\n\n\nterminal\n\npoetry show --tree\n\nDet vil gi en grafisk fremstilling av avhengighetene som vist i FigurÂ 2.\n\n\n\n\n\n\nFigurÂ 2: Visning av pakke-avhengigheter i ssb-project",
    "crumbs": [
      "Manual",
      "Kode",
      "PakkehÃ¥ndtering i Python"
    ]
  },
  {
    "objectID": "statistikkere/poetry-ssb-project.html#push-til-github",
    "href": "statistikkere/poetry-ssb-project.html#push-til-github",
    "title": "PakkehÃ¥ndtering i Python",
    "section": "Push til GitHub",
    "text": "Push til GitHub\nNÃ¥r du nÃ¥ har installert en pakke sÃ¥ har filen poetry.lock endret seg. For at dine samarbeidspartnere skal fÃ¥ tilgang til denne endringen i et SSB-project, sÃ¥ mÃ¥ du pushe en ny versjon av poetry.lock-filen opp Github, og kollegaene mÃ¥ pulle ned og bygge prosjektet pÃ¥ nytt. Du kan gjÃ¸re dette pÃ¥ fÃ¸lgende mÃ¥te etter at du har installert en ny pakke:\n\nVi kan stage alle endringer med fÃ¸lgende kommando i terminalen nÃ¥r vi stÃ¥r i prosjektmappen:\n\n\n\nterminal\n\ngit add -A\n\n\nDeretter commite en endring, dvs. ta et stillbilde av koden i dette Ã¸yeblikket, ved Ã¥ skrive fÃ¸lgende:\n\n\n\nterminal\n\ngit commit -m \"Installert pandas\"\n\n\nPush det opp til GitHub1. Anta at vi gjorde dette i branchen install-pandas som ble opprettet tidligere. Da kan vi skrive fÃ¸lgende :\n\n\n\nterminal\n\ngit push --set-upstream origin install-pandas\n\nDeretter kan kollegaene dine pulle ned endringene og bygge prosjektet pÃ¥ nytt. Vi forklarer hvordan man kan bygge prosjektet pÃ¥ nytt senere i kapitlet.\n\nHold prosjektet oppdatert\nHvis du sitter med en lokal kopi av prosjektet ditt, og flere andre jobber med den samme kodebasen, sÃ¥ er det viktig at du holder din lokale kopi oppdatert. Hvis du jobber i en branch pÃ¥ en lokal kopi, bÃ¸r du holde denne oppdatert med main-branchen pÃ¥ GitHub. Det er vanlig Git-praksis. NÃ¥r man ogsÃ¥ bruker ssb-project, sÃ¥ man huske Ã¥ ogsÃ¥ bygge prosjektet pÃ¥ nytt hver gang det er endringer som er gjort av andre i poetry.lock.-filen.",
    "crumbs": [
      "Manual",
      "Kode",
      "PakkehÃ¥ndtering i Python"
    ]
  },
  {
    "objectID": "statistikkere/poetry-ssb-project.html#dependabot",
    "href": "statistikkere/poetry-ssb-project.html#dependabot",
    "title": "PakkehÃ¥ndtering i Python",
    "section": "Dependabot",
    "text": "Dependabot\nNÃ¥r man installerer pakker sÃ¥ vil det etter hvert utvikle seg et sett av potensielt kompliserte avhengigheter mellom disse pakkene. Dette skyldes at en pakke kan benytte seg av funksjonalitet i andre pakker, som igjen benytter seg av funksjonalitet i andre pakker, osv.. Hvis noen finner en sikkerhetssÃ¥rbarhet i en pakke sÃ¥ kan det fikses ved at en ny versjon av den pakken slippes, som igjen kan fÃ¥ konsekvenser for pakker som er avhengig av denne.\nI SSB er det tilrettelagt for at alle som versjonshÃ¥ndterer koden sin pÃ¥ GitHub kan skanne pakkene sine for sÃ¥rbarheter og nye versjoner av pakker med Dependabot. Dependabot hjelper oss med Ã¥ finne og fikse sÃ¥rbarheter og gamle pakkeversjoner. Dette er spesielt viktig nÃ¥r man installerer sine egne pakker.\nDependabot sjekker med jevne mellomrom om det finnes oppdateringer i pakkene som er listet i din pyproject.toml-fil, og den tilhÃ¸rende poetry.lock. Hvis det finnes oppdateringer sÃ¥ vil den lage en pull request som du kan godkjenne. NÃ¥r du godkjenner den sÃ¥ vil den oppdatere poetry.lock-filen og lage en ny commit som du kan pushe til GitHub. Dependabot gir ogsÃ¥ en sikkerhetsvarslinger hvis det finnes kjente sÃ¥rbarheter i pakkene du bruker.\nDet er anbefalt at alle som installerer sine egne pakker i SSB skrur pÃ¥ Dependabot i sine GitHub-repoer.\n\nAktivere Dependabot\nDu kan aktivere Dependabot ved Ã¥ gi inn i GitHub-repoet ditt og gjÃ¸re fÃ¸lgende:\n\nGÃ¥ inn repoet\nTrykk pÃ¥ Settings for det repoet som vist pÃ¥ FigurÂ 3.\n\n\n\n\n\n\n\nFigurÂ 3: Ã…pne Settings for et GitHub-repo.\n\n\n\n\nI menyen til venstre velger du Code security and analysis\nUnder seksjonen Dependabot velger Enable pÃ¥ minst Dependabot alerts og Dependabot security updates, slik som vist i FigurÂ 4.\n\n\n\n\n\n\n\nFigurÂ 4: Skru pÃ¥ Dependabot i GitHub.\n\n\n\nNÃ¥r du har gjort dette vil GitHub varsle deg hvis det finnes en kjent sÃ¥rbarhet i pakkene som benyttes.\n\n\nOppdatere pakker\nHvis en av pakkene du bruker kommer med en oppdatering, sÃ¥ vil Dependabot lage en pull request (PR) i GitHub som du kan godkjenne. Dependabot sjekker ogsÃ¥ om oppdateringen er i konflikt med andre pakker du bruker. Hvis det er tilfellet sÃ¥ vil den lage en pull request som oppdaterer alle pakkene som er i konflikt.\nHvis en av pakkene du bruker har en kjent sikkerhetssÃ¥rbarhet, sÃ¥ vil Dependabot varsle deg om dette under Security-fanen i GitHub-repoet ditt. Hvis du trykker pÃ¥ View Dependabot alerts sÃ¥ vil du fÃ¥ en oversikt over alle sÃ¥rbarhetene som er funnet, og hvilken alvorlighetsgrad den har. Hvis du trykker pÃ¥ en av sÃ¥rbarhetene sÃ¥ vil du fÃ¥ mer informasjon om den, og du kan trykke pÃ¥ Create pull request for Ã¥ oppdatere pakken.\nSom nevnt over kan du enkelt oppdatere pakker fra GitHub ved hjelp av Dependabot. Men det finnes tilfeller der du vil teste om en oppdatering gjÃ¸r at deler av koden din ikke fungerer lenger. Anta at du bruker en Python-pakken Pandas i koden din, og at du fÃ¥r en pull request fra Dependabot om Ã¥ oppdatere den fra versjon 1.5 til 2.0. Hvis du Ã¸nsker Ã¥ teste om koden din fortsatt fungerer med den nye versjonen av Pandas, sÃ¥ kan du gjÃ¸re dette i Jupyterlab ved Ã¥ fÃ¸lge ved Ã¥ lage en branch som f.eks. heter update-pandas. Deretter kan du installere den nye versjonen av Pandas med fÃ¸lgende kommando fra terminalen:\n\n\nterminal\n\npoetry update pandas@2.0\n\nHvis du nÃ¥ kjÃ¸rer koden din kan du teste om den fortsatt fungerer som forventet. GjÃ¸r den ikke det kan du tilpasse koden din og pushe endringene til Github. Deretter kan du godkjenne pull requesten fra Dependabot og merge den. Etter dette kan du slette den PR-en som Dependabot lagde for deg.",
    "crumbs": [
      "Manual",
      "Kode",
      "PakkehÃ¥ndtering i Python"
    ]
  },
  {
    "objectID": "statistikkere/poetry-ssb-project.html#footnotes",
    "href": "statistikkere/poetry-ssb-project.html#footnotes",
    "title": "PakkehÃ¥ndtering i Python",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nÃ… pushe til GitHub uten Ã¥ sende ved Personal Access Token fordrer at du har lagret det lokalt sÃ¥ Git kan finne det. Her et eksempel pÃ¥ hvordan det kan gjÃ¸res.â†©ï¸",
    "crumbs": [
      "Manual",
      "Kode",
      "PakkehÃ¥ndtering i Python"
    ]
  },
  {
    "objectID": "statistikkere/kildomaten.html",
    "href": "statistikkere/kildomaten.html",
    "title": "Kildomaten",
    "section": "",
    "text": "Kildomaten er en tjeneste for Ã¥ automatisere overgangen fra kildedata til inndata. Tjenesten lar statistikkere kjÃ¸re sine egne skript automatisk pÃ¥ alle nye filer i kildedatabÃ¸tta og skrive resultatet til produktbÃ¸tta. FormÃ¥let med tjenesten er minimere behovet for tilgang til kildedata samtidig som teamet selv bestemmer hvordan transformasjonn til inndata skal foregÃ¥. Statistikkproduksjon kan da starte i en tilstand der dataminimering og pseudonymisering allerede er gjennomfÃ¸rt.\nProsessering som skal skje i overgangen fra kildedata til inndata har SSB definert til Ã¥ vÃ¦re:\nUnder forklarer vi nÃ¦rmere hvordan man bruker tjenesten. Da forutsetter vi at du har et Dapla-team med tjenesten er aktivert. les mer om hvordan du aktiverer tjenester her (lenker her).",
    "crumbs": [
      "Manual",
      "Datatjenester"
    ]
  },
  {
    "objectID": "statistikkere/kildomaten.html#forberedelser",
    "href": "statistikkere/kildomaten.html#forberedelser",
    "title": "Kildomaten",
    "section": "Forberedelser",
    "text": "Forberedelser\nFÃ¸r et Dapla-team kan ta i bruk Kildomaten mÃ¥ man tjenesten aktivert for teamet. Som standard fÃ¥r alle statistikkteam dette skrudd pÃ¥ i prod-miljÃ¸et som opprettes for teamet. Ã˜nsker du Ã¥ aktivere Kildomaten i test-miljÃ¸et kan dette gjÃ¸res selvbetjent som en feature. Alternativt kan man opprette en Kundeservice-sak og be om Ã¥ hjelp til dette.",
    "crumbs": [
      "Manual",
      "Datatjenester"
    ]
  },
  {
    "objectID": "statistikkere/kildomaten.html#sette-opp-tjenesten",
    "href": "statistikkere/kildomaten.html#sette-opp-tjenesten",
    "title": "Kildomaten",
    "section": "Sette opp tjenesten",
    "text": "Sette opp tjenesten\nI denne delen bryter vi ned prosessen med Ã¥ sette opp Kildomaten i de stegene vi mener det er hensiktsmessig Ã¥ gjÃ¸re det nÃ¥r den settes opp for fÃ¸rste gang pÃ¥ en enkeltkilde. Senere i kapitlet forklarer vi hvordan man setter den opp flere kilder og hvordan man vedlikeholder tjenesten.\nHusk at alle pÃ¥ teamet kan gjÃ¸re det meste av arbeidet her, men det er data-admins som mÃ¥ godkjenne at tjenesten rulles ut1.\n\nKlone IaC-repoet\nOppsett av Kildomaten gjÃ¸res i teamets IaC-repo2. NÃ¥r vi skal sette opp Kildomaten-kilde mÃ¥ vi gjÃ¸re gjÃ¸re endringer i teamets IaC-repo. Man finner teamets IaC-repo ved gÃ¥ inn pÃ¥ SSBs GitHub-organisasjon og sÃ¸ke etter repoet som heter &lt;teamnavn&gt;-iac. NÃ¥r du har funnet repoet sÃ¥ kan du gjÃ¸re fÃ¸lgende:\n\nKlon ned ditt teams Iac-repo git clone &lt;repo-url&gt;\nOpprett en ny branch: git checkout -b add-kildomaten-source\n\n\n\nMappestruktur i IaC-repo\nFor at Kildomaten skal fungere sÃ¥ mÃ¥ det opprettes en bestemt mappestruktur i IaC-repoet til teamet. NÃ¥r et team blir opprettet vil den grunnleggende mappestrukturen i IaC-repoet allerede vÃ¦re opprettet for prod-miljÃ¸et til teamet. F.eks. vil mappestrukturen se slik ut for team dapla-example:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\nâ”œâ”€â”€ automation/\nâ”‚   â””â”€â”€ source-data/\nâ”‚       â”œâ”€â”€ dapla-example-prod/\nâ”‚           â””â”€â”€ README.md\nâ”‚ \nâ”‚...           \n\nSkal du sette opp Kildomaten i prod-miljÃ¸et sÃ¥ kan du fÃ¸lge oppskriften som kommer senere i kapitlet uten Ã¥ gjÃ¸re noe mer enda.\nSkal du ogsÃ¥ bruke Kildomaten i test-miljÃ¸et sÃ¥ mÃ¥ opprette en ny mappe og lage en PR i IaC-repoet til teamet. Da vil strukturen se slik ut:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\nâ”œâ”€â”€ automation/\nâ”‚   â””â”€â”€ source-data/\nâ”‚       â”œâ”€â”€ dapla-example-prod/\nâ”‚       â”‚    â””â”€â”€ README.md\nâ”‚       â”œâ”€â”€ dapla-example-test/ \nâ”‚...           \n\nI mappestrukturen over sÃ¥ har vi klargjort den grunnleggende mappestrukturen for Ã¥ ta i bruk Kildomaten i prod- og test-miljÃ¸et. Neste steg blir Ã¥ legge de ulike kildene som egne mapper under dapla-example-prod og dapla-example-test. Det viser vi i neste avsnitt.\n\n\nFlere kilder\nKildomaten lar deg prosessere ulike filstier i kildebÃ¸tta med ulike python-script. Dette refereres til som at Kildomaten har flere kilder. For Ã¥ sette opp en kilde sÃ¥ mÃ¥ man fÃ¸lge en definert mappestruktur i IaC-repoet der alle kildene ligger rett under &lt;teamnavn&gt;-prod- eller &lt;teamnavn&gt;-test-mappen. Du kan ikke ha undermapper under en kilde. Du velger selv navnet pÃ¥ kildene/mappene i IaC-repoet, og det vil vÃ¦re navnet pÃ¥ kildene i Kildomaten. Senere i kapitlet ser vi at vi mÃ¥ bruke navnet for trigge re-kjÃ¸ring av kilder.\nUnder er et eksempel pÃ¥ hvordan det kan se ut for eksempel-teamet dapla-example:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\nâ”œâ”€â”€ automation/\nâ”‚   â””â”€â”€ source-data/\nâ”‚       â”œâ”€â”€ dapla-example-prod/\nâ”‚       â”‚    â””â”€â”€ altinn\nâ”‚       â”‚    â””â”€â”€ ameld\nâ”‚       â”œâ”€â”€ dapla-example-test/\nâ”‚       â”‚    â””â”€â”€ altinn\nâ”‚       â”‚    â””â”€â”€ ameld\nâ”‚       â”‚    â””â”€â”€ nudb\nâ”‚...           \n\nI eksempelet over ser vi at det er opprettet kildene altinn og ameld for bÃ¥de test- og prod-miljÃ¸et. I tillegg er det i test-miljÃ¸et kjÃ¸rende en annen kilde som heter nudb. Hver av disse kildene kan kjÃ¸re et eget Pyton-script pÃ¥ alle filer som skrives til en gitt filsti som man definerer selv.\n\n\n\n\n\n\nAlle kilder pÃ¥ samme nivÃ¥\n\n\n\nEt team kan sette opp mange kilder som skal prosesseres med ulike skript. Men Kildomaten tillater bare et nivÃ¥ under automation/source-data-&lt;teamnavn&gt;-&lt;miljÃ¸&gt;/. Det vil si at du ikke kan ha undermapper under en kilde. Det er ogsÃ¥ slik at man alltid mÃ¥ opprette en mappe for en kilde, selv om du kun har en kilde. F.eks. kan man ikke droppe mappen altinn i eksempelet over.\n\n\n\n\nKonfigurasjon og skript\nNÃ¥r mappestrukturen er opprettet kan man legge til filene som beskriver hvilke filer som skal prosesseres, og python-skriptet med koden som skal prosessere filene. Det er to filer som mÃ¥ eksistere for at tjenesten skal fungere:\n\nEn konfigurasjonsfil\nEt Python-skript\n\nNÃ¥r du har opprettet de skal de ligge pÃ¥ denne mÃ¥ten i IaC-repoet:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\nâ”œâ”€â”€ automation/\nâ”‚   â””â”€â”€ source-data/\nâ”‚       â”œâ”€â”€ dapla-example-prod/\nâ”‚           â””â”€â”€ altinn/\nâ”‚               â”œâ”€â”€ config.yaml\nâ”‚               â””â”€â”€ process_source_data.py\nâ”‚...           \n\nUnder forklares hvordan skriver innholdet i de to filene.\n\nKonfigurasjonsfil\nKildomaten trigges ved at det oppstÃ¥r nye filer i kildebÃ¸tta til teamet. Hvorvidt den skal trigges pÃ¥ alle filer, eller kun filer i en gitt undermappe, bestemmer brukeren ved Ã¥ konfigurere tjenesten i config.yaml. Her kan du ogsÃ¥ angi hvor mye ressurser prosesseringen skal fÃ¥.\nHvis vi fortsetter eksempelet vÃ¥rt fra tidligere med dapla-example, sÃ¥ kan vi tenkes oss at teamet Ã¸nsker Ã¥ Kildomaten skal trigges pÃ¥ alle filer som oppstÃ¥r i kildebÃ¸tta under filstien:\nssb-dapla-example-data-kilde-prod/ledstill/altinn/.\nFor Ã¥ konfigurere tjenesten i Kildomaten mÃ¥ vi legge til en fil som heter config.yaml under automation/source-data/dapla-example-prod/altinn/, slik som vist i forrige kapitel. I dette tilfellet vil den ha innholdet som vist til venstre under:\n\n\n\n\nconfig.yaml\n\nfolder_prefix: ledstill/altinn\nmemory_size: 1  # GiB\n\n\n\n\n\n\nssb-dapla-example-data-kilde-prod/\n\nâ”œâ”€â”€ ledstill/\nâ”‚   â””â”€â”€ altinn/\nâ”‚   â””â”€â”€ aordningen/\nâ”œâ”€â”€ sykefra/\nâ”‚   â””â”€â”€ altinn/\nâ”‚   â””â”€â”€ freg/\nâ”‚...\n       \n\n\n\nMappestrukturen til hÃ¸yre over viser hvordan vi mappestrukturen ser ut i kildebÃ¸tta, mens config.yaml-fila til venstre viser hvordan vi angir at Kildomaten kun skal trigges pÃ¥ nye filer som oppstÃ¥r i undermappen ssb-dapla-example-data-kilde-prod/ledstill/altinn/. Vi bruker nÃ¸kkelen folder_prefix for Ã¥ angi hvilken sti i kildebÃ¸tta som tjenesten skal trigges pÃ¥. NÃ¸kkelen memory_size lar brukeren angi hvor mye minne og CPU hver prosessering skal fÃ¥.\n\n\n\n\n\n\nHvor mye minne og CPU er mulig?\n\n\n\nSom standard sÃ¥ fÃ¥r hver prosessering med Kildomaten 1 CPU-kjerne og 512MB minne/RAM. Hvis man skal gjÃ¸re mye prossesering, eller filene som prosesseres er veldig store, kan man sette memory_size opp til max 32GB minne. Antall CPU-kjerner blir satt implisitt ut i fra valg av memory_size iht til begrensningene som Google setter for Cloud Run. Se de nÃ¸yaktige verdiene som blir satt her.\n\n\n\n\nPython-skript\n\n\n\n\n\n\nHusk dette nÃ¥r du skriver skriptet ditt\n\n\n\nNÃ¥r du skal skrive et Python-skript for Kildomaten er det spesielt viktig Ã¥ huske pÃ¥ 2 ting:\n\nSkriptet ditt kommer til Ã¥ bli kjÃ¸rt pÃ¥ en-og-en fil.\nSkriptet ditt mÃ¥ skrive ut et unikt navn pÃ¥ filen som skal skrives til produktbÃ¸tta, ellers risikerer du at filer blir overskrevet.\nPython-pakkene som kan benyttes er definert her. Ved behov for andre pakker, ta kontakt med Kundeservice.\n\n\n\n\nPython-skriptet er der teamet kan angi hva slags kode som skal kjÃ¸re pÃ¥ hver fil som dukker opp i den angitte mappen i kildebÃ¸tta. For at dette skal vÃ¦re mulig mÃ¥ koden fÃ¸lge disse reglene:\n\nKoden mÃ¥ ligge i en fil som heter process_source_data.py.\nKoden mÃ¥ pakkes inn i en funksjon som heter main().\n\nmain() er en funksjon med ett parameter som heter source_file som du alltid fÃ¥r av Kildomaten nÃ¥r en fil blir prosessert. NÃ¥r du skriver koden kan man derfor anta at dette parameteret blir gitt til funksjonen, og du kan benytte det inne i main-funksjonen.\nsource_file-parameteret gir main() en ferdig definert filsti til filen som skal prosesseres. For eksempel sÃ¥ kan source_file ha verdien\ngs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv hvis filen test20231010.csv dukker opp i ssb-dapla-example-data-kilde-prod/ledstill/altinn/.\nHvis vi fortsetter eksempelet fra tidligere sÃ¥ ser mappen i IaC-repoet vÃ¥rt slik ut nÃ¥:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\nâ”œâ”€â”€ automation/\nâ”‚   â””â”€â”€ source-data/\nâ”‚       â”œâ”€â”€ dapla-example-prod/\nâ”‚           â””â”€â”€ altinn/\nâ”‚               â”œâ”€â”€ config.yaml\nâ”‚               â””â”€â”€ process_source_data.py\nâ”‚\nâ”‚...         \n\nVi ser nÃ¥ at filene config.yaml og process_source_data.py ligger i mappen\nautomation/source-data/dapla-example-prod/ledstill/altinn/. Senere, nÃ¥r vi har rullet ut tjenesten, vil en ny fil i gs://ssb-dapla-exmaple-data-kilde-prod/ledstill/altinn/ trigge Kildomaten og kjÃ¸re koden i process_source_data.py pÃ¥ filen.\nUnder ser du et eksempel pÃ¥ hvordan en vanlig kodesnutt kan konverteres til Ã¥ kjÃ¸re i Kildomaten:\n\n\n\n\nVanlig kode\n\nimport dapla as dp\n\n# Stien til filen\nsource_file = \"gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv\"\n\ndf = dp.read_pandas(source_file, file_type=\"csv\")\n\n# Dataminimerer ved Ã¥ velge kolonner\ndf2 = df[['col1', 'col2', 'col3']]\n\n# Skriver ut fil til produktbÃ¸tta\ndp.write_pandas(df2, \n            \"gs://ssb-dapla-example-data-produkt-prod/test20231010.parquet\")\n\n\n\n\n\n\nKode i Kildomaten\n\nimport dapla as dp\n\ndef main(source_file):\n    df = dp.read_pandas(source_file, file_type=\"csv\")\n\n# Dataminimerer ved Ã¥ velge kolonner\n    df2 = df[['col1', 'col2', 'col3']]\n\n# Bytter ut \"kilde\" med \"produkt\" i filstien\n    new_path = source_file.replace(\"kilde\", \"produkt\")\n\n# Bytter ut \".csv\" med \".parquet\" i filstien\n    new_path2 = new_path.replace(\".csv\", \".parquet\")\n\n# Skriver ut fil til produktbÃ¸tta\n    dp.write_pandas(df2, new_path2)\n\n\n\nDe to kodesnuttene over gir det samme resultatet, bare at koden til venstre kjÃ¸res som vanlig python-kode, mens koden til hÃ¸yre kjÃ¸res i Kildomaten. Som vi ser av koden til hÃ¸yre sÃ¥ trenger vi aldri Ã¥ hardkode inn filstier i Kildomaten. Tjenesten gir oss filstien, og vi kan bruke den til Ã¥ skrive ut filen til produktbÃ¸tta.\nStrukturen pÃ¥ filene som skrives bÃ¸r tenkes nÃ¸ye gjennom nÃ¥r man automatiseres prosessering av data. Hvis vi ikke har unike filnavn eller stier sÃ¥ kan du risikere at filer blir overskrevet. Typisk er dette allerede tenkt pÃ¥ nÃ¥r filer skrives til kildebÃ¸tta, sÃ¥ hvis man kopierer den strukturen, slik vi gjorde i eksempelet over, sÃ¥ vil det ikke vÃ¦re noe problem.\n\n\n\n\n\n\nHvilke Python-biblioteker kan jeg bruke?\n\n\n\nDu kan kun bruke biblioteker som er forhÃ¥ndsinstallert i Kildomaten. Du kan se en liste over disse bibliotekene her. Ã˜nsker du andre biblioteker sÃ¥ mÃ¥ du ta kontakt med Kunderservice.\n\n\n\n\n\nTest koden\n\n\n\n\n\n\nTilgang til kildedata\n\n\n\nTilgang til kildedata i prod-miljÃ¸et er det kun gruppen data-admins som kan aktivere ved Ã¥ bruke tilgangsstyringslÃ¸sningen Just-in-Time Access (JIT). Les mer om hvordan JIT-lÃ¸sningen fungerer her. Ã˜nsker man Ã¥ kunne liste ut innhold fra bÃ¸tta mÃ¥ man aktivere rollen ssb.buckets.list. Ã˜nsker man i tillegg Ã¥ lese/skrive til bÃ¸tta mÃ¥ man ogsÃ¥ aktivere ssb.bucket.write. Tilgang til kildebÃ¸tta i test-miljÃ¸et krever ikke JIT-tilgang.\n\n\nFÃ¸r man ruller ut koden i tjenesten er det greit Ã¥ teste at alt fungerer som det skal i Jupyterlab. Hvis vi tar utgangspunkt i eksempelet over sÃ¥ kan vi teste koden ved Ã¥ kjÃ¸re fÃ¸lgende nederst i skriptet:\n\n\nprocess_source_data.py\n\nmain(\"gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv\")\n\nNÃ¥r tjenesten er rullet ut sÃ¥ vil det vÃ¦re dette som kjÃ¸res nÃ¥r en fil dukker opp i gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/. Ved Ã¥ kjÃ¸re det manuelt pÃ¥ denne mÃ¥ten fÃ¥r vi sett at ting fungerer som det skal.\nHusk Ã¥ fjerne kjÃ¸ringen av koden fÃ¸r du ruller ut tjenesten.\n\n\n\n\n\n\nPseudonymisering kan ikke kjÃ¸res fra en IDE i prod-miljÃ¸et\n\n\n\nDu kan teste kode fra Jupyter og andre IDE-er i prod-miljÃ¸et pÃ¥ Dapla. Men hvis prosesseringen innebÃ¦rer bruk av pseudonymisering, sÃ¥ vil den ikke kunne kalles fra programmeringsmiljÃ¸er som Jupyter. Grunnen til dette er at det ikke er Ã¸nskelig Ã¥ gjÃ¸re det lett Ã¥ se upseudonymisert og pseudonymisert data samtidig. Hvis du Ã¸nsker Ã¥ teste prosesseringen av pseudo-tjenesten, sÃ¥ kan du gjÃ¸re med testdata i test-miljÃ¸et.\n\n\n\n\nRull ut tjenesten\nFor Ã¥ rulle ut tjenesten gjÃ¸r du fÃ¸lgende;\n\nPush branchen til GitHub og opprette en pull request.\nPull request mÃ¥ godkjennes av en data-admins pÃ¥ teamet.\n\n\n\n\nNÃ¥r pull request er godkjent sÃ¥ sjekker du om alle tester, planer og utrullinger var vellykket, slik som vist i FigurÂ 1.\nHvis alt er vellykket sÃ¥ kan du merge branchen inn i main-branchen.\n\n\n\n\n\n\n\n\n\nFigurÂ 1: Suksessfulle tester pÃ¥ GitHub\n\n\n\n\n\nEtter at du har merget inn i main kan du fÃ¸lge med pÃ¥ utrullingen under Actions-fanen i repoet. NÃ¥r den siste jobben lyser grÃ¸nt er Kildomaten rullet ut og klar til bruk.\n\n\n\n\n\n\nVanlige feil ved utrulling\n\n\n\nFor Ã¥ gi raskt tilbakemelding pÃ¥ noen mulige feilsituasjoner, sÃ¥ kjÃ¸res det enkel validering pÃ¥ config.yaml og process_source_data.py nÃ¥r en Pull request er opprettet. FÃ¸lgende validering gjennomfÃ¸res:\n\nEr det et python-script kalt process_source_data.py?\nEr det en config.yaml med en gyldig folder_prefix: definert?\nBruker process_source_data.py biblioteker som ikke er installert i Kildomaten?\nHar koden i process_source_data.py feil iht Pyflakes?\n\nDet kan ogsÃ¥ forekomme at Atlantis, verktÃ¸yet for Ã¥ rulle ut endringer fra IaC-repoet til GCP, feiler. Da kan du prÃ¸ve Ã¥ skrive atlantis plan i kommentarfeltet til pull requestâ€™en, og testene vil kjÃ¸re pÃ¥ nytt. Hvis det fortsatt ikke fungerer sÃ¥ kontakter man Dapla kundeservice.\n\n\n\n\nTest tjenesten\nNÃ¥r du har rullet ut tjenesten kan teste tjenesten ved at data-admins flytter en fil til en den filstien Kildomaten er satt opp for. I eksempelet vi har brukt tidligere, gjÃ¸r du fÃ¸lgende: 1. Aktiverer JIT-tilgangen til kildedata (som beskrevet over). 2. Kopier en fil over til filstien:\ngs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/ 3. Sjekk om filen ble skrevet til Ã¸nsket mappe i produktbÃ¸tta.\nDu kan ogsÃ¥ sjekke logger og monitorere Kildomaten. Les mer i neste avsnitt.\n\n\nMonitorering og logging\nNÃ¥r en kilde i Kildomaten er satt opp og ruller ut, kan du monitere tjenesten i Google Cloud Console (GCC) ved Ã¥ gjÃ¸re fÃ¸lgende:\n\nLogg deg inn med SSB-bruker pÃ¥ GCC.\nVelg standardprosjektet i prosjektvelgeren3.\nSÃ¸k opp Cloud Run i sÃ¸kefeltet pÃ¥ toppen av siden og gÃ¥ inn pÃ¥ siden.\n\nPÃ¥ siden til Cloud Run vil du se en oversikt over alle kilder teamet har kjÃ¸rende i Kildomaten. De har formen source-&lt;kildenavn&gt;-processor. I eksempelet med team dapla-example tidligere, vil man da se en kilde som heter source-altinn-processor, siden mappen de opprettet under\nautomation/source-data-dapla-example-prod/ i IaC-repoet heter altinn.\nTrykker man seg inn pÃ¥ hver enkelt kilde vil man kunne monitorere aktiviteten i kilden, og man kan trykke seg videre for Ã¥ se loggene.\n\n\n\n\n\n\nSjekke logger\n\n\n\nDet er anbefalt Ã¥ se pÃ¥ Kildomaten-loggene i Logs Explorer. Det kan man enkelt gjÃ¸re ved Ã¥ trykke pÃ¥ â€œView in Logs Explorerâ€ som vist pÃ¥ bildet under:\n\n\n\nÃ…pne Kildomaten-loggene i Logs Explorer\n\n\n\n\n\n\nSkalering\nKildomaten er satt opp for Ã¥ kunne prosessere hver kilde i 5 parallelle intanser samtidig. F.eks. hvis det oppstÃ¥r 10 nye filer i en mappe som trigger en Kildomaten-kilde, sÃ¥ kan det prosesseres 5 filer samtidig. Ta kontakt med Kundeservice hvis man har behov for ytterlige skalering.\n\n\nVarsling pÃ¥ e-post\nKildomaten tilbyr e-postvarsling til teamet nÃ¥r tjenesten feiler. Opprett en Kundeservice-sak for Ã¥ fÃ¥ satt opp e-postvarsling for teamet ditt.\n\n\nFlere kilder\nMan kan sette opp sÃ¥ mange kilder man Ã¸nsker. Men nÃ¥r man setter det opp er det viktig Ã¥ huske at alle kildene mÃ¥ spesifiseres rett under automation/source-data/&lt;teamnavn&gt;-&lt;miljÃ¸&gt;/. Her er et eksempel pÃ¥ hvordan team dapla-example har to kilder i Kildomaten:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\nâ”œâ”€â”€ automation/\nâ”‚   â””â”€â”€ source-data/\nâ”‚       â”œâ”€â”€ dapla-example-prod/\nâ”‚           â””â”€â”€ altinn/\nâ”‚               â”œâ”€â”€ config.yaml\nâ”‚               â””â”€â”€ process_source_data.py\nâ”‚           â””â”€â”€ ledstill/\nâ”‚               â”œâ”€â”€ config.yaml\nâ”‚               â””â”€â”€ process_source_data.py\nâ”‚...           \n\nI eksempelet over ser vi det er to kilder: altinn og ledstill. Hver kilde trigges pÃ¥ ulike filstier i kildebÃ¸tta, og python-koden som kjÃ¸res kan vÃ¦re ulik mellom kilder.\n\n\nTest-miljÃ¸\nI eksempelet som er brukt i dette kapitlet er Kildomaten satt opp i prodmiljÃ¸et til teamet. Mens egentlig burde man teste ut nye kilder i teamets test-miljÃ¸. Kildomaten er ikke satt opp i test-miljÃ¸et som standard, og derfor mÃ¥ det skrus pÃ¥ fÃ¸r man kan anvende det. Teamet kan gjÃ¸re det selv ved Ã¥ fÃ¸lge denne beskrivelsen, eller de kan ta kontakt med Kundeservice og fÃ¥ hjelp til dette.\nEn av de store fordelene med Ã¥ sette opp Kildomaten-kilder i test-miljÃ¸et fÃ¸r man gjÃ¸r det i prod-miljÃ¸et, er at tilgangsstyringen til data er mye mindre streng. Det gjÃ¸r det lettere for alle i teamet Ã¥ utvikle koden som skal benyttes.\nNÃ¥r man skal sette opp Kildomaten i test-miljÃ¸et sÃ¥ fÃ¸lger det samme oppskrift som vi har vist for prod-miljÃ¸et over. Den store forskjellen er at test-kilder skal legges under en egen mappe under automation/source-data/ i teamets IaC-repo. Eksempelet under med team dapla-example viser hvordan man kan sette opp kildene altinn og ledstill for bÃ¥de prod- og test-testmiljÃ¸et:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\nâ”œâ”€â”€ automation/\nâ”‚   â””â”€â”€ source-data/\nâ”‚       â”œâ”€â”€ dapla-example-prod/\nâ”‚       â”‚       â”œâ”€â”€ altinn/\nâ”‚       â”‚       â”‚       â”œâ”€â”€ config.yaml\nâ”‚       â”‚       â”‚       â””â”€â”€ process_source_data.py\nâ”‚       â”‚       â””â”€â”€ ledstill/\nâ”‚       â”‚               â”œâ”€â”€ config.yaml\nâ”‚       â”‚               â””â”€â”€ process_source_data.py\nâ”‚       â”œâ”€â”€ dapla-example-test/\nâ”‚               â”œâ”€â”€ altinn/\nâ”‚               â”‚       â”œâ”€â”€ config.yaml\nâ”‚               â”‚       â””â”€â”€ process_source_data.py\nâ”‚               â””â”€â”€ ledstill/\nâ”‚                       â”œâ”€â”€ config.yaml\nâ”‚                       â””â”€â”€ process_source_data.py\nâ”‚...           \n\nSom vi ser av mappestrukturen over sÃ¥ er det to mapper under\nautomation/source-data:\n\ndapla-example-prod\ndapla-example-test\n\nDisse to mappene angir om det er henholdsvis prod- eller test-miljÃ¸et vi setter opp kilder for.\n\n\nTrigge kilde manuelt\nKildomaten er bygget for Ã¥ trigge pÃ¥ nye filer som oppstÃ¥r i en gitt filsti. Men noen ganger er det nÃ¸dvendig Ã¥ trigge kjÃ¸ring av filer for en gitt kilde pÃ¥ nytt. Dette kan gjÃ¸res med en funksjon i Python-pakken dapla-toolbelt.\n\n\n\n\n\n\nLogg inn som developers i Dapla Lab\n\n\n\nTrigging av kilder manuelt kan kun gjÃ¸res av developers pÃ¥ Dapla Lab. Ã…rsaken til dette er at vi Ã¸nsker at alle pÃ¥ teamet skal ha denne muligheten, og siden brukere som er medlem av tilgangsgruppen data-admins ogsÃ¥ kan vÃ¦re medlemmer av gruppen developers, sÃ¥ mÃ¥ man logge seg inn som developers i Dapla Lab. PÃ¥ Dapla Lab mÃ¥ man representere en av gruppene nÃ¥r man logger seg inn.\n\n\nFÃ¸r du kan gjÃ¸re dette trenger du fÃ¸lgende informasjon:\n\nproject-id for standardprosjektet. Slik finner du prosjekt-id. Merk at dette ikke er kilde-prosjektet!\nfolder_prefix er mappen i kilde-bÃ¸tten som du Ã¸nsker at koden skal trigges pÃ¥. Dette fungerer likt som tidligere forklart for config.yaml, men her har du ogsÃ¥ mulighet til Ã¥ kunne trigge prosesseringen pÃ¥ en undermappe av stien som var satt i kildens config.yaml.\nsource_name er navnet pÃ¥ mappen i iac-repoet hvor kilden du skal trigge er definert. Navnet pÃ¥ kilden i eksempelet med team dapla-example var altinn.\n\nHvis man under folder_prefix legger inn en hel filsti med filnavn og filending, sÃ¥ trigges Kildomaten kun pÃ¥ denne ene fila.\nHer er et kodeeksempel som viser hvordan man kan trigge prossesering av alle filer for kilde\n\n\nnotebook\n\nfrom dapla import trigger_source_data_processing\n\nproject_id = \"dapla-example-p\"\nsource_name = \"altinn\"\nfolder_prefix = \"ledstill/altinn/ra0678\"\n\ntrigger_source_data_processing(project_id, source_name, folder_prefix, kuben=True)          \n\nI eksempelet over trigger vi scriptet som er satt opp for kilden altinn til Ã¥ kjÃ¸re pÃ¥ alle undermapper av\nssb-dapla-example-data-kilde-prod/ledstill/altinn/ra0678/.",
    "crumbs": [
      "Manual",
      "Datatjenester"
    ]
  },
  {
    "objectID": "statistikkere/kildomaten.html#vedlikehold",
    "href": "statistikkere/kildomaten.html#vedlikehold",
    "title": "Kildomaten",
    "section": "Vedlikehold",
    "text": "Vedlikehold\nNÃ¥r tjenesten er rullet ut sÃ¥ vil den kjÃ¸re automatisk pÃ¥ alle filer som dukker opp i filsti i kildebÃ¸tta. Etter hvert vil det vÃ¦re behov for Ã¥ endre pÃ¥ skript, endre filstier som skal trigge tjenesten, eller prosessere alle filer pÃ¥ nytt. I denne delen forklarer vi hvordan du gÃ¥r frem for Ã¥ gjÃ¸re dette.\n\nEndre skript\nAlle pÃ¥ team kan endre pÃ¥ skriptet, men det er data-admins som mÃ¥ godkjenne endringene fÃ¸r de blir rullet ut. For Ã¥ endre skriptet gjÃ¸r du fÃ¸lgende:\n\nKlon repoet.\nGjÃ¸r endringene du Ã¸nsker i en branch.\nPush opp endringene til GitHub og opprett en pull request.\nFÃ¥ en data-admins pÃ¥ teamet til Ã¥ godkjenne endringene.\nNÃ¥r endringene er godkjent sÃ¥ kan du merge inn i main-branchen.\n\nEn endring i Python-skriptet krever ikke at tjenesten rulles ut pÃ¥ nytt. Derfor er det ikke like mange tester og kjÃ¸ringer som gjÃ¸res som nÃ¥r man oppretter en helt ny kilde.\n\n\nEndre config.yaml\nAlle pÃ¥ teamet kan gjÃ¸re endringer i config.yaml, men det er data-admins som mÃ¥ godkjenne endringene fÃ¸r de blir rullet ut. For Ã¥ endre config.yaml gjÃ¸r du fÃ¸lgende:\n\nKlon repoet.\nGjÃ¸r endringene du Ã¸nsker i en branch.\nPush opp endringene til GitHub og opprett en pull request.\nFÃ¥ en data-admins pÃ¥ teamet til Ã¥ godkjenne endringene.\nNÃ¥r endringene er godkjent sÃ¥ kan du merge inn i main-branchen.\n\nEn endring i config.yaml krever at tjenesten rulles ut pÃ¥ nytt og tar derfor litt mer tid enn en endring i Python-skriptet.",
    "crumbs": [
      "Manual",
      "Datatjenester"
    ]
  },
  {
    "objectID": "statistikkere/kildomaten.html#footnotes",
    "href": "statistikkere/kildomaten.html#footnotes",
    "title": "Kildomaten",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI tillegg er det data-admins som mÃ¥ teste tjenesten manuelt hvis det gjÃ¸res pÃ¥ skarpe data, siden det kun er data-admins som kan fÃ¥ tilgang til de dataene.â†©ï¸\nInfrastructure-as-Code (IaC) er repo som definerer alle ressursene til teamet pÃ¥ Dapla. Alle Dapla-team har et eget IaC-repo pÃ¥ GiHub og du finner det ved Ã¥ sÃ¸ke etter repoet -iac under statisticsnorway.â†©ï¸\nStandardprosjektet har navnestrukturen &lt;teamnavn&gt;-pâ†©ï¸",
    "crumbs": [
      "Manual",
      "Datatjenester"
    ]
  },
  {
    "objectID": "statistikkere/git-og-github.html",
    "href": "statistikkere/git-og-github.html",
    "title": "Git og GitHub",
    "section": "",
    "text": "I SSB bruker vi Git til versjonskontroll av koden vÃ¥r og deler den med andre via GitHub. For Ã¥ mestre disse verktÃ¸yene er det viktig Ã¥ forstÃ¥ forskjellen mellom Git og GitHub.\n\n\n\n\n\n\nGit vs.Â GitHub: Kort fortalt\n\n\n\nGit er et verktÃ¸y installert pÃ¥ din lokale maskin som sporer endringer i koden din.\nGitHub er en skybasert plattform som fungerer som et felles lagringssystem, der du kan dele og samarbeide med andre om kode.\n\n\nGit og GitHub er viktige verktÃ¸y for Ã¥ sikre at produksjonssystemene vÃ¥re er trygge og reproduserbare. De gjÃ¸r det enkelt Ã¥ spore endringer og gjennomgÃ¥ eller godkjenne hverandres bidrag.\nI dette kapittelet ser vi nÃ¦rmere pÃ¥ Git og GitHub og hvordan de er implementert i SSB. Selv om ssb-project gjÃ¸r det lettere Ã¥ forholde seg til Git og GitHub vil vi dette kapittelet forklare nÃ¦rmere hvordan det funker uten dette hjelpemiddelet.\n\n\n\n\n\n\nLes videre med interne ressurser\n\n\n\nDet finnes mange gode ressurser pÃ¥ huset om versjonshÃ¥ndtering i tillegg til dette kapittelet. Gruppen Kvalitet i Kode og Koding (KVAKK) har skrevet flere veiledninger pÃ¥ confluence, blant annet om Git anbefalt arbeidsflyt eller hvordan man lÃ¸ser en merge conflict. Se hele katalogen til KVAKK om Git og GitHub ved Ã¥ gÃ¥ inn pÃ¥ deres confluence-omrÃ¥de: Versjonskontroll med Git.\nI tillegg har A200 sitt stÃ¸tteteam skrevet om hva man bÃ¸r lÃ¦re seg og hvordan man gÃ¥r frem for Ã¥ lÃ¦re i confluence-dokumentet Kom i gang med Dapla (statistics-norway.atlassian.net).\nNettsiden learngitbranching.js.org er ogsÃ¥ en veldig god ressurs for Ã¥ forstÃ¥ konseptene.\n\n\n\n\n\n\nGit er en programvare for distribuert versjonshÃ¥ndtering av filer:\n\nGit tar vare pÃ¥ historien til koden din\nAlle som jobber med koden har en kopi av koden hos seg selv (distribuert)\n\nNÃ¥r man Ã¸nsker Ã¥ dele koden med andre laster man opp koden til et felles kodelager pÃ¥ GitHub kalt repository (repo). Git versjonshÃ¥ndterer filene i repoet. Vanligvis versjonshÃ¥ndteres rene tekstfiler, men git kan ogsÃ¥ versjonshÃ¥ndtere bilder og PDFer.\nGit er installert pÃ¥ maskinen du jobber pÃ¥ og brukes fra terminalen. Det finnes pek-og-klikk versjoner av Git, blant annet i Jupyterlab og RStudio, men noen situasjoner vil bare kunne lÃ¸ses i terminalen.\nI SSB anbefaler vi at du starter et nytt Git-prosjekt ved Ã¥ benytte ssb-project. Da vil du ikke bare aktivere Git, men du kan ogsÃ¥ fÃ¥ implementert en del andre gode praksiser for Ã¥ holde koden din ryddig, oversiktlig og sikker.\nMen fÃ¸r vi kan begynne Ã¥ bruke Git mÃ¥ vi konfigurere vÃ¥r egen bruker for Git, som er forklart i neste seksjon.\nLes mer om Git pÃ¥ https://git-scm.com/.\n\n\n\n\n\n\nGit i praksis: Kort fortalt\n\n\n\nMan aktiverer Git pÃ¥ en mappe i filsystemet sitt med kommandoen git init nÃ¥r man stÃ¥r i mappen som skal versjonshÃ¥nderes. Da vil Git versjonshÃ¥ndtere alle filer som er i den mappen og i eventuelle undermapper. NÃ¥r du sÃ¥ gjÃ¸r endringer pÃ¥ en fil i mappen, vil Git registrere endringer. Ã˜nsker du at endringen skal bli et punkt i historikken til prosjektet, sÃ¥ mÃ¥ du fÃ¸rst legge til filen i Git med kommandoen git add filnavn. NÃ¥r du har gjort dette, sÃ¥ kan du lagre endringen med kommandoen git commit -m \"Din melding her\". NÃ¥r du har gjort dette, sÃ¥ vil endringen vÃ¦re lagret i Git. NÃ¥r du har gjort mange endringer, sÃ¥ kan du sende endringene til GitHub med kommandoen git push. NÃ¥r du har gjort dette, sÃ¥ vil endringene vÃ¦re synlige for alle som har tilgang til GitHub-prosjektet.\n\n\n\n\n\nDenne delen er kun gjeldende for gamle jupyter, altsÃ¥ jupyter.dapla.ssb.no, og ikke DaplaLab.\n\n\n\n\n\n\nBruk SSB sin konfigurasjon!\n\n\n\nAlle som bruker GitHub i SSB bÃ¸r bruke SSB sin Git-konfigurasjon. Hvis ikke kan man bl.a. risikere at output fra Notebooks blir pushet til GitHub. Du kan bruke SSBs konfigurasjon ved Ã¥ kjÃ¸re ssb-gitconfig.py i terminalen og svare pÃ¥ spÃ¸rsmÃ¥lene som dukker opp.\n\n\nFor Ã¥ jobbe med Git sÃ¥ mÃ¥ man konfigurere brukeren sin slik at Git vet hvem som gjÃ¸r endringer i koden. I praksis betyr det at du mÃ¥ ha filen .gitconfig pÃ¥ hjemmeomrÃ¥det ditt (f.eks. /home/jovyan/.gitconfig pÃ¥ Dapla) med noe grunnleggende informasjon:\n\n\n.gitconfig\n\n# /home/jovyan/.gitconfig pÃ¥ Dapla\n[user]\n        name = Ola Nordmann\n        email = &lt;ini&gt;@ssb.no\n\nMed denne konfigurasjonen sÃ¥ kan man bruke Git lokalt. Men skal man ogsÃ¥ bruke GitHub i SSB, dvs. dele kode med andre, sÃ¥ mÃ¥ man ogsÃ¥ legge til mer informasjon i .gitconfig. Heldigvis er det skrevet et script som gjÃ¸r dette for deg. For Ã¥ fÃ¥ anbefalt konfigurasjon for Git sÃ¥ kan du kjÃ¸re fÃ¸lgende kommando i terminalen:\n\n\nterminal\n\nssb_gitconfig.py\n\nDette scriptet vil spÃ¸rre deg om ditt brukernavn i SSB, og sÃ¥ vil det opprette en fil som heter .gitconfig i hjemmeomrÃ¥det ditt. Denne filen vil inneholde informasjon om brukernavn, e-post og tilgang til GitHub. I tillegg vil den sÃ¸rge for at ditt ssb-project ikke sender output fra notebooks til GitHub.\n\n\n\nSelv om det er vanlig Ã¥ jobbe i notebooks pÃ¥ Dapla, sÃ¥ skal all kode i SSB lagres og versjonhÃ¥ndteres som rene tekstfiler i .R- eller .py-filer i prosent-formatet. I praksis har ikke dette sÃ¥ stor betydning, siden disse filtypene kan Ã¥pnes som notebooks i verktÃ¸y som Jupyterlab og VS Code, og pÃ¥ den mÃ¥ten handler det bare om hvilket filformat koden lagres til.\nLes mer om hvordan man lagrer notebooks til rent tekstformat.\n\n\n\nGit er veldig sterkt verktÃ¸y med mange muligheter. Det er ikke tenkt at alt skal adresseres i dette kapitlet. Men det er noen vanlig operasjoner som er sÃ¥ vanlige at alle som jobber med kode i SSB bÃ¸r kjenne dem.\nVi har tidligere nevnt at kommandoen for Ã¥ aktivere versjonshÃ¥ndtering med Git pÃ¥ en mappe, er git init. Dette gjÃ¸res ogsÃ¥ automatisk nÃ¥r man oppretter et nytt ssb-project.\nVanlige git-kommandoer:\n\ngit status for Ã¥ se hvilke endringer Git har oppdaget\ngit add &lt;filnavn&gt; for Ã¥ fortelle Git at endringene skal lagres\ngit commit -m \"Din melding her\" for Ã¥ gjÃ¸re endringene om til et punkt i historien til koden din * Hver commit har sin egen unike ID * Flere filer kan samles i en commit\n\nNÃ¥r man utvikler kode sÃ¥ gjÃ¸r man det fra sÃ¥kalte branches1. Hvis vi tenker oss at din eksisterende kodebase er stammen pÃ¥ et tre (ofte kalt master eller main), sÃ¥ legger Git opp til at man gjÃ¸r endringer pÃ¥ denne koden via branches eller grener av treet. Med andre ord holder vi stammen/master urÃ¸rt helt til vi vet at endringen fungerer som den skal. Til slutt merger vi den inn i master. Vi kan opprette en ny branch og gÃ¥ inn i den ved Ã¥ skrive git checkout -b &lt;branch navn&gt;. Da stÃ¥r du i en branch og kan bruke kommandoer som git add og git commit som vist tidligere.\nVi kan merge endringene fra vÃ¥r branch inn i main ved Ã¥ fÃ¸rst gÃ¥ inn i main, git switch main, og deretter skrive git merge &lt;branch navn&gt;. Men typisk vil ikke dette vÃ¦re fremgangsmÃ¥ten i SSB. NÃ¥r man er fornÃ¸yd med endringene i en branch, sÃ¥ vil man pushe den opp til GitHub, slik at en kollega kan vurdere om den skal merges inn i main. Dermed gjÃ¸res selve mergen i GitHub-grensenittet. Vi skal se nÃ¦rmere pÃ¥ GitHub i neste kapittel.\n\n\n\n\nGitHub er et nettsted som bl.a. fungerer som vÃ¥rt felles mappesystem for deling av kode. SSB har sin egen organisasjonskonto med navn statisticsnorway. Men selv om SSB har en organisasjonskonto pÃ¥ GitHub mÃ¥ alle ansatte opprette sin egen brukerprofil, og knytte den mot SSB sin organisasjonskonto. Under forklarer vi hvordan du gjÃ¸r dette.\n\n\nDette kapittelet er bare relevant hvis man ikke har en GitHub-brukerkonto fra fÃ¸r. For Ã¥ bruke ssb-project-programmet til Ã¥ generere et remote repo pÃ¥ GitHub mÃ¥ du ha en konto. Derfor starter vi med Ã¥ gjÃ¸re dette. Det er en engangsjobb og du trenger aldri gjÃ¸re det igjen.\n\n\n\n\n\n\nSSB har valgt Ã¥ ikke sette opp SSB-brukerne til de ansatte som GitHub-brukere. En viktig Ã¥rsak er at en GitHub-konto ofte regnes som en del av den ansattes CV. For de som aldri har brukt GitHub fÃ¸r kan det virke fremmed, men det er nok en fordel pÃ¥ sikt nÃ¥r alle blir godt kjent med denne arbeidsformen.\n\n\n\nSlik gjÃ¸r du det:\n\nGÃ¥ til https://GitHub.com/\nTrykk Sign up Ã¸verst i hÃ¸yre hjÃ¸rne\nI dialogboksen som Ã¥pnes, se FigurÂ 1, skriver du inn din e-postkonto og lager et passord. Dette trenger ikke vÃ¦re din SSB-bruker og e-post. Hvis du bruker en en personlig e-postkonto er det viktig at du tydeliggjÃ¸r hvem du er sÃ¥ kollegaer vet at du jobber i SSB nÃ¥r de ser aktivitet fra deg.\n\n\n\n\n\n\n\nFigurÂ 1: Dialogboks for opprettelse av GitHub-bruker.\n\n\n\nDu har nÃ¥ laget en egen GitHub-bruker. I neste steg skal vi knytte denne kontoen til din SSB-bruker.\n\n\n\nHvis du har fullfÃ¸rt forrige steg sÃ¥ har du nÃ¥ en GitHub-konto. Hvis du stÃ¥r pÃ¥ din profil-side sÃ¥ ser den ut som i FigurÂ 2.\n\n\n\n\n\n\nFigurÂ 2: Et eksempel pÃ¥ hjemmeomrÃ¥det til en GitHub-bruker\n\n\n\nDet neste vi mÃ¥ gjÃ¸re er Ã¥ aktivere 2-faktor autentisering, siden det er dette som benyttes i SSB. Hvis du stÃ¥r pÃ¥ siden i bildet over, sÃ¥ gjÃ¸r du fÃ¸lgende for Ã¥ aktivere 2-faktor autentisering mot GitHub:\n\n\n\nTrykk pÃ¥ den lille pilen Ã¸verst til hÃ¸yre og velg Settings(se FigurÂ 3).\nDeretter velger du Password and authentification i menyen til venstre.\nUnder Two-factor authentication trykker du pÃ¥ Enable.\n\n\n\n\n\n\n\n\n\nFigurÂ 3: Ã…pne settings for din GitHub-bruker.\n\n\n\n\n\n\nFigurÂ 4 viser dialogboksen som vises. Velg Enable two-factor authentification.\n\n\n\n\n\n\n\nFigurÂ 4: Dialogboks som Ã¥pnes nÃ¥r 2FA skrus pÃ¥ fÃ¸rste gang.\n\n\n\n\nFigurÂ 5 viser dialogboksen som vises for Ã¥ velge hvordan man skal autentisere seg. Her anbefales det Ã¥ velge Set up using an app, slik at du kan bruke Microsoft Authenticator-appen pÃ¥ din mobil.\n\n\n\n\n\n\n\nFigurÂ 5: Dialogboks for Ã¥ velge hvordan man skal autentisere seg med 2FA.\n\n\n\nFigurÂ 6 viser QR-koden som vises. Denne skal vi bruke i neste steg.\n\n\n\n\n\n\nFigurÂ 6: QR-kode som skannes av Microsoft Authenticator.\n\n\n\n\n\n\nStrekkoden over skal skannes i din Microsoft Authenticator-app pÃ¥ mobilen, som vist i FigurÂ 7. Ã…pne appen, trykk pÃ¥ Bekreftede ID-er, og til slutt trykk pÃ¥ Skann QR-kode. Deretter skanner du QR-koden fra punkt 5.\nNÃ¥r koden er skannet har du fÃ¥tt opp fÃ¸lgende bilde pÃ¥ appens hovedside (se bilde til hÃ¸yre). Skriv inn den 6-siffer koden pÃ¥ GitHub-siden med QR-koden.\nTil slutt lagrer du Recovery-codes et trygt sted som bare du har tilgang til.\n\n\n\n\n\n\n\n\n\nFigurÂ 7: Mobilappen Microsoft authenticator\n\n\n\n\n\nNÃ¥ har vi aktivert 2-faktor autentisering for GitHub og er klare til Ã¥ knytte vÃ¥r personlige konto til vÃ¥r SSB-bruker pÃ¥ SSBs â€œGitHub organisationâ€ statisticsnorway.\n\n\n\nI forrige steg aktiverte vi 2-faktor autentisering for GitHub. Det neste vi mÃ¥ gjÃ¸re er Ã¥ koble oss til Single Sign On (SSO) for SSB sin organisasjon pÃ¥ GitHub:\n\nTrykk pÃ¥ lenken https://GitHub.com/orgs/statisticsnorway/sso\nI dialogboksen som dukker opp trykker du pÃ¥ Continue, slik som vist i FigurÂ 8.\n\n\n\n\n\n\n\nFigurÂ 8: Single Sign on (SSO) for SSB sin organisasjon pÃ¥ GitHub\n\n\n\nNÃ¥r du har gjennomfÃ¸rt dette sÃ¥ har du tilgang til statisticsnorway pÃ¥ GitHub. GÃ¥r du inn pÃ¥ denne lenken sÃ¥ skal du nÃ¥ kunne lese bÃ¥de Public, Private og Internal repoer, slik som vist i FigurÂ 9.\n\n\n\n\n\n\nFigurÂ 9: Medlemsvisning for SSB sin GitHub-organisasjon.\n\n\n\n\n\n\nNÃ¥r vi skal jobbe med SSB-kode som ligger lagret hos statistcsnorway pÃ¥ GitHub, sÃ¥ mÃ¥ vi autentisere oss. MÃ¥ten vi gjÃ¸re det pÃ¥ er ved Ã¥ generere et Personal Access Token (ofte forkortet PAT) som vi oppgir nÃ¥r vi vil hente eller oppdatere kode pÃ¥ GitHub. Da sender vi med PAT for Ã¥ autentisere oss for GitHub.\n\n\nFor Ã¥ lage en PAT som er godkjent mot statisticsnorway sÃ¥ gjÃ¸r man fÃ¸lgende:\n\nGÃ¥ til din profilside pÃ¥ GitHub og Ã¥pne Settings slik som ble vist SeksjonÂ 1.2.2.\nVelg Developer Settings i menyen til venstre.\nI menyen til venstre velger du Personal Access Token, og deretter Tokens (classic).\nVelg Generate new token og deretter Generate new token (classic).\nUnder Note kan du gi PATâ€™en et navn. Velg et navn som er intuitivt for deg. Hvis du skal bruke PAT til Ã¥ jobbe mot Dapla, sÃ¥ ville jeg ganske enkelt kalt den dapla. Hvis du skal bruke den mot bakkemiljÃ¸et ville jeg kalt den prodsone eller noe annet som gjÃ¸r det lett for det skjÃ¸nne innholdet i ettertid.\nUnder Expiration velger du hvor lang tid som skal gÃ¥ fÃ¸r PAT blir ugyldig. Dette er en avvening mellom sikkerhet og hva som er praktisk. Det anbefales at du velger 365 dager. NÃ¥r PAT utlÃ¸per mÃ¥ du gjenta stegene i dette kapittelet.\nUnder Select scopes velger du repo og workflow slik som vist i FigurÂ 10.\n\n\n\n\n\n\n\nFigurÂ 10: Gi token et kort og beskrivende navn\n\n\n\n\nTrykk pÃ¥ Generate token nederst pÃ¥ siden og du fÃ¥r noe lignende det du ser i FigurÂ 11.\n\n\n\n\n\n\n\nFigurÂ 11: Token som ble generert.\n\n\n\n\nKopier deretter PAT til en midlertidig fil. Grunnen er at du aldri vil se det igjen her etter at vi har gjennomfÃ¸rt neste steg.\nDeretter trykker du pÃ¥ Configure SSO og velger Authorize ved siden statisticsnorway, slik som vist i FigurÂ 12. Svar deretter pÃ¥ spÃ¸rsmÃ¥lene som dukker opp.\n\n\n\n\n\n\n\nFigurÂ 12: Autorisering av Token mot SSBs GiHub-organisasjon.\n\n\n\nVi har nÃ¥ opprettet en PAT som er godkjent for bruk mot SSB sin kode pÃ¥ GitHub. Det betyr at hvis vi vil jobbe med Git pÃ¥ SSB sine maskiner i sky eller pÃ¥ bakken, sÃ¥ mÃ¥ vi sendte med dette tokenet for Ã¥ fÃ¥ lov til Ã¥ jobbe med koden som ligger pÃ¥ statisticsnorway pÃ¥ GitHub.\n\n\n\nDette gjelder gamle Jupyter. Bla lenger ned for Ã¥ lese om hvordan man lagrer PAT pÃ¥ DaplaLab\nDet er ganske upraktisk Ã¥ mÃ¥tte sende med tokenet hver gang vi skal jobbe med GitHub. Vi bÃ¸r derfor lagre det lokalt der vi jobber, slik at Git automatisk finner det. Det finnes mange mÃ¥ter Ã¥ gjÃ¸re dette pÃ¥ og det er ikke bestemt hva som skal vÃ¦re beste-praksis i SSB. Men en mÃ¥te Ã¥ gjÃ¸re det er via en .netrc-fil. Vi oppretter da en fil som heter .netrc pÃ¥ vÃ¥rt hjemmeomrÃ¥de, og legger fÃ¸lgende informasjon pÃ¥ en (hvilken som helst) linje i filen:\n\n\n~/.netrc\n\nmachine GitHub.com login &lt;GitHub-bruker&gt; password &lt;Personal Access Token&gt;\n\nGitHub-bruker er da din personlige bruker og IKKE brukernavnet ditt i SSB. Personal Access Token er det vi lagde i forrige kapittelet.\nEn veldig enkel mÃ¥te Ã¥ lagre dette er som fÃ¸lger. Anta at min personlige GitHub-bruker er SSB-Chad og at min Personal Access Token er blablabla. Da kan jeg gjÃ¸re fÃ¸lgende for Ã¥ lagre det i .netrc:\n\nGÃ¥ inn i Jupyterlab og Ã¥pne en Python-notebook.\nI den fÃ¸rste kodecellen skriver du:\n\n\n\nterminal\n\n!echo \"machine GitHub.com login SSB-Chad password blablabla\" &gt;&gt; ~/.netrc\n\nAlternativt kan du fjerne utropstegnet og kjÃ¸re kommandoen direkte i terminalen. Det vil gi samme resultat. Koden over legger til en linje med teksten\nmachine GitHub.com login SSB-Chad password blablabla\ni en .netrc-fil pÃ¥ din hjemmeomrÃ¥det, uavhengig av om du har en fra fÃ¸r eller ikke. Hvis du har en fil fra fÃ¸r som allerede har et token fra GitHub, ville jeg nok slettet det fÃ¸r jeg legger en et nytt token.\nHver gang du jobber mot GitHub vil Git sjekke om informasjon om autentisering ligger i denne filen, og bruke den hvis den ligger der.\n\n\nÃ… lagre PAT pÃ¥ DaplaLab er enkelt og gjÃ¸res kun en gang uansett hvor mange tjenester man bruker.\nHer er stegene:\n\nLogg inn pÃ¥ https://lab.dapla.ssb.no\nTrykk pÃ¥ â€˜My accountâ€™\nNaviger til Git-fanen\nLim inn token der det stÃ¥r â€˜Git Forge Personal Access Tokenâ€™ vist i FigurÂ 13\n\n\n\n\n\n\n\nFigurÂ 13: DaplaLab My account: lagre PAT\n\n\n\n\n\n\n\nI eksempelet over lagde vi en PAT som var gyldig i 90 dager. Dermed vil du ikke kunne jobbe mot GitHub med dette tokenet etter 90 dager. For Ã¥ oppdatere tokenet gjÃ¸r du fÃ¸lgende:\n\nLag et nytt PAT ved Ã¥ repetere SeksjonÂ 1.2.4.1.\nI miljÃ¸et der du skal jobbe med Git og GitHub gÃ¥r du inn i din .netrc og bytter ut token med det nye.\n\nOg med det er du klar til Ã¥ jobbe mot statisticsnorway pÃ¥ GitHub.",
    "crumbs": [
      "Manual",
      "Kode"
    ]
  },
  {
    "objectID": "statistikkere/git-og-github.html#git",
    "href": "statistikkere/git-og-github.html#git",
    "title": "Git og GitHub",
    "section": "",
    "text": "Git er en programvare for distribuert versjonshÃ¥ndtering av filer:\n\nGit tar vare pÃ¥ historien til koden din\nAlle som jobber med koden har en kopi av koden hos seg selv (distribuert)\n\nNÃ¥r man Ã¸nsker Ã¥ dele koden med andre laster man opp koden til et felles kodelager pÃ¥ GitHub kalt repository (repo). Git versjonshÃ¥ndterer filene i repoet. Vanligvis versjonshÃ¥ndteres rene tekstfiler, men git kan ogsÃ¥ versjonshÃ¥ndtere bilder og PDFer.\nGit er installert pÃ¥ maskinen du jobber pÃ¥ og brukes fra terminalen. Det finnes pek-og-klikk versjoner av Git, blant annet i Jupyterlab og RStudio, men noen situasjoner vil bare kunne lÃ¸ses i terminalen.\nI SSB anbefaler vi at du starter et nytt Git-prosjekt ved Ã¥ benytte ssb-project. Da vil du ikke bare aktivere Git, men du kan ogsÃ¥ fÃ¥ implementert en del andre gode praksiser for Ã¥ holde koden din ryddig, oversiktlig og sikker.\nMen fÃ¸r vi kan begynne Ã¥ bruke Git mÃ¥ vi konfigurere vÃ¥r egen bruker for Git, som er forklart i neste seksjon.\nLes mer om Git pÃ¥ https://git-scm.com/.\n\n\n\n\n\n\nGit i praksis: Kort fortalt\n\n\n\nMan aktiverer Git pÃ¥ en mappe i filsystemet sitt med kommandoen git init nÃ¥r man stÃ¥r i mappen som skal versjonshÃ¥nderes. Da vil Git versjonshÃ¥ndtere alle filer som er i den mappen og i eventuelle undermapper. NÃ¥r du sÃ¥ gjÃ¸r endringer pÃ¥ en fil i mappen, vil Git registrere endringer. Ã˜nsker du at endringen skal bli et punkt i historikken til prosjektet, sÃ¥ mÃ¥ du fÃ¸rst legge til filen i Git med kommandoen git add filnavn. NÃ¥r du har gjort dette, sÃ¥ kan du lagre endringen med kommandoen git commit -m \"Din melding her\". NÃ¥r du har gjort dette, sÃ¥ vil endringen vÃ¦re lagret i Git. NÃ¥r du har gjort mange endringer, sÃ¥ kan du sende endringene til GitHub med kommandoen git push. NÃ¥r du har gjort dette, sÃ¥ vil endringene vÃ¦re synlige for alle som har tilgang til GitHub-prosjektet.\n\n\n\n\n\nDenne delen er kun gjeldende for gamle jupyter, altsÃ¥ jupyter.dapla.ssb.no, og ikke DaplaLab.\n\n\n\n\n\n\nBruk SSB sin konfigurasjon!\n\n\n\nAlle som bruker GitHub i SSB bÃ¸r bruke SSB sin Git-konfigurasjon. Hvis ikke kan man bl.a. risikere at output fra Notebooks blir pushet til GitHub. Du kan bruke SSBs konfigurasjon ved Ã¥ kjÃ¸re ssb-gitconfig.py i terminalen og svare pÃ¥ spÃ¸rsmÃ¥lene som dukker opp.\n\n\nFor Ã¥ jobbe med Git sÃ¥ mÃ¥ man konfigurere brukeren sin slik at Git vet hvem som gjÃ¸r endringer i koden. I praksis betyr det at du mÃ¥ ha filen .gitconfig pÃ¥ hjemmeomrÃ¥det ditt (f.eks. /home/jovyan/.gitconfig pÃ¥ Dapla) med noe grunnleggende informasjon:\n\n\n.gitconfig\n\n# /home/jovyan/.gitconfig pÃ¥ Dapla\n[user]\n        name = Ola Nordmann\n        email = &lt;ini&gt;@ssb.no\n\nMed denne konfigurasjonen sÃ¥ kan man bruke Git lokalt. Men skal man ogsÃ¥ bruke GitHub i SSB, dvs. dele kode med andre, sÃ¥ mÃ¥ man ogsÃ¥ legge til mer informasjon i .gitconfig. Heldigvis er det skrevet et script som gjÃ¸r dette for deg. For Ã¥ fÃ¥ anbefalt konfigurasjon for Git sÃ¥ kan du kjÃ¸re fÃ¸lgende kommando i terminalen:\n\n\nterminal\n\nssb_gitconfig.py\n\nDette scriptet vil spÃ¸rre deg om ditt brukernavn i SSB, og sÃ¥ vil det opprette en fil som heter .gitconfig i hjemmeomrÃ¥det ditt. Denne filen vil inneholde informasjon om brukernavn, e-post og tilgang til GitHub. I tillegg vil den sÃ¸rge for at ditt ssb-project ikke sender output fra notebooks til GitHub.\n\n\n\nSelv om det er vanlig Ã¥ jobbe i notebooks pÃ¥ Dapla, sÃ¥ skal all kode i SSB lagres og versjonhÃ¥ndteres som rene tekstfiler i .R- eller .py-filer i prosent-formatet. I praksis har ikke dette sÃ¥ stor betydning, siden disse filtypene kan Ã¥pnes som notebooks i verktÃ¸y som Jupyterlab og VS Code, og pÃ¥ den mÃ¥ten handler det bare om hvilket filformat koden lagres til.\nLes mer om hvordan man lagrer notebooks til rent tekstformat.\n\n\n\nGit er veldig sterkt verktÃ¸y med mange muligheter. Det er ikke tenkt at alt skal adresseres i dette kapitlet. Men det er noen vanlig operasjoner som er sÃ¥ vanlige at alle som jobber med kode i SSB bÃ¸r kjenne dem.\nVi har tidligere nevnt at kommandoen for Ã¥ aktivere versjonshÃ¥ndtering med Git pÃ¥ en mappe, er git init. Dette gjÃ¸res ogsÃ¥ automatisk nÃ¥r man oppretter et nytt ssb-project.\nVanlige git-kommandoer:\n\ngit status for Ã¥ se hvilke endringer Git har oppdaget\ngit add &lt;filnavn&gt; for Ã¥ fortelle Git at endringene skal lagres\ngit commit -m \"Din melding her\" for Ã¥ gjÃ¸re endringene om til et punkt i historien til koden din * Hver commit har sin egen unike ID * Flere filer kan samles i en commit\n\nNÃ¥r man utvikler kode sÃ¥ gjÃ¸r man det fra sÃ¥kalte branches1. Hvis vi tenker oss at din eksisterende kodebase er stammen pÃ¥ et tre (ofte kalt master eller main), sÃ¥ legger Git opp til at man gjÃ¸r endringer pÃ¥ denne koden via branches eller grener av treet. Med andre ord holder vi stammen/master urÃ¸rt helt til vi vet at endringen fungerer som den skal. Til slutt merger vi den inn i master. Vi kan opprette en ny branch og gÃ¥ inn i den ved Ã¥ skrive git checkout -b &lt;branch navn&gt;. Da stÃ¥r du i en branch og kan bruke kommandoer som git add og git commit som vist tidligere.\nVi kan merge endringene fra vÃ¥r branch inn i main ved Ã¥ fÃ¸rst gÃ¥ inn i main, git switch main, og deretter skrive git merge &lt;branch navn&gt;. Men typisk vil ikke dette vÃ¦re fremgangsmÃ¥ten i SSB. NÃ¥r man er fornÃ¸yd med endringene i en branch, sÃ¥ vil man pushe den opp til GitHub, slik at en kollega kan vurdere om den skal merges inn i main. Dermed gjÃ¸res selve mergen i GitHub-grensenittet. Vi skal se nÃ¦rmere pÃ¥ GitHub i neste kapittel.",
    "crumbs": [
      "Manual",
      "Kode"
    ]
  },
  {
    "objectID": "statistikkere/git-og-github.html#github",
    "href": "statistikkere/git-og-github.html#github",
    "title": "Git og GitHub",
    "section": "",
    "text": "GitHub er et nettsted som bl.a. fungerer som vÃ¥rt felles mappesystem for deling av kode. SSB har sin egen organisasjonskonto med navn statisticsnorway. Men selv om SSB har en organisasjonskonto pÃ¥ GitHub mÃ¥ alle ansatte opprette sin egen brukerprofil, og knytte den mot SSB sin organisasjonskonto. Under forklarer vi hvordan du gjÃ¸r dette.\n\n\nDette kapittelet er bare relevant hvis man ikke har en GitHub-brukerkonto fra fÃ¸r. For Ã¥ bruke ssb-project-programmet til Ã¥ generere et remote repo pÃ¥ GitHub mÃ¥ du ha en konto. Derfor starter vi med Ã¥ gjÃ¸re dette. Det er en engangsjobb og du trenger aldri gjÃ¸re det igjen.\n\n\n\n\n\n\nSSB har valgt Ã¥ ikke sette opp SSB-brukerne til de ansatte som GitHub-brukere. En viktig Ã¥rsak er at en GitHub-konto ofte regnes som en del av den ansattes CV. For de som aldri har brukt GitHub fÃ¸r kan det virke fremmed, men det er nok en fordel pÃ¥ sikt nÃ¥r alle blir godt kjent med denne arbeidsformen.\n\n\n\nSlik gjÃ¸r du det:\n\nGÃ¥ til https://GitHub.com/\nTrykk Sign up Ã¸verst i hÃ¸yre hjÃ¸rne\nI dialogboksen som Ã¥pnes, se FigurÂ 1, skriver du inn din e-postkonto og lager et passord. Dette trenger ikke vÃ¦re din SSB-bruker og e-post. Hvis du bruker en en personlig e-postkonto er det viktig at du tydeliggjÃ¸r hvem du er sÃ¥ kollegaer vet at du jobber i SSB nÃ¥r de ser aktivitet fra deg.\n\n\n\n\n\n\n\nFigurÂ 1: Dialogboks for opprettelse av GitHub-bruker.\n\n\n\nDu har nÃ¥ laget en egen GitHub-bruker. I neste steg skal vi knytte denne kontoen til din SSB-bruker.\n\n\n\nHvis du har fullfÃ¸rt forrige steg sÃ¥ har du nÃ¥ en GitHub-konto. Hvis du stÃ¥r pÃ¥ din profil-side sÃ¥ ser den ut som i FigurÂ 2.\n\n\n\n\n\n\nFigurÂ 2: Et eksempel pÃ¥ hjemmeomrÃ¥det til en GitHub-bruker\n\n\n\nDet neste vi mÃ¥ gjÃ¸re er Ã¥ aktivere 2-faktor autentisering, siden det er dette som benyttes i SSB. Hvis du stÃ¥r pÃ¥ siden i bildet over, sÃ¥ gjÃ¸r du fÃ¸lgende for Ã¥ aktivere 2-faktor autentisering mot GitHub:\n\n\n\nTrykk pÃ¥ den lille pilen Ã¸verst til hÃ¸yre og velg Settings(se FigurÂ 3).\nDeretter velger du Password and authentification i menyen til venstre.\nUnder Two-factor authentication trykker du pÃ¥ Enable.\n\n\n\n\n\n\n\n\n\nFigurÂ 3: Ã…pne settings for din GitHub-bruker.\n\n\n\n\n\n\nFigurÂ 4 viser dialogboksen som vises. Velg Enable two-factor authentification.\n\n\n\n\n\n\n\nFigurÂ 4: Dialogboks som Ã¥pnes nÃ¥r 2FA skrus pÃ¥ fÃ¸rste gang.\n\n\n\n\nFigurÂ 5 viser dialogboksen som vises for Ã¥ velge hvordan man skal autentisere seg. Her anbefales det Ã¥ velge Set up using an app, slik at du kan bruke Microsoft Authenticator-appen pÃ¥ din mobil.\n\n\n\n\n\n\n\nFigurÂ 5: Dialogboks for Ã¥ velge hvordan man skal autentisere seg med 2FA.\n\n\n\nFigurÂ 6 viser QR-koden som vises. Denne skal vi bruke i neste steg.\n\n\n\n\n\n\nFigurÂ 6: QR-kode som skannes av Microsoft Authenticator.\n\n\n\n\n\n\nStrekkoden over skal skannes i din Microsoft Authenticator-app pÃ¥ mobilen, som vist i FigurÂ 7. Ã…pne appen, trykk pÃ¥ Bekreftede ID-er, og til slutt trykk pÃ¥ Skann QR-kode. Deretter skanner du QR-koden fra punkt 5.\nNÃ¥r koden er skannet har du fÃ¥tt opp fÃ¸lgende bilde pÃ¥ appens hovedside (se bilde til hÃ¸yre). Skriv inn den 6-siffer koden pÃ¥ GitHub-siden med QR-koden.\nTil slutt lagrer du Recovery-codes et trygt sted som bare du har tilgang til.\n\n\n\n\n\n\n\n\n\nFigurÂ 7: Mobilappen Microsoft authenticator\n\n\n\n\n\nNÃ¥ har vi aktivert 2-faktor autentisering for GitHub og er klare til Ã¥ knytte vÃ¥r personlige konto til vÃ¥r SSB-bruker pÃ¥ SSBs â€œGitHub organisationâ€ statisticsnorway.\n\n\n\nI forrige steg aktiverte vi 2-faktor autentisering for GitHub. Det neste vi mÃ¥ gjÃ¸re er Ã¥ koble oss til Single Sign On (SSO) for SSB sin organisasjon pÃ¥ GitHub:\n\nTrykk pÃ¥ lenken https://GitHub.com/orgs/statisticsnorway/sso\nI dialogboksen som dukker opp trykker du pÃ¥ Continue, slik som vist i FigurÂ 8.\n\n\n\n\n\n\n\nFigurÂ 8: Single Sign on (SSO) for SSB sin organisasjon pÃ¥ GitHub\n\n\n\nNÃ¥r du har gjennomfÃ¸rt dette sÃ¥ har du tilgang til statisticsnorway pÃ¥ GitHub. GÃ¥r du inn pÃ¥ denne lenken sÃ¥ skal du nÃ¥ kunne lese bÃ¥de Public, Private og Internal repoer, slik som vist i FigurÂ 9.\n\n\n\n\n\n\nFigurÂ 9: Medlemsvisning for SSB sin GitHub-organisasjon.\n\n\n\n\n\n\nNÃ¥r vi skal jobbe med SSB-kode som ligger lagret hos statistcsnorway pÃ¥ GitHub, sÃ¥ mÃ¥ vi autentisere oss. MÃ¥ten vi gjÃ¸re det pÃ¥ er ved Ã¥ generere et Personal Access Token (ofte forkortet PAT) som vi oppgir nÃ¥r vi vil hente eller oppdatere kode pÃ¥ GitHub. Da sender vi med PAT for Ã¥ autentisere oss for GitHub.\n\n\nFor Ã¥ lage en PAT som er godkjent mot statisticsnorway sÃ¥ gjÃ¸r man fÃ¸lgende:\n\nGÃ¥ til din profilside pÃ¥ GitHub og Ã¥pne Settings slik som ble vist SeksjonÂ 1.2.2.\nVelg Developer Settings i menyen til venstre.\nI menyen til venstre velger du Personal Access Token, og deretter Tokens (classic).\nVelg Generate new token og deretter Generate new token (classic).\nUnder Note kan du gi PATâ€™en et navn. Velg et navn som er intuitivt for deg. Hvis du skal bruke PAT til Ã¥ jobbe mot Dapla, sÃ¥ ville jeg ganske enkelt kalt den dapla. Hvis du skal bruke den mot bakkemiljÃ¸et ville jeg kalt den prodsone eller noe annet som gjÃ¸r det lett for det skjÃ¸nne innholdet i ettertid.\nUnder Expiration velger du hvor lang tid som skal gÃ¥ fÃ¸r PAT blir ugyldig. Dette er en avvening mellom sikkerhet og hva som er praktisk. Det anbefales at du velger 365 dager. NÃ¥r PAT utlÃ¸per mÃ¥ du gjenta stegene i dette kapittelet.\nUnder Select scopes velger du repo og workflow slik som vist i FigurÂ 10.\n\n\n\n\n\n\n\nFigurÂ 10: Gi token et kort og beskrivende navn\n\n\n\n\nTrykk pÃ¥ Generate token nederst pÃ¥ siden og du fÃ¥r noe lignende det du ser i FigurÂ 11.\n\n\n\n\n\n\n\nFigurÂ 11: Token som ble generert.\n\n\n\n\nKopier deretter PAT til en midlertidig fil. Grunnen er at du aldri vil se det igjen her etter at vi har gjennomfÃ¸rt neste steg.\nDeretter trykker du pÃ¥ Configure SSO og velger Authorize ved siden statisticsnorway, slik som vist i FigurÂ 12. Svar deretter pÃ¥ spÃ¸rsmÃ¥lene som dukker opp.\n\n\n\n\n\n\n\nFigurÂ 12: Autorisering av Token mot SSBs GiHub-organisasjon.\n\n\n\nVi har nÃ¥ opprettet en PAT som er godkjent for bruk mot SSB sin kode pÃ¥ GitHub. Det betyr at hvis vi vil jobbe med Git pÃ¥ SSB sine maskiner i sky eller pÃ¥ bakken, sÃ¥ mÃ¥ vi sendte med dette tokenet for Ã¥ fÃ¥ lov til Ã¥ jobbe med koden som ligger pÃ¥ statisticsnorway pÃ¥ GitHub.\n\n\n\nDette gjelder gamle Jupyter. Bla lenger ned for Ã¥ lese om hvordan man lagrer PAT pÃ¥ DaplaLab\nDet er ganske upraktisk Ã¥ mÃ¥tte sende med tokenet hver gang vi skal jobbe med GitHub. Vi bÃ¸r derfor lagre det lokalt der vi jobber, slik at Git automatisk finner det. Det finnes mange mÃ¥ter Ã¥ gjÃ¸re dette pÃ¥ og det er ikke bestemt hva som skal vÃ¦re beste-praksis i SSB. Men en mÃ¥te Ã¥ gjÃ¸re det er via en .netrc-fil. Vi oppretter da en fil som heter .netrc pÃ¥ vÃ¥rt hjemmeomrÃ¥de, og legger fÃ¸lgende informasjon pÃ¥ en (hvilken som helst) linje i filen:\n\n\n~/.netrc\n\nmachine GitHub.com login &lt;GitHub-bruker&gt; password &lt;Personal Access Token&gt;\n\nGitHub-bruker er da din personlige bruker og IKKE brukernavnet ditt i SSB. Personal Access Token er det vi lagde i forrige kapittelet.\nEn veldig enkel mÃ¥te Ã¥ lagre dette er som fÃ¸lger. Anta at min personlige GitHub-bruker er SSB-Chad og at min Personal Access Token er blablabla. Da kan jeg gjÃ¸re fÃ¸lgende for Ã¥ lagre det i .netrc:\n\nGÃ¥ inn i Jupyterlab og Ã¥pne en Python-notebook.\nI den fÃ¸rste kodecellen skriver du:\n\n\n\nterminal\n\n!echo \"machine GitHub.com login SSB-Chad password blablabla\" &gt;&gt; ~/.netrc\n\nAlternativt kan du fjerne utropstegnet og kjÃ¸re kommandoen direkte i terminalen. Det vil gi samme resultat. Koden over legger til en linje med teksten\nmachine GitHub.com login SSB-Chad password blablabla\ni en .netrc-fil pÃ¥ din hjemmeomrÃ¥det, uavhengig av om du har en fra fÃ¸r eller ikke. Hvis du har en fil fra fÃ¸r som allerede har et token fra GitHub, ville jeg nok slettet det fÃ¸r jeg legger en et nytt token.\nHver gang du jobber mot GitHub vil Git sjekke om informasjon om autentisering ligger i denne filen, og bruke den hvis den ligger der.\n\n\nÃ… lagre PAT pÃ¥ DaplaLab er enkelt og gjÃ¸res kun en gang uansett hvor mange tjenester man bruker.\nHer er stegene:\n\nLogg inn pÃ¥ https://lab.dapla.ssb.no\nTrykk pÃ¥ â€˜My accountâ€™\nNaviger til Git-fanen\nLim inn token der det stÃ¥r â€˜Git Forge Personal Access Tokenâ€™ vist i FigurÂ 13\n\n\n\n\n\n\n\nFigurÂ 13: DaplaLab My account: lagre PAT\n\n\n\n\n\n\n\nI eksempelet over lagde vi en PAT som var gyldig i 90 dager. Dermed vil du ikke kunne jobbe mot GitHub med dette tokenet etter 90 dager. For Ã¥ oppdatere tokenet gjÃ¸r du fÃ¸lgende:\n\nLag et nytt PAT ved Ã¥ repetere SeksjonÂ 1.2.4.1.\nI miljÃ¸et der du skal jobbe med Git og GitHub gÃ¥r du inn i din .netrc og bytter ut token med det nye.\n\nOg med det er du klar til Ã¥ jobbe mot statisticsnorway pÃ¥ GitHub.",
    "crumbs": [
      "Manual",
      "Kode"
    ]
  },
  {
    "objectID": "statistikkere/git-og-github.html#footnotes",
    "href": "statistikkere/git-og-github.html#footnotes",
    "title": "Git og GitHub",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nBranches kan oversettes til grener pÃ¥ norsk. Men i denne boken velger vi Ã¥ bruke det engelske ordet branches. Grunnen er at det erfaringsmessig er lettere forholde seg til det engelske ordet nÃ¥r man skal sÃ¸ke etter informasjon i annen dokumentasjonâ†©ï¸",
    "crumbs": [
      "Manual",
      "Kode"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard.html",
    "href": "statistikkere/navnestandard.html",
    "title": "Navnestandard",
    "section": "",
    "text": "Data i de permanente datatilstandene inndata, klargjorte data, statistikkdata og utdata skal lagres i Google Cloud Storage (GCS) bÃ¸tter og fÃ¸lge en definert navnestandard. Standarden gjelder for bÃ¥de statistikkprodukter og dataprodukter (se forklaringsboks under). Navnestandarden beskrevet i dette kapitlet er derfor gjeldende for alle data som lagres i bÃ¸tter i standardprosjektet, som f.eks. produktbÃ¸tta og delt-bÃ¸ttene.\nDatatilstanden kildedata omfattes ikke av navnestandarden. Grunnen er at kildedata mottas av SSB i mange former/strukturer og de deles sjelden med andre team. De unike egenskapene til kildedata er ogsÃ¥ grunnen til at de ikke har samme krav til dokumentasjon i metadatasystemene heller.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Navnestandard"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard.html#mappestruktur",
    "href": "statistikkere/navnestandard.html#mappestruktur",
    "title": "Navnestandard",
    "section": "Mappestruktur",
    "text": "Mappestruktur\nNavnestandarden for lagring av data innfÃ¸rer obligatoriske mapper som alle statistikk- og dataprodukter mÃ¥ fÃ¸lge, samt valgfrie deler hvor teamet selv kan bestemme sin mappestruktur.\n\nObligatoriske mapper\nIfÃ¸lge navnestandarden skal fÃ¸lgende mappenivÃ¥er alltid eksistere fÃ¸rst i en lagringsbÃ¸tte:\n\nStatistikkproduktets eller dataproduktets kortnavn\nDatatilstand\n\nAnta at det er team som heter dapla-example som har produserer statistikkproduktene ledstill og sykefra. I tillegg produserer de et dataprodukt som heter ameld. Deres mappestruktur i produktbÃ¸tta vil da se slik ut:\n\n\nObligatoriske mapper\n\nssb-dapla-example-data-produkt-prod/  \nâ””â”€ ledstill/  \n   â”œâ”€â”€ inndata/\n   â”œâ”€â”€ klargjorte-data/\n   â”œâ”€â”€ statistikk/\n   â””â”€â”€ utdata/\nâ””â”€ sykefra/  \n   â”œâ”€â”€ inndata/\n   â”œâ”€â”€ klargjorte-data/\n   â”œâ”€â”€ statistikk/\n   â””â”€â”€ utdata/\nâ””â”€ ameld_data/  \n   â”œâ”€â”€ inndata/\n   â”œâ”€â”€ klargjorte-data/\n   â”œâ”€â”€ statistikk/\n   â””â”€â”€ utdata/                    \n\n\n\nValgfrie mapper\nDe to fÃ¸rste mappenivÃ¥ene er bestemt og obligatoriske. Teamene kan likevel opprette egendefinerte mapper der det er behov. Det kan gjÃ¸res i fÃ¸lgende tilfeller:\n\nTeamet Ã¸nsker Ã¥ organisere dataene i undermapper for hver datatilstand.\nTeamet trenger Ã¥ lagre temporÃ¦re data.\n\nDet er anbefalt Ã¥ lage en temp-mappe pÃ¥ fÃ¸rste nivÃ¥ etter bÃ¸ttenavn, men det er ogsÃ¥ tillatt Ã¥ opprette temp-mapper andre steder i mappe-hierarkiet, f.eks. ../inndata/temp/ eller ../klargjorte-data/temp/.\n\nTeamet utfÃ¸rer oppdrag og Ã¸nsker et eget sted Ã¥ lagre data knyttet til dette. Det kan kun gjÃ¸res i en oppdrag-mappe pÃ¥ fÃ¸rste nivÃ¥ etter bÃ¸ttenavn.\n\nUnder er et nytt eksempel i produktbÃ¸tta for team dapla-example, men nÃ¥ har de kun statistikkproduktet ledstill, en temp-mappe og en oppdrag-mappe. I tillegg sÃ¥ Ã¸nsker de Ã¥ skille mellom data som er produsert pÃ¥ Dapla og data som er migrert fra tidligere plattform. De gjÃ¸r det ved Ã¥ opprette de egendefinerte mappene on-prem og dapla for hver datatilstand.\n\n\nObligatoriske og egendefinerte mapper\n\nssb-dapla-example-data-produkt-prod/  \nâ””â”€ ledstill/  \n   â”œâ”€â”€ inndata/\n       â”œâ”€â”€ on-prem/\n       â”œâ”€â”€ dapla/\n   â”œâ”€â”€ klargjorte-data/\n       â”œâ”€â”€ on-prem/\n       â”œâ”€â”€ dapla/\n   â”œâ”€â”€ statistikk/\n       â”œâ”€â”€ on-prem/\n       â”œâ”€â”€ dapla/\n   â””â”€â”€ utdata/\n       â”œâ”€â”€ on-prem/\n       â”œâ”€â”€ dapla/\nâ””â”€ temp/\nâ””â”€ oppdrag/                     \n\n\n\n\n\n\n\nMappe for oppdrag\n\n\n\nNÃ¥r man oppretter en mappe for oppdrag sÃ¥ er det viktig Ã¥ kunne knytte dataene til et Websak-saksnummer. Det er derfor anbefalt at det opprettes en undermappe med saksnummeret eller at saksnummeret er med i filnavnet.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Navnestandard"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard.html#filnavn",
    "href": "statistikkere/navnestandard.html#filnavn",
    "title": "Navnestandard",
    "section": "Filnavn",
    "text": "Filnavn\nFilnavn skal ha en fast struktur som inneholder: en kort beskrivelse, periode, versjon og filtype, slik som vist i FigurÂ 1.\n\n\n\n\n\n\nFigurÂ 1: De ulike delene av et standardisert filnavn\n\n\n\nEksempelet i FigurÂ 1 har varehandel som kort beskrivelse, dataene er gyldige for 2018-Q1, versjon er 1 og filtypen er parquet. I tillegg ser vi at periodeangivelse alltid skal prefixes med p og versjon med v. Elementene i filnavnet skal skilles med understrek.\nDet er ogsÃ¥ verdt Ã¥ merke seg at mellomrom og sÃ¦rnorske bokstaver som Ã¦, Ã¸ og Ã¥ ikke forekommer i filnavnet. FÃ¸lgende alfanumeriske tegn kan benyttes i fil- og mappenavn:\n\na-z og A-Z2.\n0-9\nBruk bindestrek -, eller understrek _, og ikke mellomrom.\n\nTabellÂ 1 viser en mer inngÃ¥ende beskrivelse av hva som inngÃ¥r i de ulike delene av et filnavn.\n\n\n\nTabellÂ 1: Autonomitet og tilganger i IaC-repo\n\n\n\n\n\nElement\nForklaring\n\n\n\n\nKort beskrivelse\nKort tekst som forklarer datasettets innhold, f.eks. â€œvarehandelâ€ eller â€œpersoninntektâ€. Bruk bindestrek hvis beskrivelsen bestÃ¥r av flere ord, f.eks. â€œgrensehandel-imputertâ€ eller â€œframskrevne-befolkningsendringerâ€.\n\n\nPeriode - inneholder data f.o.m. dato\nDatasettet inneholder data fra og med dato/tidspunkt. I filnavnet mÃ¥ perioden prefikses med â€œ_pâ€, eksempel â€œ_p2022â€ eller â€œ_p2022-01-01â€.Â Se ogsÃ¥ gyldigeÂ formater for periode (dato/tidspunkt)\n\n\nPeriode - inneholder data t.o.m. dato\nDatasettet inneholder data til og med dato/tidspunkt.Â Denne brukes ved behov, eksempelvis for datasett som inneholder forlÃ¸psdata eller datasett med flere perioder/Ã¥rganger.\n\n\nVersjon\nVersjon av datasettet. I filnavnet mÃ¥ versjonsnummeret prefikses med â€œ_vâ€, eksempel â€œv1â€, â€œv2â€ eller â€œv3â€.\n\n\nFiltype\nFilendelse som sier noen om filtypen, f.eks. â€œ.jsonâ€, â€œ.csvâ€, â€œ.xmlâ€ eller â€œ.parquetâ€.\n\n\n\n\n\n\n\nEksempler pÃ¥ gyldige filnavn\nUnder finner du et utvalg eksempler pÃ¥ gyldige filnavn for ulike tidsspenn.\n\n\n\n\n\n\n\nTidsspenn\nEksempel pÃ¥ gyldige filnavn\n\n\n\n\nÃ‰n Ã¥rgang med data\nflygende-objekter_p2019_v1.parquet\n\n\nTo Ã¥rganger med data\nufo-observasjoner_p2019_p2020_v1.parquet\n\n\nFra 2019 til 2050\nframskrevne-befolkningsendringer_p2019_p2050_v1.parquet\n\n\nFra 01.01.2022 til 31.12.2022\nsykepenger_p2022-01-01_p2022-12-31_v1.parquet\n\n\nTverrsnittsdata (status) per 01.10.2022\nutanningsnivaa_p2022-10-01_v1.parquet\n\n\nOktober, november og desember 2022\ngrensehandel-imputert_p2022-10_p2022-12_v1.parquet\n\n\nUke-nummer 1\nomsetning_p2020-W01_v1.parquet\n\n\nUke-nummer 15\nomsetning_p2020-W15_v1.parquet\n\n\nFÃ¸rste bimester i 2022\nskipsanloep_p2022-B1_v1.parquet\n\n\nFÃ¸rste kvartal i 2018 (quarter)\npensjon_p2018-Q1_v1.parquet\n\n\nFÃ¸rste tertial i 2022\nnybilreg_p2022-T1_v1.parquet\n\n\nFÃ¸rste halvÃ¥r i 2022\npersoninntekt_p2022-H1_v1.parquet\n\n\nKvartalene 1, 2, 3 og 4 i 2018\nvarehandel_p2018-Q1_p2018-Q4_v1.parquet\n\n\nDato 31.12.2024 og tid 23:59:30.000\nskjema_p2024-12-31T23-59-30.000_v1.parquet",
    "crumbs": [
      "Manual",
      "Standarder",
      "Navnestandard"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard.html#partisjonerte-data",
    "href": "statistikkere/navnestandard.html#partisjonerte-data",
    "title": "Navnestandard",
    "section": "Partisjonerte data",
    "text": "Partisjonerte data\nTeam som partisjonerer sine filer ved lagring skal fortsatt fÃ¸lge navnestandarden. Det som endrer seg er at filtype ikke blir en del av filnavnet, men heller kommer inn under partisjoneringen. Anta at team dapla-example partisjonerer et datasett i datatilstand inndata som heter skjema_p2018_p2020_v1. Anta ogsÃ¥ at de partisjonerer dataene med hensyn pÃ¥ kolonnen aar. Da vil de i henhold til navnestandarden opprette denne strukturen:\n\n\nMappestruktur partisjonert data\n\nssb-dapla-example-data-produkt-prod/  \nâ””â”€ ledstill/  \n   â”œâ”€â”€ inndata/\n        â””â”€â”€ skjema_p2018_p2020_v1\n            â””â”€â”€ aar=2018\n                â””â”€â”€ data.parquet\n            â””â”€â”€ aar=2019\n                â””â”€â”€ data.parquet\n            â””â”€â”€ aar=2020\n                â””â”€â”€ data.parquet         \n   â”œâ”€â”€ klargjorte-data/\n   â”œâ”€â”€ statistikk/\n   â””â”€â”€ utdata/                 \n\n\nEksempel: ProduktbÃ¸tte for team dapla-example\nAnta at det er team som heter dapla-example med statistikkproduktene ledstill og sykefra, og de har et dataprodukt med kortnavnet ameld. Teamet har fÃ¸lgende mappestruktur i produktbÃ¸tta:\n\n\nProduktbÃ¸tta: ledstill, sykefra og ameld\n\nssb-dapla-example-data-produkt-prod/\nâ””â”€ ledstill/  \n    â”œâ”€â”€ inndata/\n    â”‚   â”œâ”€â”€ skjema_p2024-Q1_v1.parquet\n    â”‚   â”œâ”€â”€ skjema_p2024-Q2_v1.parquet\n    â”‚   â””â”€â”€ skjema_p2024-Q2_v2.parquet\n    â”œâ”€â”€ klargjorte-data/\n    â”‚   â”œâ”€â”€ editert_p2024-Q1_v1.parquet\n    â”‚   â””â”€â”€ editert_p2024-Q2_v1.parquet\n    â”œâ”€â”€ statistikk/\n    â”‚   â”œâ”€â”€ aggregert_p2024-Q1_v1.parquet\n    â”‚   â””â”€â”€ aggregert_p2024-Q2_v1.parquet        \n    â””â”€â”€ utdata/\n    â”‚   â”œâ”€â”€ statbank_p2024-Q1_v1.parquet\n    â”‚   â””â”€â”€ statbank_p2024-Q2_v1.parquet   \n    â”‚\nâ””â”€ sykefra/  \n    â”œâ”€â”€ inndata/\n    â”‚   â”œâ”€â”€ egenmeldt_p2024-Q1_v1.parquet\n    â”‚   â”œâ”€â”€ egenmeldt_p2024-Q2_v1.parquet\n    â”‚   â”œâ”€â”€ legemeldt_p2024-Q1_v1.parquet\n    â”‚   â””â”€â”€ legemeldt_p2024-Q2_v1.parquet\n    â”œâ”€â”€ klargjorte-data/\n    â”‚   â”œâ”€â”€ sykefravaer_p2024-Q1_v1.parquet\n    â”‚   â””â”€â”€ sykefravaer_p2024-Q2_v1.parquet\n    â”œâ”€â”€ statistikk/\n    â”‚   â”œâ”€â”€ aggregert_p2024-Q1_v1.parquet\n    â”‚   â””â”€â”€ aggregert_p2024-Q2_v1.parquet\n    â””â”€â”€ utdata/\n    â”‚   â”œâ”€â”€ statbank_p2024-Q1_v1.parquet\n    â”‚   â””â”€â”€ statbank_p2024-Q2_v1.parquet\n    â”‚\nâ””â”€ ameld_data/  \n    â”œâ”€â”€ inndata/\n    â”‚   â”œâ”€â”€ ameldingen_p2024-11_v1.parquet\n    â”‚   â””â”€â”€ ameldingen_p2024-12_v1.parquet\n    â””â”€â”€ klargjorte-data/\n    â”‚   â”œâ”€â”€ ameldingen_p2024-11_v1.parquet\n    â”‚   â””â”€â”€ ameldingen_p2024-12_v1.parquet",
    "crumbs": [
      "Manual",
      "Standarder",
      "Navnestandard"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard.html#versjonering-av-datasett",
    "href": "statistikkere/navnestandard.html#versjonering-av-datasett",
    "title": "Navnestandard",
    "section": "Versjonering av datasett",
    "text": "Versjonering av datasett\nVersjonering er obligatorisk nÃ¥r man jobber med data pÃ¥ dapla. Hovedgrunnen til at vi versjonerer er for Ã¥ dekke kravet om uforanderlighet og etterprÃ¸vbarehet: at data-konsumenter (menneske eller maskin) skal ha kontroll pÃ¥ endringer. Derfor skal et datasett som er brukt i statistikkproduksjon aldri slettes - det skal opprettes en ny versjon av datasettet. Les mer om prinsippet om uforanderlighet av data pÃ¥ confluence-siden til IT-Arkitektur.\nKort fortalt innebÃ¦rer versjonering av data at datasettene har versjonsnummer fÃ¸r filendelsen. For eksempel: framskrevne-befolkningsendringer_p2019_p2050_v1.parquet\n\n\n\n\n\n\nUnntak til versjonering: nyeste versjon og temporÃ¦re data\n\n\n\nNyeste versjon kan lagres uten versjonsnummer. Dette er for at man enkelt skal kunne lese inn siste versjon av et datasett (ved Ã¥ utelate versjonssuffiks). I tilegg trenger man ikke versjonere temporÃ¦re data.\n\n\n\nNÃ¥r skal man lagre ny versjon?\nFÃ¸lgende hendelser skaper ny versjon av et datasett:\n\nReberegninger av data med nye metoder.\nKorrigeringer av verdier.\nOservasjoner legges til eller fjernes.\nOppdatert eller erstattet kodeverk.\nVariabler fjernes eller legges til.\n\nHvis det gjÃ¸res vesentlige endringer (mange variabler) sÃ¥ bÃ¸r det vurderes om dette er et helt nytt datasett.\n\nAndre strukturendringer, f.eks. bytte av datatyper eller formater.\n\nMed andre ord: enhver endring skaper en ny versjon!\n\n\nVersjonering i praksis\nFor hver versjon som oppstÃ¥r av datasettet opprettes det en ny fysisk fil hvor versjonsnummeret Ã¸kes med en. Alle gamle versjoner av et datasett skal ogsÃ¥ eksistere i mappen.\nEtterhvert som man fÃ¥r flere versjoner av et datasett kan det se slik ut:\n\n\nMappe med flere versjoner av et datasett\n\nssb-prod-team-personstatistikk-data-produkt-prod/  \nâ””â”€â”€ befolkningsframskrivinger/  \n    â””â”€â”€ klargjorte-data/  \n        â”œâ”€â”€ framskrevne-befolkningsendringer_p2019_p2050.parquet  \n        â”œâ”€â”€ framskrevne-befolkningsendringer_p2019_p2050_v1.parquet  \n        â”œâ”€â”€ framskrevne-befolkningsendringer_p2019_p2050_v2.parquet  \n        â””â”€â”€ framskrevne-befolkningsendringer_p2019_p2050_v3.parquet\n\nEksempelet over viser at siste versjon av en fil kan lagres med og uten versjonsnummer for Ã¥ gjÃ¸re det lettere Ã¥ lese inn nyeste versjon.\n\nEksempelkode med pakken ssb-fagfunksjoner\n\nPython \n\n\n\n\nPython kode fra SSB-fagfunksjoner for finne neste filversjon\n\n# importer funksjonen next_version_path() fra ssb-fagfunksjoner\nfrom fagfunksjoner import next_version_path\n\nfilsti = 'gs://ssb-prod-team-personstatistikk-data-produkt-prod/befolkningsframskrivninger/klargjorte-data/framskrevne-befolkningsendringer_p2019_p2050.parquet'\n\nny_filsti = next_version_path(filsti)\n\nprint(ny_filsti)\n# vil returnere:\n# 'gs://ssb-prod-team-personstatistikk-data-produkt-prod/befolkningsframskrivninger/klargjorte-data/framskrevne-befolkningsendringer_p2019_p2050_v4.parquet'\n\nssb-fagfunksjoner har ogsÃ¥ fÃ¸lgende funksjoner for Ã¥ gjÃ¸re versjonering lettere:\n\nget_fileversions() # Retrieves a list of file versions matching a specified pattern.\nlatest_version_number() # Function for finding latest version in use for a file.\nlatest_version_path() # Finds the path to the latest version of a specified file.\n\n\n\n\n\nR-kode fra fellesr for finne neste filversjon\n\nlibrary(fellesr)\nfil = \"/buckets/produkt/dapla-manual-examples/testdata_p2025-Q2_v1.parquet\"\nny_filsti = lag_versjonert_filsti(fil, versjon = \"ny\")\nprint(ny_filsti)\n\n# Vil returnere: \"/buckets/produkt/dapla-manual-examples/testdata_p2025-Q2_v2.parquet\"\n\nfellesr har ogsÃ¥ fÃ¸lgende funksjoner for Ã¥ gjÃ¸re versjonering lettere: * finn_fileversjoner(fil) # Skaffer en liste over filversjoner som matcher filnavn (KOMMER). * finn_versjon(lag_versjonert_filsti(fil, versjon = \"siste\")) # NÃ¸stet funksjon som finner nummeret til en riktig versjonert fil. * lag_versjonert_filsti(fil, versjon = \"siste\") # Finner stien til den siste versjonen av en spesifisert fil.\n\n\n\n\n\n\n\n\n\nPermanente data skal ikke endres eller slettes\n\n\n\nTidligere delte/publiserte data skal ikke slettes eller overskrives!\nDet mÃ¥ derfor lagres fysiske filer for hver versjon av datasettet. Dette er viktig for at SSB skal oppfylle krav om etterprÃ¸vbarhet av statistikkene.\n\n\n\n\nVersjon 0: Deling av data som ikke har oppnÃ¥dd stabil tilstand\nHvis det er behov for Ã¥ dele data som fortsatt er under innsamling eller pÃ¥gÃ¥ende klargjÃ¸ring gjÃ¸res dette ved Ã¥ bruke versjonsnummer 0 i filnavnet.\nDette versjonsnummeret skal kun brukes midlertidig fram til datasettet oppnÃ¥r stabil tilstand. Ved stabil tilstand byttes versjonsnummer for datasettet til 1 eller hÃ¸yere.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Navnestandard"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard.html#footnotes",
    "href": "statistikkere/navnestandard.html#footnotes",
    "title": "Navnestandard",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nLes mer om hvordan man henter ut informasjon fra API-et til Statistikkregisteret i denne blogg-artikkelen.â†©ï¸\nDet er anbefalt at Ã¦, Ã¸ og Ã¥ erstattes med ae, oe og aa, f.eks. naering, oekonomi eller levekaar.â†©ï¸",
    "crumbs": [
      "Manual",
      "Standarder",
      "Navnestandard"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html",
    "href": "statistikkere/hva-er-dapla-team.html",
    "title": "Hva er Dapla-team?",
    "section": "",
    "text": "For Ã¥ kunne jobbe pÃ¥ Dapla mÃ¥ man vÃ¦re en del av et Dapla-team. Et Dapla-team er en gruppe personer som har tilgang til spesifikke ressurser pÃ¥ Dapla. Ressursene kan vÃ¦re data, kode eller tjenester. FÃ¸lgelig er teamet helt sentral for tilgangsstyringen pÃ¥ Dapla. Derfor er det viktig at alle som jobber pÃ¥ Dapla gjÃ¸r seg godt kjent med innholdet i denne delen.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#opprette-dapla-team",
    "href": "statistikkere/hva-er-dapla-team.html#opprette-dapla-team",
    "title": "Hva er Dapla-team?",
    "section": "Opprette Dapla-team",
    "text": "Opprette Dapla-team\nAlle Dapla-team tilhÃ¸rer en seksjon og opprettes av seksjonslederen i den seksjonen. Dapla-team opprettes i applikasjonen Dapla-Ctrl.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#autonomitetsnivÃ¥",
    "href": "statistikkere/hva-er-dapla-team.html#autonomitetsnivÃ¥",
    "title": "Hva er Dapla-team?",
    "section": "AutonomitetsnivÃ¥",
    "text": "AutonomitetsnivÃ¥\nEt team pÃ¥ Dapla er i en av fÃ¸lgende autonomitetsnivÃ¥er:\n\nManaged\nSemi-Managed\nSelf-Managed\n\nNivÃ¥et er definert i metadataene til teamet og vises i Teamvisningen i Dapla Ctrl. Det er kun plattformteamene som kan endre nivÃ¥et til et team.\nFormÃ¥let med Ã¥ definere autonomiten til et team er Ã¥ tydeliggjÃ¸re hvem som har hvilket ansvar for de produktene teamet benytter pÃ¥ Dapla. NivÃ¥et settes nÃ¥r teamet opprettes, men kan ogsÃ¥ endres senere ved behov.\nEt Managed team benytter seg kun av tjenestene/features som tilbys av plattformen, og kan vÃ¦re sikker pÃ¥ at disse er satt opp i iht de krav som gjelder i SSB. Typisk vil de fleste statistikkteam vÃ¦re managed, og eksempler pÃ¥ tjenester er standard lagringsbÃ¸tter, Transfer Service, Kildomaten, osv.. Et Managed team kan kun benytte seg av tilgangsgruppene managers, data-admins og developers.\nEt Semi-Managed team benytter seg av tjenestene som tilbys pÃ¥ plattformen, akkurat som et Managed team, men de Ã¸nsker ogsÃ¥ ta i bruk noe funksjonalitet som ikke tilbys enda. F.eks. kan det vÃ¦re et statistikkproduserende team som Ã¸nsker Ã¥ ta i bruk en Google-tjeneste som ikke tilbys pÃ¥ Dapla. I disse tilfellene kan teamet velge Ã¥ ta et stÃ¸rre ansvar for denne tjenesten og fÃ¥ noe bredere tilganger pÃ¥ plattformen. Ansvaret fordrer at teamet har kompetansen til Ã¥ ta dette ansvaret, og de spesifikke detaljene vil avhenge hvilken tjeneste det er snakk om, og om de Ã¸nsker Ã¥ benytte tjenesten i prod- eller test-miljÃ¸et til teamet.\nEt Self-Managed team vil typisk vÃ¦re team som bestÃ¥r IT-utviklere med god kompetanse pÃ¥ skyutvikling i Google Cloud Platform. Teamet har kompetanse til Ã¥ ta i bruk de tjenestene de mener er best for Ã¥ lÃ¸se sine oppgaver.\nI resten av dette kapitlet beskrives hovedsakelig Managed teams.\n\n\n\n\n\n\nAutonomitetsnivÃ¥ og tilganger i IaC-repo\n\n\n\n\n\nSiden hvert team fÃ¥r definert sine ressurser i et eget IaC-repo, sÃ¥ er det nÃ¦r sammenheng mellom AutonomitetsnivÃ¥ og hvilke tilganger teamet har til Ã¥ gjÃ¸re endringer i IaC-repoet. TabellÂ 1 viser hvilke tilganger de ulike nivÃ¥ene gir.\n\n\n\nTabellÂ 1: Autonomitet og tilganger i IaC-repo\n\n\n\n\n\n\n\n\n\n\n\n\nManaged\nSemi-Managed\nSelf-Managed\n\n\n\n\nKan lage PR pÃ¥ IaC-kode\nJa\nJa\nJa\n\n\nTilgang pÃ¥ IaC-kode\nKun yaml-â€œstÃ¸ttefilerâ€ (team-info, iam, buckets-shared, projects) og filer de endrer (dapla-filer)\nJa\nJa\n\n\nKan ta i bruk funksjonalitet utover dapla-features\nNei\nJa\nJa\n\n\nIaC-filstruktur (under infra-mappen)\nPredefinert\nPredefinert + egne filer\nFritt\n\n\n\n\n\n\nLes mer her.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#roller-i-teamet",
    "href": "statistikkere/hva-er-dapla-team.html#roller-i-teamet",
    "title": "Hva er Dapla-team?",
    "section": "Roller i teamet",
    "text": "Roller i teamet\nMedlemskap i et Dapla-team gir tilgang pÃ¥ spesifikke ressurser pÃ¥ Dapla. Men siden kildedataene til alle team er klassifisert som sensitive, sÃ¥ kan ikke alle pÃ¥ teamet ha lik tilgang til alle ressurser. Av den grunn er det definert 3 ulike roller pÃ¥ et team. To av disse, data-admins og developers, er forbeholdt de som jobber med data pÃ¥ teamet. Mens den tredje, managers, skal innehas av de som er ansvarlige for teamet. I de fleste tilfeller vil managers vÃ¦re seksjonslederen som er ansvarlige for statistikkproduktene teamet leverer. Under forklarer vi nÃ¦rmere hva de ulike rollene innebÃ¦rer.\n\nManagers\nRollen managers skal bestÃ¥ av en eller flere data-ansvarlige (ofte omtalt som data-eiere eller seksjonsledere). managers har ansvar for fÃ¸lgende i teamet:\n\nhvem i teamet som fÃ¥r tilgang til hvilke data og tjenester.\nat teamet fÃ¸lger SSBs retningslinjer for tilgangsstyring.\nat teamet fÃ¸lger SSBs retningslinjer for klassifisering av data.\nvedlikehold og monitorering av tilganger.\nat teamet fÃ¸lger og forstÃ¥r hvordan sensitive data skal behandles i SSB.\n\nManager-rollen krever ingen tilgang til data eller databehandlende tjenester pÃ¥ Dapla.\n\n\nDevelopers\nRollen developers er den mest vanlige rollen pÃ¥ et Dapla-team. Denne rollen skal tildeles alle som jobber med data i teamet. developers har tilgang til fÃ¸lgende ressurser:\n\nalt av teamets data, med unntak av kildedata.\nalle ressurser som behandler datatilstandene fra inndata til utdata.\n\n\n\nData-admins\nRollen data-admins er en priveligert rolle blant de som jobber med data i teamet. Rollen skal kun tildeles 2-3 personer pÃ¥ et team og disse er da medlem av bÃ¥de developers- og data-admins-gruppen. De som er medlem av data-admins-gruppen kan gjÃ¸re fÃ¸lgende:\n\nde er forhÃ¥ndsgodkjent til Ã¥ gi seg selv tidsbegrenset tilgang til kildedata ved behov. Tilgang til kildedata skal kun aktiveres i sÃ¦rskilte tilfeller, der eneste lÃ¸sning er Ã¥ se pÃ¥ kildedata i klartekst. Tilgang skal kun aktiveres i en begrenset periode, og krever en skriftlig begrunnelse. managers skal lett kunne monitere hvem som aktiverer denne tilgangen og hvor ofte.\nde kan godkjenne endringer i automatiske jobber som prosesserer kildedata til inndata.\nde kan overfÃ¸re kildedata mellom bakke og sky.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#ressurser",
    "href": "statistikkere/hva-er-dapla-team.html#ressurser",
    "title": "Hva er Dapla-team?",
    "section": "Ressurser",
    "text": "Ressurser\nNÃ¥r du oppretter et dapla-team sÃ¥ fÃ¥r man en grunnpakke med ressurser som de fleste i SSB vil trenge for Ã¥ kunne jobbe med data pÃ¥ Dapla. I tillegg kan teamet selvbetjent skru pÃ¥ andre tjenester hvis man Ã¸nsker det. I det fÃ¸lgende forklarer vi hva som er inkludert i grunnpakken, og hva som er tilgjengelig for Ã¥ skru pÃ¥ ved behov.\n\nGrunnpakken\nFigurÂ 1 viser et overordnet bilde av hvilke ressurser som er inkludert i â€œgrunnpakkenâ€. Et Dapla-team fÃ¥r et testmiljÃ¸ og prodmiljÃ¸. Det er i prodmiljÃ¸et at man jobber med skarpe data, mens testmiljÃ¸et er forbeholdt arbeid med testdata. I hvert miljÃ¸ fÃ¥r teamet to Google-prosjekter. Ett for kildedata og et for datatilstandene inndata, klargjorte data, statistikkdata og utdata. Sistnevnte prosjekt kaller vi for standardprosjektet, siden det er her mesteparten av databehandlingen skjer.\n\n\n\n\n\n\nFigurÂ 1: Diagram over hvilke miljÃ¸er, Google-prosjekter og bÃ¸tter et Dapla-team som et fÃ¥r ved opprettelse.\n\n\n\nAv FigurÂ 1 ser vi at prosjektene i prodmiljÃ¸et fÃ¥r noen flere bÃ¸tter enn prosjektene i testmiljÃ¸et. Disse ekstrabÃ¸ttene er forbeholdt synkronisering av data mellom bakke og sky, noe vi ikke legger til rette for i testmiljÃ¸et1. Les mer om overfÃ¸ring av data mellom bakke og sky her.\nRessursene som opprettes for et Dapla-team reflekterer i stor grad at kildedata er klassifisert som sensitive. Dette er grunnen til at det opprettes et eget prosjekt for kildedata, og at det kun er data-admins som potensielt kan fÃ¥ tilgang til dataene her. Opprettelsen av et eget testmiljÃ¸ skyldes at Dapla-team i stÃ¸rre grad enn fÃ¸r forventes Ã¥ jobbe med testdata istedenfor skarpe data.\nAlle ressursene som opprettes for teamet er definert i tekstfiler i et GitHub-repo. Dette repoet kaller vi for et IaC-repo (Infrastructure as Code). IaC-repoet er en del av grunnpakken, og er tilgjengelig for alle pÃ¥ teamet. Statistikkere trenger ikke Ã¥ forholde seg til dette repoet i stor grad, med unntak av nÃ¥r de skal aktivere/deaktivere features og nÃ¥r de skal sette opp Kildomaten.\n\n\nFeatures\nI tillegg til grunnpakken med ressurser, sÃ¥ kan teamet selvbetjent skru pÃ¥ fÃ¸lgende features eller tjenester ved behov:\n\nTransfer Service kan skrus pÃ¥ hvis teamet trenger Ã¥ synkronisere data mellom ulike lagringssystemer. For eksempel mellom bakke og sky, eller mellom to ulike skytjenester.\nKildomaten kan skrus pÃ¥ hvis teamet trenger Ã¥ automatisere overgangen fra kildedata til inndata.\nShared-buckets kan skrus pÃ¥ hvis teamet trenger Ã¥ opprette delt-bÃ¸tter.\n\nForelÃ¸pig er det kun disse tre features som er tilgjengelig. Det vil komme flere etterhvert som behovene melder seg.\nLes mer om features her.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#github-team",
    "href": "statistikkere/hva-er-dapla-team.html#github-team",
    "title": "Hva er Dapla-team?",
    "section": "GitHub-team",
    "text": "GitHub-team\nVed opprettelsen av et Dapla-team sÃ¥ blir det ogsÃ¥ opprettet et tilsvarende GitHub-team med samme navn som Dapla-teamet. Grunnen til at det blir opprettet et GitHub-team er at GitHub er en sentral del av Dapla. Alle ressurser som skal opprettes pÃ¥ plattformen defineres av GitHub-repoer, og vi Ã¸nsker at tilganger her ogsÃ¥ skal reflektere tilgangene pÃ¥ Dapla.\nFor eksempel vil et team med navnet dapla-example fÃ¥ et GitHub-team med navnet dapla-example. Alle som er medlem av Dapla-teamet vil automatisk bli medlem av GitHub-teamet. I tillegg vil gruppetilhÃ¸righet og tilgangsroller pÃ¥ GitHub-teamet reflektere tilgangsroller pÃ¥ Dapla-teamet. For eksempel sÃ¥ kan dapla-example-data-admins gis tilgang til repo, og da vil alle som er medlem av Dapla-teamet med rollen data-admins fÃ¥ tilgang til repoet. Dette benyttes blant annet for Ã¥ gi teamet tilgang til automation-mappen i sitt IaC-repo. I tillegg kan teamet bruke GitHub-teamet til Ã¥ gi tilgang til andre GitHub-repoer som er relevante for teamet, for eksempel kodenbasen til en statistikkproduksjon eller lignende. Fordelen er at tilganger er gitt pÃ¥ teamnivÃ¥ og ikke pÃ¥ personnivÃ¥. For eksempel hvis manager for teamet fjerner en ansatt fra developers-gruppa, sÃ¥ mister de all tilgang til data, tjenester og kode pÃ¥ GitHub som er tilgjengelig for developers.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#navnestruktur",
    "href": "statistikkere/hva-er-dapla-team.html#navnestruktur",
    "title": "Hva er Dapla-team?",
    "section": "Navnestruktur",
    "text": "Navnestruktur\nNÃ¥r du oppretter et Dapla-team sÃ¥ mÃ¥ du velge et navn pÃ¥ teamet. Teamet velger selv et navn som reflekterer domene og subdomene. For eksempel kan et team som jobber med statistikkproduksjonen skattestatistikk for nÃ¦ringslivet velge Ã¥ kalle teamet Skatt nÃ¦ring. Hvis vi bruker dette teamet som et eksempel, sÃ¥ vil det fÃ¥ opprettet et teknisk navn som fÃ¸lger denne strukturen: skatt-naering. Dette navnet er det som brukes i tekniske sammenhenger, for eksempel som navn pÃ¥ GitHub-teamet, IaC-repoet, Google-prosjektene og bÃ¸ttene. TabellÂ 2 viser en tabell over hvordan ressursene for dette teamet vil se ut:\n\n\n\nTabellÂ 2: Navnestruktur for teamet Skatt nÃ¦ring sine ressurser\n\n\n\n\n\n\n\n\n\nNavn\nBeskrivelse\n\n\n\n\nskatt-naering\nTeknisk teamnavn\n\n\nskatt-naering-managers\nAD-gruppe for managers\n\n\nskatt-naering-data-admins\nAD-gruppe for data-admins og et GitHub-team\n\n\nskatt-naering-developers\nAD-gruppe for developers og et GitHub-team\n\n\nskatt-naering-kilde-p\nNavn pÃ¥ kildeprosjekt i prod\n\n\nskatt-naering-p\nNavn pÃ¥ standardprosjekt i prod\n\n\nskatt-naering-kilde-t\nNavn pÃ¥ kildeprosjekt i test\n\n\nskatt-naering-t\nNavn pÃ¥ standardprosjekt i test\n\n\n\n\n\n\nI TabellÂ 2 ser vi at teamet fÃ¥r opprettet 3 AD-grupper og 4 Google-prosjekter. AD-gruppene brukes til Ã¥ gi tilgang til ressursene pÃ¥ Dapla, mens Google-prosjektene brukes til Ã¥ organisere ressursene. I tillegg er det en fast navnestruktur for bÃ¸ttene i hvert prosjekt, slikt som vist i TabellÂ 3.\n\n\n\nTabellÂ 3: Navnestruktur for teamet Skatt nÃ¦ring sine bÃ¸tter\n\n\n\n\n\nProsjektnavn\nBÃ¸ttenavn\n\n\n\n\nskatt-naering-kilde-p\nssb-skatt-naering-data-kilde-prod\n\n\n\nssb-skatt-naering-data-kilde-frasky-prod\n\n\n\nssb-skatt-naering-data-kilde-tilsky-prod\n\n\nskatt-naering-p\nssb-skatt-naering-data-produkt-prod\n\n\n\nssb-skatt-naering-data-frasky-prod\n\n\n\nssb-skatt-naering-data-tilsky-prod\n\n\nskatt-naering-kilde-t\nssb-skatt-naering-data-kilde-test\n\n\nskatt-naering-t\nssb-skatt-naering-data-produkt-test",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#dapla-felles",
    "href": "statistikkere/hva-er-dapla-team.html#dapla-felles",
    "title": "Hva er Dapla-team?",
    "section": "Dapla Felles",
    "text": "Dapla Felles\nAlle i SSB er med i developers-gruppa til team Dapla Felles. FormÃ¥let med teamet er gjÃ¸re det lett som mulig for alle i SSB Ã¥ komme-i-gang med Dapla, samtidig som det er et egnet sted for Ã¥ dele Ã¥pne data eller kursmateriell. Teamet har autonomitetsnivÃ¥ managed, og har de samme bÃ¸ttene som et vanlig statistikkproduserende team.\nAlle i SSB har lese- og skrivetilgang til produkt-bÃ¸tta til Dapla Felles, og derfor skal det aldri deles data som ikke alle i SSB kan benytte. I tillegg mÃ¥ alle forvente at data her kan slettes og overskrives med jevne mellomrom.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#footnotes",
    "href": "statistikkere/hva-er-dapla-team.html#footnotes",
    "title": "Hva er Dapla-team?",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nTa kontakt med produkteier for Dapla hvis du trenger Ã¥ synkronisere testdata mellom bakke og skyâ†©ï¸",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/innlogging.html",
    "href": "statistikkere/innlogging.html",
    "title": "Innlogging",
    "section": "",
    "text": "Innlogging pÃ¥ Dapla er veldig enkelt. Dapla er en nettadresse som alle SSB-ere kan gÃ¥ inn pÃ¥ hvis de er logget pÃ¥ SSB sitt nettverk. Ã… vÃ¦re logget pÃ¥ SSB sitt nettverk betyr i denne sammenhengen at man er logget pÃ¥ med VPN, enten man er pÃ¥ kontoret eller pÃ¥ hjemmekontor. For Ã¥ gjÃ¸re det enda enklere har vi laget en fast snarvei til denne nettadressen pÃ¥ vÃ¥rt intranett/ByrÃ¥nettet(se FigurÂ 1).\n\n\n\n\n\n\nFigurÂ 1: Snarvei til Dapla fra intranett\n\n\n\nMen samtidig som det er lett Ã¥ logge seg pÃ¥, sÃ¥ er det noen kompliserende ting som fortjener en forklaring. Noe skyldes at vi mangler et klart sprÃ¥k for Ã¥ definere bakkemiljÃ¸et og skymiljÃ¸et slik at alle skjÃ¸nner hva man snakker om. I denne boken definerer bakkemiljÃ¸et som stedet der man har drevet med statistikkproduksjon de siste tiÃ¥rene. SkymiljÃ¸et er den nye dataplattformen Dapla pÃ¥ Google Cloud.\nDet som gjÃ¸r ting litt komplisert er at vi har 2 Jupyter-miljÃ¸er pÃ¥ bÃ¥de bakke og sky. Ã…rsaken er at vi har ett test- og ett prod-omrÃ¥de for hver, og det blir i alt 4 Jupyter-miljÃ¸er. FigurÂ 2 viser dette.\n\n\n\n\n\n\nFigurÂ 2: De 4 Jupyter-miljÃ¸ene i SSB. Et test-miljÃ¸ og et prod-miljÃ¸ pÃ¥ bakke og sky/Dapla\n\n\n\nHver av disse miljÃ¸ene har sin egen nettadresse og sitt eget bruksomrÃ¥de.\n\n\nI de fleste tilfeller vil en statistikker eller forsker Ã¸nske Ã¥ logge seg inn i prod-miljÃ¸et. Det er her man skal kjÃ¸re koden sin i et produksjonslÃ¸p som skal publiseres eller utvikles. I noen tilfeller hvor man ber om Ã¥ fÃ¥ tilgjengliggjort en ny tjeneste sÃ¥ vil denne fÃ¸rst rulles ut i testomrÃ¥det som vi kaller staging-omrÃ¥det. Ã…rsaken er at vi Ã¸nsker Ã¥ beskytte prod-miljÃ¸et fra software som potensielt Ã¸delegger for eksisterende funksjonalitet. Derfor ruller vi ut nye ting i staging fÃ¸rst. Av den grunn vil de fleste oppleve Ã¥ bli bedt om Ã¥ logge seg inn der for testing en eller annen gang. Under forklarer vi hvordan man gÃ¥r frem for Ã¥ logge seg pÃ¥ de to ulike miljÃ¸ene pÃ¥ Dapla.\n\n\nFor Ã¥ logge seg inn inn i prod-miljÃ¸et pÃ¥ Dapla kan man gjÃ¸re fÃ¸lgende:\n\nGÃ¥ inn pÃ¥ lenken https://jupyter.dapla.ssb.no/ i en Chrome-nettleser eller klikk pÃ¥ lenken pÃ¥ ByrÃ¥nettet som vist i FigurÂ 1.\nAlle i SSB har en Google Cloud-konto som mÃ¥ brukes nÃ¥r man logger seg pÃ¥ Dapla. Brukernavnet i Google er det samme som din korte epost-adresse (f.eks. cth@ssb.no). Hvis du ikke allerede er logget inn i Google vil du fÃ¥ spÃ¸rsmÃ¥l om Ã¥ velge hvilken Google-konto som skal brukes (FigurÂ 3). Logg inn med din Google-konto (ssb.no) og ditt AD-passord.\n\n\n\n\n\n\n\nFigurÂ 3: Velg en Google-konto\n\n\n\n\nDeretter blir man spurt om man godtar at ssb.no (altsÃ¥ Dapla) kan bruke din Google Cloud-konto (FigurÂ 4). Trykk Allow.\n\n\n\n\n\n\n\nFigurÂ 4: Tillat at ssb.no fÃ¥r bruke din Google Cloud-konto\n\n\n\n\nDeretter lander man pÃ¥ en side som lar deg avgjÃ¸re hvor mye maskinkraft som skal holdes av til deg (FigurÂ 5). Det Ã¸verste alternativet er valgt som standard, og er tilstrekkelig for de fleste.\n\n\n\n\n\n\n\nFigurÂ 5: Velg hvor mye maskinkraft du trenger\n\n\n\n\nVent til maskinen din starter opp (FigurÂ 6). Oppstartstiden kan variere.\n\n\n\n\n\n\n\nFigurÂ 6: Starter opp Jupyter\n\n\n\nEtter dette er man logget inn i et Jupyter-miljÃ¸ som kjÃ¸rer pÃ¥ en minimal Ubuntu-maskin. Hvis man er del av et Dapla-team fÃ¥r man ogsÃ¥ tilgang til alt teamet har tilgang til.\n\n\n\nInnlogging til staging-miljÃ¸et er identisk med innloggingen til prod-miljÃ¸et, med ett viktig unntak: nettadressen er nÃ¥ https://jupyter.dapla-staging.ssb.no/.\nLitt mer om hva som er tilgjenglig her kommer.\n\n\n\n\n\nNoen ganger kan man komme i en situasjon hvor lÃ¸sningen for Single Sign-On (pÃ¥logging pÃ¥ tvers av flere systemer) gir en feilmelding a la FigurÂ 7:\n\n\n\n\n\n\nFigurÂ 7: Feil som kan oppstÃ¥ ved pÃ¥logging\n\n\n\nI denne situasjonen mÃ¥ man trykke pÃ¥ knappen â€œAdd to existing accountâ€. Da vil skjermbildet FigurÂ 8 dukke opp:\n\n\n\n\n\n\nFigurÂ 8: Klikk pÃ¥ Google-knappen for Ã¥ logge pÃ¥ igjen\n\n\n\nHer mÃ¥ man tykke pÃ¥ Google-knappen (se pil), og deretter logge inn som vist i FigurÂ 3 tidligere i dette avsnittet.\n\n\n\n\n\nJupyter-miljÃ¸et pÃ¥ bakken bruker samme base-image1 for Ã¥ installere Jupyterlab, og er derfor identisk pÃ¥ mange mÃ¥ter. Men innloggingen er ganske forskjellig.\n\n\n\n\n\n\nNote\n\n\n\nFom. 5. desember 2022 har vi byttet ut Jupyter-miljÃ¸et pÃ¥ bakken. Beskrivelsene under gjelder derfor det nye miljÃ¸et. Fram til 15. januar vil du kunne bruke det gamle miljÃ¸et ved Ã¥ gÃ¥ inn pÃ¥ lenken https://jupyter-prod.ssb.no/ manuelt i Google Chrome. Etter 15. januar blir det gamle Jupyter-miljÃ¸et avviklet.\n\n\n\n\nDu logger deg inn pÃ¥ prod i bakkemiljÃ¸et pÃ¥ fÃ¸lgende mÃ¥te:\n\nLogg deg inn pÃ¥ Citrix-Windows i bakkemiljÃ¸et. Det kan gjÃ¸res ved Ã¥ bruke lenken Citrix pÃ¥ ByrÃ¥nettet, som ogsÃ¥ vises i FigurÂ 1.\nTrykk pÃ¥ Jupyterlab-ikonet, som vist pÃ¥ FigurÂ 9, og logg deg inn med vanlig brukernavn og passord.\n\n\n\n\n\n\n\nFigurÂ 9: Jupyterlab-ikon pÃ¥ Skrivebordet i Citrix-Windows.\n\n\n\nNÃ¥r du trykker pÃ¥ ikonet blir du tatt til nettadressen https://sl-jupyter-p.ssb.no/. Du kunne ogsÃ¥ Ã¥pnet Jupyterlab ved Ã¥pne Chrome-nettleseren og skrive inn adressen manuelt.\n\n\n\nInnlogging til staging-miljÃ¸et har ingen snarvei pÃ¥ Skrivebordet, og du mÃ¥ gjÃ¸re fÃ¸lgende for Ã¥ Ã¥pne miljÃ¸et:\n\nÃ…pne Chrome-nettleseren i Citrix-Windows.\nSkriv inn url-en https://sl-jupyter-t.ssb.no/",
    "crumbs": [
      "Manual",
      "Appendix",
      "Innlogging"
    ]
  },
  {
    "objectID": "statistikkere/innlogging.html#dapla",
    "href": "statistikkere/innlogging.html#dapla",
    "title": "Innlogging",
    "section": "",
    "text": "I de fleste tilfeller vil en statistikker eller forsker Ã¸nske Ã¥ logge seg inn i prod-miljÃ¸et. Det er her man skal kjÃ¸re koden sin i et produksjonslÃ¸p som skal publiseres eller utvikles. I noen tilfeller hvor man ber om Ã¥ fÃ¥ tilgjengliggjort en ny tjeneste sÃ¥ vil denne fÃ¸rst rulles ut i testomrÃ¥det som vi kaller staging-omrÃ¥det. Ã…rsaken er at vi Ã¸nsker Ã¥ beskytte prod-miljÃ¸et fra software som potensielt Ã¸delegger for eksisterende funksjonalitet. Derfor ruller vi ut nye ting i staging fÃ¸rst. Av den grunn vil de fleste oppleve Ã¥ bli bedt om Ã¥ logge seg inn der for testing en eller annen gang. Under forklarer vi hvordan man gÃ¥r frem for Ã¥ logge seg pÃ¥ de to ulike miljÃ¸ene pÃ¥ Dapla.\n\n\nFor Ã¥ logge seg inn inn i prod-miljÃ¸et pÃ¥ Dapla kan man gjÃ¸re fÃ¸lgende:\n\nGÃ¥ inn pÃ¥ lenken https://jupyter.dapla.ssb.no/ i en Chrome-nettleser eller klikk pÃ¥ lenken pÃ¥ ByrÃ¥nettet som vist i FigurÂ 1.\nAlle i SSB har en Google Cloud-konto som mÃ¥ brukes nÃ¥r man logger seg pÃ¥ Dapla. Brukernavnet i Google er det samme som din korte epost-adresse (f.eks. cth@ssb.no). Hvis du ikke allerede er logget inn i Google vil du fÃ¥ spÃ¸rsmÃ¥l om Ã¥ velge hvilken Google-konto som skal brukes (FigurÂ 3). Logg inn med din Google-konto (ssb.no) og ditt AD-passord.\n\n\n\n\n\n\n\nFigurÂ 3: Velg en Google-konto\n\n\n\n\nDeretter blir man spurt om man godtar at ssb.no (altsÃ¥ Dapla) kan bruke din Google Cloud-konto (FigurÂ 4). Trykk Allow.\n\n\n\n\n\n\n\nFigurÂ 4: Tillat at ssb.no fÃ¥r bruke din Google Cloud-konto\n\n\n\n\nDeretter lander man pÃ¥ en side som lar deg avgjÃ¸re hvor mye maskinkraft som skal holdes av til deg (FigurÂ 5). Det Ã¸verste alternativet er valgt som standard, og er tilstrekkelig for de fleste.\n\n\n\n\n\n\n\nFigurÂ 5: Velg hvor mye maskinkraft du trenger\n\n\n\n\nVent til maskinen din starter opp (FigurÂ 6). Oppstartstiden kan variere.\n\n\n\n\n\n\n\nFigurÂ 6: Starter opp Jupyter\n\n\n\nEtter dette er man logget inn i et Jupyter-miljÃ¸ som kjÃ¸rer pÃ¥ en minimal Ubuntu-maskin. Hvis man er del av et Dapla-team fÃ¥r man ogsÃ¥ tilgang til alt teamet har tilgang til.\n\n\n\nInnlogging til staging-miljÃ¸et er identisk med innloggingen til prod-miljÃ¸et, med ett viktig unntak: nettadressen er nÃ¥ https://jupyter.dapla-staging.ssb.no/.\nLitt mer om hva som er tilgjenglig her kommer.\n\n\n\n\n\nNoen ganger kan man komme i en situasjon hvor lÃ¸sningen for Single Sign-On (pÃ¥logging pÃ¥ tvers av flere systemer) gir en feilmelding a la FigurÂ 7:\n\n\n\n\n\n\nFigurÂ 7: Feil som kan oppstÃ¥ ved pÃ¥logging\n\n\n\nI denne situasjonen mÃ¥ man trykke pÃ¥ knappen â€œAdd to existing accountâ€. Da vil skjermbildet FigurÂ 8 dukke opp:\n\n\n\n\n\n\nFigurÂ 8: Klikk pÃ¥ Google-knappen for Ã¥ logge pÃ¥ igjen\n\n\n\nHer mÃ¥ man tykke pÃ¥ Google-knappen (se pil), og deretter logge inn som vist i FigurÂ 3 tidligere i dette avsnittet.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Innlogging"
    ]
  },
  {
    "objectID": "statistikkere/innlogging.html#bakkemiljÃ¸et",
    "href": "statistikkere/innlogging.html#bakkemiljÃ¸et",
    "title": "Innlogging",
    "section": "",
    "text": "Jupyter-miljÃ¸et pÃ¥ bakken bruker samme base-image1 for Ã¥ installere Jupyterlab, og er derfor identisk pÃ¥ mange mÃ¥ter. Men innloggingen er ganske forskjellig.\n\n\n\n\n\n\nNote\n\n\n\nFom. 5. desember 2022 har vi byttet ut Jupyter-miljÃ¸et pÃ¥ bakken. Beskrivelsene under gjelder derfor det nye miljÃ¸et. Fram til 15. januar vil du kunne bruke det gamle miljÃ¸et ved Ã¥ gÃ¥ inn pÃ¥ lenken https://jupyter-prod.ssb.no/ manuelt i Google Chrome. Etter 15. januar blir det gamle Jupyter-miljÃ¸et avviklet.\n\n\n\n\nDu logger deg inn pÃ¥ prod i bakkemiljÃ¸et pÃ¥ fÃ¸lgende mÃ¥te:\n\nLogg deg inn pÃ¥ Citrix-Windows i bakkemiljÃ¸et. Det kan gjÃ¸res ved Ã¥ bruke lenken Citrix pÃ¥ ByrÃ¥nettet, som ogsÃ¥ vises i FigurÂ 1.\nTrykk pÃ¥ Jupyterlab-ikonet, som vist pÃ¥ FigurÂ 9, og logg deg inn med vanlig brukernavn og passord.\n\n\n\n\n\n\n\nFigurÂ 9: Jupyterlab-ikon pÃ¥ Skrivebordet i Citrix-Windows.\n\n\n\nNÃ¥r du trykker pÃ¥ ikonet blir du tatt til nettadressen https://sl-jupyter-p.ssb.no/. Du kunne ogsÃ¥ Ã¥pnet Jupyterlab ved Ã¥pne Chrome-nettleseren og skrive inn adressen manuelt.\n\n\n\nInnlogging til staging-miljÃ¸et har ingen snarvei pÃ¥ Skrivebordet, og du mÃ¥ gjÃ¸re fÃ¸lgende for Ã¥ Ã¥pne miljÃ¸et:\n\nÃ…pne Chrome-nettleseren i Citrix-Windows.\nSkriv inn url-en https://sl-jupyter-t.ssb.no/",
    "crumbs": [
      "Manual",
      "Appendix",
      "Innlogging"
    ]
  },
  {
    "objectID": "statistikkere/innlogging.html#footnotes",
    "href": "statistikkere/innlogging.html#footnotes",
    "title": "Innlogging",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nHva er base-image?â†©ï¸",
    "crumbs": [
      "Manual",
      "Appendix",
      "Innlogging"
    ]
  },
  {
    "objectID": "statistikkere/altinn-dapla-suv-tools.html",
    "href": "statistikkere/altinn-dapla-suv-tools.html",
    "title": "dapla-suv-tools",
    "section": "",
    "text": "dapla-suv-tools er en python pakke med en samling verktÃ¸y for integrering med SUV-plattformen. Pakken tilbyr verktÃ¸y for skjema administrasjon, bygging av prefill og utsending av skjema pÃ¥ Altinn 3 plattformen.\nDokumentasjon av pakken ligger her. Denne gir en teknisk innfÃ¸ring som du kan fÃ¸lge og kopiere kode fra. Noe demokode ligger ogsÃ¥ i repoet og kan vÃ¦re ett godt utgangspunkt.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-suv-tools"
    ]
  },
  {
    "objectID": "statistikkere/altinn-dapla-suv-tools.html#installasjon",
    "href": "statistikkere/altinn-dapla-suv-tools.html#installasjon",
    "title": "dapla-suv-tools",
    "section": "Installasjon",
    "text": "Installasjon\n\nPip\n\n\nTerminal\n\npip install dapla-suv-tools\n\n\n\nPoetry\n\n\nTerminal\n\npoetry add dapla-suv-tools",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-suv-tools"
    ]
  },
  {
    "objectID": "statistikkere/altinn-dapla-suv-tools.html#opprette-klient",
    "href": "statistikkere/altinn-dapla-suv-tools.html#opprette-klient",
    "title": "dapla-suv-tools",
    "section": "Opprette klient",
    "text": "Opprette klient\nFor Ã¥ kunne bruke pakken mÃ¥ du importere klienten:\n\n\nnotebook\n\nfrom dapla_suv_tools.suv_client import SuvClient\n\nclient = SuvClient()",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-suv-tools"
    ]
  },
  {
    "objectID": "statistikkere/altinn-dapla-suv-tools.html#paginering",
    "href": "statistikkere/altinn-dapla-suv-tools.html#paginering",
    "title": "dapla-suv-tools",
    "section": "Paginering",
    "text": "Paginering\nFor Ã¥ sette opp paginering mÃ¥ du importere PaginationInfo\n\n\nnotebook\n\nfrom dapla_suv_tools.pagination import PaginationInfo\n\nPaginering brukes for Ã¥ hente data i mindre deler spesielt nÃ¥r et datasett er stort. Dette bidrar til Ã¥ redusere belastningen bÃ¥de pÃ¥ klient og server, og gir bedre ytelse. Side- og stÃ¸rrelsesparametere sendes som en del av forespÃ¸rselen.\n\n\n\n\n\n\nMaksimal stÃ¸rrelse\n\n\n\nMaksimal tillatt stÃ¸rrelse per side er 100 i alle forespÃ¸rsler. Hvis man angir en hÃ¸yere verdi vil forespÃ¸rselen feile eller bli begrenset til 100 poster per side.\n\n\n\nEnkel bruk av paginering\nI dette eksempelet brukes paginering for Ã¥ hente en spesifikk side med et gitt antall elementer.\n\n\nnotebook\n\np_info = PaginationInfo(page=1, size=5)\n\nresult = client.get_skjema_by_ra_nummer(\n    ra_nummer=\"RA-0666A3\", pagination_info=p_info\n)\n\nHer hentes den fÃ¸rste siden (page=1) med 5 elementer per side (size=5).\n\n\nHente alle data med paginering\nDette eksempelet viser hvordan man kan hente alle data ved Ã¥ iterere gjennom flere sider.\n\n\nnotebook\n\npage = 1\nsize = 100\nall_records = []\n\nwhile True:\n    p_info = PaginationInfo(page=page, size=size)\n    \n    response = client.get_utvalg_from_sfu(\n        delreg_nr=49430324,\n        ra_nummer='RA-0666A3',\n        pagination_info=p_info\n    )\n    \n    records = response\n    all_records.extend(records)\n    \n    if len(records) &lt; size:\n        break\n    \n    page += 1\n\nprint(f\"Totalt antall poster: {len(all_records)}\")\n)\n\nDette sikrer at alle poster hentes nÃ¥r datasettet gÃ¥r over flere sider.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-suv-tools"
    ]
  },
  {
    "objectID": "statistikkere/dashboard.html",
    "href": "statistikkere/dashboard.html",
    "title": "Dash og dashboard",
    "section": "",
    "text": "Hvis en Ã¸nsker Ã¥ lage ett dashbord som et brukergrensesnitt, sÃ¥ kan pakken Dash vÃ¦re et godt alternativ. Dash er ett rammeverk hvor man selv kan bygge opp applikasjoner i form av dashbord pÃ¥ en enklere mÃ¥te, og det bygges oppÃ¥ javascript pakker som plotly.js og react.js. Det er et produkt ved siden av og helintegrert med plotly, som ogsÃ¥ er en annen pakke i Python som gir oss interaktive grafer. Dash er et godt verktÃ¸y hvis en Ã¸nsker et dashbord som brukergrensesnitt for interaktiv visualisering av data. Dash kan kodes i Python og R, men ogsÃ¥ Julia og F#.\nI SSB kan man lage dashbord i virtuelle miljÃ¸er satt opp med ssb-project. For mer om hÃ¥ndtering av pakker i ett virtuelt miljÃ¸ satt opp med ssb-project kan man se nÃ¦rmere her.",
    "crumbs": [
      "Manual",
      "Appendix"
    ]
  },
  {
    "objectID": "statistikkere/dashboard.html#eksempel-kode-i-daplalab",
    "href": "statistikkere/dashboard.html#eksempel-kode-i-daplalab",
    "title": "Dash og dashboard",
    "section": "Eksempel kode i DaplaLab",
    "text": "Eksempel kode i DaplaLab\nI DaplaLab kan du starte opp ett dashbord ved hjelp av dash pakken enten i vscode-python tjenesten, eller i en notebook i jupyter tjenesten. Det fungerer best Ã¥ kjÃ¸re Dash-apper i en egen fane i nettleseren.\n\nvscode-python scriptjupyter notebookjupyter script\n\n\nHer er et eksempel pÃ¥ hvordan man lager en Dash-app i DaplaLab i en vscode tjeneste:\n\nInstaller pandas og dash i et ssb-project.\nOpprett et python-script i prosjektet som f.eks. heter app.py.\nKjÃ¸r scriptet ved Ã¥ kjÃ¸re fÃ¸lgende kommando fra terminalen: poetry run python ./app.py\nDeretter kommer det opp et dialog-vinduet hvor du velger Open in browser.\n\nHer er et eksempel pÃ¥ script som fungerer i Vscode-python:\n\n\napp.py\n\nfrom dash import Dash, html, dcc, callback, Output, Input\nimport plotly.express as px\nimport pandas as pd\n\n# Creating a sample dataset similar to the Gapminder dataset\ndata = {\n    'country': ['Afghanistan', 'Afghanistan', 'Afghanistan', 'Brazil', 'Brazil', 'Brazil', 'China', 'China', 'China'],\n    'continent': ['Asia', 'Asia', 'Asia', 'South America', 'South America', 'South America', 'Asia', 'Asia', 'Asia'],\n    'year': [1952, 1977, 2007, 1952, 1977, 2007, 1952, 1977, 2007],\n    'lifeExp': [28.801, 39.877, 43.828, 50.917, 59.504, 72.390, 44.000, 63.739, 72.961],\n    'pop': [8425333, 12412311, 31889923, 56602560, 80461570, 190010647, 556263527, 862030000, 1318683096],\n    'gdpPercap': [779.4453145, 786.11336, 974.5803384, 2108.944355, 5718.766744, 9065.800825, 400.448611, 1488.041713, 4959.114854]\n}\n\n# Creating DataFrame\ndf = pd.DataFrame(data)\n\n# Creating the app\nport = 8050 # must be between 1024 and 9999, default is 8050\napp = Dash(\n    __name__,\n    requests_pathname_prefix=f'/proxy/{port}/', \n    serve_locally=True\n)\n\napp.layout = [\n    html.H1(children='Title of Dash App', style={'textAlign':'center'}),\n    dcc.Dropdown(df.country.unique(), 'Canada', id='dropdown-selection'),\n    dcc.Graph(id='graph-content')\n]\n\n@callback(\n    Output('graph-content', 'figure'),\n    Input('dropdown-selection', 'value')\n)\ndef update_graph(value):\n    dff = df[df.country==value]\n    return px.line(dff, x='year', y='pop')\n\nif __name__ == '__main__':\n    app.run(port=port, debug=True)\n\nFor Ã¥ stoppe dashbordet fra Ã¥ kjÃ¸re, trykker du i terminalen ctrl + c.\n\n\nHer er et eksempel pÃ¥ hvordan man lager en Dash-app i DaplaLab i en jupyter tjeneste:\n\nInstaller pandas og dash i et ssb-project.\nOpprett en notebook i prosjektet som f.eks. heter app.ipynb.\nÃ…pne notebooken og kjÃ¸r kodecellene pÃ¥ vanlig mÃ¥te.\n\nHer er et eksempel pÃ¥ kode i notebook som fungerer i jupyter:\n\n\napp.ipynb\n\n# %%\n# Notebook cell 1\nfrom dash import Dash, html, dcc, callback, Output, Input\nimport plotly.express as px\nimport pandas as pd\nimport os\n\n# %%\n# Notebook cell 2\n# Creating a sample dataset similar to the Gapminder dataset\ndata = {\n    'country': ['Afghanistan', 'Afghanistan', 'Afghanistan', 'Brazil', 'Brazil', 'Brazil', 'China', 'China', 'China'],\n    'continent': ['Asia', 'Asia', 'Asia', 'South America', 'South America', 'South America', 'Asia', 'Asia', 'Asia'],\n    'year': [1952, 1977, 2007, 1952, 1977, 2007, 1952, 1977, 2007],\n    'lifeExp': [28.801, 39.877, 43.828, 50.917, 59.504, 72.390, 44.000, 63.739, 72.961],\n    'pop': [8425333, 12412311, 31889923, 56602560, 80461570, 190010647, 556263527, 862030000, 1318683096],\n    'gdpPercap': [779.4453145, 786.11336, 974.5803384, 2108.944355, 5718.766744, 9065.800825, 400.448611, 1488.041713, 4959.114854]\n}\n\n# Creating DataFrame\ndf = pd.DataFrame(data)\n\n# Creating the app\nport = 8050 # must be between 1024 and 9999\nservice_prefix = os.getenv('JUPYTERHUB_SERVICE_PREFIX', '/')\ndomain = os.getenv('JUPYTERHUB_HTTP_REFERER', None)\n\napp = Dash(\n    __name__,\n    requests_pathname_prefix=f\"{service_prefix}proxy/{port}/\", \n    serve_locally=True\n)\n\napp.layout = [\n    html.H1(children='Title of Dash App', style={'textAlign':'center'}),\n    dcc.Dropdown(df.country.unique(), 'Canada', id='dropdown-selection'),\n    dcc.Graph(id='graph-content')\n]\n\n@callback(\n    Output('graph-content', 'figure'),\n    Input('dropdown-selection', 'value')\n)\ndef update_graph(value):\n    dff = df[df.country==value]\n    return px.line(dff, x='year', y='pop')\n\nif __name__ == '__main__':\n    # For jupyter_mode, choose between 'external' or 'inline'.\n    # 'jupyterlab' should also be poosible, but doesn't seem to work...\n    app.run(debug=True, jupyter_mode=\"external\", jupyter_server_url=domain, port=port)\n\nFor Ã¥ stoppe dashbordet fra Ã¥ kjÃ¸re, restarter du kernelen i jupyterlab: Kernel -&gt; Restart Kernel and Clear Outputs of All Cells...\n\n\nHer er et eksempel pÃ¥ hvordan man lager en Dash-app i DaplaLab i en jupyter tjeneste:\n\nInstaller pandas og dash i et ssb-project.\nOpprett et python-script i prosjektet som f.eks. heter app.py.\nKjÃ¸r scriptet ved Ã¥ kjÃ¸re fÃ¸lgende kommando fra terminalen: poetry run python app.py\nDeretter dukker det opp en link i terminalen etter teksten â€˜Dash is running onâ€™ som du kan trykke pÃ¥ for Ã¥ fÃ¥ opp dashbordet.\n\nHer er et eksempel pÃ¥ script som fungerer i jupyter:\n\n\napp.py\n\nfrom dash import Dash, html, dcc, callback, Output, Input\nimport plotly.express as px\nimport pandas as pd\nimport os\n\n# Creating a sample dataset similar to the Gapminder dataset\ndata = {\n    'country': ['Afghanistan', 'Afghanistan', 'Afghanistan', 'Brazil', 'Brazil', 'Brazil', 'China', 'China', 'China'],\n    'continent': ['Asia', 'Asia', 'Asia', 'South America', 'South America', 'South America', 'Asia', 'Asia', 'Asia'],\n    'year': [1952, 1977, 2007, 1952, 1977, 2007, 1952, 1977, 2007],\n    'lifeExp': [28.801, 39.877, 43.828, 50.917, 59.504, 72.390, 44.000, 63.739, 72.961],\n    'pop': [8425333, 12412311, 31889923, 56602560, 80461570, 190010647, 556263527, 862030000, 1318683096],\n    'gdpPercap': [779.4453145, 786.11336, 974.5803384, 2108.944355, 5718.766744, 9065.800825, 400.448611, 1488.041713, 4959.114854]\n}\n\n# Creating DataFrame\ndf = pd.DataFrame(data)\n\n# Creating the app\nport = 8050 # must be between 1024 and 9999, default is 8050\nservice_prefix = os.getenv('JUPYTERHUB_SERVICE_PREFIX', '/')\ndomain = os.getenv('JUPYTERHUB_HTTP_REFERER', None)\nservice = f\"{service_prefix}proxy/{port}/\"\nurl = f\"{domain}{service[1:]}\"\ndefault_host = f\"http://127.0.0.1:{port}{service}\"\n\napp = Dash(\n    __name__,\n    requests_pathname_prefix=service,\n    serve_locally=True\n)\n\napp.layout = [\n    html.H1(children='Title of Dash App', style={'textAlign':'center'}),\n    dcc.Dropdown(df.country.unique(), 'Canada', id='dropdown-selection'),\n    dcc.Graph(id='graph-content')\n]\n\n@callback(\n    Output('graph-content', 'figure'),\n    Input('dropdown-selection', 'value')\n)\ndef update_graph(value):\n    dff = df[df.country==value]\n    return px.line(dff, x='year', y='pop')\n\nif __name__ == '__main__':\n    app.run(proxy = default_host + \"::\" + url, port=port, debug=True)\n\nFor Ã¥ stoppe dashbordet fra Ã¥ kjÃ¸re, trykker du i terminalen ctrl + c.",
    "crumbs": [
      "Manual",
      "Appendix"
    ]
  },
  {
    "objectID": "statistikkere/dashboard.html#aktuell-dokumentasjon",
    "href": "statistikkere/dashboard.html#aktuell-dokumentasjon",
    "title": "Dash og dashboard",
    "section": "Aktuell dokumentasjon",
    "text": "Aktuell dokumentasjon\nDiverse som er verdt Ã¥ se nÃ¦rmere pÃ¥ nÃ¥r en bygger dashbord applikasjon med Dash. Det fÃ¸lger med mange gode eksempler for bruk av diverse komponenter i dokumentasjonene under her.\n\nStartsiden til Dash\nLage interaktive grafer i Python med Plotly\nDash Core Components\nDash HTML Components\nDash Bootstrap Components\n\nVel og merke henter denne diverse materialer fra internett, og vil ikke fungere uten tilgang. Det er heller anbefalt Ã¥ ha de nÃ¸dvendige filene lagret lokalt for bruk av denne pakken.\nPakken i seg selv har en fordel i at det er lettere Ã¥ bygge opp utseende(layout) i dashbordet ditt, samt andre komponenter som ikke ligger i standard dash pakken.\n\nDash SSB Components\n\nTeam Metadata lager SSB komponenter i Dash, noe Datadoc er lagd med. Dette gir deg muligheten til Ã¥ bruke SSB komponentene i dine egne dashbord. Vel og merke er denne pakken fortsatt under utvikling, og ikke alle komponenter er pÃ¥ plass.",
    "crumbs": [
      "Manual",
      "Appendix"
    ]
  },
  {
    "objectID": "statistikkere/altinn-skjema-administrasjon.html",
    "href": "statistikkere/altinn-skjema-administrasjon.html",
    "title": "Administrasjon av skjema",
    "section": "",
    "text": "SU-V tilbyr bÃ¥de et grafisk brukergrensesnitt (GUI) og en Python-pakke for administrasjon av skjemaer. Du kan fÃ¥ tilgang til GUI via fÃ¸lgende lenker:\nMer informasjon om Python-pakken finner du her.\nAlle operasjoner og visninger som er tilgjengelige i GUI, kan ogsÃ¥ utfÃ¸res ved hjelp av kode. Administrasjon av skjema innebÃ¦rer hÃ¥ndtering av metadata knyttet til skjemaer, perioder, puljer og utsendinger.",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Administrasjon av skjema"
    ]
  },
  {
    "objectID": "statistikkere/altinn-skjema-administrasjon.html#tilgang-til-skjema",
    "href": "statistikkere/altinn-skjema-administrasjon.html#tilgang-til-skjema",
    "title": "Administrasjon av skjema",
    "section": "Tilgang til skjema",
    "text": "Tilgang til skjema\nAlle brukere har lesetilgang til skjemaene, noe som betyr at de kan se og hente data uten Ã¥ gjÃ¸re endringer.\nFor Ã¥ kunne utfÃ¸re administrasjonsoppgaver og gjÃ¸re endringer knyttet til skjemaet, mÃ¥ du vÃ¦re medlem av data-admins eller developers i Dapla-teamet som eier skjemaet. Se mer om administrasjon av team her.",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Administrasjon av skjema"
    ]
  },
  {
    "objectID": "statistikkere/altinn-skjema-administrasjon.html#hierarki-for-skjema-administrasjon",
    "href": "statistikkere/altinn-skjema-administrasjon.html#hierarki-for-skjema-administrasjon",
    "title": "Administrasjon av skjema",
    "section": "Hierarki for skjema administrasjon",
    "text": "Hierarki for skjema administrasjon\nEt skjema bestÃ¥r av fÃ¸lgende strukturer:\n\nSkjema kan inneholde flere perioder.\nEn periode kan inneholde flere puljer.\nEn pulje kan inneholde flere utsendinger.\n\nIllustrasjonen nedenfor viser sammenhengen mellom de ulike nivÃ¥ene:\n\n\n\n\n\n\nFigurÂ 1: Avhengigheter skjema administrasjon",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Administrasjon av skjema"
    ]
  },
  {
    "objectID": "statistikkere/altinn-skjema-administrasjon.html#generelle-ui-operasjoner",
    "href": "statistikkere/altinn-skjema-administrasjon.html#generelle-ui-operasjoner",
    "title": "Administrasjon av skjema",
    "section": "Generelle UI operasjoner",
    "text": "Generelle UI operasjoner\nMange av skjermbildene i GUI tilbyr de samme typene operasjoner. Disse operasjonene er beskrevet i tabellen nedenfor.\nTabellÂ 1 beskriver generelle UI operasjoner.\n\n\n\nTabellÂ 1: UI operasjoner\n\n\n\n\n\n\n\n\n\nIkon\nForklaring\n\n\n\n\n\nLegg til en ny rad.\n\n\n\nRedigere en rad.\n\n\n\nKopiere en rad.\n\n\n\nSlett en rad.\n\n\n\nVise detaljer for en rad.\n\n\n\nGÃ¥ tilbake til forrige skjermbilde.",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Administrasjon av skjema"
    ]
  },
  {
    "objectID": "statistikkere/altinn-skjema-administrasjon.html#skjema",
    "href": "statistikkere/altinn-skjema-administrasjon.html#skjema",
    "title": "Administrasjon av skjema",
    "section": "Skjema",
    "text": "Skjema\nFor Ã¥ begynne Ã¥ jobbe med et skjema, mÃ¥ du finne riktig skjemaversjon i skjemakatalogen.\nSlik sÃ¸ker du fram et skjema\n\nVelg Skjemadata i venstremenyen.\nBruk sÃ¸kefeltet i det nye skjermbildet for Ã¥ finne skjemaet ditt.\nKlikk pÃ¥ Vis detaljer-ikonet i sÃ¸keresultatet for Ã¥ se mer informasjon om skjemaet.\n\nBildet nedenfor viser eksempel pÃ¥ sÃ¸kebilde:\n\n\n\n\n\n\nFigurÂ 2: SÃ¸ke fram skjema\n\n\n\n\n\n\n\n\n\nHva hvis skjema mangler?\n\n\n\nHvis du ikke finner riktig skjemaversjon, ta kontakt med planleggeren for skjemaet pÃ¥ seksjon 821.\n\n\n\nSkjema metadata\nHvert skjema har tilknyttet metadata som gir detaljert informasjon om det. Bildet nedenfor viser et eksempel pÃ¥ metadata for et skjema: \n\n\n\n\n\n\nBeskrivelse av skjema metadata\n\n\n\n\n\nTabellÂ 2 beskriver metadatafeltene for et skjema\n\n\n\nTabellÂ 2: Skjema metadata\n\n\n\n\n\n\n\n\n\nMeta\nForklaring\n\n\n\n\nRA-nummer\nRA-nummer for skjemaet.\n\n\nVersjon\nSkjemaversjon.\n\n\nUnd.nr\nUndersÃ¸kelsesnummer.\n\n\nDatamodell\nNavn pÃ¥ datamodellen.\n\n\nNavn\nNavn pÃ¥ undersÃ¸kelsen.\n\n\nNavn nynorsk\nNavn pÃ¥ undersÃ¸kelsen (nynorsk).\n\n\nNavn engelsk\nNavn pÃ¥ undersÃ¸kelsen (engelsk).\n\n\nEier\nDapla-team.\n\n\nGyldig fra\nFra hvilken dato undersÃ¸kelsen er gyldig.\n\n\nGyldig til\nTil hvilken dato undersÃ¸kelsen er gyldig.\n\n\nURL infoside\nInformasjonsside for undersÃ¸kelsen.\n\n\nBeskrivelse\nTilleggsinformasjon om undersÃ¸kelsen.\n\n\n\n\n\n\n\n\n\n\n\nKodeeksempel\nFor Ã¥ hente ut et skjema med tilhÃ¸rende metadata i Python, kan du bruke metoden get_skjema_by_ra_nummer i SuvClient. SÃ¸rg for at du oppgir riktig RA-nummer og versjon.\n\n\nnotebook\n\nclient = SuvClient()\n\noutput = client.get_skjema_by_ra_nummer(\n            ra_nummer = \"RA-0678A3\", \n            versjon = 2 \n        )\n        \nprint(json.dumps(output, indent=4))\n\n\n\nLegge til skjema som favoritt\nFor Ã¥ gjÃ¸re det enklere Ã¥ finne skjemaene du jobber mest med, kan du legge dem til som favoritter. Dette er spesielt nyttig ettersom skjemakatalogen kan inneholde mange ulike skjemaer og versjoner.\nSlik legger du til et skjema som favoritt:\n\nSÃ¸k opp skjemaet: Bruk sÃ¸kefunksjonen for Ã¥ finne skjemaet du Ã¸nsker.\nVis detaljer: Klikk pÃ¥ skjemaet i tabellen for Ã¥ Ã¥pne detaljvisningen.\nLegg til som favoritt: Trykk pÃ¥ stjerneikonet ved siden av skjemaets navn. NÃ¥r stjernen er markert, er skjemaet lagt til som favoritt.\n\nSkjemaet vil deretter vÃ¦re tilgjengelig under Favoritter i venstremenyen.\nBildet nedenfor viser hvordan du velger et skjema som favoritt:\n\n\n\n\n\n\nFigurÂ 3: Legge til skjema som favoritt",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Administrasjon av skjema"
    ]
  },
  {
    "objectID": "statistikkere/altinn-skjema-administrasjon.html#periode",
    "href": "statistikkere/altinn-skjema-administrasjon.html#periode",
    "title": "Administrasjon av skjema",
    "section": "Periode",
    "text": "Periode\nEt skjema kan ha en eller flere perioder knyttet til seg. Hvilke perioder som er lagt inn pÃ¥ skjemaet kan du se i detaljvisningen nÃ¥r du har valgt et skjema.\nAdministrasjon av utvalg og enheter Administrasjon av utvalg og enheter skjer fortsatt fra SFU pÃ¥ bakke. Knytningen mellom skjemaet og bakkesystemene skjer ved at et delregister-nr registreres pÃ¥ perioden. Mer informasjon om hÃ¥ndtering av utvalg fra SFU finner du her.\nEksempel pÃ¥ skjema med periode Bildet nedenfor viser eksempel pÃ¥ et skjema med en tilknyttet periode:\n\n\n\n\n\n\nFigurÂ 4: Skjema periode\n\n\n\n\n\n\n\n\n\nBeskrivelse av periode metadata\n\n\n\n\n\nTabellÂ 3 beskriver metadatafeltene for en periode\n\n\n\nTabellÂ 3: Periode metadata\n\n\n\n\n\n\n\n\n\nMeta\nForklaring\n\n\n\n\nType\nUndersÃ¸kelsestype (f.eks Kvartal, MÃ¥ned, Ã…r).\n\n\nNr\nPeriode nummer.\n\n\nÃ…r\nPeriode Ã¥r.\n\n\nPeriode-dato\n\n\n\nDelreg-nr\nDelregister-nummer i SFU som er tilknyttet perioden.\n\n\nEnhet-type\nEnhetstype (f.eks Bedrift, Foretak, Person).\n\n\nOppgavebyrde\nAktivere oppgavebyrde i skjemaet.\n\n\nBrukeropplevelse\nAktivere oppgavebyrde i skjemaet.\n\n\nSkjemadata\n\n\n\nJournalnummer\n\n\n\n\n\n\n\n\n\n\n\nKodeeksempel\nFor Ã¥ lage en ny periode, kan du bruke metoden create_periode i SuvClient.\n\n\nnotebook\n\nclient = SuvClient()\n\noutput = client.create_periode(\n            skjema_id = 142,\n            periode_type = 'KVRT',\n            periode_nr = 3,\n            periode_aar = 2024,\n            delreg_nr = 21130324,\n            enhet_type = 'BEDR'\n        )\n\nprint(output)\n\nFor Ã¥ hente ut perioder med tilhÃ¸rende metadata i Python, kan du bruke metoden get_perioder_by_skjema_id i SuvClient. SÃ¸rg for at du oppgir riktig skjema_id.\n\n\nnotebook\n\nclient = SuvClient()\n\noutput = client.get_perioder_by_skjema_id(\n            skjema_id=142\n        )\n\nprint(json.dumps(output, indent=4))",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Administrasjon av skjema"
    ]
  },
  {
    "objectID": "statistikkere/altinn-skjema-administrasjon.html#pulje",
    "href": "statistikkere/altinn-skjema-administrasjon.html#pulje",
    "title": "Administrasjon av skjema",
    "section": "Pulje",
    "text": "Pulje\nEn periode kan ha en eller flere puljer knyttet til seg. Hvilke puljer som er lagt inn pÃ¥ skjemaet kan du se i detaljvisningen nÃ¥r du har valgt en periode.\nEksempel pÃ¥ periode med pulje Bildet nedenfor viser eksempel pÃ¥ en periode med en tilknyttet pulje:\n\n\n\n\n\n\nFigurÂ 5: Periode pulje\n\n\n\n\n\n\n\n\n\nBeskrivelse av pulje metadata\n\n\n\n\n\nTabellÂ 4 beskriver metadatafeltene for en pulje\n\n\n\nTabellÂ 4: Pulje metadata\n\n\n\n\n\n\n\n\n\nMeta\nForklaring\n\n\n\n\nPulje\nPulje nummer.\n\n\nTilgjengelig fra\nAngir fra nÃ¥r skjemaet er tilgjengelig for oppdragsgiver ute hos Altinn.\n\n\nSvarfrist\nAngir svarfrist for undersÃ¸kelsen.\n\n\nTvmulkt svarfrist\nAngir svarfrist fÃ¸r utsendelse av tvangsmulkt.\n\n\nSend SI\n\n\n\n\n\n\n\n\n\n\n\nKodeeksempel\nFor Ã¥ lage en ny pulje, kan du bruke metoden create_pulje i SuvClient.\n\n\nnotebook\n\nclient = SuvClient()\n\n_altinn_tilgjengelig = datetime(2024, 11, 20, 14, 30, 0)\n_altinn_svarfrist = datetime(2024, 11, 21)\n_tvangsmulkt_svarfrist_ = datetime(2024, 11, 22)\n_send_si = datetime(2024, 11, 23)\n\nclient.create_pulje(\n    periode_id = 24,\n    pulje_nr = 1,\n    altinn_tilgjengelig=_altinn_tilgjengelig,\n    altinn_svarfrist = _altinn_svarfrist,\n    tvangsmulkt_svarfris t= _tvangsmulkt_svarfrist_,\n    send_si = _send_si\n)\n\nprint(output)\n\nFor Ã¥ hente ut pulje med tilhÃ¸rende metadata i Python, kan du bruke metoden get_pulje_by_periode_id i SuvClient. SÃ¸rg for at du oppgir riktig periode_id.\n\n\nnotebook\n\nclient = SuvClient()\n\noutput = client.get_pulje_by_periode_id(\n            periode_id = 99\n        )\n\nprint(output)",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Administrasjon av skjema"
    ]
  },
  {
    "objectID": "statistikkere/altinn-skjema-administrasjon.html#utsending",
    "href": "statistikkere/altinn-skjema-administrasjon.html#utsending",
    "title": "Administrasjon av skjema",
    "section": "Utsending",
    "text": "Utsending\nEn pulje kan ha en eller flere utsendinger knyttet til seg. Hvilke utsendinger som er lagt inn pÃ¥ puljen kan du se i detaljvisningen nÃ¥r du har valgt en pulje.\nEksempel pÃ¥ pulje med utsending Bildet nedenfor viser eksempel pÃ¥ en pulje med en tilknyttet utsending:\n\n\n\n\n\n\nFigurÂ 6: Pulje utsending\n\n\n\n\n\n\n\n\n\nBeskrivelse av utsending metadata\n\n\n\n\n\nTabellÂ 5 beskriver metadatafeltene for en utsending\n\n\n\nTabellÂ 5: Pulje metadata\n\n\n\n\n\n\n\n\n\nMeta\nForklaring\n\n\n\n\nUtsendingstype\nAngir type utsending (f.eks instansiering).\n\n\nTrigger\nAngir trigger for utsending (Manuell, Auto, Ekstern).\n\n\nTest\nAngir testutsending (sendes ikke til utvalg)\n\n\nSend ut\nAngir tidspunkt for utsendelse.\n\n\n\n\n\n\n\n\n\n\nSend nÃ¥\nÃ˜nsker du Ã¥ gjÃ¸re en utsending umiddelbart mÃ¥ dette gjÃ¸res fra GUI.\nSlik gjÃ¸r du en utsending umiddelbart:\n\nVelg Skjemadata i venstremenyen.\nBruk sÃ¸kefeltet i det nye skjermbildet for Ã¥ finne skjemaet ditt.\nKlikk pÃ¥ Vis detaljer-ikonet i sÃ¸keresultatet for Ã¥ se mer informasjon om skjemaet.\nFinn Ã¸nsket periode og velg Vis detaljer-ikonet i sÃ¸keresultatet for Ã¥ se mer informasjon om perioden.\nFinn Ã¸nsket utsending og velg Vis detaljer-ikonet i sÃ¸keresultatet for Ã¥ se mer informasjon om utsendingen (eventuelt sÃ¥ kan du opprette en helt ny utsending).\nTrykk â€œSend nÃ¥â€.\n\n\n\n\n\n\n\nFigurÂ 7: Send nÃ¥\n\n\n\n\n\n\n\n\n\nUtsending til utvalgt(e) enhet(er)\n\n\n\nFor Ã¥ sende til Ã©n eller flere spesifikke enheter, kan du oppgi organisasjonsnumrene som en kommaseparert liste. Eksempel: 123456789, 987654321. Dersom ingen enheter oppgis, vil utsendingen automatisk gjelde for hele puljen.\n\n\n\n\nKodeeksempel\nFor Ã¥ lage en ny utsending, kan du bruke metoden create_utsending i SuvClient.\n\n\nnotebook\n\nclient = SuvClient()\n\n_altinn_uts_tidspunkt = datetime(2024, 11, 20, 14, 30, 0)\n\noutput = client.create_utsending(\n            pulje_id = 75,\n            utsendingstype_navn = 'instansiering',\n            altinn_uts_tidspunkt = _altinn_uts_tidspunkt\n        )\n        \nprint(output)\n\nFor Ã¥ hente ut utsending med tilhÃ¸rende metadata i Python, kan du bruke metoden get_utsending_by_id i SuvClient. SÃ¸rg for at du oppgir riktig utsending_id.\n\n\nnotebook\n\nclient = SuvClient()\n\noutput = client.client.get_utsending_by_id(\n            utsending_id = 133   \n        )\n\nprint(output)",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Administrasjon av skjema"
    ]
  },
  {
    "objectID": "statistikkere/administrasjon-av-team.html",
    "href": "statistikkere/administrasjon-av-team.html",
    "title": "Administrasjon av team",
    "section": "",
    "text": "I dette kapitlet viser vi hvordan du kan opprette et nytt team eller gjÃ¸re endringer i et eksisterende team. Typiske endringer er Ã¥:",
    "crumbs": [
      "Manual",
      "Dapla-team",
      "Administrasjon av team"
    ]
  },
  {
    "objectID": "statistikkere/administrasjon-av-team.html#opprette-dapla-team",
    "href": "statistikkere/administrasjon-av-team.html#opprette-dapla-team",
    "title": "Administrasjon av team",
    "section": "Opprette Dapla-team",
    "text": "Opprette Dapla-team\nFor Ã¥ opprette et Dapla-team sÃ¥ mÃ¥ en seksjonsleder gÃ¥ inn i Teamoversikten i Dapla Ctrl og trykke pÃ¥ ikonet Opprett team. Her blir man bedt om Ã¥ fylle inn relevant informasjon.\n\n\n\n\n\n\nNote\n\n\n\nLes mer om Dapla Ctrl her.",
    "crumbs": [
      "Manual",
      "Dapla-team",
      "Administrasjon av team"
    ]
  },
  {
    "objectID": "statistikkere/administrasjon-av-team.html#legge-til-eller-fjerne-medlemmer-i-et-team",
    "href": "statistikkere/administrasjon-av-team.html#legge-til-eller-fjerne-medlemmer-i-et-team",
    "title": "Administrasjon av team",
    "section": "Legge til eller fjerne medlemmer i et team",
    "text": "Legge til eller fjerne medlemmer i et team\nFor Ã¥ legge til, fjerne eller endre medlemmer i et team mÃ¥ kan gjÃ¸res av medlemmer i managers-gruppen i teamet. Dette gjÃ¸res i Dapla Ctrl. Les mer om hvordan dette gjÃ¸res her.\n\n\n\n\n\n\nManagers i semi- eller self-managed team\n\n\n\nManagers i semi- eller self-managed teams kan ikke legge til, fjerne eller endre medlemmer fra Dapla Ctrl enda. Disse mÃ¥ forelÃ¸pig kontakte Kundeservice for Ã¥ gjÃ¸re endringer.",
    "crumbs": [
      "Manual",
      "Dapla-team",
      "Administrasjon av team"
    ]
  },
  {
    "objectID": "statistikkere/administrasjon-av-team.html#se-medlemmer-og-roller-i-et-team",
    "href": "statistikkere/administrasjon-av-team.html#se-medlemmer-og-roller-i-et-team",
    "title": "Administrasjon av team",
    "section": "Se medlemmer og roller i et team",
    "text": "Se medlemmer og roller i et team\nDapla Ctrl lar alle i SSB se hvilke team som finnes, hvem som er medlemmer og hvilke tilgangsgrupper de ligger i. Man kan ogsÃ¥ fÃ¥ oversikt over hvilke data alle team deler. Les mer her.",
    "crumbs": [
      "Manual",
      "Dapla-team",
      "Administrasjon av team"
    ]
  },
  {
    "objectID": "statistikkere/features.html",
    "href": "statistikkere/features.html",
    "title": "Features",
    "section": "",
    "text": "Under arbeid\n\n\n\nFeatures mÃ¥ forelÃ¸pig skrus pÃ¥ av plattformteamene. Ta kontakt med Kundeservice hvis du Ã¸nsker Ã¥ fÃ¥ en feature skrudd pÃ¥.\n\n\nEn feature er en GCP-tjeneste som som er satt opp og konfigurert slik at Dapla-team kan ta det i bruk pÃ¥ en enkel og selvbetjent mÃ¥te. NÃ¥r man tar i bruk en feature kan man vÃ¦re sikker pÃ¥ at sikkerhet og beste-praksis i SSB er ivaretatt. Et viktig poeng med features er at teamene selv skal kunne skru av og pÃ¥ features etter behov.\nForelÃ¸pig er det tilgjengeliggjort fÃ¸lgende features pÃ¥ Dapla:\n\ndapla-buckets\ndapla-buckets er en feature som gir deg Google Cloud Storage bÃ¸ttene som statistikkteam skal bruke for Ã¥ lagre data i Dapla. Dvs. en bÃ¸tte for kildedata, en bÃ¸tte for produkt-data, og en bÃ¸tte for delt data.\nkildomaten kildomaten er en feature som gir deg tilgang til Kildomaten. Den lar deg automatisere prosessering av data fra kildedata til inndata ved hjelp av Cloud Run.\ntransfer-service\ntransfer-service er en feature som gir deg tilgang til Ã¥ overfÃ¸re data mellom lagringstjenester i Dapla. Den lar deg overfÃ¸re data mellom bÃ¸tter, og mellom bakke- og skyplattformen i SSB. Den er bygget pÃ¥ GCP-tjenesten Google Transfer Service.\nshared-buckets\nshared-buckets er en feature som lar teamet selv opprette delt-bÃ¸tter og styre tilganger til disse.\n\n\n\n\n\n\n\n\n\nSkru pÃ¥ en feature om gangen.\n\n\n\nHvis du Ã¸nsker Ã¥ skru pÃ¥ flere features samtidig, sÃ¥ mÃ¥ du gjÃ¸re det i flere PR-er. Atlantis vil ikke klare Ã¥ hÃ¥ndtere flere features i samme PR. FÃ¸lg oppskriften under for hver feature du Ã¸nsker Ã¥ skru pÃ¥.\n\n\nAktivering av en feature for Dapla-teamet ditt er veldig enkelt. Endringen er sÃ¥ liten at vi anbefaler Ã¥ gjÃ¸re endringen direkte i GitHubs grensesnitt, uten Ã¥ klone repoet fÃ¸rst. Slik gÃ¥r du frem:\n\nSÃ¸k opp ditt teams sitt IaC-repoet under https://github.com/statisticsnorway/.\n\n\n\n\nI repoet Ã¥pner du fila ./infra/projects.yaml. Se eksempel for teamet play-obr til hÃ¸yre.\nFinn ut om du Ã¸nsker Ã¥ skru pÃ¥ en feature i test eller prod. Hvis du Ã¸nsker Ã¥ gjÃ¸re det i prod, sÃ¥ skal du legge til en linje under features der env: prod. Hvis du Ã¸nsker Ã¥ gjÃ¸re det i test, sÃ¥ skal du legge til en linje under features der env: test.\nDu kan legge til linjen ved Ã¥ trykke pÃ¥ -ikonet Ã¸verst til hÃ¸yre i fila, endre teksten, og trykke pÃ¥ Commit changes. Velg deretter hvilket navn du Ã¸nsker pÃ¥ branchen, og trykk Create pull request.\nI PR-en venter du og ser at atlantis/plan1 har blitt kjÃ¸rt og fÃ¥r en  til venstre for hver kjÃ¸ring, slik som vist i FigurÂ 1.\n\n\n\n\n./infra/projects.yaml\n\nteam_uniform_name: play-obr\n\nprojects:\n  - project_name: play-obr\n    env: test\n    features:\n      - dapla-buckets\n      - kildomaten\n\n  - project_name: play-obr\n    env: prod\n    features:\n      - dapla-buckets\n\n\n\n\n\n\n\n\n\nFigurÂ 1: Atlantis plan planlegger endringen og sjekker om den er klar til gjennomfÃ¸ring.\n\n\n\n\nHvis alt er i orden sÃ¥ ber du en kollega om Ã¥ se over endringen og godkjenne hvis alt ser riktig ut. NÃ¥r den er godkjent vil du se et bilde som ligner det du ser i FigurÂ 2.\n\n\n\n\n\n\n\nFigurÂ 2: Sjekker av PR-er i teamets IaC-repo.\n\n\n\n\nNÃ¥r PR er godkjent av en kollega, og atlantis plan ikke viser noen feilmeldinger, sÃ¥ kan du effektuere endringene ved Ã¥ atlantis apply i kommentarfeltet til PR-en. Kommentareren vil trigge en kjÃ¸ring som effektuerer alle endringer pÃ¥ plattformen.\nEtter at atlantis apply er kjÃ¸rt, sÃ¥ kan du merge inn branchen til main.\nTil slutt kan du slette branchen.\n\nNÃ¥r dette er gjort sÃ¥ endringen effektuert pÃ¥ Dapla og klar til bruk. For eksempel, hvis vi hadde aktivert transfer-service i prod for projects.yaml over, sÃ¥ ville den etter endringen sett slik ut:\n\n\n./infra/projects.yaml\n\nteam_uniform_name: play-obr\n\nprojects:\n  - project_name: play-obr\n    env: test\n    features:\n      - dapla-buckets\n      - kildomaten\n\n  - project_name: play-obr\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service\n\n\n\n\nFor Ã¥ deaktivere en feature som ikke lenger i bruk, sÃ¥ fÃ¸lger du bare beskrivelsen for Aktivere tjeneste over, med det unntak at du nÃ¥ fjerner en linje istedenfor Ã¥ legge til.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Features"
    ]
  },
  {
    "objectID": "statistikkere/features.html#aktivere-feature",
    "href": "statistikkere/features.html#aktivere-feature",
    "title": "Features",
    "section": "",
    "text": "Skru pÃ¥ en feature om gangen.\n\n\n\nHvis du Ã¸nsker Ã¥ skru pÃ¥ flere features samtidig, sÃ¥ mÃ¥ du gjÃ¸re det i flere PR-er. Atlantis vil ikke klare Ã¥ hÃ¥ndtere flere features i samme PR. FÃ¸lg oppskriften under for hver feature du Ã¸nsker Ã¥ skru pÃ¥.\n\n\nAktivering av en feature for Dapla-teamet ditt er veldig enkelt. Endringen er sÃ¥ liten at vi anbefaler Ã¥ gjÃ¸re endringen direkte i GitHubs grensesnitt, uten Ã¥ klone repoet fÃ¸rst. Slik gÃ¥r du frem:\n\nSÃ¸k opp ditt teams sitt IaC-repoet under https://github.com/statisticsnorway/.\n\n\n\n\nI repoet Ã¥pner du fila ./infra/projects.yaml. Se eksempel for teamet play-obr til hÃ¸yre.\nFinn ut om du Ã¸nsker Ã¥ skru pÃ¥ en feature i test eller prod. Hvis du Ã¸nsker Ã¥ gjÃ¸re det i prod, sÃ¥ skal du legge til en linje under features der env: prod. Hvis du Ã¸nsker Ã¥ gjÃ¸re det i test, sÃ¥ skal du legge til en linje under features der env: test.\nDu kan legge til linjen ved Ã¥ trykke pÃ¥ -ikonet Ã¸verst til hÃ¸yre i fila, endre teksten, og trykke pÃ¥ Commit changes. Velg deretter hvilket navn du Ã¸nsker pÃ¥ branchen, og trykk Create pull request.\nI PR-en venter du og ser at atlantis/plan1 har blitt kjÃ¸rt og fÃ¥r en  til venstre for hver kjÃ¸ring, slik som vist i FigurÂ 1.\n\n\n\n\n./infra/projects.yaml\n\nteam_uniform_name: play-obr\n\nprojects:\n  - project_name: play-obr\n    env: test\n    features:\n      - dapla-buckets\n      - kildomaten\n\n  - project_name: play-obr\n    env: prod\n    features:\n      - dapla-buckets\n\n\n\n\n\n\n\n\n\nFigurÂ 1: Atlantis plan planlegger endringen og sjekker om den er klar til gjennomfÃ¸ring.\n\n\n\n\nHvis alt er i orden sÃ¥ ber du en kollega om Ã¥ se over endringen og godkjenne hvis alt ser riktig ut. NÃ¥r den er godkjent vil du se et bilde som ligner det du ser i FigurÂ 2.\n\n\n\n\n\n\n\nFigurÂ 2: Sjekker av PR-er i teamets IaC-repo.\n\n\n\n\nNÃ¥r PR er godkjent av en kollega, og atlantis plan ikke viser noen feilmeldinger, sÃ¥ kan du effektuere endringene ved Ã¥ atlantis apply i kommentarfeltet til PR-en. Kommentareren vil trigge en kjÃ¸ring som effektuerer alle endringer pÃ¥ plattformen.\nEtter at atlantis apply er kjÃ¸rt, sÃ¥ kan du merge inn branchen til main.\nTil slutt kan du slette branchen.\n\nNÃ¥r dette er gjort sÃ¥ endringen effektuert pÃ¥ Dapla og klar til bruk. For eksempel, hvis vi hadde aktivert transfer-service i prod for projects.yaml over, sÃ¥ ville den etter endringen sett slik ut:\n\n\n./infra/projects.yaml\n\nteam_uniform_name: play-obr\n\nprojects:\n  - project_name: play-obr\n    env: test\n    features:\n      - dapla-buckets\n      - kildomaten\n\n  - project_name: play-obr\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Features"
    ]
  },
  {
    "objectID": "statistikkere/features.html#deaktivere-en-feature",
    "href": "statistikkere/features.html#deaktivere-en-feature",
    "title": "Features",
    "section": "",
    "text": "For Ã¥ deaktivere en feature som ikke lenger i bruk, sÃ¥ fÃ¸lger du bare beskrivelsen for Aktivere tjeneste over, med det unntak at du nÃ¥ fjerner en linje istedenfor Ã¥ legge til.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Features"
    ]
  },
  {
    "objectID": "statistikkere/features.html#footnotes",
    "href": "statistikkere/features.html#footnotes",
    "title": "Features",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nDet som skjer nÃ¥r atlantis plan kjÃ¸res er at det genereres en detaljert beskrivelse av hvilke endringer som mÃ¥ skje pÃ¥ plattformen for at teamets feature skal aktiveres. Derfor mÃ¥ eventuelle feilmeldinger fra atlantis plan fikses fÃ¸r man faktiske kan effektuere endringene med atlantis apply. â†©ï¸",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Features"
    ]
  },
  {
    "objectID": "statistikkere/deling-av-data.html",
    "href": "statistikkere/deling-av-data.html",
    "title": "Deling av data",
    "section": "",
    "text": "Dapla-team kan dele data mellom team via sÃ¥kalte delt-bÃ¸tter. Hvert team kan opprette de delt-bÃ¸ttene de har behov for, og deretter gi tilgang til grupper i andre team. Opprettelse av bÃ¸tter skal fÃ¸lge retningslinjene som er definert her.",
    "crumbs": [
      "Manual",
      "Data",
      "Deling av data"
    ]
  },
  {
    "objectID": "statistikkere/deling-av-data.html#forberedelser",
    "href": "statistikkere/deling-av-data.html#forberedelser",
    "title": "Deling av data",
    "section": "Forberedelser",
    "text": "Forberedelser\nFor Ã¥ opprette delt-bÃ¸tter mÃ¥ man fÃ¸rst skru pÃ¥ featuren shared-buckets for teamet. Det gjÃ¸res ved at en pÃ¥ teamet gjÃ¸r fÃ¸lgende:\n\n\n\nGÃ¥ til IaC-repoet til teamet pÃ¥ Github.\nÃ…pne filen ./infra/projects.yaml.\nLegg til en linje med shared-buckets under features i miljÃ¸et du Ã¸nsker, slik som vist til hÃ¸yre.\nOpprette en PR med endringen.\nBe en i gruppa data-admins se over endringen og godkjenne.\nKjÃ¸r atlantis plan og atlantis apply slik som beskrevet her.\n\n\n\n\n\n\n\nprojects.yaml\n\nteam_uniform_name: dapla-example\n\nprojects:\n  - project_name: dapla-example\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service\n      - shared-buckets",
    "crumbs": [
      "Manual",
      "Data",
      "Deling av data"
    ]
  },
  {
    "objectID": "statistikkere/deling-av-data.html#typer-av-delt-bÃ¸tter",
    "href": "statistikkere/deling-av-data.html#typer-av-delt-bÃ¸tter",
    "title": "Deling av data",
    "section": "Typer av delt-bÃ¸tter",
    "text": "Typer av delt-bÃ¸tter\nDet finnes to typer av delt-bÃ¸tter som team kan opprette:\n\nStandard\nStandard er en type delt-bÃ¸tte som teamet selv har lese- og skriverettigheter til bÃ¸tta. Dvs. at den er ment for deling av data som det delende team ogsÃ¥ har tilgang til i produkt-bÃ¸tta.\nDelomat\nDelomat er en type delt-bÃ¸tte som kan benyttes med Delomaten-tjenesten. Denne benyttes nÃ¥r delende teamet skal dele personidentifiserende informasjon (PII) pÃ¥ en annen mÃ¥te enn de selv har tilgang til. F.eks. hvis et team som jobber med pseudonymisert PII, men mÃ¥ dele en versjon av datasettet hvor PII er i klartekst.\n\nHvilken type delt-bÃ¸tte som skal opprettes vil avgjÃ¸res av om delende og konsumerende behandler PII likt eller ikke.",
    "crumbs": [
      "Manual",
      "Data",
      "Deling av data"
    ]
  },
  {
    "objectID": "statistikkere/deling-av-data.html#opprettelse-av-bÃ¸tte",
    "href": "statistikkere/deling-av-data.html#opprettelse-av-bÃ¸tte",
    "title": "Deling av data",
    "section": "Opprettelse av bÃ¸tte",
    "text": "Opprettelse av bÃ¸tte\nOpprettelse av delt-bÃ¸tter gjÃ¸res i teamets IaC-repo. For Ã¥ opprette en delt-bÃ¸tte mÃ¥ man legge til en linje i fila ./infra/projects/&lt;teamnavn&gt;-prod/buckets-shared.yaml. Du legger bare til kortnavnet for bÃ¸tta, altsÃ¥ den delen av bÃ¸tte-navnet som teamet kan styre selv, og hvilken type bÃ¸tte som Ã¸nskes.\n\n\nbuckets-shared.yaml\n\nversion: kuben/v1\nkind: SharedBuckets\nbuckets:\n- name: ameld\n  type: standard\n- name: ledstill\n  type: standard\n- name: ameld-pii\n  type: delomat\n\nI eksempelet over sÃ¥ opprettes to standardbÃ¸tter med navn ameld og ledstill. I tillegg opprettes bÃ¸tta ameld-pii for Ã¥ dele en versjon av ameld der PII er i klartekst. Sistnevnte mÃ¥ deles via Delomaten, siden delende team ikke skal ha tilgang til disse dataene.\nDe fulle bÃ¸ttenavnene som blir opprettet fra eksempelet over vil vÃ¦re:\n\nssb--data-delt-ameld-prod\nssb--data-delt-ledstill-prod\nssb--data-delt-delomat-ameld-pii-prod\n\nEtter at endringen er gjort i buckets-shared.yaml, sÃ¥ gjÃ¸r du fÃ¸lgende:\n\nOpprette en PR pÃ¥ repoet med endringen.\nFÃ¥ en i gruppen data-admins til Ã¥ gÃ¥ gjennom og godkjenne.\nKjÃ¸r atlantis plan og atlantis apply som beskrevet her.\n\n\n\n\n\n\n\nNavngivning av bÃ¸tter\n\n\n\nNavngivning av bÃ¸tter fÃ¸lger RFC 1123. Dvs. at navnet mÃ¥ fÃ¸lge disse reglene:\n\nMax 63 tegn i navnet (gjelder fullt navn pÃ¥ bÃ¸tta)\nkun smÃ¥ bokstaver a-z og sifre 0-9\ntillatt med -\ndet er ikke tillatt med _",
    "crumbs": [
      "Manual",
      "Data",
      "Deling av data"
    ]
  },
  {
    "objectID": "statistikkere/deling-av-data.html#tilgangsstyring",
    "href": "statistikkere/deling-av-data.html#tilgangsstyring",
    "title": "Deling av data",
    "section": "Tilgangsstyring",
    "text": "Tilgangsstyring\nTilgangsstyring kan skje i samme steg som opprettelse av bÃ¸tta, men her beskrives det som et eget steg for de tilfellene der man Ã¸nsker Ã¥ gi nye tilganger til eksisterende delt-bÃ¸tter.\nTilganger til delt-bÃ¸tter kan gis av teamet som eier bÃ¸tta. Hvem som har tilgang blir definert i fila ./infra/projects/&lt;teamnavn&gt;-prod/shared-buckets.yaml i IaC-repoet til teamet. Det er den samme filen som hvor selve bÃ¸tta ble opprettet.\nHvem som skal ha tilgang angis under sharedWith for hver bÃ¸tte, og dette gir tilgangsgruppen som oppgis lesetilgang til filene i bÃ¸tta.\n\n\nbuckets-shared.yaml\n\nversion: kuben/v1\nkind: SharedBuckets\nbuckets:\n- name: ameld\n  type: standard\n  sharedWith:\n  - play-obr-b-developers\n- name: ledstill\n  type: standard\n  sharedWith:\n  - play-obr-b-developers  \n- name: ameld-pii\n  type: delomat\n  sharedWith:\n  - dapla-felles-developers\n\nAv eksempelet over ser vi at det spesifisert tilgang til play-obr-b-developers for standardbÃ¸ttene ameld og ledstill. For delomat-bÃ¸tta ameld-pii er det spedisfisert tilgang for dapla-felles-developers.\nOver ser vi at teamet dapla-example har to delt-bÃ¸tter:\n\nssb-dapla-example-data-delt-freg-prod\nssb-dapla-example-data-delt-ameld-prod\n\nDen fÃ¸rste bÃ¸tta har de gitt lesetilgang til data-admins og developers i team-alpha. Fra linje 8 og nedover ser vi at de har gitt developers i team-beta lesetilgang til ssb-dapla-example-data-delt-ameld-prod.\nNÃ¥r man har gjort endringer i iam.yaml sÃ¥ gjÃ¸r man fÃ¸lgende:\n\nOpprette en PR med endringen.\nFÃ¥ en i gruppen data-admins til Ã¥ gÃ¥ gjennom endringen og godkjenne.\nKjÃ¸r atlantis plan og atlantis apply som beskrevet her.",
    "crumbs": [
      "Manual",
      "Data",
      "Deling av data"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html",
    "href": "statistikkere/jupyter-playground.html",
    "title": "Jupyter-playground",
    "section": "",
    "text": "Jupyter-playground er en tjeneste pÃ¥ Dapla Lab som er ment for nybegynnere og andre som vil komme raskt i gang med koding i Jupyterlab. Den har mange likheter med Jupyter-tjenesten pÃ¥ Dapla Lab med den forskjellen at mange flere pakker og extensions er ferdig installert i Jupyter-playground. Tjenesten har bÃ¥de R og Python installert.\nSiden tjenesten er ment for opplÃ¦ring og utforskning sÃ¥ er det ikke anbefalt Ã¥ bygge produksjonskode fra denne tjenesten. Grunnen til det er at det er flere avhengigheter mellom programvare enn nÃ¸dvendig, noe som skaper mer komplisert kode enn nÃ¸dvendig. Derimot er det et ideelt sted for Ã¥ lÃ¦re seg R eller Python siden man slipper kompleksiteten med Ã¥ installere sine egne pakker og forholde seg til ssb-project. For de som skal utvikle produksjonskode anbefales det at koden heller utvikles fra Jupyter-tjenesten.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#forberedelser",
    "href": "statistikkere/jupyter-playground.html#forberedelser",
    "title": "Jupyter-playground",
    "section": "Forberedelser",
    "text": "Forberedelser\nFÃ¸r man starter Jupyter-playground-tjenesten bÃ¸r man ha lest kapitlet om Dapla Lab og satt opp Git- og GitHub-konfigurasjonen under Min konto. Deretter gjÃ¸r du fÃ¸lgende:\n\nLogg deg inn pÃ¥ Dapla Lab\nUnder Tjenestekatalog trykker du pÃ¥ Start-knappen for Jupyter\nGi tjenesten et navn\nÃ…pne Jupyter konfigurasjoner",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#konfigurasjon",
    "href": "statistikkere/jupyter-playground.html#konfigurasjon",
    "title": "Jupyter-playground",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nKonfigurasjonen av Jupyter-playground er identisk som for Jupyter-tjenesten. Se dokumentasjonen for konfigurasjon av Jupyter.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#datatilgang",
    "href": "statistikkere/jupyter-playground.html#datatilgang",
    "title": "Jupyter-playground",
    "section": "Datatilgang",
    "text": "Datatilgang\nSlik kan man inspisere dataene fra en terminal inne i tjenesten:\n\nÃ…pne en instans av Jupyter med data fra bÃ¸tter\nÃ…pne en terminal inne i Jupyter\nGÃ¥ til mappen med bÃ¸ttene ved Ã¥ kjÃ¸re dette fra terminalen cd /buckets\nKjÃ¸r ls -ahl i teminalen for Ã¥ se pÃ¥ hvilke bÃ¸tter som er montert.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#installere-pakker",
    "href": "statistikkere/jupyter-playground.html#installere-pakker",
    "title": "Jupyter-playground",
    "section": "Installere pakker",
    "text": "Installere pakker\nSiden det nesten ikke er installert noen pakker i tjenesten sÃ¥ kan brukeren opprette et ssb-project og installere pakker som vanlig.\nFor Ã¥ bygge et eksisterende ssb-project sÃ¥ kan brukeren ogsÃ¥ bruke ssb-project.\nFor Ã¥ installere R-pakker fÃ¸lger man beskrivelsen for renv.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#slette-tjenesten",
    "href": "statistikkere/jupyter-playground.html#slette-tjenesten",
    "title": "Jupyter-playground",
    "section": "Slette tjenesten",
    "text": "Slette tjenesten\nFor Ã¥ slette tjenesten kan man trykke pÃ¥ Slette-knappen i Dapla Lab under Mine tjenester. NÃ¥r man sletter en tjeneste sÃ¥ sletter man hele disken inne i tjenesten og frigjÃ¸r alle ressurser som er reservert. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#pause-tjenesten",
    "href": "statistikkere/jupyter-playground.html#pause-tjenesten",
    "title": "Jupyter-playground",
    "section": "Pause tjenesten",
    "text": "Pause tjenesten\nMan kan pause tjenesten ved Ã¥ trykke pÃ¥ Pause-knappen i Dapla Lab under Mine tjenester. NÃ¥r man pauser sÃ¥ slettes alt pÃ¥den lokale disken som ikke er lagret under $HOME/work. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#monitorering",
    "href": "statistikkere/jupyter-playground.html#monitorering",
    "title": "Jupyter-playground",
    "section": "Monitorering",
    "text": "Monitorering\n\n\nMan kan moniterere en instans av Jupyter ved Ã¥ trykke pÃ¥ Jupyter-teksten under Mine tjenester i Dapla Lab, slik som vist i FigurÂ 1.\nDenne funksjonaliteten er under arbeid og mer informasjon kommer snart.\n\n\n\n\n\n\n\n\n\nFigurÂ 1: Monitorering av Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html",
    "href": "statistikkere/rstudio.html",
    "title": "Rstudio",
    "section": "",
    "text": "Rstudio er en tjeneste pÃ¥ Dapla Lab for utvikling av kode i R1. MÃ¥lgruppen for tjenesten er brukere som skal skrive produksjonskode i R2.\nSiden tjenesten er ment for produksjonskode sÃ¥ er det veldig fÃ¥ forhÃ¥ndsinstallerte R-pakker. Antagelsen er at brukerene/teamet heller bÃ¸r installere de pakkene de trenger, framfor at alle pakker som alle brukere/team er forhÃ¥ndsinstallert og skal dekke behovet til alle. Det reduserer kompleksitet i tjenesten og dermed sannsynligheten for feilsituasjoner.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html#forberedelser",
    "href": "statistikkere/rstudio.html#forberedelser",
    "title": "Rstudio",
    "section": "Forberedelser",
    "text": "Forberedelser\nFÃ¸r man starter Rstudio bÃ¸r man ha lest kapitlet om Dapla Lab og satt opp Git- og GitHub-konfigurasjonen under Min konto. Deretter gjÃ¸r du fÃ¸lgende:\n\nLogg deg inn pÃ¥ Dapla Lab\nUnder Tjenestekatalog trykker du pÃ¥ Start-knappen for Rstudio\nGi tjenesten et navn\nÃ…pne Rstudio konfigurasjoner",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html#konfigurasjon",
    "href": "statistikkere/rstudio.html#konfigurasjon",
    "title": "Rstudio",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nKonfigurasjonen av RStudio er nÃ¦r identisk Jupyter-tjenesten sin konfigurasjon. Se dokumentasjonen for konfigurasjon av Jupyter.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html#datatilgang",
    "href": "statistikkere/rstudio.html#datatilgang",
    "title": "Rstudio",
    "section": "Datatilgang",
    "text": "Datatilgang\nSlik kan man inspisere dataene fra en terminal inne i tjenesten:\n\nÃ…pne en instans av Rstudio med data fra bÃ¸tter\nÃ…pne en terminal inne i Rstudio\nGÃ¥ til mappen med bÃ¸ttene ved Ã¥ kjÃ¸re dette fra terminalen cd /buckets\nKjÃ¸r ls -ahl i teminalen for Ã¥ se pÃ¥ hvilke bÃ¸tter som er montert.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html#installere-pakker",
    "href": "statistikkere/rstudio.html#installere-pakker",
    "title": "Rstudio",
    "section": "Installere pakker",
    "text": "Installere pakker\nSiden det nesten ikke er installert noen pakker i tjenesten sÃ¥ kan brukeren opprette et renv og installere pakker som Ã¸nsker.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html#slette-tjenesten",
    "href": "statistikkere/rstudio.html#slette-tjenesten",
    "title": "Rstudio",
    "section": "Slette tjenesten",
    "text": "Slette tjenesten\nFor Ã¥ slette tjenesten kan man trykke pÃ¥ Slette-knappen i Dapla Lab under Mine tjenester. NÃ¥r man sletter en tjeneste sÃ¥ sletter man hele disken inne i tjenesten og frigjÃ¸r alle ressurser som er reservert. Siden pakkene som er installert ogsÃ¥ ligger lagret pÃ¥ disken, betyr dette at pakkene mÃ¥ installeres pÃ¥ nytt etter at en tjeneste er slettet. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html#pause-tjenesten",
    "href": "statistikkere/rstudio.html#pause-tjenesten",
    "title": "Rstudio",
    "section": "Pause tjenesten",
    "text": "Pause tjenesten\nMan kan pause tjenesten ved Ã¥ trykke pÃ¥ Pause-knappen i Dapla Lab under Mine tjenester. NÃ¥r man pauser sÃ¥ slettes alt pÃ¥den lokale disken som ikke er lagret under $HOME/work. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html#monitorering",
    "href": "statistikkere/rstudio.html#monitorering",
    "title": "Rstudio",
    "section": "Monitorering",
    "text": "Monitorering\n\n\nMan kan moniterere en instans av Rstudio ved Ã¥ trykke pÃ¥ Rstudio-teksten under Mine tjenester i Dapla Lab, slik som vist i FigurÂ 1.\nDenne funksjonaliteten er under arbeid og mer informasjon kommer snart.\n\n\n\n\n\n\n\n\n\nFigurÂ 1: Monitorering av Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html#footnotes",
    "href": "statistikkere/rstudio.html#footnotes",
    "title": "Rstudio",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nRstudio er web-versjonen av Rstudio og er ikke helt identisk med desktop-versjonen som mange er kjent med.â†©ï¸\nPython er ikke installert i Rstudio-tjenesten.â†©ï¸",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/ssb-project.html",
    "href": "statistikkere/ssb-project.html",
    "title": "SSB-project",
    "section": "",
    "text": "Note\n\n\n\nDenne artikkelen fokuserer pÃ¥ SSB-project som GitHub-mal. Vi har skrevet en egen artikkel for SSB-project som verktÃ¸y for Ã¥ hÃ¥ndtere Python-pakker: PakkehÃ¥ndtering i Python.\nStatistikkproduksjon pÃ¥ Dapla mÃ¥ vÃ¦re reproduserbart, delbart og gjenkjennelig. SSB-project er et verktÃ¸y som hjelper deg med dette ved Ã¥ gjÃ¸re fÃ¸lgende:\nVi mener at ssb-project er et naturlig sted Ã¥ starte nÃ¥r man skal bygge opp koden i Python eller R. Det gjelder bÃ¥de pÃ¥ bakken og pÃ¥ sky. I denne delen av kapitlet forklarer vi deg hvordan du kan ta i bruk ssb-project.\nKort fortalt kan du kjÃ¸re denne kommandoen i en terminal:\nDa vil fÃ¥ en mappe som heter stat-testprod med fÃ¸lgende innhold:\nI tillegg lar ssb-project deg opprette et GitHub-repo hvis du Ã¸nsker. Hvis du velger Ã¥ la ssb-project opprette et GitHub-repo for deg, sÃ¥ vil det ogsÃ¥ sette opp SSBs anbefalte GitHub-oppsett. Det er viktig for at du skal kunne dele koden din med andre i SSB pÃ¥ en sikker mÃ¥te.",
    "crumbs": [
      "Manual",
      "Kode",
      "SSB-project"
    ]
  },
  {
    "objectID": "statistikkere/ssb-project.html#forberedelser",
    "href": "statistikkere/ssb-project.html#forberedelser",
    "title": "SSB-project",
    "section": "Forberedelser",
    "text": "Forberedelser\nFÃ¸r du kan ta i bruk ssb-project sÃ¥ er det et par ting som mÃ¥ vÃ¦re pÃ¥ plass:\n\nDu mÃ¥ ha konfigurert Git etter SSB sin standard (les mer om hvordan her).\nHvis du Ã¸nsker at ssb-project ogsÃ¥ skal opprette et GitHub-repo for deg mÃ¥ ogsÃ¥ fÃ¸lgende vÃ¦re pÃ¥ plass:\n\nDu mÃ¥ ha en GitHub-bruker (les hvordan her)\nSkru pÃ¥ 2-faktor autentifisering for GitHub-brukeren din (les hvordan her)\nVÃ¦re koblet mot SSBs organisasjon statisticsnorway pÃ¥ GitHub (les hvordan her)\nOpprette Personal Access Token (PAT) og godkjenne det for bruk mot statisticsnorway (les hvordan her)\n\n\nDet er ogsÃ¥ Ã¥ anbefale at du lagrer PAT lokalt/i Dapla Lab slik at du ikke trenger Ã¥ forholde deg til det nÃ¥r jobber med Git og GitHub. Hvis du har alt dette pÃ¥ plass sÃ¥ kan du bare fortsette Ã¥ fÃ¸lge de neste kapitlene.",
    "crumbs": [
      "Manual",
      "Kode",
      "SSB-project"
    ]
  },
  {
    "objectID": "statistikkere/ssb-project.html#opprett-ssb-project",
    "href": "statistikkere/ssb-project.html#opprett-ssb-project",
    "title": "SSB-project",
    "section": "Opprett ssb-project",
    "text": "Opprett ssb-project\n\nUten GitHub-repo\nFor Ã¥ opprette et nytt ssb-project uten GitHub-repo gjÃ¸r du fÃ¸lgende:\n\nÃ…pne en terminal. De fleste vil gjÃ¸re dette i Jupyterlab pÃ¥ bakke eller sky og da kan de bare trykke pÃ¥ det blÃ¥ â•-tegnet i Jupyterlab og velge Terminal.\nFÃ¸r vi kjÃ¸rer programmet mÃ¥ vi vÃ¦re obs pÃ¥ at ssb-project vil opprette en ny mappe der vi stÃ¥r. GÃ¥ derfor til den mappen du Ã¸nsker Ã¥ ha den nye prosjektmappen i. For Ã¥ opprette et prosjekt som heter stat-testprod sÃ¥ skriver du fÃ¸lgende i terminalen:\n\n\n\nterminal\n\nssb-project create stat-testprod\n\n\n\nHvis du stod i hjemmemappen din pÃ¥ nÃ¥r du skrev inn kommandoen over i terminalen, sÃ¥ har du fÃ¥tt mappestrukturen som vises i FigurÂ 1. 1. Den inneholder fÃ¸lgende :\n\n.git-mappe som blir opprettet for Ã¥ versjonshÃ¥ndtere med Git.\nsrc-mappe som skal inneholde all koden som utgjÃ¸r produksjonslÃ¸pet. src er kort for source\ntests-mappe som inneholder tester du skriver for koden din.\nLICENCE-fil som skal benyttes for public-repos i SSB.\npoetry.lock-fil som inneholder alle versjoner av Python-pakker som blir brukt.\npyproject.toml-fil som inneholder informasjon om prosjektet og hvilke pakker som blir brukt.\nREADME.md-fil som brukes for tekstlig innhold pÃ¥ GitHub-siden for prosjektet.\n\n\n\n\n\n\n\n\n\nFigurÂ 1: Mappen som ble opprettet av ssb-project.\n\n\n\n\n\n\n\nMed Github-repo\nOver sÃ¥ opprettet vi et ssb-project uten Ã¥ opprette et GitHub-repo. Hvis du Ã¸nsker Ã¥ opprette et GitHub-repo ogsÃ¥ mÃ¥ du endre kommandoen over til:\n\n\nterminal\n\nssb-project create stat-testprod --github --github-token='blablabla'\n\nKommandoen over oppretter en mappestruktur slik vi sÃ¥ tidligere, men ogsÃ¥ et ssb-project som heter stat-testprod med et GitHub-repo med samme navn. Som du ser sÃ¥ mÃ¥ vi da sende med opsjonen --github og PAT med opsjonen --github-token='blablabla'. Repoet i GitHub ser da ut som i FigurÂ 2. Hvis du Ã¸nsker Ã¥ slippe Ã¥ mÃ¥tte forholde deg til PAT hver gang interagerer med GitHub, kan du fÃ¸lge denne beskrivelsen for Ã¥ lagre den lokalt. Da kan droppe --github-token='blablabla' fra kommandoen over.\n\n\n\n\n\n\nFigurÂ 2: GitHub-repo som er opprettet av ssb-project\n\n\n\n\n\n\n\n\n\nNÃ¥r du har opprettet et nytt ssb-project, eller bygget et eksisterende prosjekt, sÃ¥ kan det ta rundt 30 sekunder fÃ¸r kernelen viser seg i Jupterlab-launcher. VÃ¦r tÃ¥lmodig!",
    "crumbs": [
      "Manual",
      "Kode",
      "SSB-project"
    ]
  },
  {
    "objectID": "statistikkere/ssb-project.html#bygg-eksisterende-ssb-project",
    "href": "statistikkere/ssb-project.html#bygg-eksisterende-ssb-project",
    "title": "SSB-project",
    "section": "Bygg eksisterende ssb-project",
    "text": "Bygg eksisterende ssb-project\nNÃ¥r vi skal samarbeide med andre om kode sÃ¥ gjÃ¸r vi dette via GitHub. NÃ¥r du pusher koden din til GitHub, sÃ¥ kan samarbeidspartnere pulle ned koden og jobbe videre med den. Men nÃ¥r de henter ned koden sÃ¥ vil de bare hente ned selve koden, ikke pakker og Python-versjonen som ble brukt. De mÃ¥ installere alt som du hadde installert. I tillegg trenger de en kernel hvis de skal jobbe i Jupyterlab. ssb-project gjÃ¸r det svÃ¦rt enkelt Ã¥ bygge opp det du trenger, siden det virtuelle miljÃ¸et har all informasjon om hva som trengs.\nFor at samarbeidspartneren din skal kunne bygge miljÃ¸et pÃ¥ nytt, mÃ¥ de ha gjort en minimal konfigurering av Git. Les mer om hvordan du frem for Ã¥ gjÃ¸re dette her.\nFor Ã¥ bygge opp et eksisterende miljÃ¸ gjÃ¸r du fÃ¸lgende:\n\nFÃ¸rst mÃ¥ du kopiere prosjektet ned lokalt, eller klone repoet med git-terminologi\n\n\n\nterminal\n\ngit clone https://github.com/statisticsnorway/&lt;prosjektnavn&gt;\n\n\nGÃ¥ inn i mappen du klonet\n\n\n\nterminal\n\ncd &lt;prosjektnavn&gt;\n\n\nSkape et virtuelt miljÃ¸ og installere en tilsvarende Jupyter kernel med\n\n\n\nterminal\n\nssb-project build",
    "crumbs": [
      "Manual",
      "Kode",
      "SSB-project"
    ]
  },
  {
    "objectID": "statistikkere/ssb-project.html#slette-ssb-project",
    "href": "statistikkere/ssb-project.html#slette-ssb-project",
    "title": "SSB-project",
    "section": "Slette ssb-project",
    "text": "Slette ssb-project\nDet vil vÃ¦re tilfeller hvor man Ã¸nsker Ã¥ slette et ssb-project, enten fordi man ikke trenger koden lenger eller fordi man bare testet litt.\n\nLokalt\nHvis man jobber med flere prosjekter sÃ¥ kan det fort bli mange Jupyter kerneler hengende igjen. Derfor er det ogsÃ¥ mulighet Ã¥ kjÃ¸re\n\n\nterminal\n\nssb-project clean stat-testprod\n\nsom sletter Jupyter-kernelen og de installerte pakkene i prosjektet. Hvis du ogsÃ¥ Ã¸nsker Ã¥ slette selve mappen med kode mÃ¥ du gjÃ¸re det manuelt2:\n\n\nterminal\n\nrm -rf ~/stat-testprod/\n\nProsjektmappen over lÃ¥ direkte i hjemmemappen min og hjemmemappen pÃ¥ Linux kan alltid referes til med et tilda-tegn ~.\n\n\nArkiver GitHub-repo\nGitHub-repoer som er opprettet under SSB sin organinasjons statisticsnorway pÃ¥ GitHub kan ikke slettes, bare arkiveres. Grunnen er at hvis man oppdager en sÃ¥rbarhet senere sÃ¥ er det viktig Ã¥ kunne se repoet for Ã¥ forstÃ¥ hva som har skjedd.\nHvis du ikke trenger et GitHub-repo lenger kan man arkivere repoet. Det gjÃ¸r du pÃ¥ fÃ¸lgende mÃ¥te:\n\nGi inn i repoet Settings slik som vist med rÃ¸d pil i FigurÂ 3.\n\n\n\n\n\n\n\nFigurÂ 3: Settings for repoet.\n\n\n\n\nUnder General scroller du deg ned til delen som heter Danger Zone og velger Archive this repository, slik som vist pÃ¥ FigurÂ 4.\n\n\n\n\n\n\n\nFigurÂ 4: Arkivering av et repo.\n\n\n\n\nI dialogboksen som dukker opp fyller du inn reponavnet som beskrevet og trykker pÃ¥ I understand the consequences, archive this repository.\n\nNÃ¥r det er gjort sÃ¥ er repoet lesbart, men man kan ikke jobbe med det. Men som vi ser av @#fig-github-repo-settings-archive-warning kan man omgjÃ¸re arkiveringen senere hvis det skulle vÃ¦re Ã¸nskelig.",
    "crumbs": [
      "Manual",
      "Kode",
      "SSB-project"
    ]
  },
  {
    "objectID": "statistikkere/ssb-project.html#spark-i-ssb-project",
    "href": "statistikkere/ssb-project.html#spark-i-ssb-project",
    "title": "SSB-project",
    "section": "Spark i ssb-project",
    "text": "Spark i ssb-project\nFor Ã¥ kunne bruke Spark i et ssb-project mÃ¥ man fÃ¸rst installere pyspark. Det gjÃ¸r du ved Ã¥ skrive fÃ¸lgende i en terminal:\n\n\nterminal\n\npoetry add pyspark==$(pip show pyspark | grep Version | egrep -o \"([0-9]{1,}\\.)+[0-9]{1,}\") --no-dev\n\nHer installerer du samme versjon av pyspark som pÃ¥ Jupyterlab.\nVidere kan vi konfigurere Spark til Ã¥ enten kjÃ¸re pÃ¥ lokal maskin eller pÃ¥ flere maskiner (sÃ¥kalte clusters). Under beskriver vi begge variantene.\n\nLokal maskin\nOppsettet for Pyspark pÃ¥ lokal maskin er det enkleste Ã¥ sette opp siden Pyspark vil ha direkte tilgang til det lokale filsystemet. Man kan bruke miljÃ¸variabelen PYSPARK_PYTHON til Ã¥ peke pÃ¥ det virtuelle miljÃ¸et, og dermed vil Pyspark ogsÃ¥ ha tilgang til alle pakkene som er installert der. I en notebook vil dette kunne settes opp slik:\n\n\nnotebook\n\nimport os\nimport subprocess\n\n# Finner filstien til det virtuelle miljÃ¸et\npython_path = subprocess.run(['poetry', 'run', 'which', 'python'],\n                             capture_output=True, text=True).stdout.rstrip('\\n')\n\nos.environ[\"PYSPARK_PYTHON\"] = python_path\nos.environ[\"PYSPARK_SUBMIT_ARGS\"] = os.environ[\"PYSPARK_LOCAL_SUBMIT_ARGS\"]\n\nNÃ¥r du oppretter en Notebook og bruker den kernelen du har laget sÃ¥ mÃ¥ du alltid ha denne pÃ¥ toppen av notebooken. Det initialiserer Spark slik at det kan brukes i notebooken.\n\n\nnotebook\n\n%run /usr/local/share/jupyter/kernels/pyspark_local/init.py\n\nDette scriptet vil sette et spark objekt som brukes for Ã¥ kalle APIâ€™et til pyspark.\n\n\nCluster\nHvis man vil kjÃ¸re Pyspark i et cluster (dvs. pÃ¥ flere maskiner) sÃ¥ vil databehandlingen foregÃ¥ pÃ¥ andre maskiner som ikke har tilgang til det lokale filsystemet. Man mÃ¥ dermed lage en â€œpakkeâ€ av det virtuelle miljÃ¸et pÃ¥ lokal maskin og tilgjengeliggjÃ¸re dette for alle maskinene i clusteret. For Ã¥ lage en slik â€œpakkeâ€ kan man bruke et bibliotek som heter venv-pack. Dette kan kjÃ¸res fra et terminalvindu slik:\n\n\nterminal\n\nvenv-pack -p .venv -o pyspark_venv.tar.gz\n\nMerk at kommandoen over mÃ¥ kjÃ¸res fra rot-mappen i prosjektet ditt. Her er pyspark_venv.tar.gz et tilfeldig valgt filnavn, men dette filnavnet skal brukes videre i notebooken.\n\n\nnotebook\n\nimport os\nimport subprocess\n\n# MiljÃ¸variabel som peker pÃ¥ en utpakket versjon av det virtuelle miljÃ¸et\nos.environ[\"PYSPARK_PYTHON\"] = \"./environment/bin/python\"\n\n# Legg til et flagg, --archives, som peker pÃ¥ \"pakken\" med det virtuelle miljÃ¸et\nconf = os.environ[\"PYSPARK_K8S_SUBMIT_ARGS\"].split(' ')\nlast_index = conf.index('pyspark-shell')\nconf[last_index:last_index] = ['--archives', 'pyspark_venv.tar.gz#environment']\nos.environ[\"PYSPARK_SUBMIT_ARGS\"] = ' '.join(conf)\n\nNÃ¥r du oppretter en Notebook og bruker den kernelen du har laget sÃ¥ mÃ¥ du alltid ha denne pÃ¥ toppen av notebooken. Det initialiserer Spark slik at det kan brukes i notebooken.\n\n\nnotebook\n\n%run /usr/local/share/jupyter/kernels/pyspark_k8s/init.py\n\nDette scriptet vil sette et spark objekt som brukes for Ã¥ kalle APIâ€™et til pyspark.",
    "crumbs": [
      "Manual",
      "Kode",
      "SSB-project"
    ]
  },
  {
    "objectID": "statistikkere/ssb-project.html#tips-og-triks",
    "href": "statistikkere/ssb-project.html#tips-og-triks",
    "title": "SSB-project",
    "section": "Tips og triks",
    "text": "Tips og triks\nI denne delen av kapitlet vil vi gi deg noen tips og triks som kan vÃ¦re nyttige nÃ¥r du jobber med ssb-project.\n\nPoetry\nssb-project bruker Poetry for Ã¥ hÃ¥ndtere virtuelle miljÃ¸er. Poetry er et verktÃ¸y som gjÃ¸r det enkelt Ã¥ installere pakker og hÃ¥ndtere versjoner av disse. Det er ogsÃ¥ Poetry som hÃ¥ndterer Jupyter-kernelen for deg.\nHvis du etterlyser funksjonalitet i et ssb-project sÃ¥ kan det vÃ¦re nyttig Ã¥ lese dokumentasjonen til Poetry for Ã¥ se om det er mulig Ã¥ fÃ¥ til det du Ã¸nsker. Les ogsÃ¥ vÃ¥r egne artikkel Poetry og SSB-project - PakkehÃ¥ndtering i Python.\n\n\nFull disk pÃ¥ Dapla\nDet â€œlokaleâ€ filsystemet pÃ¥ Dapla har kun 10GB diskplass. Har du mange virtuelle miljÃ¸er pÃ¥ denne disken kan det fort bli fullt, siden alle pakker blir installert her. Vanligvis er det 2 grunner til at disken blir full:\n\nFor mange virtuelle miljÃ¸er (ssb-projects) lagret lokalt.\nDette vil ofte kunne lÃ¸ses ved Ã¥ slette virtuelle miljÃ¸er som ikke lenger er i bruk. Hvis du har 5 virtuelle miljÃ¸er som hver bruker 1GB, og du kun jobber pÃ¥ en av de nÃ¥, sÃ¥ vil du frigjÃ¸re 40% av disken ved Ã¥ slette 4 av dem. Husk at det permanente lagringsstedet for kode er pÃ¥ GitHub, og du kan alltid klone ned et prosjekt senere og bygge det hvis det trengs.\n/home/jovyan/.cache/ har blitt for stort.\nDette er en mappe som brukes av applikasjoner til Ã¥ lagre midlertidig data slik at de kan kjÃ¸re raskere. Denne kan bli ganske stor etter hvert. Ofte kan man frigjÃ¸re flere GB ved Ã¥ slette denne. Du sletter denne mappen ved Ã¥ skrive fÃ¸lgende i en terminal:\n\n\n\nterminal\n\nrm -rf /home/jovyan/.cache/\n\nHvis du opplever at disken er full, sÃ¥ kan det anbefales Ã¥ undersÃ¸ke hvilke mapper som tar stÃ¸rst plass med fÃ¸lgende kommando i terminalen:\n\n\nterminal\n\ncd ~ && du -h --max-depth=5 | sort -rh | head -n 10 && cd -\n\nKommandoen over sjekker, fra hjemmemappen din, hvilke mapper og undermapper som tar mest plass. Den viser de 10 stÃ¸rste mappene. Hvis du Ã¸nsker Ã¥ se flere mapper sÃ¥ kan du endre tallet etter head -n. Hvis du Ã¸nsker Ã¥ se alle mapper sÃ¥ kan du fjerne head -n. --max-depth=5 betyr at den kun sjekker mapper som er 5 mapper dype fra hjemmemappen din.\nNÃ¥r du har gjort det kan selv vurdere hvilke som kan slettes for Ã¥ frigjÃ¸re plass.",
    "crumbs": [
      "Manual",
      "Kode",
      "SSB-project"
    ]
  },
  {
    "objectID": "statistikkere/ssb-project.html#footnotes",
    "href": "statistikkere/ssb-project.html#footnotes",
    "title": "SSB-project",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nFiler og mapper som starter med punktum er skjulte med mindre man ber om Ã¥ se dem. I Jupyterlab kan disse vises i filutforskeren ved Ã¥ velge View fra menylinjen, og deretter velge Show hidden files. I en terminal skriver man ls -a for Ã¥ se de.â†©ï¸\nDette kan ogsÃ¥ gjÃ¸res ved Ã¥ hÃ¸yreklikke pÃ¥ mappen i Jupyterlab sin filutforsker og velge Delete.â†©ï¸",
    "crumbs": [
      "Manual",
      "Kode",
      "SSB-project"
    ]
  },
  {
    "objectID": "statistikkere/dapla-ctrl.html",
    "href": "statistikkere/dapla-ctrl.html",
    "title": "Dapla Ctrl",
    "section": "",
    "text": "Dapla Ctrl1 er tjeneste for tilgangsstyring pÃ¥ Dapla. FormÃ¥let med appen er at det skal vÃ¦re lett Ã¥ fÃ¥ oversikt og administrere tilganger knyttet til Dapla-team.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Dapla Ctrl"
    ]
  },
  {
    "objectID": "statistikkere/dapla-ctrl.html#innlogging",
    "href": "statistikkere/dapla-ctrl.html#innlogging",
    "title": "Dapla Ctrl",
    "section": "Innlogging",
    "text": "Innlogging\nAlle som jobber i SSB kan logge seg inn pÃ¥ https://dapla-ctrl.intern.ssb.no/ for Ã¥ bruke tjenesten.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Dapla Ctrl"
    ]
  },
  {
    "objectID": "statistikkere/dapla-ctrl.html#funksjonalitet",
    "href": "statistikkere/dapla-ctrl.html#funksjonalitet",
    "title": "Dapla Ctrl",
    "section": "Funksjonalitet",
    "text": "Funksjonalitet\nAlle SSB-ansatte som logger seg inn i Dapla Ctrl fÃ¥r tilgang til Ã¥ se informasjon om Dapla-team og tilganger. I tillegg kan de som er i tilgangsgruppen managers legge til, fjerne og endre medlemmer i teamet de har denne rollen. Seksjonsledere har tilgang til Ã¥ opprette nye team.\n\nTeamoversikt\n\n\n\n\n\n\n\nFigurÂ 1: Bilde av forsiden med oversikt over Mine team i Dapla Ctrl.\n\n\n\nFigurÂ 1 viser landingssiden/forsiden som fÃ¸rst mÃ¸ter den som logger seg inn i Dapla Ctrl. den. Her fÃ¥r den som er innlogget oversikt over hvilke Dapla-team man er medlem av, og fÃ¸lgende informasjon om teamene:\n\nTeknisk teamnavn\nTeamets eierseksjon\nAntall teammedlemmer\nManagers for teamet\n\nMan kan ogsÃ¥ bytte fane fra Mine team til Alle team for Ã¥ se samme informasjon om alle team som finnes pÃ¥ Dapla.\n\n\nTeamvisning\n\n\n\n\n\n\n\nFigurÂ 2: Bilde av oversikten over et enkelt-team i Dapla Ctrl.\n\n\n\nFra Teamoversikten kan man trykke seg inn pÃ¥ et spesifikt team og fÃ¥ en oversikt slik som vist i FigurÂ 2.\nPÃ¥ toppen av siden fÃ¥r man se fÃ¸lgende informasjon:\n\nteamets visningsnavn\nteamets tekniske kortnavn\neierseksjonens seksjonsleder\neierseksjonens seksjonsnavn\nautonomitetsnivÃ¥et til teamet\n\nVidere ser vi at det er en fane for Teammedlemmer og en for Delte Data. Under fanen for Teammedlemmer ser man fÃ¸lgende informasjon om alle medlemmene av teamet:\n\nnavn pÃ¥ medlem\nhvilken seksjons de jobber pÃ¥\nhvilken tilgangsgruppe de tilhÃ¸rer pÃ¥ teamet\ne-postadresse med brukerens kortnavn\n\nUnder fanen Delte data fÃ¥r man en oversikt over hvilke bÃ¸tter teamet har opprettet for Ã¥ dele data med andre team.\n\n\n\n\n\n\nFigurÂ 3: Bilde av oversikten over hvilke bÃ¸tter teamet har opprettet for Ã¥ dele data med andre team.\n\n\n\nFigurÂ 3 viser hvilken informasjon man fÃ¥r over teamets delte data. FÃ¸lgende informasjon vises:\n\nkortnavnet pÃ¥ bÃ¸ttene\ntekniske navnet til bÃ¸ttene\ntype delt-bÃ¸tte\nhvor mange team som har tilgang\nhvor mange personer som har tilgang2\n\n\n\nDelte data\n\n\n\n\n\n\n\nFigurÂ 4: Bilde av oversikten over hvilke personer som har tilgang til en delt-bÃ¸tte.\n\n\n\nFra Teamvisningen kan man velge fanen Delte data og trykke seg pÃ¥ en av teamets delte-bÃ¸tter. FigurÂ 4 viser informasjon man fÃ¥r se i denne visningen. PÃ¥ toppen av siden fÃ¥r man se kortnavnet til bÃ¸tta, det tekniske navnet pÃ¥ bÃ¸tta, hvilket team som eier bÃ¸tta og hvilken eierseksjon teamet har. I tabellen som vises kan man undersÃ¸ke hvilke personer som har tilgang til bÃ¸tta og fÃ¥ fÃ¸lgende informasjon om de:\n\nnavn\nhvilken seksjon de jobber pÃ¥\nhvilket team-medelemskap de har tilgang i kraft av\nhvilken tilgangsgruppe de er i pÃ¥ teamet de har tilgang i kraft av\n\nFra tabellen kan man velge Ã¥ se nÃ¦rmere pÃ¥ personen som har tilgang, f.eks. se hvilke andre tilganger denne personen har, eller man kan se nÃ¦rmere pÃ¥ teamet som personen har tilgang i kraft av. Ved Ã¥ undersÃ¸ke teamet nÃ¦rmere kommer man inn pÃ¥ Teamvisningen som er beskrevet over, mens visning av Teammedlemmer forklares under.\n\n\nTeammedlemmer\n\n\n\n\n\n\n\nFigurÂ 5: Bilde av oversikten over hvilke personer som med i dine team.\n\n\n\nFigurÂ 5 viser oversikt over teammedlemmer. Ã˜verst pÃ¥ siden kan man velge mellom en fane for Mine teammedlemmer og Alle teammedlemmer. FÃ¸rstnevnte viser hvilke andre medlemmer som er i de teamene den innloggede er med i, mens sistnevnte viser alle teammedlemmer i SSB3. I tabellen under fÃ¥r man fÃ¸lgende informasjon om teammedlemmene:\n\nnavn\nhvilken seksjons de jobber pÃ¥\nhvor mange team de er medlem av\nhvor mange team de har tilgangsrollen data-admins\nnavn pÃ¥ personens seksjonsleder\n\n\n\nMedlemsvisning\n\n\n\n\n\n\n\nFigurÂ 6: Bilde av oversikten over hvilke team en person er medlem av.\n\n\n\nFigurÂ 6 viser hva man ser nÃ¥r gÃ¥r inn pÃ¥ en enkeltperson, enten via Teamoversikten eller Teammedlemmer. Ã˜verst pÃ¥ siden stÃ¥r navnet til personen, hvorvidt de har arbeidssted i Oslo eller Kongsvinger4, hvilken seksjon de jobber pÃ¥ og e-postadressen deres.\nTabellen i FigurÂ 6 fÃ¥r man en oversikt over hvilke team personen er medlem av, samt fÃ¸lgende detaljer:\n\nteamets tekniske kortnavn\nseksjonseier av teamet\nhvilke tilgangsgrupper personen er med i\nhvem som er managers for teamet\n\nVidere kan man gÃ¥ videre inn pÃ¥ et av teamene og se nÃ¦rmere pÃ¥ hvem som er medlemmer og hvilke data de deler.\n\n\nOpprette team\n\n\nDet er kun seksjonsledere i SSB som kan opprette et Dapla-team. Hvis en seksjonsleder logger seg inn i Dapla Ctrl sÃ¥ vil knappen i FigurÂ 7 vises pÃ¥ Teamoversikt-siden. Eierseksjonen til et team vil bli definert av hvilken seksjonsleder som oppretter teamet.\n\n\n\n\n\n\n\n\n\nFigurÂ 7: Bilde av knappen som vises for seksjonsledere.\n\n\n\n\n\nNÃ¥r man oppretter et team mÃ¥ man fylle ut skjemaet i FigurÂ 8. Under finner du en oversikt hva som er viktig Ã¥ vurdere nÃ¥r man fyller ut de ulike feltene.\n\n\n\nVisningsnavn\nVisningsnavn er teamets navn i et lesevennlig format. Navnet bÃ¸r bestÃ¥ av et hoveddomenet og et subdomenet. Det er tillatt med smÃ¥/store bokstaver, mellomrom, Ã†, Ã˜ og Ã….\nEksempel pÃ¥ et hoveddomenet i SSB er Skatt, og under det finnes det subdomener som Person og NÃ¦ring. Visningsnavnet til teamene er da Skatt Person og Skatt NÃ¦ring.\nI noen tilfeller gir det ikke mening med et subdomenet og da er det greit Ã¥ kun ha et hoveddomenet. Et eksempel pÃ¥ et visningsnavn som kun har hoveddomenet er Nasjonalregnskap.\n\n\n\n\n\n\n\n\n\n\nFigurÂ 8: Bilde av siden for opprettelse av team.\n\n\n\n\n\n\nOverstyr teknisk navn\nVelg dette for Ã¥ overstyre det automatisk genererte tekniske teamnavnet. Hvis man ikke krysser av denne boksen vil det genereres et teknisk teamnavn basert pÃ¥ visningsnavnet. Det kan vÃ¦re nyttig Ã¥ velge dette hvis man har et langt visningsnavn og Ã¸nsker Ã¥ forkorte det genererte tekniske teamnavnet.\n\n\nTeknisk teamnavn\nTeamets navn i et maskinvennlig format som bl.a. benyttes i filstier til lagringsbÃ¸tter. Det er ikke tillatt med mellomrom og norske tegn (Ã†, Ã˜ og Ã…). Navnet kan ikke overskride 17 tegn.\nDet tekniske teamnavnet blir automatisk generert basert pÃ¥ visningsnavn hvis man ikke velger Overstyr teknisk navn. TabellÂ 1 viser eksempler pÃ¥ visningsnavn der man bÃ¥de har overtyrt det tekniske navnet og ikke.\n\n\n\nTabellÂ 1: Eksempler pÃ¥ teamnavn\n\n\n\n\n\nVisningsnavn\nTeknisk navn\nOverstyrt\n\n\n\n\nSkatt Person\nskatt-person\nnei\n\n\nSkatt NÃ¦ring\nskatt-naering\nnei\n\n\nNasjonalregnskap\nnr\nja\n\n\nFinansmarkedsstatistikk\nfinmark\nja\n\n\n\n\n\n\n\n\nEierseksjon\nAlle team tilhÃ¸rer en seksjon og denne informasjonen ligger lagret i metadataene til teamet. Standard er at seksjonslederen som sÃ¸ker fyller ut sitt seksjonsnummer her, men det er mulig Ã¥ velge andre seksjoner.\n\n\nAutonomitetsnivÃ¥\nNivÃ¥ av frihet et team har til Ã¥ definere sin egen infrastruktur. Statistikkproduserende team er vanligvis i kategorien Managed, dvs. at de kun bruker tjenester som tilbys av plattformen. IT-team vil ofte defineres som Self-Managed fordi dette gir stÃ¸rre kontroll til teamet. Les mer her.\n\n\n\nLegge til medlemmer\n\n\nHvis man som medlem av tilgangsgruppen managers logger seg inn Dapla Ctrl, vil man fÃ¥ tilgang til Ã¥ legge til medlemmer i teamet man er i managers-gruppa for. GÃ¥r man inn pÃ¥ Teamvisning vil man se knappen i FigurÂ 9.\n\n\n\n\n\n\n\n\n\nFigurÂ 9: Bilde av knappen som vises for managers.\n\n\n\n\n\nTrykker man pÃ¥ knappen sÃ¥ fÃ¥r man opp en side for Ã¥ legge til nye medlemmer i teamet, slik som vist FigurÂ 10. Man kan sÃ¸ke opp alle ansatte i SSB, og man kan velge Ã¥ legge de til i en eller flere tilgangsgrupper. NÃ¥r man har valgt person, og hvilke tilgangsgrupper de skal legges i, sÃ¥ avslutter man med Ã¥ trykke pÃ¥ Legg til medlem for Ã¥ effektuere endringen.\n\n\n\n\n\n\nFigurÂ 10: Bilde av siden for Ã¥ legge til medlemmer.\n\n\n\nDet kan ta mellom 1-2 minutter fÃ¸r tilgangen er aktivert og klar til bruk.\n\n\nEndre eller fjerne medlemmer\nHvis man som medlem av tilgangsgruppen managers logger seg inn Dapla Ctrl, vil man fÃ¥ tilgang til Ã¥ fjerne og endre medlemmer i teamet man er i managers-gruppa for. GÃ¥r man inn pÃ¥ Teamvisning sÃ¥ vil man se Endre-knapp for hver person i teamet.\n\n\n\n\n\n\nFigurÂ 11: Bilde av Teamvisning som vises for personer i tilgangsgruppen managers.\n\n\n\n\n\nAv FigurÂ 11 ser vi at hvert medlem i teamet har en Endre-knapp. Trykker man pÃ¥ den sÃ¥ fÃ¥r man opp bilde som vises i FigurÂ 12.\nÃ˜nsker man Ã¥ fjerne et medlem fra teamet, sÃ¥ kan man bare trykke pÃ¥ Fjern fra teamet. Da vil man bli spurt om Ã¥ bekrefte at personen skal gjernes, og velger man ok sÃ¥ effektureres endringen ila et par minutter.\nÃ˜nsker man endre hvilken tilgangsgruppe en person er med i, sÃ¥ gjÃ¸r man det ved Ã¥ enten fjerne eller legge til tilganger som listet under dropdown-menyen for Tilgangsgruppe(r). For eksempel hvis en person ligger som bÃ¥de data-admins og developers, slik som eksempelet i FigurÂ 12, sÃ¥ trykker man bare pÃ¥ X-ikonet for den tilgangen, og til slutt effektuerer man endringen ved Ã¥ velge Oppdater tilgang.\n\n\n\n\n\n\n\n\n\nFigurÂ 12: Bilde av siden for Ã¥ endre eller fjerne medlemmer.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Dapla Ctrl"
    ]
  },
  {
    "objectID": "statistikkere/dapla-ctrl.html#footnotes",
    "href": "statistikkere/dapla-ctrl.html#footnotes",
    "title": "Dapla Ctrl",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nNavnet Dapla Ctrl er valgt for Ã¥ kommunisere at mÃ¥lsetning med appen er Ã¥ gi SSB-ere kontroll over tilgangsstyring pÃ¥ Dapla pÃ¥ en effektiv mÃ¥te.â†©ï¸\nAntall personer som har tilgang til en delt-bÃ¸tte viser hvor mange personer det er som har tilgang fra de teamene som har tilgang. Som regel vil det vÃ¦re slik at kun noen tilgangsgrupper i et team fÃ¥r tilgang til andre sine delte data, og ikke hele teamet.â†©ï¸\nAlle teammedlemmer vil i praksis si alle ansatte i SSB, siden teamet Dapla Felles alltid legger til alle ansatte i SSB. FormÃ¥let med dette er Ã¥ la alle ansatte fÃ¥ tilgang til testdata i en bÃ¸tte.â†©ï¸\nHvorvidt arbeidssted er Oslo eller Kongsvinger indikeres med henholdsvis O eller K fÃ¸r seksjonsnummeret.â†©ï¸",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Dapla Ctrl"
    ]
  },
  {
    "objectID": "statistikkere/standarder.html",
    "href": "statistikkere/standarder.html",
    "title": "Standarder",
    "section": "",
    "text": "NÃ¥r man driver statistikkproduksjon er det flere regler man mÃ¥ forholde seg til.\nStandardutvalget stÃ¥r for utarbeidingen av flere av disse reglene og standardene pÃ¥ Dapla, for eksempel navnestandarden som er beskrevet i artikkelen navnestandard.\n\n\n\n\n\n\nKort om standardutvalget!\n\n\n\n\n\n\nAnsvarlig for det sentrale standardiseringsarbeidet knyttet til bruk og gjenbruk av data i SSB.\nBeslutningsmydnighet fra DM, men prinsipielle saker, og saker med store konsekvenser, sendes til DM for vedtak.\nUtvikler hovedsaklig obligatoriske standarder, men statistikkprodusenter oppfordres ogsÃ¥ til Ã¥ gjÃ¸re seg kjent med anbefalingene.\nLenke: Standardutvalgets side pÃ¥ byrÃ¥nettet\nLenke: Vedtak fra Standardutvalget\n\n\n\n\nDenne artikkelen er i stor grad basert pÃ¥ standardutvalgets mandat og standardutvalgets side pÃ¥ byrÃ¥nettet.\n\nStandardutvalget\nStandardutvalget har ansvar for det sentrale standardiseringsarbeidet knyttet til bruk og gjenbruk av data i SSB. Alle avdelinger er representert med unntak av avdeling 100. Standardutvalgets side pÃ¥ byrÃ¥nettet inneholder en oversikt over utvalgsmedlemmene.\nFormÃ¥let med utvalget, slik det er beskrevet i mandatet fra 2023, er Ã¥ legge grunnlag for effektiv bruk og gjenbruk av data som SSB samler inn, bearbeider og forvalter.\nStandardutvalget har beslutningsmyndighet delegert fra DM og kan dermed vedta krav, regler, anbefalinger eller obligatoriske standarder.\n\nHvilke omrÃ¥der jobber standardutvalget med?\nStandardutvalget utvikler standarder innenfor fÃ¸lgende omrÃ¥der:\n\nKodeverk (klassifikasjoner og kodelister)\nVariabler og variabelnavn\nEnhetstyper\nNavngivning, versjonering, dokumentasjon og lagring av datasett og populasjoner i alle ledd i produksjonsprosessen\nKvalitetsindikatorer\nProsessdata\nDatatilstander\nInformasjonsmodeller\n\nStandardutvalget har ikke ansvar for standarder innenfor koding og statistiske metoder. Det hÃ¥ndteres av KVAKK (Kvalitet i kode og koding) og s811 - Seksjon for metoder.\n\n\n\nHvilke standarder finnes?\n\n1. DataDoc - dokumentasjon av datasett\n\nDataDoc - Krav til dokumentasjon av datasett pÃ¥ Dapla (Confluence)\nDatadoc-editor - Artikkel i Dapla-manualen\n\n\n\n2. Datatilstander i SSB\n\nDatatilstander - Artikkel i Dapla-manualen\nInterne dokumenter - Datatilstander, skrevet av Standardutvalget\n\n\n\n\n\n\n\nFigurÂ 1: En grafisk fremstilling av forskjellene mellom datatilstandene i SSB (Standardutvalget 2023).\n\n\n\n\n\n3. VarDef - dokumentasjon av variabler\n\nVarDef - Confluence-side\n\n\n\n4. Standardformater for lagring av data\n\nStandardformater - Confluence-side\n\n\n\n5. Navnestandard for henholdsvis:\n\nEnhetstypeidentifikatorer - DM-vedtak (internet dokument)\nGitHub repoer - Internt dokument\nNÃ¸kkelvariabler (anbefaling) - ByrÃ¥nettside\nDatasett - Dapla-manualen: Navnestandard (og versjonering)\n\n\n\n6. Kvalitetsindikatorer\n\nStandardutvalget har definert et sett med anbefalte kvalitetsindikatorer for statistikkproduksjon - sÃ¦rlig kvantitative indikatorer\nInternt dokument - Anbefalte kvalitetsindikatorer i statistikkproduksjonen\nInternt dokument - Mal for dokumentasjon av kvalitetsindikatorer\n\n\n\n7. Editering - prinsipper og retningslinjer\n\nOffentlig dokument - Prinsipper og retningslinjer for dataeditering\nDokumentet lenket i punktet over inneholder ni prinsipper for editering i SSB, blant annet:\n\nFormÃ¥let med dataediteringen skal vÃ¦re klart formulert\nKontrollene, kontrollustlagene og endringene skal vÃ¦re veldokumenterte\nAutomatiser editeringsprosessen sÃ¥ mye som mulig\n\n\n\n\n8. Kodelister (anbefaling)\n\nLes om de anbefalte standardiserinenge i byrÃ¥nettsiden for standardutvalgets vedtak\n\n\n\n\n\n\n\nReferanser\n\nStandardutvalget. 2023. â€œDatatilstander i SSB.â€ Statistisk sentralbyrÃ¥. https://ssbno.sharepoint.com/sites/Internedokumenter/Delte%20dokumenter/Forms/AllItems.aspx?id=%2Fsites%2FInternedokumenter%2FDelte%20dokumenter%2FInterne%20dokumenter%202023%2F2023%2D14%20Datatilstander%20i%20SSB%2Epdf&parent=%2Fsites%2FInternedokumenter%2FDelte%20dokumenter%2FInterne%20dokumenter%202023.",
    "crumbs": [
      "Manual",
      "Standarder"
    ]
  },
  {
    "objectID": "statistikkere/opprette-dapla-team.html",
    "href": "statistikkere/opprette-dapla-team.html",
    "title": "Opprette Dapla-team",
    "section": "",
    "text": "Opprette Dapla-team\nFor Ã¥ komme i gang med Ã¥ opprette et Dapla-team trengs det en oversikt over teamets medlemmer og hvilke tilgangsgrupper medlemmene skal vÃ¦re med i. Det trengs ogsÃ¥ informasjon om hvilke Dapla-tjenester som er aktuelle for teamet Ã¥ ta i bruk. Derfor har det blitt opprettet en egen veileder for dette kalt Dapla Start.\n\n\n\n\n\n\nGÃ¥ til Dapla Start for starte bestilling av et nytt Dapla-team.\n\n\n\nNÃ¥r teamet er opprettet fÃ¥r alle medlemmene tilgang til sitt eget prosjekt i Google Cloud Platform (GCP), som er SSBs leverandÃ¸r av skytjenester. Videre fÃ¥r hvert prosjekt et sett med tjenester og tilganger som knyttes til teamet. Det opprettes ogsÃ¥ datalagringsomrÃ¥der (kalt bÃ¸tter) som bare kan aksesseres av brukere som er med i teamets tilgangsgrupper.\nDapla-teamet vil ogsÃ¥ fÃ¥ sin egen gruppe i SSBs Active Directory slik at medlemskapet i gruppen kan administreres av Kundeservice."
  },
  {
    "objectID": "statistikkere/dapla-lab.html",
    "href": "statistikkere/dapla-lab.html",
    "title": "Dapla Lab",
    "section": "",
    "text": "Dapla Lab er SSBs arbeidsbenk for statistikkproduksjon og forskning. LÃ¸sningen er bygget pÃ¥ INSEE sin plattform Onyxia. FormÃ¥let med Dapla Lab er Ã¥ kunne tilby moderne skybaserte dataverktÃ¸y til SSB-ere pÃ¥ en effektiv og enhetlig mÃ¥te. Dapla Lab gir brukeren en enkel oversikt over hvilke verktÃ¸y som tilbys, bÃ¥de internt utviklet programvare og velkjente open-source verktÃ¸y. Alle tjenestene kan konfigureres etter brukerens Ã¸nsker.",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/dapla-lab.html#innlogging",
    "href": "statistikkere/dapla-lab.html#innlogging",
    "title": "Dapla Lab",
    "section": "Innlogging",
    "text": "Innlogging\nAlle som er pÃ¥ SSBs nettverk kan logge seg inn i Dapla ved Ã¥ gÃ¥ inn pÃ¥ nettadressen https://lab.dapla.ssb.no/ og velge Logg inn Ã¸verst i hÃ¸yre hjÃ¸rne. FigurÂ 1 viser landingssiden som mÃ¸ter brukeren.\n\n\n\n\n\n\nFigurÂ 1: Landingsside for Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/dapla-lab.html#funksjonalitet",
    "href": "statistikkere/dapla-lab.html#funksjonalitet",
    "title": "Dapla Lab",
    "section": "Funksjonalitet",
    "text": "Funksjonalitet\n\n\nSelv om Dapla Lab er en lÃ¸sning for Ã¥ tilby tjenester sÃ¥ er det ogsÃ¥ en del nyttig funksjonalitet Dapla Lab. FigurÂ 2 viser menyen i Dapla Lab som gir en oversikt over funksjonaliteten som finnes. Under beskriver vi nÃ¦rmere hvordan man blant annet kan:\n\ndefinere brukernavn og e-post for Git\nlagre GitHub-token\nlagre tjenestekonfigurasjon\nfÃ¥ oversikt over hvilke tjenester man har kjÃ¸rende\npause en tjeneste\n\nMenyen i FigurÂ 2 inkluderer ogsÃ¥ lenker til andre nettsteder som er nyttig nÃ¥r man jobber med data pÃ¥ Dapla. Meny-innslagene Dapla Manualen, Dapla Ctrl, Google Cloud Console og FAQ er alle lenker til eksterne ressurser.\n\n\n\n\n\n\n\n\n\nFigurÂ 2: Menyen i Dapla Lab\n\n\n\n\n\n\nHjem\nHjem tar deg til landingssiden i Dapla Lab, slik som vist i FigurÂ 1. Her finner du nyttige lenker til lÃ¦ringsressurser for Dapla, felleskap pÃ¥ Viva Engage og opprettelse av Dapla-team.\n\n\nMin konto\nNÃ¥r man logger seg inn i Dapla Lab sÃ¥ skjer det SSB-kontoen til brukeren. Under Min konto kan man se informasjon om sin konto og konfigurere noen nyttige verdier knyttet til brukeren din.\n\nKontoinformasjon\nUnder denne fanen kan lese ut hvilken bruker-id som er benyttet for innloggingen i Dapla Lab, ditt fulle navn og e-postadresse i Dapla Lab. Informasjonen blir definert ved innlogging og kan ikke endres i Dapla Lab.\n\n\n\n\n\n\nFigurÂ 3: Kontoinformasjon under Min konto i Dapla Lab\n\n\n\n\n\nGit\nUnder fanen Git kan man definere brukernavn og e-post for Git, og et personlig tilgangstoken for GitHub. Dette vil deretter kunne brukes i tjenester som brukeren starter i Dapla Lab.\n\n\n\n\n\n\nFigurÂ 4: Git-konfigurasjon under Min konto i Dapla Lab\n\n\n\n\n\nGrensesnittpreferanser\nUnder fanen Grensesnittpreferanser kan man tilpasse Dapla Lab til sine preferanser ved Ã¥ velge om man blant annet Ã¸nsker Dark mode eller ikke. I tillegg kan man definere hvilket sprÃ¥k man Ã¸nsker i Dapla Lab. Det finnes ogsÃ¥ avanserte valg for avanserte brukere. F.eks. man Ã¸nsker Ã¥ se hvilke Helm-kommandoer som kjÃ¸res i bakgrunnen nÃ¥r man starter en tjeneste.\n\n\n\n\n\n\nFigurÂ 5: Grensesnittpreferanser under Min konto i Dapla Lab\n\n\n\n\n\n\nTjenestekatalog\nUnder Tjenestekatalogen ligger alle tjenestene som brukeren kan velge Ã¥ starte.\n\n\n\n\n\n\nFigurÂ 6: Tjenestekatalogen i Dapla Lab\n\n\n\nFigurÂ 6 viser hvilke tjenester som nÃ¥ er tilgjengelig i Dapla Lab, inkludert en kort beskrivelse av bruksomrÃ¥det for hver tjeneste. FigurÂ 7 viser hva som mÃ¸ter nÃ¥r de starter Jupyter-tjenesten.\n\n\nTjenestekonfigurasjon\nAlle tjenester pÃ¥ Dapla Lab kan konfigureres fÃ¸r de startes opp. Trykker man pÃ¥ Start pÃ¥ en av tjenestene i tjenestekatalogen kommer man inn tjenestekonfigurasjon for akkurat den tjenesten. Felles for alle tjenester er at man kan navngi hver tjeneste og velge versjon1, slik som vist under Vennlig navn og Versjon i FigurÂ 7.\n\n\n\n\n\n\nFigurÂ 7: Tjenestekonfigurasjon i Dapla Lab\n\n\n\nEkspanderer man Jupyter konfigurasjoner vist i FigurÂ 7, sÃ¥ fÃ¥r man opp konfigurasjon som er spesifikk for akkurat den tjenesten. Hver tjenestetilbyder vurderer hvilken konfigurasjon som gir mening for den tjenesten de tilbyr.\nFor programmeringsmiljÃ¸er som Jupyter og VS Code kan brukeren velge hvilket team og tilgangsgruppe de skal representere, hvor mye ram og gpu de Ã¸nsker, hvor stor diskplass de Ã¸nsker, Git/GitHub-oppsett, etc..\nI Datadoc-tjenesten har tilbyderen kun valgt Ã¥ la brukeren velge hvilket team de representerer og versjon av tjenesten. Les mer om tjenestekonfigurasjonen til en tjeneste i dokumentasjonen til tjenesten.\n\nLagre tjenestekonfigurasjon\nVanligvis vil brukeren Ã¸nske Ã¥ starte en tjeneste med samme konfigurasjon som sist. Dapla Lab tilbyr derfor at du kan lagre en tjenestekonfigurasjon med egenvalgt navn. Etter at du har valgt verdiene du Ã¸nsker i tjenestekonfigurasjonen sÃ¥ trykker du pÃ¥ Lagre-ikonet vist i FigurÂ 7. Deretter kan du se dine lagrede konfigurasjoner under Mine tjenester, slik som vist i FigurÂ 8.\n\n\n\n\n\n\nFigurÂ 8: Tjenestekonfigurasjon i Dapla Lab\n\n\n\n\n\nDele tjenestekonfigurasjon\nMan kan ogsÃ¥ dele sin tjenestekonfigurasjon med andre i SSB. Det forutsetter at de man deler med har de samme datatilgangene som den som deler. Man kan dele lagrede tjenestekonfigurasjoner ved Ã¥ gÃ¥ til Mine tjenester, trykke pÃ¥ de tre prikkene til hÃ¸yre i tjenesten i ikonet, og deretter Kopier URL-lenke, slik som vist i FigurÂ 9. Deretter er det bare Ã¥ sende lenken til en kollega, og de kan Ã¥pne en likt konfigurert tjeneste med sine tilganger.\n\n\n\n\n\n\nFigurÂ 9: Dele lagret tjenestekonfigurasjon i Dapla Lab\n\n\n\n\n\n\n\n\n\nNoe konfigurasjon kan ikke deles\n\n\n\nKonfigurasjon som er knyttet brukerkonfigurasjon fra Dapla Lab, f.eks. GitHub-token, mÃ¥ settes manuelt av den man deler konfigurasjon med. Dette vil forhÃ¥pentligvis forbedres etter hvert.\n\n\n\n\n\nMine tjenester\nUnder Mine tjenester fÃ¥r man oversikt over hvilke tjenester som er startet av brukeren. FigurÂ 10 viser en bruker som har 3 tjenester kjÃ¸rende. For hver tjeneste vises informasjon om hvilken tjeneste som er startet, hvor lenge den har kjÃ¸rt, og muligheten til Ã¥ pause eller avslutte tjenesten, og hvilken tilgangsgruppen den ble startet med.\n\n\n\n\n\n\nFigurÂ 10: Oversikt over brukerens kjÃ¸rende tjenester\n\n\n\nHvis man trykker pÃ¥ sÃ¸ppelkasse-ikonet sÃ¥ avsluttes tjenesten og alt som er lagret inne i tjenesten blir slettet. Hvis man trykker pÃ¥ pause-knappen sÃ¥ bevares alt som brukeren har lagret under $HOME/work, mens alt annet blir slettet.\n\n\n\n\n\n\nViktigheten av Ã¥ avslutte ubrukte tjenester\n\n\n\nEn tjeneste som stÃ¥r som aktiv vil reservere ressursene (CPU, GPU, RAM, etc.) som brukeren valgte ved oppstart. Hvis tjenesten ikke benyttes bÃ¸r derfor brukeren enten avslutte eller pause tjenesten, slik at SSB ikke mÃ¥ betale for ubrukte ressurser.\n\n\n\n\nMonitorering\n\n\n\n\n\n\nAll monitorering er ikke pÃ¥ plass enda\n\n\n\nInnholdet pÃ¥ OvervÃ¥kningssiden til tjenestene er fullstendig enda. NÃ¥r du kommer inn pÃ¥ siden sÃ¥ skal loggene fra tjenesten viser, men dette er ikke pÃ¥ plass enda. Dette jobbes det med Ã¥ fÃ¥ pÃ¥ plass.\nDerimot fungerer Ekstern overvÃ¥kning-lenken (se beskrivelse under) og den tar deg til et Grafana-dashboard som viser vanlige metrikker for tjenesten. I tillegg kan man trykke pÃ¥ lenken Helm-verdier som teknisk informasjon om hvilke verdier som ble satt nÃ¥r tjenesten ble startet.\n\n\nUnder Mine tjenester fÃ¥r du oversikt over hvilke kjÃ¸rende tjenester. Hvis du Ã¸nsker Ã¥ monitorere hvor mye ram, cpu diskplass eller gpu tjenester bruker, sÃ¥ kan du inspisere et ferdig oppsatt Grafana-dashboard. For Ã¥ Ã¥pne dashboardet trykker du fÃ¸rst pÃ¥ navnet pÃ¥ tjenesten du Ã¸nsker Ã¥ monitorere, slik som vist i FigurÂ 11 (a). Det Ã¥pner en side for OvervÃ¥kning av tjenesten. PÃ¥ denne siden er det en lenke til et Grafana-dashboard, slik som vist i FigurÂ 11 (b).\n\n\n\n\n\n\n\n\n\n\n\n(a) Ã…pne OvervÃ¥kningssiden\n\n\n\n\n\n\n\n\n\n\n\n(b) Ã…pne Grafana-dashboard\n\n\n\n\n\n\n\nFigurÂ 11: Ã…pne Grafana-dashboard for kjÃ¸rende tjenester\n\n\n\nFigurÂ 12 viser hvordan et Grafana-dashboard ser ut.\n\n\n\n\n\n\nFigurÂ 12: Grafana dashbaordet for en spesifikk tjeneste pÃ¥ Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/dapla-lab.html#datatilgang",
    "href": "statistikkere/dapla-lab.html#datatilgang",
    "title": "Dapla Lab",
    "section": "Datatilgang",
    "text": "Datatilgang\nBrukere som skal ha tilgang til data fra en tjeneste mÃ¥ fÃ¸rst oppgi hvilket team og tilgangsgruppe2 de skal representere. Siden brukere ofte er medlem av flere team, sÃ¥ er dette et viktig sikkerhetstiltak for Ã¥ sikre at data ikke kobles pÃ¥ tvers av team uten at dette er godkjent av data-ansvarlige.\nFor Ã¥ fÃ¥ tilgang til lagringsbÃ¸ttene i prod-prosjektet til et team, sÃ¥ mÃ¥ man logge seg inn i prod-miljÃ¸et til Dapla Lab (https://lab.dapla.ssb.no/). Skal man ha tilgang til lagringsbÃ¸ttene i test-prosjektet til et team mÃ¥ man logge seg inn i test-miljÃ¸et til Dapla Lab (https://lab.dapla-test.ssb.no/). For team som har er et dev-miljÃ¸ sÃ¥ gjelder fÃ¸lgende dev-miljÃ¸et til Dapla Lab (https://lab.dapla-test.ssb.no/).\n\nBÃ¸tter som filsystem\nTjenestene i Dapla Lab gjÃ¸r teamets bÃ¸tter tilgjengelig som mapper i filsystemet i tjenesten. Det vil si at man kan referere til data som man er vant til pÃ¥ vanlige filsystem, og man kan bruke biblioteker uten Ã¥ autentisere seg mot bÃ¸tter.\nAlle tjenester som tilgjengeliggjÃ¸r data fra bÃ¸tter monterer filsystemet pÃ¥ stien /buckets/. Videre representeres bÃ¸ttene ved sitt kortnavn. F.eks. vil bÃ¸ttestien gs://ssb-dapla-felles-data-produkt-prod/ representeres som /buckets/produkt/ i tjenesten.\n\n\nJobbe med data\nSiden Dapla Lab tilbyr Ã¥ tilgjengliggjÃ¸re lagringsbÃ¸tter som filsystem inne i tjenesten, sÃ¥ finnes det nÃ¥ to mÃ¥ter Ã¥ aksessere data pÃ¥:\n\nBruke vanlige pakker som Pandas, Polars, Pyarrow, etc. mot filsystemet under /buckets/.\nDen â€œgamleâ€ mÃ¥ten med dapla-toolbelt som er et overbygg over Pandas og Pyarrow3.\n\nDet er anbefalt at alle benytter seg av alternativ 1 siden det er enklere for de fleste og gjÃ¸r at alle medlemmer av et team kan se hverandres endringer nÃ¥r man jobber mot samme bÃ¸tte (se boks under).\n\n\n\n\n\n\nEksterne endringer i bÃ¸tter\n\n\n\nHvis to brukere Ã¥pner en tjeneste med den samme team og tilgangsgruppe, sÃ¥ vil man kun se hverandres endringer i filsystemet4 hvis begge jobber direkte mot buckets-filstien. Hvis en av de skriver filer med dapla-toolbelt, mens den andre bruker dapla-toolbelt eller et annet verktÃ¸y, sÃ¥ vil man ikke se dette i filsystemet inne i tjenesten. Brukeren kan da kjÃ¸re refresh-buckets fra terminalen i tjenesten for Ã¥ se hva som har dukket opp. Vi anbefaler derfor alle Ã¥ bruke buckets-tilnÃ¦rmingen.\nHvis brukeren refererer til en fil som finnes i bÃ¸tta, men som ikke synes i filsystemet, sÃ¥ vil det fortsatt kunne leses inn. Dette gjelder ogsÃ¥ for filer produsert av Kildomaten. Fremover kommer vi til Ã¥ tilpasse K",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/dapla-lab.html#miljÃ¸er",
    "href": "statistikkere/dapla-lab.html#miljÃ¸er",
    "title": "Dapla Lab",
    "section": "MiljÃ¸er",
    "text": "MiljÃ¸er\nDet finnes 2 adskilte miljÃ¸er for Dapla Lab: prod og test. TabellÂ 1 viser hvilke url-er som gjelder for de ulike miljÃ¸ene.\n\n\n\nTabellÂ 1: Oversikt over miljÃ¸er og tilhÃ¸rende url-er for Dapla Lab.\n\n\n\n\n\nMiljÃ¸\nUrl\n\n\n\n\nProd\nhttps://lab.dapla.ssb.no/\n\n\nTest\nhttps://lab.dapla-test.ssb.no/\n\n\n\n\n\n\nMiljÃ¸ene er knyttet til datatilgang for prosjektene til Dapla-team. Hvert Dapla-team kan ha ressurser i prod- eller test-miljÃ¸et. For Ã¥ fÃ¥ tilgang til ressursene i et av miljÃ¸ene mÃ¥ de logge seg inn pÃ¥ tilsvarende miljÃ¸ i Dapla Lab. Det er f.eks. ikke mulig Ã¥ aksessere prod-data fra test-miljÃ¸et i Dapla Lab og omvendt.",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/dapla-lab.html#automatisk-pausing",
    "href": "statistikkere/dapla-lab.html#automatisk-pausing",
    "title": "Dapla Lab",
    "section": "Automatisk pausing",
    "text": "Automatisk pausing\nHver dag pauses alle tjenester automatisk hver hele time mellom kl. 17:00 - 05.00. Det gjÃ¸res for redusere ressursbruken og dermed kostnader. Man kan unnta en tjeneste fra Ã¥ bli automatisk pauset, men da har brukeren selv ansvar for Ã¥ pause eller slette tjenesten nÃ¥r den ikke er i bruk.\nMan kan nÃ¥r som helst, ogsÃ¥ etter at tjenesten er startet, unnta en tjeneste fra den automatiske pausingen ved Ã¥ endre visningsnavnet til tjenesten. Legger man til [nosuspend] i visningsnavnet, slik som vist i FigurÂ 13, sÃ¥ vil tjenesten aldri bli pauset.\n\n\n\n\n\n\nFigurÂ 13: Eksempel pÃ¥ en tjeneste som ikke blir pauset hver dag kl. 17.\n\n\n\nMan endrer visningsnavnet til en tjeneste ved Ã¥ trykke pÃ¥ ğŸ–Šï¸-ikonet og skrive inn et nytt navn.",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/dapla-lab.html#e-postvarsling",
    "href": "statistikkere/dapla-lab.html#e-postvarsling",
    "title": "Dapla Lab",
    "section": "E-postvarsling",
    "text": "E-postvarsling\nHver mandag morgen blir brukere av Dapla Lab varslet pÃ¥ e-post om hvilke de tjenester de har som ble startet for mer enn 7 dager siden. Det sendes en e-post per bruker per miljÃ¸. Hvis man ikke har noen tjenester som tilfredstiller kriteriene, sÃ¥ mottar man ingen e-post.\nFormÃ¥let med varslingen er Ã¥ informere brukeren om at det ikke er anbefalt Ã¥ la tjenestene leve for lenge siden det betyr at man trolig ikke kjÃ¸rer pÃ¥ siste versjon av tjenesten. Det betyr igjen at brukeren kan gÃ¥ glipp av viktige oppdateringer eller forbedringer som er blitt gjort.",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/dapla-lab.html#footnotes",
    "href": "statistikkere/dapla-lab.html#footnotes",
    "title": "Dapla Lab",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nBrukere har sjelden behov for Ã¥ endre versjon her.â†©ï¸\nHvis en bruker er medlem i bÃ¥de data-admins- og developers-gruppa til et team, sÃ¥ mÃ¥ de velge hvilken av de to gruppene de skal representere i tjenesten som startes.â†©ï¸\ndapla-toolbelt er en pakke som ble bygget som et overbygg over Pandas og Pyarrow slik at det ble lettere Ã¥ lese/skrive mot bÃ¸tter. Med bÃ¸tter som filsystem inne i tjenestene er ikke dette lenger nÃ¸dvendig.â†©ï¸\nMed endringer menes her at man oppretter en ny mappeâ†©ï¸",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/standards-toolbelt.html",
    "href": "statistikkere/standards-toolbelt.html",
    "title": "Standards",
    "section": "",
    "text": "Modulen standards i dapla-toolbelt-metadata tilbyr metoder for Ã¥ sjekke om filer i bÃ¸tter er i trÃ¥d med SSBs definerte navnestandard. Metodene inkluderer:\nFor Ã¥ effektivisere validering av bÃ¸tter med store mengder filer, benytter metoden asynkronitet.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata",
      "Standards"
    ]
  },
  {
    "objectID": "statistikkere/standards-toolbelt.html#funksjonalitet",
    "href": "statistikkere/standards-toolbelt.html#funksjonalitet",
    "title": "Standards",
    "section": "Funksjonalitet",
    "text": "Funksjonalitet\nstandards tilbyr to typer av funksjonalitet. Den fÃ¸rste er Ã¥ sjekke om en bÃ¸tte, mappe eller fil fÃ¸lger standarden. Den andre er Ã¥ produsere en rapport som oppsummerer resultatet av valideringen.\n\nValidering\nFor Ã¥ sjekke om en bÃ¸tte, mappe eller fil fÃ¸lger navnestandarden kan man benytte funksjonen check_naming_standard(). Den returnerer en liste med resultater for alle objektene du har bedt om Ã¥ fÃ¥ validert.\n\n\nNotebook\n\nfrom dapla_metadata.standards.standard_validators import check_naming_standard\n\nresults = await check_naming_standard(\"&lt;bÃ¸ttenavn/mappe/filsti&gt;\")\nresults\n\n\n\n\n\n\n\nOutput fra validering av enkeltfil\n\n\n\n\n\nValidationResult(\n    success=False, \n    file_path=\"/buckets/produkt/stat/inndata/bil_v1.parquet\", \n    messages=[\n        \"Det er oppdaget brudd pÃ¥ SSB-navnestandard:\"\n    ], \n    violations=[\n        \"Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\"\n    ]\n)\n\n\n\nSiden metoden bruker asynkrone kall, mÃ¥ nÃ¸kkelordet await brukes foran metodenavnet.\nFor Ã¥ fÃ¥ tilgang til spesifikke deler av resultatet, kan du bruke punktnotasjon .. Hvis du for eksempel Ã¸nsker Ã¥ hente ut listen med regelbrudd fra det fÃ¸rste valideringsresultatet, kan du gjÃ¸re fÃ¸lgende:\nresults[0].violations\nHvis du har validert et stort antall filer sÃ¥ kan du benytte fÃ¸lgende kode for Ã¥ fÃ¥ ut resultatene pÃ¥ en mer lesbar form:\n\n\nNotebook\n\nviolations = [r for r in results if not r.success]\n\nif not violations:\n    print(\"Gratulerer, ingen feil Ã¥ vise\")\nelse:\n    for v in violations:\n        print(v.file_path)\n        print(\"\\t\" + \"\\n\\t\".join(v.messages))\n        print(\"\\t\\t\" + \"\\n\\t\\t\".join(v.violations) + \"\\n\")\n\n\n\n\n\n\n\nEksempel pÃ¥ output fra validering av mange filer\n\n\n\n\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/butikker_kongsvinger.parquet\n    Det er oppdaget brudd pÃ¥ SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/butikkbygg_kongsvinger.parquet\n    Det er oppdaget brudd pÃ¥ SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/butikkbygg_oslo.parquet\n    Det er oppdaget brudd pÃ¥ SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/noen_boliger_kongsvinger.parquet\n    Det er oppdaget brudd pÃ¥ SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/noen_boliger_oslo.parquet\n    Det er oppdaget brudd pÃ¥ SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/butikker_oslo.parquet\n    Det er oppdaget brudd pÃ¥ SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/noen_boligbygg_oslo.parquet\n    Det er oppdaget brudd pÃ¥ SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/noen_boligbygg_kongsvinger.parquet\n    Det er oppdaget brudd pÃ¥ SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/NVE_Trafostasjon_punkt_p2023.parquet\n    Det er oppdaget brudd pÃ¥ SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/enkle_kommuner.parquet\n    Det er oppdaget brudd pÃ¥ SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/bygg_kongsvinger.parquet\n    Det er oppdaget brudd pÃ¥ SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/veger_kongsvinger.parquet\n    Det er oppdaget brudd pÃ¥ SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/noen_tettsteder_2023.parquet\n    Det er oppdaget brudd pÃ¥ SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/bygg_oslo.parquet\n    Det er oppdaget brudd pÃ¥ SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/veger_oslo.parquet\n    Det er oppdaget brudd pÃ¥ SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/ABAS_kommune_flate_p2024_v1.parquet\n    Det er oppdaget brudd pÃ¥ SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/SSB_tettsted_flate_p2022_v1.parquet\n    Det er oppdaget brudd pÃ¥ SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/SSB_tettsted_flate_p2023_v1.parquet\n    Det er oppdaget brudd pÃ¥ SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n\n\n\n\n\nRapport\nHvis du Ã¸nsker en kort oppsummering og vurdering av resultatet, kan du importere fÃ¸lgende metode:\n\n\nNotebook\n\nfrom dapla_metadata.standards.standard_validators import generate_validation_report\n\nreport = generate_validation_report(results)\n\nMetoden tar en liste med valideringsresultater som input:\nOg hvis alt ser bra ut:\n\nFor Ã¥ fÃ¥ tilgang til spesifikke deler av rapporten, kan du bruke punktnotasjon .. Hvis du for eksempel Ã¸nsker Ã¥ hente ut kun suksess raten i prosent, kan du gjÃ¸re fÃ¸lgende:\nreport.success_rate()\nEller hvis du Ã¸nsker direkte tilgang til tallene:\nreport.num_files_validated\nreport.num_success\nreport.num_failures\n\n\n\n\n\n\nBruk av validering i statistikkproduksjon\n\n\n\nHvis man Ã¸nsker Ã¥ benytte valideringsfunksjonaliteten i koden som kjÃ¸res i en statistikkproduksjon, sÃ¥ kan pre-commit hooks feile pÃ¥ grunn av nÃ¸kkelordet await benyttes utenfor en asynkron funksjon. En enkel lÃ¸sning er Ã¥ legge til # noqa: F704 pÃ¥ samme linje som await, slik:\n\n\nNotebook\n\nresults = await check_naming_standard(\"\") # noqa: F704",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata",
      "Standards"
    ]
  },
  {
    "objectID": "statistikkere/standards-toolbelt.html#footnotes",
    "href": "statistikkere/standards-toolbelt.html#footnotes",
    "title": "Standards",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nNÃ¥r man validerer en filsti som ikke eksisterer sÃ¥ fÃ¥r man beskjed om at Filen eksisterer ikke. Validerer uansett..â†©ï¸",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata",
      "Standards"
    ]
  },
  {
    "objectID": "statistikkere/vardef.html",
    "href": "statistikkere/vardef.html",
    "title": "Vardef",
    "section": "",
    "text": "Vardef er SSBs system for dokumentasjon av variabler. Hensikten med Vardef er at alle variabler i SSB skal dokumenteres og oppdateres ett sted (av ansvarlig Dapla-team) og gjenbrukes av alle som har behov for dem.\nVardef skal inneholde all nÃ¸dvendig informasjon om en variabel. Mens Datadoc dokumenterer variabelforekomstene, dvs. enkeltvariablene i et datasett1, skal Vardef dokumentere de mer overordnede beskrivelsene av en variabel. I Vardef er det variabler som gjenbrukes, enten pÃ¥ tvers i SSB, eller flere ganger over tid i samme statistikk, som skal dokumenteres. Et eksempel er f.eks. variabelen Â«organisasjonsnummerÂ» som vil vÃ¦re definert i Vardef, slik at beskrivelsen kan gjenbrukes i alle datafiler der variabelforekomsten organisasjonsnummer inngÃ¥r. I SSB har ofte organisasjonsnummer ulike navn i ulike datasett , men nÃ¥r alle variablene er knyttet til samme Vardef-variabel, kan brukerne likevel forstÃ¥ at det er samme variabel.\nI Datadoc har en mulighet for Ã¥ presisere en variabel som refereres til i Vardef. Vi kan f.eks. ha variabelen Â«YrkesinntektÂ» som brukes bÃ¥de i datasett A og B. Selve definisjonen hentes da fra Vardef og er dermed den samme i begge datasett. Men sÃ¥ kan det vÃ¦re presiseringer en mÃ¥ gjÃ¸re i Datadoc for hver variabelforekomst, f.eks. at yrkesinntekt i datasett A mÃ¥les i kroner, mens den i datasett B mÃ¥les i 1000 kroner.\nDersom Vardef-variabelen er en kvalitativ variabel (har verdier som hentes fra et kodeverk), skal variabelen referere til tilhÃ¸rende kodeverk i Klass. F.eks. skal variabelen Â«SivilstandÂ» som har definisjonen Â«Variabelen viser en persons stilling ihht ekteskapslovgivningenÂ», referere til Â«Standard for sivilstandÂ» i Klass som viser hvilke verdier (kategorier) variabelen kan anta.",
    "crumbs": [
      "Manual",
      "Metadata",
      "Vardef"
    ]
  },
  {
    "objectID": "statistikkere/vardef.html#footnotes",
    "href": "statistikkere/vardef.html#footnotes",
    "title": "Vardef",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nDette er noe vi Ã¸nsker Ã¥ unngÃ¥ pÃ¥ Dapla, det er best for brukerne om samme variabelforekomst har samme navn i alle datasett der den brukes. Dette vil ikke vÃ¦re noe krav, men dersom variabelforekomstnavnet er det samme som kortnavnet til tilhÃ¸rende variabel i Vardef, vil variabelforekomsten i Datadoc kunne lenkes maskinelt til riktig variabeldefinisjon i Vardef. Dermed slipper en mye manuelt arbeid.â†©ï¸",
    "crumbs": [
      "Manual",
      "Metadata",
      "Vardef"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html",
    "href": "statistikkere/maskinporten-guardian.html",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "Maskinporten er en tjeneste fra Digdir som gir sikker autentisering og tilgangskontroll for datautveksling mellom virksomheter.\nTilgangsstyring for data administeres via et webgrensesnitt (Samarbeidsportalen) En virksomhet som deler data (API-tilbyder) definerer sÃ¥kalte API scopes, og velger hvilke andre virksomheter (API-konsumenter) som skal ha tilgang til disse. API-konsumenter kan pÃ¥ sin side selv opprette en eller flere Maskinporten-klienter og gir de tilgang til API scopes som er delt med virksomheten.\n\n\n\n\n\n\nFigurÂ 1: Datauveksling mellom virksomheter\n\n\n\nLes mer om Maskinporten her.\n\n\n\nUtveksling av data fra en API-tilbyder gjÃ¸res ved Ã¥ inkludere et sikkerhetstoken som hentes fra Maskinporten pÃ¥ vegne av virksomheten man representerer (f.Â eks SSB). Hvilke data (API scopes) et sikkerhetstoken har tilgang til er knyttet til Maskinporten-klienten. For Ã¥ hente et sikkerhetstoken for en klient, kreves det at man autentiserer seg som virksomhet. Dette gjÃ¸res ved Ã¥ signere forespÃ¸rsler til Maskinporten ved bruk av et virksomhetssertifikat. Det er her Maskinporten Guardian kommer inn i bildet.\nMaskinporten Guardian har tilgang til SSBs virksomhetssertifikat og kan dermed signere forespÃ¸rsler mot Maskinporten for Ã¥ hente ut sikkerhetstokens.\n\n\n\nHvis en skal ta i bruk et API som er beskyttet av Maskinporten er det noen steg som mÃ¥ gjÃ¸res i forkant:\n\n\n\n\n\nflowchart TD\n    A[API-tilbyder gir SSB tilgang til API via Samarbeidsportalen]\n    A --&gt; B[M2M-teamet hos SSB oppretter Maskinporten-klienter]\n    B --&gt; C[Keycloak-klienter for Maskinporten Guardian opprettes]\n\n\n\n\n\n\n\nAPI-tilbyderen mÃ¥ gi SSB tilgang til et API scope. API-tilbydere kan gjÃ¸re dette via Samarbeidsportalen hos Digdir. Fra API-tilbyderen fÃ¥r du ogsÃ¥ annen informasjon om API-ene, som:\n\n\nURL-er som skal benyttes for Ã¥ snakke med API-et\nNavn pÃ¥ API scopes\nOm det finnes testadata\nDokumentasjon for API-endepunktene\n\n\nNÃ¥r data er delt av en API-tilbyder, og en har navnet pÃ¥ API scopes, kan M2M-teamet hos SSB kontaktes for Ã¥ fÃ¥ opprettet Maskinporten-klienter, Ã©n pr miljÃ¸ (f.Â eks prod og test). De mÃ¥ vite hvilke API scopes og miljÃ¸er (test/prod) som skal benyttes. M2M-teamet vil gi deg ID-er (f.Â eks 12345678-9abc-def0-1234-567890abcdef) for klientene som er blitt opprettet.\n\n\n\n\n\n\n\nDu kan anse ID for en Maskinporten-klient som et slags brukernavn, og behandle dette deretter. Slike ID-er er ikke sensitive i seg selv, men de bÃ¸r allikevel ikke ligge i Ã¥pne git-repoer (SSB-private repoer er OK). Det er heller ingen grunn til Ã¥ behandle disse som hemmeligheter. Det anbefales at man opererer med Maskinporten-klienter som ekstern konfigurasjon til koden, f.Â eks pÃ¥ samme mÃ¥te som man behandler URL-er til API-ene.\n\n\n\n\nNÃ¥r du har ID for Maskinporten-klienten(e), er neste steg Ã¥ fÃ¥ opprettet en Keycloak-bruker for Maskinporten Guardian. Se Opprette en Maskinporten Guardian M2M-bruker. Ta kontakt med oss pÃ¥ Dapla via Pureservice dersom du trenger hjelp til dette. Client ID og client secret for Keycloak-brukeren kan hentes fra Secret Manager (mer om det lenger ned). Medlemmer av Dapla-teamet har tilgang til Ã¥ hente disse.\n\nOm noen pÃ¥ teamet trenger personlig tilgang til API-ene sÃ¥ mÃ¥ det konfigureres i Maskinporten Guardian. Da mÃ¥ vi vite hvilke personer som skal ha denne tilgangen. Les mer om forskjellen pÃ¥ M2M og personlig tilgang lenger ned.\n\n\n\nDet er M2M-teamet i SSB som administrerer det formelle i forbindelse med integrasjoner mot eksterne API-er. For Ã¥ fÃ¥ opprettet nye Maskinporten-klienter er det dette teamet man kontakter. De mÃ¥ vite hvilke miljÃ¸er (test og/eller prod) og hvilke API scopes som skal benyttes. Hver Maskinporten-klient som opprettes identifiseres av en ID (f.Â eks 12345678-9abc-def0-1234-567890abcdef), som du vil fÃ¥ tilsendt.\nLegg merke til at det er SSB selv som oppretter og administrerer Maskinporten-klienter. Det gjÃ¸res i Samarbeidsportalen hos Difi. Klientene knyttes til API scope(s) (som er delt med SSB av API-tilbyderen). Du kan lese mer om hvordan M2M-teamet administrerer Maskinporten-klienter her.\n\n\n\n\n\n\nAlle med en SSB-epostadresse kan registerere en personlig bruker i Digdir Samarbeidsportalen for Ã¥ fÃ¥ innsyn i hvilke API-integrasjoner som finnes.\n\n\n\n\n\n\nMaskinporten Guardian er tilgjengelig fra alle SSB og NAIS sine IP-adresser, og kan nÃ¥s pÃ¥:\n\nProd: https://guardian.intern.ssb.no\nTest: https://guardian.intern.test.ssb.no\n\nMaskinporten Guardian sine endepunkter er selv beskyttet av Keycloak. To typer brukere stÃ¸ttes:\n\nMaskin-til-maskin (M2M) - Systembruker knyttet til en gitt Maskinporten-klient. For Ã¥ opptre pÃ¥ vegne av en M2M-bruker autentiserer man seg med en Keycloak client secret. Denne hemmeligheten er lagret i Google Secret Manager og kun Service Accounts eller Dapla-grupper med tilgang kan hente den ut.\nPersonlig - Din egen SSB-bruker. Man kan f.Â eks bruke dapla-toolbelt for Ã¥ hente ut sitt personlige Keycloak-token. I tillegg til Ã¥ autentisere deg mÃ¥ din bruker vÃ¦re autorisert til Ã¥ gjÃ¸re oppslag pÃ¥ vegne av en Maskinporten-klient. Dette styres i konfigurasjonen til Maskinporten Guardian.\n\n\n\n\n\n\n\nMan skal i hovedsak kun anvende M2M-brukere for datautveksling mot API-er som er beskyttet av Maskinporten. Personlige brukere skal kun brukes unntaksvis for enkeltoppslag (f.Â eks ved feilsÃ¸king) mot API-er eller for utvikling og test.\n\n\n\nLegg merke til at Maskinporten Guardian kun er tilgjengelig fra interne SSB-adresser. Bruk fÃ¸lgende URL-er: * Prod: https://guardian.intern.ssb.no * Test: https://guardian.intern.test.ssb.no\n\n\n\nFÃ¸lgende gir en oversikt over hvordan systemer henger sammen. En API-konsument kan f.Â eks vÃ¦re et Dapla-team som Ã¸nsker Ã¥ hente data, mens en API-tilbyder er en ekstern virksomhet som tilbyr data via Maskinporten. Det er noen forskjeller i flyt avhengig av om Maskinporten Guardian aksesseres med systembruker (M2M) eller personlig bruker.\n\n\n\n\n\n\nFigurÂ 2\n\n\n\n\n\n\nAPI-konsumenten henter sin Keycloak client secret for en gitt Maskinporten-klient fra Secret Manager.\nAPI-konsumenten henter et Keycloak sikkerhetstoken ved Ã¥ bruke client secret fra steg 1.\nAPI-konsumenten henter et Maskinporten sikkerhetstoken fra Maskinporten Guardian ved Ã¥ bruke Keycloak sikkerhetstoken fra steg 2. En kan alternativt angi andre API scopes enn det som er standard for Maskinporten-klienten, men dette er vanligvis ikke nÃ¸dvendig.\nMaskinporten Guardian bruker SSB sitt virksomhetssertifikat til Ã¥ signere en forespÃ¸rsel om Ã¥ hente et sikkerhetstoken fra Maskinporten (ref steg 3)\nAPI-konsumenten henter data fra den eksterne API-tilbyderen ved Ã¥ bruke sikkerhetstoken fra Maskinporten.\n\n\n\n\n\n\n\n\n\n\nSteg 1 og 2 gjelder kun for M2M-brukere. Dersom man aksesserer Maskinporten Guardian med personlig bruker sÃ¥ hentes Keycloak-tokenet f.Â eks ved hjelp av AuthClient i dapla-toolbelt.\n\n\n\n\nAPI-konsumenten henter et Maskinporten sikkerhetstoken fra Maskinporten Guardian ved Ã¥ bruke sitt personlige Keycloak sikkerhetstoken. Det mÃ¥ angis hvilken Maskinporten-klient og hvilke API scopes Maskinporten sikkerhetstokenet skal gjelde for. Den personlige brukeren mÃ¥ pÃ¥ forhÃ¥nd vÃ¦re autorisert (ref Maskinporten Guardian sin tilgangskonfigurasjon) til Ã¥ kunne hente sikkerthetstokens for Maskinporten-klienten.\nMaskinporten Guardian bruker SSB sitt virksomhetssertifikat til Ã¥ signere en forespÃ¸rsel om Ã¥ hente et sikkerhetstoken fra Maskinporten (ref steg 3)\nAPI-konsumenten henter data fra den eksterne API-tilbyderen ved Ã¥ bruke sikkerhetstoken fra Maskinporten.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDette avsnittet inneholder tekniske instrukser ment for deg som er kjent med Git og som f.Â eks jobber i et Self-managed Dapla-team. Ta kontakt med oss via Pureservice sÃ¥ hjelper vi deg gjerne med dette.\n\n\nFor Ã¥ kunne opptre pÃ¥ vegne av Maskinporten-klienten uavhengig av din personlige bruker, mÃ¥ man opprette en Keycloak systembruker. Det gjÃ¸res ved Ã¥ Ã¥pne en Pull Request (konfigurasjon som gjennomgÃ¥s av en tekniker) til keycloak-iac der du angir informasjon som ID for Maskinporten-klient, API scopes og hvem som skal ha tilgang. Legg merke til at du mÃ¥ opprette en klient pr miljÃ¸ (test og prod). Du kan se bort fra play-miljÃ¸et.\n\n\namends \".../pkl/MaskinportenGuardianClient.pkl\"\n\napi_shortname = \"Kort API-beskrivelse (maks 32 tegn)\"\nmaskinporten_client_id = \"12345678-9abc-def0-1234-567890abcdef\"\nmaskinporten_audience = \"https://maskinporten.no/\"\nmaskinporten_default_scopes {\n  \"foo:data1\"\n  \"foo:data2\"\n}\ncredentials_access {\n  \"group:play-foeniks-data-admins@groups.ssb.no\"\n  \"serviceaccount:foo-sa@play-foeniks-p-ab.iam.gserviceaccount.com\"\n}\n\n\n\n\n\n\nNote\n\n\n\nI test skal maskinporten_audience ha verdien https://test.maskinporten.no/. I prod skal det vÃ¦re https://maskinporten.no/ (merk: skrÃ¥strek pÃ¥ slutten er viktig)\n\n\nPull Requesten mÃ¥ godkjennes og behandles av en Dapla platformutvikler. NÃ¥r dette er gjort blir det opprettet en Keycloak-klient, og hemmeligheten som kan brukes for Ã¥ hente ut sikkerhetstokens for denne klienten er tilgjengelig i Secret Manager.\nSe fÃ¸lgende dokumentasjon for mer informasjon:\n\nDetaljert beskrivelse av konfigurasjonsmuligeter.\nGenerell beskrivelse av hvordan man oppretter en Keycloak-klient\nKeycloak client credentials\n\nTa kontakt med Kundeservice hvis du har spÃ¸rsmÃ¥l eller trenger ei hand Ã¥ halde i.\n\n\n\n\nFÃ¸lgende viser Python kodeeksempler for hvordan man kan hente ut et Maskinporten sikkerhetstoken.\n\n\n\"\"\"\nRetrieve maskinporten M2M access token for querying an external API.\n\nThe following code example expects a toml config file to exist named\n&lt;team_uniform_name&gt;-&lt;env&gt;-maskinporten-m2m.toml (e.g. play-foeniks-prod-maskinporten-m2m.toml)\n\nwith contents such as:\n\nkeycloak_url = \"https://auth.test.ssb.no\"\nkeycloak_clients_gcp_project_id = \"keycloak-clients-&lt;p|t&gt;-??\"\nguardian_url = \"https://guardian.intern.test.ssb.no\"\n\n[my-api]\n    maskinporten_client_id = \"12345678-9abc-def0-1234-567890abcdef\"\n\"\"\"\nimport os\nimport re\nimport requests\nimport toml\n\n\nteam_uniform_name = \"play-foeniks\"\napi_name = \"my-api\" # identifies the API in the config.toml file\n\n# Check which environment we're running and load corresponding config\ndapla_env = os.getenv(\"DAPLA_ENVIRONMENT\").lower()\nconfig = toml.load(f\"{team_uniform_name}-{dapla_env}-maskinporten-m2m.toml\")\n\n# Get Maskinporten Guardian M2M Keycloak client credentials from Google Secret Manager\n# The secret's name is deduced from the team name and maskinporten client id\nname = f\"{team_uniform_name}-ssb-maskinporten-{config[api_name]['maskinporten_client_id']}-credentials\"\nsecret = get_secret_version(project_id=config['keycloak_clients_gcp_project_id'],\n                            shortname=name)\n\n# The credentials are stored as yaml. Here we simply use a regex to parse. \nkeycloak_client_id = re.search(r'\"client_id\": \"(.*)\"', secret).group(1)\nkeycloak_client_secret = re.search(r'\"client_secret\": \"(.*)\"', secret).group(1)\n\n# Get Keycloak access token\n# This token includes custom claims with values such as maskinporten_default_scopes\nresponse = requests.post(f\"{config['keycloak_url']}/realms/ssb/protocol/openid-connect/token\",\n    headers={\n        \"Content-type\": \"application/x-www-form-urlencoded\",\n    },\n    auth=(keycloak_client_id, keycloak_client_secret),\n    data={\"grant_type\": \"client_credentials\"}\n)\nkeycloak_access_token = response.json()['access_token']\n\n# Get Maskinporten access token from Maskinporten Guardian (using the Keycloak token from above)\n# Note that you can specify custom scopes in the request body if you need to. Using defaults defined in the client config if not specified.\nrequest_body={}\nresponse = requests.post(f\"{config['guardian_url']}/maskinporten/access-token\",\n            headers={\n                \"Authorization\": f\"Bearer {keycloak_access_token}\",\n                \"Content-type\": \"application/json\",\n            },\n            json=request_body,\n        )\nmaskinporten_access_token = response.json()['accessToken']\n\n# Then use the maskinporten access token to query the external API...\nStÃ¸ttefunksjon for Ã¥ hente ut secrets fra Secret Manager\nfrom dapla import AuthClient\nfrom google.cloud import secretmanager\n\ndef get_secret_version(project_id, shortname, version_id='latest'):\n    \"\"\"\n    Access the payload for a given secret version.\n    The user's google credentials are used to authorize that the user have permission\n    to access the secret_id.\n    \n    Args:\n    - project_id (str): ID of the Google Cloud project where the secret is stored.\n    - shortname (str): Name (not full path) of the secret in Secret Manager.\n    - version_id (str, optional): The version of the secret to access. Defaults to 'latest'.\n\n    Returns:\n    - str: The payload of the secret version as a UTF-8 decoded string.\n    \"\"\"\n    client = secretmanager.SecretManagerServiceClient(credentials=AuthClient.fetch_google_credentials())\n    secret_name = f\"projects/{project_id}/secrets/{shortname}/versions/{version_id}\"\n    response = client.access_secret_version(name=secret_name)\n    return response.payload.data.decode(\"UTF-8\")\n\n\n\n\"\"\"\nRetrieve maskinporten personal access token for querying an external API.\n\nThe following code example expects a toml config file to exist named\n&lt;team_uniform_name&gt;-&lt;env&gt;-maskinporten-pers.toml (e.g. play-foeniks-prod-maskinporten-pers.toml)\n\nwith contents such as:\n\nguardian_url = \"https://guardian.intern.test.ssb.no\"\n\n[my-api]\n    maskinporten_client_id = \"12345678-9abc-def0-1234-567890abcdef\"\n    scopes = [\"some:scope1\", \"some:scope2\"]\n\"\"\"\nimport os\nimport requests\nimport toml\nfrom dapla import AuthClient\n\nteam_uniform_name = \"play-foeniks\"\napi_name = \"my-api\" # identifies the API in the config.toml file\n\n# Check which environment we're running and load corresponding config\ndapla_env = os.getenv(\"DAPLA_ENVIRONMENT\").lower()\nconfig = toml.load(f\"{team_uniform_name}-{dapla_env}-maskinporten-pers.toml\")\n\n# Get Keycloak access token\nkeycloak_access_token = AuthClient.fetch_personal_token()\n\n# Get Maskinporten access token from Maskinporten Guardian (using the personal Keycloak token from above)\nrequest_body = {\n  \"maskinportenClientId\": config[api_name]['maskinporten_client_id'],\n  \"scopes\": config[api_name]['scopes']\n}\nresponse = requests.post(f\"{config['guardian_url']}/maskinporten/access-token\",\n            headers={\n                \"Authorization\": f\"Bearer {keycloak_access_token}\",\n                \"Content-type\": \"application/json\",\n            },\n            json=request_body\n        )\n\nmaskinporten_access_token = response.json()['accessToken']\n\n# Finally, use the maskinporten access token to query the external API\nprint(maskinporten_access_token)",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#maskinporten",
    "href": "statistikkere/maskinporten-guardian.html#maskinporten",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "Maskinporten er en tjeneste fra Digdir som gir sikker autentisering og tilgangskontroll for datautveksling mellom virksomheter.\nTilgangsstyring for data administeres via et webgrensesnitt (Samarbeidsportalen) En virksomhet som deler data (API-tilbyder) definerer sÃ¥kalte API scopes, og velger hvilke andre virksomheter (API-konsumenter) som skal ha tilgang til disse. API-konsumenter kan pÃ¥ sin side selv opprette en eller flere Maskinporten-klienter og gir de tilgang til API scopes som er delt med virksomheten.\n\n\n\n\n\n\nFigurÂ 1: Datauveksling mellom virksomheter\n\n\n\nLes mer om Maskinporten her.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#hva-gjÃ¸r-maskinporten-guardian",
    "href": "statistikkere/maskinporten-guardian.html#hva-gjÃ¸r-maskinporten-guardian",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "Utveksling av data fra en API-tilbyder gjÃ¸res ved Ã¥ inkludere et sikkerhetstoken som hentes fra Maskinporten pÃ¥ vegne av virksomheten man representerer (f.Â eks SSB). Hvilke data (API scopes) et sikkerhetstoken har tilgang til er knyttet til Maskinporten-klienten. For Ã¥ hente et sikkerhetstoken for en klient, kreves det at man autentiserer seg som virksomhet. Dette gjÃ¸res ved Ã¥ signere forespÃ¸rsler til Maskinporten ved bruk av et virksomhetssertifikat. Det er her Maskinporten Guardian kommer inn i bildet.\nMaskinporten Guardian har tilgang til SSBs virksomhetssertifikat og kan dermed signere forespÃ¸rsler mot Maskinporten for Ã¥ hente ut sikkerhetstokens.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#komme-igang",
    "href": "statistikkere/maskinporten-guardian.html#komme-igang",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "Hvis en skal ta i bruk et API som er beskyttet av Maskinporten er det noen steg som mÃ¥ gjÃ¸res i forkant:\n\n\n\n\n\nflowchart TD\n    A[API-tilbyder gir SSB tilgang til API via Samarbeidsportalen]\n    A --&gt; B[M2M-teamet hos SSB oppretter Maskinporten-klienter]\n    B --&gt; C[Keycloak-klienter for Maskinporten Guardian opprettes]\n\n\n\n\n\n\n\nAPI-tilbyderen mÃ¥ gi SSB tilgang til et API scope. API-tilbydere kan gjÃ¸re dette via Samarbeidsportalen hos Digdir. Fra API-tilbyderen fÃ¥r du ogsÃ¥ annen informasjon om API-ene, som:\n\n\nURL-er som skal benyttes for Ã¥ snakke med API-et\nNavn pÃ¥ API scopes\nOm det finnes testadata\nDokumentasjon for API-endepunktene\n\n\nNÃ¥r data er delt av en API-tilbyder, og en har navnet pÃ¥ API scopes, kan M2M-teamet hos SSB kontaktes for Ã¥ fÃ¥ opprettet Maskinporten-klienter, Ã©n pr miljÃ¸ (f.Â eks prod og test). De mÃ¥ vite hvilke API scopes og miljÃ¸er (test/prod) som skal benyttes. M2M-teamet vil gi deg ID-er (f.Â eks 12345678-9abc-def0-1234-567890abcdef) for klientene som er blitt opprettet.\n\n\n\n\n\n\n\nDu kan anse ID for en Maskinporten-klient som et slags brukernavn, og behandle dette deretter. Slike ID-er er ikke sensitive i seg selv, men de bÃ¸r allikevel ikke ligge i Ã¥pne git-repoer (SSB-private repoer er OK). Det er heller ingen grunn til Ã¥ behandle disse som hemmeligheter. Det anbefales at man opererer med Maskinporten-klienter som ekstern konfigurasjon til koden, f.Â eks pÃ¥ samme mÃ¥te som man behandler URL-er til API-ene.\n\n\n\n\nNÃ¥r du har ID for Maskinporten-klienten(e), er neste steg Ã¥ fÃ¥ opprettet en Keycloak-bruker for Maskinporten Guardian. Se Opprette en Maskinporten Guardian M2M-bruker. Ta kontakt med oss pÃ¥ Dapla via Pureservice dersom du trenger hjelp til dette. Client ID og client secret for Keycloak-brukeren kan hentes fra Secret Manager (mer om det lenger ned). Medlemmer av Dapla-teamet har tilgang til Ã¥ hente disse.\n\nOm noen pÃ¥ teamet trenger personlig tilgang til API-ene sÃ¥ mÃ¥ det konfigureres i Maskinporten Guardian. Da mÃ¥ vi vite hvilke personer som skal ha denne tilgangen. Les mer om forskjellen pÃ¥ M2M og personlig tilgang lenger ned.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#administrasjon-av-api-integrasjoner-i-ssb",
    "href": "statistikkere/maskinporten-guardian.html#administrasjon-av-api-integrasjoner-i-ssb",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "Det er M2M-teamet i SSB som administrerer det formelle i forbindelse med integrasjoner mot eksterne API-er. For Ã¥ fÃ¥ opprettet nye Maskinporten-klienter er det dette teamet man kontakter. De mÃ¥ vite hvilke miljÃ¸er (test og/eller prod) og hvilke API scopes som skal benyttes. Hver Maskinporten-klient som opprettes identifiseres av en ID (f.Â eks 12345678-9abc-def0-1234-567890abcdef), som du vil fÃ¥ tilsendt.\nLegg merke til at det er SSB selv som oppretter og administrerer Maskinporten-klienter. Det gjÃ¸res i Samarbeidsportalen hos Difi. Klientene knyttes til API scope(s) (som er delt med SSB av API-tilbyderen). Du kan lese mer om hvordan M2M-teamet administrerer Maskinporten-klienter her.\n\n\n\n\n\n\nAlle med en SSB-epostadresse kan registerere en personlig bruker i Digdir Samarbeidsportalen for Ã¥ fÃ¥ innsyn i hvilke API-integrasjoner som finnes.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#hvordan-maskinporten-guardian",
    "href": "statistikkere/maskinporten-guardian.html#hvordan-maskinporten-guardian",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "Maskinporten Guardian er tilgjengelig fra alle SSB og NAIS sine IP-adresser, og kan nÃ¥s pÃ¥:\n\nProd: https://guardian.intern.ssb.no\nTest: https://guardian.intern.test.ssb.no\n\nMaskinporten Guardian sine endepunkter er selv beskyttet av Keycloak. To typer brukere stÃ¸ttes:\n\nMaskin-til-maskin (M2M) - Systembruker knyttet til en gitt Maskinporten-klient. For Ã¥ opptre pÃ¥ vegne av en M2M-bruker autentiserer man seg med en Keycloak client secret. Denne hemmeligheten er lagret i Google Secret Manager og kun Service Accounts eller Dapla-grupper med tilgang kan hente den ut.\nPersonlig - Din egen SSB-bruker. Man kan f.Â eks bruke dapla-toolbelt for Ã¥ hente ut sitt personlige Keycloak-token. I tillegg til Ã¥ autentisere deg mÃ¥ din bruker vÃ¦re autorisert til Ã¥ gjÃ¸re oppslag pÃ¥ vegne av en Maskinporten-klient. Dette styres i konfigurasjonen til Maskinporten Guardian.\n\n\n\n\n\n\n\nMan skal i hovedsak kun anvende M2M-brukere for datautveksling mot API-er som er beskyttet av Maskinporten. Personlige brukere skal kun brukes unntaksvis for enkeltoppslag (f.Â eks ved feilsÃ¸king) mot API-er eller for utvikling og test.\n\n\n\nLegg merke til at Maskinporten Guardian kun er tilgjengelig fra interne SSB-adresser. Bruk fÃ¸lgende URL-er: * Prod: https://guardian.intern.ssb.no * Test: https://guardian.intern.test.ssb.no",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#systemskisse",
    "href": "statistikkere/maskinporten-guardian.html#systemskisse",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "FÃ¸lgende gir en oversikt over hvordan systemer henger sammen. En API-konsument kan f.Â eks vÃ¦re et Dapla-team som Ã¸nsker Ã¥ hente data, mens en API-tilbyder er en ekstern virksomhet som tilbyr data via Maskinporten. Det er noen forskjeller i flyt avhengig av om Maskinporten Guardian aksesseres med systembruker (M2M) eller personlig bruker.\n\n\n\n\n\n\nFigurÂ 2\n\n\n\n\n\n\nAPI-konsumenten henter sin Keycloak client secret for en gitt Maskinporten-klient fra Secret Manager.\nAPI-konsumenten henter et Keycloak sikkerhetstoken ved Ã¥ bruke client secret fra steg 1.\nAPI-konsumenten henter et Maskinporten sikkerhetstoken fra Maskinporten Guardian ved Ã¥ bruke Keycloak sikkerhetstoken fra steg 2. En kan alternativt angi andre API scopes enn det som er standard for Maskinporten-klienten, men dette er vanligvis ikke nÃ¸dvendig.\nMaskinporten Guardian bruker SSB sitt virksomhetssertifikat til Ã¥ signere en forespÃ¸rsel om Ã¥ hente et sikkerhetstoken fra Maskinporten (ref steg 3)\nAPI-konsumenten henter data fra den eksterne API-tilbyderen ved Ã¥ bruke sikkerhetstoken fra Maskinporten.\n\n\n\n\n\n\n\n\n\n\nSteg 1 og 2 gjelder kun for M2M-brukere. Dersom man aksesserer Maskinporten Guardian med personlig bruker sÃ¥ hentes Keycloak-tokenet f.Â eks ved hjelp av AuthClient i dapla-toolbelt.\n\n\n\n\nAPI-konsumenten henter et Maskinporten sikkerhetstoken fra Maskinporten Guardian ved Ã¥ bruke sitt personlige Keycloak sikkerhetstoken. Det mÃ¥ angis hvilken Maskinporten-klient og hvilke API scopes Maskinporten sikkerhetstokenet skal gjelde for. Den personlige brukeren mÃ¥ pÃ¥ forhÃ¥nd vÃ¦re autorisert (ref Maskinporten Guardian sin tilgangskonfigurasjon) til Ã¥ kunne hente sikkerthetstokens for Maskinporten-klienten.\nMaskinporten Guardian bruker SSB sitt virksomhetssertifikat til Ã¥ signere en forespÃ¸rsel om Ã¥ hente et sikkerhetstoken fra Maskinporten (ref steg 3)\nAPI-konsumenten henter data fra den eksterne API-tilbyderen ved Ã¥ bruke sikkerhetstoken fra Maskinporten.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#opprette-en-maskinporten-guardian-m2m-bruker",
    "href": "statistikkere/maskinporten-guardian.html#opprette-en-maskinporten-guardian-m2m-bruker",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "Note\n\n\n\nDette avsnittet inneholder tekniske instrukser ment for deg som er kjent med Git og som f.Â eks jobber i et Self-managed Dapla-team. Ta kontakt med oss via Pureservice sÃ¥ hjelper vi deg gjerne med dette.\n\n\nFor Ã¥ kunne opptre pÃ¥ vegne av Maskinporten-klienten uavhengig av din personlige bruker, mÃ¥ man opprette en Keycloak systembruker. Det gjÃ¸res ved Ã¥ Ã¥pne en Pull Request (konfigurasjon som gjennomgÃ¥s av en tekniker) til keycloak-iac der du angir informasjon som ID for Maskinporten-klient, API scopes og hvem som skal ha tilgang. Legg merke til at du mÃ¥ opprette en klient pr miljÃ¸ (test og prod). Du kan se bort fra play-miljÃ¸et.\n\n\namends \".../pkl/MaskinportenGuardianClient.pkl\"\n\napi_shortname = \"Kort API-beskrivelse (maks 32 tegn)\"\nmaskinporten_client_id = \"12345678-9abc-def0-1234-567890abcdef\"\nmaskinporten_audience = \"https://maskinporten.no/\"\nmaskinporten_default_scopes {\n  \"foo:data1\"\n  \"foo:data2\"\n}\ncredentials_access {\n  \"group:play-foeniks-data-admins@groups.ssb.no\"\n  \"serviceaccount:foo-sa@play-foeniks-p-ab.iam.gserviceaccount.com\"\n}\n\n\n\n\n\n\nNote\n\n\n\nI test skal maskinporten_audience ha verdien https://test.maskinporten.no/. I prod skal det vÃ¦re https://maskinporten.no/ (merk: skrÃ¥strek pÃ¥ slutten er viktig)\n\n\nPull Requesten mÃ¥ godkjennes og behandles av en Dapla platformutvikler. NÃ¥r dette er gjort blir det opprettet en Keycloak-klient, og hemmeligheten som kan brukes for Ã¥ hente ut sikkerhetstokens for denne klienten er tilgjengelig i Secret Manager.\nSe fÃ¸lgende dokumentasjon for mer informasjon:\n\nDetaljert beskrivelse av konfigurasjonsmuligeter.\nGenerell beskrivelse av hvordan man oppretter en Keycloak-klient\nKeycloak client credentials\n\nTa kontakt med Kundeservice hvis du har spÃ¸rsmÃ¥l eller trenger ei hand Ã¥ halde i.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#kodeeksempler",
    "href": "statistikkere/maskinporten-guardian.html#kodeeksempler",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "FÃ¸lgende viser Python kodeeksempler for hvordan man kan hente ut et Maskinporten sikkerhetstoken.\n\n\n\"\"\"\nRetrieve maskinporten M2M access token for querying an external API.\n\nThe following code example expects a toml config file to exist named\n&lt;team_uniform_name&gt;-&lt;env&gt;-maskinporten-m2m.toml (e.g. play-foeniks-prod-maskinporten-m2m.toml)\n\nwith contents such as:\n\nkeycloak_url = \"https://auth.test.ssb.no\"\nkeycloak_clients_gcp_project_id = \"keycloak-clients-&lt;p|t&gt;-??\"\nguardian_url = \"https://guardian.intern.test.ssb.no\"\n\n[my-api]\n    maskinporten_client_id = \"12345678-9abc-def0-1234-567890abcdef\"\n\"\"\"\nimport os\nimport re\nimport requests\nimport toml\n\n\nteam_uniform_name = \"play-foeniks\"\napi_name = \"my-api\" # identifies the API in the config.toml file\n\n# Check which environment we're running and load corresponding config\ndapla_env = os.getenv(\"DAPLA_ENVIRONMENT\").lower()\nconfig = toml.load(f\"{team_uniform_name}-{dapla_env}-maskinporten-m2m.toml\")\n\n# Get Maskinporten Guardian M2M Keycloak client credentials from Google Secret Manager\n# The secret's name is deduced from the team name and maskinporten client id\nname = f\"{team_uniform_name}-ssb-maskinporten-{config[api_name]['maskinporten_client_id']}-credentials\"\nsecret = get_secret_version(project_id=config['keycloak_clients_gcp_project_id'],\n                            shortname=name)\n\n# The credentials are stored as yaml. Here we simply use a regex to parse. \nkeycloak_client_id = re.search(r'\"client_id\": \"(.*)\"', secret).group(1)\nkeycloak_client_secret = re.search(r'\"client_secret\": \"(.*)\"', secret).group(1)\n\n# Get Keycloak access token\n# This token includes custom claims with values such as maskinporten_default_scopes\nresponse = requests.post(f\"{config['keycloak_url']}/realms/ssb/protocol/openid-connect/token\",\n    headers={\n        \"Content-type\": \"application/x-www-form-urlencoded\",\n    },\n    auth=(keycloak_client_id, keycloak_client_secret),\n    data={\"grant_type\": \"client_credentials\"}\n)\nkeycloak_access_token = response.json()['access_token']\n\n# Get Maskinporten access token from Maskinporten Guardian (using the Keycloak token from above)\n# Note that you can specify custom scopes in the request body if you need to. Using defaults defined in the client config if not specified.\nrequest_body={}\nresponse = requests.post(f\"{config['guardian_url']}/maskinporten/access-token\",\n            headers={\n                \"Authorization\": f\"Bearer {keycloak_access_token}\",\n                \"Content-type\": \"application/json\",\n            },\n            json=request_body,\n        )\nmaskinporten_access_token = response.json()['accessToken']\n\n# Then use the maskinporten access token to query the external API...\nStÃ¸ttefunksjon for Ã¥ hente ut secrets fra Secret Manager\nfrom dapla import AuthClient\nfrom google.cloud import secretmanager\n\ndef get_secret_version(project_id, shortname, version_id='latest'):\n    \"\"\"\n    Access the payload for a given secret version.\n    The user's google credentials are used to authorize that the user have permission\n    to access the secret_id.\n    \n    Args:\n    - project_id (str): ID of the Google Cloud project where the secret is stored.\n    - shortname (str): Name (not full path) of the secret in Secret Manager.\n    - version_id (str, optional): The version of the secret to access. Defaults to 'latest'.\n\n    Returns:\n    - str: The payload of the secret version as a UTF-8 decoded string.\n    \"\"\"\n    client = secretmanager.SecretManagerServiceClient(credentials=AuthClient.fetch_google_credentials())\n    secret_name = f\"projects/{project_id}/secrets/{shortname}/versions/{version_id}\"\n    response = client.access_secret_version(name=secret_name)\n    return response.payload.data.decode(\"UTF-8\")\n\n\n\n\"\"\"\nRetrieve maskinporten personal access token for querying an external API.\n\nThe following code example expects a toml config file to exist named\n&lt;team_uniform_name&gt;-&lt;env&gt;-maskinporten-pers.toml (e.g. play-foeniks-prod-maskinporten-pers.toml)\n\nwith contents such as:\n\nguardian_url = \"https://guardian.intern.test.ssb.no\"\n\n[my-api]\n    maskinporten_client_id = \"12345678-9abc-def0-1234-567890abcdef\"\n    scopes = [\"some:scope1\", \"some:scope2\"]\n\"\"\"\nimport os\nimport requests\nimport toml\nfrom dapla import AuthClient\n\nteam_uniform_name = \"play-foeniks\"\napi_name = \"my-api\" # identifies the API in the config.toml file\n\n# Check which environment we're running and load corresponding config\ndapla_env = os.getenv(\"DAPLA_ENVIRONMENT\").lower()\nconfig = toml.load(f\"{team_uniform_name}-{dapla_env}-maskinporten-pers.toml\")\n\n# Get Keycloak access token\nkeycloak_access_token = AuthClient.fetch_personal_token()\n\n# Get Maskinporten access token from Maskinporten Guardian (using the personal Keycloak token from above)\nrequest_body = {\n  \"maskinportenClientId\": config[api_name]['maskinporten_client_id'],\n  \"scopes\": config[api_name]['scopes']\n}\nresponse = requests.post(f\"{config['guardian_url']}/maskinporten/access-token\",\n            headers={\n                \"Authorization\": f\"Bearer {keycloak_access_token}\",\n                \"Content-type\": \"application/json\",\n            },\n            json=request_body\n        )\n\nmaskinporten_access_token = response.json()['accessToken']\n\n# Finally, use the maskinporten access token to query the external API\nprint(maskinporten_access_token)",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html",
    "href": "statistikkere/datatilstander.html",
    "title": "Datatilstander",
    "section": "",
    "text": "En datatilstand er et resultat av at et datasett har gÃ¥tt gjennom gitte operasjoner og prosesser (Standardutvalget 2023, 5). Denne siden er ment som en kort innfÃ¸ring i de forskjellige datatilstandene. Siden er basert pÃ¥ det interne dokumentet Datatilstander SSB - 2. utgave. Definisjonene er direkte utdrag fra dette dokumentet. Se interndokumentet for en mer grundig gjennomgang av datatilstander i SSB.\nI SSB skiller vi mellom fem datatilstander:\nAlle datatilstander er obligatoriske bortsett fra inndata. FigurÂ 1 viser hvordan de forskjellige datatilstandene henger sammen.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Datatilstander"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#kildedata",
    "href": "statistikkere/datatilstander.html#kildedata",
    "title": "Datatilstander",
    "section": "Kildedata",
    "text": "Kildedata\nKildedata er data lagret slik de ble levert til SSB fra dataeier. Eksempler pÃ¥ kildedata er: grunndata, transaksjonsdata, administrative data, statistiske data og aggregerte data og rapporter (Standardutvalget 2023, 7). Kildedata lagres i bÃ¸tten ssb-&lt;teamnavn&gt;-data-kilde-prod. Les mer om bÃ¸tter her og lagringsstandarder her.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Datatilstander"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#inndata",
    "href": "statistikkere/datatilstander.html#inndata",
    "title": "Datatilstander",
    "section": "Inndata",
    "text": "Inndata\nInndata er kildedata som er transformert til SSBs standard lagringsformat (Standardutvalget 2023, 8). Denne transformeringer inkluderer blant annet at dataene skal benytte UTF-8 tegnsett. Les mer om SSBs standard lagringsformat her. Inndata kan ogsÃ¥ vÃ¦re andre statistikkers klargjorte data og/eller statistikkdata (Standardutvalget 2023, 8). Inndata er ikke en obligatorisk datatilstand. Inndata lagres i bÃ¸tten ssb-&lt;teamnavn&gt;-data-produkt-prod.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Datatilstander"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#klargjorte-data",
    "href": "statistikkere/datatilstander.html#klargjorte-data",
    "title": "Datatilstander",
    "section": "Klargjorte data",
    "text": "Klargjorte data\nKlargjorte data er inndata hvor:\n\nvariablene er beregnet gjennom utregninger og koblinger mellom datasett\nnÃ¸yaktigheten er forbedret\n\nfor eksempel som resultat av editering eller imputering\n\nmetadata med variabeldefinisjoner er lagt til.\n\nEnhver endring som er gjort skal vÃ¦re sporbare og dokumentert slik at statistikkene skal vÃ¦re etterprÃ¸vbare. Klargjorte date er som regel ikke aggregerte - med mindre dataen vi mottar er aggregert. Med andre ord inneholder klargjorte data oftest enkeltobservasjoner - i likhet med kildedata og inndata (Standardutvalget 2023, 9). Klargjorte data lagres i bÃ¸tten ssb-&lt;teamnavn&gt;-data-produkt-prod.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Datatilstander"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#statistikk",
    "href": "statistikkere/datatilstander.html#statistikk",
    "title": "Datatilstander",
    "section": "Statistikk",
    "text": "Statistikk\nStatistikk er â€œTallfestede opplysninger om en gruppe eller et fenomen, og som kommer frem ved en sammenstilling og bearbeidelse av opplysninger om de enkelte enhetene i gruppen eller et utvalg av disse enhetene, eller ved systematisk observasjon av fenomenetâ€ ifÃ¸lge statistikkloven Â§ 3a (Standardutvalget 2023, 10). Statistikk lagres i bÃ¸tten ssb-&lt;teamnavn&gt;-data-produkt-prod.\nStatistikk er ofte aggregerte data eller estimerte stÃ¸rrelser. Vi skiller mellom ujustert statistikk og justert statistikk. Indekser og sesongjusterte tall er eksempler pÃ¥ justert statistikk (Standardutvalget 2023, 10).\nStatistikk kan vÃ¦re inndata til andre statistikker, og kan dermed inneholde konfidensielle og detaljerte data som ikke publiseres.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Datatilstander"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#utdata",
    "href": "statistikkere/datatilstander.html#utdata",
    "title": "Datatilstander",
    "section": "Utdata",
    "text": "Utdata\nUtdata er statistikk der kravene til konfidensialtet er ivaretatt. Dette er datatilstanden som publiseres. Eksempler inkluderer: statistikkbanktabeller, tabelloppdrag og internasjonal rapportering (Standardutvalget 2023, 11). Utdata lagres i bÃ¸tten ssb-&lt;teamnavn&gt;-data-produkt-produkt.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Datatilstander"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#metadata-for-datatilstandene",
    "href": "statistikkere/datatilstander.html#metadata-for-datatilstandene",
    "title": "Datatilstander",
    "section": "Metadata for datatilstandene",
    "text": "Metadata for datatilstandene\nDet er forskjellige forventinger til metadata for de ulike datatilstandene. Forskjellene er skildret underdisse punktene:\n\nKildedata\n\nInformasjon pÃ¥ datasettnivÃ¥ som dataeier, omrÃ¥det dataene omhandler og tidsinformasjon\nMetadata om enkeltvariabler er begrenset til informasjonen dataeier selv avleverer.\n\n\n\nInndata\n\nI utgangspunktet samme som kildedata\n\n\n\nKlargjorte data\n\nVariabeldefinisjoner - beskrivelse av hver enkelt variabel og hvordan den er beregnet\nNÃ¸yaktighetsforbedrende tiltak som er utfÃ¸rt\n\n\n\nStatistikk\n\nVariabeldefinisjoner\nHvilke metoder og programmer/kode som er benyttet for Ã¥ produsere statistikken\n\n\n\nUtdata\n\nI utgangspunktet samme som for statistikk",
    "crumbs": [
      "Manual",
      "Standarder",
      "Datatilstander"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-toolbelt.html",
    "href": "statistikkere/datadoc-toolbelt.html",
    "title": "Datadoc",
    "section": "",
    "text": "Datadoc er SSBs system for dokumentasjon av datasett. Datadoc lagrer dokumentasjonen i et strukturert format ved siden av dataene. Man kan jobbe programmatisk med metadataene til Datadoc med Datadoc-delen av dapla-toolbelt-metadata.\nFÃ¸rste gang man skal dokumentere et datasett i Datadoc sÃ¥ er det anbefalt Ã¥ bruke det grafiske grensesnittet i Datadoc-editor. I lÃ¸pende produksjon er det dermed anbefalt Ã¥ benytte en programmatisk tilnÃ¦rming gjennom Datadoc-delen av Python-pakken dapla-toolbelt-metadata.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata",
      "Datadoc"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-toolbelt.html#forberedelser",
    "href": "statistikkere/datadoc-toolbelt.html#forberedelser",
    "title": "Datadoc",
    "section": "Forberedelser",
    "text": "Forberedelser\nFor Ã¥ benytte Datadoc-delen av dapla-toolbelt-metadata mÃ¥ man fÃ¸rst installere pakken i et ssb-project:\n\n\nTerminal\n\npoetry add dapla-toolbelt-metadata",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata",
      "Datadoc"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-toolbelt.html#funksjonalitet",
    "href": "statistikkere/datadoc-toolbelt.html#funksjonalitet",
    "title": "Datadoc",
    "section": "Funksjonalitet",
    "text": "Funksjonalitet\nUnder finner du hvilken funksjonalitet som tilbys for Datadoc i dapla-toolbelt-metadata.\n\nKopiere fra forrige periode uten endringer\nHvis man har dokumentert datasett for periode t med Datadoc-editor, sÃ¥ kan man programmatisk dokumentere periode t+1 ved Ã¥ benytte Datadoc-klassen i dapla-toolbelt-metadata. Det forutsetter at det ikke er noen endringer i kolonnene i datasettet, og at eneste endring er at nye observasjoner. Da kan man dokumentere den nye perioden pÃ¥ fÃ¸lgende mÃ¥te:\n\n\nNotebook\n\nfrom dapla_metadata.datasets import Datadoc\n\nmeta = Datadoc(\n1    dataset_path=\"gs://ssb-dapla-felles-data-produkt-prod/datadoc/sykefratot/klargjorte_data/person_testdata_p2022_v1.parquet\",\n2    metadata_document_path=\"gs://ssb-dapla-felles-data-produkt-prod/datadoc/sykefratot/klargjorte_data/person_testdata_p2021_v1__DOC.json\",\n)\n3meta.write_metadata_document()\n\n\n1\n\ndataset_path angir det nye datasettet som skal dokumenteres.\n\n2\n\nmetadata_document_path angir sti til tidligere periodes metadata.\n\n3\n\nwrite_metadata_document er kommandoen som produserer de nye metadataene og skriver de til filen gs://ssb-dapla-felles-data-produkt-prod/datadoc/sykefratot/klargjorte_data/person_testdata_p2022_v1__DOC.json.\n\n\nDet veldig viktig at man ikke bruker denne metoden hvis det er endringer i hvilke kolonner som finnes i datasettet eller andre stÃ¸rre endringer. Metoden over antar at den eneste informasjonen som har endret seg er den som kan leses ut av filstien. Ved stÃ¸rre endringer i selve dataene bÃ¸r man heller gjÃ¸re en manuell gjennomgang av metadataene med Datadoc-editor, eller bruke metoden som beskrives i neste avsnitt.\n\n\nKopiere fra forrige periode med endringer\nHvis det har skjedd noen endringer i datasettet ditt, men mange av kolonnene har matchende navn og likt innhold, sÃ¥ kan man lage et nytt metadatadokument basert metadataene til en annen fil.\n\n\nNotebook\n\nfrom dapla_metadata.datasets import Datadoc\n\nmeta = Datadoc(\n    dataset_path=\"gs://ssb-dapla-felles-data-produkt-prod/datadoc/sykefratot/klargjorte_data/dataset-to-document_p2022_v1.parquet\",\n    metadata_document_path=\"gs://ssb-dapla-felles-data-produkt-prod/existing-metadata__DOC.json\",\n    errors_as_warnings=True,\n)\n\nI koden over angir vi det nye datasettet som skal dokumenteres i dataset_path=. Deretter angir vi filstien til metadatadokumentet i metadata_document_path= som det delvis skal kopieres metadata fra. Til slutt angir vi at feilmeldinger skal behandles som advarsler med argumentet errors_as_warnings=True.\nMed dette utgangspunktet kan man deretter gjÃ¸re endringer pÃ¥ evt. nye eller eksisterende kolonner ved Ã¥ enten bruke [Datadoc-editor], eller en programmatisk tilnÃ¦rming som forklart senere i kapitlet.\n\n\nOpprette metadata for ny fil\nDet er i de fleste tilfeller anbefalt Ã¥ opprette metadata for en ny fil med Datadoc-editor fordi den har et er mer brukervennlig grensesnitt. Men i noen tilfeller kan det vÃ¦re nyttig Ã¥ opprette et metadatadokument for Datadoc programmatisk.\n\n\nNotebook\n\nfrom dapla_metadata.datasets import Datadoc\n\nmeta = Datadoc(\n    dataset_path = \"/buckets/produkt/datadoc/brukertest/3/sykefratot/klargjorte_data/person_testdata_p2021-12-31_p2022-12-31_v1.parquet\",\n)\n\nmeta.write_metadata_document()\n\nI koden over sÃ¥ genererer vi et metadatadokument for en gitt parquet-fil og skriver den til samme filstien som filen som dokumenteres med write_metadata_document(). Metdatadokumentet blir i eksempelet over skrevet til:\n/buckets/produkt/datadoc/brukertest/3/sykefratot/klargjorte_data/person_testdata_p2021-12-31_p2023-12-31_v1__DOC.json\nVed generering av metadataene blir all informasjon som kan genereres automatisk, faktisk generert. Dette fungerer pÃ¥ samme mÃ¥te som nÃ¥r man Ã¥pner et udokumentert datasett i Datadoc-editor. I boksen under ser man et eksempel pÃ¥ json-filen som blir generert ved Ã¥ kjÃ¸re koden over.\n\n\n\n\n\n\nEksempel pÃ¥ et generert metadatadokument i json-format\n\n\n\n\n\n{\n  \"document_version\": \"0.0.1\",\n  \"datadoc\": {\n    \"percentage_complete\": 66,\n    \"document_version\": \"4.0.0\",\n    \"dataset\": {\n      \"short_name\": \"person_testdata\",\n      \"assessment\": \"PROTECTED\",\n      \"dataset_status\": \"DRAFT\",\n      \"dataset_state\": \"PROCESSED_DATA\",\n      \"name\": null,\n      \"description\": null,\n      \"data_source\": null,\n      \"population_description\": null,\n      \"version\": \"1\",\n      \"version_description\": null,\n      \"unit_type\": null,\n      \"temporality_type\": null,\n      \"subject_field\": null,\n      \"keyword\": null,\n      \"spatial_coverage_description\": [\n        {\n          \"languageCode\": \"nb\",\n          \"languageText\": \"Norge\"\n        },\n        {\n          \"languageCode\": \"nn\",\n          \"languageText\": \"Noreg\"\n        },\n        {\n          \"languageCode\": \"en\",\n          \"languageText\": \"Norway\"\n        }\n      ],\n      \"contains_personal_data\": false,\n      \"use_restriction\": null,\n      \"use_restriction_date\": null,\n      \"custom_type\": null,\n      \"id\": \"a7be0245-8d58-48b2-9158-3df338406cc7\",\n      \"owner\": \"dapla-felles\",\n      \"file_path\": \"/buckets/produkt/datadoc/brukertest/3/sykefratot/klargjorte_data/person_testdata_p2021-12-31_p2023-12-31_v1.parquet\",\n      \"metadata_created_date\": \"2025-04-09T18:10:26.185913Z\",\n      \"metadata_created_by\": \"obr@ssb.no\",\n      \"metadata_last_updated_date\": \"2025-04-09T18:10:26.185349Z\",\n      \"metadata_last_updated_by\": \"obr@ssb.no\",\n      \"contains_data_from\": \"2021-12-31\",\n      \"contains_data_until\": \"2023-12-31\"\n    },\n    \"variables\": [\n      {\n        \"short_name\": \"fnr\",\n        \"data_element_path\": null,\n        \"name\": null,\n        \"data_type\": \"STRING\",\n        \"variable_role\": \"MEASURE\",\n        \"definition_uri\": null,\n        \"is_personal_data\": \"NOT_PERSONAL_DATA\",\n        \"data_source\": null,\n        \"population_description\": null,\n        \"comment\": null,\n        \"temporality_type\": null,\n        \"measurement_unit\": null,\n        \"multiplication_factor\": null,\n        \"format\": null,\n        \"classification_uri\": null,\n        \"special_value\": null,\n        \"invalid_value_description\": null,\n        \"custom_type\": null,\n        \"id\": \"cf8a0c60-4a62-4a93-bbbe-143053b3bf5f\",\n        \"contains_data_from\": \"2021-12-31\",\n        \"contains_data_until\": \"2023-12-31\"\n      },\n      {\n        \"short_name\": \"sivilstand\",\n        \"data_element_path\": null,\n        \"name\": null,\n        \"data_type\": \"STRING\",\n        \"variable_role\": \"MEASURE\",\n        \"definition_uri\": null,\n        \"is_personal_data\": \"NOT_PERSONAL_DATA\",\n        \"data_source\": null,\n        \"population_description\": null,\n        \"comment\": null,\n        \"temporality_type\": null,\n        \"measurement_unit\": null,\n        \"multiplication_factor\": null,\n        \"format\": null,\n        \"classification_uri\": null,\n        \"special_value\": null,\n        \"invalid_value_description\": null,\n        \"custom_type\": null,\n        \"id\": \"f25d96d6-4a49-472d-b1f0-4805a1546daf\",\n        \"contains_data_from\": \"2021-12-31\",\n        \"contains_data_until\": \"2023-12-31\"\n      },\n      {\n        \"short_name\": \"bostedskommune\",\n        \"data_element_path\": null,\n        \"name\": null,\n        \"data_type\": \"STRING\",\n        \"variable_role\": \"MEASURE\",\n        \"definition_uri\": null,\n        \"is_personal_data\": \"NOT_PERSONAL_DATA\",\n        \"data_source\": null,\n        \"population_description\": null,\n        \"comment\": null,\n        \"temporality_type\": null,\n        \"measurement_unit\": null,\n        \"multiplication_factor\": null,\n        \"format\": null,\n        \"classification_uri\": null,\n        \"special_value\": null,\n        \"invalid_value_description\": null,\n        \"custom_type\": null,\n        \"id\": \"cf139119-4a97-4b1f-a793-a2501285c81b\",\n        \"contains_data_from\": \"2021-12-31\",\n        \"contains_data_until\": \"2023-12-31\"\n      },\n      {\n        \"short_name\": \"inntekt\",\n        \"data_element_path\": null,\n        \"name\": null,\n        \"data_type\": \"INTEGER\",\n        \"variable_role\": \"MEASURE\",\n        \"definition_uri\": null,\n        \"is_personal_data\": \"NOT_PERSONAL_DATA\",\n        \"data_source\": null,\n        \"population_description\": null,\n        \"comment\": null,\n        \"temporality_type\": null,\n        \"measurement_unit\": null,\n        \"multiplication_factor\": null,\n        \"format\": null,\n        \"classification_uri\": null,\n        \"special_value\": null,\n        \"invalid_value_description\": null,\n        \"custom_type\": null,\n        \"id\": \"c9357b37-ff1a-4f83-a332-a5239aaa3cf6\",\n        \"contains_data_from\": \"2021-12-31\",\n        \"contains_data_until\": \"2023-12-31\"\n      },\n      {\n        \"short_name\": \"bankinnskudd\",\n        \"data_element_path\": null,\n        \"name\": null,\n        \"data_type\": \"INTEGER\",\n        \"variable_role\": \"MEASURE\",\n        \"definition_uri\": null,\n        \"is_personal_data\": \"NOT_PERSONAL_DATA\",\n        \"data_source\": null,\n        \"population_description\": null,\n        \"comment\": null,\n        \"temporality_type\": null,\n        \"measurement_unit\": null,\n        \"multiplication_factor\": null,\n        \"format\": null,\n        \"classification_uri\": null,\n        \"special_value\": null,\n        \"invalid_value_description\": null,\n        \"custom_type\": null,\n        \"id\": \"38f67ddf-bd07-4bdb-8ea0-411b0aaf511e\",\n        \"contains_data_from\": \"2021-12-31\",\n        \"contains_data_until\": \"2023-12-31\"\n      },\n      {\n        \"short_name\": \"dato\",\n        \"data_element_path\": null,\n        \"name\": null,\n        \"data_type\": \"STRING\",\n        \"variable_role\": \"MEASURE\",\n        \"definition_uri\": null,\n        \"is_personal_data\": \"NOT_PERSONAL_DATA\",\n        \"data_source\": null,\n        \"population_description\": null,\n        \"comment\": null,\n        \"temporality_type\": null,\n        \"measurement_unit\": null,\n        \"multiplication_factor\": null,\n        \"format\": null,\n        \"classification_uri\": null,\n        \"special_value\": null,\n        \"invalid_value_description\": null,\n        \"custom_type\": null,\n        \"id\": \"33353e56-7f5f-409f-915a-644949137228\",\n        \"contains_data_from\": \"2021-12-31\",\n        \"contains_data_until\": \"2023-12-31\"\n      }\n    ]\n  },\n  \"pseudonymization\": null\n}\n\n\n\n\n\nEndre enkeltfelt\nI noen tilfeller kan det vÃ¦re nyttig Ã¥ endre enkeltelementer i en metadatadokument pÃ¥ en programmatisk. F.eks. hvis det eneste som endrer seg ved hver periode er informasjonen i et felt, sÃ¥ kan man kopiere inn forrige periodes metadatadokument, og deretter endre verdien til det ene feltet som har endret seg. Det er ogsÃ¥ nyttig i de tilfellene der man Ã¸nsker Ã¥ opprette metadata for ny fil.\nI kodeeksempelet under sÃ¥ Ã¸nsker vi Ã¥ oppdatere feltet multiplication_factor siden det ikke har noen verdi fra fÃ¸r. FÃ¸rst henter vi inn metadataene til en filen i objektet meta.\n\n\nNotebook\n\nfrom dapla_metadata.datasets import Datadoc\n# Leser inn metadataene\nmeta = Datadoc(\n    dataset_path=\"/buckets/produkt/datadoc/brukertest/3/sykefratot/klargjorte_data/person_testdata_p2021-12-31_p2023-12-31_v1.parquet\",\n)\n\n#Oppdaterer verdien\nmeta.variables_lookup[\n    \"inntekt\"\n].multiplication_factor = 1000  # Variable expressed in thousands of kroner\n\n#Skriver de nye metadataene til dokumentet\nmeta.write_metadata_document()\n\nEtter at vi har hentet inn metadataene i koden over, sÃ¥ oppdaterer vi verdien til multiplication_factor = 1000, og til slutt skriver vi tilbake til metadatadokumentet som er lagret sammen med datasettet.\nNoen informasjonelementer i metadatadokumentet lagres som lister av dictionaries. F.eks. sÃ¥ lagres elementet spatial_coverage_description (Geografisk dekningsomrÃ¥de pÃ¥ norsk) pÃ¥ denne mÃ¥ten siden den er flersprÃ¥klig. Denne har en litt mer kompleks syntaks for oppdatering:\n\n\nNotebook\n\nfrom dapla_metadata import datadoc_model as model\nfrom dapla_metadata.datasets import Datadoc\n\n# Leser inn metadataene\nmeta = Datadoc(\n    dataset_path=\"/buckets/produkt/datadoc/brukertest/3/sykefratot/klargjorte_data/person_testdata_p2021-12-31_p2023-12-31_v1.parquet\",\n)\n\n#Oppdaterer verdien\nmeta.dataset.spatial_coverage_description = model.LanguageStringType(\n    root=[\n        model.LanguageStringTypeItem(languageCode=\"nb\", languageText=\"Test persondata\"),\n        model.LanguageStringTypeItem(languageCode=\"nn\", languageText=\"Testar persondata\"),\n        model.LanguageStringTypeItem(languageCode=\"en\", languageText=\"Test of persondata\")\n    ]\n)\n\n#Skriver de nye metadataene til dokumentet\nmeta.write_metadata_document()\n\nI koden over ser vi at oppdatering av flersprÃ¥klige informasjonselementer gjÃ¸res med Ã¥ fÃ¸rst indentifisere feltet vi Ã¸nsker Ã¥ endre, meta.dataset.spatial_coverage_description. Dette kan leses som at vi Ã¸nsker Ã¥ endre meta-objektet, under dataset-delen, og feltet spatial_coverage_description. Deretter kommer en syntax som er lik for alle flersprÃ¥klige felt.\nI boksen under finnes flere eksempler pÃ¥ hvordan man endrer informasjon i enkeltfelter.\n\n\n\n\n\n\nFlere eksempler pÃ¥ hvordan man endrer enkeltfelter\n\n\n\n\n\n\n\nNotebook\n\nfrom dapla_metadata import datadoc_model as model\nfrom dapla_metadata.datasets import Datadoc\n\n# Importerer/genererer et metadataobjekt\nmeta = Datadoc(\n    dataset_path=\"resources/sykefratot/klargjorte_data/person_testdata_p2022_v1.parquet\",\n)\n\n# Endre \"name\" pÃ¥ datasettnivÃ¥\nmeta.dataset.name = model.LanguageStringType(\n    root=[\n        model.LanguageStringTypeItem(languageCode=\"nb\", languageText=\"Test persondata\"),\n        model.LanguageStringTypeItem(languageCode=\"nn\", languageText=\"Test persondata\"),\n        model.LanguageStringTypeItem(\n            languageCode=\"en\", languageText=\"Test personal data\"\n        ),\n    ],\n)\n\n# Endre \"description\" pÃ¥ datasettnivÃ¥ (flersprÃ¥klig)\nmeta.dataset.description = model.LanguageStringType(\n    root=[\n        model.LanguageStringTypeItem(\n            languageCode=\"nb\",\n            languageText=\"Data er kun for test formÃ¥l\",\n        ),\n        model.LanguageStringTypeItem(\n            languageCode=\"nn\",\n            languageText=\"Data er kun for test formÃ¥l\",\n        ),\n        model.LanguageStringTypeItem(\n            languageCode=\"en\",\n            languageText=\"For testing purposes only\",\n        ),\n    ],\n)\n\n# Endre \"data_source\" pÃ¥ datasettnivÃ¥\nmeta.dataset.data_source = (\n    \"23\"  # Refers to code in https://www.ssb.no/klass/klassifikasjoner/712\n)\n\n#Endre \"use_restriction\" pÃ¥ dataettsnivp\nmeta.dataset.use_restriction = model.UseRestriction.PROCESS_LIMITATIONS\n\n# Endre \"name\" pÃ¥ variabelnivÃ¥ for variabelen \"inntekt\" (flersprÃ¥klig)\nmeta.variables_lookup[\"inntekt\"].name = model.LanguageStringType(\n    root=[\n        model.LanguageStringTypeItem(languageCode=\"nb\", languageText=\"Inntekt\"),\n        model.LanguageStringTypeItem(languageCode=\"nn\", languageText=\"Inntekt\"),\n        model.LanguageStringTypeItem(\n            languageCode=\"en\",\n            languageText=\"Income\",\n        ),\n    ],\n)\n\n# Endre \"multiplication_factor\" pÃ¥ variabelnivÃ¥ for variabelen \"inntekt\"\nmeta.variables_lookup[\n    \"inntekt\"\n].multiplication_factor = 1000  # Variable expressed in thousands of kroner\n\n# # Endre \"is_personal_data\" pÃ¥ variabelnivÃ¥ for variabelen \"inntekt\"\nmeta.variables_lookup[\n    \"inntekt\"\n].is_personal_data = model.IsPersonalData.NOT_PERSONAL_DATA\n\n\n\n\n\n\nLese ut informasjon\nSiden metadataene som er laget med Datadoc lagres ved siden av filen som dokumenteres som en json-fil, sÃ¥ kan man lese inn hele filen i en notebook og printe den ut hvis man Ã¸nsker det:\n\n\nNotebook\n\nimport json\n\nfile_path = \"/buckets/produkt/datadoc/brukertest/3/sykefratot/klargjorte_data/person_testdata_p2021-12-31_p2023-12-31_v1__DOC.json\"\n\nwith open(file_path, \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\nprint(json.dumps(data, indent=2, ensure_ascii=False))\n\n\n\n\n\n\n\nOutput fra eksempelfil\n\n\n\n\n\n{\n  \"document_version\": \"0.0.1\",\n  \"datadoc\": {\n    \"percentage_complete\": 66,\n    \"document_version\": \"4.0.0\",\n    \"dataset\": {\n      \"short_name\": \"person_testdata\",\n      \"assessment\": \"PROTECTED\",\n      \"dataset_status\": \"DRAFT\",\n      \"dataset_state\": \"PROCESSED_DATA\",\n      \"name\": null,\n      \"description\": null,\n      \"data_source\": null,\n      \"population_description\": null,\n      \"version\": \"1\",\n      \"version_description\": null,\n      \"unit_type\": null,\n      \"temporality_type\": null,\n      \"subject_field\": null,\n      \"keyword\": null,\n      \"spatial_coverage_description\": [\n        {\n          \"languageCode\": \"nb\",\n          \"languageText\": \"Test persondata\"\n        },\n        {\n          \"languageCode\": \"nn\",\n          \"languageText\": \"Testar persondata\"\n        },\n        {\n          \"languageCode\": \"en\",\n          \"languageText\": \"Test of persondata\"\n        }\n      ],\n      \"contains_personal_data\": false,\n      \"use_restriction\": null,\n      \"use_restriction_date\": null,\n      \"custom_type\": null,\n      \"id\": \"a7be0245-8d58-48b2-9158-3df338406cc7\",\n      \"owner\": \"dapla-felles\",\n      \"file_path\": \"/buckets/produkt/datadoc/brukertest/3/sykefratot/klargjorte_data/person_testdata_p2021-12-31_p2023-12-31_v1.parquet\",\n      \"metadata_created_date\": \"2025-04-09T18:10:26.185913Z\",\n      \"metadata_created_by\": \"obr@ssb.no\",\n      \"metadata_last_updated_date\": \"2025-04-09T18:58:28.808974Z\",\n      \"metadata_last_updated_by\": \"obr@ssb.no\",\n      \"contains_data_from\": \"2021-12-31\",\n      \"contains_data_until\": \"2023-12-31\"\n    },\n    \"variables\": [\n      {\n        \"short_name\": \"fnr\",\n        \"data_element_path\": null,\n        \"name\": null,\n        \"data_type\": \"STRING\",\n        \"variable_role\": \"MEASURE\",\n        \"definition_uri\": null,\n        \"is_personal_data\": \"NOT_PERSONAL_DATA\",\n        \"data_source\": null,\n        \"population_description\": null,\n        \"comment\": null,\n        \"temporality_type\": null,\n        \"measurement_unit\": null,\n        \"multiplication_factor\": null,\n        \"format\": null,\n        \"classification_uri\": null,\n        \"special_value\": null,\n        \"invalid_value_description\": null,\n        \"custom_type\": null,\n        \"id\": \"cf8a0c60-4a62-4a93-bbbe-143053b3bf5f\",\n        \"contains_data_from\": \"2021-12-31\",\n        \"contains_data_until\": \"2023-12-31\"\n      },\n      {\n        \"short_name\": \"sivilstand\",\n        \"data_element_path\": null,\n        \"name\": null,\n        \"data_type\": \"STRING\",\n        \"variable_role\": \"MEASURE\",\n        \"definition_uri\": null,\n        \"is_personal_data\": \"NOT_PERSONAL_DATA\",\n        \"data_source\": null,\n        \"population_description\": null,\n        \"comment\": null,\n        \"temporality_type\": null,\n        \"measurement_unit\": null,\n        \"multiplication_factor\": null,\n        \"format\": null,\n        \"classification_uri\": null,\n        \"special_value\": null,\n        \"invalid_value_description\": null,\n        \"custom_type\": null,\n        \"id\": \"f25d96d6-4a49-472d-b1f0-4805a1546daf\",\n        \"contains_data_from\": \"2021-12-31\",\n        \"contains_data_until\": \"2023-12-31\"\n      },\n      {\n        \"short_name\": \"bostedskommune\",\n        \"data_element_path\": null,\n        \"name\": null,\n        \"data_type\": \"STRING\",\n        \"variable_role\": \"MEASURE\",\n        \"definition_uri\": null,\n        \"is_personal_data\": \"NOT_PERSONAL_DATA\",\n        \"data_source\": null,\n        \"population_description\": null,\n        \"comment\": null,\n        \"temporality_type\": null,\n        \"measurement_unit\": null,\n        \"multiplication_factor\": null,\n        \"format\": null,\n        \"classification_uri\": null,\n        \"special_value\": null,\n        \"invalid_value_description\": null,\n        \"custom_type\": null,\n        \"id\": \"cf139119-4a97-4b1f-a793-a2501285c81b\",\n        \"contains_data_from\": \"2021-12-31\",\n        \"contains_data_until\": \"2023-12-31\"\n      },\n      {\n        \"short_name\": \"inntekt\",\n        \"data_element_path\": null,\n        \"name\": null,\n        \"data_type\": \"INTEGER\",\n        \"variable_role\": \"MEASURE\",\n        \"definition_uri\": null,\n        \"is_personal_data\": \"NOT_PERSONAL_DATA\",\n        \"data_source\": null,\n        \"population_description\": null,\n        \"comment\": null,\n        \"temporality_type\": null,\n        \"measurement_unit\": null,\n        \"multiplication_factor\": 1000,\n        \"format\": null,\n        \"classification_uri\": null,\n        \"special_value\": null,\n        \"invalid_value_description\": null,\n        \"custom_type\": null,\n        \"id\": \"c9357b37-ff1a-4f83-a332-a5239aaa3cf6\",\n        \"contains_data_from\": \"2021-12-31\",\n        \"contains_data_until\": \"2023-12-31\"\n      },\n      {\n        \"short_name\": \"bankinnskudd\",\n        \"data_element_path\": null,\n        \"name\": null,\n        \"data_type\": \"INTEGER\",\n        \"variable_role\": \"MEASURE\",\n        \"definition_uri\": null,\n        \"is_personal_data\": \"NOT_PERSONAL_DATA\",\n        \"data_source\": null,\n        \"population_description\": null,\n        \"comment\": null,\n        \"temporality_type\": null,\n        \"measurement_unit\": null,\n        \"multiplication_factor\": null,\n        \"format\": null,\n        \"classification_uri\": null,\n        \"special_value\": null,\n        \"invalid_value_description\": null,\n        \"custom_type\": null,\n        \"id\": \"38f67ddf-bd07-4bdb-8ea0-411b0aaf511e\",\n        \"contains_data_from\": \"2021-12-31\",\n        \"contains_data_until\": \"2023-12-31\"\n      },\n      {\n        \"short_name\": \"dato\",\n        \"data_element_path\": null,\n        \"name\": null,\n        \"data_type\": \"STRING\",\n        \"variable_role\": \"MEASURE\",\n        \"definition_uri\": null,\n        \"is_personal_data\": \"NOT_PERSONAL_DATA\",\n        \"data_source\": null,\n        \"population_description\": null,\n        \"comment\": null,\n        \"temporality_type\": null,\n        \"measurement_unit\": null,\n        \"multiplication_factor\": null,\n        \"format\": null,\n        \"classification_uri\": null,\n        \"special_value\": null,\n        \"invalid_value_description\": null,\n        \"custom_type\": null,\n        \"id\": \"33353e56-7f5f-409f-915a-644949137228\",\n        \"contains_data_from\": \"2021-12-31\",\n        \"contains_data_until\": \"2023-12-31\"\n      }\n    ]\n  },\n  \"pseudonymization\": null\n}\n\n\n\nMan kan lese ut metadata fra Datadoc-dokumenterte datasett med dapla-toolbelt-metadata. I eksempelet under henter vi ut metadataene til en fil og printer ut informasjonen pÃ¥ datasettnivÃ¥:\n\n\nNotebook\n\nfrom pprint import pprint\nfrom dapla_metadata.datasets import Datadoc\n\nmeta = Datadoc(\n    dataset_path=\"/buckets/produkt/datadoc/brukertest/3/sykefratot/klargjorte_data/person_testdata_p2021-12-31_p2023-12-31_v1.parquet\",\n)\n\npprint(vars(meta.dataset))\n\n\n\n\n\n\n\nOutput fra eksempelfil\n\n\n\n\n\n{'assessment': 'PROTECTED',\n 'contains_data_from': datetime.date(2021, 12, 31),\n 'contains_data_until': datetime.date(2023, 12, 31),\n 'contains_personal_data': False,\n 'custom_type': None,\n 'data_source': None,\n 'dataset_state': 'PROCESSED_DATA',\n 'dataset_status': 'DRAFT',\n 'description': None,\n 'file_path': '/buckets/produkt/datadoc/brukertest/3/sykefratot/klargjorte_data/person_testdata_p2021-12-31_p2023-12-31_v1.parquet',\n 'id': UUID('a7be0245-8d58-48b2-9158-3df338406cc7'),\n 'keyword': None,\n 'metadata_created_by': 'obr@ssb.no',\n 'metadata_created_date': datetime.datetime(2025, 4, 9, 18, 10, 26, 185913, tzinfo=TzInfo(UTC)),\n 'metadata_last_updated_by': 'obr@ssb.no',\n 'metadata_last_updated_date': datetime.datetime(2025, 4, 9, 18, 10, 26, 185349, tzinfo=TzInfo(UTC)),\n 'name': None,\n 'owner': 'dapla-felles',\n 'population_description': None,\n 'short_name': 'person_testdata',\n 'spatial_coverage_description': LanguageStringType(root=[LanguageStringTypeItem(languageCode='nb', languageText='Norge'), LanguageStringTypeItem(languageCode='nn', languageText='Noreg'), LanguageStringTypeItem(languageCode='en', languageText='Norway')]),\n 'subject_field': None,\n 'temporality_type': None,\n 'unit_type': None,\n 'use_restriction': None,\n 'use_restriction_date': None,\n 'version': '1',\n 'version_description': None}\n\n\n\nOver hentet vi ut fra datasets-delen av dokumentet med Python sin innebygde funksjon vars() for Ã¥ fÃ¥ outputâ€™en mer lesevennlig. Vi kan ogsÃ¥ printe ut informasjon fra variables-delen av dokumentet:\n\n\nNotebook\n\nfrom pprint import pprint\nfrom dapla_metadata.datasets import Datadoc\n\nmeta = Datadoc(\n    dataset_path=\"/buckets/produkt/datadoc/brukertest/3/sykefratot/klargjorte_data/person_testdata_p2021-12-31_p2023-12-31_v1.parquet\",\n)\n\n# Formattere en pen print\nfor var in meta.variables:\n    pprint(vars(var))\n    print(\"-\" * 60)\n\n\n\n\n\n\n\nOutput fra eksempelfil\n\n\n\n\n\n{'classification_uri': None,\n 'comment': None,\n 'contains_data_from': datetime.date(2021, 12, 31),\n 'contains_data_until': datetime.date(2023, 12, 31),\n 'custom_type': None,\n 'data_element_path': None,\n 'data_source': None,\n 'data_type': 'STRING',\n 'definition_uri': None,\n 'format': None,\n 'id': UUID('cf8a0c60-4a62-4a93-bbbe-143053b3bf5f'),\n 'invalid_value_description': None,\n 'is_personal_data': 'NOT_PERSONAL_DATA',\n 'measurement_unit': None,\n 'multiplication_factor': None,\n 'name': None,\n 'population_description': None,\n 'short_name': 'fnr',\n 'special_value': None,\n 'temporality_type': None,\n 'variable_role': 'MEASURE'}\n------------------------------------------------------------\n{'classification_uri': None,\n 'comment': None,\n 'contains_data_from': datetime.date(2021, 12, 31),\n 'contains_data_until': datetime.date(2023, 12, 31),\n 'custom_type': None,\n 'data_element_path': None,\n 'data_source': None,\n 'data_type': 'STRING',\n 'definition_uri': None,\n 'format': None,\n 'id': UUID('f25d96d6-4a49-472d-b1f0-4805a1546daf'),\n 'invalid_value_description': None,\n 'is_personal_data': 'NOT_PERSONAL_DATA',\n 'measurement_unit': None,\n 'multiplication_factor': None,\n 'name': None,\n 'population_description': None,\n 'short_name': 'sivilstand',\n 'special_value': None,\n 'temporality_type': None,\n 'variable_role': 'MEASURE'}\n------------------------------------------------------------\n{'classification_uri': None,\n 'comment': None,\n 'contains_data_from': datetime.date(2021, 12, 31),\n 'contains_data_until': datetime.date(2023, 12, 31),\n 'custom_type': None,\n 'data_element_path': None,\n 'data_source': None,\n 'data_type': 'STRING',\n 'definition_uri': None,\n 'format': None,\n 'id': UUID('cf139119-4a97-4b1f-a793-a2501285c81b'),\n 'invalid_value_description': None,\n 'is_personal_data': 'NOT_PERSONAL_DATA',\n 'measurement_unit': None,\n 'multiplication_factor': None,\n 'name': None,\n 'population_description': None,\n 'short_name': 'bostedskommune',\n 'special_value': None,\n 'temporality_type': None,\n 'variable_role': 'MEASURE'}\n------------------------------------------------------------\n{'classification_uri': None,\n 'comment': None,\n 'contains_data_from': datetime.date(2021, 12, 31),\n 'contains_data_until': datetime.date(2023, 12, 31),\n 'custom_type': None,\n 'data_element_path': None,\n 'data_source': None,\n 'data_type': 'INTEGER',\n 'definition_uri': None,\n 'format': None,\n 'id': UUID('c9357b37-ff1a-4f83-a332-a5239aaa3cf6'),\n 'invalid_value_description': None,\n 'is_personal_data': 'NOT_PERSONAL_DATA',\n 'measurement_unit': None,\n 'multiplication_factor': 1000,\n 'name': None,\n 'population_description': None,\n 'short_name': 'inntekt',\n 'special_value': None,\n 'temporality_type': None,\n 'variable_role': 'MEASURE'}\n------------------------------------------------------------\n{'classification_uri': None,\n 'comment': None,\n 'contains_data_from': datetime.date(2021, 12, 31),\n 'contains_data_until': datetime.date(2023, 12, 31),\n 'custom_type': None,\n 'data_element_path': None,\n 'data_source': None,\n 'data_type': 'INTEGER',\n 'definition_uri': None,\n 'format': None,\n 'id': UUID('38f67ddf-bd07-4bdb-8ea0-411b0aaf511e'),\n 'invalid_value_description': None,\n 'is_personal_data': 'NOT_PERSONAL_DATA',\n 'measurement_unit': None,\n 'multiplication_factor': None,\n 'name': None,\n 'population_description': None,\n 'short_name': 'bankinnskudd',\n 'special_value': None,\n 'temporality_type': None,\n 'variable_role': 'MEASURE'}\n------------------------------------------------------------\n{'classification_uri': None,\n 'comment': None,\n 'contains_data_from': datetime.date(2021, 12, 31),\n 'contains_data_until': datetime.date(2023, 12, 31),\n 'custom_type': None,\n 'data_element_path': None,\n 'data_source': None,\n 'data_type': 'STRING',\n 'definition_uri': None,\n 'format': None,\n 'id': UUID('33353e56-7f5f-409f-915a-644949137228'),\n 'invalid_value_description': None,\n 'is_personal_data': 'NOT_PERSONAL_DATA',\n 'measurement_unit': None,\n 'multiplication_factor': None,\n 'name': None,\n 'population_description': None,\n 'short_name': 'dato',\n 'special_value': None,\n 'temporality_type': None,\n 'variable_role': 'MEASURE'}\n------------------------------------------------------------\n\n\n\nNÃ¥ som vi har sett alle informasjonselementer i metadatadokumentet, sÃ¥ kan vi velge hente ut spesifikke elementer som vi er interessert i. Under leses variabelen inntekt inn og all informasjon printes ut:\n\n\nNotebook\n\nfrom pprint import pprint\nfrom dapla_metadata.datasets import Datadoc\n\nmeta = Datadoc(\n    dataset_path=\"/buckets/produkt/datadoc/brukertest/3/sykefratot/klargjorte_data/person_testdata_p2021-12-31_p2023-12-31_v1.parquet\",\n)\n\ninntekt = meta.variables_lookup[\"inntekt\"]\n\npprint(vars(inntekt))\n\n\n\n\n\n\n\nOutput fra eksempelfil\n\n\n\n\n\n{'classification_uri': None,\n 'comment': None,\n 'contains_data_from': datetime.date(2021, 12, 31),\n 'contains_data_until': datetime.date(2023, 12, 31),\n 'custom_type': None,\n 'data_element_path': None,\n 'data_source': None,\n 'data_type': 'INTEGER',\n 'definition_uri': None,\n 'format': None,\n 'id': UUID('c9357b37-ff1a-4f83-a332-a5239aaa3cf6'),\n 'invalid_value_description': None,\n 'is_personal_data': 'NOT_PERSONAL_DATA',\n 'measurement_unit': None,\n 'multiplication_factor': 1000,\n 'name': None,\n 'population_description': None,\n 'short_name': 'inntekt',\n 'special_value': None,\n 'temporality_type': None,\n 'variable_role': 'MEASURE'}\n\n\n\nVidere kan vi hente ut verdien til feltet multiplication_factor for variabelen inntekt med fÃ¸lgende kode:\n\n\nNotebook\n\nfrom dapla_metadata.datasets import Datadoc\n\nmeta = Datadoc(\n    dataset_path=\"/buckets/produkt/datadoc/brukertest/3/sykefratot/klargjorte_data/person_testdata_p2021-12-31_p2023-12-31_v1.parquet\",\n)\n\nmeta.variables_lookup[\n    \"inntekt\"\n].multiplication_factor\n\nVidere kan vi hente Dapla-teamet som eier datasettet ved hente ut verdien til feltet owner i dataset-delen av dokumentet:\n\n\nNotebook\n\nfrom dapla_metadata.datasets import Datadoc\n\nmeta = Datadoc(\n    dataset_path=\"/buckets/produkt/datadoc/brukertest/3/sykefratot/klargjorte_data/person_testdata_p2021-12-31_p2023-12-31_v1.parquet\",\n)\n\nmeta.dataset.owner",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata",
      "Datadoc"
    ]
  },
  {
    "objectID": "statistikkere/data-collector.html",
    "href": "statistikkere/data-collector.html",
    "title": "Data-Collector",
    "section": "",
    "text": "Data Collector skal avvikles\n\n\n\nDet er bestemt at Data Collector skal avvikles og derfor er det ikke Ã¸nskelig Ã¥ tilby nye team Ã¥ bruke tjenesten. Ta kontakt med team Statistikktjenester dersom du har et behov for Ã¥ bruke Data Collector.\nData Collector (DC) er et rammeverk for bruk av REST APIer som samler inn data fra eksterne ressurser og skriver det til kildebÃ¸tter. DC kjÃ¸rer en deklarativ spesifikasjon ved kjÃ¸retid som beskriver hvordan data skal samles inn. Spesifikasjonen er bygget med en veldefinert DSL.\nDC-jobb startes fra Jupyter ved Ã¥ bruke en funksjon fra Dapla Toolbelt. Innsamlingsjobber beskrives med en specification (json-fil).\nLes mer om arkitektur og funksjonalitet",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Data-Collector"
    ]
  },
  {
    "objectID": "statistikkere/data-collector.html#bruke-collector-fra-dapla-lab",
    "href": "statistikkere/data-collector.html#bruke-collector-fra-dapla-lab",
    "title": "Data-Collector",
    "section": "Bruke Collector fra Dapla-lab",
    "text": "Bruke Collector fra Dapla-lab\nFÃ¸r brukeren kan kjÃ¸re DC fra Dapla Lab, mÃ¥ en team Statistikktjenester ha satt opp en instans for teamet.\n\nSett opp collector\n\n\nnotebook\n\nimport json\n\nfrom dapla import CollectorClient\n\ncollector_url = \"https://data-collector-&lt;team_navn&gt;.intern.ssb.no/tasks\"\ncollector = CollectorClient(collector_url)\nspecification = None\n\n# Load specification from file\nwith open(\"&lt;specification_file&gt;.json\") as specification_file:\n    specification = json.load(specification_file)\n\ntopic = specification['configure'][0]['globalState']['global.topic']\nprint (topic)\n\n\n\nStart data-innsamlingsjobb\n\n\nnotebook\n\nresponse = collector.start(specification)\ntask_id = response.json()['workerId']\nprint(f\"Startet collector jobb, data skal bli skrevet til gs://&lt;kilde-bÃ¸tte&gt;/{topic}/\")\n\n\n\nListe kjÃ¸reneder tasks\n\n\nnotebook\n\nrunning_tasks = collector.running_tasks().json()\nprint(running_tasks)\n\n\n\nStoppe kjÃ¸reneder tasks\n\n\nnotebook\n\n\nstop_response = collector.stop(task_id)\nprint(stop_response)",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Data-Collector"
    ]
  },
  {
    "objectID": "statistikkere/data-collector.html#konfigurasjoner",
    "href": "statistikkere/data-collector.html#konfigurasjoner",
    "title": "Data-Collector",
    "section": "Konfigurasjoner",
    "text": "Konfigurasjoner\nDet er 3 Dapla-team med kjÃ¸rende DC-instanser i prod- og test-miljÃ¸et:\ncollector-url\n\nskatt-person\n\nTEST:\n\nskattemelding: https://data-collector-skatt-person-skattemelding.intern.test.ssb.no/tasks\nskatteoppgjor: https://data-collector-skatt-person-skatteoppgjor.intern.test.ssb.no/tasks\n\nPROD:\n\nskattemelding: https://data-collector-skatt-person-skattemelding.intern.ssb.no/tasks\nskatteoppgjor: https://data-collector-skatt-person-skatteoppgjor.intern.ssb.no/tasks\n\n\nskatt-naering\n\nTEST: https://data-collector-skatt-naering.intern.test.ssb.no/tasks\nPROD: https://data-collector-skatt-naering.intern.ssb.no/tasks\n\nstrukt-mva\n\nTEST: https://data-collector-strukt-mva.intern.test.ssb.no/tasks\nPROD: https://data-collector-strukt-mva.intern.ssb.no/tasks",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Data-Collector"
    ]
  },
  {
    "objectID": "statistikkere/data-collector.html#logger",
    "href": "statistikkere/data-collector.html#logger",
    "title": "Data-Collector",
    "section": "Logger",
    "text": "Logger\nDet er mulig Ã¥ sjekke logger fra google-console for test og prod.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Data-Collector"
    ]
  }
]