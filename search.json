[
  {
    "objectID": "blog/posts/2024-07-11-pseudo-testdata/index.html",
    "href": "blog/posts/2024-07-11-pseudo-testdata/index.html",
    "title": "Pseudonymisering med testdata",
    "section": "",
    "text": "Den strenge tilgangsstyringen til pseudonymiseringsfunksjonaliteten p√• Dapla gj√∏r at det er vanskelig for brukere √• bli kjent med funksjonaliteten ved bruk av produksjonsdata. Derfor b√∏r alle som jobber med dette starte med √• bruke testdata og jobbe i test-milj√∏et p√• Dapla."
  },
  {
    "objectID": "blog/posts/2024-07-11-pseudo-testdata/index.html#importere",
    "href": "blog/posts/2024-07-11-pseudo-testdata/index.html#importere",
    "title": "Pseudonymisering med testdata",
    "section": "Importere",
    "text": "Importere\nF√∏rst s√• importerer vi noen biblioteker som vi skal benytte. Du kan benytte standard-kernelen som heter python3, for der er biblioteket som pseudonymiserer, dapla-toolbelt-pseudo, tilgjengelig.\n\nimport json\n\nimport dapla as dp\nimport pandas as pd\nfrom dapla_pseudo import Depseudonymize, Pseudonymize\nfrom dapla_pseudo.constants import MapFailureStrategy\nfrom dapla_pseudo.utils import convert_to_date\nfrom IPython.display import JSON\n\nVersjonen av dapla-toolbelt-pseudo er 2.1.2.\nDataene vi skal bruke syntetiske f√∏dselsnummer fra testversjonen SNR-katalogen. P√• den m√•ten f√•r vi ogs√• testet pseudonymiseringen via SNR-katalogen som er veldig vanlig i SSB. Denne SNR-katalogen ligger som en fil i en b√∏tte som alle i SSB har tilgang til.\n\npath = \"gs://ssb-dapla-felles-data-produkt-test/freg/snr_kat.csv\"\n\ndf = dp.read_pandas(path, file_format=\"csv\", dtype={\"fnr\": str, \"fnr_date\": str})\n\ndf.head(n=5).style.set_table_styles([\n    {'selector': 'th', 'props': [('text-align', 'left')]}\n])\n\n\n\nTabell¬†1: Syntetisk versjon av SNR-katalogen\n\n\n\n\n\n\n\n\n¬†\nfnr\ncurrent_fnr\nsnr\ncurrent_snr\nfnr_date\ncurrent_fnr_date\n\n\n\n\n0\n16890249063\n16890249063\n026mxd3\n026mxd3\n20201222\n20201222\n\n\n1\n15854996565\n15854996565\n34qm7pt\n34qm7pt\n20201222\n20201222\n\n\n2\n27871547810\n27871547810\n53uxelp\n53uxelp\n20201222\n20201222\n\n\n3\n50889200399\n50889200399\nf35lbnf\nf35lbnf\n20201222\n20201222\n\n\n4\n22919199052\n22919199052\nc2hxvdv\nc2hxvdv\n20201222\n20201222\n\n\n\n\n\n\n\n\nFra Tabell¬†1 ser vi at datasettet inkluderer en del kolonner. For utforsking av pseudonymiseringsfunksjonalitet s√• trenger vi kun fnr-kolonnen."
  },
  {
    "objectID": "blog/posts/2024-07-11-pseudo-testdata/index.html#forberedelse-av-datasettet",
    "href": "blog/posts/2024-07-11-pseudo-testdata/index.html#forberedelse-av-datasettet",
    "title": "Pseudonymisering med testdata",
    "section": "Forberedelse av datasettet",
    "text": "Forberedelse av datasettet\nLa oss kun beholde fnr-kolonnen og kopiere den en ny kolonne slik at vi enklere kan sammenligne f√∏r og etter pseudonymisering. I tillegg kutter jeg antall rader til 10, siden vi ikke trenger noe mer for form√•let her.\n\ndf2 = df.head(n=10)\ndf3 = df2[[\"fnr\"]]\ndf4 = df3.copy()\ndf4['fnr_original'] = df4['fnr']\n\ndf4.head(n=5).style.set_table_styles([\n    {'selector': 'th', 'props': [('text-align', 'left')]}\n])\n\n\n\nTabell¬†2: Nedstrippet versjon av SNR-katalogen\n\n\n\n\n\n\n\n\n¬†\nfnr\nfnr_original\n\n\n\n\n0\n16890249063\n16890249063\n\n\n1\n15854996565\n15854996565\n\n\n2\n27871547810\n27871547810\n\n\n3\n50889200399\n50889200399\n\n\n4\n22919199052\n22919199052\n\n\n\n\n\n\n\n\nHvis du √∏nsker √• teste hvordan krypteringsalgoritmene fungerer med kolonner som inneholder navn, s√• kan vi generere noe data med et ogs√•.\n\nfornavn = [\n    \"Jo\",\n    \"Hans-August\",\n    \"Nils\",\n    \"Eva\",\n    \"Lars\",\n    \"√òyvind\",\n    \"Kenneth\",\n    \"Johnny\",\n    \"Rupinder\",\n    \"Nicolas\",\n]\netternavn = [\n    \"Nordman\",\n    \"Karlsen\",\n    \"Nordmann\",\n    \"Karlsen\",\n    \"Carlsen\",\n    \"Nordmann\",\n    \"Carlsen\",\n    \"Nordmann\",\n    \"Karlsen\",\n    \"Normann\",\n]\n\ndf4['fornavn'] = fornavn\ndf4['etternavn'] = etternavn\ndf5 = df4.copy()\n\nTil slutt legger vi p√• noen ugyldige f√∏dselsnummer slik at vi f√•r testet hvordan algoritmene h√•ndterer dette.\n\nnew_row1 = pd.DataFrame(\n    [\n        {\n            \"fnr\": \"99999999999\",\n            \"fnr_original\": \"99999999999\",\n            \"fornavn\": \"Michael\",\n            \"etternavn\": \"Norman\",\n        }\n    ]\n)\ndf6 = pd.concat([df5, new_row1], ignore_index=True)\n\nnew_row2 = pd.DataFrame(\n    [{\"fnr\": \"XX\", \"fnr_original\": \"XX\", \"fornavn\": \"Ola Glenn\", \"etternavn\": \"G√•√•s\"}]\n)\ndf7 = pd.concat([df6, new_row2], ignore_index=True)\n\nnew_row3 = pd.DataFrame(\n    [\n        {\n            \"fnr\": \"X8b7k28\",\n            \"fnr_original\": \"X8b7k28\",\n            \"fornavn\": \"Lars\",\n            \"etternavn\": \"Gaas\",\n        }\n    ]\n)\ndf8 = pd.concat([df7, new_row3], ignore_index=True)\n\ndf8.head(n=5).style.set_table_styles([\n    {'selector': 'th', 'props': [('text-align', 'left')]}\n])\n\n\n\nTabell¬†3: Datasett for √• teste pseudonymiseringsfunksjonalitet\n\n\n\n\n\n\n\n\n¬†\nfnr\nfnr_original\nfornavn\netternavn\n\n\n\n\n0\n16890249063\n16890249063\nJo\nNordman\n\n\n1\n15854996565\n15854996565\nHans-August\nKarlsen\n\n\n2\n27871547810\n27871547810\nNils\nNordmann\n\n\n3\n50889200399\n50889200399\nEva\nKarlsen\n\n\n4\n22919199052\n22919199052\nLars\nCarlsen\n\n\n\n\n\n\n\n\nTabell¬†3 viser datasettet vi skal bruke til √• teste med."
  },
  {
    "objectID": "blog/posts/2024-07-11-pseudo-testdata/index.html#pseudonymisering",
    "href": "blog/posts/2024-07-11-pseudo-testdata/index.html#pseudonymisering",
    "title": "Pseudonymisering med testdata",
    "section": "Pseudonymisering",
    "text": "Pseudonymisering\nN√• kan vi begynne √• leke med dataene. Det f√∏rste vi kan gj√∏re er √• pseudonymisere med den mest vanlige algoritmen som benyttes i produksjon: Papis-n√∏kkelen.\n\nresult = (\n    Pseudonymize.from_pandas(df8)\n    .on_fields(\"fnr\")\n    .with_stable_id()\n    .run()\n)\nresult.to_pandas()\n\nUnexpected length of metadata: 2\n\n\n\n\nTabell¬†4: Pseudonymiserer med Papis-algoritmen\n\n\n\n\n\n\n\n\n\n\nfnr\nfnr_original\nfornavn\netternavn\n\n\n\n\n0\nBnQe23u\n16890249063\nJo\nNordman\n\n\n1\nI1mQmBP\n15854996565\nHans-August\nKarlsen\n\n\n2\neVbGYLy\n27871547810\nNils\nNordmann\n\n\n3\nO4jegSM\n50889200399\nEva\nKarlsen\n\n\n4\n6JhhRKi\n22919199052\nLars\nCarlsen\n\n\n5\nvygqpLq\n66821775168\n√òyvind\nNordmann\n\n\n6\nJWzitL8\n13824498614\nKenneth\nCarlsen\n\n\n7\noIhVngD\n46927100797\nJohnny\nNordmann\n\n\n8\n0y8qqUZ\n16907699157\nRupinder\nKarlsen\n\n\n9\nFNnp5AL\n10920998203\nNicolas\nNormann\n\n\n10\n4yI2BlkviaI\n99999999999\nMichael\nNorman\n\n\n11\nXX\nXX\nOla Glenn\nG√•√•s\n\n\n12\ntKHXmUl\nX8b7k28\nLars\nGaas\n\n\n\n\n\n\n\n\n\n\nI Tabell¬†4 ser vi at kolonnen fnr har blitt pseudonymisert. Det er ogs√• verdt √• legge merke til at kolonnen ikke endrer navn. Grunnen til at lengden p√• verdiene som er pseudonymiserte er p√• 7 tegn for de opprinnelige f√∏dselsnummerne, er at det f√∏rst skjer en oversetting fra fnr til snr f√∏r det pseudonymiseres, og snr-nummerserien er p√• 7 tegn. Med andre ord s√• preserverer algoritmen lengden p√• snr-nummeret siden det er dette som pseudonymiseres.\nDet er ogs√• verdt √• merke seg at verdier som er kortere enn 4 i lengde, f.eks. XX i rad 11, ikke blir pseudonymisert i det hele tatt. Verdier som er 4 eller lengre, vil bli pseudonymisert selv om de ikke fikk treff i SNR-katalogen.\n\nMetadata\nDet genereres ogs√• 2 metadata-objekter ved pseudonymisering. Disse er:\n\nresult.datadoc\nresult.metadata\n\nLa oss se n√¶rmere p√• de:\n\ndata = json.loads(result.datadoc)\ndisplay(data)\n\n{'document_version': '0.0.1',\n 'pseudonymization': {'document_version': '0.1.0',\n  'pseudo_variables': [{'short_name': 'fnr',\n    'data_element_path': 'fnr',\n    'data_element_pattern': '/fnr',\n    'stable_identifier_type': 'FREG_SNR',\n    'stable_identifier_version': '2023-08-31',\n    'encryption_algorithm': 'TINK-FPE',\n    'encryption_key_reference': 'papis-common-key-1',\n    'encryption_algorithm_parameters': [{'keyId': 'papis-common-key-1'},\n     {'strategy': 'skip'}]}]}}\n\n\nDette er metadata som skal integreres i Datadoc etter hvert.\nLa oss se p√• den andre typen metadata:\n\ndisplay(result.metadata)\n\n{'logs': ['No SID-mapping found for fnr 999********',\n  'No SID-mapping found for fnr X8b****'],\n 'metrics': {'MAPPED_SID': 10, 'FPE_LIMITATION': 1, 'MISSING_SID': 2}}\n\n\nHer ser vo at 10 felt fikk treff i SNR-katalogen, 1 felt var for kort for algoritmen, og 2 felt fikk ikke treff SNR-katalogen. Vi f√•r ogs√• se litt f√∏dselsnummeret til de 2 som ikke fikk treff."
  },
  {
    "objectID": "blog/posts/2024-07-11-pseudo-testdata/index.html#depseudonymisering",
    "href": "blog/posts/2024-07-11-pseudo-testdata/index.html#depseudonymisering",
    "title": "Pseudonymisering med testdata",
    "section": "Depseudonymisering",
    "text": "Depseudonymisering\nLa oss ta vare p√• den pseudonymiserte kolonnen og s√• depseudonymisere og se om resultatet blir riktig:\n\nresult2 = result.to_pandas()\nresult2['pseudo_fnr'] = result2['fnr']\n\nresult_df = (\n    Depseudonymize.from_pandas(result2)         \n    .on_fields(\"fnr\")                              \n    .with_stable_id()                              \n    .run()                                         \n    .to_pandas() \n)\n\nresult_df.style.set_table_styles([\n    {'selector': 'th', 'props': [('text-align', 'left')]}\n])\n\n\n\nTabell¬†5: Depseudonymisering av f√∏dselsnummer\n\n\n\n\n\n\n\n\n¬†\nfnr\nfnr_original\nfornavn\netternavn\npseudo_fnr\n\n\n\n\n0\n16890249063\n16890249063\nJo\nNordman\nBnQe23u\n\n\n1\n15854996565\n15854996565\nHans-August\nKarlsen\nI1mQmBP\n\n\n2\n27871547810\n27871547810\nNils\nNordmann\neVbGYLy\n\n\n3\n50889200399\n50889200399\nEva\nKarlsen\nO4jegSM\n\n\n4\n22919199052\n22919199052\nLars\nCarlsen\n6JhhRKi\n\n\n5\n66821775168\n66821775168\n√òyvind\nNordmann\nvygqpLq\n\n\n6\n13824498614\n13824498614\nKenneth\nCarlsen\nJWzitL8\n\n\n7\n46927100797\n46927100797\nJohnny\nNordmann\noIhVngD\n\n\n8\n16907699157\n16907699157\nRupinder\nKarlsen\n0y8qqUZ\n\n\n9\n10920998203\n10920998203\nNicolas\nNormann\nFNnp5AL\n\n\n10\n99999999999\n99999999999\nMichael\nNorman\n4yI2BlkviaI\n\n\n11\nXX\nXX\nOla Glenn\nG√•√•s\nXX\n\n\n12\nX8b7k28\nX8b7k28\nLars\nGaas\ntKHXmUl\n\n\n\n\n\n\n\n\nTabell¬†5 viser at depseudonymiseringen returnerer de opprinnelige f√∏dselsnummerne.\nVidere kan man utforske √• pseudonymisere navn ved bruk av ulike algoritmer."
  },
  {
    "objectID": "blog/posts/2024-11-17-lonboard/index.html",
    "href": "blog/posts/2024-11-17-lonboard/index.html",
    "title": "Lonboard",
    "section": "",
    "text": "Lonboard1 er et bibliotek for √• vise kart i Jupyter notebooks. Lonboard er bygget p√• Deck.gl, et GPU akselerert, h√∏ytytende, kartvisualiseringsbibliotek for store data. Lonboard er bygget med Anywidget."
  },
  {
    "objectID": "blog/posts/2024-11-17-lonboard/index.html#hvorfor-lonboard",
    "href": "blog/posts/2024-11-17-lonboard/index.html#hvorfor-lonboard",
    "title": "Lonboard",
    "section": "Hvorfor Lonboard?",
    "text": "Hvorfor Lonboard?\nGeopandas‚Äô explore-metode og explore-funksjonen i ssb-sgis-pakken bruker kartvisningsbiblioteket Folium. Hver gang et kart skal vises med Folium, blir kartdataene konvertert til GeoJSON-formatet, som deretter sendes ukomprimert fra Jupyter-serveren til nettleseren. Dette kan f√∏re til lang overf√∏ringstid og potensielt h√∏yt minneforbruk i nettleseren, spesielt n√•r man fors√∏ker √• vise store datasett, som landsdekkende grunnkretser, tettsteder eller postnummeromr√•der.\nLonboard h√•ndterer store datamengder bedre ved √• overf√∏re data mellom serveren og nettleseren i Parquet-format i stedet for GeoJSON, som s√• leses av Deck.gl. Siden Parquet tilfeldigvis er SSBs standard lagringsformat, kan denne overf√∏ringen skje med minimal datakonvertering.\nIkke alle i SSB jobber med Pandas, og for disse brukerne kan Lonboard visualisere tabeller fra DuckDB og PyArrow, i tillegg til Geopandas-tabeller.\nHvor kommer Anywidget inn? Anywidget er et rammeverk for √• lage widgets. En widgets lar ta med interaktive komponenter inn i en Jupyter notebook, slik som en datovelger eller en filtrerbar og sorterbar tabell. Litt slik som Dash lar deg gj√∏re, men rett i Jupyter notebook. √Ö lage sin egen widget til Juypter er en ganske komplisert af√¶re. Det krever at du pakker b√•de Python kode og Javascript kode. Det krever at du m√• skrive kode for √• st√∏tte alle milj√∏er som kan kj√∏re Jupyter notbooks, slik som VScode, Jupyterlab eller Google Colab. Anywidget forenkler prosessen veldig, og s√∏rger for at widget din virker p√• alle plattformer. Den forenkler kommunikasjonen mellom Python og Javascript siden, s√• det blir enklere √• lage interaktivitet.\nAnywidget er n√• installert i Jupyter p√• Dapla Lab, slik at alle kan pr√∏ve ut b√•de Lonboard, samt andre pakker som bygger p√• Anywidget. Et eksempel p√• en slik annen pakke er Vega-altair"
  },
  {
    "objectID": "blog/posts/2024-11-17-lonboard/index.html#eksempel-med-grunnkretser-i-innlandet-fylke",
    "href": "blog/posts/2024-11-17-lonboard/index.html#eksempel-med-grunnkretser-i-innlandet-fylke",
    "title": "Lonboard",
    "section": "Eksempel med grunnkretser i Innlandet fylke",
    "text": "Eksempel med grunnkretser i Innlandet fylke\nUnder er eksempel p√• hvordan man visualisere grunnkretsene i Innlandet fylke i en Jupyterlab Notebook p√• Dapla Lab. Gj√∏r f√∏lgende f√∏rst:\n\nStart en Jupyter p√• Dapla Lab.\nOpprett et ssb-project\nInstaller n√∏dvendige pakker poetry add geopandas lonboard numpy mapclassify matplotlib libpysal\n√Öpne en ny notebook med kernelen som ble opprettet av ssb-project.\n\nI den nyopprettede notebooken kan deretter hente inn data om grunngretser i Innlandet fylke og visualisere de med Lonboard.\n\nimport geopandas as gpd\nimport lonboard\nfrom lonboard import basemap\nfrom mapclassify import greedy\nfrom matplotlib import colormaps\nimport numpy as np\n\ngrunnkretser = gpd.read_file(\n    \"https://nedlasting.geonorge.no/geonorge/Basisdata/Grunnkretser/GML/Basisdata_34_Innlandet_25833_Grunnkretser_GML.zip\",\n    layer=\"Grunnkrets\",\n    engine=\"pyogrio\",\n    columns=[\"grunnkretsnummer\", \"grunnkretsnavn\", \"kommunenummer\"],\n)\n\ngrunnkretser.head()\n\ncolor = greedy(grunnkretser, strategy=\"balanced\", balance=\"centroid\").map(\n    colormaps[\"Set2\"].colors.__getitem__\n)\ncolor = (np.stack(color.to_numpy()) * 255).astype(np.uint8)\n\ngrunnkretser_wgs84 = grunnkretser.to_crs(4326)\nlayer = lonboard.PolygonLayer.from_geopandas(\n    grunnkretser_wgs84,\n    opacity=0.2,\n    line_miter_limit=1,\n    line_width_units = \"pixels\",\n    get_fill_color=color,\n    get_line_color=[255,255,255],\n    auto_highlight=True,\n)\n\nkart = lonboard.Map(\n    [layer],\n    basemap_style=basemap.CartoBasemap.DarkMatterNoLabels,\n    _height=500,\n)\n\nkart\n\n\n\n\n\n\n    \n    Lonboard export\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLonboard har i motsetning til Folium muligheten for interaktivitet. Hvis du velger en avgrensningsboks i kartet over, men knappen oppe i h√∏yre hj√∏rne, s√• blir det utvalet tilgjengleig i Python som kart.selected_bounds\nif kart.selected_bounds:\n    xmin, ymin, xmax, ymax = kart.selected_bounds\n    utvalg = grunnkretser_wgs84.cx[xmin:xmax, ymin:ymax]\n    print(f\"Det er {len(utvalg)} grunnkretser i utvalget\")\nelse:\n  print(\"Du har ikke gjort et utvalg.\")\nDu kan ogs√• utvikle din egen widget, men selvom Anywidget forenkler prossessen mye, krever dette fremdeles ferdigheter i b√•de Python og Javascript, s√• det er kansje ikke for alle."
  },
  {
    "objectID": "blog/posts/2024-11-17-lonboard/index.html#footnotes",
    "href": "blog/posts/2024-11-17-lonboard/index.html#footnotes",
    "title": "Lonboard",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nNavnet er et ordspill. Et ¬´longboard¬ª, er en type raskt skateboard, et ¬´deck¬ª er den delen av skateboardet du st√•r p√•. ¬´Lon¬ª en mye brukt forkortelse for ¬´longitude¬ª, lengdegrad.‚Ü©Ô∏é"
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html",
    "title": "Fra Fame til Python",
    "section": "",
    "text": "Mange i SSB har data lagret i Fame som de √∏nsker √• bearbeide med Python og R. Dette er spesielt relevant n√•r man skal flytte statistikkproduksjon til Dapla. fython er en Python-pakke som gj√∏r dette p√• en enkel m√•te for deg. Den lar deg eksportere data fra Fame med en enkel funksjon, og kan returnere dataene som enten CSV eller Pandas DataFrame.\nPakken finner du p√• GitHub."
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#installasjon",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#installasjon",
    "title": "Fra Fame til Python",
    "section": "Installasjon",
    "text": "Installasjon\nPakken er avhengig av at Fame er installert milj√∏et der den benyttes. Siden den er installert p√• sl-fame-1.ssb.no1 s√• vil de f√¶rreste har behov for √• installere den selv.\nSkulle du likevel √∏nske √• installere pakken selv kan det gj√∏res med Poetry p√• f√∏lgende m√•te:\n\n\nterminal\n\npoetry add git+https://github.com/statisticsnorway/ssb-fame-to-python.git"
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#bruk-av-funksjonene",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#bruk-av-funksjonene",
    "title": "Fra Fame til Python",
    "section": "Bruk av funksjonene",
    "text": "Bruk av funksjonene\nfython har to funksjoner: fame_to_csv og fame_to_df. Begge disse funksjonene tar inn de samme argumentene og de er listet opp i Tabell¬†1.\n\n\n\nTabell¬†1: Forklaring av argumentene i funksjonene til fython\n\n\n\n\n\n\n\n\n\n\n\nArgument\nForklaring\nfame_to_csv()\nfame_to_pandas()\n\n\n\n\ndatabases\nList of Fame databases to access (with full path).\n‚úì\n‚úì\n\n\nfrequency\nFrequency of the data (‚Äòa‚Äô, ‚Äòq‚Äô, ‚Äòm‚Äô).\n‚úì\n‚úì\n\n\ndate_from\nStart date for the data in Fame syntax (e.g., ‚Äò2023:1‚Äô for quarterly, ‚Äò2023‚Äô for annual).\n‚úì\n‚úì\n\n\ndate_to\nEnd date for the data in Fame syntax (e.g., ‚Äò2023:1‚Äô for quarterly, ‚Äò2023‚Äô for annual).\n‚úì\n‚úì\n\n\nsearch_string\nQuery string for fetching specific data. The search is not case sensitive, and ‚Äú^‚Äù and ‚Äú?‚Äù are wildcards (for exactly one and any number of characters, respectively)\n‚úì\n‚úì\n\n\ndecimals\nNumber of decimal places in the fetched data (default is 10).\n‚úì\n‚úì\n\n\npath\nPath to write the csv-file.\n‚úì\n\n\n\n\n\n\n\nLa se p√• noen eksempler.\n\nEksempler\nDersom vi √∏nsker √• hente alt i database1.db og database2.db fra januar 2012 til desember 2022, og f√• det returnert i en DataFrame, kan vi skrive f√∏lgende kode:\n\n\npython\n\nfrom fython import fame_to_pandas\n\ndf = fame_to_pandas(\n  ['sti/til/database1.db', 'sti/til/database2.db', 'm', '2012:1', '2022:12', '?']\n  )\n\nDersom vi i stedet √∏nsker √• hente alle serier som begynner p√• abc, slutter p√• d etterfulgt av ett vilk√•rlig tegn, kan vi skrive f√∏lgende kode:\n\n\npython\n\nfrom fython import fame_to_pandas\n\ndf = fame_to_pandas(\n  ['sti/til/database1.db', 'sti/til/database2.db', 'm', '2012:1', '2022:12', 'abc?d^']\n  )\n\n? og ^ er alts√• jokertegn/wildcards som representerer henholdvis et vilk√•rlig antall tegn og n√∏yaktig ett tegn.\nDersom vi i stedet vil lagre dataene til en csv-fil kan vi skrive\n\n\npython\n\nfrom fython import fame_to_csv\n\nfame_to_csv(\n  ['sti/til/database1.db', 'sti/til/database2.db', 'm', '2012:1', '2022:12', 'abc?d^', 'sti/til/csv-fil.csv']\n  )\n\n\n\n\n\n\n\nWarning\n\n\n\nDet er viktig √• p√•peke at enhver serie kun skrives √©n gang, og da fra den f√∏rste databasen den finnes i (kronologisk iht. til listen med databaser)."
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#kj√∏ringer-p√•-serveren",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#kj√∏ringer-p√•-serveren",
    "title": "Fra Fame til Python",
    "section": "Kj√∏ringer p√• serveren",
    "text": "Kj√∏ringer p√• serveren\nN√•r du skal bruke fython s√• m√• du ta hensyn til hvilken server Fame er installert p√•, og hvilken server du har tenkt til √• jobbe p√•. Fame er som sagt installert p√• sl-fame-1.ssb.no, mens Jupyterlab er installert p√• sl-jupyter-p.ssb.no. Dvs. at hvis du √∏nsker √• bruke fython i en notebook i Jupyterlab, s√• m√• du bruke ssh til √• koble deg til sl-fame-1.ssb.no, og s√• kj√∏re koden derfra. Koden din kan skrive en fil til √∏nsket stammeomr√•det, som du igjen kan lese inn direkte i Jupyterlab."
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#automatiserte-uttrekk",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#automatiserte-uttrekk",
    "title": "Fra Fame til Python",
    "section": "Automatiserte uttrekk",
    "text": "Automatiserte uttrekk\nHvis man √∏nsker at utrekk fra Fame skal skje automatisk p√• gitte tidspunkter eller intervaller, s√• kan man ta kontakt med Kundeservice. Fordelen med dette er at man ikke trenger √• bruke ssh slik som beskrevet over. Man kan lese inn direkte fra stammeomr√•det."
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#overf√∏re-data-til-dapla",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#overf√∏re-data-til-dapla",
    "title": "Fra Fame til Python",
    "section": "Overf√∏re data til Dapla",
    "text": "Overf√∏re data til Dapla\nHvis man √∏nsker √• overf√∏re data fra Fame til Dapla, s√• kan dette settes opp som en MoveIt-operasjon. For √• sette opp en MoveIt-jobb m√• ma kontakte Kundeservice. Overf√∏ring til Dapla forutsetter at man har et Dapla-team, og at man setter opp en synkroniseringjobb med Transfer Service."
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#footnotes",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#footnotes",
    "title": "Fra Fame til Python",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPakken er installert i Python-versjon 3.6 p√• serveren. Du kan √•pne et Python-shell i terminalen p√• sl-fame-1.ssb.no ved √• skrive: python3.6.‚Ü©Ô∏é"
  },
  {
    "objectID": "blog/posts/2025-02-07-parquet-viewer/index.html",
    "href": "blog/posts/2025-02-07-parquet-viewer/index.html",
    "title": "Parquet-utforsker i VS Code",
    "section": "",
    "text": "I Dapla Lab tjenesten Vscode-python er n√• extension‚Äôen vscode-parquet-visualizer installert. Den lar deg √•pne en Parquet-fil uten bruk av Python- eller R-biblioteker. I tillegg lar den deg sp√∏rre mot datasettet med SQL, filtrere dataettet uten kode, sortere kolonner, gir deg metadata om kolonner og datasett, og gir en forh√•ndsvisning av komplekse celler.\nI videoen under ser man hvordan man √•pner en Parquet-fil i en b√∏tte fra et ssb-project. Filen som √•pnes har 4 kolonner, 5 millioner rader og er p√• 85 megabytes."
  },
  {
    "objectID": "blog/posts/2025-02-07-parquet-viewer/index.html#bruksomr√•de",
    "href": "blog/posts/2025-02-07-parquet-viewer/index.html#bruksomr√•de",
    "title": "Parquet-utforsker i VS Code",
    "section": "Bruksomr√•de",
    "text": "Bruksomr√•de\nBruksomr√•det for denne funksjonaliteten er √• utforske Parquet-filer og ikke prosessere data i produksjon. Selv om man kan skrive ut et filtrert datasett med l√∏sningen, s√• skal det ikke benyttes til prosessering som skal v√¶re reproduserbar siden det ikke dokumenteres med kode."
  },
  {
    "objectID": "blog/posts/2025-02-07-parquet-viewer/index.html#skrive-sql",
    "href": "blog/posts/2025-02-07-parquet-viewer/index.html#skrive-sql",
    "title": "Parquet-utforsker i VS Code",
    "section": "Skrive SQL",
    "text": "Skrive SQL\nSQL-en som skrives m√• v√¶re duckdbsql siden det er dette extension‚Äôen benytter for √• hente informasjon fra Parquet-filen."
  },
  {
    "objectID": "news/posts/2024-12-03-refresh-buckets/index.html",
    "href": "news/posts/2024-12-03-refresh-buckets/index.html",
    "title": "refresh-buckets kommando i Dapla Lab",
    "section": "",
    "text": "N√• kan man kj√∏re kommandoen refresh-buckets fra terminalen i en tjeneste p√• Dapla Lab for √• oppdatere visningen av en b√∏ttene i filsystemet. Dette kan v√¶re nyttig hvis det blir opprettet mapper med dapla-toolbelt eller Kildomaten, og ikke via buckets-mappen i filsystemet. Les mer i kapitlet om Dapla Lab."
  },
  {
    "objectID": "news/posts/2024-11-08-endring-cpu-ram/index.html",
    "href": "news/posts/2024-11-08-endring-cpu-ram/index.html",
    "title": "Endring i default-maskinkraft p√• Dapla Lab",
    "section": "",
    "text": "F√∏rstkommende mandag gj√∏r vi en endring i hvor mye RAM og CPU programmeringsmilj√∏ene p√• Dapla Lab har som default. Vi endrer antall millicores (M) CPU fra 2000M til 200M, og mengden RAM fra 8GB til 4GB. Dvs. at brukeren aktivt m√• √∏ke ressursene i tjenestekonfigurasjonen (se Figur¬†1) f√∏r oppstart hvis de √∏nsker mer enn dette.\n√Örsaken til at vi gj√∏r endringen er at vi har observert at flere brukere reserverer ressurser som ikke benyttes, og at dette skaper un√∏dvendige kostnader for SSB.\nEndringen gjelder tjenestene Jupyter, Jupyter-playground, Vscode og Rstudio.\n\n\n\n\n\n\nFigur¬†1: Bilde av Ressurser-fanen under tjenestekonfigurasjonen til Jupyter"
  },
  {
    "objectID": "news/posts/2025-01-29-dapla-lab-dagene/index.html",
    "href": "news/posts/2025-01-29-dapla-lab-dagene/index.html",
    "title": "√Öpen dag om Dapla Lab i Oslo og Kongsvinger",
    "section": "",
    "text": "10. og 11. februar arrangerer vi √•pen dag om Dapla Lab for √• hjelpe brukere med overgangen til Dapla Lab. Utviklere, tech-coacher og st√∏tteteam stiller fysisk for √• hjelpe deg med √• flytte kode, filer, sette opp Git- og GitHub-konfigurasjon, eller svare p√• sp√∏rsm√•l/innspill man m√•tte ha ifm. overgangen. Vi h√•per at s√• mange som mulig tar turen innom for √• f√• hjelp eller bare sl√• av en prat.\nArrangement i Oslo 10. februar kl. 12 i Auditoriet\nArrangement i Kongsvinger 11. februar kl. 12 i A005"
  },
  {
    "objectID": "news/posts/2024-12-23-standarder-chapter/index.html",
    "href": "news/posts/2024-12-23-standarder-chapter/index.html",
    "title": "Nytt kapittel om Standarder og standardutvalget",
    "section": "",
    "text": "Vi har skrevet et kapittel i Dapla-manualen som gir et overblikk over standardutvalget og standardene p√• Dapla. Sjekk det ut her."
  },
  {
    "objectID": "news/posts/2024-11-25-endring-i-tjenestekonfig/index.html",
    "href": "news/posts/2024-11-25-endring-i-tjenestekonfig/index.html",
    "title": "Endring i tjenestekonfigurasjon p√• Dapla Lab",
    "section": "",
    "text": "Tjenestekonfigurasjonen i Dapla Lab har endret seg. Dette er alts√• menyen man f√•r opp n√•r man starter opp en ny tjeneste, for eksempel en ny instans av Jupyter eller VSCode. Parameterne man kan konfigurere er derimot uendret. F√∏lgende artikler i manualen har derfor blitt oppdatert: jupyter, jupyter-pyspark, jupyter-playground, VSCode-python, RStudio, Datadoc, JDemetra og artikkelen om Dapla Lab.\n\n\n\n\n\n\nFigur¬†1: Tjenestekonfigurasjon for jupyter"
  },
  {
    "objectID": "news/posts/2024-11-14-kildedata-datadoc/index.html",
    "href": "news/posts/2024-11-14-kildedata-datadoc/index.html",
    "title": "Kildedata og Datadoc",
    "section": "",
    "text": "Datadoc st√∏tter ikke dokumentasjon av datasett i datatilstanden kildedata enda. Det vil st√∏ttes p√• sikt, men forel√∏pig er det ikke funksjonalitet for dette.\nI Datadoc-applikasjonen i Dapla Lab f√•r man n√• valget med √• logge seg inn i tjenesten som data-admins, og dermed en teknisk mulighet til √• dokumentere kildedata. Siden dette ikke er st√∏ttet enda, s√• ber vi alle om √• ikke gj√∏re dette. Det er ingen grunn til √• aktivere sin kildedatatilgang n√•r dette ikke st√∏ttes.\nLes mer i dokumentasjonen til Datadoc."
  },
  {
    "objectID": "news/posts/2025-01-08-metodebib-chapter/index.html",
    "href": "news/posts/2025-01-08-metodebib-chapter/index.html",
    "title": "Nytt kapittel om Metodebiblioteket",
    "section": "",
    "text": "Vi har skrevet et kapittel i Dapla-manualen om metodebiblioteket. Sjekk det ut her."
  },
  {
    "objectID": "news/posts/2024-11-29-kvakk-chapter/index.html",
    "href": "news/posts/2024-11-29-kvakk-chapter/index.html",
    "title": "Nytt kapittel om Kvalitet i kode og koding (KVAKK)",
    "section": "",
    "text": "Vi har skrevet et kapittel i Dapla-manualen om Kvalitet i kode og koding (KVAKK). Sjekk det ut her.\nKapittelet er forel√∏pig kort og henviser til confluence. Vi har planer om √• skrive mer utfyllende om KVAKK i manualen."
  },
  {
    "objectID": "news/posts/2025-01-15-overgang-dapla-lab/index.html",
    "href": "news/posts/2025-01-15-overgang-dapla-lab/index.html",
    "title": "Overgang til Dapla Lab",
    "section": "",
    "text": "Innen 15. februar m√• alle brukere av dagens Jupyter-milj√∏ flytte over til Dapla Lab, SSBs nye arbeidsbenk for staistikkproduksjon og forskning. Innen fristen m√• kode, filer og kj√∏ringer flyttes fra det gamle milj√∏et over til Dapla Lab.\nMigreringsguide\nhttps://statistics-norway.atlassian.net/wiki/spaces/DAPLA/pages/4398678345/Migreringsguide+for+Dapla+Lab\nDokumentasjon for Dapla Lab\nhttps://manual.dapla.ssb.no/statistikkere/dapla-lab.html\nDokumentasjon for tjenestene p√• Dapla Lab\nhttps://manual.dapla.ssb.no/statistikkere/jupyter.html"
  },
  {
    "objectID": "news/posts/2024-11-20-daplanytt-nov24/index.html",
    "href": "news/posts/2024-11-20-daplanytt-nov24/index.html",
    "title": "DaplaNytt-m√∏te 19.11.2024",
    "section": "",
    "text": "DaplaNytt for november 2024 ble avholdt som Teams-m√∏te 19. november 2024.\nSe opptaket her (intern lenke)."
  },
  {
    "objectID": "news/posts/2024-11-11-jupyter-pyspark-chapter/index.html",
    "href": "news/posts/2024-11-11-jupyter-pyspark-chapter/index.html",
    "title": "Nytt kapittel om Jupyter-pyspark",
    "section": "",
    "text": "Vi har skrevet et kapittel i Dapla-manualen om hvordan man bruker Jupyter-pyspark i Dapla Lab. Sjekk det ut her."
  },
  {
    "objectID": "news/posts/2024-11-11-dapla-lab-source-data/index.html",
    "href": "news/posts/2024-11-11-dapla-lab-source-data/index.html",
    "title": "Kildedata tilgjengelig fra Dapla Lab",
    "section": "",
    "text": "Brukere i tilgangsgruppen data-admins kan n√• aksessere kildedata fra tjenestene p√• Dapla Lab. Kravene for tilgangen er de samme som f√∏r:\nSom tidligere m√• de da velge hvilket team og tilgangsgruppe de √∏nsker √• starte tjenesten som, og deretter oppgi begrunnelse og lengde p√• tilgang. Dette gj√∏res fra tjenestekonfigurasjonen i Dapla Lab, slik som vist i Figur¬†1."
  },
  {
    "objectID": "news/posts/2024-11-11-dapla-lab-source-data/index.html#begrunnelse-gis-i-dapla-lab",
    "href": "news/posts/2024-11-11-dapla-lab-source-data/index.html#begrunnelse-gis-i-dapla-lab",
    "title": "Kildedata tilgjengelig fra Dapla Lab",
    "section": "Begrunnelse gis i Dapla Lab",
    "text": "Begrunnelse gis i Dapla Lab\nN√•r en bruker tidligere har aksessesert kildedata fra ‚Äúgamle‚Äù Jupyter (https://jupyter.dapla.ssb.no/), s√• ble en JIT-applikasjon for √• f√• midlertidig tilgang. Denne tiln√¶rmingen vil fortsette √• fungere for det ‚Äúgamle‚Äù mij√∏et inntil det avvikles til fordel for Dapla Lab. Men p√• Dapla Lab kan begrunnelse og tidspunkt angis direkte i tjenestekonfigurasjonen."
  },
  {
    "objectID": "news/posts/2024-11-11-dapla-lab-source-data/index.html#feil-ved-manglende-begrunnelse",
    "href": "news/posts/2024-11-11-dapla-lab-source-data/index.html#feil-ved-manglende-begrunnelse",
    "title": "Kildedata tilgjengelig fra Dapla Lab",
    "section": "Feil ved manglende begrunnelse",
    "text": "Feil ved manglende begrunnelse\nHvis man pr√∏ver √• starte en tjeneste som data-admins uten √• oppgi en begrunnelse, s√• vil man f√• feilmeldingen vist i Figur¬†2. Denne feilmeldingen er lite forklarende og er noe vi √∏nsker √• forbedre etter hvert.\n\n\n\n\n\n\nFigur¬†2: Feilmelding ved manglende begrunnelse i Dapla Lab."
  },
  {
    "objectID": "news/posts/2025-02-05-daplanytt-jan25/index.html",
    "href": "news/posts/2025-02-05-daplanytt-jan25/index.html",
    "title": "DaplaNytt-m√∏te 5.2.2025",
    "section": "",
    "text": "DaplaNytt for januar 2025 ble avholdt som Teams-m√∏te 5. februar 2025.\nSe opptaket her (intern lenke).\n\n\n\nSkjembilde fra Teams-m√∏te"
  },
  {
    "objectID": "news/posts/2024-12-11-nais-platform/index.html",
    "href": "news/posts/2024-12-11-nais-platform/index.html",
    "title": "Tar i bruk Navs applikasjonsplattform",
    "section": "",
    "text": "I 2025 skal SSB for fullt ta i bruk Navs applikasjonsplattform, NAIS.\n\nMed Nais er mange gjennomtenkte valg allerede tatt. Det blir lettere for applikasjonsutviklere i SSB √• g√• fra id√© til implementering, samtidig som vi slipper utvikling og vedlikehold som ellers m√•tte gj√∏re selv.\n\nLes mer i denne SSB-interne saken."
  },
  {
    "objectID": "statistikkere/altinn-jobbe-med-altinn3.html",
    "href": "statistikkere/altinn-jobbe-med-altinn3.html",
    "title": "Jobbe med Altinn 3",
    "section": "",
    "text": "Frem mot sommeren 2026 skal alle skjema-unders√∏kelser i SSB som gjennomf√∏res p√• Altinn 2 flyttes over til Altinn 3. Skjemaer som flyttes til Altinn 3 vil motta sine data p√• Dapla, og ikke p√• bakken som tidligere. Datafangsten h√•ndteres av Team SUV, mens statistikkseksjonene henter sine data fra Team SUV sitt lagringsomr√•de p√• Dapla. I dette kapitlet beskriver vi n√¶rmere hvordan statistikkseksjonene kan jobbe med Altinn3-data p√• Dapla. Kort oppsummert best√•r det av disse stegene:\nUnder forklarer vi mer med mer detaljer hvordan man g√•r frem for gjennomf√∏re steg 4-5 over."
  },
  {
    "objectID": "statistikkere/altinn-jobbe-med-altinn3.html#forberedelse",
    "href": "statistikkere/altinn-jobbe-med-altinn3.html#forberedelse",
    "title": "Jobbe med Altinn 3",
    "section": "Forberedelse",
    "text": "Forberedelse\nN√•r skjema-dataene kommer inn fra Altinn 3 til Team SUV sitt lagringsomr√•de, s√• er det en del ting som er verdt √• tenke p√•:\n\nHvordan er filene organisert hos Team SUV?\nFilstrukturen der Team SUV lagrer dataene som kommer inn fra Altinn 3, har en klar struktur. Du kan selv g√• inn √• kikke p√• dataene, enten fra Jupyter eller Google Cloud Console, og bli bedre kjent med strukturen2. Figur¬†1 viser en hvordan en typisk filsti ser ut p√• lagringsomr√•det til Team SUV. Det starter med navnet til b√∏tta som Team SUV har opprettet for skjemaet. Deretter viser den innvitteringsdato, deretter et teknisk navn, og til slutt selve skjemanavnet.\n\n\n\n\n\n\n\nFigur¬†1: Typisk filsti for et Altinn3-skjema.\n\n\n\n\nHvordan organisere dataene i din kildeb√∏tte?\nN√•r vi bruker Transfer Service til √• synkronisere innholdet i Team SUV sitt lagringsomr√•de til Dapla-teamet sitt lagringsomr√•de, s√• er det mest hensiktmessig √• fortsette √• bruke mappe-strukturen som Team SUV har. Grunnen er at vi ikke kan legge p√• noe logikk som lager en ny struktur. Tjenesten bare kopierer over data. Men du kan kan lage et nytt toppniv√•-mappe som du √∏nsker at dataene skal synkroniseres til. F.eks. at alle Altinn-dataene legger seg inn i en mappe som heter altinn. Det er spesielt nyttig hvis du har flere datakilder som skal ligge i samme kildeb√∏tte.\nUnike skjemanavn\nSkjemanavnet du ser i Figur¬†1 er unike. Dvs. at hvis en oppgavegiver sender inn flere skjemaer p√• samme dag, s√• er fortsatt skjemanavnet unikt. Det er viktig √• v√¶re klar over n√•r man bruker automatiseringstjenesten for kildedata senere. Siden tjenesten trigges per fil, og man er avhengig av unike navn for √• ikke skrive over filer, s√• er det nyttig √• vite at man kan videref√∏re skjemanavnet i overgangen fra kildedata til inndata."
  },
  {
    "objectID": "statistikkere/altinn-jobbe-med-altinn3.html#transfer-service",
    "href": "statistikkere/altinn-jobbe-med-altinn3.html#transfer-service",
    "title": "Jobbe med Altinn 3",
    "section": "Transfer Service",
    "text": "Transfer Service\nN√•r vi skal overf√∏re filer fra Team SUV sin b√∏tte til v√•r kildeb√∏tte, s√• kan vi gj√∏re det manuelt fra Jupyter som forklart her.. Men det er en bedre l√∏sning √• bruke en tjeneste som gj√∏r dette for deg. Transfer Service er en tjeneste som kan brukes til √• synkronisere innholdet mellom b√∏tter p√• Dapla, samt mellom bakke og sky. N√•r du skal ta i bruk tjenesten for √• overf√∏re data mellom en b√∏tte fra Team SUV sitt prosjekt suv-altinn-data-p, til en kildedata-b√∏tte i Dapla-teamet ditt, s√• gj√∏r du f√∏lgende:\n\nF√∏lg denne beskrivelsen hvordan man setter opp overf√∏ringsjobber.\nEtter at du har trykket p√• Create Transfer Job velger du Google Cloud Storage p√• begge alternativene under Get Started. Deretter g√•r du videre ved √• klikke p√• Next Step.\nUnder Choose a source s√• skal du velge hvor du skal kopiere data fra. Trykk p√• Browse. I vinduet som dukker opp trykker du p√• üîª-ikonet ved siden av Project ID. I s√∏kevinduet som dukker opp s√∏ker du opp suv-altinn-data-p og trykker p√• navnet. Da f√•r du listet opp alle b√∏ttene i suv-altinn-data-p prosjektet. Til slutt trykker du p√• b√∏tta som Team SUV har opprettet for unders√∏kelsen3 og klikker Select til nederst p√• siden. Trykk deretter Next step for √• g√• videre.\nUnder Choose a destination s√• skal du velge hvor du skal kopiere data til. Dette steget er ganske likt som forrige, men du skal n√• velge ditt eget projekt og kildeb√∏tta der. Trykk p√• Browse. I vinduet som dukker opp trykker du p√• üîª-ikonet ved siden av Project ID. I s√∏kevinduet som dukker opp s√∏ker du opp prod-&lt;ditt teamnavn&gt; og trykker p√• navnet. Da f√•r du listet opp alle b√∏ttene i ditt team sitt prosjekt. Velg kildeb√∏tta som har navnet ssb-prod-&lt;teamnavn&gt;-data-kilde. Hvis du √∏nsker √• kopiere data til en undermappe i b√∏tta, s√• trykker du p√• &gt;-ikonet ved b√∏ttenavnet og velger √∏nsket undermappe4. Til slutt trykker du p√• Select til nederst p√• siden. Trykk deretter Next step for √• g√• videre.\nUnder Choose how and when to run job velger du Run with custom frequency og Starting now i Batch-modus. Hvis du √∏nsker √• overf√∏re s√• ofte som mulig, s√• velger du Custom frequency og Repeat every 1 Hours. Til slutt trykker du Next Step nederst p√• siden.\nUnder Choose Settings s√• legger du til en kort beskrivelse av jobben du har opprettet. Under de andre valgene kan du gj√∏re f√∏lgende:\n\nUnder Advanced transfer Options trenger du ikke gj√∏re noen endringer.\nUnder When to overwrite kan du velge If different.\nUnder When to delete kan du velge Never.\nUnder Manifest trenger du ikke huke av.\nUnder Logging options velger du samme som vist i Figur¬†2.\n\n\n\n\n\n\n\n\nFigur¬†2: Valg av opsjoner for logging i Transfer Service\n\n\n\nTil slutt trykker du Create for √• aktivere tjenesten. Den vil da sjekke Team SUV sin b√∏tte hver time og kopiere over alle filer som ikke allerede eksisterer statistikkteamets kildeb√∏tte."
  },
  {
    "objectID": "statistikkere/altinn-jobbe-med-altinn3.html#automatiseringstjeneste-for-kildedata",
    "href": "statistikkere/altinn-jobbe-med-altinn3.html#automatiseringstjeneste-for-kildedata",
    "title": "Jobbe med Altinn 3",
    "section": "Automatiseringstjeneste for kildedata",
    "text": "Automatiseringstjeneste for kildedata\nN√•r du har satt opp Transfer Service til √• kopiere over filer fra Team SUV sin b√∏tte til statistikkteamets kildeb√∏tte, s√• vil det potensielt komme inn nye skjemaer hver time. Disse kan du lese inn i Jupyter og jobbe med hvis du er kildedata-ansvarlig i teamet. Hvis du ikke er det s√• m√• du vente til dataene er tilgjengeliggjort i produkt-b√∏tta til teamet.\nSiden f√• personer innehar rollen som kildedata-ansvarlig s√• er det laget en automatiseringstjeneste som kan bearbeide alle filer som kommer inn i kildeb√∏tta. Den lar deg kj√∏re et python-script p√• alle filer som kommer inn i kildeb√∏tta.\nLes mer om hvordan du kan bruker tjenesten her."
  },
  {
    "objectID": "statistikkere/altinn-jobbe-med-altinn3.html#overf√∏re-altinn3-filer-til-isee",
    "href": "statistikkere/altinn-jobbe-med-altinn3.html#overf√∏re-altinn3-filer-til-isee",
    "title": "Jobbe med Altinn 3",
    "section": "Overf√∏re Altinn3-filer til ISEE",
    "text": "Overf√∏re Altinn3-filer til ISEE\nDet er utviklet en Altinn3-pakke i Python som flater ut mottatte XML-filer og som lager en csv-fil som er p√• ISEE-format av innholdet.\nDokumentasjon og reference guide av pakken.\nPakken ssb-altinn-python er tilgjengelig i Kildomaten, og kan benyttes for √• automatisk flate ut Altinn3-skjema mellom kilde- og produktb√∏tte."
  },
  {
    "objectID": "statistikkere/altinn-jobbe-med-altinn3.html#metadata-fra-altinn-tilgjengelig-som-json-fil",
    "href": "statistikkere/altinn-jobbe-med-altinn3.html#metadata-fra-altinn-tilgjengelig-som-json-fil",
    "title": "Jobbe med Altinn 3",
    "section": "Metadata fra Altinn tilgjengelig som Json-fil",
    "text": "Metadata fra Altinn tilgjengelig som Json-fil\nFor hver ny innsending fra Altinn3 (i test og prod)- tilrettelegger SUV n√• en ekstra fil med metadata. Filen inneholder forel√∏pig to variabler:\n\nReferansenummeret som oppgavegiver f√•r ved innsending.\nTidspunkt for n√•r skjema er levert i Altinn3.\n\nBrukerbehovet er i hovedsak dublettkontroll og svartjeneste. For dette trenger man et eksakt tidspunkt for n√•r skjema faktisk ble sendt inn (trykket p√• knappen i Altinn). Merk at tidspunkt i fila er UTC.\nFilen ligger i b√∏ttene sammen med xml/pdf (og eventuelle vedlegg). Team T-Rex vil se videre p√• √• integrere dette inn i sin Python-pakke.\n{\n    \"altinnReferanse\": \"f23415ca6b2f\", \n    \"altinnTidspunktLevert\": \"2024-04-29T07:16:10.5080448Z\"\n}\n\n\n\n\n\n\nFigur¬†3: Metadata fra Altinn"
  },
  {
    "objectID": "statistikkere/altinn-jobbe-med-altinn3.html#tips-og-triks",
    "href": "statistikkere/altinn-jobbe-med-altinn3.html#tips-og-triks",
    "title": "Jobbe med Altinn 3",
    "section": "Tips og triks",
    "text": "Tips og triks\nI denne delen deles noen tips og triks for √• jobbe med Altinn3-dataene p√• Dapla. Fokuset vil v√¶re p√• hvordan du kan lese inn og transformere xml-filer fra skjema-dataene.\n\nListe ut innhold i en mappe\nFor √• se innholdet i en mappe gir det mest mening √• bruke Google Cloud Console. Her kan du se b√•de filer og mapper i et pek-og-klikk grensesnitt. Men du kan ikke se p√• innholdet i filene der. Til det m√• du bruke Jupyter.\nAnta at vi √∏nsker √• liste ut alle som leverte skjema den 10. mars 2023. Da kan vi bruke gcsfs til √• gj√∏re det5:\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ssb-suv-altinn-ra0678-01-prod/2023/3/10/**.xml\")\n\n# Print the list of parquet files\nxml_files\nHer bruker vi fs.glob()-funksjonen fra gcsfs til √• loope gjennom alle undermapper av gs://ssb-suv-altinn-ra0678-01-prod/2023/3/10/, og legge alle filer som slutter med .xml til listen xml_files. Dermed har vi et Python-objekt som kun brukes til √• hente inn de filene vi √∏nsker. Vi bruker da det vi vet om mappestrukturen i Team SUV sin b√∏tte som vi s√• tidligere i Figur¬†1.\nDapla Lab tilbyr √• tilgjengliggj√∏re lagringsb√∏tter som filsystem inne i tjenesten. Se mer om √• jobbe med data her.\n\n\nPrinte XML i Jupyter\nNoen ganger kan det v√¶re nyttig √• se en xml-fil direkte. Da kan vi lese den inn i Jupyter og printe den ut. Her er et eksempel p√• hvordan vi kan gj√∏re det, samtidig som vi formatterer den med xml.dom.minidom:\nfrom xml.dom.minidom import parseString\n\nfrom dapla import FileClient\n\n# Kobler oss p√• b√∏ttene\nfs = FileClient.get_gcs_file_system()\n\n# Sett inn filstien din her\nfile = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/form_dc551844cd74.xml\"\n\ndom = parseString(fs.cat_file(file))\npretty_xml = dom.toprettyxml(indent=\"  \")\nprint(pretty_xml)\nVi brukte da fs.cat_file() til √• hente inn innholdet i mappen, gjorde det om til xml-object, og formatterer outputen med xml.dom.minidom. Resultatet vil da se ut noe s√•nt som dette:\n&lt;?xml version=\"1.0\" ?&gt;\n&lt;melding xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" dataFormatProvider=\"SERES\" dataFormatId=\"7079\" dataFormatVersion=\"47315\"&gt;\n  &lt;InternInfo&gt;\n    &lt;raNummer&gt;RA-XXXX&lt;/raNummer&gt;\n    &lt;skjemaVersjon&gt;1.0&lt;/skjemaVersjon&gt;\n    &lt;undersoekelsesNr&gt;XXXXX&lt;/undersoekelsesNr&gt;\n    &lt;visOppgaveByrde&gt;0&lt;/visOppgaveByrde&gt;\n    &lt;visBrukeropplevelse&gt;0&lt;/visBrukeropplevelse&gt;\n    &lt;delregNr&gt;2XXXXX&lt;/delregNr&gt;\n    &lt;periodeFritekst&gt;3. kvartal 2022&lt;/periodeFritekst&gt;\n    &lt;periodeFomDato&gt;2022-07-01&lt;/periodeFomDato&gt;\n    &lt;periodeTomDato&gt;2022-09-30&lt;/periodeTomDato&gt;\n    &lt;periodeType&gt;KVRT&lt;/periodeType&gt;\n    &lt;periodeNummer&gt;3&lt;/periodeNummer&gt;\n    &lt;periodeAAr&gt;2022&lt;/periodeAAr&gt;\n    &lt;periodeDato&gt; &lt;/periodeDato&gt;\n    &lt;enhetsIdent&gt;XXXXXX&lt;/enhetsIdent&gt;\n    &lt;enhetsType&gt;BEDR&lt;/enhetsType&gt;\n    &lt;enhetsOrgNr&gt;XXXXXXXXX&lt;/enhetsOrgNr&gt;\n    &lt;enhetsNavn&gt;STATISTISK SENTRALBYR√Ö &lt;/enhetsNavn&gt;\n    &lt;enhetsGateadresse&gt;Akersveien 26&lt;/enhetsGateadresse&gt;\n    &lt;enhetsPostnr&gt;0177&lt;/enhetsPostnr&gt;\n    &lt;enhetsPoststed&gt;OSLO&lt;/enhetsPoststed&gt;\n    &lt;enhetsAvdeling&gt;AVD XXXX&lt;/enhetsAvdeling&gt;\n    &lt;reporteeOrgNr&gt;XXXXXXXXX&lt;/reporteeOrgNr&gt;\n  &lt;/InternInfo&gt;\n  &lt;Kontakt&gt;\n    &lt;kontaktPersonNavn&gt;OLA NORDMANN&lt;/kontaktPersonNavn&gt;\n    &lt;kontaktPersonEpost&gt;ola@tull.no&lt;/kontaktPersonEpost&gt;\n    &lt;kontaktPersonTelefon&gt; &lt;/kontaktPersonTelefon&gt;\n    &lt;kontaktInfoBekreftet&gt;1&lt;/kontaktInfoBekreftet&gt;\n    &lt;kontaktInfoKommentar&gt;Dette er en et eksempel for Dapla-manualen&lt;/kontaktInfoKommentar&gt;\n  &lt;/Kontakt&gt;\n  &lt;Skjemadata&gt;\n    &lt;ledigeStillinger&gt;75&lt;/ledigeStillinger&gt;\n    &lt;datoPrefill&gt; &lt;/datoPrefill&gt;\n  &lt;/Skjemadata&gt;\n&lt;/melding&gt;\n\n\nFra XML til Pandas DataFrame\nDe f√¶rreste √∏nsker √• jobbe direkte med XML-filer. Derfor er det nyttig √• kunne transformere XML-filene til et mer brukervennlig format, f.eks. en Pandas Dataframe. Her er et eksempel p√• hvordan vi kan gj√∏re det med Pandas:\n\nimport xml.etree.ElementTree as ET\nimport pandas as pd\nfrom dapla import FileClient\n\ndef single_xml_to_dataframe(file_path: str) -&gt; pd.DataFrame:\n    fs = FileClient.get_gcs_file_system()\n\n    with fs.open(file_path, mode=\"r\") as f:\n        single_xml = f.read()\n\n    root = ET.fromstring(single_xml)\n    intern_info = root.find(\"InternInfo\")\n    kontakt = root.find(\"Kontakt\")\n    skjemadata = root.find(\"Skjemadata\")\n\n    data = []\n    all_tags = set()\n\n    for element in intern_info:\n        all_tags.add(element.tag)\n\n    for element in kontakt:\n        all_tags.add(element.tag)\n\n    for element in skjemadata:\n        all_tags.add(element.tag)\n\n    for tag in all_tags:\n        element = intern_info.find(tag)\n        if element is None:\n            element = kontakt.find(tag)\n        if element is None:\n            element = skjemadata.find(tag)\n        if element is not None:\n            value = element.text\n            data.append(value)\n        else:\n            data.append(None)\n\n    # Include the full path to the XML file in the data list\n    data.append(file_path)\n\n    # Create the DataFrame inside the function\n    df = pd.DataFrame([data], columns=list(all_tags) + [\"pathToXmlFile\"])\n    path_to_xml_file_split = df[\"pathToXmlFile\"].str.split(\"/\")\n    return df\n\n# Run the function\nfile_path = \"gs://ssb-suv-altinn-ra0678-01-prod/2023/3/24/28c34dce4232_e7bed8a4-def5-42dc-b1e2-b9fc66beb769/form_28c34dce4232.xml\"\ndf = single_xml_to_dataframe(file_path=file_path)\nI funksjonen s√• s√∏ker vi etter alle elementer under taggene intern_info, kontakt og skjemadata. Dvs. at vi lager en dataframe med en rad, der vi tar med all data fra xml-filen. Til slutt legger vi til en kolonne med filstien til filen. Det kan v√¶re nyttig senere hvis man g√• tilbake til xml-filen for √• sjekke om konverteringen til en dataframe er riktig.\nFunksjonen single_xml_to_dataframe() kan brukes til √• loope over en liste med filstier (vi viste hvordan man lager en slik liste tidligere i kapitlet). Men et typisk problem som vil oppst√• da er at xml-filene har forskjellig antall elementer i seg, som gir dataframes med ulikt antall kolonner, og derfor vil ikke Pandas vite hvordan man konkatinerer disse. For √• fikse dette m√• du modifisere funksjonen til √• ta h√∏yde for dette.\n\n\nKopiere filer manuelt\nHvis vi √∏nsker √• kopiere filer manuelt (dvs. ikke med Transfer Service) fra Team SUV sine b√∏tter til egen kildeb√∏tte, kan vi gj√∏re det fra Jupyter. Vi har tidligere sett hvordan vi kan kopiere enkeltfiler mellom to b√∏tter. Under viser vi hvordan man kan kopiere alle filer under en viss filsti med kommandoen fs.copy():\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfrom_path = \"gs://ssb-prod-arbmark-skjema-data-kilde/ledstill/altinn/2022/11/21/\"\nto_path = \"gs://ssb-prod-arbmark-skjema-data-kilde/ledstill/altinn/test/\"\nfs.copy(from_path, to_path, recursive=True)\nI koden over s√• kopierer vi alle filer under from_path til to_path. Vi bruker recursive=True for √• s√∏rge for at vi kopierer alle filer under from_path.\nI eksempelet over s√• kopierer vi over alt som er from_path og dets undermapper. I tilfellet med Altinn-data s√• ligger det ogs√• pdf-filer av skjemaet som kanskje ikke √∏nsker √• kopiere. I de tilfellene kan vi f√∏rst s√∏ke etter de filene vi √∏nsker √• kopiere og legge de i en liste. Deretter kan vi kopiere over en og en fil fra listen. Denne tiln√¶rmingen er veldig nyttig hvis vi √∏nsker √• filtrere ut filer som ikke er XML-filer, eller vi √∏nsker en annen mappestruktur en den som ligger i from_path. Her er en m√•te vi kan gj√∏re det p√•:\nfrom dapla import FileClient\n\n# Lager en filsystem-instans av GCS\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ssb-suv-altinn-ra0678-01-prod/2023/3/10/**.xml\")\n\n# Stien du √∏nsker √• kopiere til.\n# Koden under foutsetter at du har med gs:// f√∏rst\nto_folder = \"&lt;b√∏ttenavn&gt;/**.xml\"\n\n# Kopierer over filene\nfor file in xml_files:\n    from_bucket = file.split(\"/\")[0]\n    to_bucket = to_folder.split(\"/\")[2]\n    to_path = file.replace(\n        from_bucket, to_bucket\n    )\n    fs.cp(file, to_path)\nI koden over s√• bruker vi fs.glob() og ** til √• s√∏ke rekursivt etter alle xml-filer under filstien gs://ssb-suv-altinn-ra0678-01-prod/2023/3/10. Deretter kopierer vi over filene til egen kildeb√∏tte med fs.cp(). N√•r vi skal kopiere over til en ny b√∏tte m√• vi bestemme oss for hvor filene skal ligge og hvor mye av den gamle filstien vi skal beholde. Anta at vi skal beholde hele mappestrukturen i Team SUV sin b√∏tte. Da kan vi egentlig bare erstatte ssb-suv-altinn-ra0678-01-prod/ med den nye b√∏tte-navnet, og vi vil f√• den samme strukturen som i Team SUV sin b√∏tte."
  },
  {
    "objectID": "statistikkere/altinn-jobbe-med-altinn3.html#footnotes",
    "href": "statistikkere/altinn-jobbe-med-altinn3.html#footnotes",
    "title": "Jobbe med Altinn 3",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nEn typisk prosessering som de fleste vil √∏nske √• gj√∏re er √• konvertere fra xml-formatet det kom p√•, og over til parquet-formatet.‚Ü©Ô∏é\nDu kan g√• inn i Google Cloud Console og s√∏ke opp prosjektet til Team SUV som de bruker for √• dele data. Det heter suv-altinn-data-p, og du finner b√∏ttene ved √• klikke deg inn p√• Cloud Storage‚Ü©Ô∏é\nB√∏ttenavnet starter alltid med RA-nummeret til unders√∏kelsen.‚Ü©Ô∏é\nAlternativt oppretter du en mappe direkte vinduet ved √• trykke p√• mappe-ikonet med en +-tegn i seg.‚Ü©Ô∏é\nFor √• jobbe mot data i GCS som i et ‚Äúvanlig‚Äù filsystem kan vi bruke FileClient.get_gcs_file_system() fra dapla-toolbelt.‚Ü©Ô∏é"
  },
  {
    "objectID": "statistikkere/opprette-dapla-team.html",
    "href": "statistikkere/opprette-dapla-team.html",
    "title": "Opprette Dapla-team",
    "section": "",
    "text": "Opprette Dapla-team\nFor √• komme i gang med √• opprette et Dapla-team trengs det en oversikt over teamets medlemmer og hvilke tilgangsgrupper medlemmene skal v√¶re med i. Det trengs ogs√• informasjon om hvilke Dapla-tjenester som er aktuelle for teamet √• ta i bruk. Derfor har det blitt opprettet en egen veileder for dette kalt Dapla Start.\n\n\n\n\n\n\nG√• til Dapla Start for starte bestilling av et nytt Dapla-team.\n\n\n\nN√•r teamet er opprettet f√•r alle medlemmene tilgang til sitt eget prosjekt i Google Cloud Platform (GCP), som er SSBs leverand√∏r av skytjenester. Videre f√•r hvert prosjekt et sett med tjenester og tilganger som knyttes til teamet. Det opprettes ogs√• datalagringsomr√•der (kalt b√∏tter) som bare kan aksesseres av brukere som er med i teamets tilgangsgrupper.\nDapla-teamet vil ogs√• f√• sin egen gruppe i SSBs Active Directory slik at medlemskapet i gruppen kan administreres av Kundeservice."
  },
  {
    "objectID": "statistikkere/dapla-lab.html",
    "href": "statistikkere/dapla-lab.html",
    "title": "Dapla Lab",
    "section": "",
    "text": "Dapla Lab er SSBs arbeidsbenk for statistikkproduksjon og forskning. L√∏sningen er bygget p√• INSEE sin plattform Onyxia. Form√•let med Dapla Lab er √• kunne tilby moderne skybaserte dataverkt√∏y til SSB-ere p√• en effektiv og enhetlig m√•te. Dapla Lab gir brukeren en enkel oversikt over hvilke verkt√∏y som tilbys, b√•de internt utviklet programvare og velkjente open-source verkt√∏y. Alle tjenestene kan konfigureres etter brukerens √∏nsker.",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/dapla-lab.html#innlogging",
    "href": "statistikkere/dapla-lab.html#innlogging",
    "title": "Dapla Lab",
    "section": "Innlogging",
    "text": "Innlogging\nAlle som er p√• SSBs nettverk kan logge seg inn i Dapla ved √• g√• inn p√• nettadressen https://lab.dapla.ssb.no/ og velge Logg inn √∏verst i h√∏yre hj√∏rne. Figur¬†1 viser landingssiden som m√∏ter brukeren.\n\n\n\n\n\n\nFigur¬†1: Landingsside for Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/dapla-lab.html#funksjonalitet",
    "href": "statistikkere/dapla-lab.html#funksjonalitet",
    "title": "Dapla Lab",
    "section": "Funksjonalitet",
    "text": "Funksjonalitet\n\n\nSelv om Dapla Lab er en l√∏sning for √• tilby tjenester s√• er det ogs√• en del nyttig funksjonalitet Dapla Lab. Figur¬†2 viser menyen i Dapla Lab som gir en oversikt over funksjonaliteten som finnes. Under beskriver vi n√¶rmere hvordan man blant annet kan:\n\ndefinere brukernavn og e-post for Git\nlagre GitHub-token\nlagre tjenestekonfigurasjon\nf√• oversikt over hvilke tjenester man har kj√∏rende\npause en tjeneste\n\nMenyen i Figur¬†2 inkluderer ogs√• lenker til andre nettsteder som er nyttig n√•r man jobber med data p√• Dapla. Meny-innslagene Dapla Manualen, Dapla Ctrl, Google Cloud Console og FAQ er alle lenker til eksterne ressurser.\n\n\n\n\n\n\n\n\n\nFigur¬†2: Menyen i Dapla Lab\n\n\n\n\n\n\nHjem\nHjem tar deg til landingssiden i Dapla Lab, slik som vist i Figur¬†1. Her finner du nyttige lenker til l√¶ringsressurser for Dapla, felleskap p√• Viva Engage og opprettelse av Dapla-team.\n\n\nMin konto\nN√•r man logger seg inn i Dapla Lab s√• skjer det SSB-kontoen til brukeren. Under Min konto kan man se informasjon om sin konto og konfigurere noen nyttige verdier knyttet til brukeren din.\n\nKontoinformasjon\nUnder denne fanen kan lese ut hvilken bruker-id som er benyttet for innloggingen i Dapla Lab, ditt fulle navn og e-postadresse i Dapla Lab. Informasjonen blir definert ved innlogging og kan ikke endres i Dapla Lab.\n\n\n\n\n\n\nFigur¬†3: Kontoinformasjon under Min konto i Dapla Lab\n\n\n\n\n\nGit\nUnder fanen Git kan man definere brukernavn og e-post for Git, og et personlig tilgangstoken for GitHub. Dette vil deretter kunne brukes i tjenester som brukeren starter i Dapla Lab.\n\n\n\n\n\n\nFigur¬†4: Git-konfigurasjon under Min konto i Dapla Lab\n\n\n\n\n\nGrensesnittpreferanser\nUnder fanen Grensesnittpreferanser kan man tilpasse Dapla Lab til sine preferanser ved √• velge om man blant annet √∏nsker Dark mode eller ikke. I tillegg kan man definere hvilket spr√•k man √∏nsker i Dapla Lab. Det finnes ogs√• avanserte valg for avanserte brukere. F.eks. man √∏nsker √• se hvilke Helm-kommandoer som kj√∏res i bakgrunnen n√•r man starter en tjeneste.\n\n\n\n\n\n\nFigur¬†5: Grensesnittpreferanser under Min konto i Dapla Lab\n\n\n\n\n\n\nTjenestekatalog\nUnder Tjenestekatalogen ligger alle tjenestene som brukeren kan velge √• starte.\n\n\n\n\n\n\nFigur¬†6: Tjenestekatalogen i Dapla Lab\n\n\n\nFigur¬†6 viser hvilke tjenester som n√• er tilgjengelig i Dapla Lab, inkludert en kort beskrivelse av bruksomr√•det for hver tjeneste. Figur¬†7 viser hva som m√∏ter n√•r de starter Jupyter-tjenesten.\n\n\nTjenestekonfigurasjon\nAlle tjenester p√• Dapla Lab kan konfigureres f√∏r de startes opp. Trykker man p√• Start p√• en av tjenestene i tjenestekatalogen kommer man inn tjenestekonfigurasjon for akkurat den tjenesten. Felles for alle tjenester er at man kan navngi hver tjeneste og velge versjon1, slik som vist under Vennlig navn og Versjon i Figur¬†7.\n\n\n\n\n\n\nFigur¬†7: Tjenestekonfigurasjon i Dapla Lab\n\n\n\nEkspanderer man Jupyter konfigurasjoner vist i Figur¬†7, s√• f√•r man opp konfigurasjon som er spesifikk for akkurat den tjeneste. Hver tjenestetilbyder vurderer hvilken konfigurasjon som gir mening for den tjenesten de tilbyr.\nFor programmeringsmilj√∏er som Jupyter og VS Code kan brukeren velge hvilket team og tilgangsgruppe de skal representere, hvor mye ram og gpu de √∏nsker, hvor stor diskplass de √∏nsker, Git/GitHub-oppsett, etc..\nI Datadoc-tjenesten har tilbyderen kun valgt √• la brukeren velge hvilket team de representerer og versjon av tjenesten. Les mer om tjenestekonfigurasjonen til en tjeneste i dokumentasjonen til tjenesten.\n\nLagre tjenestekonfigurasjon\nVanligvis vil brukeren √∏nske √• starte en tjeneste med samme konfigurasjon som sist. Dapla Lab tilbyr derfor at du kan lagre en tjenestekonfigurasjon med egenvalgt navn. Etter at du har valgt verdiene du √∏nsker i tjenestekonfigurasjonen s√• trykker du p√• Lagre-ikonet vist i Figur¬†7. Deretter kan du se dine lagrede konfigurasjoner under Mine tjenester, slik som vist i Figur¬†8.\n\n\n\n\n\n\nFigur¬†8: Tjenestekonfigurasjon i Dapla Lab\n\n\n\n\n\nDele tjenestekonfigurasjon\nMan kan ogs√• dele sin tjenestekonfigurasjon med andre i SSB. Det forutsetter at de man deler med har de samme datatilgangene som den som deler. Man kan dele lagrede tjenestekonfigurasjoner ved √• g√• til Mine tjenester, trykke p√• de tre prikkene til h√∏yre i tjenesten i ikonet, og deretter Kopier URL-lenke, slik som vist i Figur¬†9. Deretter er det bare √• sende lenken til en kollega, og de kan √•pne en likt konfigurert tjeneste med sine tilganger.\n\n\n\n\n\n\nFigur¬†9: Dele lagret tjenestekonfigurasjon i Dapla Lab\n\n\n\n\n\n\n\n\n\nNoe konfigurasjon kan ikke deles\n\n\n\nKonfigurasjon som er knyttet brukerkonfigurasjon fra Dapla Lab, f.eks. GitHub-token, m√• settes manuelt av den man deler konfigurasjon med. Dette vil forh√•pentligvis forbedres etter hvert.\n\n\n\n\n\nMine tjenester\nUnder Mine tjenester f√•r man oversikt over hvilke tjenester som er startet av brukeren. Figur¬†10 viser en bruker som har 3 tjenester kj√∏rende. Her f√•r man informasjon om hvilken tjeneste som er starter, hvor lenge den har kj√∏rt, og muligheten til √• pause eller avslutte tjenesten.\n\n\n\n\n\n\nFigur¬†10: Oversikt over brukerens kj√∏rende tjenester\n\n\n\nHvis man trykker p√• s√∏ppelkasse-ikonet s√• avsluttes tjenesten og alt som er lagret inne i tjenesten blir slettet. Hvis man trykker p√• pause-knappen s√• bevares alt som brukeren har lagret under $HOME/work, mens alt annet blir slettet.\n\n\n\n\n\n\nViktigheten av √• avslutte ubrukte tjenester\n\n\n\nEn tjeneste som st√•r som aktiv vil reservere ressursene (CPU, GPU, RAM, etc.) som brukeren valgte ved oppstart. Hvis tjenesten ikke benyttes b√∏r derfor brukeren enten avslutte eller pause tjenesten, slik at SSB ikke m√• betale for ubrukte ressurser.\n\n\n\n\nMonitorering\n\n\n\n\n\n\nAll monitorering er ikke p√• plass enda\n\n\n\nInnholdet p√• Overv√•kningssiden til tjenestene er fullstendig enda. N√•r du kommer inn p√• siden s√• skal loggene fra tjenesten viser, men dette er ikke p√• plass enda. Dette jobbes det med √• f√• p√• plass.\nDerimot fungerer Ekstern overv√•kning-lenken (se beskrivelse under) og den tar deg til et Grafana-dashboard som viser vanlige metrikker for tjenesten. I tillegg kan man trykke p√• lenken Helm-verdier som teknisk informasjon om hvilke verdier som ble satt n√•r tjenesten ble startet.\n\n\nUnder Mine tjenester f√•r du oversikt over hvilke kj√∏rende tjenester. Hvis du √∏nsker √• monitorere hvor mye ram, cpu diskplass eller gpu tjenester bruker, s√• kan du inspisere et ferdig oppsatt Grafana-dashboard. For √•pne dashboardet trykker du f√∏rst p√• navnet p√• tjenesten du √∏nsker √• monitorere, slik som vist i Figur¬†11 (a). Det √•pner en side for Overv√•kning av tjenesten. P√• denne siden er det en lenke til et Grafana-dashboard, slik som vist i Figur¬†11 (b).\n\n\n\n\n\n\n\n\n\n\n\n(a) √Öpne Overv√•kningssiden\n\n\n\n\n\n\n\n\n\n\n\n(b) √Öpne Grafana-dashboard\n\n\n\n\n\n\n\nFigur¬†11: √Öpne Grafana-dashboard for for kj√∏rende tjenester\n\n\n\nFigur¬†12 viser hvordan et Grafana-dashboard ser ut.\n\n\n\n\n\n\nFigur¬†12: Grafana dashbaordet for en spesifikk tjeneste p√• Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/dapla-lab.html#datatilgang",
    "href": "statistikkere/dapla-lab.html#datatilgang",
    "title": "Dapla Lab",
    "section": "Datatilgang",
    "text": "Datatilgang\nBrukere som skal ha tilgang til data fra en tjeneste m√• f√∏rst oppgi hvilket team og tilgangsgruppe2 de skal representere. Siden brukere ofte er medlem av flere team, s√• er dette et viktig sikkerhetstiltak for √• sikre at data ikke kobles p√• tvers av team uten at dette er godkjent av data-ansvarlige.\nFor √• f√• tilgang til lagringsb√∏ttene i prod-prosjektet til et team, s√• m√• man logge seg inn i prod-milj√∏et til Dapla Lab (https://lab.dapla.ssb.no/). Skal man ha tilgang til lagringsb√∏ttene i test-prosjektet til et team m√• man logge seg inn i test-milj√∏et til Dapla Lab (https://lab.dapla-test.ssb.no/). For team som har er et dev-milj√∏ s√• gjelder f√∏lgende dev-milj√∏et til Dapla Lab (https://lab.dapla-test.ssb.no/).\n\nB√∏tter som filsystem\nTjenestene i Dapla Lab gj√∏r teamets b√∏tter tilgjengelig som mapper i filsystemet i tjenesten. Det vil si at man kan referere til data som man er vant til p√• vanlige filsystem, og man kan bruke biblioteker uten √• autentisere seg mot b√∏tter.\nAlle tjenester som tilgjengeliggj√∏r data fra b√∏tter monterer filsystemet p√• stien /buckets/. Videre representeres b√∏ttene ved sitt kortnavn. F.eks. vil b√∏ttestien gs://ssb-dapla-felles-data-produkt-prod/ representeres som /buckets/produkt/ i tjenesten.\n\n\nJobbe med data\nSiden Dapla Lab tilbyr √• tilgjengliggj√∏re lagringsb√∏tter som filsystem inne i tjenesten, s√• finnes det n√• to m√•ter √• aksessere data p√•:\n\nBruke vanlige pakker som Pandas, Polars, Pyarrow, etc. mot filsystemet under /buckets/.\nDen ‚Äúgamle‚Äù m√•ten med dapla-toolbelt som er et overbygg over Pandas og Pyarrow3.\n\nDet er anbefalt at alle benytter seg av alternativ 1 siden det er enklere for de fleste og gj√∏r at alle medlemmer av et team kan se hverandres endringer n√•r man jobber mot samme b√∏tte (se boks under).\n\n\n\n\n\n\nEksterne endringer i b√∏tter\n\n\n\nHvis to brukere √•pner en tjeneste med den samme team og tilgangsgruppe, s√• vil man kun se hverandres endringer i filsystemet4 hvis begge jobber direkte mot buckets-filstien. Hvis en av de skriver filer med dapla-toolbelt, mens den andre bruker dapla-toolbelt eller et annet verkt√∏y, s√• vil man ikke se dette i filsystemet inne i tjenesten. Brukeren kan da kj√∏re refresh-buckets fra terminalen i tjenesten for √• se hva som har dukket opp. Vi anbefaler derfor alle √• bruke buckets-tiln√¶rmingen.\nHvis brukeren refererer til en fil som finnes i b√∏tta, men som ikke synes i filsystemet, s√• vil det fortsatt kunne leses inn. Dette gjelder ogs√• for filer produsert av Kildomaten. Fremover kommer vi til √• tilpasse K",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/dapla-lab.html#milj√∏er",
    "href": "statistikkere/dapla-lab.html#milj√∏er",
    "title": "Dapla Lab",
    "section": "Milj√∏er",
    "text": "Milj√∏er\nDet finnes 2 adskilte milj√∏er for Dapla Lab: prod og test. Tabell¬†1 viser hvilke url-er som gjelder for de ulike milj√∏ene.\n\n\n\nTabell¬†1: Oversikt over milj√∏er og tilh√∏rende url-er for Dapla Lab.\n\n\n\n\n\nMilj√∏\nUrl\n\n\n\n\nProd\nhttps://lab.dapla.ssb.no/\n\n\nTest\nhttps://lab.dapla-test.ssb.no/\n\n\n\n\n\n\nMilj√∏ene er knyttet til datatilgang for prosjektene til Dapla-team. Hvert Dapla-team kan ha ressurser i prod- eller test-milj√∏et. For √• f√• tilgang til ressursene i et av milj√∏ene m√• de logge seg inn p√• tilsvarende milj√∏ i Dapla Lab. Det er f.eks. ikke mulig √• aksessere prod-data fra test-milj√∏et i Dapla Lab og omvendt.",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/dapla-lab.html#automatisk-pausing",
    "href": "statistikkere/dapla-lab.html#automatisk-pausing",
    "title": "Dapla Lab",
    "section": "Automatisk pausing",
    "text": "Automatisk pausing\nHver kveld klokka 22:00 blir alle tjenester som kj√∏rer p√• Dapla pauset automatisk. Det gj√∏res for redusere ressursbruken og redusere kostnader.\nHvis man har tjenester som trenger √• kj√∏re hele natten, s√• kan man melde tjenesten uten av pausingen ved √• gi tjenestens visningsnavn et suffix med teksten [nosuspend], slik som vist i Figur¬†13.\n\n\n\n\n\n\nFigur¬†13: Eksempel p√• en tjeneste som ikke blir pauset hver kveld kl. 22.\n\n\n\nPausingen vil til enhver tid s√∏ke etter tjenesten som ikke har dette suffixet, og derfor kan du endre visningsnavnet n√•r som helst ved √• trykke p√• üñäÔ∏è-ikonet, for √• aktivere eller deaktivere dette.",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/dapla-lab.html#footnotes",
    "href": "statistikkere/dapla-lab.html#footnotes",
    "title": "Dapla Lab",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nDet er sjelden behov for √• brukere √• endre versjon her.‚Ü©Ô∏é\nHvis en bruker er medlem i b√•de data-admins- og developers-gruppa til et team, s√• m√• de velge hvilken av de to gruppene de skal representere i tjenesten som startes.‚Ü©Ô∏é\ndapla-toolbelt er en pakke som ble bygget som et overbygg over Pandas og Pyarrow slik at det ble lettere √• lese/skrive mot b√∏tter. Med b√∏tter som filsystem inne i tjenestene er ikke dette lenger n√∏dvendig.‚Ü©Ô∏é\nMed endringer menes her at man oppretter en ny mappe‚Ü©Ô∏é",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/altinn-bygge-prefill.html",
    "href": "statistikkere/altinn-bygge-prefill.html",
    "title": "Bygge skjemaprefill",
    "section": "",
    "text": "Denne siden forklarer hvordan du g√•r fram for √• bygge din egen skjemaprefill. Med skjemaprefill menes prefill som g√•r ut over det som er standard prefill i skjemaet."
  },
  {
    "objectID": "statistikkere/altinn-bygge-prefill.html#skjema-prefill-meta",
    "href": "statistikkere/altinn-bygge-prefill.html#skjema-prefill-meta",
    "title": "Bygge skjemaprefill",
    "section": "Skjema prefill meta",
    "text": "Skjema prefill meta\nFor hver skjemaversjon eksisterer det en skjema prefill meta tabell. Tabellen gir informasjon om hvilke skjemaspesifikke prefill-felter som skjemaet kan inneholde.\n\n\n\n\n\n\nFigur¬†1: Eksempel p√• skjema prefill meta tabell\n\n\n\n\n\n\n\n\n\nHva hvis skjemaprefill meta mangler?\n\n\n\nTa kontakt med planleggeren for skjemaet p√• seksjon 821 dersom skjemaet ditt skal inneholde skjemaprefill og meta-tabellen ikke inneholder data.\n\n\n\nMetadata beskrivelse\nTabell¬†1 beskriver feltene i skjemaprefill meta-tabellen.\n\n\n\nTabell¬†1: Prefill Meta\n\n\n\n\n\n\n\n\n\nMeta\nForklaring\n\n\n\n\nKomplett sti\nRefererer til hele stien som brukes for √• navigere fra SkjemaData til en spesifikk verdi. Den beskriver n√∏yaktig plasseringen av dataene i hierarkiet og kan best√• av flere niv√•er.\n\n\nType\nBeskriver hvilken datatype feltet kan inneholde.\n\n\nMin\nBegrensing (minimum) p√• datatypen.\n\n\nMaks\nBegrensing (makimum) p√• datatypen.\n\n\nObligatorisk\nIndikerer om feltet er p√•krevd for prefill.\n\n\nStatistikk navn\nKan brukes for mapping til statistikkteamenes interne systemer.\n\n\nKommentar\nEventuelle merknader eller tilleggsinformasjon om feltet.\n\n\n\n\n\n\n\n\nHente prefill meta med kode\nFor √• hente ut skjema prefill meta i Python, kan du bruke metoden get_prefill_meta_by_skjema_def i SuvClient. S√∏rg for at du oppgir riktig RA-nummer, versjon og unders√∏kelsesnummer.\n\n\nnotebook\n\nclient = SuvClient()\n\noutput = client.get_prefill_meta_by_skjema_def(\n            ra_nummer = 'RA-0666A3',\n            versjon = 1,\n            undersokelse_nr = '1060'\n        )\n\nprint(json.dumps(output, indent=4))"
  },
  {
    "objectID": "statistikkere/altinn-bygge-prefill.html#struktur-p√•-skjemaprefill",
    "href": "statistikkere/altinn-bygge-prefill.html#struktur-p√•-skjemaprefill",
    "title": "Bygge skjemaprefill",
    "section": "Struktur p√• skjemaprefill",
    "text": "Struktur p√• skjemaprefill\nSkjemaprefill kan representeres som en hierarkisk struktur basert p√• sti_navn. Dette gir en oversikt over hvordan dataene er organisert. F√∏lgende kode viser hvordan du kan vise strukturen med valgfri inkludering av metadata.\n\n\nnotebook\n\nclient = SuvClient()\n\nresultat = client.get_prefill_meta_by_skjema_def(\n                ra_nummer = 'RA-0666A3',\n                versjon = 1,\n                undersokelse_nr = '1060'            \n)\n\ndef build_structure(data, include_metadata=False):\n    structure = {}\n\n    for item in data:\n        path = item['sti_navn'].split('.') if item['sti_navn'] else [item['navn']]\n        current_level = structure\n\n        for part in path:\n            if part not in current_level:\n                current_level[part] = {\"metadata\": {}} if include_metadata else {}\n            current_level = current_level[part]\n\n        if include_metadata:\n            current_level[\"metadata\"][\"type\"] = item.get(\"type\")\n            current_level[\"metadata\"][\"kontroll\"] = item.get(\"kontroll\")\n            current_level[\"metadata\"][\"stat_navn\"] = item.get(\"stat_navn\")\n            current_level[\"metadata\"][\"kommentar\"] = item.get(\"kommentar\")\n\n    return structure          \n\nresult_without_metadata = build_structure(resultat, include_metadata=False)\nprint(json.dumps(result_without_metadata, indent=4))\n\nEksempel p√• output for hierarkisk struktur uten metadata:\n{\n    \"innkjoptElektriskKraft\": {},\n    \"innkjoptElektriskKost\": {},\n    \"GassOgPetroleumKost\": {\n        \"produktTypeBruktId\": {},\n        \"produktTypeBrukt\": {},\n        \"produktEnhet\": {}\n    },\n    \"EgneprodEnergiProd\": {\n        \"egenprodEnergiProdTypeID\": {},\n        \"egenprodEnergiProdType\": {},\n        \"egenprodEnergiProdEnhet\": {}\n    }\n}"
  },
  {
    "objectID": "statistikkere/altinn-bygge-prefill.html#lagre-prefill-data",
    "href": "statistikkere/altinn-bygge-prefill.html#lagre-prefill-data",
    "title": "Bygge skjemaprefill",
    "section": "Lagre prefill data",
    "text": "Lagre prefill data\nFor √• lagre prefill for en enhet bruker du metoden save_prefill_for_enhet i SuvClient. Metoden lagrer skjemaprefill for en spesifikk enhet i utvalget. Beskrivelse av hvordan du henter utvalg finner du p√• siden Utvalg fra SFU\n\n\nnotebook\n\nclient = SuvClient()\n\noutput = client.save_prefill_for_enhet(\n     ra_nummer = 'RA-0666A3',\n     versjon = 1,\n     periode_aar = 2024,\n     periode_type = 'KVRT',\n     periode_nr = 2,\n     enhetsident = 'A3TF0018',\n     enhetstype = 'FRTK',\n     prefill = {\n        \"innkjoptElektriskKost\": 10, \n        \"innkjoptElektriskKraft\": 20, \n        \"GassOgPetroleumKost\": [\n            {\"produktTypeBruktId\": \"1\"},\n            {\"produktTypeBruktId\": \"2\"}\n        ]\n    }                  \n )\nprint(output)    \n\nTabell¬†2 viser en beskrivelse av de ulike parameterne i save_prefill_for_enhet\n\n\n\nTabell¬†2: Beskrivelse av parametere\n\n\n\n\n\n\n\n\n\nParameter\nForklaring\n\n\n\n\nRA-nummer\nRA-nummer for skjemaet.\n\n\nversjon\nSkjemaversjon.\n\n\nperiode_aar\n√Örstall for perioden.\n\n\nperiode_type\nPeriode type. Gyldige verdier er AAR, MND, KVRT, UKE\n\n\nperiode_nr\nPeriode nummer\n\n\nenhetsident\nIdentifikator for enheten\n\n\nenhetstype\nType enhet. Gyldige verdier er FRTK, BEDR, PERS\n\n\nprefill\nPrefill-data i gyldig JSON-format."
  },
  {
    "objectID": "statistikkere/altinn-bygge-prefill.html#validering-av-prefill-data",
    "href": "statistikkere/altinn-bygge-prefill.html#validering-av-prefill-data",
    "title": "Bygge skjemaprefill",
    "section": "Validering av prefill data",
    "text": "Validering av prefill data\nValidering av skjemaprefill sikrer at dataene oppfyller kravene til struktur og innhold. Dette gj√∏res automatisk i save_prefill_for_enhet. Feil i valideringen vil f√∏re til at dataene ikke lagres, og det gis en feilmelding. Dersom du √∏nsker √• sjekke om prefill er gyldig f√∏r lagring er dette mulig ved hjelp av metoden validate_skjemadata.\n\n\nnotebook\n\nclient = SuvClient()\n\nclient.validate_skjemadata(      \n        ra_nummer = 'RA-0678A3',\n        versjon = 2,\n        skjemadata = {\"antallAnsattePrefill\": \"10\"}\n)   \n\ndapla-suv-tools pakken inneholder ogs√• metoder for √• hente ut lagret prefill og slette prefill. Det er mulig √• slette prefill enten p√• enhetsniv√• eller skjemaniv√•.\nValideringen er basert p√• JSON Schema.\n\n\n\n\n\n\nWarning\n\n\n\nDersom skjemaet blir instansiert med ugyldige prefill data vil skjemaet feile ved instansiering hos Altinn. Skjemaet vil i dette tilfellet bli slettet fra innboksen til oppdragsgiver."
  },
  {
    "objectID": "statistikkere/altinn-bygge-prefill.html#eksempelkode",
    "href": "statistikkere/altinn-bygge-prefill.html#eksempelkode",
    "title": "Bygge skjemaprefill",
    "section": "Eksempelkode",
    "text": "Eksempelkode\nNoe demokode ligger i repoet, og kan v√¶re ett godt utgangspunkt √• kopiere og endre fra.\n\nRA-0678 (Ledige stillinger)\nDette eksemplet viser hvordan du kan bygge skjemaprefill for RA-0678 (Ledige stillinger). Nedenfor ser du hvordan innholdet i SkjemaData blokka er strukturert.\n{\n    \"SkjemaData\": {\n        \"antallAnsattePrefill\": {\n            \"type\": \"string\"\n        },\n        \"datoPrefill\": {\n            \"type\": \"string\"\n        }\n    }\n}\nSkjemaet inneholder to felter som skal forh√•ndsutfylles. Det er antallAnsattePrefill og datoPrefill.\nTabell¬†3 beskriver feltene som kan forh√•ndsutfylles.\n\n\n\nTabell¬†3: Prefill felter\n\n\n\n\n\n\n\n\n\nFelt\nSti\n\n\n\n\nantallAnsattePrefill\n-\n\n\ndatoPrefill\n-\n\n\n\n\n\n\nEksempelet leser prefill-data fra en tekstfil og bygger den n√∏dvendige strukturen for √• forh√•ndsutfylle skjemaet for utvalgte enheter.\n\n\nRA-0692 (Utenrikshandel med tjenester)\nDette eksemplet viser hvordan du kan bygge skjemaprefill for RA-0692 (Utenrikshandel med tjenester). Nedenfor ser du hvordan innholdet i SkjemaData blokka er strukturert.\n{\n   \"SkjemaData\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"leveransetype\": {\n                \"type\": \"string\",\n                \"enum\": [\n                    \"0\",\n                    \"1\",\n                    \"2\"\n                  ]\n            },\n            \"Eksport\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"$ref\": \"#/$defs/Eksport\"                \n              }\n            },\n            \"Import\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"$ref\": \"#/$defs/Import\"\n            }\n        }\n    },   \n    \"Eksport\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"cpaLevel1Eksport\": {\n                \"type\": \"string\"        \n            },\n            \"cpaEksport\": {\n                \"type\": \"string\"\n            },\n            \"PostEksport\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"$ref\": \"#/$defs/PostEksport\"\n            }\n        }\n    },\n    \"Import\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"cpaLevel1Import\": {\n                \"type\": \"string\",          \n            },\n            \"cpaImport\": {\n                \"type\": \"string\"\n            },\n            \"PostImport\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"$ref\": \"#/$defs/PostImport\"\n            }\n        } \n    },\n    \"PostEksport\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"landkodeEksport\": {\n                \"type\": \"string\"\n            },\n            \"forrigeKvartalKrEksport\": {\n                \"type\": \"integer\"\n            },\n            \"forrigeKonsernIntKrEksport\": {\n                \"type\": \"integer\"\n            },\n        }\n    },\n    \"PostImport\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"landkodeImport\": {\n                \"type\": \"string\",\n            },\n            \"forrigeKvartalKrImport\": {\n                \"type\": \"integer\"\n            },\n            \"forrigeKonsernIntKrImport\": {\n                \"type\": \"integer\"\n            },\n        }\n    }\n}\nMerk at b√•de Eksport/Import og PostEksport/PostImport er repeterende. Den ytterste gruppa er CPA-verdien, mens den innerste gruppa inneholder landskodene og kronebel√∏pene.\nTabell¬†4 beskriver feltene som kan forh√•ndsutfylles.\n\n\n\nTabell¬†4: Prefill felter\n\n\n\n\n\n\n\n\n\nFelt\nSti\n\n\n\n\nleveransetype\n-\n\n\ncpaLevel1Eksport\nEksport\n\n\ncpaEksport\nEksport\n\n\nlandkodeEksport\nEksport.PostEksport\n\n\nforrigeKvartalKrEksport\nEksport.PostEksport\n\n\nforrigeKonsernIntKrEksport\nEksport.PostEksport\n\n\ncpaLevel1Import\nImport\n\n\ncpaImport\nImport\n\n\nlandkodeImport\nImport.PostImport\n\n\nforrigeKvartalKrImport\nImport.PostImport\n\n\nforrigeKonsernIntKrImport\nImport.PostImport\n\n\n\n\n\n\nEksempelet leser prefill-data fra flere tekstfiler og bygger den n√∏dvendige strukturen for √• forh√•ndsutfylle skjemaet for utvalgte enheter."
  },
  {
    "objectID": "statistikkere/statistikkbanken.html",
    "href": "statistikkere/statistikkbanken.html",
    "title": "dapla-statbank-client",
    "section": "",
    "text": "Pakken ‚Äúdapla-statbank-client‚Äù kan brukes til √• overf√∏re tabeller til Statistikkbanken fra Jupyterlab i prodsonen og p√• Dapla. Den henter ogs√• ‚Äúfilbeskrivelsen‚Äù som beskriver formen dataene skal ha n√•r de sendes inn til Statistikkbanken. Og den kan ogs√• hente publiserte data fra Statistikkbanken. Pakken er en python-pakke som baserer seg p√• at dataene (deltabellene) lastes inn i en eller flere pandas DataFrames f√∏r overf√∏ring. Ved √• hente ned ‚Äúfilbeskrivelsen‚Äù kan man validere dataene sine (dataframene) mot denne lokalt, uten √• sende dataene til Statistikkbanken. Dette kan v√¶re til hjelp under setting av formen p√• dataene. √Ö hente publiserte data fra Statistikkbanken kan gj√∏res gjennom l√∏se funksjoner, eller via ‚Äúklienten‚Äù.\nLenker: - Pakken ligger her p√• Pypi. Og kan installeres via poetry med: poetry add dapla-statbank-client - Kodebasen for pakken ligger her, readme-en gir en teknisk innf√∏ring som du kan f√∏lge og kopiere kode fra, og om du finner noe du vil rapportere om bruken av pakken s√• gj√∏r det gjerne under ‚Äúissues‚Äù p√• github-sidene. - Noe demokode ligger i repoet, og kan v√¶re ett godt utgangspunkt √• kopiere og endre fra.\n\n\nStatistikkbanken opererer med ulike databaser for testing og produksjonssetting. Om du vil at tallene faktisk skal oppdateres p√• nettsidene s√• m√• du sende til Statistikkbankens ‚ÄúPROD‚Äù-database. Om du kun vil teste innsending skal du sende til databasen ‚ÄúTEST‚Äù. Disse databasene har de samme lastebrukerne, men passordene er ulike. Om du vil teste innsending m√• du derfor skaffe deg ‚Äútest-passordet‚Äù til den lastebrukeren som du har tilgjengelig. For √• gj√∏re tester via pakken m√• du v√¶re i staging p√• dapla: https://jupyter.dapla-staging.ssb.no/ eller staging i prodosonen: https://sl-jupyter-t.ssb.no/ Om du faktisk vil sende inn data til publisering i Statistikkbanken m√• du v√¶re i dapla-prod (den vanlige): https://jupyter.dapla.ssb.no/ eller i prodsonen p√•: https://sl-jupyter-p.ssb.no/ For √• teste er det fint √• skaffe seg noe data fra fjor√•rets publisering p√• et produksjonsl√∏p man kjenner fra f√∏r. Evt. kan man hente data fra Statistikkbanken og sende disse tilbake snudd rett, og med riktig antall prikke-kolonner lagt til.\n\n\n\nSe mer detaljer i readme-en p√• prosjektets kodebase.\n\n\nFor √• kunne bruke pakken m√• du importere klienten:\n\n\nnotebook\n\nfrom statbank import StatbankClient\n\nS√• initialiserer du klienten med de innstillingene som oftest er faste p√• tvers av alle innsendingene fra ett produksjonsl√∏p:\n\n\nnotebook\n\nstatcli = StatbankClient(date=\"2050-01-01\", overwrite=True, approve=2)\n\nHer vil du bli bedt om √• skrive inn lastebruker, og passordet til lastebrukeren.\n\n\n\nOm du har dataene klare er det bare √• overf√∏re, men du m√• vite navnet p√• deltabell-dat-filene. (Statistikkbanken lagrer disse ned igjen som dat-filer.)\n\n\nnotebook\n\ndata_07495 = {\"kargrs01fylker1.dat\" : df_07495_fylker,\n              \"kargrs01landet1.dat\" : df_07495_landet,}\nstatcli.transfer(data_07495, tableid=\"07495\")\n\nEtter innsending kommer det en link til Statistikkbankens GUI for √• f√∏lge med p√• om innsendingen gikk bra hos dem. Om det var det du √∏nsket, s√• er du n√• ferdig‚Ä¶ Men det finnes mer funksjonalitet her‚Ä¶\n\n\n\nFor √• hente filbeskrivelsen til en hovedtabell bruker du denne metoden under klienten:\n\n\nnotebook\n\nfilbeskrivelse = statcli.get_description(tableid=\"07495\")\nprint(filbeskrivelse)\n\nMed filbeskrivelsen kan du lett f√• en mal p√• dictionaryet du m√• plassere dataene i:\n\n\nnotebook\n\nfilbeskrivelse.transferdata_template()\n\nDu kan ogs√• validere dataene dine mot filbeskrivelsen:\n\n\nnotebook\n\nfilbeskrivelse.validate(data_07495, tableid=\"07495\")\n\n\n\n\n\nDet tas noe ekstra ansvar i pakken rundt avrunding av desimaltall, da filbeskrivelsen inneholder informasjon om hvor mange desimaler som blir lagret per kolonne, kan vi runde av til rett antall samtidig som vi tar ett bevisst valg rundt ‚Äúhvilken vei vi skal runde av‚Äù. P√• barneskolen l√¶rte vi at ved 2,5 avrundet til 0 desimaler, s√• runder vi opp til 3. Det samme skjer i utgangspunktet i SAS og Excel. Python og R runder derimot ‚Äúmot n√¶rmeste partall‚Äù, s√• fra 2,5 blir det rundet til 2, men fra 1,5 blir det ogs√• rundet til 2. Dette er for √• forhindre bias i en retning. Dvs. om alle tall rundes opp, vil en hel kolonne med tall ‚Äúdras oppover‚Äù, ved √• gj√∏re annenhver opp og ned, vil ikke helheten bli ‚Äúdratt en spesifikk vei‚Äù. Siden ‚Äúround to even‚Äù ikke er det folk er vandte til, gj√∏r vi derfor noe annet i denne pakken, enn det som er vanlig oppf√∏rsel i Python. Vi runder opp. Om du bruker f√∏lgende metoden under filbeskrivelsen p√• dataene, s√• vil denne runde oppover, samtidig som den konverterer til en streng for √• bevare formateringen.\n\n\nnotebook\n\ndata_07495 = filbeskrivelse.round_data(data_07495)  # For √• ta vare p√• endringene, s√• m√• du skrive tilbake over variabelen\n\n\n\n\n\nEn date-widget for √• visuelt endre til en valid dato.\nLagring av overf√∏ring og filbeskrivelses-objekter til json-filer\nUthenting av logg fra klienten\nHenting av publisert data fra statbanken\nRotering av tabeller fra statbanken",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-statbank-client"
    ]
  },
  {
    "objectID": "statistikkere/statistikkbanken.html#testoverf√∏ring-fra-staging---faktisk-oppdatering-fra-prod",
    "href": "statistikkere/statistikkbanken.html#testoverf√∏ring-fra-staging---faktisk-oppdatering-fra-prod",
    "title": "dapla-statbank-client",
    "section": "",
    "text": "Statistikkbanken opererer med ulike databaser for testing og produksjonssetting. Om du vil at tallene faktisk skal oppdateres p√• nettsidene s√• m√• du sende til Statistikkbankens ‚ÄúPROD‚Äù-database. Om du kun vil teste innsending skal du sende til databasen ‚ÄúTEST‚Äù. Disse databasene har de samme lastebrukerne, men passordene er ulike. Om du vil teste innsending m√• du derfor skaffe deg ‚Äútest-passordet‚Äù til den lastebrukeren som du har tilgjengelig. For √• gj√∏re tester via pakken m√• du v√¶re i staging p√• dapla: https://jupyter.dapla-staging.ssb.no/ eller staging i prodosonen: https://sl-jupyter-t.ssb.no/ Om du faktisk vil sende inn data til publisering i Statistikkbanken m√• du v√¶re i dapla-prod (den vanlige): https://jupyter.dapla.ssb.no/ eller i prodsonen p√•: https://sl-jupyter-p.ssb.no/ For √• teste er det fint √• skaffe seg noe data fra fjor√•rets publisering p√• et produksjonsl√∏p man kjenner fra f√∏r. Evt. kan man hente data fra Statistikkbanken og sende disse tilbake snudd rett, og med riktig antall prikke-kolonner lagt til.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-statbank-client"
    ]
  },
  {
    "objectID": "statistikkere/statistikkbanken.html#kode-eksempler",
    "href": "statistikkere/statistikkbanken.html#kode-eksempler",
    "title": "dapla-statbank-client",
    "section": "",
    "text": "Se mer detaljer i readme-en p√• prosjektets kodebase.\n\n\nFor √• kunne bruke pakken m√• du importere klienten:\n\n\nnotebook\n\nfrom statbank import StatbankClient\n\nS√• initialiserer du klienten med de innstillingene som oftest er faste p√• tvers av alle innsendingene fra ett produksjonsl√∏p:\n\n\nnotebook\n\nstatcli = StatbankClient(date=\"2050-01-01\", overwrite=True, approve=2)\n\nHer vil du bli bedt om √• skrive inn lastebruker, og passordet til lastebrukeren.\n\n\n\nOm du har dataene klare er det bare √• overf√∏re, men du m√• vite navnet p√• deltabell-dat-filene. (Statistikkbanken lagrer disse ned igjen som dat-filer.)\n\n\nnotebook\n\ndata_07495 = {\"kargrs01fylker1.dat\" : df_07495_fylker,\n              \"kargrs01landet1.dat\" : df_07495_landet,}\nstatcli.transfer(data_07495, tableid=\"07495\")\n\nEtter innsending kommer det en link til Statistikkbankens GUI for √• f√∏lge med p√• om innsendingen gikk bra hos dem. Om det var det du √∏nsket, s√• er du n√• ferdig‚Ä¶ Men det finnes mer funksjonalitet her‚Ä¶\n\n\n\nFor √• hente filbeskrivelsen til en hovedtabell bruker du denne metoden under klienten:\n\n\nnotebook\n\nfilbeskrivelse = statcli.get_description(tableid=\"07495\")\nprint(filbeskrivelse)\n\nMed filbeskrivelsen kan du lett f√• en mal p√• dictionaryet du m√• plassere dataene i:\n\n\nnotebook\n\nfilbeskrivelse.transferdata_template()\n\nDu kan ogs√• validere dataene dine mot filbeskrivelsen:\n\n\nnotebook\n\nfilbeskrivelse.validate(data_07495, tableid=\"07495\")",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-statbank-client"
    ]
  },
  {
    "objectID": "statistikkere/statistikkbanken.html#problematikk-knyttet-til-avrunding",
    "href": "statistikkere/statistikkbanken.html#problematikk-knyttet-til-avrunding",
    "title": "dapla-statbank-client",
    "section": "",
    "text": "Det tas noe ekstra ansvar i pakken rundt avrunding av desimaltall, da filbeskrivelsen inneholder informasjon om hvor mange desimaler som blir lagret per kolonne, kan vi runde av til rett antall samtidig som vi tar ett bevisst valg rundt ‚Äúhvilken vei vi skal runde av‚Äù. P√• barneskolen l√¶rte vi at ved 2,5 avrundet til 0 desimaler, s√• runder vi opp til 3. Det samme skjer i utgangspunktet i SAS og Excel. Python og R runder derimot ‚Äúmot n√¶rmeste partall‚Äù, s√• fra 2,5 blir det rundet til 2, men fra 1,5 blir det ogs√• rundet til 2. Dette er for √• forhindre bias i en retning. Dvs. om alle tall rundes opp, vil en hel kolonne med tall ‚Äúdras oppover‚Äù, ved √• gj√∏re annenhver opp og ned, vil ikke helheten bli ‚Äúdratt en spesifikk vei‚Äù. Siden ‚Äúround to even‚Äù ikke er det folk er vandte til, gj√∏r vi derfor noe annet i denne pakken, enn det som er vanlig oppf√∏rsel i Python. Vi runder opp. Om du bruker f√∏lgende metoden under filbeskrivelsen p√• dataene, s√• vil denne runde oppover, samtidig som den konverterer til en streng for √• bevare formateringen.\n\n\nnotebook\n\ndata_07495 = filbeskrivelse.round_data(data_07495)  # For √• ta vare p√• endringene, s√• m√• du skrive tilbake over variabelen",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-statbank-client"
    ]
  },
  {
    "objectID": "statistikkere/statistikkbanken.html#annen-funksjonalitet-du-kan-lese-om-i-readme-en-er",
    "href": "statistikkere/statistikkbanken.html#annen-funksjonalitet-du-kan-lese-om-i-readme-en-er",
    "title": "dapla-statbank-client",
    "section": "",
    "text": "En date-widget for √• visuelt endre til en valid dato.\nLagring av overf√∏ring og filbeskrivelses-objekter til json-filer\nUthenting av logg fra klienten\nHenting av publisert data fra statbanken\nRotering av tabeller fra statbanken",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-statbank-client"
    ]
  },
  {
    "objectID": "statistikkere/data-collector.html",
    "href": "statistikkere/data-collector.html",
    "title": "Data-Collector",
    "section": "",
    "text": "Data Collector skal avvikles\n\n\n\nDet er bestemt at Data Collector skal avvikles og derfor er det ikke √∏nskelig √• tilby nye team √• bruke tjenesten. Ta kontakt med team Statistikktjenester dersom du har et behov for √• bruke Data Collector.\nData Collector (DC) er et rammeverk for bruk av REST APIer som samler inn data fra eksterne ressurser og skriver det til kildeb√∏tter. DC kj√∏rer en deklarativ spesifikasjon ved kj√∏retid som beskriver hvordan data skal samles inn. Spesifikasjonen er bygget med en veldefinert DSL.\nDC-jobb startes fra Jupyter ved √• bruke en funksjon fra Dapla Toolbelt. Innsamlingsjobber beskrives med en specification (json-fil).\nLes mer om arkitektur og funksjonalitet",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Data-Collector"
    ]
  },
  {
    "objectID": "statistikkere/data-collector.html#bruke-collector-fra-dapla-lab",
    "href": "statistikkere/data-collector.html#bruke-collector-fra-dapla-lab",
    "title": "Data-Collector",
    "section": "Bruke Collector fra Dapla-lab",
    "text": "Bruke Collector fra Dapla-lab\nF√∏r brukeren kan kj√∏re DC fra Dapla Lab, m√• en team Statistikktjenester ha satt opp en instans for teamet.\n\nSett opp collector\n\n\nnotebook\n\nimport json\n\nfrom dapla import CollectorClient\n\ncollector_url = \"https://data-collector-&lt;team_navn&gt;.intern.ssb.no/tasks\"\ncollector = CollectorClient(collector_url)\nspecification = None\n\n# Load specification from file\nwith open(\"&lt;specification_file&gt;.json\") as specification_file:\n    specification = json.load(specification_file)\n\ntopic = specification['configure'][0]['globalState']['global.topic']\nprint (topic)\n\n\n\nStart data-innsamlingsjobb\n\n\nnotebook\n\nresponse = collector.start(specification)\ntask_id = response.json()['workerId']\nprint(f\"Startet collector jobb, data skal bli skrevet til gs://&lt;kilde-b√∏tte&gt;/{topic}/\")\n\n\n\nListe kj√∏reneder tasks\n\n\nnotebook\n\nrunning_tasks = collector.running_tasks().json()\nprint(running_tasks)\n\n\n\nStoppe kj√∏reneder tasks\n\n\nnotebook\n\n\nstop_response = collector.stop(task_id)\nprint(stop_response)",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Data-Collector"
    ]
  },
  {
    "objectID": "statistikkere/data-collector.html#konfigurasjoner",
    "href": "statistikkere/data-collector.html#konfigurasjoner",
    "title": "Data-Collector",
    "section": "Konfigurasjoner",
    "text": "Konfigurasjoner\nDet er 3 Dapla-team med kj√∏rende DC-instanser i prod- og test-milj√∏et:\ncollector-url\n\nskatt-person\n\nTEST:\n\nskattemelding: https://data-collector-skatt-person-skattemelding.intern.test.ssb.no/tasks\nskatteoppgjor: https://data-collector-skatt-person-skatteoppgjor.intern.test.ssb.no/tasks\n\nPROD:\n\nskattemelding: https://data-collector-skatt-person-skattemelding.intern.ssb.no/tasks\nskatteoppgjor: https://data-collector-skatt-person-skatteoppgjor.intern.ssb.no/tasks\n\n\nskatt-naering\n\nTEST: https://data-collector-skatt-naering.intern.test.ssb.no/tasks\nPROD: https://data-collector-skatt-naering.intern.ssb.no/tasks\n\nstrukt-mva\n\nTEST: https://data-collector-strukt-mva.intern.test.ssb.no/tasks\nPROD: https://data-collector-strukt-mva.intern.ssb.no/tasks",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Data-Collector"
    ]
  },
  {
    "objectID": "statistikkere/data-collector.html#logger",
    "href": "statistikkere/data-collector.html#logger",
    "title": "Data-Collector",
    "section": "Logger",
    "text": "Logger\nDet er mulig √• sjekke logger fra google-console for test og prod.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Data-Collector"
    ]
  },
  {
    "objectID": "statistikkere/kartdata.html",
    "href": "statistikkere/kartdata.html",
    "title": "Kartdata",
    "section": "",
    "text": "Det er tilrettelagt kartdata i Dapla til analyse, statistikkproduksjon og visualisering. Det er de samme dataene som er i GEODB i vanlig produksjonssone. Kartdataene er lagret p√• ssb-kart-data-delt-geo-prod. De er lagret som parquetfiler i standardprojeksjonen vi bruker i SSB (UTM sone 33N) hvis ikke annet er angitt. Det er ogs√• SSBs standard-rutenett i ulike st√∏rrelser samt Eurostats rutenett over Norge.\nI tillegg ligger det noe testdata i fellesb√∏tta her: ssb-dapla-felles-data-produkt-prod/GIS/testdata\n\n\n\n\nGeopandas er et Python-bibliotek som integrerer Pandas med GIS-analysering og kartlegging. Geopandas er bygd opp rundt GeoDataFrames, som er en pandas DataFrame med geografisk informasjon. Disse kan lastes inn direkte hvis man har geografiske filer, eller man kan konvertere fra DataFrames med koordinat-kolonner (se eksempel under).\nGeopandas kan blant annet brukes til √• kartlegge dataene, beregne avstander og labe variabler for n√¶rmilj√∏ ved √• koble datasett sammen basert p√• geografisk overlapp.\nFor lesing og skriving av geodata i Dapla, kan man bruke SSB-pakken sgis. Med denne kan man blant annet ogs√• beregne reisetider langs veg og lage publiseringsverdige kart.\nPakkene kan installeres i Dapla s√•nn her:\n\n\nterminal\n\npoetry add geopandas\npoetry add ssb-sgis\n\nOg s√• importeres det i Python p√• vanlig vis.\n\n\nnotebook\n\nimport geopandas as gpd\nimport sgis as sg\n\nEksempel p√• lesing/skriving av kartdata\nMan kan lese/skrive geodata i Dapla med sgis-funksjonene read_geopandas og write_geopandas. St√∏ttede formater er blant annet (geo)parquet, geojson og gml. En rekke kartdata for hele landet til analyse- og visualiseringsform√•l ligger i b√∏tta ‚Äúkart‚Äù. For eksempel kommuneflater til analyse eller statistikk:\n\n\nnotebook\n\nfilsti = \"ssb-kart-data-delt-geo-prod/analyse_data/klargjorte-data/2024/ABAS_kommune_flate_p2024_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\n\nEksempel p√• lesing av kommuneflater for visualisering. Disse dataene er generalisert slik at de egner seg for √• lage kart, men blir un√∏yaktige til statistikk:\n\n\nnotebook\n\nfilsti = \"ssb-kart-data-delt-geo-prod/visualisering_data/klargjorte-data/2024/parquet/N5000_kommune_flate_p2024_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\n\nTilsvarende for skriving til parquet eller annet geodataformat:\n\n\nnotebook\n\nsg.write_geopandas(kommuner, filsti_ny) \n\nMan kan ogs√• gj√∏re vanlige tabell-filer geografiske hvis man har koordinat-kolonner. For eksempel situasjonsuttak fra Virksomhets- og foretaksregisteret (VoF):\n\n\nnotebook\n\nimport dapla as dp\n\nVOFSTI = \"ssb-vof-data-delt-stedfesting-prod/klargjorte-data/parquet\"\nvof_df = dp.read_pandas(\n    f\"{VOFSTI}/stedfesting-situasjonsuttak_p2023-01_v1.parquet\"\n)\nvof_gdf = gpd.GeoDataFrame(\n    vof_df, \n    geometry=gpd.points_from_xy(\n        vof_df[\"y_koordinat\"],\n        vof_df[\"x_koordinat\"],\n    ),\n    crs=25833,\n)\nvof_gdf\n\n\n\n\nI tillegg til kartdataene, ligger det testdata i fellesb√∏tta som man kan leke seg med. Testdataene finnes for Oslo og Kongsvinger, og kan leses s√•nn her:\n\n\nnotebook\n\ntestdatasti = \"ssb-dapla-felles-data-produkt-prod/GIS/testdata\"\nveger = sg.read_geopandas(f\"{testdatasti}/veger_oslo.parquet\")\n\nUnder f√∏lger noen eksempler p√• GIS-prosessering med testdataene.\nEksempel p√• avstandsberegning\nAvstand i luftlinje finner man enkelt med sjoin_nearest. S√•nn her finner man avstand fra boligbygg (1000 tilfeldige) i Oslo til n√¶rmeste butikkbygg.\n\n\nnotebook\n\nboligbygg = sg.read_geopandas(f\"{testdatasti}/noen_boligbygg_oslo.parquet\")\nbutikkbygg = sg.read_geopandas(f\"{testdatasti}/butikkbygg_oslo.parquet\")\n\nboligbygg_med_butikkavstand = boligbygg.sjoin_nearest(butikkbygg, distance_col=\"meter_til_butikk\")\n\nFor √• finne avstand eller reisetid langs veier, kan man gj√∏re nettverksanalyse med sgis. Man m√• f√∏rst klargj√∏re vegnettet og bestemme regler for beregningen(e):\n\n\nnotebook\n\nveger_klargjort = (\n    sg.get_connected_components(veger)\n    .loc[lambda x: x[\"connected\"] == 1]\n    .pipe(sg.make_directed_network_norway, dropnegative=True)\n)\n\nregler = sg.NetworkAnalysisRules(directed=True, weight=\"minutes\")\n\nnwa = sg.NetworkAnalysis(network=veger_klargjort, rules=regler)\n\nS√• kan man beregne reisetider fra boligbygg til butikkbygg:\n\n\nnotebook\n\navstander = nwa.od_cost_matrix(boligbygg, butikkbygg)\n\nKorteste reisetid per bolig kan kobles p√• som kolonne i boligdataene s√•nn her:\n\n\nnotebook\n\nboligbygg[\"minutter_til_butikk\"] = avstander.groupby(\"origin\")[\"minutes\"].min()\n\nUnders√∏k resultatene i interaktivt kart:\n\n\nnotebook\n\nsg.explore(boligbygg, butikkbygg, \"minutter_til_butikk\")\n\nSe mer informasjon og eksempler i dokumentasjonen.\nOg flere nettverksanalyse-eksempler her.\nEksempel p√• geografisk kobling\nDatasett kan kobles basert p√• geografisk overlapp med geopandas.sjoin. Denne fungerer akkurat som pandas.merge, bare at det kobles basert p√• geometrien.\nKodesnutten under returnerer √©n kommuneflate for hvert bygg som overlapper, med kolonner fra begge datasettene. Inner-join er default, men med left-join beholder man ogs√• geometriene som ikke overlapper.\n\n\nnotebook\n\nkommuner = sg.read_geopandas(f\"{testdatasti}/enkle_kommuner.parquet\")\nkommuner_med_boliginfo = kommuner.sjoin(boligbygg, how=\"left\")\n\nMed motsatt rekkef√∏lge, f√•r man ett boligbygg per kommune den overlapper med. Siden byggene er punkter, vil hvert bygg havne i maksimalt √©n kommune.\n\n\nnotebook\n\nboligbygg_med_kommuneinfo = boligbygg.sjoin(kommuner, how=\"left\")\n\nEksempel p√• √• lage kart\nMan kan lage tematiske kart med sgis-klassen ThematicMap. Her er et enkelt eksempel p√• et kart over arealet i kommuner.\n\n\nnotebook\n\nkommuner[\"Kvadratmeter\"] = kommuner.area\n\nm = sg.ThematicMap(kommuner, column=\"Kvadratmeter\", size=15)\nm.title = \"Areal i kommunene\"\nm.plot()\n\nSe flere kart-eksempler her.\nKonvertere DataFrame til GeoDataFrame\nEn pandas.DataFrame med koordinat-kolonner, kan konverteres til GeoDataFrame s√•nn her:\n\n\nnotebook\n\nimport pandas as pd\ndf = pd.DataFrame({\"x\": [10.8, 10.7, 10.9], \"y\": [59.9, 60, 59.85]})\n\ngdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.x, df.y), crs=4326)\n\nFor √• beregne avtand i meter og kunne koble med annen geodata i Dapla, m√• man ha UTM-koordinater (hvis man ikke hadde det fra f√∏r):\n\n\nnotebook\n\ngdf = gdf.to_crs(25833)\n\nSe ogs√• geopandas‚Äô dokumentasjon for mer utfyllende informasjon.\n\n\n\n\nDen viktigste GIS-pakken i R er sf. Pakken er bygd opp rundt sf-objekter, som er data.frames med geografisk informasjon. Man kan gj√∏re standard tidyverse-opersjoner p√• sf-objektene, pluss GIS-operasjoner.\nSindre Mikael Haugen har lagd funksjoner for √• lese og skrive blant annet geodata i Dapla. For √• f√• geodata, setter man parametret ‚Äòsf‚Äô til TRUE:\n\n\nnotebook\n\nlibrary(fellesr)\n\nveger = read_SSB(\"ssb-dapla-felles-data-produkt-prod/GIS/testdata/veger_oslo.parquet\", sf = TRUE)\n\nHaugen har ogs√• lagd en pakke for √• gj√∏re nettverksanalyse, som ogs√• lar deg geokode adresser, alts√• √• finne adressenes koordinater.\n\n\nnotebook\n\nlibrary(GISSB)\n\nLite eksempel p√• kobling basert geografisk sammenfall (enkle kommuner er egentlig til visualisering). Her f√•r man ett bygg per kommune som overlapper (som maksimalt er √©n kommune siden dette er bygningspunkter):\n\n\nnotebook\n\nlibrary(sf)\n\nboligbygg = read_SSB(\"ssb-dapla-felles-data-produkt-prod/GIS/testdata/noen_boligbygg_oslo.parquet\", sf = TRUE)\nenkle_kommuner = read_SSB(\"ssb-dapla-felles-data-produkt-prod/GIS/testdata/enkle_kommuner.parquet\", sf = TRUE)\n\nboligbygg_med_kommuneinfo = st_join(boligbygg, enkle_kommuner)\n\nMed motsatt rekkef√∏lge, f√•r man √©n kommuneflate per bolig som overlapper:\n\n\nnotebook\n\nkommuner_med_boliginfo = st_join(enkle_kommuner, boligbygg)",
    "crumbs": [
      "Manual",
      "Appendix",
      "Kartdata"
    ]
  },
  {
    "objectID": "statistikkere/kartdata.html#python",
    "href": "statistikkere/kartdata.html#python",
    "title": "Kartdata",
    "section": "",
    "text": "Geopandas er et Python-bibliotek som integrerer Pandas med GIS-analysering og kartlegging. Geopandas er bygd opp rundt GeoDataFrames, som er en pandas DataFrame med geografisk informasjon. Disse kan lastes inn direkte hvis man har geografiske filer, eller man kan konvertere fra DataFrames med koordinat-kolonner (se eksempel under).\nGeopandas kan blant annet brukes til √• kartlegge dataene, beregne avstander og labe variabler for n√¶rmilj√∏ ved √• koble datasett sammen basert p√• geografisk overlapp.\nFor lesing og skriving av geodata i Dapla, kan man bruke SSB-pakken sgis. Med denne kan man blant annet ogs√• beregne reisetider langs veg og lage publiseringsverdige kart.\nPakkene kan installeres i Dapla s√•nn her:\n\n\nterminal\n\npoetry add geopandas\npoetry add ssb-sgis\n\nOg s√• importeres det i Python p√• vanlig vis.\n\n\nnotebook\n\nimport geopandas as gpd\nimport sgis as sg\n\nEksempel p√• lesing/skriving av kartdata\nMan kan lese/skrive geodata i Dapla med sgis-funksjonene read_geopandas og write_geopandas. St√∏ttede formater er blant annet (geo)parquet, geojson og gml. En rekke kartdata for hele landet til analyse- og visualiseringsform√•l ligger i b√∏tta ‚Äúkart‚Äù. For eksempel kommuneflater til analyse eller statistikk:\n\n\nnotebook\n\nfilsti = \"ssb-kart-data-delt-geo-prod/analyse_data/klargjorte-data/2024/ABAS_kommune_flate_p2024_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\n\nEksempel p√• lesing av kommuneflater for visualisering. Disse dataene er generalisert slik at de egner seg for √• lage kart, men blir un√∏yaktige til statistikk:\n\n\nnotebook\n\nfilsti = \"ssb-kart-data-delt-geo-prod/visualisering_data/klargjorte-data/2024/parquet/N5000_kommune_flate_p2024_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\n\nTilsvarende for skriving til parquet eller annet geodataformat:\n\n\nnotebook\n\nsg.write_geopandas(kommuner, filsti_ny) \n\nMan kan ogs√• gj√∏re vanlige tabell-filer geografiske hvis man har koordinat-kolonner. For eksempel situasjonsuttak fra Virksomhets- og foretaksregisteret (VoF):\n\n\nnotebook\n\nimport dapla as dp\n\nVOFSTI = \"ssb-vof-data-delt-stedfesting-prod/klargjorte-data/parquet\"\nvof_df = dp.read_pandas(\n    f\"{VOFSTI}/stedfesting-situasjonsuttak_p2023-01_v1.parquet\"\n)\nvof_gdf = gpd.GeoDataFrame(\n    vof_df, \n    geometry=gpd.points_from_xy(\n        vof_df[\"y_koordinat\"],\n        vof_df[\"x_koordinat\"],\n    ),\n    crs=25833,\n)\nvof_gdf\n\n\n\n\nI tillegg til kartdataene, ligger det testdata i fellesb√∏tta som man kan leke seg med. Testdataene finnes for Oslo og Kongsvinger, og kan leses s√•nn her:\n\n\nnotebook\n\ntestdatasti = \"ssb-dapla-felles-data-produkt-prod/GIS/testdata\"\nveger = sg.read_geopandas(f\"{testdatasti}/veger_oslo.parquet\")\n\nUnder f√∏lger noen eksempler p√• GIS-prosessering med testdataene.\nEksempel p√• avstandsberegning\nAvstand i luftlinje finner man enkelt med sjoin_nearest. S√•nn her finner man avstand fra boligbygg (1000 tilfeldige) i Oslo til n√¶rmeste butikkbygg.\n\n\nnotebook\n\nboligbygg = sg.read_geopandas(f\"{testdatasti}/noen_boligbygg_oslo.parquet\")\nbutikkbygg = sg.read_geopandas(f\"{testdatasti}/butikkbygg_oslo.parquet\")\n\nboligbygg_med_butikkavstand = boligbygg.sjoin_nearest(butikkbygg, distance_col=\"meter_til_butikk\")\n\nFor √• finne avstand eller reisetid langs veier, kan man gj√∏re nettverksanalyse med sgis. Man m√• f√∏rst klargj√∏re vegnettet og bestemme regler for beregningen(e):\n\n\nnotebook\n\nveger_klargjort = (\n    sg.get_connected_components(veger)\n    .loc[lambda x: x[\"connected\"] == 1]\n    .pipe(sg.make_directed_network_norway, dropnegative=True)\n)\n\nregler = sg.NetworkAnalysisRules(directed=True, weight=\"minutes\")\n\nnwa = sg.NetworkAnalysis(network=veger_klargjort, rules=regler)\n\nS√• kan man beregne reisetider fra boligbygg til butikkbygg:\n\n\nnotebook\n\navstander = nwa.od_cost_matrix(boligbygg, butikkbygg)\n\nKorteste reisetid per bolig kan kobles p√• som kolonne i boligdataene s√•nn her:\n\n\nnotebook\n\nboligbygg[\"minutter_til_butikk\"] = avstander.groupby(\"origin\")[\"minutes\"].min()\n\nUnders√∏k resultatene i interaktivt kart:\n\n\nnotebook\n\nsg.explore(boligbygg, butikkbygg, \"minutter_til_butikk\")\n\nSe mer informasjon og eksempler i dokumentasjonen.\nOg flere nettverksanalyse-eksempler her.\nEksempel p√• geografisk kobling\nDatasett kan kobles basert p√• geografisk overlapp med geopandas.sjoin. Denne fungerer akkurat som pandas.merge, bare at det kobles basert p√• geometrien.\nKodesnutten under returnerer √©n kommuneflate for hvert bygg som overlapper, med kolonner fra begge datasettene. Inner-join er default, men med left-join beholder man ogs√• geometriene som ikke overlapper.\n\n\nnotebook\n\nkommuner = sg.read_geopandas(f\"{testdatasti}/enkle_kommuner.parquet\")\nkommuner_med_boliginfo = kommuner.sjoin(boligbygg, how=\"left\")\n\nMed motsatt rekkef√∏lge, f√•r man ett boligbygg per kommune den overlapper med. Siden byggene er punkter, vil hvert bygg havne i maksimalt √©n kommune.\n\n\nnotebook\n\nboligbygg_med_kommuneinfo = boligbygg.sjoin(kommuner, how=\"left\")\n\nEksempel p√• √• lage kart\nMan kan lage tematiske kart med sgis-klassen ThematicMap. Her er et enkelt eksempel p√• et kart over arealet i kommuner.\n\n\nnotebook\n\nkommuner[\"Kvadratmeter\"] = kommuner.area\n\nm = sg.ThematicMap(kommuner, column=\"Kvadratmeter\", size=15)\nm.title = \"Areal i kommunene\"\nm.plot()\n\nSe flere kart-eksempler her.\nKonvertere DataFrame til GeoDataFrame\nEn pandas.DataFrame med koordinat-kolonner, kan konverteres til GeoDataFrame s√•nn her:\n\n\nnotebook\n\nimport pandas as pd\ndf = pd.DataFrame({\"x\": [10.8, 10.7, 10.9], \"y\": [59.9, 60, 59.85]})\n\ngdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.x, df.y), crs=4326)\n\nFor √• beregne avtand i meter og kunne koble med annen geodata i Dapla, m√• man ha UTM-koordinater (hvis man ikke hadde det fra f√∏r):\n\n\nnotebook\n\ngdf = gdf.to_crs(25833)\n\nSe ogs√• geopandas‚Äô dokumentasjon for mer utfyllende informasjon.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Kartdata"
    ]
  },
  {
    "objectID": "statistikkere/kartdata.html#r",
    "href": "statistikkere/kartdata.html#r",
    "title": "Kartdata",
    "section": "",
    "text": "Den viktigste GIS-pakken i R er sf. Pakken er bygd opp rundt sf-objekter, som er data.frames med geografisk informasjon. Man kan gj√∏re standard tidyverse-opersjoner p√• sf-objektene, pluss GIS-operasjoner.\nSindre Mikael Haugen har lagd funksjoner for √• lese og skrive blant annet geodata i Dapla. For √• f√• geodata, setter man parametret ‚Äòsf‚Äô til TRUE:\n\n\nnotebook\n\nlibrary(fellesr)\n\nveger = read_SSB(\"ssb-dapla-felles-data-produkt-prod/GIS/testdata/veger_oslo.parquet\", sf = TRUE)\n\nHaugen har ogs√• lagd en pakke for √• gj√∏re nettverksanalyse, som ogs√• lar deg geokode adresser, alts√• √• finne adressenes koordinater.\n\n\nnotebook\n\nlibrary(GISSB)\n\nLite eksempel p√• kobling basert geografisk sammenfall (enkle kommuner er egentlig til visualisering). Her f√•r man ett bygg per kommune som overlapper (som maksimalt er √©n kommune siden dette er bygningspunkter):\n\n\nnotebook\n\nlibrary(sf)\n\nboligbygg = read_SSB(\"ssb-dapla-felles-data-produkt-prod/GIS/testdata/noen_boligbygg_oslo.parquet\", sf = TRUE)\nenkle_kommuner = read_SSB(\"ssb-dapla-felles-data-produkt-prod/GIS/testdata/enkle_kommuner.parquet\", sf = TRUE)\n\nboligbygg_med_kommuneinfo = st_join(boligbygg, enkle_kommuner)\n\nMed motsatt rekkef√∏lge, f√•r man √©n kommuneflate per bolig som overlapper:\n\n\nnotebook\n\nkommuner_med_boliginfo = st_join(enkle_kommuner, boligbygg)",
    "crumbs": [
      "Manual",
      "Appendix",
      "Kartdata"
    ]
  },
  {
    "objectID": "statistikkere/kvakk.html",
    "href": "statistikkere/kvakk.html",
    "title": "Kvalitet i kode og koding (KVAKK)",
    "section": "",
    "text": "KVAKK er en tverrfaglig gruppe som utarbeider regler, anbefalinger og veiledninger for god prakis for jobbing med kode og koding. Gruppa har mandat fra DM og best√•r av representanter fra alle statistikkavdelinger, forskning, metode og IT.\nGruppa jobber temabasert og publiserer regler, anbefalinger og veiledninger for hvert tema under beste praksis omr√•det p√• Confluence. Regler skal f√∏lges, med mindre man har en dokumentert begrunnelse p√• hvorfor regelen avvikes. F√∏lgende regler er fastsatt i SSB:\n\nAll produksjonskode skal v√¶re under versjonskontroll i GitHub\nKildekode i GitHub skal ikke inneholde ukrypterte passord eller hemmeligheter\nGit-klienter skal konfigureres slik at resultat fra kj√∏ringer i Jupyter Notebooks ikke lagres p√• GitHub\nAlle biblioteker skal ha en eier\nBruk SSB-mal for PyPI-biblioteker n√•r du skal lage et python-bibliotek\nGrensesnittet til biblioteket skal v√¶re dokumentert\nAlle biblioteker skal ha tilh√∏rende tester\nAll produksjonskode skal lagres i et format som st√∏tter kodeanalyse\n\nAnbefalinger er god praksis som de fleste b√∏r f√∏lge. Disse finner du p√• KVAKKs sider om regler, anbefalinger og veiledninger.\nDu finner mer informasjon om KVAKK, hvem som er med og arbeidet deres p√• KVAKK-siden.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Kvalitet i kode og koding (KVAKK)"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html",
    "href": "statistikkere/rstudio.html",
    "title": "Rstudio",
    "section": "",
    "text": "Rstudio er en tjeneste p√• Dapla Lab for utvikling av kode i R1. M√•lgruppen for tjenesten er brukere som skal skrive produksjonskode i R2.\nSiden tjenesten er ment for produksjonskode s√• er det veldig f√• forh√•ndsinstallerte R-pakker. Antagelsen er at brukerene/teamet heller b√∏r installere de pakkene de trenger, framfor at alle pakker som alle brukere/team er forh√•ndsinstallert og skal dekke behovet til alle. Det reduserer kompleksitet i tjenesten og dermed sannsynligheten for feilsituasjoner.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html#forberedelser",
    "href": "statistikkere/rstudio.html#forberedelser",
    "title": "Rstudio",
    "section": "Forberedelser",
    "text": "Forberedelser\nF√∏r man starter Rstudio b√∏r man ha lest kapitlet om Dapla Lab og satt opp Git- og GitHub-konfigurasjonen under Min konto. Deretter gj√∏r du f√∏lgende:\n\nLogg deg inn p√• Dapla Lab\nUnder Tjenestekatalog trykker du p√• Start-knappen for Rstudio\nGi tjenesten et navn\n√Öpne Rstudio konfigurasjoner",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html#konfigurasjon",
    "href": "statistikkere/rstudio.html#konfigurasjon",
    "title": "Rstudio",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nKonfigurasjonen av RStudio er n√¶r identisk Jupyter-tjenesten sin konfigurasjon. Se dokumentasjonen for konfigurasjon av Jupyter.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html#datatilgang",
    "href": "statistikkere/rstudio.html#datatilgang",
    "title": "Rstudio",
    "section": "Datatilgang",
    "text": "Datatilgang\nSlik kan man inspisere dataene fra en terminal inne i tjenesten:\n\n√Öpne en instans av Rstudio med data fra b√∏tter\n√Öpne en terminal inne i Rstudio\nG√• til mappen med b√∏ttene ved √• kj√∏re dette fra terminalen cd /buckets\nKj√∏r ls -ahl i teminalen for √• se p√• hvilke b√∏tter som er montert.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html#installere-pakker",
    "href": "statistikkere/rstudio.html#installere-pakker",
    "title": "Rstudio",
    "section": "Installere pakker",
    "text": "Installere pakker\nSiden det nesten ikke er installert noen pakker i tjenesten s√• kan brukeren opprette et renv og installere pakker som √∏nsker.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html#slette-tjenesten",
    "href": "statistikkere/rstudio.html#slette-tjenesten",
    "title": "Rstudio",
    "section": "Slette tjenesten",
    "text": "Slette tjenesten\nFor √• slette tjenesten kan man trykke p√• Slette-knappen i Dapla Lab under Mine tjenester. N√•r man sletter en tjeneste s√• sletter man hele disken inne i tjenesten og frigj√∏r alle ressurser som er reservert. Siden pakkene som er installert ogs√• ligger lagret p√• disken, betyr dette at pakkene m√• installeres p√• nytt etter at en tjeneste er slettet. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html#pause-tjenesten",
    "href": "statistikkere/rstudio.html#pause-tjenesten",
    "title": "Rstudio",
    "section": "Pause tjenesten",
    "text": "Pause tjenesten\nMan kan pause tjenesten ved √• trykke p√• Pause-knappen i Dapla Lab under Mine tjenester. N√•r man pauser s√• slettes alt p√•den lokale disken som ikke er lagret under $HOME/work. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html#monitorering",
    "href": "statistikkere/rstudio.html#monitorering",
    "title": "Rstudio",
    "section": "Monitorering",
    "text": "Monitorering\n\n\nMan kan moniterere en instans av Rstudio ved √• trykke p√• Rstudio-teksten under Mine tjenester i Dapla Lab, slik som vist i Figur¬†1.\nDenne funksjonaliteten er under arbeid og mer informasjon kommer snart.\n\n\n\n\n\n\n\n\n\nFigur¬†1: Monitorering av Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html#footnotes",
    "href": "statistikkere/rstudio.html#footnotes",
    "title": "Rstudio",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nRstudio er web-versjonen av Rstudio og er ikke helt identisk med desktop-versjonen som mange er kjent med.‚Ü©Ô∏é\nPython er ikke installert i Rstudio-tjenesten.‚Ü©Ô∏é",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/standarder.html",
    "href": "statistikkere/standarder.html",
    "title": "Standarder",
    "section": "",
    "text": "N√•r man driver statistikkproduksjon er det flere regler man m√• forholde seg til.\nStandardutvalget st√•r for utarbeidingen av flere av disse reglene og standardene p√• Dapla, for eksempel navnestandarden som er beskrevet i artikkelen navnestandard.\n\n\n\n\n\n\nKort om standardutvalget!\n\n\n\n\n\n\nAnsvarlig for det sentrale standardiseringsarbeidet knyttet til bruk og gjenbruk av data i SSB.\nBeslutningsmydnighet fra DM, men prinsipielle saker, og saker med store konsekvenser, sendes til DM for vedtak.\nUtvikler hovedsaklig obligatoriske standarder, men statistikkprodusenter oppfordres ogs√• til √• gj√∏re seg kjent med anbefalingene.\nLenke: Standardutvalgets side p√• byr√•nettet\nLenke: Vedtak fra Standardutvalget\n\n\n\n\nDenne artikkelen er i stor grad basert p√• standardutvalgets mandat og standardutvalgets side p√• byr√•nettet.\n\nStandardutvalget\nStandardutvalget har ansvar for det sentrale standardiseringsarbeidet knyttet til bruk og gjenbruk av data i SSB. Alle avdelinger er representert med unntak av avdeling 100. Standardutvalgets side p√• byr√•nettet inneholder en oversikt over utvalgsmedlemmene.\nForm√•let med utvalget, slik det er beskrevet i mandatet fra 2023, er √• legge grunnlag for effektiv bruk og gjenbruk av data som SSB samler inn, bearbeider og forvalter.\nStandardutvalget har beslutningsmyndighet delegert fra DM og kan dermed vedta krav, regler, anbefalinger eller obligatoriske standarder.\n\nHvilke omr√•der jobber standardutvalget med?\nStandardutvalget utvikler standarder innenfor f√∏lgende omr√•der:\n\nKodeverk (klassifikasjoner og kodelister)\nVariabler og variabelnavn\nEnhetstyper\nNavngivning, versjonering, dokumentasjon og lagring av datasett og populasjoner i alle ledd i produksjonsprosessen\nKvalitetsindikatorer\nProsessdata\nDatatilstander\nInformasjonsmodeller\n\nStandardutvalget har ikke ansvar for standarder innenfor koding og statistiske metoder. Det h√•ndteres av KVAKK (Kvalitet i kode og koding) og s811 - Seksjon for metoder.\n\n\n\nHvilke standarder finnes?\n\n1. DataDoc - dokumentasjon av datasett\n\nDataDoc - Krav til dokumentasjon av datasett p√• Dapla (Confluence)\nDatadoc-editor - Artikkel i Dapla-manualen\n\n\n\n2. Datatilstander i SSB\n\nDatatilstander - Artikkel i Dapla-manualen\nInterne dokumenter - Datatilstander, skrevet av Standardutvalget\n\n\n\n\n\n\n\nFigur¬†1: En grafisk fremstilling av forskjellene mellom datatilstandene i SSB (Standardutvalget 2023).\n\n\n\n\n\n3. VarDef - dokumentasjon av variabler\n\nVarDef - Confluence-side\n\n\n\n4. Standardformater for lagring av data\n\nStandardformater - Confluence-side\n\n\n\n5. Navnestandard for henholdsvis:\n\nEnhetstypeidentifikatorer - DM-vedtak (internet dokument)\nGitHub repoer - Internt dokument\nN√∏kkelvariabler (anbefaling) - Byr√•nettside\nDatasett - Dapla-manualen: Navnestandard (og versjonering)\n\n\n\n6. Kvalitetsindikatorer\n\nStandardutvalget har definert et sett med anbefalte kvalitetsindikatorer for statistikkproduksjon - s√¶rlig kvantitative indikatorer\nInternt dokument - Anbefalte kvalitetsindikatorer i statistikkproduksjonen\nInternt dokument - Mal for dokumentasjon av kvalitetsindikatorer\n\n\n\n7. Editering - prinsipper og retningslinjer\n\nOffentlig dokument - Prinsipper og retningslinjer for dataeditering\nDokumentet lenket i punktet over inneholder ni prinsipper for editering i SSB, blant annet:\n\nForm√•let med dataediteringen skal v√¶re klart formulert\nKontrollene, kontrollustlagene og endringene skal v√¶re veldokumenterte\nAutomatiser editeringsprosessen s√• mye som mulig\n\n\n\n\n8. Kodelister (anbefaling)\n\nLes om de anbefalte standardiserinenge i byr√•nettsiden for standardutvalgets vedtak\n\n\n\n\n\n\n\nReferanser\n\nStandardutvalget. 2023. ‚ÄúDatatilstander i SSB.‚Äù Statistisk sentralbyr√•. https://ssbno.sharepoint.com/sites/Internedokumenter/Delte%20dokumenter/Forms/AllItems.aspx?id=%2Fsites%2FInternedokumenter%2FDelte%20dokumenter%2FInterne%20dokumenter%202023%2F2023%2D14%20Datatilstander%20i%20SSB%2Epdf&parent=%2Fsites%2FInternedokumenter%2FDelte%20dokumenter%2FInterne%20dokumenter%202023.",
    "crumbs": [
      "Manual",
      "Standarder"
    ]
  },
  {
    "objectID": "statistikkere/metodebibliotek.html",
    "href": "statistikkere/metodebibliotek.html",
    "title": "Metodebiblioteket",
    "section": "",
    "text": "Metodebiblioteket er SSBs bibliotek for statistiske metode funksjoner. Biblioteket finner du her : https://statisticsnorway.github.io/ssb-metodebiblioteket/.\nMetodene er organisert som en liste av funksjoner skrevet i R eller Python. Funksjonene kan kj√∏res p√• b√•de p√• Dapla og i bakke-milj√∏ene. Alle funksjoner er testet av Seksjon for Metode for bruk i produksjon av offisiell statistikk, og alle er brukt i minst ett produksjonsl√∏p eller i SSBs interne metodekurs. Funksjonene er b√•de utviklet internt og hentet fra godkjente eksterne pakker.\nMetodebiblioteket kan s√∏kes i enten via en generell liste eller gjennom steg i prosessmodellen.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Metodebiblioteket"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html",
    "href": "statistikkere/gcc.html",
    "title": "Google Cloud Console (GCC)",
    "section": "",
    "text": "Google Cloud Console (GCC) er et web-basert grensesnitt for √• administrere ressurser og tjenester p√• Google Cloud Platform (GCP). Alle i SSB kan logge seg inn i GCC med sin SSB-bruker. Dapla-team har sjelden mulighet til √• opprette nye ressurser fra dette grensesnittet, siden vi √∏nsker at det skal gj√∏res med kode. Men det er likevel et nyttig verkt√∏y for √• se p√• ressurser og gj√∏re endringer p√• eksisterende ressurser. I SSB bruker bruker vi GCC hovedsakelig til f√∏lgende:",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Google Cloud Console (GCC)"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html#innlogging",
    "href": "statistikkere/gcc.html#innlogging",
    "title": "Google Cloud Console (GCC)",
    "section": "Innlogging",
    "text": "Innlogging\nFor √• logge inn i GCC s√• gj√∏r du f√∏lgende:\n\n√Öpne Google Cloud Console i en nettleser.\nLogg in med din SSB-bruker.\n\nHvis du ogs√• har en privat Google-konto som benyttes i samme nettleser, m√• du noen ganger passe p√• at du er logget inn med riktig konto. Dette kan du sjekke ved √• trykke p√• profilbildet ditt √∏verst til h√∏yre i GCC. Hvis du ikke er logget inn med riktig konto, s√• trykker du p√• Logg ut og logger inn p√• nytt med riktig konto.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Google Cloud Console (GCC)"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html#prosjektvelger",
    "href": "statistikkere/gcc.html#prosjektvelger",
    "title": "Google Cloud Console (GCC)",
    "section": "Prosjektvelger",
    "text": "Prosjektvelger\nEtter at du har logget deg p√• med din SSB-bruker, s√• m√• du velge hvilket av ditt teams prosjekter du √∏nsker √• jobbe med. Dette gj√∏r du ved √• trykke p√• prosjektvelgeren √∏verst til venstre p√• siden. Vidoen under viser hvordan du velger et prosjekt og lister ut hvilke b√∏tter som finnes i prosjektet.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Google Cloud Console (GCC)"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html#filutforsker",
    "href": "statistikkere/gcc.html#filutforsker",
    "title": "Google Cloud Console (GCC)",
    "section": "Filutforsker",
    "text": "Filutforsker\nFor √• utforske b√∏tter og filer i et Dapla-team sitt Google-prosjekt s√• kan man bruke Cloud Storage-grensesnittet i GCC. For √• bruke denne funksjonaliteten gj√∏r du f√∏lgende:\n\nBruk prosjektvelgeren til √• velge √∏nsket prosjekt.\nDeretter s√∏ker du opp Google Storage i s√∏kefeltet √∏verst p√• siden.\n\nDa f√•r du en oversikt over alle b√∏ttene i prosjektet. Velg √∏nsker b√∏tte for √• utforske innholdet i b√∏tta.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Google Cloud Console (GCC)"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html#logger",
    "href": "statistikkere/gcc.html#logger",
    "title": "Google Cloud Console (GCC)",
    "section": "Logger",
    "text": "Logger\nKommer snart.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Google Cloud Console (GCC)"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html#transfer-service",
    "href": "statistikkere/gcc.html#transfer-service",
    "title": "Google Cloud Console (GCC)",
    "section": "Transfer Service",
    "text": "Transfer Service\nLes mer om hvordan man bruker Transfer Service her.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Google Cloud Console (GCC)"
    ]
  },
  {
    "objectID": "statistikkere/datadoc.html",
    "href": "statistikkere/datadoc.html",
    "title": "Datadoc",
    "section": "",
    "text": "Datadoc er SSBs system for dokumentasjon av datasett p√• Dapla. Hensikten er at alle datasett i de obligatoriske datatilstandene (klargjorte data, statistikk og utdata) skal dokumenteres ett sted, og informasjonen skal knyttes tett til tilh√∏rende datasett. Den obligatoriske datatilstanden kildedata skal ogs√• dokumenters i Datadoc, men her vil det kreves mindre detaljert informasjon enn for de tre andre tilstandene. Kravene til dokumentasjon er st√∏rre for klargjorte data, statistikk og utdata siden dette er datatilstander som skal kunne deles med andre1.\nI Datadoc skal b√•de informasjon om alle kolonner, og informasjon om datasettet som helhet, dokumenteres. Informasjon om datasettet kan f.eks. v√¶re en beskrivelse av hva datasettet inneholder, hvilket Dapla-team som eier det, om det inneholder personopplysninger, og om det er bruksrestriksjoner knyttet til dataene. I tillegg vil en del felter bli maskingenerert, f.eks. identifikator, filsti og hvilke datoer datasettet inneholder data fra og til. Hele modellen for datasett finnes her: DataDoc - Krav til dokumentasjon av datasett p√• Dapla - Metadata p√• DAPLA - Confluence.\nInformasjon om enkeltkolonner (ofte omtalt som variabelforekomster) i datasettet skal ogs√• dokumenteres. Denne informasjonen skal bl.a. inneholde en beskrivelse av variabelen. Dette skal prim√¶rt gj√∏res ved at en lenker til tilh√∏rende variabeldefinisjon i Vardef. I tillegg til beskrivelsen, skal bl.a. m√•leenhet (hvis det er en kvantitativ variabel), datatype og (dersom variabelen er en personopplysning) om verdiene er pseudonymisert, dokumenteres. Hele modellen for variabelforekomster finne her: Variabelforekomst - Metadata p√• DAPLA - Confluence.\nFigur¬†1 viser et datasett der en av variabelforekomstene er sivst. Den er lenket opp til definisjonen av sivilstand i Vardef, som igjen er lenket opp til kodeverket som beskriver kategoriene i ¬´Standard for sivilstand¬ª i Klass.",
    "crumbs": [
      "Manual",
      "Metadata",
      "Datadoc"
    ]
  },
  {
    "objectID": "statistikkere/datadoc.html#footnotes",
    "href": "statistikkere/datadoc.html#footnotes",
    "title": "Datadoc",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nUtdata er √•pne data, mens klargjorte data og statistikkdata kan deles med brukere som har rett p√• tilgang.‚Ü©Ô∏é",
    "crumbs": [
      "Manual",
      "Metadata",
      "Datadoc"
    ]
  },
  {
    "objectID": "statistikkere/jdemetra.html",
    "href": "statistikkere/jdemetra.html",
    "title": "Jdemetra",
    "section": "",
    "text": "Jdemetra er en tjeneste som tilbyr et grafisk grensesnitt (GUI) for sesongjustering og tidsserie-analyse. Form√•let med tjenesten er √• tilby statistikere i SSB et velkjent verkt√∏y for opprette nye Jdemetra-workspaces, visuelt inspisere mange tidsserier samtidig, og benytte funksjonalitet som finnes for √•rlige evalueringer av modellene som benyttes.\nJdemetra+ er navnet p√• en samling programvare for tidsserie-analyse og sesongjustering som er utviklet av Belgias nasjonalbank i samarbeid med Eurostat, Insee og Deutche Bundesbank. I tillegg til GUI-et som tilbys p√• Dapla Lab, finnes det ogs√• et CLI for batch-prosessering som heter jwsacruncher, og en R-pakke ved navn RJDemetra. Alle bygger p√• de samme grunnleggende komponentene. Jwsacruncher er installert i Jupyter og Rstudio p√• Dapla Lab, mens RJdemetra kan installeres av brukeren selv i de samme tjenestene.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jdemetra"
    ]
  },
  {
    "objectID": "statistikkere/jdemetra.html#forberedelser",
    "href": "statistikkere/jdemetra.html#forberedelser",
    "title": "Jdemetra",
    "section": "Forberedelser",
    "text": "Forberedelser\nF√∏r man starter Jdemetra-tjenesten b√∏r man ha lest kapitlet om Dapla Lab. Deretter gj√∏r du f√∏lgende:\n\nLogg deg inn p√• Dapla Lab\nUnder Tjenestekatalog trykker du p√• Start-knappen for Jdemetra\nGi tjenesten et navn\n√Öpne Jdemetra konfigurasjoner",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jdemetra"
    ]
  },
  {
    "objectID": "statistikkere/jdemetra.html#konfigurasjon",
    "href": "statistikkere/jdemetra.html#konfigurasjon",
    "title": "Jdemetra",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nF√∏r man √•pner en tjeneste kan man konfigurere hvilket team og tilgangsgruppe man skal representere og dermed hvilke b√∏tter man f√•r tilgang til i Jdemetra. Man kan ogs√• velge hvilken versjon av Jdemetra man √∏nsker √• kj√∏re, der default er siste versjon.\n\n\n\n\n\n\nFigur¬†1: Detaljert tjenestekonfigurasjon i JDemetra-tjenesten: data\n\n\n\nFigur¬†1 viser hvilke valg man gj√∏re under menyen Data. I tillegg viser bildet neddtrekksmenyen for hvilken versjon av Jdemetra man vil bruke. F√∏rst kan man velge hvilket team og tilgangsgruppe man √∏nsker √• representere. I tillegg kan man aktivere kildedatatilgang. Alle i SSB er medlem av developers-gruppa i teamet Dapla Felles, derfor kan man velge dette teamet hvis man √∏nsker teste ut tjenesten.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jdemetra"
    ]
  },
  {
    "objectID": "statistikkere/jdemetra.html#datatilgang-og-lagring",
    "href": "statistikkere/jdemetra.html#datatilgang-og-lagring",
    "title": "Jdemetra",
    "section": "Datatilgang og lagring",
    "text": "Datatilgang og lagring\nN√•r man √•pner Jdemetra, og velger √• representere team og tilgangsgruppe, s√• blir b√∏ttene som den tilgangsgruppa har tilgang til, tilgjengeliggjort som et filsystem under /buckets/. Som bruker kan du da lese og skrive til b√∏ttene ved benytte denne filstien. F.eks. vil et statistikkteam som √•pner Jdemetra som developers-gruppa ha et filsystem som typisk ser slik ut:\n\n\n/buckets/\n\n/buckets/  \n‚îî‚îÄ produkt/  \n   ‚îú‚îÄ‚îÄ inndata/\n   ‚îú‚îÄ‚îÄ klargjorte-data/\n   ‚îú‚îÄ‚îÄ statistikk/\n   ‚îî‚îÄ‚îÄ utdata/\n‚îî‚îÄ frasky/  \n‚îî‚îÄ tilsky/                     \n\nI eksempelet over ser vi at b√∏ttene produkt, frasky og tilsky ligger under /buckets/.\n\n√Öpne eksisterende workspace\nHvis jeg velger √• representere gruppen dapla-felles-developers, s√• kan jeg √•pne et Jdemetra-workspace som ligger i produktb√∏tta til team Dapla Felles ved √• gj√∏re f√∏lgende:\n\nVelg File/Open workspace i menyen.\nFinn roten av filsystemet og √•pne mappen /buckets/\nVelg xml-filen som definerer workspacet trykk Open\n\n\n\nOpprette nytt workspace\nFor √• opprette et nytt workspace s√• importerer du input-filene p√• vanlig m√•te under Providers, legger de til i en workspace, og velger /File/Save Workspace As i menyen. Velg en filsti under /buckets/produkt/ for √• lagre workspacet permanent i en b√∏tte.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jdemetra"
    ]
  },
  {
    "objectID": "statistikkere/jdemetra.html#slette-tjenesten",
    "href": "statistikkere/jdemetra.html#slette-tjenesten",
    "title": "Jdemetra",
    "section": "Slette tjenesten",
    "text": "Slette tjenesten\nFor √• slette tjenesten kan man trykke p√• Slette-knappen i Dapla Lab under Mine tjenester. N√•r man sletter en tjeneste s√• sletter man hele disken inne i tjenesten og frigj√∏r alle ressurser som er reservert. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jdemetra"
    ]
  },
  {
    "objectID": "statistikkere/jdemetra.html#pause-tjenesten",
    "href": "statistikkere/jdemetra.html#pause-tjenesten",
    "title": "Jdemetra",
    "section": "Pause tjenesten",
    "text": "Pause tjenesten\nMan kan pause tjenesten ved √• trykke p√• Pause-knappen i Dapla Lab under Mine tjenester. N√•r man pauser s√• slettes alt p√•den lokale disken som ikke er lagret under $HOME/work. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jdemetra"
    ]
  },
  {
    "objectID": "statistikkere/jdemetra.html#monitorering",
    "href": "statistikkere/jdemetra.html#monitorering",
    "title": "Jdemetra",
    "section": "Monitorering",
    "text": "Monitorering\n\n\nMan kan moniterere en instans ved √• trykke p√• navnet p√• tjenesten under Mine tjenester i Dapla Lab, slik som vist i Figur¬†2 med en jupyter-instans.\nDenne funksjonaliteten er under arbeid og mer informasjon kommer snart.\n\n\n\n\n\n\n\n\n\nFigur¬†2: Monitorering av Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jdemetra"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html",
    "href": "statistikkere/jobbe-med-kode.html",
    "title": "Jobbe med kode",
    "section": "",
    "text": "P√• Dapla jobber vi med utvikling av Python- og R-kode i et Jupyter-milj√∏. For de som √∏nsker det, er det mulig √• enkelt √•pne en notebook med en av v√•re forh√•ndskonfigurerte kernels1. Man kan umiddelbart begynne √• skrive kode og deretter lagre den i det lokale filsystemet. Dette er ideelt for enkel datautforskning eller for pedagogiske form√•l.\nN√•r koden skal settes i produksjon, er det essensielt √• ta hensyn til f√∏lgende:\n\nResultater b√∏r v√¶re reproduserbare.\nKoden m√• kunne deles med andre.\nKoden b√∏r v√¶re organisert slik at den er gjenkjennelig for kollegaer.\n\nFor √• lette etterlevelsen av beste praksis for kodeutvikling p√• Dapla, har vi utviklet et verkt√∏y kalt ssb-project. Dette er et CLI-verkt√∏y2 som enkelt lar deg opprette et prosjekt med en standard mappestruktur, et virtuelt milj√∏ og integrasjon med Git for versjonsh√•ndtering. Som en bonus kan det ogs√• opprette et GitHub-repositorium for deg ved behov.\nI dette kapitlet vil vi veilede deg gjennom bruken av ssb-project. Du vil l√¶re √• opprette et nytt prosjekt, installere pakker, h√•ndtere versjoner med Git, bygge et eksisterende prosjekt og vedlikeholde prosjektet over tid.\n\n\n\n\n\n\nSSB-project st√∏tter ikke R enda\n\n\n\nPer n√• st√∏tter SSB-project kun prosjekter skrevet i Python. Dette skyldes begrensninger ved det popul√¶re virtuelle milj√∏-verkt√∏yet for R, renv. Mens renv effektivt h√•ndterer versjoner av R-pakker, har det ikke kapasitet til √• ta vare p√• spesifikke R-installasjonsversjoner. Dette kan potensielt gj√∏re det mer utfordrende √• reprodusere tidligere publiserte resultater ved bruk av ssb-project. Vi arbeider mot en l√∏sning for √• inkludere st√∏tte for R i fremtiden."
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#forberedelser",
    "href": "statistikkere/jobbe-med-kode.html#forberedelser",
    "title": "Jobbe med kode",
    "section": "Forberedelser",
    "text": "Forberedelser\nF√∏r du kan ta i bruk ssb-project s√• er det et par ting som m√• v√¶re p√• plass:\n\nDu m√• ha konfigurert Git etter SSB sin standard (les mer om hvordan her).\nHvis du √∏nsker at ssb-project ogs√• skal opprette et GitHub-repo for deg m√• du ogs√• f√∏lgende v√¶re p√• plass:\n\nDu m√• ha en GitHub-bruker (les hvordan her)\nSkru p√• 2-faktor autentifisering for GitHub-brukeren din (les hvordan her)\nV√¶re koblet mot SSBs organisasjon statisticsnorway p√• GitHub (les hvordan her)\nOpprette Personal Access Token (PAT) og godkjenne det for bruk mot statisticsnorway (les hvordan her)\n\n\nDet er ogs√• √• anbefale at du lagrer PAT lokalt slik at du ikke trenger √• forholde deg til det n√•r jobber med Git og GitHub. Hvis du har alt dette p√• plass s√• kan du bare fortsette √• f√∏lge de neste kapitlene."
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#opprett-ssb-project",
    "href": "statistikkere/jobbe-med-kode.html#opprett-ssb-project",
    "title": "Jobbe med kode",
    "section": "Opprett ssb-project",
    "text": "Opprett ssb-project\n\n\n\n\n\n\nHar du Github bruker? Noe funksjonalitet i ssb-project krever det. Finn ut hvordan ved √• lese forrige kapittel.\n\n\n\nssb-project lar deg opprette en prosjekt-mappe med og uten GitHub-repo. La oss ta for oss hver av alternativene.\n\nUten GitHub-repo\nFor √• opprette et nytt ssb-project uten GitHub-repo gj√∏r du f√∏lgende:\n\n√Öpne en terminal. De fleste vil gj√∏re dette i Jupyterlab p√• bakke eller sky og da kan de bare trykke p√• det bl√• ‚ûï-tegnet i Jupyterlab og velge Terminal.\nF√∏r vi kj√∏rer programmet m√• vi v√¶re obs p√• at ssb-project vil opprette en ny mappe der vi st√•r. G√• derfor til den mappen du √∏nsker √• ha den nye prosjektmappen. For √• opprette et prosjekt som heter stat-testprod s√• skriver du f√∏lgende i terminalen:\n\n\n\nterminal\n\nssb-project create stat-testprod\n\n\n\nHvis du stod i hjemmemappen din p√• n√•r du skrev inn kommandoen over i terminalen, s√• har du f√•tt mappestrukturen som vises i Figur¬†1. 3. Den inneholder f√∏lgende :\n\n.git-mappe som blir opprettet for √• versjonsh√•ndtere med Git.\nsrc-mappe som skal inneholde all koden som utgj√∏r produksjonsl√∏pet.\ntests-mappe som inneholder tester du skriver for koden din.\nLICENCE-fil som skal benyttes for public-repos i SSB.\npoetry.lock-fil som inneholder alle versjoner av Python-pakker som blir brukt.\npyproject.toml-fil som inneholder informasjon om prosjektet og hvilke pakker som blir brukt.\nREADME.md-fil som brukes for tekstlig innhold p√• GitHub-siden for prosjektet.\n\n\n\n\n\n\n\n\n\nFigur¬†1: Mappen som ble opprettet av ssb-project.\n\n\n\n\n\n\n\nMed Github-repo\nOver s√• opprettet vi et ssb-project uten √• opprette et GitHub-repo. Hvis du √∏nsker √• opprette et GitHub-repo ogs√• m√• du endre kommandoen over til:\n\n\nterminal\n\nssb-project create stat-testprod --github --github-token='blablabla'\n\nKommandoen over oppretter en mappestruktur slik vi s√• tidligere, men ogs√• et ssb-project som heter stat-testprod med et GitHub-repo med samme navn. Som du ser s√• m√• vi da sende med opsjonen --github og PAT med opsjonen --github-token='blablabla'. Repoet i GitHub ser da ut som i Figur¬†2. Hvis du √∏nsker √• slippe m√•tte forholde deg til PAT hver gang interagerer med GitHub, kan du f√∏lge denne beskrivelsen for √• lagre den lokalt. Da kan droppe --github-token='blablabla' fra kommandoen over.\n\n\n\n\n\n\nFigur¬†2: GitHub-repo som er opprettet av ssb-project\n\n\n\n\n\n\n\n\n\nN√•r du har opprettet et nytt ssb-project, eller bygget et eksisterende prosjekt, s√• kan det ta rundt 30 sekunder f√∏r kernelen viser seg i Jupterlab-launcher. V√¶r t√•lmodig!"
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#installere-pakker",
    "href": "statistikkere/jobbe-med-kode.html#installere-pakker",
    "title": "Jobbe med kode",
    "section": "Installere pakker",
    "text": "Installere pakker\nN√•r du har opprettet et ssb-project s√• kan du installere de python-pakkene du trenger fra PyPI. Men f√∏r du installerer en pakke b√∏r gj√∏re f√∏lgende for √• sikre deg at du ikke installerer en pakke med skadelig kode:\n\nS√∏k opp pakken p√• PyPI.\nSjekk om pakken er et popul√¶rt/velkjent prosjekt ved √• bes√∏ke repoet der koden ligger. Antall Stars og Forks p√• gitHub er en grei indikasjon p√• dette.\nHvis du er i tvil om pakken er trygg √• installere, s√• kan du sp√∏rre kollegaer om de har erfaring med den, eller sp√∏rre p√• en egnet Yammer-kanal i SSB.\nHvis du fortsatt √∏nsker √• installere pakken s√• anbefaler vi √• copy-paste navnet fra PyPi, ikke skrive det inn manuelt n√•r du installerer.\n\nSelve installeringen av pakken gj√∏res enkelt p√• f√∏lgende m√•te:\n\n√Öpne en terminal i Jupyterlab.\nG√• inn i prosjektmappen din ved √• skrive:\n\n\n\nterminal\n\ncd &lt;sti til prosjektmappe&gt;\n\n\nLag en branch/utviklingsbranch som f.eks. heter install-pandas:\n\n\n\nterminal\n\ngit checkout -b install-pandas\n\n\nInstaller, f.eks. Pandas, ved √• skrive f√∏lgende:\n\n\n\nterminal\n\npoetry add pandas\n\n\n\n\n\n\n\nFigur¬†3: Installasjon av Pandas med ssb-project\n\n\n\nFigur¬†3 viser hvordan dette vil se ut i en Jupyterlab-terminal. Kommandoen for √• installere noe er poetry add etterfulgt av pakkenavnet. Vi ser ogs√• at den automatisk legger til Pandas-versjonen i filen poetry.lock.\nDu kan ogs√• spesifisere en konkret versjon av pakken som skal installeres med f√∏lgende kommando:\n\n\nterminal\n\npoetry add pandas@1.2.3\n\n\nAvinstallere pakker\nOfte eksperimenterer man med nye pakker, og alle blir ikke med videre i produksjonskoden. Det er god praksis √• fjerne pakker som ikke brukes, blant annet for √• unng√• at de blir en sikkerhetsrisiko. Det gj√∏r du enkelt ved √• skrive f√∏lgende i terminalen:\n\n\nterminal\n\npoetry remove pandas\n\n\n\nOppdatere pakker\nHvis det kommer en ny versjon av en pakke du bruker, s√• kan du oppdatere den med f√∏lgende kommando:\n\n\nterminal\n\npoetry update pandas\n\nhvis du kj√∏rer poetry update uten noe pakkenavn, s√• vil alle pakkene dine oppdateres til siste versjon, med mindre du har spesifisert versjonsbegrensninger i pyproject.toml-filen.\n\n\nUnders√∏k avhengigheter\nHvis du lurer p√• hvilke pakker som har hvilke avhengigheter, s√• kan du lett liste ut dette i terminalen med f√∏lgende kommando:\n\n\nterminal\n\npoetry show --tree\n\nDet vil gi en grafisk fremstilling av avhengighetene som vist i Figur¬†4.\n\n\n\n\n\n\nFigur¬†4: Visning av pakke-avhengigheter i ssb-project"
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#push-til-github",
    "href": "statistikkere/jobbe-med-kode.html#push-til-github",
    "title": "Jobbe med kode",
    "section": "Push til GitHub",
    "text": "Push til GitHub\nN√•r du n√• har installert en pakke s√• har filen poetry.lock endret seg. For at dine samarbeidspartnere skal f√• tilgang til denne endringen i et SSB-project, s√• m√• du pushe en ny versjon av poetry.lock-filen opp Github, og kollegaene m√• pulle ned og bygge prosjektet p√• nytt. Du kan gj√∏re dette p√• f√∏lgende m√•te etter at du har installert en ny pakke:\n\nVi kan stage alle endringer med f√∏lgende kommando i terminalen n√•r vi st√•r i prosjektmappen:\n\n\n\nterminal\n\ngit add -A\n\n\nDeretter commite en endring, dvs. ta et stillbilde av koden i dette √∏yeblikket, ved √• skrive f√∏lgende:\n\n\n\nterminal\n\ngit commit -m \"Installert pandas\"\n\n\nPush det opp til GitHub4. Anta at vi gjorde dette i branchen install-pandas som ble opprettet tidligere. Da kan vi skrive f√∏lgende :\n\n\n\nterminal\n\ngit push --set-upstream origin install-pandas\n\nDeretter kan kollegaene dine pulle ned endringene og bygge prosjektet p√• nytt. Vi forklarer hvordan man kan bygge prosjektet p√• nytt senere i kapitlet."
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#dependabot",
    "href": "statistikkere/jobbe-med-kode.html#dependabot",
    "title": "Jobbe med kode",
    "section": "Dependabot",
    "text": "Dependabot\nN√•r man installerer pakker s√• vil det etter hvert utvikle seg et sett av potensielt kompliserte avhengigheter mellom disse pakkene. Dette skyldes at en pakke kan benytte seg av funksjonalitet i andre pakker, som igjen benytter seg av funksjonalitet i andre pakker, osv.. Hvis noen finner en sikkerhetss√•rbarhet i en pakke s√• kan det fikses ved at en ny versjon av den pakken slippes, som igjen kan f√• konsekvenser for pakker som er avhengig av denne.\nI SSB er det tilrettelagt for at alle som versjonsh√•ndterer koden sin p√• GitHub kan skanne pakkene sine for s√•rbarheter og nye versjoner av pakker med Dependabot. Dependabot hjelper oss med √• finne og fikse s√•rbarheter og gamle pakkeversjoner. Dette er spesielt viktig n√•r man installerer sine egne pakker.\nDependabot sjekker med jevne mellomrom om det finnes oppdateringer i pakkene som er listet i din pyproject.toml-fil, og den tilh√∏rende poetry.lock. Hvis det finnes oppdateringer s√• vil den lage en pull request som du kan godkjenne. N√•r du godkjenner den s√• vil den oppdatere poetry.lock-filen og lage en ny commit som du kan pushe til GitHub. Dependabot gir ogs√• en sikkerhetsvarslinger hvis det finnes kjente s√•rbarheter i pakkene du bruker.\nDet er anbefalt at alle som installerer sine egne pakker i SSB skrur p√• Dependabot i sine GitHub-repoer.\n\nAktivere Dependabot\nDu kan aktivere Dependabot ved √• gi inn i GitHub-repoet ditt og gj√∏re f√∏lgende:\n\nG√• inn repoet\nTrykk p√• Settings for det repoet som vist p√• Figur¬†5.\n\n\n\n\n\n\n\nFigur¬†5: √Öpne Settings for et GitHub-repo.\n\n\n\n\nI menyen til venstre velger du Code security and analysis\nUnder seksjonen Dependabot velger Enable p√• minst Dependabot alerts og Dependabot security updates, slik som vist i Figur¬†6.\n\n\n\n\n\n\n\nFigur¬†6: Skru p√• Dependabot i GitHub.\n\n\n\nN√•r du har gjort dette vil GitHub varsle deg hvis det finnes en kjent s√•rbarhet i pakkene som benyttes.\n\n\nOppdatere pakker\nHvis en av pakkene du bruker kommer med en oppdatering, s√• vil Dependabot lage en pull request (PR) i GitHub som du kan godkjenne. Dependabot sjekker ogs√• om oppdateringen er i konflikt med andre pakker du bruker. Hvis det er tilfellet s√• vil den lage en pull request som oppdaterer alle pakkene som er i konflikt.\nHvis en av pakkene du bruker har en kjent sikkerhetss√•rbarhet, s√• vil Dependabot varsle deg om dette under Security-fanen i GitHub-repoet ditt. Hvis du trykker p√• View Dependabot alerts s√• vil du f√• en oversikt over alle s√•rbarhetene som er funnet, og hvilken alvorlighetsgrad den har. Hvis du trykker p√• en av s√•rbarhetene s√• vil du f√• mer informasjon om den, og du kan trykke p√• Create pull request for √• oppdatere pakken.\nSom nevnt over kan du enkelt oppdatere pakker fra GitHub ved hjelp av Dependabot. Men det finnes tilfeller der du vil teste om en oppdatering gj√∏r at deler av koden din ikke fungerer lenger. Anta at du bruker en Python-pakken Pandas i koden din, og at du f√•r en pull request fra Dependabot om √• oppdatere den fra versjon 1.5 til 2.0. Hvis du √∏nsker √• teste om koden din fortsatt fungerer med den nye versjonen av Pandas, s√• kan du gj√∏re dette i Jupyterlab ved √• f√∏lge ved √• lage en branch som f.eks. heter update-pandas. Deretter kan du installere den nye versjonen av Pandas med f√∏lgende kommando fra terminalen:\n\n\nterminal\n\npoetry update pandas@2.0\n\nHvis du n√• kj√∏rer koden din kan du teste om den fortsatt fungerer som forventet. Gj√∏r den ikke det kan du tilpasse koden din og pushe endringene til Github. Deretter kan du godkjenne pull requesten fra Dependabot og merge den. Etter dette kan du slette den PR-en som Dependabot lagde for deg."
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#bygg-eksisterende-ssb-project",
    "href": "statistikkere/jobbe-med-kode.html#bygg-eksisterende-ssb-project",
    "title": "Jobbe med kode",
    "section": "Bygg eksisterende ssb-project",
    "text": "Bygg eksisterende ssb-project\nN√•r vi skal samarbeide med andre om kode s√• gj√∏r vi dette via GitHub. N√•r du pusher koden din til GitHub, s√• kan samarbeidspartnere pulle ned koden og jobbe videre med den. Men n√•r de henter ned koden s√• vil de bare hente ned selve koden, ikke pakker og Python-versjonen som ble brukt. De m√• installere alt som du hadde installert. I tillegg trenger de en kernel hvis de skal jobbe i Jupyterlab. ssb-project gj√∏r det sv√¶rt enkelt √• bygge opp det du trenger, siden det virtuelle milj√∏et har all informasjon om hva som trengs.\nFor at samarbeidspartneren din skal kunne bygge milj√∏et p√• nytt, m√• de ha gjort en minimal konfigurering av Git. Les mer om hvordan du frem for √• gj√∏re dette her.\nFor √• bygge opp et eksisterende milj√∏ gj√∏r du f√∏lgende:\n\nF√∏rst m√• du kopiere prosjektet ned lokalt, eller klone repoet med git-terminologi\n\n\n\nterminal\n\ngit clone https://github.com/statisticsnorway/&lt;prosjektnavn&gt;\n\n\nG√• inn i mappen du klonet\n\n\n\nterminal\n\ncd &lt;prosjektnavn&gt;\n\n\nSkape et virtuelt milj√∏ og installere en tilsvarende Jupyter kernel med\n\n\n\nterminal\n\nssb-project build"
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#slette-ssb-project",
    "href": "statistikkere/jobbe-med-kode.html#slette-ssb-project",
    "title": "Jobbe med kode",
    "section": "Slette ssb-project",
    "text": "Slette ssb-project\nDet vil v√¶re tilfeller hvor man √∏nsker √• slette et ssb-project, enten fordi man ikke trenger koden lenger eller fordi man bare testet litt.\n\nLokalt\nHvis man jobber med flere prosjekter s√• kan det fort bli mange Jupyter kerneler hengende igjen. Derfor er det ogs√• mulighet √• kj√∏re\n\n\nterminal\n\nssb-project clean stat-testprod\n\nsom sletter Jupyter-kernelen og de installerte pakkene i prosjektet. Hvis du ogs√• √∏nsker √• slette selve mappen med kode m√• du gj√∏re det manuelt5:\n\n\nterminal\n\nrm -rf ~/stat-testprod/\n\nProsjektmappen over l√• direkte i hjemmemappen min og hjemmemappen p√• Linux kan alltid referes til med et tilda-tegn ~.\n\n\nArkiver GitHub-repo\nGitHub-repoer som er opprettet under SSB sin organinasjons statisticsnorway p√• GitHub kan ikke slettes, bare arkiveres. Grunnen er at hvis man oppdager en s√•rbarhet senere s√• er det viktig √• kunne se repoet for √• forst√• hva som har skjedd.\nHvis du ikke trenger et GitHub-repo lenger kan man arkivere repoet. Det gj√∏r du p√• f√∏lgende m√•te:\n\nGi inn i repoet Settings slik som vist med r√∏d pil i Figur¬†7.\n\n\n\n\n\n\n\nFigur¬†7: Settings for repoet.\n\n\n\n\nUnder General scroller du deg ned til delen som heter Danger Zone og velger Archive this repository, slik som vist p√• Figur¬†8.\n\n\n\n\n\n\n\nFigur¬†8: Arkivering av et repo.\n\n\n\n\nI dialogboksen som dukker opp fyller du inn reponavnet som beskrevet og trykker p√• I understand the consequences, archive this repository.\n\nN√•r det er gjort s√• er repoet lesbart, men man kan ikke jobbe med det. Men som vi ser av @#fig-github-repo-settings-archive-warning kan man omgj√∏re arkiveringen senere hvis det skulle v√¶re √∏nskelig."
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#spark-i-ssb-project",
    "href": "statistikkere/jobbe-med-kode.html#spark-i-ssb-project",
    "title": "Jobbe med kode",
    "section": "Spark i ssb-project",
    "text": "Spark i ssb-project\nFor √• kunne bruke Spark i et ssb-project m√• man f√∏rst installere pyspark. Det gj√∏r du ved √• skrive f√∏lgende i en terminal:\n\n\nterminal\n\npoetry add pyspark==$(pip show pyspark | grep Version | egrep -o \"([0-9]{1,}\\.)+[0-9]{1,}\") --no-dev\n\nHer installerer du samme versjon av pyspark som p√• Jupyterlab.\nVidere kan vi konfigurere Spark til √• enten kj√∏re p√• lokal maskin eller p√• flere maskiner (s√•kalte clusters). Under beskriver vi begge variantene.\n\nLokal maskin\nOppsettet for Pyspark p√• lokal maskin er det enkleste √• sette opp siden Pyspark vil ha direkte tilgang til det lokale filsystemet. Man kan bruke milj√∏variabelen PYSPARK_PYTHON til √• peke p√• det virtuelle milj√∏et, og dermed vil Pyspark ogs√• ha tilgang til alle pakkene som er installert der. I en notebook vil dette kunne settes opp slik:\n\n\nnotebook\n\nimport os\nimport subprocess\n\n# Finner filstien til det virtuelle milj√∏et\npython_path = subprocess.run(['poetry', 'run', 'which', 'python'],\n                             capture_output=True, text=True).stdout.rstrip('\\n')\n\nos.environ[\"PYSPARK_PYTHON\"] = python_path\nos.environ[\"PYSPARK_SUBMIT_ARGS\"] = os.environ[\"PYSPARK_LOCAL_SUBMIT_ARGS\"]\n\nN√•r du oppretter en Notebook og bruker den kernelen du har laget s√• m√• du alltid ha denne p√• toppen av notebooken. Det initialiserer Spark slik at det kan brukes i notebooken.\n\n\nnotebook\n\n%run /usr/local/share/jupyter/kernels/pyspark_local/init.py\n\nDette scriptet vil sette et spark objekt som brukes for √• kalle API‚Äôet til pyspark.\n\n\nCluster\nHvis man vil kj√∏re Pyspark i et cluster (dvs. p√• flere maskiner) s√• vil databehandlingen foreg√• p√• andre maskiner som ikke har tilgang til det lokale filsystemet. Man m√• dermed lage en ‚Äúpakke‚Äù av det virtuelle milj√∏et p√• lokal maskin og tilgjengeliggj√∏re dette for alle maskinene i clusteret. For √• lage en slik ‚Äúpakke‚Äù kan man bruke et bibliotek som heter venv-pack. Dette kan kj√∏res fra et terminalvindu slik:\n\n\nterminal\n\nvenv-pack -p .venv -o pyspark_venv.tar.gz\n\nMerk at kommandoen over m√• kj√∏res fra rot-mappen i prosjektet ditt. Her er pyspark_venv.tar.gz et tilfeldig valgt filnavn, men dette filnavnet skal brukes videre i notebooken.\n\n\nnotebook\n\nimport os\nimport subprocess\n\n# Milj√∏variabel som peker p√• en utpakket versjon av det virtuelle milj√∏et\nos.environ[\"PYSPARK_PYTHON\"] = \"./environment/bin/python\"\n\n# Legg til et flagg, --archives, som peker p√• \"pakken\" med det virtuelle milj√∏et\nconf = os.environ[\"PYSPARK_K8S_SUBMIT_ARGS\"].split(' ')\nlast_index = conf.index('pyspark-shell')\nconf[last_index:last_index] = ['--archives', 'pyspark_venv.tar.gz#environment']\nos.environ[\"PYSPARK_SUBMIT_ARGS\"] = ' '.join(conf)\n\nN√•r du oppretter en Notebook og bruker den kernelen du har laget s√• m√• du alltid ha denne p√• toppen av notebooken. Det initialiserer Spark slik at det kan brukes i notebooken.\n\n\nnotebook\n\n%run /usr/local/share/jupyter/kernels/pyspark_k8s/init.py\n\nDette scriptet vil sette et spark objekt som brukes for √• kalle API‚Äôet til pyspark."
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#tips-og-triks",
    "href": "statistikkere/jobbe-med-kode.html#tips-og-triks",
    "title": "Jobbe med kode",
    "section": "Tips og triks",
    "text": "Tips og triks\nI denne delen av kapitlet vil vi gi deg noen tips og triks som kan v√¶re nyttige n√•r du jobber med ssb-project.\n\nPoetry\nssb-project bruker Poetry for √• h√•ndtere virtuelle milj√∏er. Poetry er et verkt√∏y som gj√∏r det enkelt √• installere pakker og h√•ndtere versjoner av disse. Det er ogs√• Poetry som h√•ndterer Jupyter-kernelen for deg.\nHvis du etterlyser funksjonalitet i et ssb-project s√• kan det v√¶re nyttig √• lese dokumentasjonen til Poetry for √• se om det er mulig √• f√• til det du √∏nsker.\n\n\nFull disk p√• Dapla\nDet ‚Äúlokale‚Äù filsystemet p√• Dapla har kun 10GB diskplass. Har du mange virtuelle milj√∏er p√• denne disken kan det fort bli fullt, siden alle pakker blir installert her. Vanligvis er det 2 grunner til at disken blir full:\n\nFor mange virtuelle milj√∏er (ssb-projects) lagret lokalt.\nDette vil ofte kunne l√∏ses ved √• slette virtuelle milj√∏er som ikke lenger er i bruk. Hvis du har 5 virtuelle milj√∏er som hver bruker 1GB, og du kun jobber p√• en av de n√•, s√• vil du frigj√∏re 40% av disken ved √• slette 4 av dem. Husk at det permanente lagringsstedet for kode er p√• GitHub, og du kan alltid klone ned et prosjekt senere og bygge det hvis det trengs.\n/home/jovyan/.cache/ har blitt for stort.\nDette er en mappe som brukes av applikasjoner til √• lagre midlertidig data slik at de kan kj√∏re raskere. Denne kan bli ganske stor etter hvert. Ofte kan man frigj√∏re flere GB ved √• slette denne. Du sletter denne mappen ved √• skrive f√∏lgende i en terminal:\n\n\n\nterminal\n\nrm -rf /home/jovyan/.cache/\n\nHvis du opplever at disken er full, s√• kan det anbefales √• unders√∏ke hvilke mapper som tar st√∏rst plass med f√∏lgende kommando i terminalen:\n\n\nterminal\n\ncd ~ && du -h --max-depth=5 | sort -rh | head -n 10 && cd -\n\nKommandoen over sjekker, fra hjemmemappen din, hvilke mapper og undermapper som tar mest plass. Den viser de 10 st√∏rste mappene. Hvis du √∏nsker √• se flere mapper s√• kan du endre tallet etter head -n. Hvis du √∏nsker √• se alle mapper s√• kan du fjerne head -n. --max-depth=5 betyr at den kun sjekker mapper som er 5 mapper dype fra hjemmemappen din.\nN√•r du har gjort det kan selv vurdere hvilke som kan slettes for √• frigj√∏re plass.\n\n\nHold prosjektet oppdatert\nHvis du sitter med en lokal kopi av prosjektet ditt, og flere andre jobber med den samme kodebasen, s√• er det viktig at du holder din lokale kopi oppdatert. Hvis du jobber i en branch p√• en lokal kopi, b√∏r du holde denne oppdatert med main-branchen p√• GitHub. Det er vanlig Git-praksis. N√•r man ogs√• bruker ssb-project, s√• man huske √• ogs√• bygge prosjektet p√• nytt hver gang det er endringer som er gjort av andre i poetry.lock.-filen."
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#footnotes",
    "href": "statistikkere/jobbe-med-kode.html#footnotes",
    "title": "Jobbe med kode",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nEn kernel refererer til en Python- eller R-installasjon som er optimalisert for bruk med Jupyterlab Notebooks.‚Ü©Ô∏é\nCLI = Command-Line-Interface, som betyr et program designet for bruk i terminalen med kommandoer.‚Ü©Ô∏é\nFiler og mapper som starter med punktum er skjulte med mindre man ber om √• se dem. I Jupyterlab kan disse vises i filutforskeren ved √• velge View fra menylinjen, og deretter velge Show hidden files. I en terminal skriver man ls -a for √• se de.‚Ü©Ô∏é\n√Ö pushe til GitHub uten √• sende ved Personal Access Token fordrer at du har lagret det lokalt s√• Git kan finne det. Her et eksempel p√• hvordan det kan gj√∏res.‚Ü©Ô∏é\nDette kan ogs√• gj√∏res ved √• h√∏yreklikke p√• mappen i Jupyterlab sin filutforsker og velge Delete.‚Ü©Ô∏é"
  },
  {
    "objectID": "statistikkere/datadoc-editor.html",
    "href": "statistikkere/datadoc-editor.html",
    "title": "Datadoc-editor",
    "section": "",
    "text": "Datadoc editor er et grafisk grensesnitt for √• dokumentere datasett og variablene som utgj√∏r datasettet.\nForm√•let med tjenesten er √• tilby et lett-√•-bruke grensesnitt som hovedsakelig vil benyttes f√∏rste gang man dokumenterer en type datasett.\nSiden l√∏pende statistikkproduksjon ofte inneb√¶rer at nye data legges til data fra tidligere perioder, uten at strukturen i datasett endres, s√• tilbys det ogs√• et annet verkt√∏y som lar brukeren programmatisk gjenbruke metadata fra en tidligere periode. Les mer om Python-pakken dapla-toolbelt-metadata.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Datadoc-editor"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-editor.html#forberedelser",
    "href": "statistikkere/datadoc-editor.html#forberedelser",
    "title": "Datadoc-editor",
    "section": "Forberedelser",
    "text": "Forberedelser\nF√∏r man starter Datadoc-editor-tjenesten b√∏r man ha lest kapitlet om Dapla Lab. Deretter gj√∏r du f√∏lgende:\n\nLogg deg inn p√• Dapla Lab\nUnder Tjenestekatalog trykker du p√• Start-knappen for Datadoc-editor\nGi tjenesten et navn\n√Öpne Datadoc-editor konfigurasjoner og gj√∏r √∏nskede konfigurasjoner (se neste kapittel).\nTrykk Start igjen for √• √•pne tjenesten.\n\nDatadoc editor bruker ca. 1 minutt p√• starte og etter det klart for dokumentere datasett.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Datadoc-editor"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-editor.html#konfigurasjon",
    "href": "statistikkere/datadoc-editor.html#konfigurasjon",
    "title": "Datadoc-editor",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nF√∏r man starter Datadoc editor b√∏r man konfigurere tjenesten. Dette er spesielt viktig siden du bare kan representere et Dapla-team for hver Datadoc editor man starter. I tjenestekonfigurasjonen til Datadoc er det to nedtrekksmenyer: Data og Tjeneste.\n\nData\nUnder Data kan man velge Team og tilgangsgruppe. I denne menyen f√•r du listet alle team og tilgangsgrupper du er med i. Listen vises p√• formen &lt;daplateam&gt;-&lt;tilgangsgruppe&gt;.\nFigur¬†1 viser tilfellet der det er valgt √• representere tilgangsgruppen developers i teamet Dapla Felles, derav dapla-felles-developers. Dette er standardvalget.\n\n\n\n\n\n\nFigur¬†1: Data-menyen i tjenestekonfigurasjonen for Datadoc editor.\n\n\n\nDatadoc editor st√∏tter for √∏yeblikket ikke kildedata selv om man kan velge begrunnelse og tilgangvarighet¬†fra konfigurasjons fanen.\n\n\nTjeneste\nUnder menyen Tjeneste kan man velge versjon av tjenesten. Det vil v√¶re sv√¶rt sjelden at brukere trenger √• endre p√• noe her. Som standard √•pnes alltid siste versjon av tjenesten.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Datadoc-editor"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-editor.html#datatilgang",
    "href": "statistikkere/datadoc-editor.html#datatilgang",
    "title": "Datadoc-editor",
    "section": "Datatilgang",
    "text": "Datatilgang\nN√•r man starter en Datadoc-editor tjeneste m√• man p√• forh√•nd velge hvilket team og tilgangsgruppe man skal representere, som forklart i forrige del.\n\n\n\n\n\n\ndata-admins ikke tilgjengelig enda\n\n\n\nDet er ikke mulig √• velge andre tilgangsgrupper enn developers for √∏yeblikket. Av den grunn kan man ikke bruke Datadoc editor til √• dokumentere kildedata enda.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Datadoc-editor"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-editor.html#funksjonalitet",
    "href": "statistikkere/datadoc-editor.html#funksjonalitet",
    "title": "Datadoc-editor",
    "section": "Funksjonalitet",
    "text": "Funksjonalitet\n\n√Öpne datasett\nF√∏r man kan benytte Datadoc editor, m√• man √•pne et Datasett. Det gj√∏res enkelt ved √• lime inn stien til datasettet i Filsti tekstboksen (Punkt 1 i Figur¬†2) og trykke p√• √Öpne fil knappen (Punkt 2 i Figur¬†2).\nDatadoc editor benytter brukerens innloggingsopplysninger for √• aksessere data. Det betyr at man i utgangspunktet har tilgang til de samme filene som ellers p√• Dapla.\n\n\n\n\n\n\nWarning\n\n\n\nMan m√• inkludere gs:// p√• begynnelsen av stien n√•r man jobber med et datasett i en b√∏tte.\n\n\n\n\n\n\n\n\nNote\n\n\n\nMan kan finne filstien gjennom Google konsollet eller ved √• benytte Dapla toolbelt\n\n\n\n\n\n\n\n\nFigur¬†2: Input-feltet for √• oppgi filsti i Datadoc editor.\n\n\n\n\nVellykket √•pning\nEtter at man har trykket p√• √Öpne fil knappen b√∏r man se meldingen vist i Figur¬†3\n\n\n\n\n\n\nFigur¬†3: Meldingen at det var velykket √• √•pne datasettet.\n\n\n\n\n\nVellykket √•pning med advarsel\nHvis man √•pner et datasett som ikke f√∏lger navnestandarden, vil det komme en advarsel (Figur¬†4). Det er fortsatt fullt mulig √• bruke Datadoc editor for √• dokumentere datasettet, men ikke like mye metadata kan utledes automatisk (TODO: lenke til seksjonen om utledning).\n\n\n\n\n\n\nNote\n\n\n\nDette kan v√¶re en fin anledning til √• justere p√• navngivning og strukturen i teamets b√∏tter slik at alt f√∏lger navnestandarden. Det er en lenke til navnestandarden i meldingen.\n\n\n\n\n\n\n\n\nFigur¬†4: Meldingen at datasettet ikke f√∏lger navnestandarden.\n\n\n\n\n\nFeil ved √•pning\nHvis Datadoc editor ikke klarer √• √•pne datasettet vises en r√∏d error melding (Figur¬†5). Som oftest for√•rsakes dette av at filen ikke finnes (skrivefeil) eller fordi man ikke har tilgang til filen.\n\n\n\n\n\n\nFigur¬†5: Meldingen at det var en feil ved √•pning av datasettet.\n\n\n\n\n\n√Öpne et datasett n√•r metadatadokument eksisterer\nHvis et metadatadokument eksisterer, er det denne informasjonen som lastes inn. Det utledes ingenting fra datasettet.\n\n\n\nUtledet informasjon\nInformasjon som kan utledes vil bli fylt inn n√•r du √•pner datasettet. Informasjonen hentes enten fra filstien eller settes inn som en default verdi (*). Det er mulig √• korrigere informasjonen i ettertid. F√∏lgende felter blir fors√∏kt utledet:\nDatasett:\n\nVerdivurdering\nStatus (*)\nDatatilstand\nVersjon\nStatistikkomr√•de\nInneholder data f.o.m.\nInneholder data t.o.m.\nGeografisk dekningsomr√•de (*)\n\nVariabler:\n\nKortnavn\nDatatype\n\n\n\nDokumentere datasett-metadata\nDokumentasjon av datasettet som helthet gj√∏res i datasettfanen i Datadoc editor.\nAlle felter har en ordforklaring  du kan trykke p√•. Her vil du f√• en kort forklaring til hva som skal st√• i feltet.\nFlere felter har verdilister hvor mange er hentet fra KLASS, mens noen er fritekstfelter. For noen av fritekstfeltene gj√∏res det en sjekk av innholdet og du vil f√• en feilmelding hvis kriteriene ikke er oppfylt.\n\nObligatorisk\nAlt som st√•r under obligatorisk m√• fylles inn.\n\n\nAnbefalt\nAnbefalte felter er frivillig √• fylle ut.\n\n\nMaskingenerert\nFeltene her genereres automatisk og kan ikke redigeres. De er kun med til informasjon.\n\n\n\nDokumentere variabelforekomst-metadata\nDokumentasjon av variabelforekomster for et datasett kan gj√∏res i variabelfanen i Datadoc editor. Her vil man se en liste av alle kortnavnene til variabelforekomstene i datasettet. Ved √• trykke seg inn p√• et av kortnavnene kan man dokumentere de obligatoriske og anbefalte feltene for en variabelforekomst.\n\nArv mellom datasett og variabelforekomst fanen\nFor √• forenkle dokumentasjonen av variabelforekomster vil noen felt arve verdiene som blir satt i datasettfanen. Dette gjelder f√∏lgende felter:\n\nDatakilde\nPopulasjon\nTemporalitetstype\nInneholder data f.o.m\nInneholder data t.o.m\n\nDet er mulig √• redigere vediene i variabelforekomst fanen etter en verdi er satt i datasettfanen. Hvis disse feltene blir endret i datasettfanen senere, vil de alltid overskrive det som er satt i variabelforekomst fanen.\n\n\nS√∏k i variabelforekomster\nDet er mulig √• s√∏ke gjennom variabelforekomstene sine kortnavn. Dette filtrerer p√• listen over variabelforekomster.\n\n\n\n\n\n\nFigur¬†6: S√∏k gjennom kortnavn til variabelforekomster\n\n\n\n\n\n\nLagre metadata\n\n\n\n\n\n\nViktig informasjon\n\n\n\nDatadoc editor mellomlagrer ikke utfylt metadata.\nPass p√• √• lagre metadataene ofte ved √• trykke lagre og legg merke til om du f√•r en bekreftelse p√• at metadataene er lagret.\n\n\n\nVed lagring\nN√•r du trykker Lagre metadata knappen vil du f√• en bekreftelse p√• vellykket lagring. \nHvis ikke alle obligatoriske felt er utfylt vil du f√• opp en advarsel for datasett og variabelforekomstene. Advarselen for datasett viser en liste over hvilke felt som mangler. For variabelforekomster vises b√•de variabelens kortnavn og manglende felt.\nN√•r du fyller ut de manglende obligatoriske feltene m√• du lagre p√• nytt og advarslene vil forsvinne n√•r alle obligatoriske felt er fylt ut.\nVed lagring gj√∏res det ogs√• en sjekk p√• om variabel kortnavene avviker¬†for navnestandarden¬†for variabelnavn. Om det finnes avvik vil disse kortnavene vises i en gul advarsel boks. Navnestandarden for variabelkortnavn er som f√∏lger:\n\nAlfanumerisk begrenset til a-z (kun sm√• bokstaver)\n0-9\n_ (understrek).\n\n\n\nMetadata filen\nN√•r du trykker p√• Lagre metadata knappen i Datadoc editor skrives alle metadata til en fil i samme mappe (katalog) som datafilen. Dette er en JSON-fil med nesten samme navn som datafilen. Navnekonvensjonen for metadatafilen er\n&lt;navn p√• datasettfilen uten endelse&gt;__DOC.json\nEksempelvis vil Datadoc lagre metadata i filen skattedata_p2022_v1__DOC.json hvis datafilen har navnet skattedata_p2022_v1.parquet.\nFordelen med √• benytte en JSON-fil til √• lagre metadata er at denne filen kan kopieres og flyttes like enkelt som selve datafilen. JSON-filer er strukturerte tekstfiler som kan leses av b√•de maskiner (Python/R) og av mennesker (√•pnes i en tekst-editor).\nSe et eksempel p√• JSON metadata-fil lagret av DataDoc.\n\n\n\nModifisere metadata\n√ònsker du √• endre eller legge til metadata, √•pner du et datasett slik som beskrevet i √Öpne et datasett. Da vil innholdet fra metadata-filen leses inn i Datadoc editor og kan redigeres videre. Endringene blir lagret n√•r man trykker Lagre metadata.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Datadoc-editor"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-editor.html#kildekode",
    "href": "statistikkere/datadoc-editor.html#kildekode",
    "title": "Datadoc-editor",
    "section": "Kildekode",
    "text": "Kildekode\nKildekoden til Datadoc editor er offentlig tilgjengelig p√• Github: https://github.com/statisticsnorway/datadoc",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Datadoc-editor"
    ]
  },
  {
    "objectID": "statistikkere/ssb-project.html",
    "href": "statistikkere/ssb-project.html",
    "title": "SSB-project",
    "section": "",
    "text": "Note\n\n\n\nDenne artikkelen fokuserer p√• SSB-project som GitHub-mal. Vi har skrevet en egen artikkel for SSB-project som verkt√∏y for √• h√•ndtere Python-pakker: Pakkeh√•ndtering i Python.\nStatistikkproduksjon p√• Dapla m√• v√¶re reproduserbart, delbart og gjenkjennelig. SSB-project er et verkt√∏y som hjelper deg med dette ved √• gj√∏re f√∏lgende:\nVi mener at ssb-project er et naturlig sted √• starte n√•r man skal bygge opp koden i Python eller R. Det gjelder b√•de p√• bakken og p√• sky. I denne delen av kapitlet forklarer vi deg hvordan du kan ta i bruk ssb-project.\nKort fortalt kan du kj√∏re denne kommandoen i en terminal:\nDa vil f√• en mappe som heter stat-testprod med f√∏lgende innhold:\nI tillegg lar ssb-project deg opprette et GitHub-repo hvis du √∏nsker. Hvis du velger √• la ssb-project opprette et GitHub-repo for deg, s√• vil det ogs√• sette opp SSBs anbefalte GitHub-oppsett. Det er viktig for at du skal kunne dele koden din med andre i SSB p√• en sikker m√•te.",
    "crumbs": [
      "Manual",
      "Kode",
      "SSB-project"
    ]
  },
  {
    "objectID": "statistikkere/ssb-project.html#forberedelser",
    "href": "statistikkere/ssb-project.html#forberedelser",
    "title": "SSB-project",
    "section": "Forberedelser",
    "text": "Forberedelser\nF√∏r du kan ta i bruk ssb-project s√• er det et par ting som m√• v√¶re p√• plass:\n\nDu m√• ha konfigurert Git etter SSB sin standard (les mer om hvordan her).\nHvis du √∏nsker at ssb-project ogs√• skal opprette et GitHub-repo for deg m√• ogs√• f√∏lgende v√¶re p√• plass:\n\nDu m√• ha en GitHub-bruker (les hvordan her)\nSkru p√• 2-faktor autentifisering for GitHub-brukeren din (les hvordan her)\nV√¶re koblet mot SSBs organisasjon statisticsnorway p√• GitHub (les hvordan her)\nOpprette Personal Access Token (PAT) og godkjenne det for bruk mot statisticsnorway (les hvordan her)\n\n\nDet er ogs√• √• anbefale at du lagrer PAT lokalt/i Dapla Lab slik at du ikke trenger √• forholde deg til det n√•r jobber med Git og GitHub. Hvis du har alt dette p√• plass s√• kan du bare fortsette √• f√∏lge de neste kapitlene.",
    "crumbs": [
      "Manual",
      "Kode",
      "SSB-project"
    ]
  },
  {
    "objectID": "statistikkere/ssb-project.html#opprett-ssb-project",
    "href": "statistikkere/ssb-project.html#opprett-ssb-project",
    "title": "SSB-project",
    "section": "Opprett ssb-project",
    "text": "Opprett ssb-project\n\nUten GitHub-repo\nFor √• opprette et nytt ssb-project uten GitHub-repo gj√∏r du f√∏lgende:\n\n√Öpne en terminal. De fleste vil gj√∏re dette i Jupyterlab p√• bakke eller sky og da kan de bare trykke p√• det bl√• ‚ûï-tegnet i Jupyterlab og velge Terminal.\nF√∏r vi kj√∏rer programmet m√• vi v√¶re obs p√• at ssb-project vil opprette en ny mappe der vi st√•r. G√• derfor til den mappen du √∏nsker √• ha den nye prosjektmappen i. For √• opprette et prosjekt som heter stat-testprod s√• skriver du f√∏lgende i terminalen:\n\n\n\nterminal\n\nssb-project create stat-testprod\n\n\n\nHvis du stod i hjemmemappen din p√• n√•r du skrev inn kommandoen over i terminalen, s√• har du f√•tt mappestrukturen som vises i Figur¬†1. 1. Den inneholder f√∏lgende :\n\n.git-mappe som blir opprettet for √• versjonsh√•ndtere med Git.\nsrc-mappe som skal inneholde all koden som utgj√∏r produksjonsl√∏pet. src er kort for source\ntests-mappe som inneholder tester du skriver for koden din.\nLICENCE-fil som skal benyttes for public-repos i SSB.\npoetry.lock-fil som inneholder alle versjoner av Python-pakker som blir brukt.\npyproject.toml-fil som inneholder informasjon om prosjektet og hvilke pakker som blir brukt.\nREADME.md-fil som brukes for tekstlig innhold p√• GitHub-siden for prosjektet.\n\n\n\n\n\n\n\n\n\nFigur¬†1: Mappen som ble opprettet av ssb-project.\n\n\n\n\n\n\n\nMed Github-repo\nOver s√• opprettet vi et ssb-project uten √• opprette et GitHub-repo. Hvis du √∏nsker √• opprette et GitHub-repo ogs√• m√• du endre kommandoen over til:\n\n\nterminal\n\nssb-project create stat-testprod --github --github-token='blablabla'\n\nKommandoen over oppretter en mappestruktur slik vi s√• tidligere, men ogs√• et ssb-project som heter stat-testprod med et GitHub-repo med samme navn. Som du ser s√• m√• vi da sende med opsjonen --github og PAT med opsjonen --github-token='blablabla'. Repoet i GitHub ser da ut som i Figur¬†2. Hvis du √∏nsker √• slippe √• m√•tte forholde deg til PAT hver gang interagerer med GitHub, kan du f√∏lge denne beskrivelsen for √• lagre den lokalt. Da kan droppe --github-token='blablabla' fra kommandoen over.\n\n\n\n\n\n\nFigur¬†2: GitHub-repo som er opprettet av ssb-project\n\n\n\n\n\n\n\n\n\nN√•r du har opprettet et nytt ssb-project, eller bygget et eksisterende prosjekt, s√• kan det ta rundt 30 sekunder f√∏r kernelen viser seg i Jupterlab-launcher. V√¶r t√•lmodig!",
    "crumbs": [
      "Manual",
      "Kode",
      "SSB-project"
    ]
  },
  {
    "objectID": "statistikkere/ssb-project.html#bygg-eksisterende-ssb-project",
    "href": "statistikkere/ssb-project.html#bygg-eksisterende-ssb-project",
    "title": "SSB-project",
    "section": "Bygg eksisterende ssb-project",
    "text": "Bygg eksisterende ssb-project\nN√•r vi skal samarbeide med andre om kode s√• gj√∏r vi dette via GitHub. N√•r du pusher koden din til GitHub, s√• kan samarbeidspartnere pulle ned koden og jobbe videre med den. Men n√•r de henter ned koden s√• vil de bare hente ned selve koden, ikke pakker og Python-versjonen som ble brukt. De m√• installere alt som du hadde installert. I tillegg trenger de en kernel hvis de skal jobbe i Jupyterlab. ssb-project gj√∏r det sv√¶rt enkelt √• bygge opp det du trenger, siden det virtuelle milj√∏et har all informasjon om hva som trengs.\nFor at samarbeidspartneren din skal kunne bygge milj√∏et p√• nytt, m√• de ha gjort en minimal konfigurering av Git. Les mer om hvordan du frem for √• gj√∏re dette her.\nFor √• bygge opp et eksisterende milj√∏ gj√∏r du f√∏lgende:\n\nF√∏rst m√• du kopiere prosjektet ned lokalt, eller klone repoet med git-terminologi\n\n\n\nterminal\n\ngit clone https://github.com/statisticsnorway/&lt;prosjektnavn&gt;\n\n\nG√• inn i mappen du klonet\n\n\n\nterminal\n\ncd &lt;prosjektnavn&gt;\n\n\nSkape et virtuelt milj√∏ og installere en tilsvarende Jupyter kernel med\n\n\n\nterminal\n\nssb-project build",
    "crumbs": [
      "Manual",
      "Kode",
      "SSB-project"
    ]
  },
  {
    "objectID": "statistikkere/ssb-project.html#slette-ssb-project",
    "href": "statistikkere/ssb-project.html#slette-ssb-project",
    "title": "SSB-project",
    "section": "Slette ssb-project",
    "text": "Slette ssb-project\nDet vil v√¶re tilfeller hvor man √∏nsker √• slette et ssb-project, enten fordi man ikke trenger koden lenger eller fordi man bare testet litt.\n\nLokalt\nHvis man jobber med flere prosjekter s√• kan det fort bli mange Jupyter kerneler hengende igjen. Derfor er det ogs√• mulighet √• kj√∏re\n\n\nterminal\n\nssb-project clean stat-testprod\n\nsom sletter Jupyter-kernelen og de installerte pakkene i prosjektet. Hvis du ogs√• √∏nsker √• slette selve mappen med kode m√• du gj√∏re det manuelt2:\n\n\nterminal\n\nrm -rf ~/stat-testprod/\n\nProsjektmappen over l√• direkte i hjemmemappen min og hjemmemappen p√• Linux kan alltid referes til med et tilda-tegn ~.\n\n\nArkiver GitHub-repo\nGitHub-repoer som er opprettet under SSB sin organinasjons statisticsnorway p√• GitHub kan ikke slettes, bare arkiveres. Grunnen er at hvis man oppdager en s√•rbarhet senere s√• er det viktig √• kunne se repoet for √• forst√• hva som har skjedd.\nHvis du ikke trenger et GitHub-repo lenger kan man arkivere repoet. Det gj√∏r du p√• f√∏lgende m√•te:\n\nGi inn i repoet Settings slik som vist med r√∏d pil i Figur¬†3.\n\n\n\n\n\n\n\nFigur¬†3: Settings for repoet.\n\n\n\n\nUnder General scroller du deg ned til delen som heter Danger Zone og velger Archive this repository, slik som vist p√• Figur¬†4.\n\n\n\n\n\n\n\nFigur¬†4: Arkivering av et repo.\n\n\n\n\nI dialogboksen som dukker opp fyller du inn reponavnet som beskrevet og trykker p√• I understand the consequences, archive this repository.\n\nN√•r det er gjort s√• er repoet lesbart, men man kan ikke jobbe med det. Men som vi ser av @#fig-github-repo-settings-archive-warning kan man omgj√∏re arkiveringen senere hvis det skulle v√¶re √∏nskelig.",
    "crumbs": [
      "Manual",
      "Kode",
      "SSB-project"
    ]
  },
  {
    "objectID": "statistikkere/ssb-project.html#spark-i-ssb-project",
    "href": "statistikkere/ssb-project.html#spark-i-ssb-project",
    "title": "SSB-project",
    "section": "Spark i ssb-project",
    "text": "Spark i ssb-project\nFor √• kunne bruke Spark i et ssb-project m√• man f√∏rst installere pyspark. Det gj√∏r du ved √• skrive f√∏lgende i en terminal:\n\n\nterminal\n\npoetry add pyspark==$(pip show pyspark | grep Version | egrep -o \"([0-9]{1,}\\.)+[0-9]{1,}\") --no-dev\n\nHer installerer du samme versjon av pyspark som p√• Jupyterlab.\nVidere kan vi konfigurere Spark til √• enten kj√∏re p√• lokal maskin eller p√• flere maskiner (s√•kalte clusters). Under beskriver vi begge variantene.\n\nLokal maskin\nOppsettet for Pyspark p√• lokal maskin er det enkleste √• sette opp siden Pyspark vil ha direkte tilgang til det lokale filsystemet. Man kan bruke milj√∏variabelen PYSPARK_PYTHON til √• peke p√• det virtuelle milj√∏et, og dermed vil Pyspark ogs√• ha tilgang til alle pakkene som er installert der. I en notebook vil dette kunne settes opp slik:\n\n\nnotebook\n\nimport os\nimport subprocess\n\n# Finner filstien til det virtuelle milj√∏et\npython_path = subprocess.run(['poetry', 'run', 'which', 'python'],\n                             capture_output=True, text=True).stdout.rstrip('\\n')\n\nos.environ[\"PYSPARK_PYTHON\"] = python_path\nos.environ[\"PYSPARK_SUBMIT_ARGS\"] = os.environ[\"PYSPARK_LOCAL_SUBMIT_ARGS\"]\n\nN√•r du oppretter en Notebook og bruker den kernelen du har laget s√• m√• du alltid ha denne p√• toppen av notebooken. Det initialiserer Spark slik at det kan brukes i notebooken.\n\n\nnotebook\n\n%run /usr/local/share/jupyter/kernels/pyspark_local/init.py\n\nDette scriptet vil sette et spark objekt som brukes for √• kalle API‚Äôet til pyspark.\n\n\nCluster\nHvis man vil kj√∏re Pyspark i et cluster (dvs. p√• flere maskiner) s√• vil databehandlingen foreg√• p√• andre maskiner som ikke har tilgang til det lokale filsystemet. Man m√• dermed lage en ‚Äúpakke‚Äù av det virtuelle milj√∏et p√• lokal maskin og tilgjengeliggj√∏re dette for alle maskinene i clusteret. For √• lage en slik ‚Äúpakke‚Äù kan man bruke et bibliotek som heter venv-pack. Dette kan kj√∏res fra et terminalvindu slik:\n\n\nterminal\n\nvenv-pack -p .venv -o pyspark_venv.tar.gz\n\nMerk at kommandoen over m√• kj√∏res fra rot-mappen i prosjektet ditt. Her er pyspark_venv.tar.gz et tilfeldig valgt filnavn, men dette filnavnet skal brukes videre i notebooken.\n\n\nnotebook\n\nimport os\nimport subprocess\n\n# Milj√∏variabel som peker p√• en utpakket versjon av det virtuelle milj√∏et\nos.environ[\"PYSPARK_PYTHON\"] = \"./environment/bin/python\"\n\n# Legg til et flagg, --archives, som peker p√• \"pakken\" med det virtuelle milj√∏et\nconf = os.environ[\"PYSPARK_K8S_SUBMIT_ARGS\"].split(' ')\nlast_index = conf.index('pyspark-shell')\nconf[last_index:last_index] = ['--archives', 'pyspark_venv.tar.gz#environment']\nos.environ[\"PYSPARK_SUBMIT_ARGS\"] = ' '.join(conf)\n\nN√•r du oppretter en Notebook og bruker den kernelen du har laget s√• m√• du alltid ha denne p√• toppen av notebooken. Det initialiserer Spark slik at det kan brukes i notebooken.\n\n\nnotebook\n\n%run /usr/local/share/jupyter/kernels/pyspark_k8s/init.py\n\nDette scriptet vil sette et spark objekt som brukes for √• kalle API‚Äôet til pyspark.",
    "crumbs": [
      "Manual",
      "Kode",
      "SSB-project"
    ]
  },
  {
    "objectID": "statistikkere/ssb-project.html#tips-og-triks",
    "href": "statistikkere/ssb-project.html#tips-og-triks",
    "title": "SSB-project",
    "section": "Tips og triks",
    "text": "Tips og triks\nI denne delen av kapitlet vil vi gi deg noen tips og triks som kan v√¶re nyttige n√•r du jobber med ssb-project.\n\nPoetry\nssb-project bruker Poetry for √• h√•ndtere virtuelle milj√∏er. Poetry er et verkt√∏y som gj√∏r det enkelt √• installere pakker og h√•ndtere versjoner av disse. Det er ogs√• Poetry som h√•ndterer Jupyter-kernelen for deg.\nHvis du etterlyser funksjonalitet i et ssb-project s√• kan det v√¶re nyttig √• lese dokumentasjonen til Poetry for √• se om det er mulig √• f√• til det du √∏nsker. Les ogs√• v√•r egne artikkel Poetry og SSB-project - Pakkeh√•ndtering i Python.\n\n\nFull disk p√• Dapla\nDet ‚Äúlokale‚Äù filsystemet p√• Dapla har kun 10GB diskplass. Har du mange virtuelle milj√∏er p√• denne disken kan det fort bli fullt, siden alle pakker blir installert her. Vanligvis er det 2 grunner til at disken blir full:\n\nFor mange virtuelle milj√∏er (ssb-projects) lagret lokalt.\nDette vil ofte kunne l√∏ses ved √• slette virtuelle milj√∏er som ikke lenger er i bruk. Hvis du har 5 virtuelle milj√∏er som hver bruker 1GB, og du kun jobber p√• en av de n√•, s√• vil du frigj√∏re 40% av disken ved √• slette 4 av dem. Husk at det permanente lagringsstedet for kode er p√• GitHub, og du kan alltid klone ned et prosjekt senere og bygge det hvis det trengs.\n/home/jovyan/.cache/ har blitt for stort.\nDette er en mappe som brukes av applikasjoner til √• lagre midlertidig data slik at de kan kj√∏re raskere. Denne kan bli ganske stor etter hvert. Ofte kan man frigj√∏re flere GB ved √• slette denne. Du sletter denne mappen ved √• skrive f√∏lgende i en terminal:\n\n\n\nterminal\n\nrm -rf /home/jovyan/.cache/\n\nHvis du opplever at disken er full, s√• kan det anbefales √• unders√∏ke hvilke mapper som tar st√∏rst plass med f√∏lgende kommando i terminalen:\n\n\nterminal\n\ncd ~ && du -h --max-depth=5 | sort -rh | head -n 10 && cd -\n\nKommandoen over sjekker, fra hjemmemappen din, hvilke mapper og undermapper som tar mest plass. Den viser de 10 st√∏rste mappene. Hvis du √∏nsker √• se flere mapper s√• kan du endre tallet etter head -n. Hvis du √∏nsker √• se alle mapper s√• kan du fjerne head -n. --max-depth=5 betyr at den kun sjekker mapper som er 5 mapper dype fra hjemmemappen din.\nN√•r du har gjort det kan selv vurdere hvilke som kan slettes for √• frigj√∏re plass.",
    "crumbs": [
      "Manual",
      "Kode",
      "SSB-project"
    ]
  },
  {
    "objectID": "statistikkere/ssb-project.html#footnotes",
    "href": "statistikkere/ssb-project.html#footnotes",
    "title": "SSB-project",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nFiler og mapper som starter med punktum er skjulte med mindre man ber om √• se dem. I Jupyterlab kan disse vises i filutforskeren ved √• velge View fra menylinjen, og deretter velge Show hidden files. I en terminal skriver man ls -a for √• se de.‚Ü©Ô∏é\nDette kan ogs√• gj√∏res ved √• h√∏yreklikke p√• mappen i Jupyterlab sin filutforsker og velge Delete.‚Ü©Ô∏é",
    "crumbs": [
      "Manual",
      "Kode",
      "SSB-project"
    ]
  },
  {
    "objectID": "statistikkere/filinnsamling-moveit.html",
    "href": "statistikkere/filinnsamling-moveit.html",
    "title": "Filinnsamling via MoveIT",
    "section": "",
    "text": "Her finner du fremgangsm√•ten beskrevet for hvordan ditt team kan f√• p√• plass automatisert overf√∏ring av filer samlet inn via Moveit p√• bakken til teamets kildedatab√∏tte p√• Dapla.\nFremgangsm√•ten beskrevet her inneb√¶rer en begrensning p√• at overflytting av filer til Dapla ikke kan utf√∏res oftere enn en gang i timen. For de aller fleste er ikke dette en utfordring. Dersom dette er en utfordring for ditt team, ta kontakt p√• arkitektur@ssb.no.\nTransfer Service er en tjeneste som brukes til √• flytte filer mellom bakke og sky. N√•r du skal ta i bruk tjenesten for √• overf√∏re data fra bakken til en kildedata-b√∏tte i Dapla-teamet ditt, s√• f√∏lger du denne beskrivelsen p√• hvordan man setter opp overf√∏ringsjobber. F√∏r du kan kan ta i bruk Transfer Service, m√• filene samlet inn via Moveit f√∏rst flyttes til filomr√•det p√• ‚Äúkilde‚Äù som Transfer Service benytter (/ssb/cloud_sync/‚Äùdaplateamnavn‚Äù/tilsky).\nDette m√• settes opp av s782, og henvendelsen m√• g√• via Kundeservice. Henvendelsen m√• inneholde:\nN√•r flyttingen av filene fra Moveit til Cloud_sync-omr√•det er etablert, s√• m√• du sette opp tjenesten for √• overf√∏re filene til en kildedata-b√∏tte i Dapla-teamet ditt. Da f√∏lger du denne beskrivelsen p√• hvordan man setter opp slike overf√∏ringsjobber.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Filinnsamling via MoveIT"
    ]
  },
  {
    "objectID": "statistikkere/filinnsamling-moveit.html#sette-opp-en-helt-ny-innsamling-i-moveit",
    "href": "statistikkere/filinnsamling-moveit.html#sette-opp-en-helt-ny-innsamling-i-moveit",
    "title": "Filinnsamling via MoveIT",
    "section": "Sette opp en helt ny innsamling i Moveit",
    "text": "Sette opp en helt ny innsamling i Moveit\nDersom det er en ny innsamling i Moveit, s√• m√• det ogs√• opprettes en filsluse-konto for den eksterne leverand√∏ren av datafilene. Dette m√• settes opp av s782, og henvendelsen m√• g√• via Kundeservice (Kundeservice@ssb.no) med f√∏lgende informasjon:\n\nEn kort beskrivelse av form√•let med innsamlingen og hva slags data vil det gjelde for.\n\n\nPersonlig informasjon p√• ekstern leverand√∏r:\n\nFullt navn\nE-postadresse\nTelefonnummer\nFirmanavn\nInformasjon om datafilene\n\n\n\nInformasjon for automatisk overf√∏ring av datafilene via MoveIT Automation:\n\nDaplateamets navn (teamet som datafilene skal overf√∏res til p√• Dapla)\n√ònsket frekvens/evt tidspunkt for overf√∏ringen\nEventuelle spesifikke krav eller preferanser for overf√∏ringen\n\nN√•r b√•de kontoen til den eksterne leverand√∏ren er p√• plass, og flyttingen av filene fra Moveit til Cloud_sync-omr√•det er etablert, s√• m√• du sette opp tjenesten for √• overf√∏re filene til en kildedata-b√∏tte i Dapla-teamet ditt. Da f√∏lger du denne beskrivelsen p√• hvordan man setter opp slike overf√∏ringsjobber.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Filinnsamling via MoveIT"
    ]
  },
  {
    "objectID": "statistikkere/renv.html",
    "href": "statistikkere/renv.html",
    "title": "Pakkeh√•ndtering i R - renv",
    "section": "",
    "text": "R-pakken, renv, er et verkt√∏y som lar oss opprette et milj√∏ for R og installere pakker. Det er anbefalt √• bruke renv for √• sikre at alle som jobber med prosjektet har samme versjon av pakkene. I tillegg er det enkelt √• dele prosjektet med andre.\nOppretting av renv-milj√∏ og installering av pakker m√• gj√∏res fra terminal for √• f√• riktig oppsett. For √• starte R i terminalen i Jupyter/DAPLA:\n\n√Öpne en terminal fra Launcher\nSt√• i mappen der du vil aktivere det virtuelle milj√∏et/installere pakker, dvs prosjekt mappen.\nStarte R ved √• skrive in R\n\n\n\n\n\n\n\nStarte renv i et eksisterende prosjekt\nFor √• installere dine egne R-pakker m√• du opprette et renv-milj√∏ med renv. Dette kan gj√∏res ved √• starte R i terminalen (se over) og skrive:\n\n\nterminal\n\nrenv::init()\n\n\n\n\n\n\nKommandoen aktiverer et renv-milj√∏ i mappen du st√•r i. Rent praktisk vil det si at du fikk f√∏lgende filer/mapper i mappen din:\n\nrenv.lock\nEn fil som inneholder versjoner av alle pakker du benytter i koden din.\n.Rprofile En fil som inneholder informasjon om oppsetting av milj√∏.\nrenv\nMappe som inneholder alle pakkene du installerer.\nrenv/activate.R En fil som aktiverer renv milj√∏et for et prosjekt.\n\n\n\nKobler en .R eller notebook fil til et renv-milj√∏\nFor √• ta i bruk et renv-milj√∏ m√• det aktiveres ved starten av koden som skal kj√∏res. I JupyterLab n√•r du √•pner en .R-fil som en notebook m√• renv-milj√∏et aktiveres ved:\n\n\nnotebook\n\nrenv::autoload()\n\nDeretter kan du benytte pakker som er installerte (se neste avsnitt) og funksjoner ved library() osv. Funkjsonen renv::autoload() vil lete etter filene i prosjektet for √• aktivere milj√∏et, b√•de i prosjekt-mappen og i foreldre-mapper.\n\n\n\n\n\n\n\nInstallering av pakker\nPakker kan installeres fra R p√• terminalen eller i en notebook/.R fil. Vi installerer pakker med funksjonen renv::install(). For eksempel, for √• installere pakken PxWebApiData:\n\n\nnotebook\n\nrenv::install(\"PxWebApiData\")\n\n\n\n\n\n\nFor √• installere R-pakker som ligger p√• ‚Äòstatisticsnorway‚Äô omr√•det p√• github m√• det spesifiseres foran pakkenavnet:\n\n\nnotebook\n\nrenv::install(\"statisticsnorway/ssb-klassr\")\n\n\n\n\n\n\nFor √• installere en spesifikk versjon av en pakke kan du spesifisere dette med installering med @ og versjonsnummer. For eksempel √• installere PxWEbApiData versjon 0.4.0:\n\n\nterminal\n\nrenv::install(\"PxWebApiData@0.4.0\")\n\nFor √• lagre versjonsnummer av de nye pakkene som er installerte til renv.lock filen, kj√∏r:\n\n\nnotebook\n\n`renv::snapshot()`\n\n\n\nDele prosjektet og renv-milj√∏et med andre\nF√∏r du deler prosjektet forsikre det om at renv.lock-filen er oppdatert. Dette kan gj√∏res ved √• kj√∏re:\n\n\nnotebook\n\nrenv::snapshot()\n\nFor at en pakke skal lagres i renv.lock m√• pakken benyttes ved library() et sted i prosjektet (p√• en .R eller .ipynb fil).\nFor √• dele renv-milj√∏et som en del av prosjektet skal f√∏lgende filene v√¶re p√• github: renv.lock, .Rprofile og renv/activate.R\n\n\nTa i bruk et prosjekt og renv-milj√∏ fra andre\nHvis prosjektet er opprettet av noen andre, og har blitt delt med deg, kan alle pakkene i prosjektet installeres samtidig. Clone repository og start deretter R i terminalen. Kj√∏r f√∏lgende for √• installere alle n√∏dvendige pakker:\n\n\nterminal\n\nrenv:restore()\n\n\n\n\n\n\n\n\nAvinstallering\nIndividuelle pakker kan fjernes fra renv-milj√∏et ved renv::remove()-funksjonen. For eksempel:\n\n\nterminal\n\nrenv::remove(\"PxWebApiData\")\n\nFor √• fjerne fra renv.lock-filen ogs√• m√• du ta en snapshot() etterp√•.\n\n\nterminal\n\nrenv::snapshot()\n\nEn annen nyttig funksjon er renv::clean(). Dette fjerner alle pakker fra library som ikke er i bruk\n\n\nterminal\n\nrenv::clean()\n\nIgjen m√• du ta en snapshot() for at endringer skal lagres p√• renv.lock-filen\n\n\nOppgradere pakker\nFor √• oppgradere en pakke kan du bruke renv::update(). For eksempel, for √• oppgradere PxWebApiData skriv:\n\n\nterminal\n\nrenv::update(\"PxWebApiData\")\n\nHusk √• ta en snapshot() etterp√• for √• lagre endringer til renv.lock-filen. Det betyr at du og andre kan gjenskape milj√∏et p√• nytt.\n\n\nterminal\n\nrenv::snapshot()\n\n\n\nOppgradering av R\nI Jupyterlab p√• Dapla og i produksjonssone vil versjonen av R oppgraderes jevnlig. Dette er fordi operativsystemet og programmer som R er avhengig skal holdes oppdatert. For at R skal fungere optimalt m√• det oppgraderes ofte. Dette skaper noen utfordringer for renv-milj√∏er som er avhengig av en spesifikk versjon av R.\nHvis du plutselig f√•r en feilmelding ved oppstart eller n√•r du kj√∏re renv::autoload(), om at R-versjonen er forskjellig fra det den som e r oppgitt lock-filen, har det trolig v√¶rt en oppgradering av R siden sist noen jobbet med koden. F√∏lg denne oppskriften for √• l√∏se opp i problemene:\nBenytt prosjekt-biblioteket ved √• kj√∏re:\n\nStart R i terminal.\nOppgrader versjon av renv ved:\n\n\n\nterminal\n\nrenv::upgrade()\n\n\nOppgrader alle pakkene ved:\n\n\n\nterminal\n\nrenv::hydrate(update = \"all\")\n\n\nLagre renv.lock filen:\n\n\n\nterminal\n\nrenv::snapshot()\n\n\n\n\n\n\nFor mer informasjon kan du lese denne artikkelen om oppdatering av renv her\n\n\nTa det lang tid til √• starte et renv-milj√∏?\nVed √• aktivere et renv-milj√∏ ved renv::autoload() s√∏ker R i alle programfiler for √• sjekke at alle pakker som benyttes er installerte. Hvis du har en veldig stor repository kan dette ta en lang tid. En l√∏sning er √• opprette en .renvignore fil som spesifisere hvilke filer skal ignoreres. For eksempel om alle R kode ligger i .R filer kan .renvignore inneholder *.ipynb (ignorere all notebook filer).\n\n\n\n\n\n\n\nFlytting mellom jupyter p√• bakken og Dapla\nPakker installeres fra ulike sted n√•r vi jobber p√• Dapla vs jupyter p√• bakken. For √• bruke et renv-milj√∏ i en repo som var laget p√• bakken, p√• Dapla, m√• vi endre adressen hvor pakkene skal installeres fra i renv.lock. Addressen skal se slik ut:\n\n\nrenv.lock\n\n\"Repositories\": [\n      {\n        \"Name\": \"CRAN\",\n        \"URL\": \"https://packagemanager.posit.co/cran/latest\"\n      }\n    ]\n\n\n\nPakke installering med renv p√• bakken\nProsessen med √• installere pakker for R p√• bakken er det samme som p√• Dapla. Noen pakker (for eksempel devtools) kan forel√∏pig ikke installeres p√• bakken p√• egenh√•nd pga 3. parti avhengigheter. Vi jobber med √• finne en l√∏sning til dette.\nFor √• installere arrow, kopier og kj√∏r f√∏lgende kommando i R:\n\n\nterminal\n\ninstall.packages(\"/ssb/bruker/felles/R_pakker/arrow_11.0.0.2.tar.gz\",\n                repos = NULL,\n                type = \"source\")",
    "crumbs": [
      "Manual",
      "Kode",
      "Pakkeh√•ndtering i R - renv"
    ]
  },
  {
    "objectID": "statistikkere/dapla-pseudo.html",
    "href": "statistikkere/dapla-pseudo.html",
    "title": "dapla-toolbelt-pseudo",
    "section": "",
    "text": "dapla-toolbelt-pseudo er en python-pakke som har som sitt hovedform√•l √• gi Dapla-brukere muligheten til √• pseudonymisere, de-pseudonymisere og re-pseudonymisere data. Det skal sikre at brukerne av Dapla har verkt√∏yene de trenger for √• jobbe med direkte identifiserende opplysninger i henhold til lovverk og SSBs tolkninger av disse.\nSiden tilgang til direkte identifiserende opplysninger er underlagt strenge regler, s√• krever bruken av dapla-pseudo-toolbelt at man forholder seg til vedtatte standarder som datatilstander og systemer som Kildomaten. I tillegg er det en streng tilgangsstyring til hvor man kan kalle funksjonaliteten fra. Tjenestene er satt opp p√• en slik m√•te at Dapla-team skal v√¶re selvbetjent i bruken av funksjonaliteten, samtidig som regler, prosesser og standarder etterleves p√• enklest mulig m√•te.\n\n\n\n\n\n\nStandardisert klassifisering av datatilstander\n\n\n\nI SSB er det bestemt at all data skal klassifiseres p√• en standardisert m√•te basert p√• datatilstander for √• avgj√∏re om de er sensitive, skjermet eller √•pen. Den eneste datatilstanden som klassifiseres som sensitiv er kildedata. Det er derfor bestemt at pseudonymisering er en av prosesseringene som skal skje mellom datatilstandene kildedata og inndata.\n\n\n\n\nF√∏r man tar i bruk funksjonaliteten er det viktig at man kjenner godt til tilgangstyring i Dapla-team og Kildomaten, og har diskutert med seksjonen hvordan man skal behandle direkte identifiserende opplysninger i de aktuelle dataene.\nFor at et Dapla-team skal kunne bruke dapla-toolbelt-pseudo m√• Kildomaten v√¶re skrudd p√• for milj√∏et1 man √∏nsker √• jobbe fra. Som standard f√•r alle statistikkteam skrudd p√• Kildomaten i prod-milj√∏et og ikke i test-milj√∏et. √ònsker du √• aktivere Kildomaten i test-milj√∏et kan dette gj√∏res selvbetjent som en feature.\n\n\n\nTilgang til √• funksjonalitet i dapla-toolbelt-pseudo kan regnes som sensitivt i seg selv, og derfor er det en streng tilgangsstyring for bruk av tjenesten. I prod-milj√∏et kan man kun ta i bruk funksjonaliteten ved √• prosessere dataene i Kildomaten, og det er bare tilgangsgruppen data-admins som har tilgang til √• godkjenne slike automatiske prosesseringer. I test-milj√∏et derimot kan alle p√• teamet benytte seg av all funksjonalitet, siden det aldri skal forekomme ekte data her.\n\n\n\nTabell¬†1: Tilgangsstyring til dapla-pseudo-toolbelt\n\n\n\n\n\n(a) Test-milj√∏\n\n\n\n\n\n\n\n\n\n\n\n\nAkt√∏r\nValidator\nPseudonymize()\nDepseudonymize()\nRepseudonymize()\n\n\n\n\nKildomaten\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\ndata-admins (interaktivt)\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\ndevelopers (interaktivt)\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n\n\n\n\n\n\n\n(b) Prod-milj√∏\n\n\n\n\n\n\n\n\n\n\n\n\nAkt√∏r\nValidator\nPseudonymize()\nDepseudonymize()\nRepseudonymize()\n\n\n\n\nKildomaten\n‚úÖ\n‚úÖ\nüö´\nüö´\n\n\ndata-admins (interaktivt)\nüö´\nüö´\nüö´\nüö´\n\n\ndevelopers (interaktivt)\nüö´\nüö´\nüö´\nüö´\n\n\n\n\n\n\n\n\n\nI Tabell¬†1 ser vi fra Tabell¬†1 (a) at man i test-milj√∏et har full tilgang til funksjonaliteten i dapla-toolbelt-pseudo, b√•de fra Kildomaten og n√•r man jobber interaktivt2 i Jupyterlab. Tabell¬†1 (b) viser at det kun er tilgang til pseudonymize() og validator() fra Kildomaten i prod-milj√∏et, og man kan aldri interaktivt kan kalle p√• funksjoner som potensielt avsl√∏rer et pseudonym. Av den grunn er det alltid anbefalt √• teste ut koden sin i test-milj√∏et f√∏r den produksjonssettes i i prod-milj√∏et med Kildomaten.\n\n\n\n\n\n\nUlike pseudonymer i prod og test\n\n\n\nSelv om man har videre rettigheter til √• bruke funksjonaliteten i dapla-toolbelt-pseudo fra test-milj√∏et sammenlignet med prod-milj√∏et, s√• betyr ikke det at samme input i de to milj√∏ene vil samme output. N√•r funksjonaliteten kalles fra test-milj√∏et s√• benyttes det automatisk en annen krypteringsn√∏kkel enn den som benyttes i prod. P√• den m√•ten vil et pseudonym produsert fra prod-milj√∏et aldri v√¶re likt det som produseres fra prod-milj√∏et selv om input skulle v√¶re den samme.\n\n\n\n\n\nI denne delen viser vi hvilken funksjonalitet som tilbys gjennom dapla-toolbelt-pseudo. Eksempelkoden under viser hvordan man ville kj√∏rt det fra en notebook i test-milj√∏et til teamet, og ikke hvordan koden m√• skrives n√•r det skal kj√∏res i Kildomaten3.\n\n\ndapla-toolbelt-pseudo er ferdig installert i Kildomaten. Men √∏nsker du √• bruke den i test-milj√∏et til teamet s√• kan du installere det i et ssb-project fra PyPI med denne kommandoen:\n\n\nTerminal\n\npoetry add dapla-toolbelt-pseudo\n\n\n\n\ndapla-toolbelt-pseudo st√∏tter innlesing av f√∏lgende dataformater:\n\nCSV\nJSON\nPandas dataframes\nPolars dataframes\n\nEksemplene under viser hovedskelig innlesing av dataframes fra minnet4, men du kan lese mer om filbasert prosessering lenger ned i kapitlet.\n\n\n\nPseudonymisering tilbys via Pseudonymize-metoden i dapla-toolbelt-pseudo. Den f√∏lger et builder-pattern der man spesifiserer hva og i hvilken rekkef√∏lge operasjonene skal gj√∏res. Anta at det finnes en Polars dataframe i minnet som heter df hvor kolonnen fnr skal pseudonymiseres. Da vil koden se slik ut:\n\n\nNotebook\n\nfrom dapla_pseudo import Pseudonymize\n\nresult_df = (\n    Pseudonymize.from_polars(df) \n    .on_fields(\"fnr\")\n    .with_default_encryption()                     \n    .run()                                \n    .to_polars()                                   \n)\n\nI koden over s√• angir from_polars(df) at kolonnen vi √∏nsker √• pseudonymisere ligger i en Polars dataframe i minnet og heter df. Deretter spesifiserer vi at kolonnen fnr er den som skal behandles med funksjonen on_fields(\"fnr\"). Videre angir with_default_encryption() at fnr skal pseudonymiseres med dapla-toolbelt-speudo sin standard-algoritme5. Til slutt ber vi om at det ovennevnte blir kj√∏rt med funksjonen run(), og at dataene skal returneres som en Polars dataframe med funksjonen to_polars().\nSe flere eksempler i dokumentasjonen.\n\n\n\nDe-pseudonymisering tilbys via Depseudonymize-metoden i dapla-toolbelt-pseudo. Den f√∏lger et builder-pattern der man spesifiserer hva og i hvilken rekkef√∏lge operasjonene skal gj√∏res. Anta at det finnes i en Polars dataframe i minnet som heter df hvor kolonnen fnr skal de-pseudonymiseres. Da vil koden se slik ut:\n\n\nNotebook\n\nresult_df = (\n    Depseudonymize.from_polars(df)            \n    .on_fields(\"fnr\")                          \n    .with_default_encryption()                     \n    .run()                                         \n    .to_polars()                                   \n)\n\nOppbygningen av koden med Depseudonymize() er helt lik som for Pseudonomize(). Les beskrivelsen der for √• se hva de ulike funksjonskallene gj√∏r. Se flere eksempler i dokumentasjonen.\nDe-pseudonymisering er ogs√• st√∏ttet for informasjon som f√∏rst er transformert til stabil ID og deretter pseudonymisert med Papis-n√∏kkelen. I disse tilfellene kan det ogs√• v√¶re behov for √• spesifisere hvilken versjon av snr-katalogen man √∏nsker √• benytte for √• erstatte snr med f√∏dselsnummer:\n\n\nNotebook\n\nfrom dapla_pseudo import Depseudonymize\n\nresult_df = (\n    Depseudonymize.from_pandas(df)            \n    .on_fields(\"fnr\")                          \n    .with_stable_id(\n      sid_snapshot_date=\"2023-05-29\")                    \n    .run()                                         \n    .to_pandas()                                   \n)\n\nI eksempelet over spesifiserer vi datoen 2023-05-29 og da benyttes snr-katalogen som ligger n√¶rmest i tid til denne datoen. Hvis sid_snapshot_date ikke oppgis benyttes siste tilgjengelige versjon av katalogen.\n\n\n\n\n\n\nDe-pseudonymisering ikke tilgjengelig i prod-milj√∏\n\n\n\nForel√∏pig er det kun tilgang til √• pseudonymisere i test-milj√∏et med test-data. Ta kontakt med Dapla-teamene dersom det dukker opp behov for √• kunne de-pseudonymisere i prod-milj√∏et.\n\n\n\n\n\nUnder utvikling.\n\n\n\nI statistikkproduksjon og forskning er det viktig √• kunne f√∏lge de samme personene over tid. Derfor har f√∏dselsnummer ofte blitt oversatt til en mer stabilt identifikator, ofte kalt SNR eller stabil ID6. Funksjonene with_stable_id() og validator() bruker stabilID-katalogen til √• henholdsvis bytte ut f√∏dselsnummer med stabil ID, og for √• validere om f√∏dselsnummer er gyldige(se under).\nDu kan selv spesifisere hvilken versjon av SNR-katalogen du √∏nsker √• bruke. Det gj√∏r du ved √• oppgi en gyldighetsdato og s√• finner dapla-toolbelt-pseudo hvilken versjon av katalogen som ligger n√¶rmest i tid.\n\n\n\nDapla tilbyr samme pseudonym som Papis-prosjektet7. Denne kan benyttes p√• 2 m√•ter:\n\nPseudonymisere hvilken som helst informasjon med samme n√∏kkel som Papis.\nTransformere f√∏dselsnummer til snr-nummer og deretter pseudonymisere med samme n√∏kkel som Papis.\n\nPunkt 1 er nyttig for de som har pseudonymisert informasjon p√• bakken tidligere og vil ha samme pseudonym p√• Dapla8. Dette kan gjelde hvilken som helst informasjon, ogs√• direkte pseudonymisering av f√∏dselsnummer, uten at det er g√•tt via snr-nummer. Her er et eksempel p√• hvordan man pseudonymiserer p√• denne m√•ten:\n\n\nNotebook\n\nresult = (\n    Pseudonymize.from_pandas(df)\n    .on_fields(\"fornavn\")                      \n    .with_papis_compatible_encryption()         \n    .run()                               \n    .to_pandas()                                  \n)\n\nPunkt 2 over er nok den som benyttes mest i SSB siden den sikrer at pseudonymisert f√∏dselsnummer kan kobles med data som er pseudonymisert p√• bakken. Her er et eksempel p√• hvordan man pseudonymiserer snr med Papis-n√∏kkelen:\n\n\nNotebook\n\nresult = (\n    Pseudonymize.from_pandas(df)\n    .on_fields(\"fnr\")                      \n    .with_stable_id()         \n    .run()                               \n    .to_pandas()                                  \n)\n\n\n\n\n\n\n\nHva betyr det √• tilby samme pseudonym?\n\n\n\nAt Papis og Dapla tilbyr samme pseudonum betyr egentlig at vi bruker samme krypteringsalgoritme, og en felles krypteringsn√∏kkel. Krypteringsalgoritmen som benyttes er formatpreserverende (FPE) og biblioteket som brukes er Tink FPE Python. En begrensning med algorimen er at kun karakterer som finnes i et forh√•ndsdefinert karaktersett (tall, store og sm√• bokstaver fra a-z) blir vurdert. Andre karakterer f.¬†eks √¶√∏√• blir ikke kryptert. Papis-n√∏kkelen (som brukes f.¬†eks for fnr og snr) benytter en SKIP-strategi for karakterer som faller utenom, som betyr at algoritmen simpelthen ‚Äúhopper over‚Äù disse. FPE-algoritmen er ogs√• avhengig av st√∏rrelsen p√• det predefinterte karaktersettet for √• avgj√∏re minimumslengden p√• teksten som pseudonymiseres. For Papis-n√∏kkelen betyr det at teksten minst m√• v√¶re 4 karakterer lang. Kortere tekster blir ikke kryptert.\n\n\n\n\n\nValidator-metoden kan benyttes til √• sjekke om f√∏dselsnummer finnes i SNR-katalogen (se over), og returnerer de ugyldige f√∏dselsnummerne tilbake. Her kan man ogs√• spesifisere hvilken versjon av SNR-katalogen man √∏nsker √• bruke. Standard, hvis ingenting velges, er at siste tilgjengelige versjon velges. Under er et eksempel p√• hvordan man validerer f√∏dselsnummer for en gitt gyldighetsdato:\n\n\nNotebook\n\nfrom dapla_pseudo import Validator\n\nresult = (\n    Validator.from_polars(df)\n    .on_field(\"fnr\")\n    .validate_map_to_stable_id(\n        sid_snapshot_date=\"2023-08-29\"\n    )\n)\n# Vis hvilken versjon av SNR-katalogen som er benyttet\nresult.metadata\n# Vis f√∏dselsnummer som ikke fikk treff i SNR-katalogen som en Polars dataframe\nresult.to_polars()\n\n\n\n\nSelv om mange av eksemplene i dette kapitlet viser hvordan man bruker dapla-toolbelt-pseudo ved √• gi funksjonene et datasett fra minnet, s√• st√∏tter den ogs√• √• prosessere filer som er lagret p√• disk/b√∏tter. Dette kan v√¶re en fordel hvis dataene er for store for Kildomaten (&gt;32GB RAM) eller de har en dyp hierarkisk struktur (f.eks. json-filer).\ndapla-toolbelt-pseudo st√∏tter f√∏lgende filtyper:\n\ncsv\njson\n\nHer er et eksempel p√• hvordan man pseudonymiserer en fil som ligger lagret p√• disk:\n\n\nNotebook\n\nfrom dapla_pseudo import Pseudonymize\nfrom dapla import AuthClient\n\n\nfile_path = \"gs://bucket/pseudo-examples/andeby_personer.csv\"\n\noptions = {\n    \"dtypes\": {\"fnr\": pl.Utf8, \"fornavn\": pl.Utf8, \"etternavn\": pl.Utf8, \"kjonn\": pl.Categorical, \"fodselsdato\": pl.Utf8}\n}\n\nresult_df = (\n    Pseudonymize.from_file(file_path)\n    .on_fields(\"fnr\")\n    .with_default_encryption()\n    .run()\n    .to_polars(**options)\n)\n\nSe flere eksempler i dokumentasjonen.\n\n\nKommer snart.\n\n\n\n\nKommer snart.\n\n\n\nKommer snart.\n\n\n\nDet genereres to typer av metadata n√•r man pseudonymiserer:\n\nDatadoc\nMetadata\n\nDe to typene av metadata returneres til brukeren i to forskjellige objekter.\n\n\n\n\nDatadoc-metadata er p√• et format som er planlagt integrert i Datadoc9 p√• et senere tidspunkt. I koden til h√∏yre s√• printes metadataene fra et kall til Pseudonomize ved √• skrive print(result.datadoc). Da printer man objektet interaktivt i f.eks. Jupyterlab, noe som kun er mulig i test-milj√∏et. Skal man kj√∏re dette i Kildomaten s√• er det lettere √• skrive filen direkte til riktig json-format med to_file-funksjonen. Da f√•r f√•r filen endelsen __DOC p√• slutten av filnavnet, og man slipper √• tenke p√• om filen skrives med riktig formattering, osv..\n\n\n\n\n\nNotebook\n\nresult = (\n    Pseudonymize.from_polars(df)    \n    .on_fields(\"fnr\")           \n    .with_stable_id()\n    .run()                      \n)\n\n1print(result.datadoc)\n2result.to_file(\"gs://bucket/test.parquet\")\n\n\n1\n\nPrinter metadata i en Notebook.\n\n2\n\nSkriver metadata direkte til b√∏tte.\n\n\n\n\nN√•r man kj√∏rer pseudonymisering fra Kildomaten er det viktig √• tenke p√• at felter som er pseudonymisert ikke m√• endres uten at metadataene ogs√• endrer. Da kan man risikere at metadatene ikke lenger beskriver riktig informasjon.\nUnder ser man hvilken informasjon som genereres fra pseudonymiseringen.\n\n\nDatadoc\n\n{\n  \"document_version\": \"0.0.1\",\n  \"datadoc\": null,\n  \"pseudonymization\": {\n    \"document_version\": \"0.1.0\",\n    \"pseudo_dataset\": null,\n    \"pseudo_variables\": [\n      {\n        \"short_name\": \"fnr\",   \n        \"data_element_path\": \"fnr\",\n        \"data_element_pattern\": \"**\",\n        \"stable_identifier_type\": \"FREG_SNR\",\n        \"stable_identifier_version\": \"2023-08-31\",\n        \"encryption_algorithm\": \"TINK-FPE\",\n        \"encryption_key_reference\": \"papis-common-key-1\",\n        \"encryption_algorithm_parameters\": [\n          {\n            \"keyId\": \"papis-common-key-1\"\n          },\n          {\n            \"strategy\": \"SKIP\"\n          }\n        ],\n        \"source_variable\": null,\n        \"source_variable_datatype\": null\n      }\n    ]\n  }\n}\n\nAv metadatene kan vi se fra pseudo_variables at det bare var feltet fnr som ble pseudonymisert. Vi ser ogs√• av stable_identifier_type ser vi at fnr ble oversatt til snr, og at versjonen av SNR-katalogen var fra 2023-08-31. encryption_algorithm angir at det var det var den formatpreserverende algoritmen TINK-FPE som ble benyttet. keyID: \"papis-common-key-1\" angir hvilken n√∏kkel-id som ble benyttet. strategy: \"SKIP\" refererer til at den format-preserverende algoritmen skal ‚Äúhoppe over‚Äù ugyldige karakterer og la de v√¶re som de er.\nDenne informasjonen vil v√¶re sv√¶rt nyttig i SSB hvis man senere skal kunne de-pseudonymisere eller re-pseudonymisere data.\n\n\n\nDen andre typen av metadata kan hentes ut etter et kall til Pseudonymize med kommandoen result.metadata. Den returnerer en python dictionary. Den inneholder hovedsaklig logginformasjon og metrikker forel√∏pig. For de som pseudonymiserer med with_stable_id() kan output se slik ut:\n\n\nMetadata\n\n{\n    'fnr': {\n        'logs': [\n            'No SID-mapping found for fnr 999999*****',\n            'No SID-mapping found for fnr XX',\n            'No SID-mapping found for fnr X8b7k2*'\n        ],\n        'metrics': [\n            {'MAPPED_SID': 10},\n            {'MISSING_SID': 3}\n        ]\n    }\n}\n\nI Kildomaten kan det v√¶rt nyttig √• printe denne informasjonen til loggene. Av eksempelet over ser vi at verdier som er over 6 karakterer lange blir maskert.\n\n\n\n\n\nP√• grunn av den strenge tilgangsstyringen til dapla-pseudo-toolbelt og kildedata er det anbefalt √• utvikle kode for overgangen fra kildedata til inndata i test-milj√∏et til teamet. I denne delen viser vi hvordan denne arbeidsflyten kan se ut, fra testing til en automatisert produksjon med ekte data, med et helt konkret eksempel.\n\n\nFor √• kunne kj√∏re pseudonymiseringen interaktivt i f.eks. en notebook i Jupyter, s√• m√• man jobbe i test-milj√∏et til teamet.\n\n\n\nKommer snart.\n\n\n\nKommer snart.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-pseudo"
    ]
  },
  {
    "objectID": "statistikkere/dapla-pseudo.html#forberedelser",
    "href": "statistikkere/dapla-pseudo.html#forberedelser",
    "title": "dapla-toolbelt-pseudo",
    "section": "",
    "text": "F√∏r man tar i bruk funksjonaliteten er det viktig at man kjenner godt til tilgangstyring i Dapla-team og Kildomaten, og har diskutert med seksjonen hvordan man skal behandle direkte identifiserende opplysninger i de aktuelle dataene.\nFor at et Dapla-team skal kunne bruke dapla-toolbelt-pseudo m√• Kildomaten v√¶re skrudd p√• for milj√∏et1 man √∏nsker √• jobbe fra. Som standard f√•r alle statistikkteam skrudd p√• Kildomaten i prod-milj√∏et og ikke i test-milj√∏et. √ònsker du √• aktivere Kildomaten i test-milj√∏et kan dette gj√∏res selvbetjent som en feature.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-pseudo"
    ]
  },
  {
    "objectID": "statistikkere/dapla-pseudo.html#tilgangsstyring",
    "href": "statistikkere/dapla-pseudo.html#tilgangsstyring",
    "title": "dapla-toolbelt-pseudo",
    "section": "",
    "text": "Tilgang til √• funksjonalitet i dapla-toolbelt-pseudo kan regnes som sensitivt i seg selv, og derfor er det en streng tilgangsstyring for bruk av tjenesten. I prod-milj√∏et kan man kun ta i bruk funksjonaliteten ved √• prosessere dataene i Kildomaten, og det er bare tilgangsgruppen data-admins som har tilgang til √• godkjenne slike automatiske prosesseringer. I test-milj√∏et derimot kan alle p√• teamet benytte seg av all funksjonalitet, siden det aldri skal forekomme ekte data her.\n\n\n\nTabell¬†1: Tilgangsstyring til dapla-pseudo-toolbelt\n\n\n\n\n\n(a) Test-milj√∏\n\n\n\n\n\n\n\n\n\n\n\n\nAkt√∏r\nValidator\nPseudonymize()\nDepseudonymize()\nRepseudonymize()\n\n\n\n\nKildomaten\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\ndata-admins (interaktivt)\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\ndevelopers (interaktivt)\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n\n\n\n\n\n\n\n(b) Prod-milj√∏\n\n\n\n\n\n\n\n\n\n\n\n\nAkt√∏r\nValidator\nPseudonymize()\nDepseudonymize()\nRepseudonymize()\n\n\n\n\nKildomaten\n‚úÖ\n‚úÖ\nüö´\nüö´\n\n\ndata-admins (interaktivt)\nüö´\nüö´\nüö´\nüö´\n\n\ndevelopers (interaktivt)\nüö´\nüö´\nüö´\nüö´\n\n\n\n\n\n\n\n\n\nI Tabell¬†1 ser vi fra Tabell¬†1 (a) at man i test-milj√∏et har full tilgang til funksjonaliteten i dapla-toolbelt-pseudo, b√•de fra Kildomaten og n√•r man jobber interaktivt2 i Jupyterlab. Tabell¬†1 (b) viser at det kun er tilgang til pseudonymize() og validator() fra Kildomaten i prod-milj√∏et, og man kan aldri interaktivt kan kalle p√• funksjoner som potensielt avsl√∏rer et pseudonym. Av den grunn er det alltid anbefalt √• teste ut koden sin i test-milj√∏et f√∏r den produksjonssettes i i prod-milj√∏et med Kildomaten.\n\n\n\n\n\n\nUlike pseudonymer i prod og test\n\n\n\nSelv om man har videre rettigheter til √• bruke funksjonaliteten i dapla-toolbelt-pseudo fra test-milj√∏et sammenlignet med prod-milj√∏et, s√• betyr ikke det at samme input i de to milj√∏ene vil samme output. N√•r funksjonaliteten kalles fra test-milj√∏et s√• benyttes det automatisk en annen krypteringsn√∏kkel enn den som benyttes i prod. P√• den m√•ten vil et pseudonym produsert fra prod-milj√∏et aldri v√¶re likt det som produseres fra prod-milj√∏et selv om input skulle v√¶re den samme.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-pseudo"
    ]
  },
  {
    "objectID": "statistikkere/dapla-pseudo.html#funksjonalitet",
    "href": "statistikkere/dapla-pseudo.html#funksjonalitet",
    "title": "dapla-toolbelt-pseudo",
    "section": "",
    "text": "I denne delen viser vi hvilken funksjonalitet som tilbys gjennom dapla-toolbelt-pseudo. Eksempelkoden under viser hvordan man ville kj√∏rt det fra en notebook i test-milj√∏et til teamet, og ikke hvordan koden m√• skrives n√•r det skal kj√∏res i Kildomaten3.\n\n\ndapla-toolbelt-pseudo er ferdig installert i Kildomaten. Men √∏nsker du √• bruke den i test-milj√∏et til teamet s√• kan du installere det i et ssb-project fra PyPI med denne kommandoen:\n\n\nTerminal\n\npoetry add dapla-toolbelt-pseudo\n\n\n\n\ndapla-toolbelt-pseudo st√∏tter innlesing av f√∏lgende dataformater:\n\nCSV\nJSON\nPandas dataframes\nPolars dataframes\n\nEksemplene under viser hovedskelig innlesing av dataframes fra minnet4, men du kan lese mer om filbasert prosessering lenger ned i kapitlet.\n\n\n\nPseudonymisering tilbys via Pseudonymize-metoden i dapla-toolbelt-pseudo. Den f√∏lger et builder-pattern der man spesifiserer hva og i hvilken rekkef√∏lge operasjonene skal gj√∏res. Anta at det finnes en Polars dataframe i minnet som heter df hvor kolonnen fnr skal pseudonymiseres. Da vil koden se slik ut:\n\n\nNotebook\n\nfrom dapla_pseudo import Pseudonymize\n\nresult_df = (\n    Pseudonymize.from_polars(df) \n    .on_fields(\"fnr\")\n    .with_default_encryption()                     \n    .run()                                \n    .to_polars()                                   \n)\n\nI koden over s√• angir from_polars(df) at kolonnen vi √∏nsker √• pseudonymisere ligger i en Polars dataframe i minnet og heter df. Deretter spesifiserer vi at kolonnen fnr er den som skal behandles med funksjonen on_fields(\"fnr\"). Videre angir with_default_encryption() at fnr skal pseudonymiseres med dapla-toolbelt-speudo sin standard-algoritme5. Til slutt ber vi om at det ovennevnte blir kj√∏rt med funksjonen run(), og at dataene skal returneres som en Polars dataframe med funksjonen to_polars().\nSe flere eksempler i dokumentasjonen.\n\n\n\nDe-pseudonymisering tilbys via Depseudonymize-metoden i dapla-toolbelt-pseudo. Den f√∏lger et builder-pattern der man spesifiserer hva og i hvilken rekkef√∏lge operasjonene skal gj√∏res. Anta at det finnes i en Polars dataframe i minnet som heter df hvor kolonnen fnr skal de-pseudonymiseres. Da vil koden se slik ut:\n\n\nNotebook\n\nresult_df = (\n    Depseudonymize.from_polars(df)            \n    .on_fields(\"fnr\")                          \n    .with_default_encryption()                     \n    .run()                                         \n    .to_polars()                                   \n)\n\nOppbygningen av koden med Depseudonymize() er helt lik som for Pseudonomize(). Les beskrivelsen der for √• se hva de ulike funksjonskallene gj√∏r. Se flere eksempler i dokumentasjonen.\nDe-pseudonymisering er ogs√• st√∏ttet for informasjon som f√∏rst er transformert til stabil ID og deretter pseudonymisert med Papis-n√∏kkelen. I disse tilfellene kan det ogs√• v√¶re behov for √• spesifisere hvilken versjon av snr-katalogen man √∏nsker √• benytte for √• erstatte snr med f√∏dselsnummer:\n\n\nNotebook\n\nfrom dapla_pseudo import Depseudonymize\n\nresult_df = (\n    Depseudonymize.from_pandas(df)            \n    .on_fields(\"fnr\")                          \n    .with_stable_id(\n      sid_snapshot_date=\"2023-05-29\")                    \n    .run()                                         \n    .to_pandas()                                   \n)\n\nI eksempelet over spesifiserer vi datoen 2023-05-29 og da benyttes snr-katalogen som ligger n√¶rmest i tid til denne datoen. Hvis sid_snapshot_date ikke oppgis benyttes siste tilgjengelige versjon av katalogen.\n\n\n\n\n\n\nDe-pseudonymisering ikke tilgjengelig i prod-milj√∏\n\n\n\nForel√∏pig er det kun tilgang til √• pseudonymisere i test-milj√∏et med test-data. Ta kontakt med Dapla-teamene dersom det dukker opp behov for √• kunne de-pseudonymisere i prod-milj√∏et.\n\n\n\n\n\nUnder utvikling.\n\n\n\nI statistikkproduksjon og forskning er det viktig √• kunne f√∏lge de samme personene over tid. Derfor har f√∏dselsnummer ofte blitt oversatt til en mer stabilt identifikator, ofte kalt SNR eller stabil ID6. Funksjonene with_stable_id() og validator() bruker stabilID-katalogen til √• henholdsvis bytte ut f√∏dselsnummer med stabil ID, og for √• validere om f√∏dselsnummer er gyldige(se under).\nDu kan selv spesifisere hvilken versjon av SNR-katalogen du √∏nsker √• bruke. Det gj√∏r du ved √• oppgi en gyldighetsdato og s√• finner dapla-toolbelt-pseudo hvilken versjon av katalogen som ligger n√¶rmest i tid.\n\n\n\nDapla tilbyr samme pseudonym som Papis-prosjektet7. Denne kan benyttes p√• 2 m√•ter:\n\nPseudonymisere hvilken som helst informasjon med samme n√∏kkel som Papis.\nTransformere f√∏dselsnummer til snr-nummer og deretter pseudonymisere med samme n√∏kkel som Papis.\n\nPunkt 1 er nyttig for de som har pseudonymisert informasjon p√• bakken tidligere og vil ha samme pseudonym p√• Dapla8. Dette kan gjelde hvilken som helst informasjon, ogs√• direkte pseudonymisering av f√∏dselsnummer, uten at det er g√•tt via snr-nummer. Her er et eksempel p√• hvordan man pseudonymiserer p√• denne m√•ten:\n\n\nNotebook\n\nresult = (\n    Pseudonymize.from_pandas(df)\n    .on_fields(\"fornavn\")                      \n    .with_papis_compatible_encryption()         \n    .run()                               \n    .to_pandas()                                  \n)\n\nPunkt 2 over er nok den som benyttes mest i SSB siden den sikrer at pseudonymisert f√∏dselsnummer kan kobles med data som er pseudonymisert p√• bakken. Her er et eksempel p√• hvordan man pseudonymiserer snr med Papis-n√∏kkelen:\n\n\nNotebook\n\nresult = (\n    Pseudonymize.from_pandas(df)\n    .on_fields(\"fnr\")                      \n    .with_stable_id()         \n    .run()                               \n    .to_pandas()                                  \n)\n\n\n\n\n\n\n\nHva betyr det √• tilby samme pseudonym?\n\n\n\nAt Papis og Dapla tilbyr samme pseudonum betyr egentlig at vi bruker samme krypteringsalgoritme, og en felles krypteringsn√∏kkel. Krypteringsalgoritmen som benyttes er formatpreserverende (FPE) og biblioteket som brukes er Tink FPE Python. En begrensning med algorimen er at kun karakterer som finnes i et forh√•ndsdefinert karaktersett (tall, store og sm√• bokstaver fra a-z) blir vurdert. Andre karakterer f.¬†eks √¶√∏√• blir ikke kryptert. Papis-n√∏kkelen (som brukes f.¬†eks for fnr og snr) benytter en SKIP-strategi for karakterer som faller utenom, som betyr at algoritmen simpelthen ‚Äúhopper over‚Äù disse. FPE-algoritmen er ogs√• avhengig av st√∏rrelsen p√• det predefinterte karaktersettet for √• avgj√∏re minimumslengden p√• teksten som pseudonymiseres. For Papis-n√∏kkelen betyr det at teksten minst m√• v√¶re 4 karakterer lang. Kortere tekster blir ikke kryptert.\n\n\n\n\n\nValidator-metoden kan benyttes til √• sjekke om f√∏dselsnummer finnes i SNR-katalogen (se over), og returnerer de ugyldige f√∏dselsnummerne tilbake. Her kan man ogs√• spesifisere hvilken versjon av SNR-katalogen man √∏nsker √• bruke. Standard, hvis ingenting velges, er at siste tilgjengelige versjon velges. Under er et eksempel p√• hvordan man validerer f√∏dselsnummer for en gitt gyldighetsdato:\n\n\nNotebook\n\nfrom dapla_pseudo import Validator\n\nresult = (\n    Validator.from_polars(df)\n    .on_field(\"fnr\")\n    .validate_map_to_stable_id(\n        sid_snapshot_date=\"2023-08-29\"\n    )\n)\n# Vis hvilken versjon av SNR-katalogen som er benyttet\nresult.metadata\n# Vis f√∏dselsnummer som ikke fikk treff i SNR-katalogen som en Polars dataframe\nresult.to_polars()\n\n\n\n\nSelv om mange av eksemplene i dette kapitlet viser hvordan man bruker dapla-toolbelt-pseudo ved √• gi funksjonene et datasett fra minnet, s√• st√∏tter den ogs√• √• prosessere filer som er lagret p√• disk/b√∏tter. Dette kan v√¶re en fordel hvis dataene er for store for Kildomaten (&gt;32GB RAM) eller de har en dyp hierarkisk struktur (f.eks. json-filer).\ndapla-toolbelt-pseudo st√∏tter f√∏lgende filtyper:\n\ncsv\njson\n\nHer er et eksempel p√• hvordan man pseudonymiserer en fil som ligger lagret p√• disk:\n\n\nNotebook\n\nfrom dapla_pseudo import Pseudonymize\nfrom dapla import AuthClient\n\n\nfile_path = \"gs://bucket/pseudo-examples/andeby_personer.csv\"\n\noptions = {\n    \"dtypes\": {\"fnr\": pl.Utf8, \"fornavn\": pl.Utf8, \"etternavn\": pl.Utf8, \"kjonn\": pl.Categorical, \"fodselsdato\": pl.Utf8}\n}\n\nresult_df = (\n    Pseudonymize.from_file(file_path)\n    .on_fields(\"fnr\")\n    .with_default_encryption()\n    .run()\n    .to_polars(**options)\n)\n\nSe flere eksempler i dokumentasjonen.\n\n\nKommer snart.\n\n\n\n\nKommer snart.\n\n\n\nKommer snart.\n\n\n\nDet genereres to typer av metadata n√•r man pseudonymiserer:\n\nDatadoc\nMetadata\n\nDe to typene av metadata returneres til brukeren i to forskjellige objekter.\n\n\n\n\nDatadoc-metadata er p√• et format som er planlagt integrert i Datadoc9 p√• et senere tidspunkt. I koden til h√∏yre s√• printes metadataene fra et kall til Pseudonomize ved √• skrive print(result.datadoc). Da printer man objektet interaktivt i f.eks. Jupyterlab, noe som kun er mulig i test-milj√∏et. Skal man kj√∏re dette i Kildomaten s√• er det lettere √• skrive filen direkte til riktig json-format med to_file-funksjonen. Da f√•r f√•r filen endelsen __DOC p√• slutten av filnavnet, og man slipper √• tenke p√• om filen skrives med riktig formattering, osv..\n\n\n\n\n\nNotebook\n\nresult = (\n    Pseudonymize.from_polars(df)    \n    .on_fields(\"fnr\")           \n    .with_stable_id()\n    .run()                      \n)\n\n1print(result.datadoc)\n2result.to_file(\"gs://bucket/test.parquet\")\n\n\n1\n\nPrinter metadata i en Notebook.\n\n2\n\nSkriver metadata direkte til b√∏tte.\n\n\n\n\nN√•r man kj√∏rer pseudonymisering fra Kildomaten er det viktig √• tenke p√• at felter som er pseudonymisert ikke m√• endres uten at metadataene ogs√• endrer. Da kan man risikere at metadatene ikke lenger beskriver riktig informasjon.\nUnder ser man hvilken informasjon som genereres fra pseudonymiseringen.\n\n\nDatadoc\n\n{\n  \"document_version\": \"0.0.1\",\n  \"datadoc\": null,\n  \"pseudonymization\": {\n    \"document_version\": \"0.1.0\",\n    \"pseudo_dataset\": null,\n    \"pseudo_variables\": [\n      {\n        \"short_name\": \"fnr\",   \n        \"data_element_path\": \"fnr\",\n        \"data_element_pattern\": \"**\",\n        \"stable_identifier_type\": \"FREG_SNR\",\n        \"stable_identifier_version\": \"2023-08-31\",\n        \"encryption_algorithm\": \"TINK-FPE\",\n        \"encryption_key_reference\": \"papis-common-key-1\",\n        \"encryption_algorithm_parameters\": [\n          {\n            \"keyId\": \"papis-common-key-1\"\n          },\n          {\n            \"strategy\": \"SKIP\"\n          }\n        ],\n        \"source_variable\": null,\n        \"source_variable_datatype\": null\n      }\n    ]\n  }\n}\n\nAv metadatene kan vi se fra pseudo_variables at det bare var feltet fnr som ble pseudonymisert. Vi ser ogs√• av stable_identifier_type ser vi at fnr ble oversatt til snr, og at versjonen av SNR-katalogen var fra 2023-08-31. encryption_algorithm angir at det var det var den formatpreserverende algoritmen TINK-FPE som ble benyttet. keyID: \"papis-common-key-1\" angir hvilken n√∏kkel-id som ble benyttet. strategy: \"SKIP\" refererer til at den format-preserverende algoritmen skal ‚Äúhoppe over‚Äù ugyldige karakterer og la de v√¶re som de er.\nDenne informasjonen vil v√¶re sv√¶rt nyttig i SSB hvis man senere skal kunne de-pseudonymisere eller re-pseudonymisere data.\n\n\n\nDen andre typen av metadata kan hentes ut etter et kall til Pseudonymize med kommandoen result.metadata. Den returnerer en python dictionary. Den inneholder hovedsaklig logginformasjon og metrikker forel√∏pig. For de som pseudonymiserer med with_stable_id() kan output se slik ut:\n\n\nMetadata\n\n{\n    'fnr': {\n        'logs': [\n            'No SID-mapping found for fnr 999999*****',\n            'No SID-mapping found for fnr XX',\n            'No SID-mapping found for fnr X8b7k2*'\n        ],\n        'metrics': [\n            {'MAPPED_SID': 10},\n            {'MISSING_SID': 3}\n        ]\n    }\n}\n\nI Kildomaten kan det v√¶rt nyttig √• printe denne informasjonen til loggene. Av eksempelet over ser vi at verdier som er over 6 karakterer lange blir maskert.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-pseudo"
    ]
  },
  {
    "objectID": "statistikkere/dapla-pseudo.html#brukerveiledning",
    "href": "statistikkere/dapla-pseudo.html#brukerveiledning",
    "title": "dapla-toolbelt-pseudo",
    "section": "",
    "text": "P√• grunn av den strenge tilgangsstyringen til dapla-pseudo-toolbelt og kildedata er det anbefalt √• utvikle kode for overgangen fra kildedata til inndata i test-milj√∏et til teamet. I denne delen viser vi hvordan denne arbeidsflyten kan se ut, fra testing til en automatisert produksjon med ekte data, med et helt konkret eksempel.\n\n\nFor √• kunne kj√∏re pseudonymiseringen interaktivt i f.eks. en notebook i Jupyter, s√• m√• man jobbe i test-milj√∏et til teamet.\n\n\n\nKommer snart.\n\n\n\nKommer snart.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-pseudo"
    ]
  },
  {
    "objectID": "statistikkere/dapla-pseudo.html#footnotes",
    "href": "statistikkere/dapla-pseudo.html#footnotes",
    "title": "dapla-toolbelt-pseudo",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nEt Dapla-team har b√•de et test- og et prod-milj√∏. Kildomaten m√• v√¶re skrudd p√• i det milj√∏et du √∏nkser √• benytte dapla-toolbelt-pseudo fra.‚Ü©Ô∏é\nMed interaktiv jobbing menes at man skriver og kode og f√•r tilbake output i samme verkt√∏y. F.eks. er Jupyterlab et eksempel p√• et verkt√∏y som lar deg jobbe interaktivt med data.‚Ü©Ô∏é\nI Kildomaten m√• koden blant annet pakke inn i main()-funksjon.‚Ü©Ô∏é\nPandas og Polars dataframes er eksempler p√• dataformater som lever i minnet, og m√• konverteres f√∏r de skrives til et lagringsommr√•de. I praksis vil det ofte si at man jobber med dataframes n√•r man jobber i verkt√∏y som Jupyterlab, mens man skriver til lagringsomr√•de n√•r man er ferdig i Jupyterlab.‚Ü©Ô∏é\nStandardalgoritmen i dapla-toolbelt-pseudo er den deterministiske krypteringsalgoritmen Deterministic Authenticated Encryption with Associated Data, eller DAEAD-algoritmen.‚Ü©Ô∏é\nSNR-katalogen eies og tilbys av Team Register p√• Dapla.‚Ü©Ô∏é\nPapis var et prosjekt med fokus p√• bakkesystemene i SSB som skulle bringe SSBs behandling av personopplysninger, som brukes i statistikkproduksjon, i samsvar med GDPR gjennom en enhetlig pseudonymiseringsl√∏sning.‚Ü©Ô∏é\nGenerelt sett er det ikke √• anbefale √• benytte denne n√∏kkelen p√• annen informasjon enn f√∏dselsnummer. Grunnen er at den er svakere enn andre algoritmer, der blant annet tekst som er kortere enn 4 karakter lang ikke blir pseudonymisert.‚Ü©Ô∏é\nDatadoc er det nye systemet for dokumentasjon av datasett i SSB. Systemet er fortsatt under utvikling.‚Ü©Ô∏é",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-pseudo"
    ]
  },
  {
    "objectID": "statistikkere/altinn-dapla-suv-tools.html",
    "href": "statistikkere/altinn-dapla-suv-tools.html",
    "title": "dapla-suv-tools",
    "section": "",
    "text": "dapla-suv-tools er en python pakke med en samling verkt√∏y for integrering med SUV-plattformen. Pakken tilbyr verkt√∏y for skjema administrasjon, bygging av prefill og utsending av skjema p√• Altinn 3 plattformen.\nDokumentasjon av pakken ligger her. Denne gir en teknisk innf√∏ring som du kan f√∏lge og kopiere kode fra. Noe demokode ligger ogs√• i repoet og kan v√¶re ett godt utgangspunkt."
  },
  {
    "objectID": "statistikkere/altinn-dapla-suv-tools.html#installasjon",
    "href": "statistikkere/altinn-dapla-suv-tools.html#installasjon",
    "title": "dapla-suv-tools",
    "section": "Installasjon",
    "text": "Installasjon\n\nPip\n\n\nTerminal\n\npip install dapla-suv-tools\n\n\n\nPoetry\n\n\nTerminal\n\npoetry add dapla-suv-tools"
  },
  {
    "objectID": "statistikkere/altinn-dapla-suv-tools.html#opprette-klient",
    "href": "statistikkere/altinn-dapla-suv-tools.html#opprette-klient",
    "title": "dapla-suv-tools",
    "section": "Opprette klient",
    "text": "Opprette klient\nFor √• kunne bruke pakken m√• du importere klienten:\n\n\nnotebook\n\nfrom dapla_suv_tools.suv_client import SuvClient\n\nclient = SuvClient()"
  },
  {
    "objectID": "statistikkere/altinn-dapla-suv-tools.html#paginering",
    "href": "statistikkere/altinn-dapla-suv-tools.html#paginering",
    "title": "dapla-suv-tools",
    "section": "Paginering",
    "text": "Paginering\nFor √• sette opp paginering m√• du importere PaginationInfo\n\n\nnotebook\n\nfrom dapla_suv_tools.pagination import PaginationInfo\n\nPaginering brukes for √• hente data i mindre deler spesielt n√•r et datasett er stort. Dette bidrar til √• redusere belastningen b√•de p√• klient og server, og gir bedre ytelse. Side- og st√∏rrelsesparametere sendes som en del av foresp√∏rselen.\n\n\n\n\n\n\nMaksimal st√∏rrelse\n\n\n\nMaksimal tillatt st√∏rrelse per side er 100 i alle foresp√∏rsler. Hvis man angir en h√∏yere verdi vil foresp√∏rselen feile eller bli begrenset til 100 poster per side.\n\n\n\nEnkel bruk av paginering\nI dette eksempelet brukes paginering for √• hente en spesifikk side med et gitt antall elementer.\n\n\nnotebook\n\np_info = PaginationInfo(page=1, size=5)\n\nresult = client.get_skjema_by_ra_nummer(\n    ra_nummer=\"RA-0666A3\", pagination_info=p_info\n)\n\nHer hentes den f√∏rste siden (page=1) med 5 elementer per side (size=5).\n\n\nHente alle data med paginering\nDette eksempelet viser hvordan man kan hente alle data ved √• iterere gjennom flere sider.\n\n\nnotebook\n\npage = 1\nsize = 100\nall_records = []\n\nwhile True:\n    p_info = PaginationInfo(page=page, size=size)\n    \n    response = client.get_utvalg_from_sfu(\n        delreg_nr=49430324,\n        ra_nummer='RA-0666A3',\n        pagination_info=p_info\n    )\n    \n    records = response\n    all_records.extend(records)\n    \n    if len(records) &lt; size:\n        break\n    \n    page += 1\n\nprint(f\"Totalt antall poster: {len(all_records)}\")\n)\n\nDette sikrer at alle poster hentes n√•r datasettet g√•r over flere sider."
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html",
    "href": "statistikkere/hva-er-dapla-team.html",
    "title": "Hva er Dapla-team?",
    "section": "",
    "text": "For √• kunne jobbe p√• Dapla m√• man v√¶re en del av et Dapla-team. Et Dapla-team er en gruppe personer som har tilgang til spesifikke ressurser p√• Dapla. Ressursene kan v√¶re data, kode eller tjenester. F√∏lgelig er teamet helt sentral for tilgangsstyringen p√• Dapla. Derfor er det viktig at alle som jobber p√• Dapla gj√∏r seg godt kjent med innholdet i denne delen.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#opprette-dapla-team",
    "href": "statistikkere/hva-er-dapla-team.html#opprette-dapla-team",
    "title": "Hva er Dapla-team?",
    "section": "Opprette Dapla-team",
    "text": "Opprette Dapla-team\nAlle Dapla-team tilh√∏rer en seksjon og opprettes av seksjonslederen i den seksjonen. Dapla-team opprettes i applikasjonen Dapla-Ctrl.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#autonomitetsniv√•",
    "href": "statistikkere/hva-er-dapla-team.html#autonomitetsniv√•",
    "title": "Hva er Dapla-team?",
    "section": "Autonomitetsniv√•",
    "text": "Autonomitetsniv√•\nEt team p√• Dapla er i en av f√∏lgende autonomitetsniv√•er:\n\nManaged\nSemi-Managed\nSelf-Managed\n\nNiv√•et er definert i metadataene til teamet og vises i Teamvisningen i Dapla Ctrl. Det er kun plattformteamene som kan endre niv√•et til et team.\nForm√•let med √• definere autonomiten til et team er √• tydeliggj√∏re hvem som har hvilket ansvar for de produktene teamet benytter p√• Dapla. Niv√•et settes n√•r teamet opprettes, men kan ogs√• endres senere ved behov.\nEt Managed team benytter seg kun av tjenestene/features som tilbys av plattformen, og kan v√¶re sikker p√• at disse er satt opp i iht de krav som gjelder i SSB. Typisk vil de fleste statistikkteam v√¶re managed, og eksempler p√• tjenester er standard lagringsb√∏tter, Transfer Service, Kildomaten, osv.. Et Managed team kan kun benytte seg av tilgangsgruppene managers, data-admins og developers.\nEt Semi-Managed team benytter seg av tjenestene som tilbys p√• plattformen, akkurat som et Managed team, men de √∏nsker ogs√• ta i bruk noe funksjonalitet som ikke tilbys enda. F.eks. kan det v√¶re et statistikkproduserende team som √∏nsker √• ta i bruk en Google-tjeneste som ikke tilbys p√• Dapla. I disse tilfellene kan teamet velge √• ta et st√∏rre ansvar for denne tjenesten og f√• noe bredere tilganger p√• plattformen. Ansvaret fordrer at teamet har kompetansen til √• ta dette ansvaret, og de spesifikke detaljene vil avhenge hvilken tjeneste det er snakk om, og om de √∏nsker √• benytte tjenesten i prod- eller test-milj√∏et til teamet.\nEt Self-Managed team vil typisk v√¶re team som best√•r IT-utviklere med god kompetanse p√• skyutvikling i Google Cloud Platform. Teamet har kompetanse til √• ta i bruk de tjenestene de mener er best for √• l√∏se sine oppgaver.\nI resten av dette kapitlet beskrives hovedsakelig Managed teams.\n\n\n\n\n\n\nAutonomitetsniv√• og tilganger i IaC-repo\n\n\n\n\n\nSiden hvert team f√•r definert sine ressurser i et eget IaC-repo, s√• er det n√¶r sammenheng mellom Autonomitetsniv√• og hvilke tilganger teamet har til √• gj√∏re endringer i IaC-repoet. Tabell¬†1 viser hvilke tilganger de ulike niv√•ene gir.\n\n\n\nTabell¬†1: Autonomitet og tilganger i IaC-repo\n\n\n\n\n\n\n\n\n\n\n\n\nManaged\nSemi-Managed\nSelf-Managed\n\n\n\n\nKan lage PR p√• IaC-kode\nJa\nJa\nJa\n\n\nTilgang p√• IaC-kode\nKun yaml-‚Äúst√∏ttefiler‚Äù (team-info, iam, buckets-shared, projects) og filer de endrer (dapla-filer)\nJa\nJa\n\n\nKan ta i bruk funksjonalitet utover dapla-features\nNei\nJa\nJa\n\n\nIaC-filstruktur (under infra-mappen)\nPredefinert\nPredefinert + egne filer\nFritt\n\n\n\n\n\n\nLes mer her.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#roller-i-teamet",
    "href": "statistikkere/hva-er-dapla-team.html#roller-i-teamet",
    "title": "Hva er Dapla-team?",
    "section": "Roller i teamet",
    "text": "Roller i teamet\nMedlemskap i et Dapla-team gir tilgang p√• spesifikke ressurser p√• Dapla. Men siden kildedataene til alle team er klassifisert som sensitive, s√• kan ikke alle p√• teamet ha lik tilgang til alle ressurser. Av den grunn er det definert 3 ulike roller p√• et team. To av disse, data-admins og developers, er forbeholdt de som jobber med data p√• teamet. Mens den tredje, managers, skal innehas av de som er ansvarlige for teamet. I de fleste tilfeller vil managers v√¶re seksjonslederen som er ansvarlige for statistikkproduktene teamet leverer. Under forklarer vi n√¶rmere hva de ulike rollene inneb√¶rer.\n\nManagers\nRollen managers skal best√• av en eller flere data-ansvarlige (ofte omtalt som data-eiere eller seksjonsledere). managers har ansvar for f√∏lgende i teamet:\n\nhvem i teamet som f√•r tilgang til hvilke data og tjenester.\nat teamet f√∏lger SSBs retningslinjer for tilgangsstyring.\nat teamet f√∏lger SSBs retningslinjer for klassifisering av data.\nvedlikehold og monitorering av tilganger.\nat teamet f√∏lger og forst√•r hvordan sensitive data skal behandles i SSB.\n\nManager-rollen krever ingen tilgang til data eller databehandlende tjenester p√• Dapla.\n\n\nDevelopers\nRollen developers er den mest vanlige rollen p√• et Dapla-team. Denne rollen skal tildeles alle som jobber med data i teamet. developers har tilgang til f√∏lgende ressurser:\n\nalt av teamets data, med unntak av kildedata.\nalle ressurser som behandler datatilstandene fra inndata til utdata.\n\n\n\nData-admins\nRollen data-admins er en priveligert rolle blant de som jobber med data i teamet. Rollen skal kun tildeles 2-3 personer p√• et team og disse er da medlem av b√•de developers- og data-admins-gruppen. De som er medlem av data-admins-gruppen kan gj√∏re f√∏lgende:\n\nde er forh√•ndsgodkjent til √• gi seg selv tidsbegrenset tilgang til kildedata ved behov. Tilgang til kildedata skal kun aktiveres i s√¶rskilte tilfeller, der eneste l√∏sning er √• se p√• kildedata i klartekst. Tilgang skal kun aktiveres i en begrenset periode, og krever en skriftlig begrunnelse. managers skal lett kunne monitere hvem som aktiverer denne tilgangen og hvor ofte.\nde kan godkjenne endringer i automatiske jobber som prosesserer kildedata til inndata.\nde kan overf√∏re kildedata mellom bakke og sky.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#ressurser",
    "href": "statistikkere/hva-er-dapla-team.html#ressurser",
    "title": "Hva er Dapla-team?",
    "section": "Ressurser",
    "text": "Ressurser\nN√•r du oppretter et dapla-team s√• f√•r man en grunnpakke med ressurser som de fleste i SSB vil trenge for √• kunne jobbe med data p√• Dapla. I tillegg kan teamet selvbetjent skru p√• andre tjenester hvis man √∏nsker det. I det f√∏lgende forklarer vi hva som er inkludert i grunnpakken, og hva som er tilgjengelig for √• skru p√• ved behov.\n\nGrunnpakken\nFigur¬†1 viser et overordnet bilde av hvilke ressurser som er inkludert i ‚Äúgrunnpakken‚Äù. Et Dapla-team f√•r et testmilj√∏ og prodmilj√∏. Det er i prodmilj√∏et at man jobber med skarpe data, mens testmilj√∏et er forbeholdt arbeid med testdata. I hvert milj√∏ f√•r teamet to Google-prosjekter. Ett for kildedata og et for datatilstandene inndata, klargjorte data, statistikkdata og utdata. Sistnevnte prosjekt kaller vi for standardprosjektet, siden det er her mesteparten av databehandlingen skjer.\n\n\n\n\n\n\nFigur¬†1: Diagram over hvilke milj√∏er, Google-prosjekter og b√∏tter et Dapla-team som et f√•r ved opprettelse.\n\n\n\nAv Figur¬†1 ser vi at prosjektene i prodmilj√∏et f√•r noen flere b√∏tter enn prosjektene i testmilj√∏et. Disse ekstrab√∏ttene er forbeholdt synkronisering av data mellom bakke og sky, noe vi ikke legger til rette for i testmilj√∏et1. Les mer om overf√∏ring av data mellom bakke og sky her.\nRessursene som opprettes for et Dapla-team reflekterer i stor grad at kildedata er klassifisert som sensitive. Dette er grunnen til at det opprettes et eget prosjekt for kildedata, og at det kun er data-admins som potensielt kan f√• tilgang til dataene her. Opprettelsen av et eget testmilj√∏ skyldes at Dapla-team i st√∏rre grad enn f√∏r forventes √• jobbe med testdata istedenfor skarpe data.\nAlle ressursene som opprettes for teamet er definert i tekstfiler i et GitHub-repo. Dette repoet kaller vi for et IaC-repo (Infrastructure as Code). IaC-repoet er en del av grunnpakken, og er tilgjengelig for alle p√• teamet. Statistikkere trenger ikke √• forholde seg til dette repoet i stor grad, med unntak av n√•r de skal aktivere/deaktivere features og n√•r de skal sette opp Kildomaten.\n\n\nFeatures\nI tillegg til grunnpakken med ressurser, s√• kan teamet selvbetjent skru p√• f√∏lgende features eller tjenester ved behov:\n\nTransfer Service kan skrus p√• hvis teamet trenger √• synkronisere data mellom ulike lagringssystemer. For eksempel mellom bakke og sky, eller mellom to ulike skytjenester.\nKildomaten kan skrus p√• hvis teamet trenger √• automatisere overgangen fra kildedata til inndata.\nShared-buckets kan skrus p√• hvis teamet trenger √• opprette delt-b√∏tter.\n\nForel√∏pig er det kun disse tre features som er tilgjengelig. Det vil komme flere etterhvert som behovene melder seg.\nLes mer om features her.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#github-team",
    "href": "statistikkere/hva-er-dapla-team.html#github-team",
    "title": "Hva er Dapla-team?",
    "section": "GitHub-team",
    "text": "GitHub-team\nVed opprettelsen av et Dapla-team s√• blir det ogs√• opprettet et tilsvarende GitHub-team med samme navn som Dapla-teamet. Grunnen til at det blir opprettet et GitHub-team er at GitHub er en sentral del av Dapla. Alle ressurser som skal opprettes p√• plattformen defineres av GitHub-repoer, og vi √∏nsker at tilganger her ogs√• skal reflektere tilgangene p√• Dapla.\nFor eksempel vil et team med navnet dapla-example f√• et GitHub-team med navnet dapla-example. Alle som er medlem av Dapla-teamet vil automatisk bli medlem av GitHub-teamet. I tillegg vil gruppetilh√∏righet og tilgangsroller p√• GitHub-teamet reflektere tilgangsroller p√• Dapla-teamet. For eksempel s√• kan dapla-example-data-admins gis tilgang til repo, og da vil alle som er medlem av Dapla-teamet med rollen data-admins f√• tilgang til repoet. Dette benyttes blant annet for √• gi teamet tilgang til automation-mappen i sitt IaC-repo. I tillegg kan teamet bruke GitHub-teamet til √• gi tilgang til andre GitHub-repoer som er relevante for teamet, for eksempel kodenbasen til en statistikkproduksjon eller lignende. Fordelen er at tilganger er gitt p√• teamniv√• og ikke p√• personniv√•. For eksempel hvis manager for teamet fjerner en ansatt fra developers-gruppa, s√• mister de all tilgang til data, tjenester og kode p√• GitHub som er tilgjengelig for developers.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#navnestruktur",
    "href": "statistikkere/hva-er-dapla-team.html#navnestruktur",
    "title": "Hva er Dapla-team?",
    "section": "Navnestruktur",
    "text": "Navnestruktur\nN√•r du oppretter et Dapla-team s√• m√• du velge et navn p√• teamet. Teamet velger selv et navn som reflekterer domene og subdomene. For eksempel kan et team som jobber med statistikkproduksjonen skattestatistikk for n√¶ringslivet velge √• kalle teamet Skatt n√¶ring. Hvis vi bruker dette teamet som et eksempel, s√• vil det f√• opprettet et teknisk navn som f√∏lger denne strukturen: skatt-naering. Dette navnet er det som brukes i tekniske sammenhenger, for eksempel som navn p√• GitHub-teamet, IaC-repoet, Google-prosjektene og b√∏ttene. Tabell¬†2 viser en tabell over hvordan ressursene for dette teamet vil se ut:\n\n\n\nTabell¬†2: Navnestruktur for teamet Skatt n√¶ring sine ressurser\n\n\n\n\n\n\n\n\n\nNavn\nBeskrivelse\n\n\n\n\nskatt-naering\nTeknisk teamnavn\n\n\nskatt-naering-managers\nAD-gruppe for managers\n\n\nskatt-naering-data-admins\nAD-gruppe for data-admins og et GitHub-team\n\n\nskatt-naering-developers\nAD-gruppe for developers og et GitHub-team\n\n\nskatt-naering-kilde-p\nNavn p√• kildeprosjekt i prod\n\n\nskatt-naering-p\nNavn p√• standardprosjekt i prod\n\n\nskatt-naering-kilde-t\nNavn p√• kildeprosjekt i test\n\n\nskatt-naering-t\nNavn p√• standardprosjekt i test\n\n\n\n\n\n\nI Tabell¬†2 ser vi at teamet f√•r opprettet 3 AD-grupper og 4 Google-prosjekter. AD-gruppene brukes til √• gi tilgang til ressursene p√• Dapla, mens Google-prosjektene brukes til √• organisere ressursene. I tillegg er det en fast navnestruktur for b√∏ttene i hvert prosjekt, slikt som vist i Tabell¬†3.\n\n\n\nTabell¬†3: Navnestruktur for teamet Skatt n√¶ring sine b√∏tter\n\n\n\n\n\nProsjektnavn\nB√∏ttenavn\n\n\n\n\nskatt-naering-kilde-p\nssb-skatt-naering-data-kilde-prod\n\n\n\nssb-skatt-naering-data-kilde-frasky-prod\n\n\n\nssb-skatt-naering-data-kilde-tilsky-prod\n\n\nskatt-naering-p\nssb-skatt-naering-data-produkt-prod\n\n\n\nssb-skatt-naering-data-frasky-prod\n\n\n\nssb-skatt-naering-data-tilsky-prod\n\n\nskatt-naering-kilde-t\nssb-skatt-naering-data-kilde-test\n\n\nskatt-naering-t\nssb-skatt-naering-data-produkt-test",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#dapla-felles",
    "href": "statistikkere/hva-er-dapla-team.html#dapla-felles",
    "title": "Hva er Dapla-team?",
    "section": "Dapla Felles",
    "text": "Dapla Felles\nAlle i SSB er med i developers-gruppa til team Dapla Felles. Form√•let med teamet er gj√∏re det lett som mulig for alle i SSB √• komme-i-gang med Dapla, samtidig som det er et egnet sted for √• dele √•pne data eller kursmateriell. Teamet har autonomitetsniv√• managed, og har de samme b√∏ttene som et vanlig statistikkproduserende team.\nAlle i SSB har lese- og skrivetilgang til produkt-b√∏tta til Dapla Felles, og derfor skal det aldri deles data som ikke alle i SSB kan benytte. I tillegg m√• alle forvente at data her kan slettes og overskrives med jevne mellomrom.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#footnotes",
    "href": "statistikkere/hva-er-dapla-team.html#footnotes",
    "title": "Hva er Dapla-team?",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nTa kontakt med produkteier for Dapla hvis du trenger √• synkronisere testdata mellom bakke og sky‚Ü©Ô∏é",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/arkivering.html",
    "href": "statistikkere/arkivering.html",
    "title": "Arkivering",
    "section": "",
    "text": "Alle som flytter produksjon til Dapla m√• fortsatt arkivere dataene i bakkemilj√∏et. Grunnen til dette er at det enda ikke er bestemt hvordan arkivering skal foreg√• p√• Dapla. Inntill videre m√• derfor statistikkteam arkivere i de gamle systemene.\n\n\nF√∏r man kan arkivere data m√• det skrives ut slik det er definert i Datadok. Mens man tidligere gjorde dette fra SAS, skal man p√• Dapla gj√∏re det fra R eller Python.\n\n\n\nEtter at filen er skrevet m√• den flyttes fra Dapla til bakkemilj√∏et, og til slutt inn i riktig arkiv-mappe. Overf√∏ring av filer mellom bakke og sky gj√∏res med Transfer Service. N√•r filen er flyttet til bakkemilj√∏et, m√• brukeren selv flytte filen til arkiv-mappen. √ònsker man √• automatisere flyttingen, s√• kan man sende en foresp√∏rsel til Kundeservice.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Arkivering"
    ]
  },
  {
    "objectID": "statistikkere/arkivering.html#skrive-fil",
    "href": "statistikkere/arkivering.html#skrive-fil",
    "title": "Arkivering",
    "section": "",
    "text": "F√∏r man kan arkivere data m√• det skrives ut slik det er definert i Datadok. Mens man tidligere gjorde dette fra SAS, skal man p√• Dapla gj√∏re det fra R eller Python.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Arkivering"
    ]
  },
  {
    "objectID": "statistikkere/arkivering.html#overf√∏re-fil",
    "href": "statistikkere/arkivering.html#overf√∏re-fil",
    "title": "Arkivering",
    "section": "",
    "text": "Etter at filen er skrevet m√• den flyttes fra Dapla til bakkemilj√∏et, og til slutt inn i riktig arkiv-mappe. Overf√∏ring av filer mellom bakke og sky gj√∏res med Transfer Service. N√•r filen er flyttet til bakkemilj√∏et, m√• brukeren selv flytte filen til arkiv-mappen. √ònsker man √• automatisere flyttingen, s√• kan man sende en foresp√∏rsel til Kundeservice.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Arkivering"
    ]
  },
  {
    "objectID": "statistikkere/administrasjon-av-team.html",
    "href": "statistikkere/administrasjon-av-team.html",
    "title": "Administrasjon av team",
    "section": "",
    "text": "I dette kapitlet viser vi hvordan du kan opprette et nytt team eller gj√∏re endringer i et eksisterende team. Typiske endringer er √•:\n\nLegge til, fjerne eller endre medlemmer i et team\nListe ut medlemmer og tilgangsgrupper i et team\n\n\n\nFor √• opprette et Dapla-team s√• m√• en seksjonsleder g√• inn i Teamoversikten i Dapla Ctrl og trykke p√• ikonet Opprett team. Her blir man bedt om √• fylle inn relevant informasjon.\n\n\n\n\n\n\nNote\n\n\n\nLes mer om Dapla Ctrl her.\n\n\n\n\n\nFor √• legge til, fjerne eller endre medlemmer i et team m√• kan gj√∏res av medlemmer i managers-gruppen i teamet. Dette gj√∏res i Dapla Ctrl. Les mer om hvordan dette gj√∏res her.\n\n\n\n\n\n\nManagers i semi- eller self-managed team\n\n\n\nManagers i semi- eller self-managed teams kan ikke legge til, fjerne eller endre medlemmer fra Dapla Ctrl enda. Disse m√• forel√∏pig kontakte Kundeservice for √• gj√∏re endringer.\n\n\n\n\n\nDapla Ctrl lar alle i SSB se hvilke team som finnes, hvem som er medlemmer og hvilke tilgangsgrupper de ligger i. Man kan ogs√• f√• oversikt over hvilke data alle team deler. Les mer her.",
    "crumbs": [
      "Manual",
      "Dapla-team",
      "Administrasjon av team"
    ]
  },
  {
    "objectID": "statistikkere/administrasjon-av-team.html#opprette-dapla-team",
    "href": "statistikkere/administrasjon-av-team.html#opprette-dapla-team",
    "title": "Administrasjon av team",
    "section": "",
    "text": "For √• opprette et Dapla-team s√• m√• en seksjonsleder g√• inn i Teamoversikten i Dapla Ctrl og trykke p√• ikonet Opprett team. Her blir man bedt om √• fylle inn relevant informasjon.\n\n\n\n\n\n\nNote\n\n\n\nLes mer om Dapla Ctrl her.",
    "crumbs": [
      "Manual",
      "Dapla-team",
      "Administrasjon av team"
    ]
  },
  {
    "objectID": "statistikkere/administrasjon-av-team.html#legge-til-eller-fjerne-medlemmer-i-et-team",
    "href": "statistikkere/administrasjon-av-team.html#legge-til-eller-fjerne-medlemmer-i-et-team",
    "title": "Administrasjon av team",
    "section": "",
    "text": "For √• legge til, fjerne eller endre medlemmer i et team m√• kan gj√∏res av medlemmer i managers-gruppen i teamet. Dette gj√∏res i Dapla Ctrl. Les mer om hvordan dette gj√∏res her.\n\n\n\n\n\n\nManagers i semi- eller self-managed team\n\n\n\nManagers i semi- eller self-managed teams kan ikke legge til, fjerne eller endre medlemmer fra Dapla Ctrl enda. Disse m√• forel√∏pig kontakte Kundeservice for √• gj√∏re endringer.",
    "crumbs": [
      "Manual",
      "Dapla-team",
      "Administrasjon av team"
    ]
  },
  {
    "objectID": "statistikkere/administrasjon-av-team.html#se-medlemmer-og-roller-i-et-team",
    "href": "statistikkere/administrasjon-av-team.html#se-medlemmer-og-roller-i-et-team",
    "title": "Administrasjon av team",
    "section": "",
    "text": "Dapla Ctrl lar alle i SSB se hvilke team som finnes, hvem som er medlemmer og hvilke tilgangsgrupper de ligger i. Man kan ogs√• f√• oversikt over hvilke data alle team deler. Les mer her.",
    "crumbs": [
      "Manual",
      "Dapla-team",
      "Administrasjon av team"
    ]
  },
  {
    "objectID": "statistikkere/poetry-ssb-project.html",
    "href": "statistikkere/poetry-ssb-project.html",
    "title": "Pakkeh√•ndtering i Python",
    "section": "",
    "text": "I tillegg til √• opprette GitHub repoer etter v√•r SSB-mal hjelper SSB-project deg med √• lage kernels og holde styr p√• Python-pakkene dine ved bruk av Poetry",
    "crumbs": [
      "Manual",
      "Kode",
      "Pakkeh√•ndtering i Python"
    ]
  },
  {
    "objectID": "statistikkere/poetry-ssb-project.html#installere-pakker",
    "href": "statistikkere/poetry-ssb-project.html#installere-pakker",
    "title": "Pakkeh√•ndtering i Python",
    "section": "Installere pakker",
    "text": "Installere pakker\n\n\n\n\n\n\nForsikre deg om at pakken er trygg!\n\n\n\n\n\nF√∏r du installerer en pakke b√∏r gj√∏re f√∏lgende for √• sikre deg at du ikke installerer en pakke med skadelig kode:\n\nS√∏k opp pakken p√• PyPI.\nSjekk om pakken er et popul√¶rt/velkjent prosjekt ved √• bes√∏ke repoet der koden ligger. Antall Stars og Forks p√• gitHub er en grei indikasjon p√• dette.\nHvis du er i tvil om pakken er trygg √• installere, s√• kan du sp√∏rre kollegaer om de har erfaring med den, eller sp√∏rre p√• en egnet Yammer-kanal i SSB.\nHvis du fortsatt √∏nsker √• installere pakken s√• anbefaler vi √• copy-paste navnet fra PyPi, ikke skrive det inn manuelt n√•r du installerer.\n\n\n\n\nN√•r du har opprettet et ssb-project kan du installere de python-pakkene du trenger fra PyPI.\nSelve installeringen av pakken gj√∏res enkelt p√• f√∏lgende m√•te:\n\n√Öpne en terminal i Jupyterlab.\nG√• inn i prosjektmappen din ved √• skrive:\n\n\n\nterminal\n\ncd &lt;sti til prosjektmappe&gt;\n\n\nLag en branch/utviklingsbranch som f.eks. heter install-pandas:\n\n\n\nterminal\n\ngit checkout -b install-pandas\n\n\nInstaller, f.eks. Pandas, ved √• skrive f√∏lgende:\n\n\n\nterminal\n\npoetry add pandas\n\n\n\n\n\n\n\nFigur¬†1: Installasjon av Pandas med ssb-project\n\n\n\nFigur¬†1 viser hvordan dette vil se ut i en Jupyterlab-terminal. Kommandoen for √• installere noe er poetry add etterfulgt av pakkenavnet. Vi ser ogs√• at den automatisk legger til Pandas-versjonen i filen poetry.lock.\nDu kan ogs√• spesifisere en konkret versjon av pakken som skal installeres med f√∏lgende kommando:\n\n\nterminal\n\npoetry add pandas@1.2.3\n\n\nAvinstallere pakker\nOfte eksperimenterer man med nye pakker, og alle blir ikke med videre i produksjonskoden. Det er god praksis √• fjerne pakker som ikke brukes, blant annet for √• unng√• at de blir en sikkerhetsrisiko. Det gj√∏r du enkelt ved √• skrive f√∏lgende i terminalen:\n\n\nterminal\n\npoetry remove pandas\n\n\n\nOppdatere pakker\nHvis det kommer en ny versjon av en pakke du bruker, s√• kan du oppdatere den med f√∏lgende kommando:\n\n\nterminal\n\npoetry update pandas\n\nhvis du kj√∏rer poetry update uten noe pakkenavn, s√• vil alle pakkene dine oppdateres til siste versjon, med mindre du har spesifisert versjonsbegrensninger i pyproject.toml-filen.\n\n\nUnders√∏k avhengigheter\nHvis du lurer p√• hvilke pakker som har hvilke avhengigheter, s√• kan du lett liste ut dette i terminalen med f√∏lgende kommando:\n\n\nterminal\n\npoetry show --tree\n\nDet vil gi en grafisk fremstilling av avhengighetene som vist i Figur¬†2.\n\n\n\n\n\n\nFigur¬†2: Visning av pakke-avhengigheter i ssb-project",
    "crumbs": [
      "Manual",
      "Kode",
      "Pakkeh√•ndtering i Python"
    ]
  },
  {
    "objectID": "statistikkere/poetry-ssb-project.html#push-til-github",
    "href": "statistikkere/poetry-ssb-project.html#push-til-github",
    "title": "Pakkeh√•ndtering i Python",
    "section": "Push til GitHub",
    "text": "Push til GitHub\nN√•r du n√• har installert en pakke s√• har filen poetry.lock endret seg. For at dine samarbeidspartnere skal f√• tilgang til denne endringen i et SSB-project, s√• m√• du pushe en ny versjon av poetry.lock-filen opp Github, og kollegaene m√• pulle ned og bygge prosjektet p√• nytt. Du kan gj√∏re dette p√• f√∏lgende m√•te etter at du har installert en ny pakke:\n\nVi kan stage alle endringer med f√∏lgende kommando i terminalen n√•r vi st√•r i prosjektmappen:\n\n\n\nterminal\n\ngit add -A\n\n\nDeretter commite en endring, dvs. ta et stillbilde av koden i dette √∏yeblikket, ved √• skrive f√∏lgende:\n\n\n\nterminal\n\ngit commit -m \"Installert pandas\"\n\n\nPush det opp til GitHub1. Anta at vi gjorde dette i branchen install-pandas som ble opprettet tidligere. Da kan vi skrive f√∏lgende :\n\n\n\nterminal\n\ngit push --set-upstream origin install-pandas\n\nDeretter kan kollegaene dine pulle ned endringene og bygge prosjektet p√• nytt. Vi forklarer hvordan man kan bygge prosjektet p√• nytt senere i kapitlet.\n\nHold prosjektet oppdatert\nHvis du sitter med en lokal kopi av prosjektet ditt, og flere andre jobber med den samme kodebasen, s√• er det viktig at du holder din lokale kopi oppdatert. Hvis du jobber i en branch p√• en lokal kopi, b√∏r du holde denne oppdatert med main-branchen p√• GitHub. Det er vanlig Git-praksis. N√•r man ogs√• bruker ssb-project, s√• man huske √• ogs√• bygge prosjektet p√• nytt hver gang det er endringer som er gjort av andre i poetry.lock.-filen.",
    "crumbs": [
      "Manual",
      "Kode",
      "Pakkeh√•ndtering i Python"
    ]
  },
  {
    "objectID": "statistikkere/poetry-ssb-project.html#dependabot",
    "href": "statistikkere/poetry-ssb-project.html#dependabot",
    "title": "Pakkeh√•ndtering i Python",
    "section": "Dependabot",
    "text": "Dependabot\nN√•r man installerer pakker s√• vil det etter hvert utvikle seg et sett av potensielt kompliserte avhengigheter mellom disse pakkene. Dette skyldes at en pakke kan benytte seg av funksjonalitet i andre pakker, som igjen benytter seg av funksjonalitet i andre pakker, osv.. Hvis noen finner en sikkerhetss√•rbarhet i en pakke s√• kan det fikses ved at en ny versjon av den pakken slippes, som igjen kan f√• konsekvenser for pakker som er avhengig av denne.\nI SSB er det tilrettelagt for at alle som versjonsh√•ndterer koden sin p√• GitHub kan skanne pakkene sine for s√•rbarheter og nye versjoner av pakker med Dependabot. Dependabot hjelper oss med √• finne og fikse s√•rbarheter og gamle pakkeversjoner. Dette er spesielt viktig n√•r man installerer sine egne pakker.\nDependabot sjekker med jevne mellomrom om det finnes oppdateringer i pakkene som er listet i din pyproject.toml-fil, og den tilh√∏rende poetry.lock. Hvis det finnes oppdateringer s√• vil den lage en pull request som du kan godkjenne. N√•r du godkjenner den s√• vil den oppdatere poetry.lock-filen og lage en ny commit som du kan pushe til GitHub. Dependabot gir ogs√• en sikkerhetsvarslinger hvis det finnes kjente s√•rbarheter i pakkene du bruker.\nDet er anbefalt at alle som installerer sine egne pakker i SSB skrur p√• Dependabot i sine GitHub-repoer.\n\nAktivere Dependabot\nDu kan aktivere Dependabot ved √• gi inn i GitHub-repoet ditt og gj√∏re f√∏lgende:\n\nG√• inn repoet\nTrykk p√• Settings for det repoet som vist p√• Figur¬†3.\n\n\n\n\n\n\n\nFigur¬†3: √Öpne Settings for et GitHub-repo.\n\n\n\n\nI menyen til venstre velger du Code security and analysis\nUnder seksjonen Dependabot velger Enable p√• minst Dependabot alerts og Dependabot security updates, slik som vist i Figur¬†4.\n\n\n\n\n\n\n\nFigur¬†4: Skru p√• Dependabot i GitHub.\n\n\n\nN√•r du har gjort dette vil GitHub varsle deg hvis det finnes en kjent s√•rbarhet i pakkene som benyttes.\n\n\nOppdatere pakker\nHvis en av pakkene du bruker kommer med en oppdatering, s√• vil Dependabot lage en pull request (PR) i GitHub som du kan godkjenne. Dependabot sjekker ogs√• om oppdateringen er i konflikt med andre pakker du bruker. Hvis det er tilfellet s√• vil den lage en pull request som oppdaterer alle pakkene som er i konflikt.\nHvis en av pakkene du bruker har en kjent sikkerhetss√•rbarhet, s√• vil Dependabot varsle deg om dette under Security-fanen i GitHub-repoet ditt. Hvis du trykker p√• View Dependabot alerts s√• vil du f√• en oversikt over alle s√•rbarhetene som er funnet, og hvilken alvorlighetsgrad den har. Hvis du trykker p√• en av s√•rbarhetene s√• vil du f√• mer informasjon om den, og du kan trykke p√• Create pull request for √• oppdatere pakken.\nSom nevnt over kan du enkelt oppdatere pakker fra GitHub ved hjelp av Dependabot. Men det finnes tilfeller der du vil teste om en oppdatering gj√∏r at deler av koden din ikke fungerer lenger. Anta at du bruker en Python-pakken Pandas i koden din, og at du f√•r en pull request fra Dependabot om √• oppdatere den fra versjon 1.5 til 2.0. Hvis du √∏nsker √• teste om koden din fortsatt fungerer med den nye versjonen av Pandas, s√• kan du gj√∏re dette i Jupyterlab ved √• f√∏lge ved √• lage en branch som f.eks. heter update-pandas. Deretter kan du installere den nye versjonen av Pandas med f√∏lgende kommando fra terminalen:\n\n\nterminal\n\npoetry update pandas@2.0\n\nHvis du n√• kj√∏rer koden din kan du teste om den fortsatt fungerer som forventet. Gj√∏r den ikke det kan du tilpasse koden din og pushe endringene til Github. Deretter kan du godkjenne pull requesten fra Dependabot og merge den. Etter dette kan du slette den PR-en som Dependabot lagde for deg.",
    "crumbs": [
      "Manual",
      "Kode",
      "Pakkeh√•ndtering i Python"
    ]
  },
  {
    "objectID": "statistikkere/poetry-ssb-project.html#footnotes",
    "href": "statistikkere/poetry-ssb-project.html#footnotes",
    "title": "Pakkeh√•ndtering i Python",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\n√Ö pushe til GitHub uten √• sende ved Personal Access Token fordrer at du har lagret det lokalt s√• Git kan finne det. Her et eksempel p√• hvordan det kan gj√∏res.‚Ü©Ô∏é",
    "crumbs": [
      "Manual",
      "Kode",
      "Pakkeh√•ndtering i Python"
    ]
  },
  {
    "objectID": "statistikkere/spark.html",
    "href": "statistikkere/spark.html",
    "title": "Apache Spark",
    "section": "",
    "text": "Koding i R og Python har historisk sett foreg√•tt p√• en enkelt maskin og v√¶rt begrenset av minnet (RAM) og prosessorkraften p√• maskinen. For bearbeiding av sm√• og mellomstore datasett er det sjelden et problem p√• kj√∏re p√• en enkelt maskin. Popul√¶re pakker som dplyr for R, og pandas for Python, blir ofte brukt i denne typen databehandling. I senere √•r har det ogs√• kommet pakker som er optimalisert for √• kj√∏re kode parallelt p√• flere kjerner p√• en enkelt maskin, skrevet i minne-effektive spr√•k som Rust og C++.\nMen selv om man kommer langt med √• kj√∏re kode p√• en enkelt maskin, vil enkelte oppgaver kreve mer minne og prosessorkraft enn dette. For st√∏rre datasett, eller store beregninger, kan det v√¶re nyttig √• bruke et rammeverk som kan kj√∏re kode parallelt p√• flere maskiner. Et slikt rammeverk er Apache Spark.\nApache Spark er et rammeverk for √• kj√∏re kode parallelt p√• flere maskiner. Det er bygget for √• h√•ndtere store datasett og store beregninger. Det er derfor et nyttig verkt√∏y for √• l√∏se problemer som er for store for √• kj√∏re p√• en enkelt maskin. Men det finnes ogs√• andre bruksomr√•der som er nyttige for Apache Spark. Her er noen eksempler:\nDette er noen av bruksomr√•dene der Spark kan l√∏se problemer som er for store for √• kj√∏re p√• en enkelt maskin med for eksempel Pandas eller dplyr.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Apache Spark"
    ]
  },
  {
    "objectID": "statistikkere/spark.html#spark-p√•-dapla",
    "href": "statistikkere/spark.html#spark-p√•-dapla",
    "title": "Apache Spark",
    "section": "Spark p√• Dapla",
    "text": "Spark p√• Dapla\nDapla kj√∏rer p√• et Kubernetes-kluster og er derfor er et sv√¶rt egnet sted for √• kj√∏re kode parallelt p√• flere maskiner. Jupyter p√• Dapla har ogs√• en flere klargjorte kernels for √• kj√∏re kode i Apache Spark. Denne koden vil kj√∏re p√• et eget kluster av maskiner som er dedikert til Apache Spark, slik som vist i Figur¬†1.\n\n\n\n\n\n\n\n\n\n\n\n(a) PySpark p√• kubernetes\n\n\n\n\n\n\n\n\n\n\n\n(b) PySpark p√• 1 maskin\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) SparkR p√• kubernetes\n\n\n\n\n\n\n\n\n\n\n\n(d) SparkR p√• 1 maskin\n\n\n\n\n\n\n\nFigur¬†1: Ferdigkonfigurerte kernels for Spark p√• Dapla.\n\n\n\nFigur¬†1 (a) og Figur¬†1 (c) kan velges hvis du √∏nsker √• bruke Spark for √• kj√∏re store jobber p√• flere maskiner, for henholdsvis Python- og R-grensesnittene for Spark.\nFigur¬†1 (b) og Figur¬†1 (d) b√∏r du velge hvis du √∏nsker √• bruke Spark av andre grunner enn √• kj√∏re store jobber p√• flere maskiner. For eksempel hvis du √∏nsker √• bruke en av de mange pakker som er bygget p√• Spark, eller hvis du √∏nsker √• bruke Spark til √• lese og skrive data fra Dapla.\nHvis du √∏nsker √• sette opp et eget virtuelt milj√∏ for √• kj√∏re Spark, s√• kan du bruke ssb-project. Se ssb-project for mer informasjon.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Apache Spark"
    ]
  },
  {
    "objectID": "statistikkere/spark.html#spark-i-og-python",
    "href": "statistikkere/spark.html#spark-i-og-python",
    "title": "Apache Spark",
    "section": "Spark i  og Python",
    "text": "Spark i  og Python\nSpark er implementert i programmeringsspr√•ket Scala. Men det tilbys ogs√• mange grensesnitt for √• bruke Spark fra andre spr√•k. De mest popul√¶re grensesnittene er PySpark for Python og SparkR for R. Disse grensesnittene er bygget p√• Spark, og gir tilgang til Spark-funksjonalitet fra Python og R.\n\nPySpark\nPySpark er et Python-grensesnitt for Apache Spark. Det er en Python-pakke som gir tilgang til Spark-funksjonalitet fra Python. Det er enkelt √• bruke, og har mange av de samme funksjonene som Pandas.\nUnder ser du datasettet som benyttes for i vedlagt notebook pyspark-intro.ipynb. Den viser hvordan man kan gj√∏re vanlige databehandling med PySpark. I eksempelet brukes kernel som vist i Figur¬†1 (b).\n\n\n\nCode\n# Legger til row index til DataFrame f√∏r join med dates_df\ndata_df = data_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n\n# Joiner de to datasettene\ndf = (\n    dates_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n    .join(data_df, \"row_index\")\n    .drop(\"row_index\")\n)\n\n# Legger til √•r, kvartal og mnd\ndf = df.withColumn(\"Year\", date_format(df.Date, \"yyyy\"))\ndf = df.withColumn(\"Quarter\", expr(\"quarter(Date)\"))\ndf = df.withColumn(\"Month\", date_format(df.Date, \"MM\"))\n\ndf.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows\n\n\n\nSource: Introduksjon til PySpark\nDet finnes ogs√• et Pandas API/grensesnitt mot Spark. M√•let med en er √• gj√∏re overgangen fra Pandas til Spark lettere for nybegynneren. Men hvis man skal gj√∏re litt mer avansert databehandling anbefales det at man bruker PySpark direkte og ikke Pandas API-et.\n\n\nSparkR\nSparkR er et R-grensesnitt for Apache Spark. Det er en R-pakke som gir tilgang til Spark-funksjonalitet fra R. Det er enkelt √• bruke, og har mange av de samme funksjonene som dplyr. Se eksempel i notebook under:\n\n\nselectedColumns &lt;- select(file, \"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\")\nshowDF(selectedColumns, numRows = 5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows\n\n\nSource: Introduksjon til SparkR",
    "crumbs": [
      "Manual",
      "Appendix",
      "Apache Spark"
    ]
  },
  {
    "objectID": "statistikkere/spark.html#lakehouse-arkitektur",
    "href": "statistikkere/spark.html#lakehouse-arkitektur",
    "title": "Apache Spark",
    "section": "Lakehouse-arkitektur",
    "text": "Lakehouse-arkitektur\n\n\n\n\n\n\nWarning\n\n\n\nI denne delen viser vi hvordan funksjonalitet som kan bli relevant for SSB √• benytte seg av i fremtiden. Men det er fortsatt under testing og ta det i betraktning f√∏r man eventuelt implementerer dette i produksjon.\n\n\nEn av utvidelsene som er laget rundt Apache Spark er den s√•kalte Lakehouse-arkitekturen. Kort fortalt kan den dekke behovene som et klassisk datavarehus har tatt seg av tidligere. I kontekst av SSB sine teknologier kan det ogs√• benyttes som et databaselag over Parquet-filer i b√∏tter. Det finnes flere open source l√∏sninger for dette, men mest aktuelle er:\n\nDelta Lake\nApache Hudi\nApache Iceberg\n\nI det f√∏lgende omtaler vi hovedsakelig egenskapene til Delta Lake, men alle rammeverkene har mange av de samme egenskapene. Delta Lake kan ogs√• benyttes p√• Dapla n√•.\nSentrale egenskaper ved Delta Lake er:\n\nACID-transactions som sikrer data-integritet og stabilitet, ogs√• n√•r det skjer feil.\nMetadata som bli h√•ndtert akkurat som all annen data og er veldig skalebar. Den st√∏tter ogs√• egendefinert metadata.\nSchema Enforcement and Evolution sikrer at skjemaet til dataene blir h√•ndhevet, og den tillater ogs√• den kan endres over tid.\nTime travel sikrer at alle versjoner av dataene er blitt lagret, og at du kan g√• tilbake til tidligere versjoner av en fil.\nAudit history sikrer at du kan f√• full oversikt over hvilke operasjoner som utf√∏rt p√• dataene.\nInserts, updates and deletes betyr at du lettere kan manipulere data enn hva som er tilfellet med vanlige Parquet-filer.\nIndexing er st√∏ttes for forbedre sp√∏rringer mot store datamengder.\n\nI vedlagt notebook deltalake-intro.ipynb finner du blant annet eksempler p√• hvordan du legger til f√∏lgende metadata i spesifikk versjon av en fil:\n\n\n\nCode\n# Lagrer egendefinert metadata i en json-fil\nimport json\n\nmetadata = {\n    \"comment\": \"Kontaktet oppgavegiver og kranglet!\",\n    \"manueltEditert\": \"True\",\n    \"maskineltEditert\": \"False\",\n}\nmetadata\n\n\n{'comment': 'Kontaktet oppgavegiver og kranglet!',\n 'manueltEditert': 'True',\n 'maskineltEditert': 'False'}\n\n\nSource: Introduksjon til Delta Lake",
    "crumbs": [
      "Manual",
      "Appendix",
      "Apache Spark"
    ]
  },
  {
    "objectID": "statistikkere/index.html",
    "href": "statistikkere/index.html",
    "title": "Hurtigstart",
    "section": "",
    "text": "Velkommen til hurtigstart-siden! Her presenterer vi manualen og setter deg i gang p√• 1-2-3!\nDenne manualen tar sikte p√• √• gi SSB-ansatte mulighet til √• ta i bruk Dapla uten hjelp fra eksperter. Manualen inneholder artikler om blant annet kodespr√•kene Python og R, Git, Dapla Lab, Metadata og mye mer. I tillegg har manualen en blogg og en nyhetsside.\nUnder hjelp-fanen √∏verst p√• nettsiden finner du blant annet siden FAQ hvor ofte stilte sp√∏rsm√•l besvares. I denne boken omtaler vi den gamle produksjonssonen, ofte kalt prodsonen, som bakke, og det nye skymilj√∏et Google Cloud som sky. Det er ikke helt presist men duger for form√•lene i denne boken.\n\nLes om‚Ä¶\n\nDapla Nyheter - Hva skjer p√• Dapla om dagen?\nVi har v√•r egen nyhetsside Dapla Nyheter. Bruk den for √• holde deg oppdatert p√• hva som skjer p√• Dapla!\n\n\nHva er Dapla Team?\nStatistikkproduksjon p√• Dapla foreg√•r med Dapla-team som midtpunktet. Vi anbefaler at alle begynner med √• lese artikkelen Hva er Dapla-team!\n\n\nHvor er dataene v√•re? I b√∏tter!\nDataene v√•re lagres p√• Google Cloud platform i det som kalles b√∏tter. B√∏ttene er basert p√• de obligatoriske datatilstandene. Les artikkelen Hva er b√∏tter? og artikkelen om datatilstander\n\n\nDapla lab - arbeidsbenken v√•r\nDapla lab er der vi finner verkt√∏y og tjenester som Jupyter, RStudio og Datadoc-editor. Les hovedartikkelen om Dapla-lab. Det finnes ogs√• artikler for hver tjeneste vi har i Dapla lab.\n\n\nGit, GitHub og malen v√•r SSB-project\nGit og GitHub brukes for √• lagre (og versjonsh√•ndtere) koden v√•r. SSB-project brukes som mal for GitHub-repoer og for √• h√•ndtere Pythonpakker. Her finner du artikkelen om Git og GitHub, SSB-project (oversikt) og SSB-project for pakkeh√•ndtering i Pyton.\n\n\n\n\n\n\nVi trenger bidragsytere!\n\n\n\nDapla er i konstant utvikling og det er manualen og! Derfor trenger vi flere bidragsytere til √• fjerne utdatert informasjon, forbedre eksisterende artikler og skrive nye.\nKunne du tenkt deg √• bidra? Les om hvordan du kan bidra i denne artikkelen i appendiksen. Har du lyst til √• bidra, men er ikke helt sikker p√• hva du kan bidra med? Ta en titt p√• issues i GitHub-repoet.\n\n\nKommentarer og √∏nsker vedr√∏rende boken tas imot med √•pne armer. Dette kan gj√∏res ved √• lage en issue i GitHub-repoet.\nGod forn√∏yelseüòÅ",
    "crumbs": [
      "Manual",
      "Hurtigstart"
    ]
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "Sp√∏rsm√•l og svar",
    "section": "",
    "text": "Prosjekt-ID-en til et Google-prosjekt er en unik identifikator som brukes til √• identifisere prosjektet i Google Cloud Platform. Prosjekt-ID-en er en streng som best√•r av sm√• bokstaver, tall og bindestrek. Prosjekt-ID-en er ikke det samme som prosjektnavnet, som kan inneholde store bokstaver og mellomrom.\nDu finner prosjekt-ID ved logge deg inn p√• GCC, √•pne prosjektvelgeren, s√∏k opp ditt prosjekt, og s√• ser du det i h√∏yre kolonne, slik som vist i denne sladdete kolonnen i Figur¬†1.\n\n\n\n\n\n\nFigur¬†1: Prosjektvelgeren i Google Cloud Console"
  },
  {
    "objectID": "faq.html#footnotes",
    "href": "faq.html#footnotes",
    "title": "Sp√∏rsm√•l og svar",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nFeaturen dapla-buckets inkluderer produkt- og kildeb√∏tta i henholdsvis standard- og kildeprosjektet som de fleste statistikkproduserende team f√•r ved opprettelse.‚Ü©Ô∏é\nI produktb√∏tta blir noncurrent versjoner slettet hvis det er mer enn 2 nyere versjoner, mens for kildeb√∏tta er grensen p√• 3 nyere versjoner‚Ü©Ô∏é"
  },
  {
    "objectID": "utviklere/dokumentere-for-backstage.html",
    "href": "utviklere/dokumentere-for-backstage.html",
    "title": "Hvordan dokumentere systemer for backstage?",
    "section": "",
    "text": "Backstage er en utviklerportal utviklet hos spotify. Vi drifter v√•r egen instans p√• NAIS og du kan n√• den fra VPN, eller med naisdevice, p√• backstage.intern.ssb.no. Tech lead gruppen vedlikeholder techradaren v√•r der, og det er bestemt at alle utviklingsteam hos SSB skal dokumentere sine systemer i backstage sin software katalog.\n√Ö dokumentere et system i backstage er ganske enkelt. Alt du trenger er kunnskap om formatet, s√• kommer du fort igang. Dokumentasjonen skal ligge s√• n√¶rme koden som mulig s√• den er enkel √• vedlikeholde for utviklerne.\nI dette dokumentet g√•r vi gjennom hvordan backstage fungerer ved √• f√∏rst bli kjent med terminologien. Deretter bruker vi et fiktivt microdata-system som eksempel for hvordan vi dokumenterer de forskjellige entitetene. I bunnen av dokumentet finner du ogs√• SSB-spesifikke retningslinjer for backstage dokumenteringen.\nOm du har noen sp√∏rsm√•l som ikke blir besvart i l√∏pet av dette dokumentet, kontakt gjerne techlead gruppen p√• slack: #tech-lead-forum.\n\n\nFor √• dokumentere v√•re systemer i backstage m√• vi v√¶re kjent med entitietene i backstage sin system modell. Her er en kort beskrivelse av hvordan vi bruker disse i SSB:\n\nUser: Er en enkelt ansatt som hentet fra v√•r EntraID\nGroup: Er et team som hentet fra v√•r EntraID f.eks.: microdata-developers\nDomain: Grupperer systemene under domener. Vi har valgt √• binde domenene til emnene i veikartet:\n\nformidling\ndapla\nfellesfunksjoner\n\nSystem: En samling software og ressurser som sammen utf√∏rer en funksjon\nComponent: Et stykke software i et system\nAPI: Et grensesnitt for kommunikasjon mellom komponenter\nResource: Et stykke fysisk eller virituell infrastruktur for som trengs for √• operere en komponent\n\n\n\n\nFor √• dokumentere entiteter (systemer, komponenter, api‚Äôer og ressurser) m√• vi til statisticsnorway sin github. Backstage g√•r nemlig gjennom alle repoene vi eier jevnlig og sjekker om nye backstage-definisjoner har blitt postet. Alt man trenger for at backstage skal legge merke til definisjonene dine er √•:\n\nSette backstage som topic i repoet. Du gj√∏r dette ved √• trykke p√• tannhjulet ved siden av About p√• repo siden.\nLegge en backstage.yaml fil i roten av repoet med en gyldig backstage definisjon\n\nLa oss ta for oss et fiktivt microdata-system for √• forklare hvordan man dokumenterer alle de forskjellige entitetene.\n\n\nSom sagt tidligere er users og groups hentet fra EntraID, og domenene er allerede definert sentralt. N√•r vi starter √• dokumentere systemet v√•rt er derfor det f√∏rste vi m√• starte med systemet selv.\nEttersom definisjonen av systemet selv ikke h√∏rer hjemme i noe spesielt repo, har vi valgt √• lage et repo statisticsnorway/microdata-docs der vi lagrer dokumentasjon som tilh√∏rer microdatasystemet som helhet. Om vi tagger dette repoet med backstage-taggen, kan vi definere systemet v√•rt i roten av repoet med en backstage.yaml slik:\napiVersion: backstage.io/v1alpha1\nkind: System\nmetadata:\n  name: microdata\n  title: microdata\n  description: Tilgang p√• registerdata uten s√∏knadsprosess\n  links:\n    - title: microdata.no\n      url: https://microdata.no\n    - title: data-administrasjon\n      url: https://microdata.no/datastore-admin\n  tags:\n    - on-premises\n    - python\n    - typescript\nspec:\n  owner: microdata-developers\n  domain: formidling\nLa oss se p√• feltene og hva de betyr:\n\napiVersion: spesifiserer backstage sitt dokumentformat\nkind: Vi √∏nsker √• definere et System\n\n\n\n\nname: Navnet p√• systemet i kebab-case\ntitle: Menneskeleselig navn p√• Systemet\ndescription: En kort beskrivelse av systemet\nlinks: En liste med relevante lenker for systemet\ntags: En ustrukturert liste med emnemerker. Se retningslinjene for tags i SSB\n\n\n\n\n\nowner: Gruppen som eier systemet\ndomain: Domenetilh√∏rligheten til systemet\n\nEtter noen minutter, kan vi navigere til backstage websiden og se at systemet v√•r har dukket opp.\n\n\n\n\nI microdata teamet publiserer vi et python bibliotek til PyPI. Denne pakken brukes av andre komponenter i systemet. La oss g√• til github repoet til komponenten statisticsnorway/microdata-tools, markere repoet med backstage som topic, og legge til en backstage.yaml i roten av repoet:\napiVersion: backstage.io/v1alpha1\nkind: Component\nmetadata:\n  name: microdata-tools\n  title: Microdata tools\n  description: |\n    Tools for packaging, encrypting and validating microdata datasets\n  annotations:\n    github.com/project-slug: statisticsnorway/microdata-tools\n  tags:\n    - python\n    - pyarrow\n    - pydantic\nspec:\n  type: library\n  system: microdata\n  owner: microdata-developers\n  lifecycle: production\n  dependencyOf:\n    - component:microdata-job-service\nMange av feltene ligner veldig for √• dokumentere en komponent. Dette er en komponent av type library. La oss se p√• hva feltene betyr:\n\n\n\nname: Navnet p√• komponenten i kebab-case Se retningslinjene for navn i SSB\ntitle: Menneskeleselig navn p√• komponenten\ndescription: En kort beskrivelse av komponenten\nannotations: Lokasjonen til komponenten p√• github\ntags: En ustrukturert liste med emnemerker. Se retningslinjene for tags i SSB\n\n\n\n\n\ntype: Hva slags type komponent dette er Se retningslinjene for type i SSB\nsystem: Systemet denne komponenten er en del av\nowner: Gruppen som eier kompoonenten\nlifecycle: M√• v√¶re satt til en av:\n\nexperimental: Om komponenten er under utvikling, men ikke blitt prodsatt\nproduction: Om komponenten er prodsatt og i drift\ndeprecated: Om komponenten er markert for nedleggelse\n\ndependencyOf: Her kan du spesifisere hvilke andre entiteter som er avhengige av denne komponenten. Det er viktig √• vite at om det finnes noen som bruker dette biblioteket, og marker seg selv som en avhengig av denne, vil det fortsatt registreres av backstage selv om denne avhengigheten ikke er til stedet under dependencyOf-feltet her.\n\n\n\n\n\nVi har et rest-api som kj√∏rer on-prem som vi kaller job-service. Job-service eksponerer et rest-api, og er avhengig av microdata-tools som avhengighet. Dette betyr at for √• representere job-service m√• vi bruke to komponenter i backstage-modellen. En komponent for √• beskrive tjenesten selv (type: service) og en api definisjon for √• beskrive grensesnittet. Vi kan beskrive flere entiteter i samme backstage.yaml ved √• putter tre bindestreker p√• en linje mellom definisjonene. La oss g√• til github repoet til komponenten statisticsnorway/microdata-job-service, markere repoet med backstage som topic, og legge til en backstage.yaml i roten av repoet:\napiVersion: backstage.io/v1alpha1\nkind: Component\nLa oss ta for oss et fiktivt microdata-system for √• forklare hvordan man dokumenterer alle de forskjellige entitetene.\nmetadata:\n  name: microdata-job-service\n  title:  Job service\n  description: |\n    Lookup service for jobs\n  tags:\n    - python\n    - flask\n    - pymongo\n  annotations:\n    github.com/project-slug: statisticsnorway/microdata-job-service\nspec:\n  type: service\n  system: microdata\n  owner: microdata-developers\n  lifecycle: production\n  providesApis:\n    - job-service-api\n  dependsOn:\n    - component:microdata-tools\n    - resource:microdata-job-db\n---\napiVersion: backstage.io/v1alpha1\nkind: API\nmetadata:\n  name: microdata-job-service-api\n  description: Job service\nspec:\n  type: openapi\n  system: microdata\n  owner: microdata-developers\n  lifecycle: production\n  definition:\n    $text: ./doc/openapi.yaml\nHer er mange av feltene for Component delen like som i sist eksempel, med unntak av:\n\nprovidesApis: Peker p√• grensesnitt ved en, eller flere, API entiteter denne komponenten eksponerer\ndependsOn: Peker p√• en eller flere komponenter og ressurser denne tjenesten er avhengig av\n\nFelter man kan ta i bruk som man ikke ser her er ogs√•:\n\nConsumesApis: Peker p√• grensesnitt ved en, eller flere, API entiteter denne komponenten konsumerer\n\nFor API spesifikasjonen som finnes under ---:\n\n\n\nname: Navnet p√• APIet i kebab-case Se retningslinjene for navn i SSB\ntitle: Menneskeleselig navn p√• APIet\ndescription: En kort beskrivelse av APIet\n\n\n\n\n\ntype: Hva slags type grensesnitt dette APIet er. Se retningslinjene for type i SSB\nsystem: Systemet dette APIet er en del av\nowner: Gruppen som eier APIet\nlifecycle: M√• v√¶re satt til en av:\n\nexperimental: Om komponenten er under utvikling, men ikke blitt prodsatt\nproduction: Om komponenten er prodsatt og i drift\ndeprecated: Om komponenten er markert for nedleggelse\n\ndefinition: Om dette er et openapi kan det defineres med openapi formatet i en annen fil. Oppgi path til denne filen i samme repo med $text:\n\nVi f√•r n√• se avhengigheter tydelig i grafene som backstage generer. Vi kan ogs√• unders√∏ke API definisjonene i backstage websiden ved √• g√• til ‚Äúdefinition‚Äù fanen i API ressursen vi har definert.\n\n\n\n\nI microdata.no drifter vi ogs√• en mongodb som vi s√• over at job-service var avhengig av. Mongodb er en database-ressurs. Vi har et repo der vi bygger imaget til denne databasen, men her kunne du lagt ved ressursdefinisjonen sammen med applikasjonen eller i iac-repoet til teamen. Backstage definisjonen b√∏r v√¶re s√• n√¶rme den aktuelle koden som mulig, s√• tenk pragmatisk p√• det beste stedet √• legge den. I dette tilfellet g√•r vi til github repoet til ressursen statisticsnorway/microdata-job-db, markere repoet med backstage som topic, og legge til en backstage.yaml i roten av repoet:\napiVersion: backstage.io/v1alpha1\nkind: Resource\nmetadata:\n  name: microdata-job-db\n  description: |\n    MongoDB that stores jobs and job information in the microdata platform\n  tags:\n    - mongodb\n  annotations:\n    github.com/project-slug: statisticsnorway/microdata-job-db\nspec:\n  type: database\n  owner: microdata-developers\n  lifecycle: production\n  system: microdata\n\n\n\nname: Navnet p√• ressursen i kebab-case Se retningslinjene for navn i SSB\ntitle: Menneskeleselig navn p√• ressursen\ndescription: En kort beskrivelse av ressursen\n\n\n\n\n\ntype: Hva slags type ressurs dette er. Se retningslinjene for type i SSB\nsystem: Systemet denne ressursen er en del av\nowner: Gruppen som eier ressursen\nlifecycle: M√• v√¶re satt til en av:\n\nexperimental: Om komponenten er under utvikling, men ikke blitt prodsatt\nproduction: Om komponenten er prodsatt og i drift\ndeprecated: Om komponenten er markert for nedleggelse\n\n\nDa har vi dokumentert alle de forskjellige entitetene vi trenger i backstage. Det er viktig at vi opprettholder denne informasjonen for en bedre utvikleropplevelse i SSB. Om du har noen sp√∏rsm√•l ang√•ende denne guiden, eller lurer p√• noe ang√•ende backstage; ta kontakt med tech lead gruppen p√• slack under #tech-lead-forum.\n\n\n\n\n\n\n\nFor √• forsikre s√∏kbarhet og god kommunikasjon er det viktig at vi bruker samme spr√•k for √• beskrive systemene v√•re. Det er anbefalt av backstage at organisasjonen tar stilling til bruk av type-feltet. Vi fors√∏ker √• holde mengden definisjoner til et minimum, og bruker samme terminologi som NAIS n√•r vi har mulighet. Om du mener det mangler en type i listene her, ta gjerne kontakt med techlead-teamet p√• slack for diskusjon, eller post en pull request med forslaget til denne dokumentasjonen.\n\n\nFor type p√• komponenter skal kun en av disse brukes:\n\nservice: For langtlevende tjenester\nlibrary: For biblioteker/pakker som eksponeres p√• maven/pypi/crates el.\njob: For applikasjoner som er ment √• kj√∏res p√• et skjema, one-shot eller p√• en trigger\nwebsite: For applikasjoner som skal eksponeres med browser\n\n\n\n\nFor type p√• APIer skal kun en av disse typene brukes:\n\nopenapi: Dette gj√∏r at api‚Äôet kan dokumenteres med openapi dokumentasjon\n\n\n\n\nFor type p√• ressurser skal kun en av disse brukes:\n\ndatabase: for alle databaser\nbucket: for b√∏tter\nqueue: for meldingsk√∏er som pub/sub og kafka\n\n\n\n\n\nP√• samme m√•te √∏nsker vi at alle tagger sine systemer p√• en konsistent m√•te.\n\n\nTags for et system skal BARE inneholde:\n\nHvor systemet kj√∏rer. f.eks.: on-premises, bip, nais\nProgrammeringsspr√•kene brukt i systemet f.eks.: python, kotlin, rust\n\n\n\n\nTags for komponenter skal BARE inneholde:\n\nProgrammeringsspr√•kene brukt i systemet f.eks.: python, kotlin, rust\nKjerneteknologier brukt i komponenten f.eks.: micronaut, flask, pyarrow\n\n\n\n\nTags for ressurser kan BARE inneholde:\n\nSpesifisering av teknologi. ex.: postgresql, mongodb, pubsub\n\n\n\n\n\nFor at komponenter og ressurser skal kunne kobles sammen og vises korrekt i avheninghetsgrafen, s√• er vi avhengig av at det er unike tekniske navn p√• disse p√• tvers av Systemer i Backstage. Dette gj√∏r vi enklest ved √• prefikse med Systemet de tilh√∏rer: name: &lt;system&gt;-&lt;navn&gt;.\n\n\n\nMan kan validere at Backstage yaml filer er gyldige vha. f√∏lgende kommando:\nnpx @roadiehq/backstage-entity-validator backstage.yaml\n\n\n\n\n\nBackstage\nBackstage Docs: The system model\nADR for bruk av backstage i SBB",
    "crumbs": [
      "Utviklere",
      "Hvordan dokumentere systemer for backstage?"
    ]
  },
  {
    "objectID": "utviklere/dokumentere-for-backstage.html#beskrivelse-av-entiteter-i-backstage",
    "href": "utviklere/dokumentere-for-backstage.html#beskrivelse-av-entiteter-i-backstage",
    "title": "Hvordan dokumentere systemer for backstage?",
    "section": "",
    "text": "For √• dokumentere v√•re systemer i backstage m√• vi v√¶re kjent med entitietene i backstage sin system modell. Her er en kort beskrivelse av hvordan vi bruker disse i SSB:\n\nUser: Er en enkelt ansatt som hentet fra v√•r EntraID\nGroup: Er et team som hentet fra v√•r EntraID f.eks.: microdata-developers\nDomain: Grupperer systemene under domener. Vi har valgt √• binde domenene til emnene i veikartet:\n\nformidling\ndapla\nfellesfunksjoner\n\nSystem: En samling software og ressurser som sammen utf√∏rer en funksjon\nComponent: Et stykke software i et system\nAPI: Et grensesnitt for kommunikasjon mellom komponenter\nResource: Et stykke fysisk eller virituell infrastruktur for som trengs for √• operere en komponent",
    "crumbs": [
      "Utviklere",
      "Hvordan dokumentere systemer for backstage?"
    ]
  },
  {
    "objectID": "utviklere/dokumentere-for-backstage.html#dokumentasjon",
    "href": "utviklere/dokumentere-for-backstage.html#dokumentasjon",
    "title": "Hvordan dokumentere systemer for backstage?",
    "section": "",
    "text": "For √• dokumentere entiteter (systemer, komponenter, api‚Äôer og ressurser) m√• vi til statisticsnorway sin github. Backstage g√•r nemlig gjennom alle repoene vi eier jevnlig og sjekker om nye backstage-definisjoner har blitt postet. Alt man trenger for at backstage skal legge merke til definisjonene dine er √•:\n\nSette backstage som topic i repoet. Du gj√∏r dette ved √• trykke p√• tannhjulet ved siden av About p√• repo siden.\nLegge en backstage.yaml fil i roten av repoet med en gyldig backstage definisjon\n\nLa oss ta for oss et fiktivt microdata-system for √• forklare hvordan man dokumenterer alle de forskjellige entitetene.\n\n\nSom sagt tidligere er users og groups hentet fra EntraID, og domenene er allerede definert sentralt. N√•r vi starter √• dokumentere systemet v√•rt er derfor det f√∏rste vi m√• starte med systemet selv.\nEttersom definisjonen av systemet selv ikke h√∏rer hjemme i noe spesielt repo, har vi valgt √• lage et repo statisticsnorway/microdata-docs der vi lagrer dokumentasjon som tilh√∏rer microdatasystemet som helhet. Om vi tagger dette repoet med backstage-taggen, kan vi definere systemet v√•rt i roten av repoet med en backstage.yaml slik:\napiVersion: backstage.io/v1alpha1\nkind: System\nmetadata:\n  name: microdata\n  title: microdata\n  description: Tilgang p√• registerdata uten s√∏knadsprosess\n  links:\n    - title: microdata.no\n      url: https://microdata.no\n    - title: data-administrasjon\n      url: https://microdata.no/datastore-admin\n  tags:\n    - on-premises\n    - python\n    - typescript\nspec:\n  owner: microdata-developers\n  domain: formidling\nLa oss se p√• feltene og hva de betyr:\n\napiVersion: spesifiserer backstage sitt dokumentformat\nkind: Vi √∏nsker √• definere et System\n\n\n\n\nname: Navnet p√• systemet i kebab-case\ntitle: Menneskeleselig navn p√• Systemet\ndescription: En kort beskrivelse av systemet\nlinks: En liste med relevante lenker for systemet\ntags: En ustrukturert liste med emnemerker. Se retningslinjene for tags i SSB\n\n\n\n\n\nowner: Gruppen som eier systemet\ndomain: Domenetilh√∏rligheten til systemet\n\nEtter noen minutter, kan vi navigere til backstage websiden og se at systemet v√•r har dukket opp.\n\n\n\n\nI microdata teamet publiserer vi et python bibliotek til PyPI. Denne pakken brukes av andre komponenter i systemet. La oss g√• til github repoet til komponenten statisticsnorway/microdata-tools, markere repoet med backstage som topic, og legge til en backstage.yaml i roten av repoet:\napiVersion: backstage.io/v1alpha1\nkind: Component\nmetadata:\n  name: microdata-tools\n  title: Microdata tools\n  description: |\n    Tools for packaging, encrypting and validating microdata datasets\n  annotations:\n    github.com/project-slug: statisticsnorway/microdata-tools\n  tags:\n    - python\n    - pyarrow\n    - pydantic\nspec:\n  type: library\n  system: microdata\n  owner: microdata-developers\n  lifecycle: production\n  dependencyOf:\n    - component:microdata-job-service\nMange av feltene ligner veldig for √• dokumentere en komponent. Dette er en komponent av type library. La oss se p√• hva feltene betyr:\n\n\n\nname: Navnet p√• komponenten i kebab-case Se retningslinjene for navn i SSB\ntitle: Menneskeleselig navn p√• komponenten\ndescription: En kort beskrivelse av komponenten\nannotations: Lokasjonen til komponenten p√• github\ntags: En ustrukturert liste med emnemerker. Se retningslinjene for tags i SSB\n\n\n\n\n\ntype: Hva slags type komponent dette er Se retningslinjene for type i SSB\nsystem: Systemet denne komponenten er en del av\nowner: Gruppen som eier kompoonenten\nlifecycle: M√• v√¶re satt til en av:\n\nexperimental: Om komponenten er under utvikling, men ikke blitt prodsatt\nproduction: Om komponenten er prodsatt og i drift\ndeprecated: Om komponenten er markert for nedleggelse\n\ndependencyOf: Her kan du spesifisere hvilke andre entiteter som er avhengige av denne komponenten. Det er viktig √• vite at om det finnes noen som bruker dette biblioteket, og marker seg selv som en avhengig av denne, vil det fortsatt registreres av backstage selv om denne avhengigheten ikke er til stedet under dependencyOf-feltet her.\n\n\n\n\n\nVi har et rest-api som kj√∏rer on-prem som vi kaller job-service. Job-service eksponerer et rest-api, og er avhengig av microdata-tools som avhengighet. Dette betyr at for √• representere job-service m√• vi bruke to komponenter i backstage-modellen. En komponent for √• beskrive tjenesten selv (type: service) og en api definisjon for √• beskrive grensesnittet. Vi kan beskrive flere entiteter i samme backstage.yaml ved √• putter tre bindestreker p√• en linje mellom definisjonene. La oss g√• til github repoet til komponenten statisticsnorway/microdata-job-service, markere repoet med backstage som topic, og legge til en backstage.yaml i roten av repoet:\napiVersion: backstage.io/v1alpha1\nkind: Component\nLa oss ta for oss et fiktivt microdata-system for √• forklare hvordan man dokumenterer alle de forskjellige entitetene.\nmetadata:\n  name: microdata-job-service\n  title:  Job service\n  description: |\n    Lookup service for jobs\n  tags:\n    - python\n    - flask\n    - pymongo\n  annotations:\n    github.com/project-slug: statisticsnorway/microdata-job-service\nspec:\n  type: service\n  system: microdata\n  owner: microdata-developers\n  lifecycle: production\n  providesApis:\n    - job-service-api\n  dependsOn:\n    - component:microdata-tools\n    - resource:microdata-job-db\n---\napiVersion: backstage.io/v1alpha1\nkind: API\nmetadata:\n  name: microdata-job-service-api\n  description: Job service\nspec:\n  type: openapi\n  system: microdata\n  owner: microdata-developers\n  lifecycle: production\n  definition:\n    $text: ./doc/openapi.yaml\nHer er mange av feltene for Component delen like som i sist eksempel, med unntak av:\n\nprovidesApis: Peker p√• grensesnitt ved en, eller flere, API entiteter denne komponenten eksponerer\ndependsOn: Peker p√• en eller flere komponenter og ressurser denne tjenesten er avhengig av\n\nFelter man kan ta i bruk som man ikke ser her er ogs√•:\n\nConsumesApis: Peker p√• grensesnitt ved en, eller flere, API entiteter denne komponenten konsumerer\n\nFor API spesifikasjonen som finnes under ---:\n\n\n\nname: Navnet p√• APIet i kebab-case Se retningslinjene for navn i SSB\ntitle: Menneskeleselig navn p√• APIet\ndescription: En kort beskrivelse av APIet\n\n\n\n\n\ntype: Hva slags type grensesnitt dette APIet er. Se retningslinjene for type i SSB\nsystem: Systemet dette APIet er en del av\nowner: Gruppen som eier APIet\nlifecycle: M√• v√¶re satt til en av:\n\nexperimental: Om komponenten er under utvikling, men ikke blitt prodsatt\nproduction: Om komponenten er prodsatt og i drift\ndeprecated: Om komponenten er markert for nedleggelse\n\ndefinition: Om dette er et openapi kan det defineres med openapi formatet i en annen fil. Oppgi path til denne filen i samme repo med $text:\n\nVi f√•r n√• se avhengigheter tydelig i grafene som backstage generer. Vi kan ogs√• unders√∏ke API definisjonene i backstage websiden ved √• g√• til ‚Äúdefinition‚Äù fanen i API ressursen vi har definert.\n\n\n\n\nI microdata.no drifter vi ogs√• en mongodb som vi s√• over at job-service var avhengig av. Mongodb er en database-ressurs. Vi har et repo der vi bygger imaget til denne databasen, men her kunne du lagt ved ressursdefinisjonen sammen med applikasjonen eller i iac-repoet til teamen. Backstage definisjonen b√∏r v√¶re s√• n√¶rme den aktuelle koden som mulig, s√• tenk pragmatisk p√• det beste stedet √• legge den. I dette tilfellet g√•r vi til github repoet til ressursen statisticsnorway/microdata-job-db, markere repoet med backstage som topic, og legge til en backstage.yaml i roten av repoet:\napiVersion: backstage.io/v1alpha1\nkind: Resource\nmetadata:\n  name: microdata-job-db\n  description: |\n    MongoDB that stores jobs and job information in the microdata platform\n  tags:\n    - mongodb\n  annotations:\n    github.com/project-slug: statisticsnorway/microdata-job-db\nspec:\n  type: database\n  owner: microdata-developers\n  lifecycle: production\n  system: microdata\n\n\n\nname: Navnet p√• ressursen i kebab-case Se retningslinjene for navn i SSB\ntitle: Menneskeleselig navn p√• ressursen\ndescription: En kort beskrivelse av ressursen\n\n\n\n\n\ntype: Hva slags type ressurs dette er. Se retningslinjene for type i SSB\nsystem: Systemet denne ressursen er en del av\nowner: Gruppen som eier ressursen\nlifecycle: M√• v√¶re satt til en av:\n\nexperimental: Om komponenten er under utvikling, men ikke blitt prodsatt\nproduction: Om komponenten er prodsatt og i drift\ndeprecated: Om komponenten er markert for nedleggelse\n\n\nDa har vi dokumentert alle de forskjellige entitetene vi trenger i backstage. Det er viktig at vi opprettholder denne informasjonen for en bedre utvikleropplevelse i SSB. Om du har noen sp√∏rsm√•l ang√•ende denne guiden, eller lurer p√• noe ang√•ende backstage; ta kontakt med tech lead gruppen p√• slack under #tech-lead-forum.",
    "crumbs": [
      "Utviklere",
      "Hvordan dokumentere systemer for backstage?"
    ]
  },
  {
    "objectID": "utviklere/dokumentere-for-backstage.html#retningslinjer",
    "href": "utviklere/dokumentere-for-backstage.html#retningslinjer",
    "title": "Hvordan dokumentere systemer for backstage?",
    "section": "",
    "text": "For √• forsikre s√∏kbarhet og god kommunikasjon er det viktig at vi bruker samme spr√•k for √• beskrive systemene v√•re. Det er anbefalt av backstage at organisasjonen tar stilling til bruk av type-feltet. Vi fors√∏ker √• holde mengden definisjoner til et minimum, og bruker samme terminologi som NAIS n√•r vi har mulighet. Om du mener det mangler en type i listene her, ta gjerne kontakt med techlead-teamet p√• slack for diskusjon, eller post en pull request med forslaget til denne dokumentasjonen.\n\n\nFor type p√• komponenter skal kun en av disse brukes:\n\nservice: For langtlevende tjenester\nlibrary: For biblioteker/pakker som eksponeres p√• maven/pypi/crates el.\njob: For applikasjoner som er ment √• kj√∏res p√• et skjema, one-shot eller p√• en trigger\nwebsite: For applikasjoner som skal eksponeres med browser\n\n\n\n\nFor type p√• APIer skal kun en av disse typene brukes:\n\nopenapi: Dette gj√∏r at api‚Äôet kan dokumenteres med openapi dokumentasjon\n\n\n\n\nFor type p√• ressurser skal kun en av disse brukes:\n\ndatabase: for alle databaser\nbucket: for b√∏tter\nqueue: for meldingsk√∏er som pub/sub og kafka\n\n\n\n\n\nP√• samme m√•te √∏nsker vi at alle tagger sine systemer p√• en konsistent m√•te.\n\n\nTags for et system skal BARE inneholde:\n\nHvor systemet kj√∏rer. f.eks.: on-premises, bip, nais\nProgrammeringsspr√•kene brukt i systemet f.eks.: python, kotlin, rust\n\n\n\n\nTags for komponenter skal BARE inneholde:\n\nProgrammeringsspr√•kene brukt i systemet f.eks.: python, kotlin, rust\nKjerneteknologier brukt i komponenten f.eks.: micronaut, flask, pyarrow\n\n\n\n\nTags for ressurser kan BARE inneholde:\n\nSpesifisering av teknologi. ex.: postgresql, mongodb, pubsub\n\n\n\n\n\nFor at komponenter og ressurser skal kunne kobles sammen og vises korrekt i avheninghetsgrafen, s√• er vi avhengig av at det er unike tekniske navn p√• disse p√• tvers av Systemer i Backstage. Dette gj√∏r vi enklest ved √• prefikse med Systemet de tilh√∏rer: name: &lt;system&gt;-&lt;navn&gt;.\n\n\n\nMan kan validere at Backstage yaml filer er gyldige vha. f√∏lgende kommando:\nnpx @roadiehq/backstage-entity-validator backstage.yaml",
    "crumbs": [
      "Utviklere",
      "Hvordan dokumentere systemer for backstage?"
    ]
  },
  {
    "objectID": "utviklere/dokumentere-for-backstage.html#lenker",
    "href": "utviklere/dokumentere-for-backstage.html#lenker",
    "title": "Hvordan dokumentere systemer for backstage?",
    "section": "",
    "text": "Backstage\nBackstage Docs: The system model\nADR for bruk av backstage i SBB",
    "crumbs": [
      "Utviklere",
      "Hvordan dokumentere systemer for backstage?"
    ]
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html",
    "href": "notebooks/spark/deltalake-intro.html",
    "title": "Introduksjon til Delta Lake",
    "section": "",
    "text": "Delta Lake er et databaselag som kan legges over parquet-filer i b√∏tter. Det kan gi oss mye av den funksjonaliteten vi har v√¶rt vant til i relasjonelle databaser og datavarehus. I denne notebooken vises hvordan man kan ta det i bruk p√• Dapla."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#oppsett",
    "href": "notebooks/spark/deltalake-intro.html#oppsett",
    "title": "Introduksjon til Delta Lake",
    "section": "Oppsett",
    "text": "Oppsett\nHvis du logger deg inn p√• https://jupyter.dapla.ssb.no/ kan du ta i bruk Delta Lake via Spark. Men for √• gj√∏re det m√• du installere delta-spark. For √• installere pakken m√• du jobbe i et ssb-project. I tillegg m√• du installere delta-spark-versjon som er kompatibel med Spark-versjonen som er installert p√• Dapla. Gj√∏r derfor f√∏lgende:\n\nOpprett et ssb-project med kommandoen:\nssb-project create test-delta-lake\nI terminalen skriver du f√∏lgende for √• sjekke Spark-versjonen som er installert:\nspark-shell --version\nSjekk hvilken delta-spark-versjon som er kompatibel med din Spark-versjon og installer den med kommandoen1:\npoetry add delta-spark@2.3\n√Öpne en ny notebook og velg kernel test-delta-lake.\n\nN√• har du satt opp et virtuelt milj√∏ med en PySpark-kernel som kj√∏rer en maskin (s√•kalt Pyspark local kernel), der du har installert delta-spark. Vi kan n√• importere de bibliotekene vi trenger og sette igang en Spark-session.\n\n# Importerer biblioteker\nimport pyspark\nfrom delta import *\n\nDeretter initialiserer jeg en Spark-session. %%capture_output er med for √• forhindre at det ikke blir printet ut sensitiv informasjon i en notebook som skal inn i Dapla-manualen.\n\n%%capture captured_output\n%run /usr/local/share/jupyter/kernels/pyspark_local/init.py"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#genererer-noe-data",
    "href": "notebooks/spark/deltalake-intro.html#genererer-noe-data",
    "title": "Introduksjon til Delta Lake",
    "section": "Genererer noe data",
    "text": "Genererer noe data\n\n# Genererer noe data med Spark\ndata = spark.range(10, 15)\ndata.show()\n\n+---+\n| id|\n+---+\n| 10|\n| 11|\n| 12|\n| 13|\n| 14|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#skriver-ut-en-delta-lake-table",
    "href": "notebooks/spark/deltalake-intro.html#skriver-ut-en-delta-lake-table",
    "title": "Introduksjon til Delta Lake",
    "section": "Skriver ut en Delta Lake Table",
    "text": "Skriver ut en Delta Lake Table\nLa oss skrive ut Spark DataFrame som en Delta Lake Table og bli kjent med strukturen i objektet:\n\n%%time\ndata.write.format(\"delta\").mode(\"overwrite\").save(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp4\"\n)\n\nCPU times: user 3.38 ms, sys: 1.7 ms, total: 5.08 ms\nWall time: 5.59 s\n\n\nVi kan deretter printe ut hva som ble opprettet n√•r vi skrev ut en Delta Lake Table:\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/temp4/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp4/_delta_log',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000000.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-9b3b81a9-2771-4fb4-9f0f-659fd160d643-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-0f2f8ba5-3161-41e8-b5d1-2084128a5bed-c000.snappy.parquet']\n\n\n\nDelta Lake Tabellstruktur\nMappenstrukturen du ser over er typisk for en Delta Lake-tabell. La oss bryte ned komponentene:\n\n_delta_log/:\n\nDette er transaksjonsloggmappen for Delta Lake-tabellen. Den inneholder en serie med JSON-filer som registrerer alle transaksjoner gjort p√• tabellen.\nTransaksjonsloggen er avgj√∏rende for Delta Lakes ACID-transaksjonsegenskaper, som muliggj√∏r funksjoner som atomiske forpliktelser, tilbakerullinger og tid-reise-foresp√∏rsler.\n\n_delta_log/00000000000000000000.json:\n\nDette er en JSON-fil innenfor transaksjonsloggkatalogen. Hver JSON-fil i denne mappen tilsvarer en transaksjon (eller en batch av transaksjoner) gjort p√• tabellen.\nFilnavnet er null-fylt og representerer transaksjonsversjonen. I dette tilfellet representerer 00000000000000000000.json den aller f√∏rste versjonen (versjon 0) av tabellen. Etter hvert som flere transaksjoner blir gjort, vil nye filer bli lagt til med √∏kende versjonsnumre.\n\npart-00000-450715bd-6b0c-4827-bb8a-b0265505ca72-c000.snappy.parquet og part-00001-977ed55f-5ce5-469f-8a1d-9eafb143c215-c000.snappy.parquet:\n\nDette er de faktiske datafilene til Delta Lake-tabellen, lagret i Parquet-format.\n.snappy.parquet-utvidelsen indikerer at dataene er komprimert ved hjelp av Snappy-komprimeringsalgoritmen.\nFilnavnene er typiske for Sparks navngivningskonvensjon for datadeler. Prefiksene part-00000 og part-00001 indikerer partisjonsnumre. De lange UUID-lignende strengene er unike identifikatorer for datafilene. Suffikset c000 indikerer Spark-oppgaven som skrev filen.\n\n\nMappen representerer en Delta Lake-tabell med data lagret i Parquet-format og en transaksjonslogg som sporer endringer i tabellen. Tilstedev√¶relsen av _delta_log-mappen og innholdet er det som skiller en Delta Lake-tabell fra et vanlig Parquet-datasett."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#lese-inn-tabell",
    "href": "notebooks/spark/deltalake-intro.html#lese-inn-tabell",
    "title": "Introduksjon til Delta Lake",
    "section": "Lese inn tabell",
    "text": "Lese inn tabell\n\ndeltaTable = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 13|\n| 14|\n| 10|\n| 11|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#modifisere-datasettet",
    "href": "notebooks/spark/deltalake-intro.html#modifisere-datasettet",
    "title": "Introduksjon til Delta Lake",
    "section": "Modifisere datasettet",
    "text": "Modifisere datasettet\nLa oss modifisere datasettet ved √• bytte ut verdien 13 med 15 i id-kolonnen:\n\nfrom pyspark.sql.functions import col, lit\n\n# Update the cell with value 13 to 15\ndeltaTable.update(condition=(col(\"id\") == 13), set={\"id\": lit(15)})\n\nEt viktig poeng √• f√• med seg her er at vi n√• oppdaterte Delta Lake Table objektet b√•de i minnet og p√• disk. La oss bevise det ved √• lese inn fra disk:\n\ndeltaTable2 = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\ndeltaTable2.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n+---+\n\n\n\nOg deretter ved √• printe ut det opprinnelige objektet vi leste inn:\n\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#append-data",
    "href": "notebooks/spark/deltalake-intro.html#append-data",
    "title": "Introduksjon til Delta Lake",
    "section": "Append data",
    "text": "Append data\nLa oss legge til verdiene 20 og 21 til datasettet. F√∏rst lager vi en Spark dataframe:\n\nnew_data = [(20,), (21,)]\nnew_df = spark.createDataFrame(new_data, [\"id\"])\nnew_df.show()\n\n+---+\n| id|\n+---+\n| 20|\n| 21|\n+---+\n\n\n\nDeretter kan vi appendere det til v√•r opprinnelige dataframe:\n\nnew_df.write.format(\"delta\").mode(\"append\").save(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp4\"\n)\n\n\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n| 21|\n| 20|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#historien-og-metadata-til-filen",
    "href": "notebooks/spark/deltalake-intro.html#historien-og-metadata-til-filen",
    "title": "Introduksjon til Delta Lake",
    "section": "Historien og metadata til filen",
    "text": "Historien og metadata til filen\nN√• som vi har gjort noen endringer kan vi se p√• historien til filen:\n\n# Lister ut filene i b√∏tta\nfs = FileClient.get_gcs_file_system()\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/temp4/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp4/_delta_log',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000000.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000001.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000002.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-73e5052f-1b82-48da-ab37-2cbc01bb46c1-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-9b3b81a9-2771-4fb4-9f0f-659fd160d643-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-d04d0ca2-8e8b-42e9-a8a3-0fed9a0e4e41-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-0f2f8ba5-3161-41e8-b5d1-2084128a5bed-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-30d707e4-dd9a-4bfd-a4c7-7fbb1933e9ae-c000.snappy.parquet']\n\n\nVi ser av transaksjonsloggen i _delta_log-mappen at det n√• har v√¶rt 3 transaksjoner p√• datasettet. vi ser ogs√• av navnene p√• parquet-filene, part-00000 og part-00001, at det finnes to versjoner av filen. Hvis vi √∏nsker √• bli bedre kjent med hva slags informasjon som blir lagret fra transaksjonene, s√• kan vi printe ut den siste transaksjonen som heter 00000000000000000002.json:\n\nimport json\n\nfrom dapla import FileClient\n\n# Kobler oss p√• b√∏ttene\nfs = FileClient.get_gcs_file_system()\n\n# Filsti\npath = \"gs://ssb-dapla-felles-data-produkt-prod/temp4/_delta_log/00000000000000000002.json\"\n\nwith fs.open(path, \"r\") as f:\n    for line in f:\n        data = json.loads(line)\n        pretty_content = json.dumps(data, indent=4)\n        print(pretty_content)\n        print(\"-\" * 50)  # Print separator for clarity\n\n{\n    \"commitInfo\": {\n        \"timestamp\": 1696942544879,\n        \"operation\": \"WRITE\",\n        \"operationParameters\": {\n            \"mode\": \"Append\",\n            \"partitionBy\": \"[]\"\n        },\n        \"readVersion\": 1,\n        \"isolationLevel\": \"Serializable\",\n        \"isBlindAppend\": true,\n        \"operationMetrics\": {\n            \"numFiles\": \"2\",\n            \"numOutputRows\": \"2\",\n            \"numOutputBytes\": \"956\"\n        },\n        \"engineInfo\": \"Apache-Spark/3.3.1 Delta-Lake/2.3.0\",\n        \"txnId\": \"a3dcd582-8362-4fc2-a8ce-57613d2eb2b8\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00000-73e5052f-1b82-48da-ab37-2cbc01bb46c1-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 478,\n        \"modificationTime\": 1696942544755,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":20},\\\"maxValues\\\":{\\\"id\\\":20},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00001-30d707e4-dd9a-4bfd-a4c7-7fbb1933e9ae-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 478,\n        \"modificationTime\": 1696942544833,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":21},\\\"maxValues\\\":{\\\"id\\\":21},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n\n\nHer ser vi at vi f√•r masse informasjon om endringen, b√•de metadata om transaksjonen i commitInfo, og informasjon om nye data-filer i add-delen. Vi ser at det er en veldig rik beskrivelse av endringer, men det kan v√¶re vanskeig √• lese innholdet direkte. La oss heller bruke history()-funksjonen som kommer med delta:\n\nhistory = deltaTable.history()\nhistory.show()\n\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n|      2|2023-10-10 12:55:...|  null|    null|    WRITE|{mode -&gt; Append, ...|null|    null|     null|          1|  Serializable|         true|{numFiles -&gt; 2, n...|        null|Apache-Spark/3.3....|\n|      1|2023-10-10 12:55:...|  null|    null|   UPDATE|{predicate -&gt; (id...|null|    null|     null|          0|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n|      0|2023-10-10 12:55:...|  null|    null|    WRITE|{mode -&gt; Overwrit...|null|    null|     null|       null|  Serializable|        false|{numFiles -&gt; 2, n...|        null|Apache-Spark/3.3....|\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n\n\n\nSiden det blit trangt i tabellen over s√• kan vi velge hvilke variabler vi √∏nsker √• se p√•:\n\n# Oversikt over alle kolonner som finnes i historien\nhistory.columns\n\n['version',\n 'timestamp',\n 'userId',\n 'userName',\n 'operation',\n 'operationParameters',\n 'job',\n 'notebook',\n 'clusterId',\n 'readVersion',\n 'isolationLevel',\n 'isBlindAppend',\n 'operationMetrics',\n 'userMetadata',\n 'engineInfo']\n\n\n\n# Velger de kolonnene jeg er interessert i\nselected_history = deltaTable.history().select(\n    \"version\", \"timestamp\", \"operation\", \"operationParameters\"\n)\n\n\n# Display the selected columns\nselected_history.show(truncate=50)\n\n+-------+-----------------------+---------+--------------------------------------+\n|version|              timestamp|operation|                   operationParameters|\n+-------+-----------------------+---------+--------------------------------------+\n|      2|2023-10-10 12:55:45.014|    WRITE|   {mode -&gt; Append, partitionBy -&gt; []}|\n|      1|2023-10-10 12:55:37.054|   UPDATE|        {predicate -&gt; (id#4452L = 13)}|\n|      0|2023-10-10 12:55:29.048|    WRITE|{mode -&gt; Overwrite, partitionBy -&gt; []}|\n+-------+-----------------------+---------+--------------------------------------+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#egendefinert-metadata",
    "href": "notebooks/spark/deltalake-intro.html#egendefinert-metadata",
    "title": "Introduksjon til Delta Lake",
    "section": "Egendefinert metadata",
    "text": "Egendefinert metadata\nDelta Lake st√∏tter ogs√• egendefinert metadata. Det kan for eksempel v√¶re nyttig hvis man √∏nsker √• bruke Delta Lake som en backend for et GUI som lar brukeren oppdatere en tabell fra GUI-et. Da √∏nsker man typisk √• lagre hvem som gjorde endringer og n√•r det ble gjort. La oss legge p√• noe slik metadata:\n\n# Leser inn filen\ndf = spark.read.format(\"delta\").load(\"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\n\n\n\nCode\n# Lagrer egendefinert metadata i en json-fil\nimport json\n\nmetadata = {\n    \"comment\": \"Kontaktet oppgavegiver og kranglet!\",\n    \"manueltEditert\": \"True\",\n    \"maskineltEditert\": \"False\",\n}\nmetadata\n\n\n{'comment': 'Kontaktet oppgavegiver og kranglet!',\n 'manueltEditert': 'True',\n 'maskineltEditert': 'False'}\n\n\nVi kan deretter appende dette til den siste versjonen av fila.\n\n(\n    df.write.format(\"delta\")\n    .mode(\"append\")\n    .option(\"userMetadata\", json.dumps(metadata))  # Serialize metadata to a string\n    .save(\"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\n)\n\n\n# Laster inn tabellen\ndeltaTable = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\n\n# Henter ut historien\nhistory_df = deltaTable.history()\n\n\n# Show the operation details, including metadata\nhistory_df.select(\n    \"version\", \"timestamp\", \"operation\", \"operationParameters\", \"userMetadata\"\n).show(truncate=10)\n\n+-------+----------+---------+-------------------+------------+\n|version| timestamp|operation|operationParameters|userMetadata|\n+-------+----------+---------+-------------------+------------+\n|      3|2023-10...|    WRITE|         {mode -...|  {\"comme...|\n|      2|2023-10...|    WRITE|         {mode -...|        null|\n|      1|2023-10...|   UPDATE|         {predic...|        null|\n|      0|2023-10...|    WRITE|         {mode -...|        null|\n+-------+----------+---------+-------------------+------------+\n\n\n\n\nhistory_df.select(\"version\", \"userMetadata\").show(truncate=50)\n\n+-------+--------------------------------------------------+\n|version|                                      userMetadata|\n+-------+--------------------------------------------------+\n|      3|{\"comment\": \"Kontaktet oppgavegiver og kranglet...|\n|      2|                                              null|\n|      1|                                              null|\n|      0|                                              null|\n+-------+--------------------------------------------------+\n\n\n\nVi ser at vi la til v√•r egen metadata i versjon 3 av fila. Vi kan printe ut den r√• transaksjonsloggen som tidligere, men n√• er vi p√• transaksjon 3 00000000000000000003.json:\n\nfrom dapla import FileClient\n\n# Kobler oss p√• b√∏ttene\nfs = FileClient.get_gcs_file_system()\n\n# Filsti\npath = \"gs://ssb-dapla-felles-data-produkt-prod/temp4/_delta_log/00000000000000000003.json\"\n\nwith fs.open(path, \"r\") as f:\n    for line in f:\n        data = json.loads(line)\n        pretty_content = json.dumps(data, indent=4)\n        print(pretty_content)\n        print(\"-\" * 50)  # Print separator for clarity\n\n{\n    \"commitInfo\": {\n        \"timestamp\": 1696942553907,\n        \"operation\": \"WRITE\",\n        \"operationParameters\": {\n            \"mode\": \"Append\",\n            \"partitionBy\": \"[]\"\n        },\n        \"readVersion\": 2,\n        \"isolationLevel\": \"Serializable\",\n        \"isBlindAppend\": false,\n        \"operationMetrics\": {\n            \"numFiles\": \"2\",\n            \"numOutputRows\": \"7\",\n            \"numOutputBytes\": \"989\"\n        },\n        \"userMetadata\": \"{\\\"comment\\\": \\\"Kontaktet oppgavegiver og kranglet!\\\", \\\"manueltEditert\\\": \\\"True\\\", \\\"maskineltEditert\\\": \\\"False\\\"}\",\n        \"engineInfo\": \"Apache-Spark/3.3.1 Delta-Lake/2.3.0\",\n        \"txnId\": \"e7de92bf-b0f9-4341-8bbb-9b382f2f3eb6\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00000-96369f3d-fe4a-4365-a0df-00c813027399-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 503,\n        \"modificationTime\": 1696942553856,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":5,\\\"minValues\\\":{\\\"id\\\":10},\\\"maxValues\\\":{\\\"id\\\":15},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00001-0f1bc8e6-093b-49a9-ad0b-78d5a148cfb6-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 486,\n        \"modificationTime\": 1696942553853,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":2,\\\"minValues\\\":{\\\"id\\\":20},\\\"maxValues\\\":{\\\"id\\\":21},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n\n\nVi er da at metadataene ligger som forventet i metadata-delen."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#footnotes",
    "href": "notebooks/spark/deltalake-intro.html#footnotes",
    "title": "Introduksjon til Delta Lake",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI eksempelet er det Spark V3.3.1 som er installert og jeg installerer derfor delta-spark v2.3‚Ü©Ô∏é"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html",
    "href": "notebooks/spark/deltalake-intro.out.html",
    "title": "Introduksjon til Delta Lake",
    "section": "",
    "text": "Delta Lake er et databaselag som kan legges over parquet-filer i b√∏tter. Det kan gi oss mye av den funksjonaliteten vi har v√¶rt vant til i relasjonelle databaser og datavarehus. I denne notebooken vises hvordan man kan ta det i bruk p√• Dapla."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#oppsett",
    "href": "notebooks/spark/deltalake-intro.out.html#oppsett",
    "title": "Introduksjon til Delta Lake",
    "section": "Oppsett",
    "text": "Oppsett\nHvis du logger deg inn p√• https://jupyter.dapla.ssb.no/ kan du ta i bruk Delta Lake via Spark. Men for √• gj√∏re det m√• du installere delta-spark. For √• installere pakken m√• du jobbe i et ssb-project. I tillegg m√• du installere delta-spark-versjon som er kompatibel med Spark-versjonen som er installert p√• Dapla. Gj√∏r derfor f√∏lgende:\n\nOpprett et ssb-project med kommandoen:\nssb-project create test-delta-lake\nI terminalen skriver du f√∏lgende for √• sjekke Spark-versjonen som er installert:\nspark-shell --version\nSjekk hvilken delta-spark-versjon som er kompatibel med din Spark-versjon og installer den med kommandoen[1]:\npoetry add delta-spark@2.3\n√Öpne en ny notebook og velg kernel test-delta-lake.\n\nN√• har du satt opp et virtuelt milj√∏ med en PySpark-kernel som kj√∏rer en maskin (s√•kalt Pyspark local kernel), der du har installert delta-spark. Vi kan n√• importere de bibliotekene vi trenger og sette igang en Spark-session.\n[1] I eksempelet er det Spark V3.3.1 som er installert og jeg installerer derfor delta-spark v2.3\n\n# Importerer biblioteker\nimport pyspark\nfrom delta import *\n\nDeretter initialiserer jeg en Spark-session. %%capture_output er med for √• forhindre at det ikke blir printet ut sensitiv informasjon i en notebook som skal inn i Dapla-manualen.\n\n%%capture captured_output\n%run /usr/local/share/jupyter/kernels/pyspark_local/init.py"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#genererer-noe-data",
    "href": "notebooks/spark/deltalake-intro.out.html#genererer-noe-data",
    "title": "Introduksjon til Delta Lake",
    "section": "Genererer noe data",
    "text": "Genererer noe data\n\n# Genererer noe data med Spark\ndata = spark.range(10, 15)\ndata.show()\n\n+---+\n| id|\n+---+\n| 10|\n| 11|\n| 12|\n| 13|\n| 14|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#skriver-ut-en-delta-lake-table",
    "href": "notebooks/spark/deltalake-intro.out.html#skriver-ut-en-delta-lake-table",
    "title": "Introduksjon til Delta Lake",
    "section": "Skriver ut en Delta Lake Table",
    "text": "Skriver ut en Delta Lake Table\nLa oss skrive ut Spark DataFrame som en Delta Lake Table og bli kjent med strukturen i objektet:\n\n%%time\ndata.write.format(\"delta\").mode(\"overwrite\").save(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp4\"\n)\n\nCPU times: user 3.38 ms, sys: 1.7 ms, total: 5.08 ms\nWall time: 5.59 s\n\n\nVi kan deretter printe ut hva som ble opprettet n√•r vi skrev ut en Delta Lake Table:\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/temp4/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp4/_delta_log',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000000.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-9b3b81a9-2771-4fb4-9f0f-659fd160d643-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-0f2f8ba5-3161-41e8-b5d1-2084128a5bed-c000.snappy.parquet']\n\n\n\nDelta Lake Tabellstruktur\nMappenstrukturen du ser over er typisk for en Delta Lake-tabell. La oss bryte ned komponentene:\n\n_delta_log/:\n\nDette er transaksjonsloggmappen for Delta Lake-tabellen. Den inneholder en serie med JSON-filer som registrerer alle transaksjoner gjort p√• tabellen.\nTransaksjonsloggen er avgj√∏rende for Delta Lakes ACID-transaksjonsegenskaper, som muliggj√∏r funksjoner som atomiske forpliktelser, tilbakerullinger og tid-reise-foresp√∏rsler.\n\n_delta_log/00000000000000000000.json:\n\nDette er en JSON-fil innenfor transaksjonsloggkatalogen. Hver JSON-fil i denne mappen tilsvarer en transaksjon (eller en batch av transaksjoner) gjort p√• tabellen.\nFilnavnet er null-fylt og representerer transaksjonsversjonen. I dette tilfellet representerer 00000000000000000000.json den aller f√∏rste versjonen (versjon 0) av tabellen. Etter hvert som flere transaksjoner blir gjort, vil nye filer bli lagt til med √∏kende versjonsnumre.\n\npart-00000-450715bd-6b0c-4827-bb8a-b0265505ca72-c000.snappy.parquet og part-00001-977ed55f-5ce5-469f-8a1d-9eafb143c215-c000.snappy.parquet:\n\nDette er de faktiske datafilene til Delta Lake-tabellen, lagret i Parquet-format.\n.snappy.parquet-utvidelsen indikerer at dataene er komprimert ved hjelp av Snappy-komprimeringsalgoritmen.\nFilnavnene er typiske for Sparks navngivningskonvensjon for datadeler. Prefiksene part-00000 og part-00001 indikerer partisjonsnumre. De lange UUID-lignende strengene er unike identifikatorer for datafilene. Suffikset c000 indikerer Spark-oppgaven som skrev filen.\n\n\nMappen representerer en Delta Lake-tabell med data lagret i Parquet-format og en transaksjonslogg som sporer endringer i tabellen. Tilstedev√¶relsen av _delta_log-mappen og innholdet er det som skiller en Delta Lake-tabell fra et vanlig Parquet-datasett."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#lese-inn-tabell",
    "href": "notebooks/spark/deltalake-intro.out.html#lese-inn-tabell",
    "title": "Introduksjon til Delta Lake",
    "section": "Lese inn tabell",
    "text": "Lese inn tabell\n\ndeltaTable = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 13|\n| 14|\n| 10|\n| 11|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#modifisere-datasettet",
    "href": "notebooks/spark/deltalake-intro.out.html#modifisere-datasettet",
    "title": "Introduksjon til Delta Lake",
    "section": "Modifisere datasettet",
    "text": "Modifisere datasettet\nLa oss modifisere datasettet ved √• bytte ut verdien 13 med 15 i id-kolonnen:\n\nfrom pyspark.sql.functions import col, lit\n\n# Update the cell with value 13 to 15\ndeltaTable.update(condition=(col(\"id\") == 13), set={\"id\": lit(15)})\n\nEt viktig poeng √• f√• med seg her er at vi n√• oppdaterte Delta Lake Table objektet b√•de i minnet og p√• disk. La oss bevise det ved √• lese inn fra disk:\n\ndeltaTable2 = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\ndeltaTable2.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n+---+\n\n\nOg deretter ved √• printe ut det opprinnelige objektet vi leste inn:\n\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#append-data",
    "href": "notebooks/spark/deltalake-intro.out.html#append-data",
    "title": "Introduksjon til Delta Lake",
    "section": "Append data",
    "text": "Append data\nLa oss legge til verdiene 20 og 21 til datasettet. F√∏rst lager vi en Spark dataframe:\n\nnew_data = [(20,), (21,)]\nnew_df = spark.createDataFrame(new_data, [\"id\"])\nnew_df.show()\n\n+---+\n| id|\n+---+\n| 20|\n| 21|\n+---+\n\n\nDeretter kan vi appendere det til v√•r opprinnelige dataframe:\n\nnew_df.write.format(\"delta\").mode(\"append\").save(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp4\"\n)\n\n\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n| 21|\n| 20|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#historien-og-metadata-til-filen",
    "href": "notebooks/spark/deltalake-intro.out.html#historien-og-metadata-til-filen",
    "title": "Introduksjon til Delta Lake",
    "section": "Historien og metadata til filen",
    "text": "Historien og metadata til filen\nN√• som vi har gjort noen endringer kan vi se p√• historien til filen:\n\n# Lister ut filene i b√∏tta\nfs = FileClient.get_gcs_file_system()\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/temp4/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp4/_delta_log',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000000.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000001.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000002.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-73e5052f-1b82-48da-ab37-2cbc01bb46c1-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-9b3b81a9-2771-4fb4-9f0f-659fd160d643-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-d04d0ca2-8e8b-42e9-a8a3-0fed9a0e4e41-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-0f2f8ba5-3161-41e8-b5d1-2084128a5bed-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-30d707e4-dd9a-4bfd-a4c7-7fbb1933e9ae-c000.snappy.parquet']\n\n\nVi ser av transaksjonsloggen i _delta_log-mappen at det n√• har v√¶rt 3 transaksjoner p√• datasettet. vi ser ogs√• av navnene p√• parquet-filene, part-00000 og part-00001, at det finnes to versjoner av filen. Hvis vi √∏nsker √• bli bedre kjent med hva slags informasjon som blir lagret fra transaksjonene, s√• kan vi printe ut den siste transaksjonen som heter 00000000000000000002.json:\n\nimport json\n\nfrom dapla import FileClient\n\n# Kobler oss p√• b√∏ttene\nfs = FileClient.get_gcs_file_system()\n\n# Filsti\npath = \"gs://ssb-dapla-felles-data-produkt-prod/temp4/_delta_log/00000000000000000002.json\"\n\nwith fs.open(path, \"r\") as f:\n    for line in f:\n        data = json.loads(line)\n        pretty_content = json.dumps(data, indent=4)\n        print(pretty_content)\n        print(\"-\" * 50)  # Print separator for clarity\n\n{\n    \"commitInfo\": {\n        \"timestamp\": 1696942544879,\n        \"operation\": \"WRITE\",\n        \"operationParameters\": {\n            \"mode\": \"Append\",\n            \"partitionBy\": \"[]\"\n        },\n        \"readVersion\": 1,\n        \"isolationLevel\": \"Serializable\",\n        \"isBlindAppend\": true,\n        \"operationMetrics\": {\n            \"numFiles\": \"2\",\n            \"numOutputRows\": \"2\",\n            \"numOutputBytes\": \"956\"\n        },\n        \"engineInfo\": \"Apache-Spark/3.3.1 Delta-Lake/2.3.0\",\n        \"txnId\": \"a3dcd582-8362-4fc2-a8ce-57613d2eb2b8\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00000-73e5052f-1b82-48da-ab37-2cbc01bb46c1-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 478,\n        \"modificationTime\": 1696942544755,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":20},\\\"maxValues\\\":{\\\"id\\\":20},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00001-30d707e4-dd9a-4bfd-a4c7-7fbb1933e9ae-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 478,\n        \"modificationTime\": 1696942544833,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":21},\\\"maxValues\\\":{\\\"id\\\":21},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n\n\nHer ser vi at vi f√•r masse informasjon om endringen, b√•de metadata om transaksjonen i commitInfo, og informasjon om nye data-filer i add-delen. Vi ser at det er en veldig rik beskrivelse av endringer, men det kan v√¶re vanskeig √• lese innholdet direkte. La oss heller bruke history()-funksjonen som kommer med delta:\n\nhistory = deltaTable.history()\nhistory.show()\n\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n|      2|2023-10-10 12:55:...|  null|    null|    WRITE|{mode -&gt; Append, ...|null|    null|     null|          1|  Serializable|         true|{numFiles -&gt; 2, n...|        null|Apache-Spark/3.3....|\n|      1|2023-10-10 12:55:...|  null|    null|   UPDATE|{predicate -&gt; (id...|null|    null|     null|          0|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n|      0|2023-10-10 12:55:...|  null|    null|    WRITE|{mode -&gt; Overwrit...|null|    null|     null|       null|  Serializable|        false|{numFiles -&gt; 2, n...|        null|Apache-Spark/3.3....|\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n\n\nSiden det blit trangt i tabellen over s√• kan vi velge hvilke variabler vi √∏nsker √• se p√•:\n\n# Oversikt over alle kolonner som finnes i historien\nhistory.columns\n\n['version',\n 'timestamp',\n 'userId',\n 'userName',\n 'operation',\n 'operationParameters',\n 'job',\n 'notebook',\n 'clusterId',\n 'readVersion',\n 'isolationLevel',\n 'isBlindAppend',\n 'operationMetrics',\n 'userMetadata',\n 'engineInfo']\n\n\n\n# Velger de kolonnene jeg er interessert i\nselected_history = deltaTable.history().select(\n    \"version\", \"timestamp\", \"operation\", \"operationParameters\"\n)\n\n\n# Display the selected columns\nselected_history.show(truncate=50)\n\n+-------+-----------------------+---------+--------------------------------------+\n|version|              timestamp|operation|                   operationParameters|\n+-------+-----------------------+---------+--------------------------------------+\n|      2|2023-10-10 12:55:45.014|    WRITE|   {mode -&gt; Append, partitionBy -&gt; []}|\n|      1|2023-10-10 12:55:37.054|   UPDATE|        {predicate -&gt; (id#4452L = 13)}|\n|      0|2023-10-10 12:55:29.048|    WRITE|{mode -&gt; Overwrite, partitionBy -&gt; []}|\n+-------+-----------------------+---------+--------------------------------------+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#egendefinert-metadata",
    "href": "notebooks/spark/deltalake-intro.out.html#egendefinert-metadata",
    "title": "Introduksjon til Delta Lake",
    "section": "Egendefinert metadata",
    "text": "Egendefinert metadata\nDelta Lake st√∏tter ogs√• egendefinert metadata. Det kan for eksempel v√¶re nyttig hvis man √∏nsker √• bruke Delta Lake som en backend for et GUI som lar brukeren oppdatere en tabell fra GUI-et. Da √∏nsker man typisk √• lagre hvem som gjorde endringer og n√•r det ble gjort. La oss legge p√• noe slik metadata:\n\n# Leser inn filen\ndf = spark.read.format(\"delta\").load(\"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\n\n\n# Lagrer egendefinert metadata i en json-fil\nimport json\n\nmetadata = {\n    \"comment\": \"Kontaktet oppgavegiver og kranglet!\",\n    \"manueltEditert\": \"True\",\n    \"maskineltEditert\": \"False\",\n}\nmetadata\n\n{'comment': 'Kontaktet oppgavegiver og kranglet!',\n 'manueltEditert': 'True',\n 'maskineltEditert': 'False'}\n\n\nVi kan deretter appende dette til den siste versjonen av fila.\n\n(\n    df.write.format(\"delta\")\n    .mode(\"append\")\n    .option(\"userMetadata\", json.dumps(metadata))  # Serialize metadata to a string\n    .save(\"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\n)\n\n\n# Laster inn tabellen\ndeltaTable = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\n\n# Henter ut historien\nhistory_df = deltaTable.history()\n\n\n# Show the operation details, including metadata\nhistory_df.select(\n    \"version\", \"timestamp\", \"operation\", \"operationParameters\", \"userMetadata\"\n).show(truncate=10)\n\n+-------+----------+---------+-------------------+------------+\n|version| timestamp|operation|operationParameters|userMetadata|\n+-------+----------+---------+-------------------+------------+\n|      3|2023-10...|    WRITE|         {mode -...|  {\"comme...|\n|      2|2023-10...|    WRITE|         {mode -...|        null|\n|      1|2023-10...|   UPDATE|         {predic...|        null|\n|      0|2023-10...|    WRITE|         {mode -...|        null|\n+-------+----------+---------+-------------------+------------+\n\n\n\nhistory_df.select(\"version\", \"userMetadata\").show(truncate=50)\n\n+-------+--------------------------------------------------+\n|version|                                      userMetadata|\n+-------+--------------------------------------------------+\n|      3|{\"comment\": \"Kontaktet oppgavegiver og kranglet...|\n|      2|                                              null|\n|      1|                                              null|\n|      0|                                              null|\n+-------+--------------------------------------------------+\n\n\nVi ser at vi la til v√•r egen metadata i versjon 3 av fila. Vi kan printe ut den r√• transaksjonsloggen som tidligere, men n√• er vi p√• transaksjon 3 00000000000000000003.json:\n\nfrom dapla import FileClient\n\n# Kobler oss p√• b√∏ttene\nfs = FileClient.get_gcs_file_system()\n\n# Filsti\npath = \"gs://ssb-dapla-felles-data-produkt-prod/temp4/_delta_log/00000000000000000003.json\"\n\nwith fs.open(path, \"r\") as f:\n    for line in f:\n        data = json.loads(line)\n        pretty_content = json.dumps(data, indent=4)\n        print(pretty_content)\n        print(\"-\" * 50)  # Print separator for clarity\n\n{\n    \"commitInfo\": {\n        \"timestamp\": 1696942553907,\n        \"operation\": \"WRITE\",\n        \"operationParameters\": {\n            \"mode\": \"Append\",\n            \"partitionBy\": \"[]\"\n        },\n        \"readVersion\": 2,\n        \"isolationLevel\": \"Serializable\",\n        \"isBlindAppend\": false,\n        \"operationMetrics\": {\n            \"numFiles\": \"2\",\n            \"numOutputRows\": \"7\",\n            \"numOutputBytes\": \"989\"\n        },\n        \"userMetadata\": \"{\\\"comment\\\": \\\"Kontaktet oppgavegiver og kranglet!\\\", \\\"manueltEditert\\\": \\\"True\\\", \\\"maskineltEditert\\\": \\\"False\\\"}\",\n        \"engineInfo\": \"Apache-Spark/3.3.1 Delta-Lake/2.3.0\",\n        \"txnId\": \"e7de92bf-b0f9-4341-8bbb-9b382f2f3eb6\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00000-96369f3d-fe4a-4365-a0df-00c813027399-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 503,\n        \"modificationTime\": 1696942553856,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":5,\\\"minValues\\\":{\\\"id\\\":10},\\\"maxValues\\\":{\\\"id\\\":15},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00001-0f1bc8e6-093b-49a9-ad0b-78d5a148cfb6-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 486,\n        \"modificationTime\": 1696942553853,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":2,\\\"minValues\\\":{\\\"id\\\":20},\\\"maxValues\\\":{\\\"id\\\":21},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n\n\nVi er da at metadataene ligger som forventet i metadata-delen."
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.out.html",
    "href": "notebooks/spark/sparkr-intro.out.html",
    "title": "Introduksjon til SparkR",
    "section": "",
    "text": "Akkurat som PySpark s√• gir SparkR oss et grensesnitt mot Apache Spark fra R. I denne notebooken viser vi noen eksempler hvordan du gj√∏re vanlige operasjoner med SparkR."
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.out.html#oppsett",
    "href": "notebooks/spark/sparkr-intro.out.html#oppsett",
    "title": "Introduksjon til SparkR",
    "section": "Oppsett",
    "text": "Oppsett\nEksemplene i notebooken bruker SparkR (k8s cluster) p√• https://jupyter.dapla.ssb.no/. Det vil si at den kan distribuere kj√∏ringene p√• flere maskiner i Kubernetes.\n\nspark\n\nJava ref type org.apache.spark.sql.SparkSession id 1"
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.out.html#lese-inn-fil",
    "href": "notebooks/spark/sparkr-intro.out.html#lese-inn-fil",
    "title": "Introduksjon til SparkR",
    "section": "Lese inn fil",
    "text": "Lese inn fil\n\nfile = read.parquet(\"gs://ssb-dapla-felles-data-produkt-prod/temp/timeseries.parquet\")\n\n\nselectedColumns &lt;- select(file, \"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\")\nshowDF(selectedColumns, numRows = 5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Eksempler",
    "section": "",
    "text": "Eksempler\nSe menyen til venstre for eksempler.",
    "crumbs": [
      "Eksempler"
    ]
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.html",
    "href": "notebooks/spark/sparkr-intro.html",
    "title": "Introduksjon til SparkR",
    "section": "",
    "text": "Akkurat som PySpark s√• gir SparkR oss et grensesnitt mot Apache Spark fra R. I denne notebooken viser vi noen eksempler hvordan du gj√∏re vanlige operasjoner med SparkR."
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.html#oppsett",
    "href": "notebooks/spark/sparkr-intro.html#oppsett",
    "title": "Introduksjon til SparkR",
    "section": "Oppsett",
    "text": "Oppsett\nEksemplene i notebooken bruker SparkR (k8s cluster) p√• https://jupyter.dapla.ssb.no/. Det vil si at den kan distribuere kj√∏ringene p√• flere maskiner i Kubernetes.\n\nspark\n\nJava ref type org.apache.spark.sql.SparkSession id 1"
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.html#lese-inn-fil",
    "href": "notebooks/spark/sparkr-intro.html#lese-inn-fil",
    "title": "Introduksjon til SparkR",
    "section": "Lese inn fil",
    "text": "Lese inn fil\n\nfile = read.parquet(\"gs://ssb-dapla-felles-data-produkt-prod/temp/timeseries.parquet\")\n\n\nselectedColumns &lt;- select(file, \"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\")\nshowDF(selectedColumns, numRows = 5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html",
    "href": "notebooks/spark/pyspark-intro.out.html",
    "title": "Introduksjon til PySpark",
    "section": "",
    "text": "Apache Spark er et sterkt verkt√∏y som utvider mulighetsrommet for databehandling med R og Python. Kjernefunksjonaliteten ligger i at den lar deg kj√∏re en jobb p√• flere maskiner samtidig, noe som ikke er mulig med klassiske rammeverk som Pandas og Tidyverse. F√∏lgelig er det et rammeverk som blant annet er veldig egnet for √• prosessere store datamengder eller gj√∏re store beregninger. Les om mer om Apache Spark i Dapla-manualen\nI denne notebooken vises noen enkle eksempler p√• hvordan du kan jobbe med data med PySpark, et Python-grensesnitt mot Spark."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html#oppsett",
    "href": "notebooks/spark/pyspark-intro.out.html#oppsett",
    "title": "Introduksjon til PySpark",
    "section": "Oppsett",
    "text": "Oppsett\nN√•r du logger deg inn p√• Dapla kan du velge mellom 2 ferdigoppsatte kernels for √• jobbe med PySpark:\n\nPyspark (local)\nPyspark (k8s cluster)\n\nDen f√∏rste lar deg bruke Spark p√• en enkeltmaskin, mens den andre lar deg distribuere kj√∏ringen p√• mange maskiner avhengig av hvor store jobbene er. I eksemplene under brukes Pyspark (local).\n\n# Importer biblioteker\nimport numpy as np\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import date_format, explode, expr, sequence, sum, avg\nfrom pyspark.sql.types import DateType, DoubleType, StructField, StructType\n\n# Initialierer en SparkSession\nspark = (\n    SparkSession.builder.master(\"local[2]\")\n    .appName(\"Dapla-manual-example\")\n    .getOrCreate()\n)\n\nI koden over importerer vi de bibliotekene vi skal bruke under. Grunnen til at vi importerer pyspark.sql er at dette er at Spark SQL er Apache Spark sin modul for √• jobbe med strukturerte data. Og som navnet tilsier vil det si at vi kan blande Python og SQL i koden v√•r. Vi skal n√¶rmere p√• hvordan man bruke SQL fra PySpark-notebook senere.\nSpark tilbyr ogs√• et eget grensesnitt, Spark UI, for √• monitorere hva som skjer under en SparkSession. Vi kan bruke f√∏lgende kommando for √• f√• opp en lenke til Spark UI i notebooken v√•r:\n\nspark.sparkContext\n\n\n        \n            SparkContext\n\n            Spark UI\n\n            \n              Version\n                v3.3.1\n              Master\n                local[*]\n              AppName\n                pyspark-shell\n            \n        \n        \n\n\nKlikker du p√• Spark UI-lenken s√• tar den deg til dashboard som lar deg monitorere, debugge, optimalisere og forst√• kj√∏ringene dine. Det kan v√¶re et sv√¶rt nyttig verkt√∏y i mange tilfeller."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html#generere-data",
    "href": "notebooks/spark/pyspark-intro.out.html#generere-data",
    "title": "Introduksjon til PySpark",
    "section": "Generere data",
    "text": "Generere data\nVi kan begynne med √• generere en Spark DataFrame med en kolonne som inneholder m√•nedlige datoer for perioden 2000M1-2023M8.\n\n# Genererer m√•nedlige data\ndates_df = spark.range(1).select(\n    explode(\n        sequence(\n            start=expr(\"date '2000-01-01'\"),\n            stop=expr(\"date '2023-08-01'\"),\n            step=expr(\"interval 1 month\"),\n        )\n    ).alias(\"Date\")\n)\ndates_df.show(5)\n\n+----------+\n|      Date|\n+----------+\n|2000-01-01|\n|2000-02-01|\n|2000-03-01|\n|2000-04-01|\n|2000-05-01|\n+----------+\nonly showing top 5 rows\n\n\nEn Spark DataFrame er en distribuert samling av data som er organisert inn i kolonner. Siden Spark lar deg distribuere kj√∏ringer p√• flere maskiner, er DataFrames optimalisert for √• kunne splittes opp slik at de kan brukes p√• flere maskiner. Med andre er dette ikke det samme som en Pandas dataframe mange kjenner fra f√∏r.\nOver genererte vi en datokolonne. For √• f√• litt mer data kan vi ogs√• generere 100 kolonner med tidsseriedata og s√• printer vi de 2 f√∏rste av disse:\n\n# Genererer random walk data\nschema = StructType(\n    [StructField(f\"serie{i:02d}\", DoubleType(), True) for i in range(100)]\n)\n\ndata = [\n    tuple((10 + np.random.normal(0, 1, 100)).cumsum().tolist())\n    for _ in range(284)  # 284 months from 2000-01 to 2023-08\n]\n\ndata_df = spark.createDataFrame(data, schema=schema)\n\ndata_df.select(\"serie00\", \"serie01\").show(5)\n\n+------------------+------------------+\n|           serie00|           serie01|\n+------------------+------------------+\n|10.410703377184355| 21.06318801110079|\n|10.509249410154466|  19.5674295298024|\n| 9.618310122060274|17.635805093465642|\n| 9.691112692298294|18.593842915949082|\n| 9.903675228685067|20.012215769058564|\n+------------------+------------------+\nonly showing top 5 rows\n\n\nTil slutt kan vi joine de to datasettene sammen og lage noen kolonner som viser √•r, kvartal og m√•ned. Deretter printer vi ut noen av kolonnene med kommandoen show().\n\n# Legger til row index til DataFrame f√∏r join med dates_df\ndata_df = data_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n\n# Joiner de to datasettene\ndf = (\n    dates_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n    .join(data_df, \"row_index\")\n    .drop(\"row_index\")\n)\n\n# Legger til √•r, kvartal og mnd\ndf = df.withColumn(\"Year\", date_format(df.Date, \"yyyy\"))\ndf = df.withColumn(\"Quarter\", expr(\"quarter(Date)\"))\ndf = df.withColumn(\"Month\", date_format(df.Date, \"MM\"))\n\ndf.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows\n\n\nOg med det har vi noe data vi kan jobbe med i resten av notebooken."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html#skrive-til-parquet",
    "href": "notebooks/spark/pyspark-intro.out.html#skrive-til-parquet",
    "title": "Introduksjon til PySpark",
    "section": "Skrive til Parquet",
    "text": "Skrive til Parquet\nPySpark tilbyr mange opsjoner ved skriving til parquet-filer som vi kanskje ikke er vant til √• forholde oss til med enklere rammeverk som Pandas. Den enkleste m√•ten √• skrive ut en fil er som f√∏lger:\ndf.write.parquet(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp/timeseries.parquet\"\n)\nDette vil fungere hvis filen ikke finnes fra f√∏r. Hvis den finnes fra f√∏r s√• vil den feile. Grunnen er at vi ikke har spesifisert hva vi √∏nsker at den skal gj√∏re. Vi kan velge mellom overwrite, append, ignore eller errorifexists. Sistnevnte er ogs√• default-oppf√∏rsel hvis du ikke ber den gj√∏re noe annet.\nUnder bruker vi opsjonen overwrite, det vil si at den skriver over en evt eksisterende fil med samme navn.\n\ndf.write.mode(\"overwrite\").parquet(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp/timeseries.parquet\"\n)\n\nVi kan inspisere hva som ble skrevet ved √• liste ut innholder i b√∏tta.\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/temp/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp/',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/_SUCCESS',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/part-00000-b32e7299-0590-4b31-bcc2-dc3d58725529-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/part-00001-b32e7299-0590-4b31-bcc2-dc3d58725529-c000.snappy.parquet']\n\n\nHvis denne parquet-filen hadde v√¶rt partisjonert etter en kolonne, s√• ville det v√¶rt egne undermapper med navnestruktur column_name=value som indikerte hva filen er partisjonert p√•. Siden vi her bruker en maskin og har et lite datasett, valgte Spark √• ikke partisjonere."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html#lese-inn-parquet",
    "href": "notebooks/spark/pyspark-intro.out.html#lese-inn-parquet",
    "title": "Introduksjon til PySpark",
    "section": "Lese inn Parquet",
    "text": "Lese inn Parquet\nApache Spark kan lese inn flere parquet-filer samtidig. Syntaxen er like enkel som den for √• skrive ut.\n\ndf_ts = spark.read.parquet(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp/timeseries.parquet\"\n)\ndf_ts.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie66\", \"serie55\").show(5)\n\n+----------+----+-------+-----+-----------------+-----------------+\n|      Date|Year|Quarter|Month|          serie66|          serie55|\n+----------+----+-------+-----+-----------------+-----------------+\n|2000-01-01|2000|      1|   01|670.2679830025959|562.4312808525777|\n|2000-02-01|2000|      1|   02|675.4233411662802|562.5168447360121|\n|2000-03-01|2000|      1|   03|687.3412458214908|568.6203957584232|\n|2000-04-01|2000|      2|   04|673.1128047244955|557.4633871253379|\n|2000-05-01|2000|      2|   05| 667.513406101114|561.7766450346327|\n+----------+----+-------+-----+-----------------+-----------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html#pyspark-og-sql",
    "href": "notebooks/spark/pyspark-intro.out.html#pyspark-og-sql",
    "title": "Introduksjon til PySpark",
    "section": "PySpark og SQL",
    "text": "PySpark og SQL\nDu kan ogs√• skrive SQL med Spark. For √• skrive SQL m√• vi f√∏rst lage et temporary view. Under kaller vi viewt for tidsserie.\n\ndf.createOrReplaceTempView(\"tidsserie\")\n\nVi kan deretter skrive en SQL-statement som vi √∏nsker √• kj√∏re p√• viewet:\n\nquery = \"SELECT * FROM tidsserie WHERE Year = 2010\"\n\nDeretter kan vi bruke det til √• filtrere datasettet:\n\nresult_df = spark.sql(query)\nresult_df.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2010-01-01|2010|      1|   01| 11.26910423907778|21.730128215168268|\n|2010-02-01|2010|      1|   02| 8.722783282690738| 17.46851086792347|\n|2010-03-01|2010|      1|   03|10.376873608348605|20.109386343182802|\n|2010-04-01|2010|      2|   04|11.459959305590926|21.995141825523866|\n|2010-05-01|2010|      2|   05|10.441456792180572| 21.25096473981906|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html#aggregering",
    "href": "notebooks/spark/pyspark-intro.out.html#aggregering",
    "title": "Introduksjon til PySpark",
    "section": "Aggregering",
    "text": "Aggregering\nvi kan aggregere opp enten med PySpark-syntax eller SQL-syntax. Under viser vi begge:\n\nfrom pyspark.sql import functions as F\n\n# Assuming df_ts is your DataFrame\naggregated_df = df_ts.groupBy(\"Quarter\").agg(F.sum(\"serie00\").alias(\"Sum\"),\n                                             F.avg(\"serie00\").alias(\"Average\"),\n                                             F.max(\"serie00\").alias(\"Maximum\"))\n\n# Show the result\naggregated_df.show()\n\n+-------+------------------+------------------+------------------+\n|Quarter|               Sum|           Average|           Maximum|\n+-------+------------------+------------------+------------------+\n|      1|363.95869885234185|10.109963857009497|11.829453550532005|\n|      3|365.68324879453405| 10.15786802207039|12.233378837422391|\n|      4| 342.2334082209804|10.065688477087658|12.210138970053695|\n|      2|  361.991445506568|10.055317930738001|12.276030776082463|\n+-------+------------------+------------------+------------------+\n\n\nLa oss gj√∏re det samme med SQL, men grupperer etter to variabler og sorterer output etterp√•.\n\n# Assuming df_ts is your DataFrame\ndf_ts.createOrReplaceTempView(\"temp_table\")\n\n# Now you can run a SQL query\nquery = \"\"\"\n    SELECT\n        Year,\n        Quarter,\n        SUM(serie00) AS Sum,\n        AVG(serie00) AS Average,\n        MAX(serie00) AS Maximum\n    FROM \n        temp_table\n    GROUP BY \n        Year, Quarter\n    ORDER BY\n        YEAR, QUARTER\n\"\"\"\n\naggregated_df_sql = spark.sql(query)\n\n# Show the result\naggregated_df_sql.show(10)\n\n+----+-------+------------------+------------------+------------------+\n|Year|Quarter|               Sum|           Average|           Maximum|\n+----+-------+------------------+------------------+------------------+\n|2000|      1|31.323050432219453|10.441016810739818|11.118293927071951|\n|2000|      2|28.911192473027377| 9.637064157675793| 9.900977410685329|\n|2000|      3|33.670797229797415|11.223599076599138|12.233378837422391|\n|2000|      4|28.094793356286914| 9.364931118762305| 10.32000478359274|\n|2001|      1|31.636678535169537|10.545559511723178|11.367822302191831|\n|2001|      2|29.629770128521507| 9.876590042840503|11.135215930381191|\n|2001|      3| 30.75408440118315| 10.25136146706105|10.723803326978505|\n|2001|      4|30.361048932627902|  10.1203496442093|10.368365984482093|\n|2002|      1|31.184163218551227|10.394721072850409|10.550579652234951|\n|2002|      2|29.128978392451202|   9.7096594641504|10.186365745367246|\n+----+-------+------------------+------------------+------------------+\nonly showing top 10 rows"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html",
    "href": "notebooks/spark/pyspark-intro.html",
    "title": "Introduksjon til PySpark",
    "section": "",
    "text": "Apache Spark er et sterkt verkt√∏y som utvider mulighetsrommet for databehandling med R og Python. Kjernefunksjonaliteten ligger i at den lar deg kj√∏re en jobb p√• flere maskiner samtidig, noe som ikke er mulig med klassiske rammeverk som Pandas og Tidyverse. F√∏lgelig er det et rammeverk som blant annet er veldig egnet for √• prosessere store datamengder eller gj√∏re store beregninger. Les om mer om Apache Spark i Dapla-manualen\nI denne notebooken vises noen enkle eksempler p√• hvordan du kan jobbe med data med PySpark, et Python-grensesnitt mot Spark."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#oppsett",
    "href": "notebooks/spark/pyspark-intro.html#oppsett",
    "title": "Introduksjon til PySpark",
    "section": "Oppsett",
    "text": "Oppsett\nN√•r du logger deg inn p√• Dapla kan du velge mellom 2 ferdigoppsatte kernels for √• jobbe med PySpark:\n\nPyspark (local)\nPyspark (k8s cluster)\n\nDen f√∏rste lar deg bruke Spark p√• en enkeltmaskin, mens den andre lar deg distribuere kj√∏ringen p√• mange maskiner avhengig av hvor store jobbene er. I eksemplene under brukes Pyspark (local).\n\n# Importer biblioteker\nimport numpy as np\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import date_format, explode, expr, sequence, sum, avg\nfrom pyspark.sql.types import DateType, DoubleType, StructField, StructType\n\n# Initialierer en SparkSession\nspark = (\n    SparkSession.builder.master(\"local[2]\")\n    .appName(\"Dapla-manual-example\")\n    .getOrCreate()\n)\n\nI koden over importerer vi de bibliotekene vi skal bruke under. Grunnen til at vi importerer pyspark.sql er at dette er at Spark SQL er Apache Spark sin modul for √• jobbe med strukturerte data. Og som navnet tilsier vil det si at vi kan blande Python og SQL i koden v√•r. Vi skal n√¶rmere p√• hvordan man bruke SQL fra PySpark-notebook senere.\nSpark tilbyr ogs√• et eget grensesnitt, Spark UI, for √• monitorere hva som skjer under en SparkSession. Vi kan bruke f√∏lgende kommando for √• f√• opp en lenke til Spark UI i notebooken v√•r:\n\nspark.sparkContext\n\n\n        \n            SparkContext\n\n            Spark UI\n\n            \n              Version\n                v3.3.1\n              Master\n                local[*]\n              AppName\n                pyspark-shell\n            \n        \n        \n\n\nKlikker du p√• Spark UI-lenken s√• tar den deg til dashboard som lar deg monitorere, debugge, optimalisere og forst√• kj√∏ringene dine. Det kan v√¶re et sv√¶rt nyttig verkt√∏y i mange tilfeller."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#generere-data",
    "href": "notebooks/spark/pyspark-intro.html#generere-data",
    "title": "Introduksjon til PySpark",
    "section": "Generere data",
    "text": "Generere data\nVi kan begynne med √• generere en Spark DataFrame med en kolonne som inneholder m√•nedlige datoer for perioden 2000M1-2023M8.\n\n# Genererer m√•nedlige data\ndates_df = spark.range(1).select(\n    explode(\n        sequence(\n            start=expr(\"date '2000-01-01'\"),\n            stop=expr(\"date '2023-08-01'\"),\n            step=expr(\"interval 1 month\"),\n        )\n    ).alias(\"Date\")\n)\ndates_df.show(5)\n\n+----------+\n|      Date|\n+----------+\n|2000-01-01|\n|2000-02-01|\n|2000-03-01|\n|2000-04-01|\n|2000-05-01|\n+----------+\nonly showing top 5 rows\n\n\n\nEn Spark DataFrame er en distribuert samling av data som er organisert inn i kolonner. Siden Spark lar deg distribuere kj√∏ringer p√• flere maskiner, er DataFrames optimalisert for √• kunne splittes opp slik at de kan brukes p√• flere maskiner. Med andre er dette ikke det samme som en Pandas dataframe mange kjenner fra f√∏r.\nOver genererte vi en datokolonne. For √• f√• litt mer data kan vi ogs√• generere 100 kolonner med tidsseriedata og s√• printer vi de 2 f√∏rste av disse:\n\n# Genererer random walk data\nschema = StructType(\n    [StructField(f\"serie{i:02d}\", DoubleType(), True) for i in range(100)]\n)\n\ndata = [\n    tuple((10 + np.random.normal(0, 1, 100)).cumsum().tolist())\n    for _ in range(284)  # 284 months from 2000-01 to 2023-08\n]\n\ndata_df = spark.createDataFrame(data, schema=schema)\n\ndata_df.select(\"serie00\", \"serie01\").show(5)\n\n+------------------+------------------+\n|           serie00|           serie01|\n+------------------+------------------+\n|10.410703377184355| 21.06318801110079|\n|10.509249410154466|  19.5674295298024|\n| 9.618310122060274|17.635805093465642|\n| 9.691112692298294|18.593842915949082|\n| 9.903675228685067|20.012215769058564|\n+------------------+------------------+\nonly showing top 5 rows\n\n\n\nTil slutt kan vi joine de to datasettene sammen og lage noen kolonner som viser √•r, kvartal og m√•ned. Deretter printer vi ut noen av kolonnene med kommandoen show().\n\n\nCode\n# Legger til row index til DataFrame f√∏r join med dates_df\ndata_df = data_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n\n# Joiner de to datasettene\ndf = (\n    dates_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n    .join(data_df, \"row_index\")\n    .drop(\"row_index\")\n)\n\n# Legger til √•r, kvartal og mnd\ndf = df.withColumn(\"Year\", date_format(df.Date, \"yyyy\"))\ndf = df.withColumn(\"Quarter\", expr(\"quarter(Date)\"))\ndf = df.withColumn(\"Month\", date_format(df.Date, \"MM\"))\n\ndf.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows\n\n\n\nOg med det har vi noe data vi kan jobbe med i resten av notebooken."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#skrive-til-parquet",
    "href": "notebooks/spark/pyspark-intro.html#skrive-til-parquet",
    "title": "Introduksjon til PySpark",
    "section": "Skrive til Parquet",
    "text": "Skrive til Parquet\nPySpark tilbyr mange opsjoner ved skriving til parquet-filer som vi kanskje ikke er vant til √• forholde oss til med enklere rammeverk som Pandas. Den enkleste m√•ten √• skrive ut en fil er som f√∏lger:\ndf.write.parquet(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp/timeseries.parquet\"\n)\nDette vil fungere hvis filen ikke finnes fra f√∏r. Hvis den finnes fra f√∏r s√• vil den feile. Grunnen er at vi ikke har spesifisert hva vi √∏nsker at den skal gj√∏re. Vi kan velge mellom overwrite, append, ignore eller errorifexists. Sistnevnte er ogs√• default-oppf√∏rsel hvis du ikke ber den gj√∏re noe annet.\nUnder bruker vi opsjonen overwrite, det vil si at den skriver over en evt eksisterende fil med samme navn.\n\ndf.write.mode(\"overwrite\").parquet(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp/timeseries.parquet\"\n)\n\nVi kan inspisere hva som ble skrevet ved √• liste ut innholder i b√∏tta.\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/temp/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp/',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/_SUCCESS',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/part-00000-b32e7299-0590-4b31-bcc2-dc3d58725529-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/part-00001-b32e7299-0590-4b31-bcc2-dc3d58725529-c000.snappy.parquet']\n\n\nHvis denne parquet-filen hadde v√¶rt partisjonert etter en kolonne, s√• ville det v√¶rt egne undermapper med navnestruktur column_name=value som indikerte hva filen er partisjonert p√•. Siden vi her bruker en maskin og har et lite datasett, valgte Spark √• ikke partisjonere."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#lese-inn-parquet",
    "href": "notebooks/spark/pyspark-intro.html#lese-inn-parquet",
    "title": "Introduksjon til PySpark",
    "section": "Lese inn Parquet",
    "text": "Lese inn Parquet\nApache Spark kan lese inn flere parquet-filer samtidig. Syntaxen er like enkel som den for √• skrive ut.\n\ndf_ts = spark.read.parquet(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp/timeseries.parquet\"\n)\ndf_ts.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie66\", \"serie55\").show(5)\n\n+----------+----+-------+-----+-----------------+-----------------+\n|      Date|Year|Quarter|Month|          serie66|          serie55|\n+----------+----+-------+-----+-----------------+-----------------+\n|2000-01-01|2000|      1|   01|670.2679830025959|562.4312808525777|\n|2000-02-01|2000|      1|   02|675.4233411662802|562.5168447360121|\n|2000-03-01|2000|      1|   03|687.3412458214908|568.6203957584232|\n|2000-04-01|2000|      2|   04|673.1128047244955|557.4633871253379|\n|2000-05-01|2000|      2|   05| 667.513406101114|561.7766450346327|\n+----------+----+-------+-----+-----------------+-----------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#pyspark-og-sql",
    "href": "notebooks/spark/pyspark-intro.html#pyspark-og-sql",
    "title": "Introduksjon til PySpark",
    "section": "PySpark og SQL",
    "text": "PySpark og SQL\nDu kan ogs√• skrive SQL med Spark. For √• skrive SQL m√• vi f√∏rst lage et temporary view. Under kaller vi viewt for tidsserie.\n\ndf.createOrReplaceTempView(\"tidsserie\")\n\nVi kan deretter skrive en SQL-statement som vi √∏nsker √• kj√∏re p√• viewet:\n\nquery = \"SELECT * FROM tidsserie WHERE Year = 2010\"\n\nDeretter kan vi bruke det til √• filtrere datasettet:\n\nresult_df = spark.sql(query)\nresult_df.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2010-01-01|2010|      1|   01| 11.26910423907778|21.730128215168268|\n|2010-02-01|2010|      1|   02| 8.722783282690738| 17.46851086792347|\n|2010-03-01|2010|      1|   03|10.376873608348605|20.109386343182802|\n|2010-04-01|2010|      2|   04|11.459959305590926|21.995141825523866|\n|2010-05-01|2010|      2|   05|10.441456792180572| 21.25096473981906|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#aggregering",
    "href": "notebooks/spark/pyspark-intro.html#aggregering",
    "title": "Introduksjon til PySpark",
    "section": "Aggregering",
    "text": "Aggregering\nvi kan aggregere opp enten med PySpark-syntax eller SQL-syntax. Under viser vi begge:\n\nfrom pyspark.sql import functions as F\n\n# Assuming df_ts is your DataFrame\naggregated_df = df_ts.groupBy(\"Quarter\").agg(F.sum(\"serie00\").alias(\"Sum\"),\n                                             F.avg(\"serie00\").alias(\"Average\"),\n                                             F.max(\"serie00\").alias(\"Maximum\"))\n\n# Show the result\naggregated_df.show()\n\n+-------+------------------+------------------+------------------+\n|Quarter|               Sum|           Average|           Maximum|\n+-------+------------------+------------------+------------------+\n|      1|363.95869885234185|10.109963857009497|11.829453550532005|\n|      3|365.68324879453405| 10.15786802207039|12.233378837422391|\n|      4| 342.2334082209804|10.065688477087658|12.210138970053695|\n|      2|  361.991445506568|10.055317930738001|12.276030776082463|\n+-------+------------------+------------------+------------------+\n\n\n\nLa oss gj√∏re det samme med SQL, men grupperer etter to variabler og sorterer output etterp√•.\n\n# Assuming df_ts is your DataFrame\ndf_ts.createOrReplaceTempView(\"temp_table\")\n\n# Now you can run a SQL query\nquery = \"\"\"\n    SELECT\n        Year,\n        Quarter,\n        SUM(serie00) AS Sum,\n        AVG(serie00) AS Average,\n        MAX(serie00) AS Maximum\n    FROM \n        temp_table\n    GROUP BY \n        Year, Quarter\n    ORDER BY\n        YEAR, QUARTER\n\"\"\"\n\naggregated_df_sql = spark.sql(query)\n\n# Show the result\naggregated_df_sql.show(10)\n\n+----+-------+------------------+------------------+------------------+\n|Year|Quarter|               Sum|           Average|           Maximum|\n+----+-------+------------------+------------------+------------------+\n|2000|      1|31.323050432219453|10.441016810739818|11.118293927071951|\n|2000|      2|28.911192473027377| 9.637064157675793| 9.900977410685329|\n|2000|      3|33.670797229797415|11.223599076599138|12.233378837422391|\n|2000|      4|28.094793356286914| 9.364931118762305| 10.32000478359274|\n|2001|      1|31.636678535169537|10.545559511723178|11.367822302191831|\n|2001|      2|29.629770128521507| 9.876590042840503|11.135215930381191|\n|2001|      3| 30.75408440118315| 10.25136146706105|10.723803326978505|\n|2001|      4|30.361048932627902|  10.1203496442093|10.368365984482093|\n|2002|      1|31.184163218551227|10.394721072850409|10.550579652234951|\n|2002|      2|29.128978392451202|   9.7096594641504|10.186365745367246|\n+----+-------+------------------+------------------+------------------+\nonly showing top 10 rows"
  },
  {
    "objectID": "faq-dapla-lab.html",
    "href": "faq-dapla-lab.html",
    "title": "Sp√∏rsm√•l og svar for Dapla Lab",
    "section": "",
    "text": "Hvor finner jeg dokumentasjonen til Dapla Lab tjenester der?\nHer finner du Dokumentasjonen for Dapla Lab.\nDokumentasjon for tjenestene finner du i Dapla-manualen under Datatjenester/Dapla Lab tjenester.\n\n\nHvorfor f√•r jeg feilmelding i Dapla Lab etter √• ha v√¶rt borte fra datamaskinen min en stund?\nI de fleste tilfeller vil dette l√∏ses seg ved at laster inn siden p√• nytt eller refresher siden.\nGrunnen til at det skjer er at Dapla Lab bare er tilgjengelig for de som er logget p√• SSBs nettverk. Hvis du forlater maskinen med Dapla Lab √•pnet i nettleseren over lang tid, s√• kan det v√¶re at du har falt ut av SSBs nettverk og Dapla Lab ikke oppdager at du er logget inn igjen.\n\n\nHva betyr det at tjenesten min er suspendert?\n\nAt en tjeneste er suspendert betyr at tjenesten er satt p√• pause. I praksis vil det si at ressursene, f.eks. CPU og RAM, som tjenesten har reservert har blitt frigjort. Man kan starte tjenesten p√• nytt ved √• trykke p√• Play-knappen i bildet til h√∏yre, og man starter opp tjenesten p√• nytt med de samme ressursene. Alt som ble lagret i tjenestens lokale filsystem under /home/onyxia/work/blir ogs√• gjennopprettet, mens resten blir borte. Derfor b√∏r alltid kode lagres under work-mappen.\n\n\nKan jeg jobbe med kildedata fra Dapla Lab?\nJa. Medlemmer av gruppen data-admins kan gi seg selv midlertidig tilgang til kildedata. Dette gj√∏res ved √• bruke Just-in-Time Access (JIT) applikasjonen: https://jitaccess.dapla.ssb.no. Les manual-artikkelen om JIT for en veiledning.\n\n\nHvorfor kan jeg ikke jobbe med dataene til alle team jeg er med i fra tjenestene i Dapla Lab?\nMange ansatte i SSB er med i flere Dapla-team, og hvert av brukernes teammedlemskap gir ofte tilgang til data iht til forma√•let for teamet. hver av teamene har tilgang til sine data og potensielt andres delt data. For √• unng√• for brede tilganger til data s√• innf√∏rer vi at en bruker m√• bestemme seg for hvilket team og tilgangsgruppe de skal representere f√∏r de starter arbeid med data i et programmeringsmilj√∏. Dette gj√∏r at vi kan v√¶re sikker p√• at ingen kan koble data p√• tvers av team.\n\n\nHvordan jobber jeg med notebooks i VS Code?\nFor √• jobbe med notebooks i VS Code kan du gj√∏re f√∏lgende:\n\n√Öpne en ny Vscode-python\n√Öpne en terminal under File/Terminal/New terminal\nOpprett et ssb-project som vanlig under mappen /home/onyxia/work/ ved √• skrive ssb-project create &lt;prosjektnavn&gt;.\n\nDersom du allerede har et ssb-project, skriver du cd &lt;prosjektnavn&gt; etterfulgt av ssb-project build. Da blir prosjektets tilh√∏rende pakker installert i en kernel med samme navn som prosjektet.\n\nG√• inn i mappen med cd &lt;prosjektnavn&gt;\nInstaller √∏nskede pakker poetry add pandas, f.eks. Pandas.\nVelg prosjektmappen i filutforskeren ved √• g√• til File/Open folder/ og √•pne prosjektmappen i boksen som vises.\nOpprett en notebook-fil og √•pne den.\nTrykk Refresh p√• hele nettsiden for √• f√• opp visningen av den nyopprettede kernels.\n√òverst til h√∏yre i Notebooken kan du trykke p√• Select kernel og utforskeren velger du Jupyter kernels og deretter den som har samme navn som prosjektet ditt.\nTil slutt kan du teste at riktig Python-installasjon er valgt ved √• importere pakken du akkurat installerte.\n\nDu m√• kj√∏re en ssb-project build, som beskrevet over, hver gang en instans av Jupyter, eller VSCode blir startet. Hvis du isteden √•pner en pauset (suspendert) instans, vil du kunne fortsette arbeidet fra forrige kj√∏ring uten √• m√•tte kj√∏re ssb-project build.\n\n\nHvordan kj√∏rer man en Dash-app?\n\nFra vscode-python tjenesten\nDet fungerer best √• kj√∏re Dash-apper i en egen fane i nettleseren. Inline-visning av figurer fungerer ikke enda.\nHer er et eksempel p√• hvordan man lager en Dash-app i DaplaLab i en vscode tjeneste:\n\nInstaller pandas og dash i et ssb-project.\nOpprett et python-script i prosjektet som f.eks. heter app.py.\nKj√∏r scriptet ved √• kj√∏re f√∏lgende kommando fra terminalen: poetry run python ./app.py\nDeretter kommer det opp et dialog-vinduet hvor du velger Open in browser.\n\nHer er et eksempel p√• script som fungerer i Vscode-python:\n\n\napp.py\n\nfrom dash import Dash, html, dcc, callback, Output, Input\nimport plotly.express as px\nimport pandas as pd\n\n# Creating a sample dataset similar to the Gapminder dataset\ndata = {\n    'country': ['Afghanistan', 'Afghanistan', 'Afghanistan', 'Brazil', 'Brazil', 'Brazil', 'China', 'China', 'China'],\n    'continent': ['Asia', 'Asia', 'Asia', 'South America', 'South America', 'South America', 'Asia', 'Asia', 'Asia'],\n    'year': [1952, 1977, 2007, 1952, 1977, 2007, 1952, 1977, 2007],\n    'lifeExp': [28.801, 39.877, 43.828, 50.917, 59.504, 72.390, 44.000, 63.739, 72.961],\n    'pop': [8425333, 12412311, 31889923, 56602560, 80461570, 190010647, 556263527, 862030000, 1318683096],\n    'gdpPercap': [779.4453145, 786.11336, 974.5803384, 2108.944355, 5718.766744, 9065.800825, 400.448611, 1488.041713, 4959.114854]\n}\n\n# Creating DataFrame\ndf = pd.DataFrame(data)\n\n# Creating the app\napp = Dash(\n    __name__,\n    requests_pathname_prefix='/proxy/8050/', \n    serve_locally=True\n)\n\napp.layout = [\n    html.H1(children='Title of Dash App', style={'textAlign':'center'}),\n    dcc.Dropdown(df.country.unique(), 'Canada', id='dropdown-selection'),\n    dcc.Graph(id='graph-content')\n]\n\n@callback(\n    Output('graph-content', 'figure'),\n    Input('dropdown-selection', 'value')\n)\ndef update_graph(value):\n    dff = df[df.country==value]\n    return px.line(dff, x='year', y='pop')\n\nif __name__ == '__main__':\n    app.run(debug=True)\n\n\n\nFra jupyter tjenesten\nDet fungerer best √• kj√∏re Dash-apper i en egen fane i nettleseren, men inline-visning skal ogs√• v√¶re mulig i notebook.\nHer er et eksempel p√• hvordan man lager en Dash-app i DaplaLab i en jupyter tjeneste:\n\nInstaller pandas og dash i et ssb-project.\nOpprett en notebook i prosjektet som f.eks. heter app.ipynb.\n\nHer er et eksempel p√• kode som fungerer i jupyter:\n\n\napp.ipynb\n\n# %%\n# Notebook cell 1\nfrom dash import Dash, html, dcc, callback, Output, Input\nimport plotly.express as px\nimport pandas as pd\n\n# %%\n# Notebook cell 2\n# Creating a sample dataset similar to the Gapminder dataset\ndata = {\n    'country': ['Afghanistan', 'Afghanistan', 'Afghanistan', 'Brazil', 'Brazil', 'Brazil', 'China', 'China', 'China'],\n    'continent': ['Asia', 'Asia', 'Asia', 'South America', 'South America', 'South America', 'Asia', 'Asia', 'Asia'],\n    'year': [1952, 1977, 2007, 1952, 1977, 2007, 1952, 1977, 2007],\n    'lifeExp': [28.801, 39.877, 43.828, 50.917, 59.504, 72.390, 44.000, 63.739, 72.961],\n    'pop': [8425333, 12412311, 31889923, 56602560, 80461570, 190010647, 556263527, 862030000, 1318683096],\n    'gdpPercap': [779.4453145, 786.11336, 974.5803384, 2108.944355, 5718.766744, 9065.800825, 400.448611, 1488.041713, 4959.114854]\n}\n\n# Creating DataFrame\ndf = pd.DataFrame(data)\n\n# Creating the app\nport = 8050 # must be between 1024 and 9999\nservice_prefix = os.getenv('JUPYTERHUB_SERVICE_PREFIX', '/')\ndomain = os.getenv('JUPYTERHUB_HTTP_REFERER', None)\n\napp = Dash(\n    __name__,\n    requests_pathname_prefix=f\"{service_prefix}proxy/{port}/\", \n    serve_locally=True\n)\n\napp.layout = [\n    html.H1(children='Title of Dash App', style={'textAlign':'center'}),\n    dcc.Dropdown(df.country.unique(), 'Canada', id='dropdown-selection'),\n    dcc.Graph(id='graph-content')\n]\n\n@callback(\n    Output('graph-content', 'figure'),\n    Input('dropdown-selection', 'value')\n)\ndef update_graph(value):\n    dff = df[df.country==value]\n    return px.line(dff, x='year', y='pop')\n\nif __name__ == '__main__':\n    # For jupyter_mode, choose between 'external' or 'inline'.\n    # 'jupyterlab' should also be poosible, but doesn't seem to work...\n    app.run(jupyter_mode=\"external\", jupyter_server_url=domain, port=port)\n\n\n\n\nHvilke milj√∏variabler er tilgjengelig i utviklingstjenestene?\nDette er milj√∏variablene som alltid vil v√¶re tilgjengelig i Jupyterlab, Rstudio og VSCode:\n\n\n\n\n\n\n\nMilj√∏variabel\nForklaring\n\n\n\n\nDAPLA_ENVIRONMENT\nHvilket dapla-lab milj√∏ du kj√∏rer i f.eks PROD, TEST eller DEV\n\n\nDAPLA_REGION\nHvilket dapla milj√∏ du kj√∏rer i f.eks. DAPLA_LAB eller BIP\n\n\nDAPLA_USER\nEpostaddressen til den innloggede brukeren f.eks. obr@ssb.no\n\n\nDAPLA_SERVICE\nDapla tjenesten som er i bruk f.eks. JUPYTERLAB eller VS_CODE\n\n\nSTAT_TEMPLATE_DEFAULT_REFERENCE\nHvilken versjon av ssb-project templatet som skal brukes f.eks. 1.1.8\n\n\nPSEUDO_SERVICE_URL\nURL til pseudotjenesten for bruk til testing\n\n\nSTATBANK_BASE_URL\nURL til statistikkbanken\n\n\nSTATBANK_ENCRYPT_URL\nURL til ‚Äòstatbank-authenticator‚Äô sitt krypteringsendepunkt"
  },
  {
    "objectID": "utviklere/lenker.html",
    "href": "utviklere/lenker.html",
    "title": "Lenker",
    "section": "",
    "text": "Dokumentasjon for utviklere i SSB er spredt over mange ulike sider. Denne siden er en oversikt over dokumentasjonen som finnes.\n\n\n\nSlack\nGitHub\nConfluence\nByr√•nettet\n\n\n\n\n\n\nhttps://backstage.intern.ssb.no\n\nProgramvarekatalog\nTeamkatalog\nTeknologiradar\n\n\n\n\n\nOffisielle docs fra nav: https://docs.ssb.cloud.nais.io/\nIntern system docs: https://statisticsnorway.github.io/nais-system (krever GitHub innlogging)\nNais console: https://console.ssb.cloud.nais.io/ (krever naisdevice)\n\n\n\n\n\nDapla ctrl (teamkatalog): https://dapla-ctrl.intern.ssb.no/\n\n\n\n\n\nhttps://lab.dapla.ssb.no/\nIntern system docs: https://statisticsnorway.github.io/dapla-lab-system (krever GitHub innlogging)\nBrukerdokumentasjon: https://manual.dapla.ssb.no/statistikkere/dapla-lab.html\n\n\n\n\n\nhttps://docs.bip.ssb.no (krever GitHub innlogging)\nhttps://docs.dapla.ssb.no/ (krever GitHub innlogging)\n\n\n\n\n\n\nArkitekturprinsipper\nArchitecture Decision Records (krever GitHub innlogging)\nTeknologiradar (krever naisdevice)\n\n\n\n\nGenerellt er utviklere i SSB veldig fri til √• velge sin lokal dev-oppsett utifra preferanser og teknologivalg. F√∏lgende lenker g√•r p√• SSB spesifikk oppsett, s√¶rlig relatert til nettverkstilgang og sikkerhet ellers.\n\nKundeservice FAQ\n\nWiFi\nMicrosoft Authenticator\nVPN\nEpost\nPassordbytte\n\nGit/GitHub\nIntelliJ lisenser\nInstaller naisdevice\n\n\n\n\n\n\n\nDependabot\nSnyk\nDetectify\nSonarcloud\nMabl\n\n\n\n\n\n\nOperasjonell modell\nIT Avdelingens leveranse demo (finner sted en gang i m√•neden p√• fredag)",
    "crumbs": [
      "Utviklere",
      "Lenker"
    ]
  },
  {
    "objectID": "utviklere/lenker.html#samhandling",
    "href": "utviklere/lenker.html#samhandling",
    "title": "Lenker",
    "section": "",
    "text": "Slack\nGitHub\nConfluence\nByr√•nettet",
    "crumbs": [
      "Utviklere",
      "Lenker"
    ]
  },
  {
    "objectID": "utviklere/lenker.html#plattform",
    "href": "utviklere/lenker.html#plattform",
    "title": "Lenker",
    "section": "",
    "text": "https://backstage.intern.ssb.no\n\nProgramvarekatalog\nTeamkatalog\nTeknologiradar\n\n\n\n\n\nOffisielle docs fra nav: https://docs.ssb.cloud.nais.io/\nIntern system docs: https://statisticsnorway.github.io/nais-system (krever GitHub innlogging)\nNais console: https://console.ssb.cloud.nais.io/ (krever naisdevice)\n\n\n\n\n\nDapla ctrl (teamkatalog): https://dapla-ctrl.intern.ssb.no/\n\n\n\n\n\nhttps://lab.dapla.ssb.no/\nIntern system docs: https://statisticsnorway.github.io/dapla-lab-system (krever GitHub innlogging)\nBrukerdokumentasjon: https://manual.dapla.ssb.no/statistikkere/dapla-lab.html\n\n\n\n\n\nhttps://docs.bip.ssb.no (krever GitHub innlogging)\nhttps://docs.dapla.ssb.no/ (krever GitHub innlogging)",
    "crumbs": [
      "Utviklere",
      "Lenker"
    ]
  },
  {
    "objectID": "utviklere/lenker.html#arkitektur",
    "href": "utviklere/lenker.html#arkitektur",
    "title": "Lenker",
    "section": "",
    "text": "Arkitekturprinsipper\nArchitecture Decision Records (krever GitHub innlogging)\nTeknologiradar (krever naisdevice)",
    "crumbs": [
      "Utviklere",
      "Lenker"
    ]
  },
  {
    "objectID": "utviklere/lenker.html#oppsett",
    "href": "utviklere/lenker.html#oppsett",
    "title": "Lenker",
    "section": "",
    "text": "Generellt er utviklere i SSB veldig fri til √• velge sin lokal dev-oppsett utifra preferanser og teknologivalg. F√∏lgende lenker g√•r p√• SSB spesifikk oppsett, s√¶rlig relatert til nettverkstilgang og sikkerhet ellers.\n\nKundeservice FAQ\n\nWiFi\nMicrosoft Authenticator\nVPN\nEpost\nPassordbytte\n\nGit/GitHub\nIntelliJ lisenser\nInstaller naisdevice",
    "crumbs": [
      "Utviklere",
      "Lenker"
    ]
  },
  {
    "objectID": "utviklere/lenker.html#test-og-kvalitet",
    "href": "utviklere/lenker.html#test-og-kvalitet",
    "title": "Lenker",
    "section": "",
    "text": "Dependabot\nSnyk\nDetectify\nSonarcloud\nMabl",
    "crumbs": [
      "Utviklere",
      "Lenker"
    ]
  },
  {
    "objectID": "utviklere/lenker.html#prosess",
    "href": "utviklere/lenker.html#prosess",
    "title": "Lenker",
    "section": "",
    "text": "Operasjonell modell\nIT Avdelingens leveranse demo (finner sted en gang i m√•neden p√• fredag)",
    "crumbs": [
      "Utviklere",
      "Lenker"
    ]
  },
  {
    "objectID": "om-dapla.html",
    "href": "om-dapla.html",
    "title": "Om Dapla",
    "section": "",
    "text": "Om Dapla\nDapla st√•r for dataplattform, og er en skybasert l√∏sning for statistikkproduksjon og forskning.\n\n\nHvorfor Dapla?\nSom dataplattform skal Dapla stimulerere til √∏kt kvalitet p√• statistikk og forskning, samtidig som den gj√∏r organisasjonen mer tilpasningsdyktig i m√∏te med fremtiden.\n\n\nDen nye skybaserte dataplattformen (Dapla) skal bli viktig for √• effektivisere arbeids-og produksjonsprosesser, den skal sikre effektiv lagring og gjenfinning av data og metadata, og st√∏tte opp under deling av data p√• tvers av statistikkomr√•der.\n\nKilde: Langtidsplan for SSB (2022-2024)\n\n\n\nM√•let med Dapla er √• tilby tjenester og verkt√∏y som lar statistikkprodusenter og forskere produsere resultater p√• en sikker og effektiv m√•te."
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html",
    "href": "statistikkere/maskinporten-guardian.html",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "Maskinporten er en tjeneste fra Digdir som gir sikker autentisering og tilgangskontroll for datautveksling mellom virksomheter.\nTilgangsstyring for data administeres via et webgrensesnitt (Samarbeidsportalen) En virksomhet som deler data (API-tilbyder) definerer s√•kalte API scopes, og velger hvilke andre virksomheter (API-konsumenter) som skal ha tilgang til disse. API-konsumenter kan p√• sin side selv opprette en eller flere Maskinporten-klienter og gir de tilgang til API scopes som er delt med virksomheten.\n\n\n\n\n\n\nFigur¬†1: Datauveksling mellom virksomheter\n\n\n\nLes mer om Maskinporten her.\n\n\n\nUtveksling av data fra en API-tilbyder gj√∏res ved √• inkludere et sikkerhetstoken som hentes fra Maskinporten p√• vegne av virksomheten man representerer (f.¬†eks SSB). Hvilke data (API scopes) et sikkerhetstoken har tilgang til er knyttet til Maskinporten-klienten. For √• hente et sikkerhetstoken for en klient, kreves det at man autentiserer seg som virksomhet. Dette gj√∏res ved √• signere foresp√∏rsler til Maskinporten ved bruk av et virksomhetssertifikat. Det er her Maskinporten Guardian kommer inn i bildet.\nMaskinporten Guardian har tilgang til SSBs virksomhetssertifikat og kan dermed signere foresp√∏rsler mot Maskinporten for √• hente ut sikkerhetstokens.\n\n\n\nHvis en skal ta i bruk et API som er beskyttet av Maskinporten er det noen steg som m√• gj√∏res i forkant:\n\n\n\n\n\nflowchart TD\n    A[API-tilbyder gir SSB tilgang til API via Samarbeidsportalen]\n    A --&gt; B[M2M-teamet hos SSB oppretter Maskinporten-klienter]\n    B --&gt; C[Keycloak-klienter for Maskinporten Guardian opprettes]\n\n\n\n\n\n\n\nAPI-tilbyderen m√• gi SSB tilgang til et API scope. API-tilbydere kan gj√∏re dette via Samarbeidsportalen hos Digdir. Fra API-tilbyderen f√•r du ogs√• annen informasjon om API-ene, som:\n\n\nURL-er som skal benyttes for √• snakke med API-et\nNavn p√• API scopes\nOm det finnes testadata\nDokumentasjon for API-endepunktene\n\n\nN√•r data er delt av en API-tilbyder, og en har navnet p√• API scopes, kan M2M-teamet hos SSB kontaktes for √• f√• opprettet Maskinporten-klienter, √©n pr milj√∏ (f.¬†eks prod og test). De m√• vite hvilke API scopes og milj√∏er (test/prod) som skal benyttes. M2M-teamet vil gi deg ID-er (f.¬†eks 12345678-9abc-def0-1234-567890abcdef) for klientene som er blitt opprettet.\n\n\n\n\n\n\n\nDu kan anse ID for en Maskinporten-klient som et slags brukernavn, og behandle dette deretter. Slike ID-er er ikke sensitive i seg selv, men de b√∏r allikevel ikke ligge i √•pne git-repoer (SSB-private repoer er OK). Det er heller ingen grunn til √• behandle disse som hemmeligheter. Det anbefales at man opererer med Maskinporten-klienter som ekstern konfigurasjon til koden, f.¬†eks p√• samme m√•te som man behandler URL-er til API-ene.\n\n\n\n\nN√•r du har ID for Maskinporten-klienten(e), er neste steg √• f√• opprettet en Keycloak-bruker for Maskinporten Guardian. Se Opprette en Maskinporten Guardian M2M-bruker. Ta kontakt med oss p√• Dapla via Pureservice dersom du trenger hjelp til dette. Client ID og client secret for Keycloak-brukeren kan hentes fra Secret Manager (mer om det lenger ned). Medlemmer av Dapla-teamet har tilgang til √• hente disse.\n\nOm noen p√• teamet trenger personlig tilgang til API-ene s√• m√• det konfigureres i Maskinporten Guardian. Da m√• vi vite hvilke personer som skal ha denne tilgangen. Les mer om forskjellen p√• M2M og personlig tilgang lenger ned.\n\n\n\nDet er M2M-teamet i SSB som administrerer det formelle i forbindelse med integrasjoner mot eksterne API-er. For √• f√• opprettet nye Maskinporten-klienter er det dette teamet man kontakter. De m√• vite hvilke milj√∏er (test og/eller prod) og hvilke API scopes som skal benyttes. Hver Maskinporten-klient som opprettes identifiseres av en ID (f.¬†eks 12345678-9abc-def0-1234-567890abcdef), som du vil f√• tilsendt.\nLegg merke til at det er SSB selv som oppretter og administrerer Maskinporten-klienter. Det gj√∏res i Samarbeidsportalen hos Difi. Klientene knyttes til API scope(s) (som er delt med SSB av API-tilbyderen). Du kan lese mer om hvordan M2M-teamet administrerer Maskinporten-klienter her.\n\n\n\n\n\n\nAlle med en SSB-epostadresse kan registerere en personlig bruker i Digdir Samarbeidsportalen for √• f√• innsyn i hvilke API-integrasjoner som finnes.\n\n\n\n\n\n\nMaskinporten Guardian er tilgjengelig fra alle SSB og NAIS sine IP-adresser, og kan n√•s p√•:\n\nProd: https://guardian.intern.ssb.no\nTest: https://guardian.intern.test.ssb.no\n\nMaskinporten Guardian sine endepunkter er selv beskyttet av Keycloak. To typer brukere st√∏ttes:\n\nMaskin-til-maskin (M2M) - Systembruker knyttet til en gitt Maskinporten-klient. For √• opptre p√• vegne av en M2M-bruker autentiserer man seg med en Keycloak client secret. Denne hemmeligheten er lagret i Google Secret Manager og kun Service Accounts eller Dapla-grupper med tilgang kan hente den ut.\nPersonlig - Din egen SSB-bruker. Man kan f.¬†eks bruke dapla-toolbelt for √• hente ut sitt personlige Keycloak-token. I tillegg til √• autentisere deg m√• din bruker v√¶re autorisert til √• gj√∏re oppslag p√• vegne av en Maskinporten-klient. Dette styres i konfigurasjonen til Maskinporten Guardian.\n\n\n\n\n\n\n\nMan skal i hovedsak kun anvende M2M-brukere for datautveksling mot API-er som er beskyttet av Maskinporten. Personlige brukere skal kun brukes unntaksvis for enkeltoppslag (f.¬†eks ved feils√∏king) mot API-er eller for utvikling og test.\n\n\n\nLegg merke til at Maskinporten Guardian kun er tilgjengelig fra interne SSB-adresser. Bruk f√∏lgende URL-er: * Prod: https://guardian.intern.ssb.no * Test: https://guardian.intern.test.ssb.no\n\n\n\nF√∏lgende gir en oversikt over hvordan systemer henger sammen. En API-konsument kan f.¬†eks v√¶re et Dapla-team som √∏nsker √• hente data, mens en API-tilbyder er en ekstern virksomhet som tilbyr data via Maskinporten. Det er noen forskjeller i flyt avhengig av om Maskinporten Guardian aksesseres med systembruker (M2M) eller personlig bruker.\n\n\n\n\n\n\nFigur¬†2\n\n\n\n\n\n\nAPI-konsumenten henter sin Keycloak client secret for en gitt Maskinporten-klient fra Secret Manager.\nAPI-konsumenten henter et Keycloak sikkerhetstoken ved √• bruke client secret fra steg 1.\nAPI-konsumenten henter et Maskinporten sikkerhetstoken fra Maskinporten Guardian ved √• bruke Keycloak sikkerhetstoken fra steg 2. En kan alternativt angi andre API scopes enn det som er standard for Maskinporten-klienten, men dette er vanligvis ikke n√∏dvendig.\nMaskinporten Guardian bruker SSB sitt virksomhetssertifikat til √• signere en foresp√∏rsel om √• hente et sikkerhetstoken fra Maskinporten (ref steg 3)\nAPI-konsumenten henter data fra den eksterne API-tilbyderen ved √• bruke sikkerhetstoken fra Maskinporten.\n\n\n\n\n\n\n\n\n\n\nSteg 1 og 2 gjelder kun for M2M-brukere. Dersom man aksesserer Maskinporten Guardian med personlig bruker s√• hentes Keycloak-tokenet f.¬†eks ved hjelp av AuthClient i dapla-toolbelt.\n\n\n\n\nAPI-konsumenten henter et Maskinporten sikkerhetstoken fra Maskinporten Guardian ved √• bruke sitt personlige Keycloak sikkerhetstoken. Det m√• angis hvilken Maskinporten-klient og hvilke API scopes Maskinporten sikkerhetstokenet skal gjelde for. Den personlige brukeren m√• p√• forh√•nd v√¶re autorisert (ref Maskinporten Guardian sin tilgangskonfigurasjon) til √• kunne hente sikkerthetstokens for Maskinporten-klienten.\nMaskinporten Guardian bruker SSB sitt virksomhetssertifikat til √• signere en foresp√∏rsel om √• hente et sikkerhetstoken fra Maskinporten (ref steg 3)\nAPI-konsumenten henter data fra den eksterne API-tilbyderen ved √• bruke sikkerhetstoken fra Maskinporten.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDette avsnittet inneholder tekniske instrukser ment for deg som er kjent med Git og som f.¬†eks jobber i et Self-managed Dapla-team. Ta kontakt med oss via Pureservice s√• hjelper vi deg gjerne med dette.\n\n\nFor √• kunne opptre p√• vegne av Maskinporten-klienten uavhengig av din personlige bruker, m√• man opprette en Keycloak systembruker. Det gj√∏res ved √• √•pne en Pull Request (konfigurasjon som gjennomg√•s av en tekniker) til keycloak-iac der du angir informasjon som ID for Maskinporten-klient, API scopes og hvem som skal ha tilgang. Legg merke til at du m√• opprette en klient pr milj√∏ (test og prod). Du kan se bort fra play-milj√∏et.\n\n\namends \".../pkl/MaskinportenGuardianClient.pkl\"\n\napi_shortname = \"Kort API-beskrivelse (maks 32 tegn)\"\nmaskinporten_client_id = \"12345678-9abc-def0-1234-567890abcdef\"\nmaskinporten_audience = \"https://maskinporten.no/\"\nmaskinporten_default_scopes {\n  \"foo:data1\"\n  \"foo:data2\"\n}\ncredentials_access {\n  \"group:play-foeniks-data-admins@groups.ssb.no\"\n  \"serviceaccount:foo-sa@play-foeniks-p-ab.iam.gserviceaccount.com\"\n}\n\n\n\n\n\n\nNote\n\n\n\nI test skal maskinporten_audience ha verdien https://test.maskinporten.no/. I prod skal det v√¶re https://maskinporten.no/ (merk: skr√•strek p√• slutten er viktig)\n\n\nPull Requesten m√• godkjennes og behandles av en Dapla platformutvikler. N√•r dette er gjort blir det opprettet en Keycloak-klient, og hemmeligheten som kan brukes for √• hente ut sikkerhetstokens for denne klienten er tilgjengelig i Secret Manager.\nSe f√∏lgende dokumentasjon for mer informasjon:\n\nDetaljert beskrivelse av konfigurasjonsmuligeter.\nGenerell beskrivelse av hvordan man oppretter en Keycloak-klient\nKeycloak client credentials\n\nTa kontakt med Kundeservice hvis du har sp√∏rsm√•l eller trenger ei hand √• halde i.\n\n\n\n\nF√∏lgende viser Python kodeeksempler for hvordan man kan hente ut et Maskinporten sikkerhetstoken.\n\n\n\"\"\"\nRetrieve maskinporten M2M access token for querying an external API.\n\nThe following code example expects a toml config file to exist named\n&lt;team_uniform_name&gt;-&lt;env&gt;-maskinporten-m2m.toml (e.g. play-foeniks-prod-maskinporten-m2m.toml)\n\nwith contents such as:\n\nkeycloak_url = \"https://auth.test.ssb.no\"\nkeycloak_clients_gcp_project_id = \"keycloak-clients-&lt;p|t&gt;-??\"\nguardian_url = \"https://guardian.intern.test.ssb.no\"\n\n[my-api]\n    maskinporten_client_id = \"12345678-9abc-def0-1234-567890abcdef\"\n\"\"\"\nimport os\nimport re\nimport requests\nimport toml\n\n\nteam_uniform_name = \"play-foeniks\"\napi_name = \"my-api\" # identifies the API in the config.toml file\n\n# Check which environment we're running and load corresponding config\ndapla_env = os.getenv(\"DAPLA_ENVIRONMENT\").lower()\nconfig = toml.load(f\"{team_uniform_name}-{dapla_env}-maskinporten-m2m.toml\")\n\n# Get Maskinporten Guardian M2M Keycloak client credentials from Google Secret Manager\n# The secret's name is deduced from the team name and maskinporten client id\nname = f\"{team_uniform_name}-ssb-maskinporten-{config[api_name]['maskinporten_client_id']}-credentials\"\nsecret = get_secret_version(project_id=config['keycloak_clients_gcp_project_id'],\n                            shortname=name)\n\n# The credentials are stored as yaml. Here we simply use a regex to parse. \nkeycloak_client_id = re.search(r'\"client_id\": \"(.*)\"', secret).group(1)\nkeycloak_client_secret = re.search(r'\"client_secret\": \"(.*)\"', secret).group(1)\n\n# Get Keycloak access token\n# This token includes custom claims with values such as maskinporten_default_scopes\nresponse = requests.post(f\"{config['keycloak_url']}/realms/ssb/protocol/openid-connect/token\",\n    headers={\n        \"Content-type\": \"application/x-www-form-urlencoded\",\n    },\n    auth=(keycloak_client_id, keycloak_client_secret),\n    data={\"grant_type\": \"client_credentials\"}\n)\nkeycloak_access_token = response.json()['access_token']\n\n# Get Maskinporten access token from Maskinporten Guardian (using the Keycloak token from above)\n# Note that you can specify custom scopes in the request body if you need to. Using defaults defined in the client config if not specified.\nrequest_body={}\nresponse = requests.post(f\"{config['guardian_url']}/maskinporten/access-token\",\n            headers={\n                \"Authorization\": f\"Bearer {keycloak_access_token}\",\n                \"Content-type\": \"application/json\",\n            },\n            json=request_body,\n        )\nmaskinporten_access_token = response.json()['accessToken']\n\n# Then use the maskinporten access token to query the external API...\nSt√∏ttefunksjon for √• hente ut secrets fra Secret Manager\nfrom dapla import AuthClient\nfrom google.cloud import secretmanager\n\ndef get_secret_version(project_id, shortname, version_id='latest'):\n    \"\"\"\n    Access the payload for a given secret version.\n    The user's google credentials are used to authorize that the user have permission\n    to access the secret_id.\n    \n    Args:\n    - project_id (str): ID of the Google Cloud project where the secret is stored.\n    - shortname (str): Name (not full path) of the secret in Secret Manager.\n    - version_id (str, optional): The version of the secret to access. Defaults to 'latest'.\n\n    Returns:\n    - str: The payload of the secret version as a UTF-8 decoded string.\n    \"\"\"\n    client = secretmanager.SecretManagerServiceClient(credentials=AuthClient.fetch_google_credentials())\n    secret_name = f\"projects/{project_id}/secrets/{shortname}/versions/{version_id}\"\n    response = client.access_secret_version(name=secret_name)\n    return response.payload.data.decode(\"UTF-8\")\n\n\n\n\"\"\"\nRetrieve maskinporten personal access token for querying an external API.\n\nThe following code example expects a toml config file to exist named\n&lt;team_uniform_name&gt;-&lt;env&gt;-maskinporten-pers.toml (e.g. play-foeniks-prod-maskinporten-pers.toml)\n\nwith contents such as:\n\nguardian_url = \"https://guardian.intern.test.ssb.no\"\n\n[my-api]\n    maskinporten_client_id = \"12345678-9abc-def0-1234-567890abcdef\"\n    scopes = [\"some:scope1\", \"some:scope2\"]\n\"\"\"\nimport os\nimport requests\nimport toml\nfrom dapla import AuthClient\n\nteam_uniform_name = \"play-foeniks\"\napi_name = \"my-api\" # identifies the API in the config.toml file\n\n# Check which environment we're running and load corresponding config\ndapla_env = os.getenv(\"DAPLA_ENVIRONMENT\").lower()\nconfig = toml.load(f\"{team_uniform_name}-{dapla_env}-maskinporten-pers.toml\")\n\n# Get Keycloak access token\nkeycloak_access_token = AuthClient.fetch_personal_token()\n\n# Get Maskinporten access token from Maskinporten Guardian (using the personal Keycloak token from above)\nrequest_body = {\n  \"maskinportenClientId\": config[api_name]['maskinporten_client_id'],\n  \"scopes\": config[api_name]['scopes']\n}\nresponse = requests.post(f\"{config['guardian_url']}/maskinporten/access-token\",\n            headers={\n                \"Authorization\": f\"Bearer {keycloak_access_token}\",\n                \"Content-type\": \"application/json\",\n            },\n            json=request_body\n        )\n\nmaskinporten_access_token = response.json()['accessToken']\n\n# Finally, use the maskinporten access token to query the external API\nprint(maskinporten_access_token)",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#maskinporten",
    "href": "statistikkere/maskinporten-guardian.html#maskinporten",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "Maskinporten er en tjeneste fra Digdir som gir sikker autentisering og tilgangskontroll for datautveksling mellom virksomheter.\nTilgangsstyring for data administeres via et webgrensesnitt (Samarbeidsportalen) En virksomhet som deler data (API-tilbyder) definerer s√•kalte API scopes, og velger hvilke andre virksomheter (API-konsumenter) som skal ha tilgang til disse. API-konsumenter kan p√• sin side selv opprette en eller flere Maskinporten-klienter og gir de tilgang til API scopes som er delt med virksomheten.\n\n\n\n\n\n\nFigur¬†1: Datauveksling mellom virksomheter\n\n\n\nLes mer om Maskinporten her.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#hva-gj√∏r-maskinporten-guardian",
    "href": "statistikkere/maskinporten-guardian.html#hva-gj√∏r-maskinporten-guardian",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "Utveksling av data fra en API-tilbyder gj√∏res ved √• inkludere et sikkerhetstoken som hentes fra Maskinporten p√• vegne av virksomheten man representerer (f.¬†eks SSB). Hvilke data (API scopes) et sikkerhetstoken har tilgang til er knyttet til Maskinporten-klienten. For √• hente et sikkerhetstoken for en klient, kreves det at man autentiserer seg som virksomhet. Dette gj√∏res ved √• signere foresp√∏rsler til Maskinporten ved bruk av et virksomhetssertifikat. Det er her Maskinporten Guardian kommer inn i bildet.\nMaskinporten Guardian har tilgang til SSBs virksomhetssertifikat og kan dermed signere foresp√∏rsler mot Maskinporten for √• hente ut sikkerhetstokens.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#komme-igang",
    "href": "statistikkere/maskinporten-guardian.html#komme-igang",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "Hvis en skal ta i bruk et API som er beskyttet av Maskinporten er det noen steg som m√• gj√∏res i forkant:\n\n\n\n\n\nflowchart TD\n    A[API-tilbyder gir SSB tilgang til API via Samarbeidsportalen]\n    A --&gt; B[M2M-teamet hos SSB oppretter Maskinporten-klienter]\n    B --&gt; C[Keycloak-klienter for Maskinporten Guardian opprettes]\n\n\n\n\n\n\n\nAPI-tilbyderen m√• gi SSB tilgang til et API scope. API-tilbydere kan gj√∏re dette via Samarbeidsportalen hos Digdir. Fra API-tilbyderen f√•r du ogs√• annen informasjon om API-ene, som:\n\n\nURL-er som skal benyttes for √• snakke med API-et\nNavn p√• API scopes\nOm det finnes testadata\nDokumentasjon for API-endepunktene\n\n\nN√•r data er delt av en API-tilbyder, og en har navnet p√• API scopes, kan M2M-teamet hos SSB kontaktes for √• f√• opprettet Maskinporten-klienter, √©n pr milj√∏ (f.¬†eks prod og test). De m√• vite hvilke API scopes og milj√∏er (test/prod) som skal benyttes. M2M-teamet vil gi deg ID-er (f.¬†eks 12345678-9abc-def0-1234-567890abcdef) for klientene som er blitt opprettet.\n\n\n\n\n\n\n\nDu kan anse ID for en Maskinporten-klient som et slags brukernavn, og behandle dette deretter. Slike ID-er er ikke sensitive i seg selv, men de b√∏r allikevel ikke ligge i √•pne git-repoer (SSB-private repoer er OK). Det er heller ingen grunn til √• behandle disse som hemmeligheter. Det anbefales at man opererer med Maskinporten-klienter som ekstern konfigurasjon til koden, f.¬†eks p√• samme m√•te som man behandler URL-er til API-ene.\n\n\n\n\nN√•r du har ID for Maskinporten-klienten(e), er neste steg √• f√• opprettet en Keycloak-bruker for Maskinporten Guardian. Se Opprette en Maskinporten Guardian M2M-bruker. Ta kontakt med oss p√• Dapla via Pureservice dersom du trenger hjelp til dette. Client ID og client secret for Keycloak-brukeren kan hentes fra Secret Manager (mer om det lenger ned). Medlemmer av Dapla-teamet har tilgang til √• hente disse.\n\nOm noen p√• teamet trenger personlig tilgang til API-ene s√• m√• det konfigureres i Maskinporten Guardian. Da m√• vi vite hvilke personer som skal ha denne tilgangen. Les mer om forskjellen p√• M2M og personlig tilgang lenger ned.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#administrasjon-av-api-integrasjoner-i-ssb",
    "href": "statistikkere/maskinporten-guardian.html#administrasjon-av-api-integrasjoner-i-ssb",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "Det er M2M-teamet i SSB som administrerer det formelle i forbindelse med integrasjoner mot eksterne API-er. For √• f√• opprettet nye Maskinporten-klienter er det dette teamet man kontakter. De m√• vite hvilke milj√∏er (test og/eller prod) og hvilke API scopes som skal benyttes. Hver Maskinporten-klient som opprettes identifiseres av en ID (f.¬†eks 12345678-9abc-def0-1234-567890abcdef), som du vil f√• tilsendt.\nLegg merke til at det er SSB selv som oppretter og administrerer Maskinporten-klienter. Det gj√∏res i Samarbeidsportalen hos Difi. Klientene knyttes til API scope(s) (som er delt med SSB av API-tilbyderen). Du kan lese mer om hvordan M2M-teamet administrerer Maskinporten-klienter her.\n\n\n\n\n\n\nAlle med en SSB-epostadresse kan registerere en personlig bruker i Digdir Samarbeidsportalen for √• f√• innsyn i hvilke API-integrasjoner som finnes.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#hvordan-maskinporten-guardian",
    "href": "statistikkere/maskinporten-guardian.html#hvordan-maskinporten-guardian",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "Maskinporten Guardian er tilgjengelig fra alle SSB og NAIS sine IP-adresser, og kan n√•s p√•:\n\nProd: https://guardian.intern.ssb.no\nTest: https://guardian.intern.test.ssb.no\n\nMaskinporten Guardian sine endepunkter er selv beskyttet av Keycloak. To typer brukere st√∏ttes:\n\nMaskin-til-maskin (M2M) - Systembruker knyttet til en gitt Maskinporten-klient. For √• opptre p√• vegne av en M2M-bruker autentiserer man seg med en Keycloak client secret. Denne hemmeligheten er lagret i Google Secret Manager og kun Service Accounts eller Dapla-grupper med tilgang kan hente den ut.\nPersonlig - Din egen SSB-bruker. Man kan f.¬†eks bruke dapla-toolbelt for √• hente ut sitt personlige Keycloak-token. I tillegg til √• autentisere deg m√• din bruker v√¶re autorisert til √• gj√∏re oppslag p√• vegne av en Maskinporten-klient. Dette styres i konfigurasjonen til Maskinporten Guardian.\n\n\n\n\n\n\n\nMan skal i hovedsak kun anvende M2M-brukere for datautveksling mot API-er som er beskyttet av Maskinporten. Personlige brukere skal kun brukes unntaksvis for enkeltoppslag (f.¬†eks ved feils√∏king) mot API-er eller for utvikling og test.\n\n\n\nLegg merke til at Maskinporten Guardian kun er tilgjengelig fra interne SSB-adresser. Bruk f√∏lgende URL-er: * Prod: https://guardian.intern.ssb.no * Test: https://guardian.intern.test.ssb.no",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#systemskisse",
    "href": "statistikkere/maskinporten-guardian.html#systemskisse",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "F√∏lgende gir en oversikt over hvordan systemer henger sammen. En API-konsument kan f.¬†eks v√¶re et Dapla-team som √∏nsker √• hente data, mens en API-tilbyder er en ekstern virksomhet som tilbyr data via Maskinporten. Det er noen forskjeller i flyt avhengig av om Maskinporten Guardian aksesseres med systembruker (M2M) eller personlig bruker.\n\n\n\n\n\n\nFigur¬†2\n\n\n\n\n\n\nAPI-konsumenten henter sin Keycloak client secret for en gitt Maskinporten-klient fra Secret Manager.\nAPI-konsumenten henter et Keycloak sikkerhetstoken ved √• bruke client secret fra steg 1.\nAPI-konsumenten henter et Maskinporten sikkerhetstoken fra Maskinporten Guardian ved √• bruke Keycloak sikkerhetstoken fra steg 2. En kan alternativt angi andre API scopes enn det som er standard for Maskinporten-klienten, men dette er vanligvis ikke n√∏dvendig.\nMaskinporten Guardian bruker SSB sitt virksomhetssertifikat til √• signere en foresp√∏rsel om √• hente et sikkerhetstoken fra Maskinporten (ref steg 3)\nAPI-konsumenten henter data fra den eksterne API-tilbyderen ved √• bruke sikkerhetstoken fra Maskinporten.\n\n\n\n\n\n\n\n\n\n\nSteg 1 og 2 gjelder kun for M2M-brukere. Dersom man aksesserer Maskinporten Guardian med personlig bruker s√• hentes Keycloak-tokenet f.¬†eks ved hjelp av AuthClient i dapla-toolbelt.\n\n\n\n\nAPI-konsumenten henter et Maskinporten sikkerhetstoken fra Maskinporten Guardian ved √• bruke sitt personlige Keycloak sikkerhetstoken. Det m√• angis hvilken Maskinporten-klient og hvilke API scopes Maskinporten sikkerhetstokenet skal gjelde for. Den personlige brukeren m√• p√• forh√•nd v√¶re autorisert (ref Maskinporten Guardian sin tilgangskonfigurasjon) til √• kunne hente sikkerthetstokens for Maskinporten-klienten.\nMaskinporten Guardian bruker SSB sitt virksomhetssertifikat til √• signere en foresp√∏rsel om √• hente et sikkerhetstoken fra Maskinporten (ref steg 3)\nAPI-konsumenten henter data fra den eksterne API-tilbyderen ved √• bruke sikkerhetstoken fra Maskinporten.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#opprette-en-maskinporten-guardian-m2m-bruker",
    "href": "statistikkere/maskinporten-guardian.html#opprette-en-maskinporten-guardian-m2m-bruker",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "Note\n\n\n\nDette avsnittet inneholder tekniske instrukser ment for deg som er kjent med Git og som f.¬†eks jobber i et Self-managed Dapla-team. Ta kontakt med oss via Pureservice s√• hjelper vi deg gjerne med dette.\n\n\nFor √• kunne opptre p√• vegne av Maskinporten-klienten uavhengig av din personlige bruker, m√• man opprette en Keycloak systembruker. Det gj√∏res ved √• √•pne en Pull Request (konfigurasjon som gjennomg√•s av en tekniker) til keycloak-iac der du angir informasjon som ID for Maskinporten-klient, API scopes og hvem som skal ha tilgang. Legg merke til at du m√• opprette en klient pr milj√∏ (test og prod). Du kan se bort fra play-milj√∏et.\n\n\namends \".../pkl/MaskinportenGuardianClient.pkl\"\n\napi_shortname = \"Kort API-beskrivelse (maks 32 tegn)\"\nmaskinporten_client_id = \"12345678-9abc-def0-1234-567890abcdef\"\nmaskinporten_audience = \"https://maskinporten.no/\"\nmaskinporten_default_scopes {\n  \"foo:data1\"\n  \"foo:data2\"\n}\ncredentials_access {\n  \"group:play-foeniks-data-admins@groups.ssb.no\"\n  \"serviceaccount:foo-sa@play-foeniks-p-ab.iam.gserviceaccount.com\"\n}\n\n\n\n\n\n\nNote\n\n\n\nI test skal maskinporten_audience ha verdien https://test.maskinporten.no/. I prod skal det v√¶re https://maskinporten.no/ (merk: skr√•strek p√• slutten er viktig)\n\n\nPull Requesten m√• godkjennes og behandles av en Dapla platformutvikler. N√•r dette er gjort blir det opprettet en Keycloak-klient, og hemmeligheten som kan brukes for √• hente ut sikkerhetstokens for denne klienten er tilgjengelig i Secret Manager.\nSe f√∏lgende dokumentasjon for mer informasjon:\n\nDetaljert beskrivelse av konfigurasjonsmuligeter.\nGenerell beskrivelse av hvordan man oppretter en Keycloak-klient\nKeycloak client credentials\n\nTa kontakt med Kundeservice hvis du har sp√∏rsm√•l eller trenger ei hand √• halde i.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#kodeeksempler",
    "href": "statistikkere/maskinporten-guardian.html#kodeeksempler",
    "title": "Maskinporten Guardian",
    "section": "",
    "text": "F√∏lgende viser Python kodeeksempler for hvordan man kan hente ut et Maskinporten sikkerhetstoken.\n\n\n\"\"\"\nRetrieve maskinporten M2M access token for querying an external API.\n\nThe following code example expects a toml config file to exist named\n&lt;team_uniform_name&gt;-&lt;env&gt;-maskinporten-m2m.toml (e.g. play-foeniks-prod-maskinporten-m2m.toml)\n\nwith contents such as:\n\nkeycloak_url = \"https://auth.test.ssb.no\"\nkeycloak_clients_gcp_project_id = \"keycloak-clients-&lt;p|t&gt;-??\"\nguardian_url = \"https://guardian.intern.test.ssb.no\"\n\n[my-api]\n    maskinporten_client_id = \"12345678-9abc-def0-1234-567890abcdef\"\n\"\"\"\nimport os\nimport re\nimport requests\nimport toml\n\n\nteam_uniform_name = \"play-foeniks\"\napi_name = \"my-api\" # identifies the API in the config.toml file\n\n# Check which environment we're running and load corresponding config\ndapla_env = os.getenv(\"DAPLA_ENVIRONMENT\").lower()\nconfig = toml.load(f\"{team_uniform_name}-{dapla_env}-maskinporten-m2m.toml\")\n\n# Get Maskinporten Guardian M2M Keycloak client credentials from Google Secret Manager\n# The secret's name is deduced from the team name and maskinporten client id\nname = f\"{team_uniform_name}-ssb-maskinporten-{config[api_name]['maskinporten_client_id']}-credentials\"\nsecret = get_secret_version(project_id=config['keycloak_clients_gcp_project_id'],\n                            shortname=name)\n\n# The credentials are stored as yaml. Here we simply use a regex to parse. \nkeycloak_client_id = re.search(r'\"client_id\": \"(.*)\"', secret).group(1)\nkeycloak_client_secret = re.search(r'\"client_secret\": \"(.*)\"', secret).group(1)\n\n# Get Keycloak access token\n# This token includes custom claims with values such as maskinporten_default_scopes\nresponse = requests.post(f\"{config['keycloak_url']}/realms/ssb/protocol/openid-connect/token\",\n    headers={\n        \"Content-type\": \"application/x-www-form-urlencoded\",\n    },\n    auth=(keycloak_client_id, keycloak_client_secret),\n    data={\"grant_type\": \"client_credentials\"}\n)\nkeycloak_access_token = response.json()['access_token']\n\n# Get Maskinporten access token from Maskinporten Guardian (using the Keycloak token from above)\n# Note that you can specify custom scopes in the request body if you need to. Using defaults defined in the client config if not specified.\nrequest_body={}\nresponse = requests.post(f\"{config['guardian_url']}/maskinporten/access-token\",\n            headers={\n                \"Authorization\": f\"Bearer {keycloak_access_token}\",\n                \"Content-type\": \"application/json\",\n            },\n            json=request_body,\n        )\nmaskinporten_access_token = response.json()['accessToken']\n\n# Then use the maskinporten access token to query the external API...\nSt√∏ttefunksjon for √• hente ut secrets fra Secret Manager\nfrom dapla import AuthClient\nfrom google.cloud import secretmanager\n\ndef get_secret_version(project_id, shortname, version_id='latest'):\n    \"\"\"\n    Access the payload for a given secret version.\n    The user's google credentials are used to authorize that the user have permission\n    to access the secret_id.\n    \n    Args:\n    - project_id (str): ID of the Google Cloud project where the secret is stored.\n    - shortname (str): Name (not full path) of the secret in Secret Manager.\n    - version_id (str, optional): The version of the secret to access. Defaults to 'latest'.\n\n    Returns:\n    - str: The payload of the secret version as a UTF-8 decoded string.\n    \"\"\"\n    client = secretmanager.SecretManagerServiceClient(credentials=AuthClient.fetch_google_credentials())\n    secret_name = f\"projects/{project_id}/secrets/{shortname}/versions/{version_id}\"\n    response = client.access_secret_version(name=secret_name)\n    return response.payload.data.decode(\"UTF-8\")\n\n\n\n\"\"\"\nRetrieve maskinporten personal access token for querying an external API.\n\nThe following code example expects a toml config file to exist named\n&lt;team_uniform_name&gt;-&lt;env&gt;-maskinporten-pers.toml (e.g. play-foeniks-prod-maskinporten-pers.toml)\n\nwith contents such as:\n\nguardian_url = \"https://guardian.intern.test.ssb.no\"\n\n[my-api]\n    maskinporten_client_id = \"12345678-9abc-def0-1234-567890abcdef\"\n    scopes = [\"some:scope1\", \"some:scope2\"]\n\"\"\"\nimport os\nimport requests\nimport toml\nfrom dapla import AuthClient\n\nteam_uniform_name = \"play-foeniks\"\napi_name = \"my-api\" # identifies the API in the config.toml file\n\n# Check which environment we're running and load corresponding config\ndapla_env = os.getenv(\"DAPLA_ENVIRONMENT\").lower()\nconfig = toml.load(f\"{team_uniform_name}-{dapla_env}-maskinporten-pers.toml\")\n\n# Get Keycloak access token\nkeycloak_access_token = AuthClient.fetch_personal_token()\n\n# Get Maskinporten access token from Maskinporten Guardian (using the personal Keycloak token from above)\nrequest_body = {\n  \"maskinportenClientId\": config[api_name]['maskinporten_client_id'],\n  \"scopes\": config[api_name]['scopes']\n}\nresponse = requests.post(f\"{config['guardian_url']}/maskinporten/access-token\",\n            headers={\n                \"Authorization\": f\"Bearer {keycloak_access_token}\",\n                \"Content-type\": \"application/json\",\n            },\n            json=request_body\n        )\n\nmaskinporten_access_token = response.json()['accessToken']\n\n# Finally, use the maskinporten access token to query the external API\nprint(maskinporten_access_token)",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten Guardian"
    ]
  },
  {
    "objectID": "statistikkere/metadata.html",
    "href": "statistikkere/metadata.html",
    "title": "Metadata",
    "section": "",
    "text": "Metadata er informasjonen som trengs for √• produsere, finne, forst√• og gjenbruke data. For √• bruke et datasett riktig m√• vi forst√• hva det inneholder. Vi m√• vite hva de ulike variablene representerer, for eksempel om variabelen inntekt i datasett A er den samme som inntekt i datasett B og derfor kan sammenliknes. Vi trenger ogs√• innsikt i hva kodene i datasettet betyr, som for eksempel hva sivilstandskode 2 representerer. I tillegg er det viktig √• vite om datasettet inneholder personopplysninger, slik at vi kan oppfylle krav fra Datatilsynet.\nAlt dette er eksempler p√• metadata som er tett knyttet til selve dataene. I tillegg har SSB mange andre typer metadata, som kvalitetsindikatorer og prosessbeskrivelser. Her velger vi imidlertid √• fokusere p√• de metadataene som er n√∏dvendige for √• kunne bruke et datasett riktig.\nSSB har i flere √•r hatt systemer for √• dokumentere ¬´datan√¶re¬ª metadata. Vardok dokumenterer variabler, som definisjoner og eierseksjoner. Klass h√•ndterer kodeverk, inkludert klassifikasjoner og kodelister. Datadok fokuserer p√• dokumentasjon av datasett, for eksempel variabler, variabeltyper og antall desimaler.\nSSB valgte √• dokumentere metadata i flere separate systemer, b√•de fordi andre statistikkbyr√•er hadde erfart at ett samlet system ble for stort og komplekst, og fordi SSB allerede hadde en eksisterende l√∏sning, Datadok, som skulle videref√∏res. I stedet for √• lage ett enkelt system, ble strategien √• utvikle flere systemer som skulle fungere sammen som en helhet. Dette f√∏rte til integrasjoner mellom systemene, som for eksempel en kobling mellom Vardok og Klass. En variabel som N√¶ring i Vardok har en lenke til Standard for n√¶ringsgruppering i Klass. I tillegg er Vardok koblet til Statistikkbanken og MetaDB (NUDB og FD-Trygd) for utveksling av variabeldefinisjoner. Datadok er ogs√• koblet til Vardok, slik at variablene i Datadok kan referere til tilh√∏rende variabeldefinisjoner fra Vardok.\nVerken Datadok eller Vardok kan brukes p√• Dapla. Derfor er en ny l√∏sning for dokumentasjon av datasett, Datadoc, allerede implementert p√• Dapla. En ny l√∏sning for dokumentasjon av variabeldefinisjoner, Vardef, er under utvikling. N√•r Vardef er ferdigstilt, skal variabler fra Vardok flyttes dit, og alle fremtidige oppdateringer vil skje i Vardef. Klass kan derimot brukes fra Dapla, og det er derfor enn√• ikke besluttet om og n√•r denne l√∏sningen skal flyttes eller eventuelt implementeres p√• nytt p√• Dapla.\nP√• Dapla skal det etter hvert ogs√• implementeres en datakatalog som skal v√¶re en portal inn til metadatal√∏sningene. Via datakatalogen skal en kunne s√∏ke i alle disse metadataene.\n\n\n\n\n\n\nFigur¬†1: Metadata p√• Dapla",
    "crumbs": [
      "Manual",
      "Metadata"
    ]
  },
  {
    "objectID": "statistikkere/vardef.html",
    "href": "statistikkere/vardef.html",
    "title": "Vardef",
    "section": "",
    "text": "Vardef er SSBs system for dokumentasjon av variabler. Hensikten med Vardef er at alle variabler i SSB skal dokumenteres og oppdateres ett sted (av ansvarlig Dapla-team) og gjenbrukes av alle som har behov for dem.\nVardef skal inneholde all n√∏dvendig informasjon om en variabel. Mens Datadoc dokumenterer variabelforekomstene, dvs. enkeltvariablene i et datasett1, skal Vardef dokumentere de mer overordnede beskrivelsene av en variabel. I Vardef er det variabler som gjenbrukes, enten p√• tvers i SSB, eller flere ganger over tid i samme statistikk, som skal dokumenteres. Et eksempel er f.eks. variabelen ¬´organisasjonsnummer¬ª som vil v√¶re definert i Vardef, slik at beskrivelsen kan gjenbrukes i alle datafiler der variabelforekomsten organisasjonsnummer inng√•r. I SSB har ofte organisasjonsnummer ulike navn i ulike datasett , men n√•r alle variablene er knyttet til samme Vardef-variabel, kan brukerne likevel forst√• at det er samme variabel.\nI Datadoc har en mulighet for √• presisere en variabel som refereres til i Vardef. Vi kan f.eks. ha variabelen ¬´Yrkesinntekt¬ª som brukes b√•de i datasett A og B. Selve definisjonen hentes da fra Vardef og er dermed den samme i begge datasett. Men s√• kan det v√¶re presiseringer en m√• gj√∏re i Datadoc for hver variabelforekomst, f.eks. at yrkesinntekt i datasett A m√•les i kroner, mens den i datasett B m√•les i 1000 kroner.\nDersom Vardef-variabelen er en kvalitativ variabel (har verdier som hentes fra et kodeverk), skal variabelen referere til tilh√∏rende kodeverk i Klass. F.eks. skal variabelen ¬´Sivilstand¬ª som har definisjonen ¬´Variabelen viser en persons stilling ihht ekteskapslovgivningen¬ª, referere til ¬´Standard for sivilstand¬ª i Klass som viser hvilke verdier (kategorier) variabelen kan anta.",
    "crumbs": [
      "Manual",
      "Metadata",
      "Vardef"
    ]
  },
  {
    "objectID": "statistikkere/vardef.html#footnotes",
    "href": "statistikkere/vardef.html#footnotes",
    "title": "Vardef",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nDette er noe vi √∏nsker √• unng√• p√• Dapla, det er best for brukerne om samme variabelforekomst har samme navn i alle datasett der den brukes. Dette vil ikke v√¶re noe krav, men dersom variabelforekomstnavnet er det samme som kortnavnet til tilh√∏rende variabel i Vardef, vil variabelforekomsten i Datadoc kunne lenkes maskinelt til riktig variabeldefinisjon i Vardef. Dermed slipper en mye manuelt arbeid.‚Ü©Ô∏é",
    "crumbs": [
      "Manual",
      "Metadata",
      "Vardef"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard.html",
    "href": "statistikkere/navnestandard.html",
    "title": "Navnestandard",
    "section": "",
    "text": "Data i de permanente datatilstandene inndata, klargjorte data, statistikkdata og utdata skal lagres i Google Cloud Storage (GCS) b√∏tter og f√∏lge en definert navnestandard. Standarden gjelder for b√•de statistikkprodukter og dataprodukter (se forklaringsboks under). Navnestandarden beskrevet i dette kapitlet er derfor gjeldende for alle data som lagres i b√∏tter i standardprosjektet, som f.eks. produktb√∏tta og delt-b√∏ttene.\nDatatilstanden kildedata omfattes ikke av navnestandarden. Grunnen er at kildedata mottas av SSB i mange former/strukturer og de deles sjelden med andre team. De unike egenskapene til kildedata er ogs√• grunnen til at de ikke har samme krav til dokumentasjon i metadatasystemene heller.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Navnestandard"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard.html#mappestruktur",
    "href": "statistikkere/navnestandard.html#mappestruktur",
    "title": "Navnestandard",
    "section": "Mappestruktur",
    "text": "Mappestruktur\nNavnestandarden for lagring av data innf√∏rer obligatoriske mapper som alle statistikk- og dataprodukter m√• f√∏lge, samt valgfrie deler hvor teamet selv kan bestemme sin mappestruktur.\n\nObligatoriske mapper\nIf√∏lge navnestandarden skal f√∏lgende mappeniv√•er alltid eksistere f√∏rst i en lagringsb√∏tte:\n\nStatistikkproduktets eller dataproduktets kortnavn\nDatatilstand\n\nAnta at det er team som heter dapla-example som har produserer statistikkproduktene ledstill og sykefra. I tillegg produserer de et dataprodukt som heter ameld. Deres mappestruktur i produktb√∏tta vil da se slik ut:\n\n\nObligatoriske mapper\n\nssb-dapla-example-data-produkt-prod/  \n‚îî‚îÄ ledstill/  \n   ‚îú‚îÄ‚îÄ inndata/\n   ‚îú‚îÄ‚îÄ klargjorte-data/\n   ‚îú‚îÄ‚îÄ statistikk/\n   ‚îî‚îÄ‚îÄ utdata/\n‚îî‚îÄ sykefra/  \n   ‚îú‚îÄ‚îÄ inndata/\n   ‚îú‚îÄ‚îÄ klargjorte-data/\n   ‚îú‚îÄ‚îÄ statistikk/\n   ‚îî‚îÄ‚îÄ utdata/\n‚îî‚îÄ ameld_data/  \n   ‚îú‚îÄ‚îÄ inndata/\n   ‚îú‚îÄ‚îÄ klargjorte-data/\n   ‚îú‚îÄ‚îÄ statistikk/\n   ‚îî‚îÄ‚îÄ utdata/                    \n\n\n\nValgfrie mapper\nDe to f√∏rste mappeniv√•ene er bestemt og obligatoriske. Teamene kan likevel opprette egendefinerte mapper der det er behov. Det kan gj√∏res i f√∏lgende tilfeller:\n\nTeamet √∏nsker √• organisere dataene i undermapper for hver datatilstand.\nTeamet trenger √• lagre tempor√¶re data.\n\nDet er anbefalt √• lage en temp-mappe p√• f√∏rste niv√• etter b√∏ttenavn, men det er ogs√• tillatt √• opprette temp-mapper andre steder i mappe-hierarkiet, f.eks. ../inndata/temp/ eller ../klargjorte-data/temp/.\n\nTeamet utf√∏rer oppdrag og √∏nsker et eget sted √• lagre data knyttet til dette. Det kan kun gj√∏res i en oppdrag-mappe p√• f√∏rste niv√• etter b√∏ttenavn.\n\nUnder er et nytt eksempel i produktb√∏tta for team dapla-example, men n√• har de kun statistikkproduktet ledstill, en temp-mappe og en oppdrag-mappe. I tillegg s√• √∏nsker de √• skille mellom data som er produsert p√• Dapla og data som er migrert fra tidligere plattform. De gj√∏r det ved √• opprette de egendefinerte mappene on-prem og dapla for hver datatilstand.\n\n\nObligatoriske og egendefinerte mapper\n\nssb-dapla-example-data-produkt-prod/  \n‚îî‚îÄ ledstill/  \n   ‚îú‚îÄ‚îÄ inndata/\n       ‚îú‚îÄ‚îÄ on-prem/\n       ‚îú‚îÄ‚îÄ dapla/\n   ‚îú‚îÄ‚îÄ klargjorte-data/\n       ‚îú‚îÄ‚îÄ on-prem/\n       ‚îú‚îÄ‚îÄ dapla/\n   ‚îú‚îÄ‚îÄ statistikk/\n       ‚îú‚îÄ‚îÄ on-prem/\n       ‚îú‚îÄ‚îÄ dapla/\n   ‚îî‚îÄ‚îÄ utdata/\n       ‚îú‚îÄ‚îÄ on-prem/\n       ‚îú‚îÄ‚îÄ dapla/\n‚îî‚îÄ temp/\n‚îî‚îÄ oppdrag/                     \n\n\n\n\n\n\n\nMappe for oppdrag\n\n\n\nN√•r man oppretter en mappe for oppdrag s√• er det viktig √• kunne knytte dataene til et Websak-saksnummer. Det er derfor anbefalt at det opprettes en undermappe med saksnummeret eller at saksnummeret er med i filnavnet.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Navnestandard"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard.html#filnavn",
    "href": "statistikkere/navnestandard.html#filnavn",
    "title": "Navnestandard",
    "section": "Filnavn",
    "text": "Filnavn\nFilnavn skal ha en fast struktur som inneholder: en kort beskrivelse, periode, versjon og filtype, slik som vist i Figur¬†1.\n\n\n\n\n\n\nFigur¬†1: De ulike delene av et standardisert filnavn\n\n\n\nEksempelet i Figur¬†1 har varehandel som kort beskrivelse, dataene er gyldige for 2018-Q1, versjon er 1 og filtypen er parquet. I tillegg ser vi at periodeangivelse alltid skal prefixes med p og versjon med v. Elementene i filnavnet skal skilles med understrek.\nDet er ogs√• verdt √• merke seg at mellomrom og s√¶rnorske bokstaver som √¶, √∏ og √• ikke forekommer i filnavnet. F√∏lgende alfanumeriske tegn kan benyttes i fil- og mappenavn:\n\na-z og A-Z2.\n0-9\nBruk bindestrek -, eller understrek _, og ikke mellomrom.\n\nTabell¬†1 viser en mer inng√•ende beskrivelse av hva som inng√•r i de ulike delene av et filnavn.\n\n\n\nTabell¬†1: Autonomitet og tilganger i IaC-repo\n\n\n\n\n\nElement\nForklaring\n\n\n\n\nKort beskrivelse\nKort tekst som forklarer datasettets innhold, f.eks. ‚Äúvarehandel‚Äù eller ‚Äúpersoninntekt‚Äù. Bruk bindestrek hvis beskrivelsen best√•r av flere ord, f.eks. ‚Äúgrensehandel-imputert‚Äù eller ‚Äúframskrevne-befolkningsendringer‚Äù.\n\n\nPeriode - inneholder data f.o.m. dato\nDatasettet inneholder data fra og med dato/tidspunkt. I filnavnet m√• perioden prefikses med ‚Äú_p‚Äù, eksempel ‚Äú_p2022‚Äù eller ‚Äú_p2022-01-01‚Äù.¬†Se ogs√• gyldige¬†formater for periode (dato/tidspunkt)\n\n\nPeriode - inneholder data t.o.m. dato\nDatasettet inneholder data til og med dato/tidspunkt.¬†Denne brukes ved behov, eksempelvis for datasett som inneholder forl√∏psdata eller datasett med flere perioder/√•rganger.\n\n\nVersjon\nVersjon av datasettet. I filnavnet m√• versjonsnummeret prefikses med ‚Äú_v‚Äù, eksempel ‚Äúv1‚Äù, ‚Äúv2‚Äù eller ‚Äúv3‚Äù.\n\n\nFiltype\nFilendelse som sier noen om filtypen, f.eks. ‚Äú.json‚Äù, ‚Äú.csv‚Äù, ‚Äú.xml‚Äù eller ‚Äú.parquet‚Äù.\n\n\n\n\n\n\n\nEksempler p√• gyldige filnavn\nUnder finner du et utvalg eksempler p√• gyldige filnavn for ulike tidsspenn.\n\n\n\n\n\n\n\nTidsspenn\nEksempel p√• gyldige filnavn\n\n\n\n\n√ân √•rgang med data\nflygende-objekter_p2019_v1.parquet\n\n\nTo √•rganger med data\nufo-observasjoner_p2019_p2020_v1.parquet\n\n\nFra 2019 til 2050\nframskrevne-befolkningsendringer_p2019_p2050_v1.parquet\n\n\nFra 01.01.2022 til 31.12.2022\nsykepenger_p2022-01-01_p2022-12-31_v1.parquet\n\n\nTverrsnittsdata (status) per 01.10.2022\nutanningsnivaa_p2022-10-01_v1.parquet\n\n\nOktober, november og desember 2022\ngrensehandel-imputert_p2022-10_p2022-12_v1.parquet\n\n\nUke-nummer 1\nomsetning_p2020-W01_v1.parquet\n\n\nUke-nummer 15\nomsetning_p2020-W15_v1.parquet\n\n\nF√∏rste bimester i 2022\nskipsanloep_p2022-B1_v1.parquet\n\n\nF√∏rste kvartal i 2018 (quarter)\npensjon_p2018-Q1_v1.parquet\n\n\nF√∏rste tertial i 2022\nnybilreg_p2022-T1_v1.parquet\n\n\nF√∏rste halv√•r i 2022\npersoninntekt_p2022-H1_v1.parquet\n\n\nKvartalene 1, 2, 3 og 4 i 2018\nvarehandel_p2018-Q1_p2018-Q4_v1.parquet\n\n\nDato 31.12.2024 og tid 23:59:30.000\nskjema_p2024-12-31T23-59-30.000_v1.parquet",
    "crumbs": [
      "Manual",
      "Standarder",
      "Navnestandard"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard.html#partisjonerte-data",
    "href": "statistikkere/navnestandard.html#partisjonerte-data",
    "title": "Navnestandard",
    "section": "Partisjonerte data",
    "text": "Partisjonerte data\nTeam som partisjonerer sine filer ved lagring skal fortsatt f√∏lge navnestandarden. Det som endrer seg er at filtype ikke blir en del av filnavnet, men heller kommer inn under partisjoneringen. Anta at team dapla-example partisjonerer et datasett i datatilstand inndata som heter skjema_p2018_p2020_v1. Anta ogs√• at de partisjonerer dataene med hensyn p√• kolonnen aar. Da vil de i henhold til navnestandarden opprette denne strukturen:\n\n\nMappestruktur partisjonert data\n\nssb-dapla-example-data-produkt-prod/  \n‚îî‚îÄ ledstill/  \n   ‚îú‚îÄ‚îÄ inndata/\n        ‚îî‚îÄ‚îÄ skjema_p2018_p2020_v1\n            ‚îî‚îÄ‚îÄ aar=2018\n                ‚îî‚îÄ‚îÄ data.parquet\n            ‚îî‚îÄ‚îÄ aar=2019\n                ‚îî‚îÄ‚îÄ data.parquet\n            ‚îî‚îÄ‚îÄ aar=2020\n                ‚îî‚îÄ‚îÄ data.parquet         \n   ‚îú‚îÄ‚îÄ klargjorte-data/\n   ‚îú‚îÄ‚îÄ statistikk/\n   ‚îî‚îÄ‚îÄ utdata/                 \n\n\nEksempel: Produktb√∏tte for team dapla-example\nAnta at det er team som heter dapla-example med statistikkproduktene ledstill og sykefra, og de har et dataprodukt med kortnavnet ameld. Teamet har f√∏lgende mappestruktur i produktb√∏tta:\n\n\nProduktb√∏tta: ledstill, sykefra og ameld\n\nssb-dapla-example-data-produkt-prod/\n‚îî‚îÄ ledstill/  \n    ‚îú‚îÄ‚îÄ inndata/\n    ‚îÇ   ‚îú‚îÄ‚îÄ skjema_p2024-Q1_v1.parquet\n    ‚îÇ   ‚îú‚îÄ‚îÄ skjema_p2024-Q2_v1.parquet\n    ‚îÇ   ‚îî‚îÄ‚îÄ skjema_p2024-Q2_v2.parquet\n    ‚îú‚îÄ‚îÄ klargjorte-data/\n    ‚îÇ   ‚îú‚îÄ‚îÄ editert_p2024-Q1_v1.parquet\n    ‚îÇ   ‚îî‚îÄ‚îÄ editert_p2024-Q2_v1.parquet\n    ‚îú‚îÄ‚îÄ statistikk/\n    ‚îÇ   ‚îú‚îÄ‚îÄ aggregert_p2024-Q1_v1.parquet\n    ‚îÇ   ‚îî‚îÄ‚îÄ aggregert_p2024-Q2_v1.parquet        \n    ‚îî‚îÄ‚îÄ utdata/\n    ‚îÇ   ‚îú‚îÄ‚îÄ statbank_p2024-Q1_v1.parquet\n    ‚îÇ   ‚îî‚îÄ‚îÄ statbank_p2024-Q2_v1.parquet   \n    ‚îÇ\n‚îî‚îÄ sykefra/  \n    ‚îú‚îÄ‚îÄ inndata/\n    ‚îÇ   ‚îú‚îÄ‚îÄ egenmeldt_p2024-Q1_v1.parquet\n    ‚îÇ   ‚îú‚îÄ‚îÄ egenmeldt_p2024-Q2_v1.parquet\n    ‚îÇ   ‚îú‚îÄ‚îÄ legemeldt_p2024-Q1_v1.parquet\n    ‚îÇ   ‚îî‚îÄ‚îÄ legemeldt_p2024-Q2_v1.parquet\n    ‚îú‚îÄ‚îÄ klargjorte-data/\n    ‚îÇ   ‚îú‚îÄ‚îÄ sykefravaer_p2024-Q1_v1.parquet\n    ‚îÇ   ‚îî‚îÄ‚îÄ sykefravaer_p2024-Q2_v1.parquet\n    ‚îú‚îÄ‚îÄ statistikk/\n    ‚îÇ   ‚îú‚îÄ‚îÄ aggregert_p2024-Q1_v1.parquet\n    ‚îÇ   ‚îî‚îÄ‚îÄ aggregert_p2024-Q2_v1.parquet\n    ‚îî‚îÄ‚îÄ utdata/\n    ‚îÇ   ‚îú‚îÄ‚îÄ statbank_p2024-Q1_v1.parquet\n    ‚îÇ   ‚îî‚îÄ‚îÄ statbank_p2024-Q2_v1.parquet\n    ‚îÇ\n‚îî‚îÄ ameld_data/  \n    ‚îú‚îÄ‚îÄ inndata/\n    ‚îÇ   ‚îú‚îÄ‚îÄ ameldingen_p2024-11_v1.parquet\n    ‚îÇ   ‚îî‚îÄ‚îÄ ameldingen_p2024-12_v1.parquet\n    ‚îî‚îÄ‚îÄ klargjorte-data/\n    ‚îÇ   ‚îú‚îÄ‚îÄ ameldingen_p2024-11_v1.parquet\n    ‚îÇ   ‚îî‚îÄ‚îÄ ameldingen_p2024-12_v1.parquet",
    "crumbs": [
      "Manual",
      "Standarder",
      "Navnestandard"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard.html#versjonering-av-datasett",
    "href": "statistikkere/navnestandard.html#versjonering-av-datasett",
    "title": "Navnestandard",
    "section": "Versjonering av datasett",
    "text": "Versjonering av datasett\nVersjonering er obligatorisk n√•r man jobber med data p√• dapla. Hovedgrunnen til at vi versjonerer er for √• dekke kravet om uforanderlighet og etterpr√∏vbarehet: at data-konsumenter (menneske eller maskin) skal ha kontroll p√• endringer. Derfor skal et datasett som er brukt i statistikkproduksjon aldri slettes - det skal opprettes en ny versjon av datasettet. Les mer om prinsippet om uforanderlighet av data p√• confluence-siden til IT-Arkitektur.\nKort fortalt inneb√¶rer versjonering av data at datasettene har versjonsnummer f√∏r filendelsen. For eksempel: framskrevne-befolkningsendringer_p2019_p2050_v1.parquet\n\n\n\n\n\n\nUnntak til versjonering: nyeste versjon og tempor√¶re data\n\n\n\nNyeste versjon kan lagres uten versjonsnummer. Dette er for at man enkelt skal kunne lese inn siste versjon av et datasett (ved √• utelate versjonssuffiks). I tilegg trenger man ikke versjonere tempor√¶re data.\n\n\n\nN√•r skal man lagre ny versjon?\nF√∏lgende hendelser skaper ny versjon av et datasett:\n\nReberegninger av data med nye metoder.\nKorrigeringer av verdier.\nOservasjoner legges til eller fjernes.\nOppdatert eller erstattet kodeverk.\nVariabler fjernes eller legges til.\n\nHvis det gj√∏res vesentlige endringer (mange variabler) s√• b√∏r det vurderes om dette er et helt nytt datasett.\n\nAndre strukturendringer, f.eks. bytte av datatyper eller formater.\n\nMed andre ord: enhver endring skaper en ny versjon!\n\n\nVersjonering i praksis\nFor hver versjon som oppst√•r av datasettet opprettes det en ny fysisk fil hvor versjonsnummeret √∏kes med en. Alle gamle versjoner av et datasett skal ogs√• eksistere i mappen.\nEtterhvert som man f√•r flere versjoner av et datasett kan det se slik ut:\n\n\nMappe med flere versjoner av et datasett\n\nssb-prod-team-personstatistikk-data-produkt-prod/  \n‚îî‚îÄ‚îÄ befolkningsframskrivinger/  \n    ‚îî‚îÄ‚îÄ klargjorte-data/  \n        ‚îú‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050.parquet  \n        ‚îú‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050_v1.parquet  \n        ‚îú‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050_v2.parquet  \n        ‚îî‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050_v3.parquet\n\nEksempelet over viser at siste versjon av en fil kan lagres med og uten versjonsnummer for √• gj√∏re det lettere √• lese inn nyeste versjon.\n\nEksempelkode med pakken ssb-fagfunksjoner\n\n\nPython kode fra SSB-fagfunksjoner for finne neste filversjon\n\n# importer funksjonen next_version_path() fra ssb-fagfunksjoner\nfrom fagfunksjoner import next_version_path\n\nfilsti = 'gs://ssb-prod-team-personstatistikk-data-produkt-prod/befolkningsframskrivninger/klargjorte-data/framskrevne-befolkningsendringer_p2019_p2050.parquet'\n\nny_filsti = next_version_path(filsti)\n\nprint(ny_filsti)\n# vil returnere:\n# 'gs://ssb-prod-team-personstatistikk-data-produkt-prod/befolkningsframskrivninger/klargjorte-data/framskrevne-befolkningsendringer_p2019_p2050_v4.parquet'\n\nssb-fagfunksjoner har ogs√• f√∏lgende funksjoner for √• gj√∏re versjonering lettere:\n\nget_fileversions() # Retrieves a list of file versions matching a specified pattern.\nlatest_version_number() # Function for finding latest version in use for a file.\nlatest_version_path() # Finds the path to the latest version of a specified file.\n\n\n\n\n\n\n\nPermanente data skal ikke endres eller slettes\n\n\n\nTidligere delte/publiserte data skal ikke slettes eller overskrives!\nDet m√• derfor lagres fysiske filer for hver versjon av datasettet. Dette er viktig for at SSB skal oppfylle krav om etterpr√∏vbarhet av statistikkene.\n\n\n\n\nVersjon 0: Deling av data som ikke har oppn√•dd stabil tilstand\nHvis det er behov for √• dele data som fortsatt er under innsamling eller p√•g√•ende klargj√∏ring gj√∏res dette ved √• bruke versjonsnummer 0 i filnavnet.\nDette versjonsnummeret skal kun brukes midlertidig fram til datasettet oppn√•r stabil tilstand. Ved stabil tilstand byttes versjonsnummer for datasettet til 1 eller h√∏yere.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Navnestandard"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard.html#footnotes",
    "href": "statistikkere/navnestandard.html#footnotes",
    "title": "Navnestandard",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nLes mer om hvordan man henter ut informasjon fra API-et til Statistikkregisteret i denne blogg-artikkelen.‚Ü©Ô∏é\nDet er anbefalt at √¶, √∏ og √• erstattes med ae, oe og aa, f.eks. naering, oekonomi eller levekaar.‚Ü©Ô∏é",
    "crumbs": [
      "Manual",
      "Standarder",
      "Navnestandard"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html",
    "href": "statistikkere/jupyter-playground.html",
    "title": "Jupyter-playground",
    "section": "",
    "text": "Jupyter-playground er en tjeneste p√• Dapla Lab som er ment for nybegynnere og andre som vil komme raskt i gang med koding i Jupyterlab. Den har mange likheter med Jupyter-tjenesten p√• Dapla Lab med den forskjellen at mange flere pakker og extensions er ferdig installert i Jupyter-playground. Tjenesten har b√•de R og Python installert.\nSiden tjenesten er ment for oppl√¶ring og utforskning s√• er det ikke anbefalt √• bygge produksjonskode fra denne tjenesten. Grunnen til det er at det er flere avhengigheter mellom programvare enn n√∏dvendig, noe som skaper mer komplisert kode enn n√∏dvendig. Derimot er det et ideelt sted for √• l√¶re seg R eller Python siden man slipper kompleksiteten med √• installere sine egne pakker og forholde seg til ssb-project. For de som skal utvikle produksjonskode anbefales det at koden heller utvikles fra Jupyter-tjenesten.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#forberedelser",
    "href": "statistikkere/jupyter-playground.html#forberedelser",
    "title": "Jupyter-playground",
    "section": "Forberedelser",
    "text": "Forberedelser\nF√∏r man starter Jupyter-playground-tjenesten b√∏r man ha lest kapitlet om Dapla Lab og satt opp Git- og GitHub-konfigurasjonen under Min konto. Deretter gj√∏r du f√∏lgende:\n\nLogg deg inn p√• Dapla Lab\nUnder Tjenestekatalog trykker du p√• Start-knappen for Jupyter\nGi tjenesten et navn\n√Öpne Jupyter konfigurasjoner",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#konfigurasjon",
    "href": "statistikkere/jupyter-playground.html#konfigurasjon",
    "title": "Jupyter-playground",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nKonfigurasjonen av Jupyter-playground er identisk som for Jupyter-tjenesten. Se dokumentasjonen for konfigurasjon av Jupyter.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#datatilgang",
    "href": "statistikkere/jupyter-playground.html#datatilgang",
    "title": "Jupyter-playground",
    "section": "Datatilgang",
    "text": "Datatilgang\nSlik kan man inspisere dataene fra en terminal inne i tjenesten:\n\n√Öpne en instans av Jupyter med data fra b√∏tter\n√Öpne en terminal inne i Jupyter\nG√• til mappen med b√∏ttene ved √• kj√∏re dette fra terminalen cd /buckets\nKj√∏r ls -ahl i teminalen for √• se p√• hvilke b√∏tter som er montert.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#installere-pakker",
    "href": "statistikkere/jupyter-playground.html#installere-pakker",
    "title": "Jupyter-playground",
    "section": "Installere pakker",
    "text": "Installere pakker\nSiden det nesten ikke er installert noen pakker i tjenesten s√• kan brukeren opprette et ssb-project og installere pakker som vanlig.\nFor √• bygge et eksisterende ssb-project s√• kan brukeren ogs√• bruke ssb-project.\nFor √• installere R-pakker f√∏lger man beskrivelsen for renv.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#slette-tjenesten",
    "href": "statistikkere/jupyter-playground.html#slette-tjenesten",
    "title": "Jupyter-playground",
    "section": "Slette tjenesten",
    "text": "Slette tjenesten\nFor √• slette tjenesten kan man trykke p√• Slette-knappen i Dapla Lab under Mine tjenester. N√•r man sletter en tjeneste s√• sletter man hele disken inne i tjenesten og frigj√∏r alle ressurser som er reservert. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#pause-tjenesten",
    "href": "statistikkere/jupyter-playground.html#pause-tjenesten",
    "title": "Jupyter-playground",
    "section": "Pause tjenesten",
    "text": "Pause tjenesten\nMan kan pause tjenesten ved √• trykke p√• Pause-knappen i Dapla Lab under Mine tjenester. N√•r man pauser s√• slettes alt p√•den lokale disken som ikke er lagret under $HOME/work. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#monitorering",
    "href": "statistikkere/jupyter-playground.html#monitorering",
    "title": "Jupyter-playground",
    "section": "Monitorering",
    "text": "Monitorering\n\n\nMan kan moniterere en instans av Jupyter ved √• trykke p√• Jupyter-teksten under Mine tjenester i Dapla Lab, slik som vist i Figur¬†1.\nDenne funksjonaliteten er under arbeid og mer informasjon kommer snart.\n\n\n\n\n\n\n\n\n\nFigur¬†1: Monitorering av Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-data.html",
    "href": "statistikkere/jobbe-med-data.html",
    "title": "Jobbe med data",
    "section": "",
    "text": "N√•r man oppretter et dapla-team s√• f√•r vi tildelt et eget omr√•det for lagring av data. For √• kunne lese og skrive data fra Jupyter til disse omr√•dene m√• vi autentisere oss, siden Jupyter og lagringsomr√•det er to separate sikkerhetsoner.\nFigur¬†1 viser dette klarer skillet mellom hvor vi koder og hvor dataene ligger p√• Dapla1. I dette kapitlet beskriver vi n√¶rmere hvordan du kan jobbe med dataene dine p√• Dapla.",
    "crumbs": [
      "Manual",
      "Data"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-data.html#ssb-biblioteker",
    "href": "statistikkere/jobbe-med-data.html#ssb-biblioteker",
    "title": "Jobbe med data",
    "section": "SSB-biblioteker",
    "text": "SSB-biblioteker\nFor √• gj√∏re det enklere √• jobbe data p√• tvers av Jupyter og lagringsomr√•det er det laget noen egne SSB-utviklede biblioteker for √• gj√∏re vanlige operasjoner mot lagringsomr√•det. Siden b√•de R og Python skal brukes p√• Dapla, s√• er det laget to biblioteker, en for hver av disse spr√•kene. fellesr er biblioteket for R, og dapla-toolbelt er biblioteket for Python.\n\ndapla-toolbelt\ndapla-toolbelt er en en pakke som lar deg enkelt lese og skrive til lagringsomr√•det uten √• m√•tte autentifisere deg manuelt. Den har en Pandas-aktig syntaks som forh√•pentlig er gjenkjennbar for de fleste. Pakken er installert i alle Python-kernels p√• Dapla, s√• du trenger ikke √• installere den selv hvis du √•pner en notebook med Python3 for eksempel. For √• importere hele biblioteket i en notebook skriver du bare\n\n\nnotebook\n\nimport dapla as dp\n\ndapla-toolbelt bruker en pakke som heter gcsfs for √• kommunisere med lagringsomr√•det. gcsfs er en pakke som lar deg bruke Google Cloud Storage (GCS) som om det var en filsystem. Det betyr at du kan bruke samme syntaks som du bruker for √• lese og skrive til filer p√• din egen maskin. Du kan lese mulighetene i gcsfs her. Et eksempel p√• hvordan de to pakkene kan brukes sammen ser du her:\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Example of how you can use gcsfs and dapla-toolbelt together\nfs.touch(\"gs://my-bucket/my-folder/\")\n\nI koden over brukte jeg kommandoen touch fra gcsfs og FileClient fra dapla-toolbelt for √• opprette en mappe i lagringsomr√•det.\nI kapitlene under finner du konkrete eksempler p√• hvordan du kan bruke dapla-toolbelt til √• jobbe med data i SSBs lagringsomr√•det.\n\n\nfellesr\nR-pakken fellesr er under utvikling og gir mye av den samme funksjonaliteten som dapla-toolbelt gir for Python. I tillegg til √• kunne lese og skrive til lagringsomr√•det p√• Dapla, s√• har fellesr ogs√• funksjoner for √• jobbe med metadata p√• Dapla.\nfellesr er installert p√• Dapla og funksjoner kan benyttes ved:\n\n\nnotebook\n\nlibrary(fellesr)\n\nHvis du benytte en renv milj√∏, m√• pakken installeres en gang. Dette kan gj√∏res ved:\n\n\nnotebook\n\nrenv::install(\"statisticsnorway/fellesr\")",
    "crumbs": [
      "Manual",
      "Data"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-data.html#vanlige-operasjoner",
    "href": "statistikkere/jobbe-med-data.html#vanlige-operasjoner",
    "title": "Jobbe med data",
    "section": "Vanlige operasjoner",
    "text": "Vanlige operasjoner\nI denne delen viser vi hvordan man gj√∏r veldig vanlige operasjoner n√•r man koder et produksonsl√∏p for en statistikk. Flere eksempler p√• nyttige systemkommandoer finner du her.\n\nListe ut innhold i mappe\n\n\n\n\n\n\nEksempeldata i Dapla Felles\n\n\n\nDapla Felles er et team der alle i SSB er med i developers-gruppa. Dvs. at alle har lese- og skrivetilgang til f√∏lgende omr√•der:\ngs://ssb-dapla-felles-data-produkt-prod/ i prod-milj√∏et p√• Dapla, og\ngs://ssb-dapla-felles-data-produkt-test/ i test-milj√∏et. Eksemplene under bruker f√∏rstnevnte i koden, slik at alle kan kj√∏re koden selv.\nKode-eksemplene finnes for b√•de R og Python, og du kan velge hvilken du skal se ved √• trykke p√• den arkfanen du er interessert i.\n\n\n√Ö liste ut innhold i et gitt mappe p√• Dapla er ganske enkelt. Under ser du hvordan du kan liste ut innholdet i f√∏lgende mappe:\ngs://ssb-dapla-felles-data-produkt-prod/datadoc/brukertest/10/sykefratot/klargjorte_data\n\nPython \n\n\nVi bruker modulen FileClient fra dapla-toolbelt for √• liste ut innholdet i en mappe.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Set path to folder\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/datadoc/brukertest/10/sykefratot/klargjorte_data\"\n\nFileClient.ls(file_path)\n\nMed kommandoen over f√•r du listet ut alle filnavn i mappen. Hvis du vil ha mer informasjon om filene s√• kan du bruke ls-kommandoen med detail = True, som under:\n\n\nnotebook\n\nFileClient.ls(file_path, detail = True)\n\nSyntaksen med ls er veldig lik det man kjenner fra en Linux-terminal. Men n√•r vi bruker detail = True blir metadata om filene returnert som en Python-liste med dictionaries. Det kan v√¶re sv√¶rt nyttig n√•r du f.eks. trenger √• vite dato og tidspunkt for n√•r en fil ble opprettet, eller n√•r den sist ble oppdatert.\n\n\n\n\nnotebook\n\n# Loading functions into notebook\nlibrary(fellesr)\n\n# Path to folder\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/datadoc/brukertest/10/sykefratot/klargjorte_data\"\n\n# List files in folder \nlist.files(file_path)\n\nMerknad: N√•r du spesifisere b√∏tter i R, trenger du ikke ‚Äúgs://‚Äù foran.\n\n\n\n\n\nSkrive ut filer\n√Ö skrive filer til et lagringsomr√•de p√• Dapla er ogs√• ganske enkelt. Det ligner mye p√• den syntaksen vi er kjent med fra vanlige R- og Python-pakker, med noen sm√• unntak.\n\nParquet\nUnder lager vi en dataframe i en notebook og skriver den ut til en parquet-fil.\n\nPython \n\n\nN√•r vi leser en Parquet-fil med dapla-toolbelt s√• bruker den pyarrow i bakgrunnen. Dette er en av de raskeste m√•tene √• lese og skrive Parquet-filer p√•.\n\n\nnotebook\n\nimport dapla as dp\nimport pandas as pd\nimport numpy as np\n\n# Set path to folder\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-manual-examples\"\n\n# Create pandas dataframe\npurchases = pd.DataFrame(np.random.randn(10, 5),\n                        columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\n# Write pandas dataframe as parquet to GCS bucket\ndp.write_pandas(df = purchases,\n                gcs_path = f\"{file_path}/data.parquet\",\n                file_format = \"parquet\",)\n\nN√•r vi kalte write_pandas over s√• spesifiserte vi at filformatet skulle v√¶re parquet. Dette er default, s√• vi kunne ogs√• ha skrevet det slik:\n\n\nnotebook\n\ndp.write_pandas(df = purchases,\n                gcs_path = f\"{file_path}/data.parquet\")\n\nMen for de andre filformatene m√• vi alts√• spesifisere dette.\n\n\nN√•r vi jobber med Parquet-fil i R, bruker vi pakken arrow. Dette er en del av fellesr pakken s√• du trenger kun √• kalle inn dette. Pakken inneholder funksjonen write_SSB som kan brukes til √• skrive data til b√∏tte p√• Dapla.\n\n\nnotebook\n\nlibrary(fellesr)\n\n# Set stien til hvor data skal lagres\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-manual-examples\"\n\n# Lage en eksempel dataframe\npurchases = data.frame(A = runif(10), B= runif(10), C=runif(10))\n\n# Skrive data til b√∏ttet som en parquet\nwrite_SSB(purchases, file.path(file_path, \"purchases.parquet\"))\n\nMerknad: N√•r du spesifisere b√∏tter i R, trenger du ikke ‚Äúgs://‚Äù foran.\n\n\n\n\n\nTekstfiler\nNoen ganger √∏nsker vi √• lagre data i andre formatter slik som CSV, JSON og XML.\n\nPython \n\n\ndapla-toolbelt kan skrive ut json, csv og posisjonsfiler (fixed-width-files/fwf) til lagringsomr√•det. M√•ten den gj√∏r det p√• er √• bruke Pandas sine funksjoner read_json, read_csv, read_fwf under panseret. Dette kan v√¶re nyttig √• vite for skj√∏nne hvordan dapla-toolbelt h√•ndterer ulike strukturer i (spesielt hierarkiske) tekstfiler. Under ser du hvordan du kan skrive ut en dataframe til en json-fil.\n\n\nnotebook\n\nimport numpy as np\nimport pandas as pd\nfrom dapla import FileClient\n\n# Set path to folder\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-manual-examples\"\n\n# Create a dataframe with Pandas\ndf = pd.DataFrame(np.random.randn(10, 5), columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\n# Save dataframe as json with dapla-toolbelt\ndp.write_pandas(df = df,\n                gcs_path = f\"{file_path}/test.json\",\n                file_format = \"json\")\n\nSom vi ser at syntaksen over s√• kunne vi skrevet ut til noe annet enn json ved √• endre verdien i argumentet file_format.\n\n\nPakken fellesr kan ogs√• brukes til √• skrive andre type filer, for eksempel csv, til b√∏tter. Dette gj√∏res med funksjonen write_SSB og spesifisere √∏nsket filtype i filnavn.\nF√∏rst kaller vi biblioteket og lage noe test data ved:\n\n\nnotebook\n\nlibrary(fellesr)\n\n# Set stien til hvor data skal lagres\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-manual-examples\"\n\n# Lage en eksempel dataframe\npurchases = data.frame(A = runif(10), B= runif(10), C=runif(10))\n\n# Skrive til csv\nwrite_SSB(purchases, file.path(file_path, \"purchases.csv\")\n\n\n\n\n\n\nxlsx\nDet er ikke anbefalt √• bruke xlsx-formatet, men her ser du hvordan det kan skrives ut. Mer kommer.\n\nPython \n\n\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import AuthClient\n\n# Henter token for √• kunne lese fra Dapla\ntoken = AuthClient.fetch_google_credentials()\n\n# Filsti\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-manual-examples\"\n\ndf.to_excel(f\"{file_path}/test.xlsx\",\n           storage_options={\"token\": token})\n\n\n\nKommer snart\n\n\n\n\n\n\nLese inn filer\nUnder finner du eksempler p√• hvordan du kan lese inn data til en Jupyter Notebooks p√• Dapla.\n\nParquet\n\nPython \n\n\n\n\nnotebook\n\nimport dapla as dp\n\n# Set path to folder\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/datadoc/brukertest/10/sykefratot/klargjorte_data/person_testdata_p2021_v1.parquet\"\n\n# Read path into pandas dataframe \ndp.read_pandas(gcs_path= file_path,\n               file_format = \"parquet\",\n               columns = None,)\n\nSom vi s√• med write_pandas s√• er file_format default satt til parquet, og default for columns = None, s√• vi kunne ogs√• ha skrevet det slik:\ndp.read_pandas(gcs_path= file_path)\ncolumns-argumentet er en liste med kolonnenavn som vi √∏nsker √• lese inn. Hvis vi ikke spesifiserer noen kolonner s√• vil alle kolonnene leses inn.\n\n\nPakken fellesr kan brukes til √• lese inn data. Funksjonen read_SSB() kan lese inn filer i flere format inkluderende parquet.\nHer er et eksempel av √• lese inn parquet fil ‚Äú1987‚Äù.\n\n\nnotebook\n\nlibrary(fellesr)\n\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/datadoc/brukertest/10/sykefratot/klargjorte_data/person_testdata_p2021_v1.parquet\"\n\ndt_1987 &lt;- read_SSB(file.path(file_path))\n\nVi kan ogs√• filtrere hvilke variabel vi √∏nsker √• lese inn ved √• spesifisere parameter col_select. For eksempel:\ndt_1987 &lt;- read_SSB(file.path(file_path),\n                    col_select = c(\"fnr\", \"sivilstand\"))\nInnlesning av parquet som er kartdata finner du her: Lese kartdata\n\n\n\n\n\nTekstfiler\nKommer mer snart. Python-koden under bygger p√• eksempelet over.\n\nPython \n\n\n\n\nnotebook\n\nimport dapla as dp\n\n# Path to write to\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-manual-examples/test.json\"\n\n# Read in json-file from dapla-storage\ndf = dp.read_pandas(gcs_path = file_path,\n               file_format = \"json\")\n\n\n\nFunksjonen read_SSB() kan lese inn flere type av fil-format, slik som csv og json. Du trenger ikke √• endre koden, kun spesifisere hele filnavn.\nF√∏rst kaller vi inn biblioteket fellesr og spesifisere b√∏tte/mappen:\n\n\nnotebook\n\nlibrary(fellesr)\n\n# Filsti\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-manual-examples/purchases.csv\"\n\n# Lese inn CSV-fil\ndt_1987 &lt;- read_SSB(file.path(file_path))\n\nFor √• lese inn en json-fil kan skrive f√∏lgende:\n\n\nnotebook\n\ndt_1987 &lt;- read_SSB(file.path(\"ssb-dapla-felles-data-produkt-prod/dapla-manual-examples/test.json\"))\n\n\n\n\n\n\nxlsx\n\nPython \n\n\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import AuthClient\n\n# Hent token\ntoken = AuthClient.fetch_google_credentials()\n\n# Les inn fil\ndf = pd.read_excel(\"gs://ssb-dapla-felles-data-produkt-prod/dapla-manual-examples/test.xlsx\",\n    storage_options={\"token\": token})\n\n\n\nKommer snart\n\n\n\n\n\nSAS\nHer er et eksempel p√• hvordan man leser inn en sas7bdat-fil p√• Dapla som har blitt generert i prodsonen.\n\nPython \n\n\n\n\nnotebook\n\nimport dapla as dp\n\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-manual-examples/statbank_ledstill.sas7bdat\"\n\ndp.read_pandas(file_path, file_format=\"sas7bdat\", encoding=\"latin1\")\n\n\n\nSiden innlesing av sas7bdat-filer ikke er st√∏ttet i fellesr, s√• kan vi bruke R-pakken reticulate for √• benytte oss av funksjonaliteten i Python-pakken dapla-toolbelt. Dette fordrer at man kj√∏rer R i et kj√∏remilj√∏ som ogs√• har Python installert slik som Jupyter. Reticulate-pakken st√∏ttes ikke i Rstudio p√• Dapla.\n\n\nnotebook\n\nlibrary(reticulate)\ndp &lt;- import(\"dapla\")\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-manual-examples/statbank_ledstill.sas7bdat\"\ndp$read_pandas(file_path, file_format=\"sas7bdat\", encoding=\"latin1\")\n\n\n\n\n\n\n\nSlette filer\n√Ö slette filer fra lagringsomr√•det kan gj√∏res p√• flere m√•ter. I kapitlet om sletting av data viste vi hvordan man gj√∏r det med pek-og-klikk i Google Cloud Console. Under ser du hvordan du kan slette filer med dapla-toolbelt og gcsfs eller fellesr.\n\nPython \n\n\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Skriv inn full filsti til filen som skal slettes\nfile_path = \"\"\n\nfs.rm(file_path)\n\n\n\nFunksjonen gc_delete_object kan brukes til √• slette data p√• lagringsomr√•det.\n\n\nnotebook\n\nlibrary(fellesr)\n\n# Skriv inn full filsti til filen som skal slettes\nfile_path = \"\"\n\ngcs_delete_object(file.path(file_path))\n\n\n\n\n\n\nKopiere filer\n√Ö kopiere filer mellom mapper p√• et Linux-filsystem inneb√¶rer som regel bruke cp-kommandoen. P√• Dapla er det ikke s√• mye forskjell. Vi bruker en ligende tiln√¶rming n√• vi skal kopiere mellom b√∏tter eller mapper p√• lagringsomr√•det til SSB. Under ser du hvordan du kan kopiere en fil fra en mappe til en annen.\n\nPython \n\n\nLa oss begynne med et eksempel der vi kopierer en fil fra en mappe til en annen i samme b√∏tte.\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Path to folders\nbucket = \"gs://ssb-dapla-felles-data-produkt-prod\"\nfrom_folder = \"felles/veiledning/python/eksempler/purchases\"\nto_folder = \"felles/veiledning/python/eksempler\"\n\n# Copy file\nfs.cp(f\"{bucket}/{from_folder}/data.parquet\",\n      f\"{bucket}/{to_folder}/data_copy.parquet\")\n\nDet fungerer ogs√• for √• kopiere filer mellom b√∏tter.\nEt annet scenario vi ofte vil st√∏te p√• er at vi √∏nsker √• kopiere en fil fra v√•rt Jupyter-filsystem til en mappe p√• lagringsomr√•det. Her kan vi bruke fs.put-metoden.\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Create a new file in your home directory called test.txt\nwith open('/home/jovyan/test.txt', 'w') as f:\n    f.write('Create a new text file!')\n\n#Path to folder\nbucket = \"gs://ssb-dapla-felles-data-produkt-prod\"\nfolder = \"felles/veiledning/python/eksempler\"\n\n# Copy file from local to remote file system\nfs.put(lpath=f\"/home/jovyan/test.txt\", rpath=f\"{bucket}/{folder}/test.txt\")\n\n√ònsker vi √• kopiere en hel mappe fra lagringsomr√•det til Jupyter-filsystemet, kan vi bruke fs.get-metoden, med opsjonen recursive=True.\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Copy file\nfs.get(&lt;from_bucket&gt;,\n      \"/home/jovyan/sesongjustering/\",\n      recursive=True)\n\n\n\nKommer snart\n\n\n\n\n\nFlytte filer\n\nPython \n\n\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\nbucket = \"gs://ssb-dapla-felles-data-produkt-prod\"\nfrom_folder = \"felles/veiledning/python/eksempler/purchases\"\nto_folder = \"felles/veiledning/python/eksempler\"\n\nfs.mv(f\"{bucket}/{from_folder}/data.parquet\", f\"{bucket}/{to_folder}/data.parquet\")\n\n\n\nKommer snart\n\n\n\n\n\nOpprette mapper\nSelv om b√∏tter ikke har mapper med en hierarkisk struktur slik man er kjent med fra klassike filsystemer, s√• kan man opprette det som ser ut som mapper i objektnavnet. I realiteten blir bare / oppfattet som en del av navnet p√• objektet. Skulle du likevel √∏nske √• opprette dette s√• kan du gj√∏re det f√∏lgende m√•te:\n\nPython \n\n\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n#Path to folder\nbucket = \"gs://ssb-dapla-felles-data-produkt-prod\"\nfolder = \"felles/veiledning/python/eksempler\"\n\n# Create folder\nfs.touch(f\"{bucket}/{folder}/testmappe/\")\n\n\n\nKommer snart",
    "crumbs": [
      "Manual",
      "Data"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-data.html#footnotes",
    "href": "statistikkere/jobbe-med-data.html#footnotes",
    "title": "Jobbe med data",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI de tidligere systemene p√• bakken s√• var det ikke n√∏dvendig med autentisering mellom kodemilj√∏ og datalagringen‚Ü©Ô∏é",
    "crumbs": [
      "Manual",
      "Data"
    ]
  },
  {
    "objectID": "statistikkere/innlogging.html",
    "href": "statistikkere/innlogging.html",
    "title": "Innlogging",
    "section": "",
    "text": "Innlogging p√• Dapla er veldig enkelt. Dapla er en nettadresse som alle SSB-ere kan g√• inn p√• hvis de er logget p√• SSB sitt nettverk. √Ö v√¶re logget p√• SSB sitt nettverk betyr i denne sammenhengen at man er logget p√• med VPN, enten man er p√• kontoret eller p√• hjemmekontor. For √• gj√∏re det enda enklere har vi laget en fast snarvei til denne nettadressen p√• v√•rt intranett/Byr√•nettet(se Figur¬†1).\n\n\n\n\n\n\nFigur¬†1: Snarvei til Dapla fra intranett\n\n\n\nMen samtidig som det er lett √• logge seg p√•, s√• er det noen kompliserende ting som fortjener en forklaring. Noe skyldes at vi mangler et klart spr√•k for √• definere bakkemilj√∏et og skymilj√∏et slik at alle skj√∏nner hva man snakker om. I denne boken definerer bakkemilj√∏et som stedet der man har drevet med statistikkproduksjon de siste ti√•rene. Skymilj√∏et er den nye dataplattformen Dapla p√• Google Cloud.\nDet som gj√∏r ting litt komplisert er at vi har 2 Jupyter-milj√∏er p√• b√•de bakke og sky. √Örsaken er at vi har ett test- og ett prod-omr√•de for hver, og det blir i alt 4 Jupyter-milj√∏er. Figur¬†2 viser dette.\n\n\n\n\n\n\nFigur¬†2: De 4 Jupyter-milj√∏ene i SSB. Et test-milj√∏ og et prod-milj√∏ p√• bakke og sky/Dapla\n\n\n\nHver av disse milj√∏ene har sin egen nettadresse og sitt eget bruksomr√•de.\n\n\nI de fleste tilfeller vil en statistikker eller forsker √∏nske √• logge seg inn i prod-milj√∏et. Det er her man skal kj√∏re koden sin i et produksjonsl√∏p som skal publiseres eller utvikles. I noen tilfeller hvor man ber om √• f√• tilgjengliggjort en ny tjeneste s√• vil denne f√∏rst rulles ut i testomr√•det som vi kaller staging-omr√•det. √Örsaken er at vi √∏nsker √• beskytte prod-milj√∏et fra software som potensielt √∏delegger for eksisterende funksjonalitet. Derfor ruller vi ut nye ting i staging f√∏rst. Av den grunn vil de fleste oppleve √• bli bedt om √• logge seg inn der for testing en eller annen gang. Under forklarer vi hvordan man g√•r frem for √• logge seg p√• de to ulike milj√∏ene p√• Dapla.\n\n\nFor √• logge seg inn inn i prod-milj√∏et p√• Dapla kan man gj√∏re f√∏lgende:\n\nG√• inn p√• lenken https://jupyter.dapla.ssb.no/ i en Chrome-nettleser eller klikk p√• lenken p√• Byr√•nettet som vist i Figur¬†1.\nAlle i SSB har en Google Cloud-konto som m√• brukes n√•r man logger seg p√• Dapla. Brukernavnet i Google er det samme som din korte epost-adresse (f.eks. cth@ssb.no). Hvis du ikke allerede er logget inn i Google vil du f√• sp√∏rsm√•l om √• velge hvilken Google-konto som skal brukes (Figur¬†3). Logg inn med din Google-konto (ssb.no) og ditt AD-passord.\n\n\n\n\n\n\n\nFigur¬†3: Velg en Google-konto\n\n\n\n\nDeretter blir man spurt om man godtar at ssb.no (alts√• Dapla) kan bruke din Google Cloud-konto (Figur¬†4). Trykk Allow.\n\n\n\n\n\n\n\nFigur¬†4: Tillat at ssb.no f√•r bruke din Google Cloud-konto\n\n\n\n\nDeretter lander man p√• en side som lar deg avgj√∏re hvor mye maskinkraft som skal holdes av til deg (Figur¬†5). Det √∏verste alternativet er valgt som standard, og er tilstrekkelig for de fleste.\n\n\n\n\n\n\n\nFigur¬†5: Velg hvor mye maskinkraft du trenger\n\n\n\n\nVent til maskinen din starter opp (Figur¬†6). Oppstartstiden kan variere.\n\n\n\n\n\n\n\nFigur¬†6: Starter opp Jupyter\n\n\n\nEtter dette er man logget inn i et Jupyter-milj√∏ som kj√∏rer p√• en minimal Ubuntu-maskin. Hvis man er del av et Dapla-team f√•r man ogs√• tilgang til alt teamet har tilgang til.\n\n\n\nInnlogging til staging-milj√∏et er identisk med innloggingen til prod-milj√∏et, med ett viktig unntak: nettadressen er n√• https://jupyter.dapla-staging.ssb.no/.\nLitt mer om hva som er tilgjenglig her kommer.\n\n\n\n\n\nNoen ganger kan man komme i en situasjon hvor l√∏sningen for Single Sign-On (p√•logging p√• tvers av flere systemer) gir en feilmelding a la Figur¬†7:\n\n\n\n\n\n\nFigur¬†7: Feil som kan oppst√• ved p√•logging\n\n\n\nI denne situasjonen m√• man trykke p√• knappen ‚ÄúAdd to existing account‚Äù. Da vil skjermbildet Figur¬†8 dukke opp:\n\n\n\n\n\n\nFigur¬†8: Klikk p√• Google-knappen for √• logge p√• igjen\n\n\n\nHer m√• man tykke p√• Google-knappen (se pil), og deretter logge inn som vist i Figur¬†3 tidligere i dette avsnittet.\n\n\n\n\n\nJupyter-milj√∏et p√• bakken bruker samme base-image1 for √• installere Jupyterlab, og er derfor identisk p√• mange m√•ter. Men innloggingen er ganske forskjellig.\n\n\n\n\n\n\nNote\n\n\n\nFom. 5. desember 2022 har vi byttet ut Jupyter-milj√∏et p√• bakken. Beskrivelsene under gjelder derfor det nye milj√∏et. Fram til 15. januar vil du kunne bruke det gamle milj√∏et ved √• g√• inn p√• lenken https://jupyter-prod.ssb.no/ manuelt i Google Chrome. Etter 15. januar blir det gamle Jupyter-milj√∏et avviklet.\n\n\n\n\nDu logger deg inn p√• prod i bakkemilj√∏et p√• f√∏lgende m√•te:\n\nLogg deg inn p√• Citrix-Windows i bakkemilj√∏et. Det kan gj√∏res ved √• bruke lenken Citrix p√• Byr√•nettet, som ogs√• vises i Figur¬†1.\nTrykk p√• Jupyterlab-ikonet, som vist p√• Figur¬†9, og logg deg inn med vanlig brukernavn og passord.\n\n\n\n\n\n\n\nFigur¬†9: Jupyterlab-ikon p√• Skrivebordet i Citrix-Windows.\n\n\n\nN√•r du trykker p√• ikonet blir du tatt til nettadressen https://sl-jupyter-p.ssb.no/. Du kunne ogs√• √•pnet Jupyterlab ved √•pne Chrome-nettleseren og skrive inn adressen manuelt.\n\n\n\nInnlogging til staging-milj√∏et har ingen snarvei p√• Skrivebordet, og du m√• gj√∏re f√∏lgende for √• √•pne milj√∏et:\n\n√Öpne Chrome-nettleseren i Citrix-Windows.\nSkriv inn url-en https://sl-jupyter-t.ssb.no/",
    "crumbs": [
      "Manual",
      "Appendix",
      "Innlogging"
    ]
  },
  {
    "objectID": "statistikkere/innlogging.html#dapla",
    "href": "statistikkere/innlogging.html#dapla",
    "title": "Innlogging",
    "section": "",
    "text": "I de fleste tilfeller vil en statistikker eller forsker √∏nske √• logge seg inn i prod-milj√∏et. Det er her man skal kj√∏re koden sin i et produksjonsl√∏p som skal publiseres eller utvikles. I noen tilfeller hvor man ber om √• f√• tilgjengliggjort en ny tjeneste s√• vil denne f√∏rst rulles ut i testomr√•det som vi kaller staging-omr√•det. √Örsaken er at vi √∏nsker √• beskytte prod-milj√∏et fra software som potensielt √∏delegger for eksisterende funksjonalitet. Derfor ruller vi ut nye ting i staging f√∏rst. Av den grunn vil de fleste oppleve √• bli bedt om √• logge seg inn der for testing en eller annen gang. Under forklarer vi hvordan man g√•r frem for √• logge seg p√• de to ulike milj√∏ene p√• Dapla.\n\n\nFor √• logge seg inn inn i prod-milj√∏et p√• Dapla kan man gj√∏re f√∏lgende:\n\nG√• inn p√• lenken https://jupyter.dapla.ssb.no/ i en Chrome-nettleser eller klikk p√• lenken p√• Byr√•nettet som vist i Figur¬†1.\nAlle i SSB har en Google Cloud-konto som m√• brukes n√•r man logger seg p√• Dapla. Brukernavnet i Google er det samme som din korte epost-adresse (f.eks. cth@ssb.no). Hvis du ikke allerede er logget inn i Google vil du f√• sp√∏rsm√•l om √• velge hvilken Google-konto som skal brukes (Figur¬†3). Logg inn med din Google-konto (ssb.no) og ditt AD-passord.\n\n\n\n\n\n\n\nFigur¬†3: Velg en Google-konto\n\n\n\n\nDeretter blir man spurt om man godtar at ssb.no (alts√• Dapla) kan bruke din Google Cloud-konto (Figur¬†4). Trykk Allow.\n\n\n\n\n\n\n\nFigur¬†4: Tillat at ssb.no f√•r bruke din Google Cloud-konto\n\n\n\n\nDeretter lander man p√• en side som lar deg avgj√∏re hvor mye maskinkraft som skal holdes av til deg (Figur¬†5). Det √∏verste alternativet er valgt som standard, og er tilstrekkelig for de fleste.\n\n\n\n\n\n\n\nFigur¬†5: Velg hvor mye maskinkraft du trenger\n\n\n\n\nVent til maskinen din starter opp (Figur¬†6). Oppstartstiden kan variere.\n\n\n\n\n\n\n\nFigur¬†6: Starter opp Jupyter\n\n\n\nEtter dette er man logget inn i et Jupyter-milj√∏ som kj√∏rer p√• en minimal Ubuntu-maskin. Hvis man er del av et Dapla-team f√•r man ogs√• tilgang til alt teamet har tilgang til.\n\n\n\nInnlogging til staging-milj√∏et er identisk med innloggingen til prod-milj√∏et, med ett viktig unntak: nettadressen er n√• https://jupyter.dapla-staging.ssb.no/.\nLitt mer om hva som er tilgjenglig her kommer.\n\n\n\n\n\nNoen ganger kan man komme i en situasjon hvor l√∏sningen for Single Sign-On (p√•logging p√• tvers av flere systemer) gir en feilmelding a la Figur¬†7:\n\n\n\n\n\n\nFigur¬†7: Feil som kan oppst√• ved p√•logging\n\n\n\nI denne situasjonen m√• man trykke p√• knappen ‚ÄúAdd to existing account‚Äù. Da vil skjermbildet Figur¬†8 dukke opp:\n\n\n\n\n\n\nFigur¬†8: Klikk p√• Google-knappen for √• logge p√• igjen\n\n\n\nHer m√• man tykke p√• Google-knappen (se pil), og deretter logge inn som vist i Figur¬†3 tidligere i dette avsnittet.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Innlogging"
    ]
  },
  {
    "objectID": "statistikkere/innlogging.html#bakkemilj√∏et",
    "href": "statistikkere/innlogging.html#bakkemilj√∏et",
    "title": "Innlogging",
    "section": "",
    "text": "Jupyter-milj√∏et p√• bakken bruker samme base-image1 for √• installere Jupyterlab, og er derfor identisk p√• mange m√•ter. Men innloggingen er ganske forskjellig.\n\n\n\n\n\n\nNote\n\n\n\nFom. 5. desember 2022 har vi byttet ut Jupyter-milj√∏et p√• bakken. Beskrivelsene under gjelder derfor det nye milj√∏et. Fram til 15. januar vil du kunne bruke det gamle milj√∏et ved √• g√• inn p√• lenken https://jupyter-prod.ssb.no/ manuelt i Google Chrome. Etter 15. januar blir det gamle Jupyter-milj√∏et avviklet.\n\n\n\n\nDu logger deg inn p√• prod i bakkemilj√∏et p√• f√∏lgende m√•te:\n\nLogg deg inn p√• Citrix-Windows i bakkemilj√∏et. Det kan gj√∏res ved √• bruke lenken Citrix p√• Byr√•nettet, som ogs√• vises i Figur¬†1.\nTrykk p√• Jupyterlab-ikonet, som vist p√• Figur¬†9, og logg deg inn med vanlig brukernavn og passord.\n\n\n\n\n\n\n\nFigur¬†9: Jupyterlab-ikon p√• Skrivebordet i Citrix-Windows.\n\n\n\nN√•r du trykker p√• ikonet blir du tatt til nettadressen https://sl-jupyter-p.ssb.no/. Du kunne ogs√• √•pnet Jupyterlab ved √•pne Chrome-nettleseren og skrive inn adressen manuelt.\n\n\n\nInnlogging til staging-milj√∏et har ingen snarvei p√• Skrivebordet, og du m√• gj√∏re f√∏lgende for √• √•pne milj√∏et:\n\n√Öpne Chrome-nettleseren i Citrix-Windows.\nSkriv inn url-en https://sl-jupyter-t.ssb.no/",
    "crumbs": [
      "Manual",
      "Appendix",
      "Innlogging"
    ]
  },
  {
    "objectID": "statistikkere/innlogging.html#footnotes",
    "href": "statistikkere/innlogging.html#footnotes",
    "title": "Innlogging",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nHva er base-image?‚Ü©Ô∏é",
    "crumbs": [
      "Manual",
      "Appendix",
      "Innlogging"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html",
    "href": "statistikkere/vscode-python.html",
    "title": "Vscode-python",
    "section": "",
    "text": "Vscode-python er en tjeneste p√• Dapla Lab for utvikling av kode i Python1. M√•lgruppen for tjenesten er brukere som skal skrive produksjonskode i Python.\nSiden tjenesten er ment for produksjonskode s√• er det veldig f√• forh√•ndsinstallerte Python-pakker som er installert. Antagelsen er at brukerene/teamet heller b√∏r installere de pakkene de trenger, framfor at alle pakker som alle brukere/team er forh√•ndsinstallert og skal dekke behovet til alle. Det reduserer kompleksitet i tjenesten og dermed sannsynligheten for feilsituasjoner.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#forberedelser",
    "href": "statistikkere/vscode-python.html#forberedelser",
    "title": "Vscode-python",
    "section": "Forberedelser",
    "text": "Forberedelser\nF√∏r man starter Vscode-python b√∏r man ha lest kapitlet om Dapla Lab og satt opp Git- og GitHub-konfigurasjonen under Min konto. Deretter gj√∏r du f√∏lgende:\n\nLogg deg inn p√• Dapla Lab\nUnder Tjenestekatalog trykker du p√• Start-knappen for Vscode-python\nGi tjenesten et navn\n√Öpne Vscode-python konfigurasjoner",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#konfigurasjon",
    "href": "statistikkere/vscode-python.html#konfigurasjon",
    "title": "Vscode-python",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nKonfigurasjonen av VSCode er n√¶r identisk Jupyter-tjenesten sin konfigurasjon. Se dokumentasjonen for konfigurasjon av Jupyter.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#datatilgang",
    "href": "statistikkere/vscode-python.html#datatilgang",
    "title": "Vscode-python",
    "section": "Datatilgang",
    "text": "Datatilgang\nSlik kan man inspisere dataene fra en terminal inne i tjenesten:\n\n√Öpne en instans av Vscode-python med data fra b√∏tter\n√Öpne en terminal inne i Vscode-python\nG√• til mappen med b√∏ttene ved √• kj√∏re dette fra terminalen cd /buckets\nKj√∏r ls -ahl i teminalen for √• se p√• hvilke b√∏tter som er montert.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#installere-pakker",
    "href": "statistikkere/vscode-python.html#installere-pakker",
    "title": "Vscode-python",
    "section": "Installere pakker",
    "text": "Installere pakker\nSiden det nesten ikke er installert noen pakker i tjenesten s√• kan brukeren opprette et ssb-project og installere pakker som vanlig.\nFor √• bygge et eksisterende ssb-project s√• kan brukeren ogs√• bruke ssb-project.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#slette-tjenesten",
    "href": "statistikkere/vscode-python.html#slette-tjenesten",
    "title": "Vscode-python",
    "section": "Slette tjenesten",
    "text": "Slette tjenesten\nFor √• slette tjenesten kan man trykke p√• Slette-knappen i Dapla Lab under Mine tjenester. N√•r man sletter en tjeneste s√• sletter man hele disken inne i tjenesten og frigj√∏r alle ressurser som er reservert. Siden pakkene som er installert ogs√• ligger lagret p√• disken, betyr dette at pakkene m√• installeres p√• nytt etter at en tjeneste er slettet. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#pause-tjenesten",
    "href": "statistikkere/vscode-python.html#pause-tjenesten",
    "title": "Vscode-python",
    "section": "Pause tjenesten",
    "text": "Pause tjenesten\nMan kan pause tjenesten ved √• trykke p√• Pause-knappen i Dapla Lab under Mine tjenester. N√•r man pauser s√• slettes alt p√•den lokale disken som ikke er lagret under $HOME/work. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#monitorering",
    "href": "statistikkere/vscode-python.html#monitorering",
    "title": "Vscode-python",
    "section": "Monitorering",
    "text": "Monitorering\n\n\nMan kan moniterere en instans av Vscode-python ved √• trykke p√• Vscode-python-teksten under Mine tjenester i Dapla Lab, slik som vist i Figur 1.\n\n\n\n\n\n\n\n\n\nFigur¬†1: Monitorering av Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#extensions",
    "href": "statistikkere/vscode-python.html#extensions",
    "title": "Vscode-python",
    "section": "Extensions",
    "text": "Extensions\nVSCode f√∏lger med et sett med extensions ferdig installert. Disse kan per n√• ikke installeres av brukeren selv.\n\nJupytext\nJupytext-filer kan jobbes med som notebooks i Jupyter. For √• gj√∏re dette, m√• man legge til Jupytext som en Python-avhengighet i ditt Python-prosjekt:\npoetry add --group dev \"jupytext &gt;=1\"\n..og deretter velge din pakkes Python-versjon som √• v√¶re interpreter. Dette gj√∏r man ved √• trykke p√• den r√∏de boksen p√• bildet, og velge interpreteren p√• filstien &lt;PAKKENAVN&gt;/.venv/bin/python.\n\n\n\n\n\n\nFigur¬†2: Monitorering av Jupyter-tjenesten i Dapla Lab\n\n\n\nDeretter kan man h√∏yreklikke p√• filen og trykke ‚ÄúOpen as Jupyter Notebook‚Äù.\n\n\n\n\n\n\nFigur¬†3: Konfigurasjon av Git for Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#footnotes",
    "href": "statistikkere/vscode-python.html#footnotes",
    "title": "Vscode-python",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nVscode-python er web-versjonen av VS Code og er ikke helt identisk med desktop-versjonen av VS Code mange er kjent med. Blant annet er det kun extensions fra Open VSX Registry som kan installeres.‚Ü©Ô∏é",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/altinn-skjema-administrasjon.html",
    "href": "statistikkere/altinn-skjema-administrasjon.html",
    "title": "Administrasjon av skjema",
    "section": "",
    "text": "SU-V tilbyr b√•de et grafisk brukergrensesnitt (GUI) og en Python-pakke for administrasjon av skjemaer. Du kan f√• tilgang til GUI via f√∏lgende lenker:\nMer informasjon om Python-pakken finner du her.\nAlle operasjoner og visninger som er tilgjengelige i GUI, kan ogs√• utf√∏res ved hjelp av kode. Administrasjon av skjema inneb√¶rer h√•ndtering av metadata knyttet til skjemaer, perioder, puljer og utsendinger."
  },
  {
    "objectID": "statistikkere/altinn-skjema-administrasjon.html#tilgang-til-skjema",
    "href": "statistikkere/altinn-skjema-administrasjon.html#tilgang-til-skjema",
    "title": "Administrasjon av skjema",
    "section": "Tilgang til skjema",
    "text": "Tilgang til skjema\nAlle brukere har lesetilgang til skjemaet, noe som betyr at de kan se og hente data uten √• gj√∏re endringer.\nFor √• kunne utf√∏re administrasjonsoppgaver og gj√∏re endringer p√• skjemaet, m√• du v√¶re medlem av data-admins i Dapla-teamet som eier skjemaet. Kun data-admins har mulighet til √• redigere eller konfigurere skjemaet. Se mer om administrasjon av team her."
  },
  {
    "objectID": "statistikkere/altinn-skjema-administrasjon.html#hierarki-for-skjema-administrasjon",
    "href": "statistikkere/altinn-skjema-administrasjon.html#hierarki-for-skjema-administrasjon",
    "title": "Administrasjon av skjema",
    "section": "Hierarki for skjema administrasjon",
    "text": "Hierarki for skjema administrasjon\nEt skjema best√•r av f√∏lgende strukturer:\n\nSkjema kan inneholde flere perioder.\nEn periode kan inneholde flere puljer.\nEn pulje kan inneholde flere utsendinger.\n\nIllustrasjonen nedenfor viser sammenhengen mellom de ulike niv√•ene:\n\n\n\n\n\n\nFigur¬†1: Avhengigheter skjema administrasjon"
  },
  {
    "objectID": "statistikkere/altinn-skjema-administrasjon.html#generelle-ui-operasjoner",
    "href": "statistikkere/altinn-skjema-administrasjon.html#generelle-ui-operasjoner",
    "title": "Administrasjon av skjema",
    "section": "Generelle UI operasjoner",
    "text": "Generelle UI operasjoner\nMange av skjermbildene i GUI tilbyr de samme typene operasjoner. Disse operasjonene er beskrevet i tabellen nedenfor.\nTabell¬†1 beskriver generelle UI operasjoner.\n\n\n\nTabell¬†1: UI operasjoner\n\n\n\n\n\n\n\n\n\nIkon\nForklaring\n\n\n\n\n\nLegg til en ny rad.\n\n\n\nRedigere en rad.\n\n\n\nKopiere en rad.\n\n\n\nSlett en rad.\n\n\n\nVise detaljer for en rad.\n\n\n\nG√• tilbake til forrige skjermbilde."
  },
  {
    "objectID": "statistikkere/altinn-skjema-administrasjon.html#skjema",
    "href": "statistikkere/altinn-skjema-administrasjon.html#skjema",
    "title": "Administrasjon av skjema",
    "section": "Skjema",
    "text": "Skjema\nFor √• begynne √• jobbe med et skjema, m√• du finne riktig skjemaversjon i skjemakatalogen.\nSlik s√∏ker du fram et skjema\n\nVelg Skjemadata i venstremenyen.\nBruk s√∏kefeltet i det nye skjermbildet for √• finne skjemaet ditt.\nKlikk p√• Vis detaljer-ikonet i s√∏keresultatet for √• se mer informasjon om skjemaet.\n\nBildet nedenfor viser eksempel p√• s√∏kebilde:\n\n\n\n\n\n\nFigur¬†2: S√∏ke fram skjema\n\n\n\n\n\n\n\n\n\nHva hvis skjema mangler?\n\n\n\nHvis du ikke finner riktig skjemaversjon, ta kontakt med planleggeren for skjemaet p√• seksjon 821.\n\n\n\nSkjema metadata\nHvert skjema har tilknyttet metadata som gir detaljert informasjon om det. Bildet nedenfor viser et eksempel p√• metadata for et skjema: \n\n\n\n\n\n\nBeskrivelse av skjema metadata\n\n\n\n\n\nTabell¬†2 beskriver metadatafeltene for et skjema\n\n\n\nTabell¬†2: Skjema metadata\n\n\n\n\n\n\n\n\n\nMeta\nForklaring\n\n\n\n\nRA-nummer\nRA-nummer for skjemaet.\n\n\nVersjon\nSkjemaversjon.\n\n\nUnd.nr\nUnders√∏kelsesnummer.\n\n\nDatamodell\nNavn p√• datamodellen.\n\n\nNavn\nNavn p√• unders√∏kelsen.\n\n\nNavn nynorsk\nNavn p√• unders√∏kelsen (nynorsk).\n\n\nNavn engelsk\nNavn p√• unders√∏kelsen (engelsk).\n\n\nEier\nDapla-team.\n\n\nGyldig fra\nFra hvilken dato unders√∏kelsen er gyldig.\n\n\nGyldig til\nTil hvilken dato unders√∏kelsen er gyldig.\n\n\nURL infoside\nInformasjonsside for unders√∏kelsen.\n\n\nBeskrivelse\nTilleggsinformasjon om unders√∏kelsen.\n\n\n\n\n\n\n\n\n\n\n\nKodeeksempel\nFor √• hente ut et skjema med tilh√∏rende metadata i Python, kan du bruke metoden get_skjema_by_ra_nummer i SuvClient. S√∏rg for at du oppgir riktig RA-nummer og versjon.\n\n\nnotebook\n\nclient = SuvClient()\n\noutput = client.get_skjema_by_ra_nummer(\n            ra_nummer = \"RA-0678A3\", \n            versjon = 2 \n        )\n        \nprint(json.dumps(output, indent=4))\n\n\n\nLegge til skjema som favoritt\nFor √• gj√∏re det enklere √• finne skjemaene du jobber mest med, kan du legge dem til som favoritter. Dette er spesielt nyttig ettersom skjemakatalogen kan inneholde mange ulike skjemaer og versjoner.\nSlik legger du til et skjema som favoritt:\n\nS√∏k opp skjemaet: Bruk s√∏kefunksjonen for √• finne skjemaet du √∏nsker.\nVis detaljer: Klikk p√• skjemaet i tabellen for √• √•pne detaljvisningen.\nLegg til som favoritt: Trykk p√• stjerneikonet ved siden av skjemaets navn. N√•r stjernen er markert, er skjemaet lagt til som favoritt.\n\nSkjemaet vil deretter v√¶re tilgjengelig under Favoritter i venstremenyen.\nBildet nedenfor viser hvordan du velger et skjema som favoritt:\n\n\n\n\n\n\nFigur¬†3: Legge til skjema som favoritt"
  },
  {
    "objectID": "statistikkere/altinn-skjema-administrasjon.html#periode",
    "href": "statistikkere/altinn-skjema-administrasjon.html#periode",
    "title": "Administrasjon av skjema",
    "section": "Periode",
    "text": "Periode\nEt skjema kan ha en eller flere perioder knyttet til seg. Hvilke perioder som er lagt inn p√• skjemaet kan du se i detaljvisningen n√•r du har valgt et skjema.\nAdministrasjon av utvalg og enheter Administrasjon av utvalg og enheter skjer fortsatt fra SFU p√• bakke. Knytningen mellom skjemaet og bakkesystemene skjer ved at et delregister-nr registreres p√• perioden. Mer informasjon om h√•ndtering av utvalg fra SFU finner du her.\nEksempel p√• skjema med periode Bildet nedenfor viser eksempel p√• et skjema med en tilknyttet periode:\n\n\n\n\n\n\nFigur¬†4: Skjema periode\n\n\n\n\n\n\n\n\n\nBeskrivelse av periode metadata\n\n\n\n\n\nTabell¬†3 beskriver metadatafeltene for en periode\n\n\n\nTabell¬†3: Periode metadata\n\n\n\n\n\n\n\n\n\nMeta\nForklaring\n\n\n\n\nType\nUnders√∏kelsestype (f.eks Kvartal, M√•ned, √Ör).\n\n\nNr\nPeriode nummer.\n\n\n√Ör\nPeriode √•r.\n\n\nPeriode-dato\n\n\n\nDelreg-nr\nDelregister-nummer i SFU som er tilknyttet perioden.\n\n\nEnhet-type\nEnhetstype (f.eks Bedrift, Foretak, Person).\n\n\nOppgavebyrde\nAktivere oppgavebyrde i skjemaet.\n\n\nBrukeropplevelse\nAktivere oppgavebyrde i skjemaet.\n\n\nSkjemadata\n\n\n\nJournalnummer\n\n\n\n\n\n\n\n\n\n\n\nKodeeksempel\nFor √• lage en ny periode, kan du bruke metoden create_periode i SuvClient.\n\n\nnotebook\n\nclient = SuvClient()\n\noutput = client.create_periode(\n            skjema_id = 142,\n            periode_type = 'KVRT',\n            periode_nr = 3,\n            periode_aar = 2024,\n            delreg_nr = 21130324,\n            enhet_type = 'BEDR'\n        )\n\nprint(output)\n\nFor √• hente ut perioder med tilh√∏rende metadata i Python, kan du bruke metoden get_perioder_by_skjema_id i SuvClient. S√∏rg for at du oppgir riktig skjema_id.\n\n\nnotebook\n\nclient = SuvClient()\n\noutput = client.get_perioder_by_skjema_id(\n            skjema_id=142\n        )\n\nprint(json.dumps(output, indent=4))"
  },
  {
    "objectID": "statistikkere/altinn-skjema-administrasjon.html#pulje",
    "href": "statistikkere/altinn-skjema-administrasjon.html#pulje",
    "title": "Administrasjon av skjema",
    "section": "Pulje",
    "text": "Pulje\nEn periode kan ha en eller flere puljer knyttet til seg. Hvilke puljer som er lagt inn p√• skjemaet kan du se i detaljvisningen n√•r du har valgt en periode.\nEksempel p√• periode med pulje Bildet nedenfor viser eksempel p√• en periode med en tilknyttet pulje:\n\n\n\n\n\n\nFigur¬†5: Periode pulje\n\n\n\n\n\n\n\n\n\nBeskrivelse av pulje metadata\n\n\n\n\n\nTabell¬†4 beskriver metadatafeltene for en pulje\n\n\n\nTabell¬†4: Pulje metadata\n\n\n\n\n\n\n\n\n\nMeta\nForklaring\n\n\n\n\nPulje\nPulje nummer.\n\n\nTilgjengelig fra\nAngir fra n√•r skjemaet er tilgjengelig for oppdragsgiver ute hos Altinn.\n\n\nSvarfrist\nAngir svarfrist for unders√∏kelsen.\n\n\nTvmulkt svarfrist\nAngir svarfrist f√∏r utsendelse av tvangsmulkt.\n\n\nSend SI\n\n\n\n\n\n\n\n\n\n\n\nKodeeksempel\nFor √• lage en ny pulje, kan du bruke metoden create_pulje i SuvClient.\n\n\nnotebook\n\nclient = SuvClient()\n\n_altinn_tilgjengelig = datetime(2024, 11, 20, 14, 30, 0)\n_altinn_svarfrist = datetime(2024, 11, 21)\n_tvangsmulkt_svarfrist_ = datetime(2024, 11, 22)\n_send_si = datetime(2024, 11, 23)\n\nclient.create_pulje(\n    periode_id = 24,\n    pulje_nr = 1,\n    altinn_tilgjengelig=_altinn_tilgjengelig,\n    altinn_svarfrist = _altinn_svarfrist,\n    tvangsmulkt_svarfris t= _tvangsmulkt_svarfrist_,\n    send_si = _send_si\n)\n\nprint(output)\n\nFor √• hente ut pulje med tilh√∏rende metadata i Python, kan du bruke metoden get_pulje_by_periode_id i SuvClient. S√∏rg for at du oppgir riktig periode_id.\n\n\nnotebook\n\nclient = SuvClient()\n\noutput = client.get_pulje_by_periode_id(\n            periode_id = 99\n        )\n\nprint(output)"
  },
  {
    "objectID": "statistikkere/altinn-skjema-administrasjon.html#utsending",
    "href": "statistikkere/altinn-skjema-administrasjon.html#utsending",
    "title": "Administrasjon av skjema",
    "section": "Utsending",
    "text": "Utsending\nEn pulje kan ha en eller flere utsendinger knyttet til seg. Hvilke utsendinger som er lagt inn p√• puljen kan du se i detaljvisningen n√•r du har valgt en pulje.\nEksempel p√• pulje med utsending Bildet nedenfor viser eksempel p√• en pulje med en tilknyttet utsending:\n\n\n\n\n\n\nFigur¬†6: Pulje utsending\n\n\n\n\n\n\n\n\n\nBeskrivelse av utsending metadata\n\n\n\n\n\nTabell¬†5 beskriver metadatafeltene for en utsending\n\n\n\nTabell¬†5: Pulje metadata\n\n\n\n\n\n\n\n\n\nMeta\nForklaring\n\n\n\n\nUtsendingstype\nAngir type utsending (f.eks instansiering).\n\n\nTrigger\nAngir trigger for utsending (Manuell, Auto, Ekstern).\n\n\nTest\n\n\n\nSend ut\nAngir tidspunkt for utsendelse.\n\n\n\n\n\n\n\n\n\n\nSend n√•\n√ònsker du √• gj√∏re en utsending umiddelbart m√• dette gj√∏res fra GUI.\nSlik gj√∏r du en utsending umiddelbart:\n\nVelg Skjemadata i venstremenyen.\nBruk s√∏kefeltet i det nye skjermbildet for √• finne skjemaet ditt.\nKlikk p√• Vis detaljer-ikonet i s√∏keresultatet for √• se mer informasjon om skjemaet.\nFinn √∏nsket periode og velg Vis detaljer-ikonet i s√∏keresultatet for √• se mer informasjon om perioden.\nFinn √∏nsket utsending og velg Vis detaljer-ikonet i s√∏keresultatet for √• se mer informasjon om utsendingen (eventuelt s√• kan du opprette en helt ny utsending).\nTrykk ‚ÄúSend n√•‚Äù.\n\n\n\n\n\n\n\nFigur¬†7: Send n√•\n\n\n\n\n\n\n\n\n\nUtsending til utvalgt(e) enhet(er)\n\n\n\nFor √• sende til √©n eller flere spesifikke enheter, kan du oppgi organisasjonsnumrene som en kommaseparert liste. Eksempel: 123456789, 987654321. Dersom ingen enheter oppgis, vil utsendingen automatisk gjelde for hele puljen.\n\n\n\n\nKodeeksempel\nFor √• lage en ny utsending, kan du bruke metoden create_pulje i SuvClient.\n\n\nnotebook\n\nclient = SuvClient()\n\n_altinn_uts_tidspunkt = datetime(2024, 11, 20, 14, 30, 0)\n\noutput = client.create_utsending(\n            pulje_id = 75,\n            utsendingstype_navn = 'instansiering',\n            altinn_uts_tidspunkt = _altinn_uts_tidspunkt\n        )\n        \nprint(output)\n\nFor √• hente ut utsending med tilh√∏rende metadata i Python, kan du bruke metoden get_utsending_by_id i SuvClient. S√∏rg for at du oppgir riktig utsending_id.\n\n\nnotebook\n\nclient = SuvClient()\n\noutput = client.client.get_utsending_by_id(\n            utsending_id = 133   \n        )\n\nprint(output)"
  },
  {
    "objectID": "statistikkere/altinn-skjema-administrasjon.html#eksempelkode-sammenstilling-av-data",
    "href": "statistikkere/altinn-skjema-administrasjon.html#eksempelkode-sammenstilling-av-data",
    "title": "Administrasjon av skjema",
    "section": "Eksempelkode sammenstilling av data",
    "text": "Eksempelkode sammenstilling av data\nNoe demokode for sammenstilling av data ligger i repoet, og kan v√¶re ett godt utgangspunkt √• kopiere og endre fra."
  },
  {
    "objectID": "statistikkere/dapla-toolbelt-metadata.html",
    "href": "statistikkere/dapla-toolbelt-metadata.html",
    "title": "dapla-toolbelt-metadata",
    "section": "",
    "text": "dapla-toolbelt-metadata er en Python-pakke for √• jobbe med metadatasystemene p√• Dapla. Pakken gir brukeren et Python-grensesnitt for √• jobbe mot Datadoc og Vardef.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata"
    ]
  },
  {
    "objectID": "statistikkere/dapla-toolbelt-metadata.html#forberedelser",
    "href": "statistikkere/dapla-toolbelt-metadata.html#forberedelser",
    "title": "dapla-toolbelt-metadata",
    "section": "Forberedelser",
    "text": "Forberedelser\ndapla-toolbelt-metadata kan installeres i tjenester p√• Dapla Lab. Siden det er en Python-pakke s√• m√• den installeres i en tjeneste der Python er installert. Deretter gj√∏r du f√∏lgende:\n\n√Öpne en tjeneste p√• Dapla Lab med Python installert. F√∏r du √•pner tjenesten m√• du velge √• representere team og tilgangsgruppe som har tilgang til dataene som skal dokumenteres.\nInstaller pakken i et ssb-project p√• f√∏lgende m√•te:\n\n\n\nTerminal\n\npoetry add dapla-toolbelt-metadata\n\nEtter det er du klar for bruke funksjonaliteten i dapla-toolbelt-metadata i en notebook.\n\n\n\n\n\n\nDapla Lab og dapla-toolbelt-metadata\n\n\n\nI Dapla Lab velger man hvilket team og tilgangsgruppe man skal representere f√∏r man √•pner en tjeneste. Hvis man f.eks. skal dokumentere et datasett i datatilstanden inndata for et team som heter dapla-felles, s√• m√• man logge seg inn i en tjeneste som dapla-felles-developers, hvis ikke har man ikke tilgang til datasettet.\nI tillegg s√• benytter Vardef bl.a. informasjon om hvilket team og tilgangsgruppe du logget deg inn som, for definere hvilket team som blir eier av en nyopprettet variabeldefinisjon. P√• samme m√•te bruker Vardef denne informasjonen til √• avgj√∏re om en bruker har tilgang til √• endre en definisjon. F.eks. vil en bruker som logger seg inn som dapla-felles-developers og oppretter en ny definisjon, s√• vil dapla-felles st√• som eier av definisjonen og det vil kun v√¶re medlemmer av dette teamet som kan gj√∏re endringer i definisjonen.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata"
    ]
  },
  {
    "objectID": "statistikkere/dapla-toolbelt-metadata.html#datadoc",
    "href": "statistikkere/dapla-toolbelt-metadata.html#datadoc",
    "title": "dapla-toolbelt-metadata",
    "section": "Datadoc",
    "text": "Datadoc\nDatadoc er SSBs system for dokumentasjon av datasett. F√∏rste gang man skal dokumentere et datasett i Datadoc s√• er det anbefalt √• bruke det grafiske grensesnittet i Datadoc-editor. I l√∏pende produksjon er det dermed anbefalt √• benytte en programmatisk tiln√¶rming gjennom Datadoc-klassen i Python-pakken dapla-toolbelt-metadata.\n\nDokumentere ny periode\nHvis man har dokumentert datasett for periode t med Datadoc-editor, s√• kan man programmatisk dokumentere periode t+1 ved √• benytte Datadoc-klassen i dapla-toolbelt-metadata. Det forutsetter at det kun nye data som har kommet til og at det er den eneste endringen i dataene. Da kan man dokumentere den nye perioden p√• f√∏lgende m√•te:\n\n\nNotebook\n\nfrom dapla_metadata.datasets import Datadoc\n\nmeta = Datadoc(\n1    dataset_path=\"gs://ssb-dapla-felles-data-produkt-prod/datadoc/sykefratot/klargjorte_data/person_testdata_p2022_v1.parquet\",\n2    metadata_document_path=\"gs://ssb-dapla-felles-data-produkt-prod/datadoc/sykefratot/klargjorte_data/person_testdata_p2021_v1__DOC.json\",\n)\n3meta.write_metadata_document()\n\n\n1\n\ndataset_path angir det nye datasettet som skal dokumenteres.\n\n2\n\nmetadata_document_path angir sti til tidligere periodes metadata.\n\n3\n\nwrite_metadata_document er kommandoen som produserer de nye metadataene og skriver de til filen gs://ssb-dapla-felles-data-produkt-prod/datadoc/sykefratot/klargjorte_data/person_testdata_p2022_v1__DOC.json.\n\n\nDet veldig viktig at man ikke bruker denne metoden hvis det er endringer i hvilke kolonner som finnes i datasettet eller andre st√∏rre endringer. Metoden over antar at den eneste informasjonen som har endret seg er den som kan leses ut av filstien. Ved st√∏rre endringer i selve dataene b√∏r man heller gj√∏re en manuell gjennomgang av metadataene med Datadoc-editor",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata"
    ]
  },
  {
    "objectID": "statistikkere/dapla-toolbelt-metadata.html#vardef-wip",
    "href": "statistikkere/dapla-toolbelt-metadata.html#vardef-wip",
    "title": "dapla-toolbelt-metadata",
    "section": "Vardef (WIPüë∑‚Äç‚ôÇÔ∏è)",
    "text": "Vardef (WIPüë∑‚Äç‚ôÇÔ∏è)\nVardef er SSBs system for dokumentasjon av variabler. Vardef best√•r av et sentralt datalager som man kan interagere med via et API. Statistikere og forskere i SSB kan benytte seg av API-et gjennom Vardef-klassen i Python-pakken dapla-toolbelt-metadata, som er et mer brukervennlig grensesnitt tilpasset Dapla Lab. Pakken lar brukeren bl.a. gj√∏re f√∏lgende:\n\nListe ut alle definisjoner.\nMigrere definisjoner fra Vardok1.\nOpprette ny definisjon\nEndre eksisterende definisjon\n\nEksemplene under forutsetter at du har gjort f√∏lgende importer i en notebook p√• Dapla Lab:\n\n\nNotebook\n\nfrom dapla_metadata.variable_definitions import Vardef, models\n\n\nListe ut variabeldefinisjoner\nFor √• lese inn alle variabeldefinisjoner fra Vardef kan man bruke list_variable_definitions()-funksjonen:\n\n\nNotebook\n\nvariable_definitions = Vardef.list_variable_definitions()\nvariable_definitions\n\nlist_variable_definitions() returnerer en liste med dictionaries med alle variabeldefinisjoner i Vardef, inkludert Utkast, Publisert internt og Publisert eksternt. Under er det noen kode-eksempler p√• hvordan man kan kan filtrere eller endre p√• denne listen med Python for de som m√•tte √∏nske det.\n\nFiltrere enkeltfelt for alle definisjoner\nBruk f√∏lgende kode for √• liste ut ID, navn, kortnavn, definisjon, eierteam og status for alle definisjoner i Vardef:\n\n\nNotebook\n\nfor variable in variable_definitions:\n    print(\n        f\"Id: {variable.id}\\n\"\n        f\"Navn: {variable.name}\\n\"\n        f\"Kortnavn: {variable.short_name}\\n\"\n        f\"Definisjon: {variable.definition}\\n\"\n        f\"Eier: {variable.owner}\\n\"\n        f\"Status: {variable.variable_status}\\n\"\n    )\n\n\n\n\n\n\n\nSe output fra kode\n\n\n\n\n\n\n\nNotebook\n\nId: 5eC5YUVR\nNavn: {'nb': 'test navn', 'nn': 'test namn', 'en': 'test name'}\nKortnavn: test_shortname2\nDefinisjon: {'nb': 'Skriv en definisjonstekst p√• norsk bokm√•l.', 'nn': 'Skriv ein definisjonstekst p√• nynorsk.', 'en': 'Write a definition text in english.'}\nEier: {'team': 'dapla-felles', 'groups': ['dapla-felles-developers']}\nStatus: VariableStatus.PUBLISHED_INTERNAL\n\nId: fDun32UF\nNavn: {'nb': 'test navn', 'nn': 'test namn', 'en': 'test name'}\nKortnavn: test_shortname_obr\nDefinisjon: {'nb': 'Skriv en definisjonstekst p√• norsk bokm√•l.', 'nn': 'Skriv ein definisjonstekst p√• nynorsk.', 'en': 'Write a definition text in english.'}\nEier: {'team': 'dapla-felles', 'groups': ['dapla-felles-developers']}\nStatus: VariableStatus.DRAFT\n\nId: Xww0Swou\nNavn: {'nb': 'Inntekt etter skatt', 'nn': None, 'en': 'After-tax income'}\nKortnavn: wies\nDefinisjon: {'nb': 'Yrkesinntekter, kapitalinntekter, skattepliktige og skattefrie overf√∏ringer, i l√∏pet av kalender√•ret. Utlignet skatt og negative overf√∏ringer er trukket i fra.', 'nn': None, 'en': 'After-tax income is calculated as the sum of wages and salaries, income from self-employment, property income and transfers received minus total assessed taxes and negative transfers.'}\nEier: {'team': 'dapla-felles', 'groups': ['dapla-felles-developers']}\nStatus: VariableStatus.DRAFT\n\nId: ouMrO8E6\nNavn: {'nb': 'test navn', 'nn': 'test namn', 'en': 'test name'}\nKortnavn: test_shortname\nDefinisjon: {'nb': 'Skriv en definisjonstekst p√• norsk bokm√•l.', 'nn': 'Skriv ein definisjonstekst p√• nynorsk.', 'en': 'Write a definition text in english.'}\nEier: {'team': 'dapla-felles', 'groups': ['dapla-felles-developers']}\nStatus: VariableStatus.DRAFT\n\n\n\n\n\n\nFiltrere p√• ID\nHvis man vet ID-en til definisjon i Vardef s√• kan man hente ut variabeldefinisjonen med f√∏lgende kode:\n\n\nNotebook\n\nsingle_definition = Vardef.get_variable_definition(\"fDun32UF\")\nprint(single_definition)\n\nI eksempelet printes all feltene i definisjonen med ID fDun32UF.\n\n\n\n\n\n\nSe output fra kode\n\n\n\n\n\n\n\nNotebook\n\n{\n  \"id\": \"fDun32UF\",\n  \"patch_id\": 1,\n  \"name\": {\n    \"nb\": \"test navn\",\n    \"nn\": \"test namn\",\n    \"en\": \"test name\"\n  },\n  \"short_name\": \"test_shortname_obr\",\n  \"definition\": {\n    \"nb\": \"Skriv en definisjonstekst p√• norsk bokm√•l.\",\n    \"nn\": \"Skriv ein definisjonstekst p√• nynorsk.\",\n    \"en\": \"Write a definition text in english.\"\n  },\n  \"classification_reference\": \"91\",\n  \"unit_types\": [\n    \"01\"\n  ],\n  \"subject_fields\": [\n    \"al\"\n  ],\n  \"contains_special_categories_of_personal_data\": false,\n  \"variable_status\": \"DRAFT\",\n  \"measurement_type\": null,\n  \"valid_from\": \"2024-12-18\",\n  \"valid_until\": null,\n  \"external_reference_uri\": null,\n  \"comment\": null,\n  \"related_variable_definition_uris\": null,\n  \"owner\": {\n    \"team\": \"dapla-felles\",\n    \"groups\": [\n      \"dapla-felles-developers\"\n    ]\n  },\n  \"contact\": null,\n  \"created_at\": \"2025-02-03T13:15:44.366000\",\n  \"created_by\": \"obr@ssb.no\",\n  \"last_updated_at\": \"2025-02-03T13:15:44.366000\",\n  \"last_updated_by\": \"obr@ssb.no\"\n}\n\n\n\n\n\n\nFiltrere p√• gyldighetsperiode\nSiden alle definisjoner i Vardef er versjonert og har en gyldighetsperiode, s√• kan man filtrere hvilke definisjoner som var gyldig p√• et gitt tidspunkt:\n\n\nNotebook\n\nfrom datetime import date\n\ndesired_validity = date(2030,2,24)\nfiltered_variable_definitions = Vardef.list_variable_definitions(desired_validity)\n\nI koden over hentes alle som definert som gyldig per 24. februar 2030, inkludert definisjoner som ikke har en sluttdato. Vi importerer ogs√• datetime-pakken for √• sikre at datoformatet blir riktig.\nVi kan igjen printe spesifikk informasjon om felter vi er interessert i:\n\n\nNotebook\n\nprint(f\"Valid at {desired_validity}:\\n\")\n\nfor variable in filtered_variable_definitions:\n    print(\n        f\"Id: {variable.id}\\n\"\n        f\"Name: {variable.name}\\n\"\n        f\"Short name: {variable.short_name}\\n\"\n        f\"Definition: {variable.definition}\\n\"\n        f\"Owner: {variable.owner}\\n\"\n        f\"Valid: {variable.valid_from} to {variable.valid_until}\\n\"\n    )\n\n\n\n\n\n\n\nSe output fra koden\n\n\n\n\n\n\n\nNotebook\n\nValid at 2030-02-24:\n\nId: fDun32UF\nName: {'nb': 'test navn', 'nn': 'test namn', 'en': 'test name'}\nShort name: test_shortname_obr\nDefinition: {'nb': 'Skriv en definisjonstekst p√• norsk bokm√•l.', 'nn': 'Skriv ein definisjonstekst p√• nynorsk.', 'en': 'Write a definition text in english.'}\nOwner: {'team': 'dapla-felles', 'groups': ['dapla-felles-developers']}\nValid: 2024-12-18 to None\n\nId: Xww0Swou\nName: {'nb': 'Inntekt etter skatt', 'nn': None, 'en': 'After-tax income'}\nShort name: wies\nDefinition: {'nb': 'Yrkesinntekter, kapitalinntekter, skattepliktige og skattefrie overf√∏ringer, i l√∏pet av kalender√•ret. Utlignet skatt og negative overf√∏ringer er trukket i fra.', 'nn': None, 'en': 'After-tax income is calculated as the sum of wages and salaries, income from self-employment, property income and transfers received minus total assessed taxes and negative transfers.'}\nOwner: {'team': 'dapla-felles', 'groups': ['dapla-felles-developers']}\nValid: 1993-01-01 to None\n\n\n\n\n\n\nFiltrere p√• eierteam\nMan kan filtrere ut alle definisjoner som eies av speisifikt team. I eksempelet under listes det ut hvilke definisjoner som eies av team dapla-felles:\n\n\nNotebook\n\nowner_team = \"dapla-felles\"\nprint(f\"\\nFiltrer etter eierteam {owner_team}: \\n\")\nmy_team_variables = [variable for variable in variable_definitions if variable.owner[\"team\"] == owner_team]\n\nfor variable in my_team_variables:\n    print(f\"Id: {variable.id}\\nName: {variable.name['nb']}\\nShort name: {variable.short_name}\\nDefinition: {variable.definition['nb']}\\nOwner: {variable.owner['team']}\\n\")\n\n\n\n\n\n\n\nSe output fra koden\n\n\n\n\n\n\n\nNotebook\n\nFiltrer etter eierteam dapla-felles: \n\nId: fDun32UF\nName: test navn\nShort name: test_shortname_obr\nDefinition: Skriv en definisjonstekst p√• norsk bokm√•l.\nOwner: dapla-felles\n\nId: Xww0Swou\nName: Inntekt etter skatt\nShort name: wies\nDefinition: Yrkesinntekter, kapitalinntekter, skattepliktige og skattefrie overf√∏ringer, i l√∏pet av kalender√•ret. Utlignet skatt og negative overf√∏ringer er trukket i fra.\nOwner: dapla-felles\n\n\n\n\n\n\nFiltrere etter status\nMan kan filtrere etter status for en definisjon. Her filtrerer vi ut alle definisjoner med status Draft:\n\n\nNotebook\n\nprint(\"\\nFiltrer etter status `DRAFT`: \")\ndraft_variables = [variable for variable in variable_definitions if variable.variable_status == models.VariableStatus.DRAFT]\nprint(draft_variables)\n\nP√• samme m√•te kan vi filtrere ut definisjoner som er publisert internt:\n\n\nNotebook\n\nprint(\"\\nFiltrer etter status `PUBLISHED INTERNAL`: \")\npublished_intern_variables = [variable for variable in variable_definitions if variable.variable_status == models.VariableStatus.PUBLISHED_INTERNAL]\n\nprint(published_intern_variables)\n\n\n\nFiltrere etter kortnavn\nMan kan hente ut en definisjon basert p√• kortnavn p√• f√∏lgende m√•te:\n\n\nNotebook\n\nvariable_by_short_name = next(variable for variable in variable_definitions if variable.short_name == \"wies\")\n\nprint(variable_by_short_name)\n\n\n\nEnkeltfelter i en definisjon\nHvis man kun √∏nsker √• hente ut verdien til et enkeltfelt i en variabeldefinisjon kan man gj√∏re det p√• f√∏lgende m√•te:\n\n\nNotebook\n\nwies_vardef = next(variable for variable in variable_definitions if variable.short_name == \"wies\")\nwies_definition_nb = wies_vardef.definition[\"nb\"]\nwies_definition_nb\n\n\n\n\n\n\n\nSe output fra koden\n\n\n\n\n\n\n\nNotebook\n\n'Yrkesinntekter, kapitalinntekter, skattepliktige og skattefrie overf√∏ringer, i l√∏pet av kalender√•ret. Utlignet skatt og negative overf√∏ringer er trukket i fra.'\n\n\n\n\nI koden over hentes beskrivelsen av variabelen med kortnavn wies p√• norsk bokm√•l. Sp√∏rring returnerer en string-objekt.\n\n\n\nOpprette ny definisjon\nN√•r man skal opprette en ny definisjon i Vardef, s√• m√• man forholde seg til hvordan eierskapet til definisjonen blir definert. Nye definisjoner kan kun opprettes fra en tjeneste p√• Dapla Lab med Python installert, og teamet man velger √• representere n√•r man logger seg inn i tjenesten, blir automatisk satt som eier av definisjonen i Vardef. Dette kan endres senere, men det letteste er at noen fra teamet som skal eie definisjonen gj√∏r opprettelsen fra Dapla Lab.\nStegene for √• opprette en ny variabeldefinisjon er som f√∏lger:\n\nOpprette et utkast (draft) av definisjonen med all obligatorisk informasjon.\nPublisere variabelen internt eller eksternt.\n\nPublisering av en definisjon internt at det kun vil v√¶re SSB-ansatte som f√•r tilgang til den. √Ö publisere eksternt betyr at den er tilgjengelige for andre ogs√•. F√∏r man publiserer et utkast er det god praksis at informasjonen er kvalitetssikret av flere personen, spesielt siden en publisert definisjon aldri kan slettes. Endringer i en definisjon genererer kun en ny versjon eller mindre endring.\n\nOpprette et utkast\nF√∏rste steg for √• publisere en ny variabeldefinisjon til Vardef er √• opprette et utkast eller draft. Et utkast m√• inneholde all obligatorisk informasjon for en variabeldefinisjon. Et viktig poeng her er at utkastet ogs√• lagres datalageret til Vardef, og det sjekkes derfor ved opprettelse om all obligatorisk informasjon er fylt ut. Men n√•r en variabel har status utkast i Vardef s√• er det tillatt √• endre alle felter uten at en ny versjon opprettes.\nMan kan opprette en ny definisjon ved √• bruke Draft-funksjonen models-klassen:\n\n\nNotebook\n\nutkast = models.Draft(\n    name = {\n        \"nb\": \"test navn\",\n        \"nn\": \"test namn\",\n        \"en\": \"test name\",\n    },\n    short_name= \"test_shortname\",\n    definition= {\n        \"nb\": \"Skriv en definisjonstekst p√• norsk bokm√•l.\",\n        \"nn\": \"Skriv ein definisjonstekst p√• nynorsk.\",\n        \"en\": \"Write a definition text in english.\",\n    },\n    classification_reference=\"91\",\n    unit_types=[\n        \"01\",\n    ],\n    subject_fields=[\n        \"al\",\n    ],\n    contains_special_categories_of_personal_data=False,\n    measurement_type=None,\n    valid_from=date(2024,12,18),\n    external_reference_uri=None,\n    comment=None,\n    related_variable_definition_uris=None,\n    contact=None,\n)\n\nmy_draft = Vardef.create_draft(utkast)\n\nI koden over s√• har vi fylt inn all obligatorisk informasjon for de nye variabeldefinisjon i utkast-variabelen. Deretter har vi publisert den til datalageret til Vardef med create_draft-funksjonen. I tillegg ble den nye variabelen lagret i en variabel kalt my_draft for √• lettere kunne endre p√• variabelen senere.\nVi kan innholdet i variabelen som vi har lagret i datalageret til Vardef p√• f√∏lgende m√•te:\n\n\nNotebook\n\nprint(my_draft)\n\nVi kan ogs√• aksessere enkeltfelter via dette objektet. F.eks. kan vi hente ut det maskingenererte feltet created_by for √• validere brukerens navn er registrert:\n\n\nNotebook\n\nprint(my_draft.created_by)\n\n\n\nPublisere et utkast\nEtter at informasjonen i et utkast er kvalitetssikret og ferdig, kan den publiseres internt (kun SSB-ansatte) eller eksternt (√•pent for alle). Det er verdt √• merke seg at en definisjon som er publisert eksternt ikke kan endres til √• kun v√¶re publisert internt.\nN√•r man publiserer en definisjon er det anbefalt √• bruke VariableStatus-klassen for √• minimere sannsynligheten for skrivefeil i dette viktige steget.\n\n\nNotebook\n\nupdate_status = models.UpdateDraft(\n    variable_status=models.VariableStatus.PUBLISHED_INTERNAL,\n)\n\nmy_draft.update_draft(update_status)\n\nLister man ut alle variabeldefinisjoner vil man n√• se at denne variabelen har f√•tt\n\"variable_status\": \"PUBLISHED_INTERNAL\".\n\n\n\nEndre eksisterende definisjon\nVariabeldefinisjoner i Vardef har enten status som publisert internt, publisert eksternt eller utkast. En definisjon med status utkast kan endres av eierteamet p√• en enkel m√•te. Men endringer i variablerdefinisjoner som er publisert internt eller eksternt omfattes av prinsippet om uforanderlighet i Vardef. En definisjon som har blitt publisert slettes aldri, og endringer kan kun skje ved √• opprette nye versjoner. Det finnes to typer av versjoner i Vardef:\n\nMindre endringer (patches)\nMindre endringer som ikke betyr noen innholdsmessige endring i beskrivelsen av variabelen. F.eks. retting av skrivefeil, legge til en oversettelse eller legge til subject_field.\nSt√∏rre endringer\nEndringer som inneb√¶rer innholdsmessige endringer i variabelen. F.eks. en ny skatteregel krever at beskrivelsen for en skattevariabel m√• oppdateres.\n\n\n\nSlette et utkast\nMan kan slette et utkast p√• f√∏lgende m√•te:\n\n\nNotebook\n\nmy_vardok_draft.delete_draft()\n\nDette kan kun gj√∏res p√• definisjoner med status utkast/draft.\n\nMindre endringer\nMindre endringer, kalt patches i Vardef, er mindre endringer som ikke krever en ny gyldighetsperiode for variabelen. Under er det noen eksempler p√• slike endringer og hvordan de gj√∏res:\n\nEndre eierteam\n\n\nNotebook\n\nfrom dapla_metadata.variable_definitions.generated.vardef_client.models.owner import (\n    Owner,\n)\nfrom dapla_metadata.variable_definitions.generated.vardef_client.models.patch import (\n    Patch,\n)\n\nnew_owner = Patch(\n    owner=Owner(\n    team=\"dapla-felles\",\n    groups=[\n        \"dapla-felles-developers\",\n        \"play-enhjoern-a-developers\",\n    ],\n),\n)\n\nmy_draft.create_patch(new_owner)\n\nmy_draft = Vardef.get_variable_definition(my_draft.id)\n\nprint(my_draft.owner)\n\n\n\n\nSt√∏rre endringer\nFor √• opprette en ny gyldighetsperiode m√• inndataene inneholde oppdatert beskrivelsestekst for alle gjeldende spr√•k og en ny gyldig fra dato. En ny gyldighetsperiode b√∏r kun opprettes n√•r den grunnleggende definisjonen av variabelen har endret seg.\n\n\nNotebook\n\nvalid_validity_period = models.ValidityPeriod(\n    definition={\n        \"nb\": \"ny definisjon2\",\n        \"nn\": \"ny definisjon2\",\n        \"en\": \"new definition2\",\n    },\n    valid_from=date(2040,4,5),\n)\nmy_draft.create_validity_period(valid_validity_period)\n\n\n\n\nMigrere definisjon fra Vardok\nEksternt publiserte variabeldefinisjoner fra Vardok kan migreres til Vardef med funksjonen migrate_from_vardok()-funksjonen i dapla-toolbelt-metadata. For √• migrere en variabeldefinisjon m√• man referere til variabelens ID i Vardok. Siden alle definisjoner er eid av en seksjon s√• har alle seksjoner en oversikt over hvilke definisjoner de er ansvarlig for, med tilh√∏rende ID, og skal kun migrere disse definisjonene.\nmigrate_from_vardok()-funksjonen migrerer alle feltene fra Vardok som skal videref√∏res til Vardef, oppdaterer felter automatisk der det er mulig, og publiserer den som et Utkast i Vardef. Deretter m√• hvert fagansvarlig g√• gjennom all informasjon, gj√∏re n√∏dvendige endringer og kvalitetssikre informasjonen, f√∏r den kan publiseres. Husk at eierteamet for definisjon blir bestemt av hvilket team man representerer i Dapla Lab n√•r migrate_from_vardok()-funksjonen kj√∏res.\nI koden under vises et eksempel der definisjonen for Spesifisert registreringstype migreres fra Vardok til Vardef. ID-en til variabelen er 90 og derfor er det denne det refereres til i funksjonen:\n\n\nNotebook\n\nmy_vardok_draft = Vardef.migrate_from_vardok(\"90\")\nprint(my_vardok_draft)\n\nAlle definisjoner i Vardef m√• ha et gyldig kortnavn,. Hvis en definisjon som migreres fra Vardok ikke har et kortnavn s√• vil det genereres et midlertidig kortnavn ved migrering. Dette kortnavnet m√• endres f√∏r definisjonen publiseres internt eller eksternt. Automatisk genererte kortnavn starter alltid med generert. Man kan liste ut alle definisjoner i Vardef med et generert kortnavn for et gitt Dapla-team, med f√∏lgende kode:\n\n\nNotebook\n\nmy_vardok_missing_short_name = Vardef.migrate_from_vardok(\"123\")\n\nmy_owner_team = \"dapla-felles\"\n\nmy_team_variables_generert_kortnavn = [\n    {variable.short_name, variable.id}\n    for variable in Vardef.list_variable_definitions()\n    if variable.owner[\"team\"] == my_owner_team and variable.short_name.startswith(\"generert\")\n]\n\nprint(my_team_variables_generert_kortnavn)\n\n\n\n\n\n\n\nDefinisjoner i Vardok kan kun migreres en gang\n\n\n\nEn definisjon i Vardok kan kun migreres en gang. Hvis noen flere pr√∏ver √• migrere en variabel vil man f√• en feilmelding i dapla-toolbelt-pseudo. Hvis den migrerte definisjonen har status Utkast, s√• er det mulig for eierteamet √• slette utkastet, slik at et annet team kan migrere definisjonen. Eierteamet kan ogs√• overf√∏re eierskapet til en definisjon i Vardef til et annet team.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata"
    ]
  },
  {
    "objectID": "statistikkere/dapla-toolbelt-metadata.html#footnotes",
    "href": "statistikkere/dapla-toolbelt-metadata.html#footnotes",
    "title": "dapla-toolbelt-metadata",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nVardok er SSBs tidligere system for variabeldefinisjoner og som erstattes av Vardef.‚Ü©Ô∏é",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata"
    ]
  },
  {
    "objectID": "statistikkere/jit.html",
    "href": "statistikkere/jit.html",
    "title": "Just-in-Time Access",
    "section": "",
    "text": "Just-in-Time Access (JIT) er en applikasjon der data-admins kan gi seg selv midlertidig tilgang til kildedata. P√• Dapla benyttes for dette for √• unng√• at data-admins har permanent tilgang til sensitive data, siden all kildedata er definert som sensitiv informasjon. data-admins m√• bruke JIT-applikasjonen for √• gi seg selv korte, begrunnede tilganger til kildedata ved behov. Den som er ansvarlig for teamet kan monitere i hvilken grad teamets data-admins benytter seg av denne muligheten.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Just-in-Time Access"
    ]
  },
  {
    "objectID": "statistikkere/jit.html#hvordan-bruke-jit",
    "href": "statistikkere/jit.html#hvordan-bruke-jit",
    "title": "Just-in-Time Access",
    "section": "Hvordan bruke JIT",
    "text": "Hvordan bruke JIT\nFor √• bruke JIT m√• du v√¶re data-admin for teamet som eier kildedataene du √∏nsker tilgang til. Du aktiverer tilganger i JIT-appen ved gj√∏re f√∏lgende:\n\nG√• inn p√• nettsiden https://jitaccess.dapla.ssb.no/1\nOppgi Prosjekt-id for prosjektet der kildedataene du √∏nsker tilgang til er lagret.\nHuk av hvilke tilgangsroller du √∏nsker aktivert. Se beskrivelse av roller i Tabell¬†1.\nVelg hvor lenge tilgangen du √∏nsker at tilgangen skal vare. Den kan maksimalt vare i 8 timer.\nOppgi en begrunnelse for at aktiverer tilgangen.\nTil slutt trykker du p√• Request access.\n\nTilgangen vil deretter v√¶re aktiv ila. noen minutter.\n\n\n\n\n\n\nHvordan b√∏r begrunnelsen skrives?\n\n\n\nPr√∏v √• gj√∏r begrunnelsen forst√•elig for andre enn deg selv p√• det tidspunktet. Den som er ansvarlig for teamet skal kunne forst√• begrunnelsen n√•r de ser p√• loggene. I tillegg vil sentralt i SSB monitere i hvilken grad Dapla-teamene benytter seg av tilgangene.\n\n\n\n\n\nTabell¬†1: De mest relevante tilgangene som kan aktiveres i JIT-applikasjonen\n\n\n\n\n\n\n\n\n\nRoller\nHva?\n\n\n\n\nssb.buckets.list\nListe ut b√∏ttene i kildeprosjektet.\n\n\nstoragetransfer.admin\nSette opp jobb med Transfer Service i kildeprosjektet.\n\n\nssb.bucket.write\nLese og skrive til kildeb√∏tta.\n\n\n\n\n\n\nSkal du sette opp en Transfer Service overf√∏ring med kildedata s√• m√• du aktivere ssb.bucket.write og storagetransfer.admin. SKal du skrive og lese fra et milj√∏ som Jupyter, s√• m√• du aktivere rollene ssb.bucket.write og ssb.buckets.list.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Just-in-Time Access"
    ]
  },
  {
    "objectID": "statistikkere/jit.html#footnotes",
    "href": "statistikkere/jit.html#footnotes",
    "title": "Just-in-Time Access",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nHvis du jobber fra hjemmekontor s√• m√• du v√¶re p√• VPN for √• f√• tilgang til JIT-appen.‚Ü©Ô∏é",
    "crumbs": [
      "Manual",
      "Appendix",
      "Just-in-Time Access"
    ]
  },
  {
    "objectID": "statistikkere/klass.html",
    "href": "statistikkere/klass.html",
    "title": "Klass",
    "section": "",
    "text": "Klass er SSBs system for dokumentasjon av kodeverk1. Hensikten er at alle kodeverk skal dokumenteres og oppdateres ett sted (av ansvarlig seksjon), og gjenbrukes av alle som har behov for dem. Ansvarlig seksjon vil da sikre at alle f√•r tak i gyldig versjon (eller en tidligere versjon dersom det er det brukeren trenger).\nAlle2 kodeverk som brukes i SSBs statistikkproduksjon skal dokumenteres i Klass. Hvert kodeverk har en eierseksjon, og det er eierseksjonen som har ansvar for √• dokumentere, og senere oppdatere, kodeverket. Klass viser hvilke kategorier de ulike kodeverkene inneholder p√• en gitt dato, eller i l√∏pet av en gitt tidsperiode, hvordan kategoriene endrer seg over tid (f.eks. endring i kommuneinndelingen Standard for kommuneinndeling). I tillegg kan Klass vise mappingen mellom to ulike kodeverk (f.eks mellom politidistrikt og kommuner Standard for politidistrikt). En kan ogs√• lage en variant av et kodeverk, der en f.eks. aggregerer ulike koder i det opprinnelige kodeverket (se f.eks. Standard for n√¶ringsgruppering (SN) som er en variant der kodene i Standard for n√¶ringsgruppering er blitt aggregert). Kodeverkene kan gjenbrukes av alle som trenger dem, b√•de SSB-ansatte og eksterne brukere. Mange av kodeverkene i Klass, f.eks. Standard for kommuneinndeling og Standard for n√¶ringsgruppering, gjenbrukes i stor grad b√•de av interne og eksterne brukere.\nDet kan refereres til Klass b√•de fra Vardef og Datadoc.\nKodeverkene kan hentes ut via et API eller lastes ned i csv-format. API-er det mest fleksible √• bruke. Her kan en f.eks. hente ut kodeverket slik det var p√• en bestemt dato, i et bestemt tidsrom, hvilke endringer som har skjedd innen et visst tidsrom og korrespondansetabeller som viser mappingen mellom to ulike kodeverk. Det finnes ogs√• veiledning til bruk av Klass i statistikkproduksjonen (sas, R og Python) her: Bruk av Klass i statistikkproduksjonen.",
    "crumbs": [
      "Manual",
      "Metadata",
      "Klass"
    ]
  },
  {
    "objectID": "statistikkere/klass.html#footnotes",
    "href": "statistikkere/klass.html#footnotes",
    "title": "Klass",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nKodeverk er en fellesbetegnelse for klassifikasjoner og kodelister. En klassifikasjon er et mer ¬´formelt¬ª kodeverk som oppfyller flere betingelser, bl.a. at den er normativ og utt√∏mmende. Alle klassifikasjoner i SSB har navn som begynner med Standard for, f.eks. Standard for n√¶ringsgruppering eller Standard for kommuneinndeling. Ei kodeliste er ofte skreddersydd for en bestemt statistikk, er ikke normativ og trenger ikke v√¶re utt√∏mmende.‚Ü©Ô∏é\nDet finnes kodeverk som ikke kan legges inn i Klass fordi de ikke oppfyller kravene som stilles i den internasjonale modellen som Klass bygger p√•.‚Ü©Ô∏é",
    "crumbs": [
      "Manual",
      "Metadata",
      "Klass"
    ]
  },
  {
    "objectID": "statistikkere/altinn-utvalg-sfu.html",
    "href": "statistikkere/altinn-utvalg-sfu.html",
    "title": "Utvalg fra SFU",
    "section": "",
    "text": "Denne siden beskriver hvordan du bruker dapla-suv-tools pakken for √• hente utvalg og enhetsinformasjon fra SFU i Dapla-milj√∏et.\nDet finnes fortsatt avhengigheter til bakkesystemene for √• kunne kj√∏re et fullstendig produksjonsl√∏p p√• Dapla. SU-V har laget integrasjoner mellom bakke- og skyl√∏sninger der det er behov for dette. Administrasjon av utvalg og enheter skjer fortsatt fra SFU p√• bakke."
  },
  {
    "objectID": "statistikkere/altinn-utvalg-sfu.html#forutsetninger",
    "href": "statistikkere/altinn-utvalg-sfu.html#forutsetninger",
    "title": "Utvalg fra SFU",
    "section": "Forutsetninger",
    "text": "Forutsetninger\n\n\n\n\n\n\nTilgang til bakkesystemer\n\n\n\nFor √• kunne hente enhetsinformasjon fra SFU m√• du ha tilgang til delregisteret i riktig milj√∏ (DB1T/DB1P)."
  },
  {
    "objectID": "statistikkere/altinn-utvalg-sfu.html#kodeeksempler",
    "href": "statistikkere/altinn-utvalg-sfu.html#kodeeksempler",
    "title": "Utvalg fra SFU",
    "section": "Kodeeksempler",
    "text": "Kodeeksempler\nEksemplene viser hvordan du henter utvalg og enhetsinformasjon fra SFU.\n\nHente utvalg fra SFU\nFor √• hente utvalg fra SFU, bruk metoden get_utvalg_from_sfu i SuvClient. S√∏rg for at du oppgir riktig delregisternummer og RA-nummer for utvalget. Dersom utvalget er delt inn i puljer kan du oppgi pulje som parameter.\n\n\nnotebook\n\nclient = SuvClient()\n\nresponse = client.get_utvalg_from_sfu(\n    delreg_nr=49430224,\n    ra_nummer='RA-0666A3',\n    pulje='2'\n)\n\nprint(json.dumps(response, indent=4))\n\n\n\nHente enhetsinformasjon fra SFU\nFor √• hente enhetsinformajon fra SFU, bruk metoden get_enhet_from_sfu i SuvClient. S√∏rg for at du oppgir riktig delregisternummer og organisajonsnummer.\n\n\nnotebook\n\nclient = SuvClient()\n\nresponse = client.get_enhet_from_sfu(\n    delreg_nr=49430224,\n    orgnr='123456789' \n)\n\nprint(json.dumps(response, indent=4))\n\n\n\nEksempel p√• output\nResultatet returneres i en json struktur\n{\n    \"delreg_nr\": 49430224,\n    \"ident_nr\": \"A3TF0019\",\n    \"orgnr\": \"123456789\",\n    \"enhets_type\": \"FRTK\",\n    \"foretak\": \"A3TF0019\",\n    \"orgnr_foretak\": \"123456789\",\n    \"flv\": \"0\",\n    \"navn1\": \"MITT REGNSKAP\",\n    \"navn2\": null,\n    \"navn3\": null,\n    \"f_adresse1\": \"Testvegen 19\",\n    \"f_adresse2\": null,\n    \"f_adresse3\": null,\n    \"f_postnr\": \"0019\",\n    \"f_poststed\": \"OSLO\",\n    \"maalform\": null,\n    \"kontaktperson\": \"OLA NORDMANN\",\n    \"kont_telefon\": \"12121212\",\n    \"kont_mobiltlf\": null,\n    \"kont_epost\": null,\n    \"h_var1_n\": null,\n    \"h_var2_n\": null,\n    \"h_var3_n\": null,\n    \"h_var1_a\": null,\n    \"h_var2_a\": null,\n    \"h_var3_a\": null,\n    \"utvalgsstatus\": null,\n    \"pulje_nr\": 2,\n    \"vedtak_tvmulkt\": \"N\",\n    \"sendt_si\": \"N\",\n    \"status\": null,\n    \"org_form\": null,\n    \"sn07_1\": \"17.120\",\n    \"str_kode\": null,\n    \"viktig_enhet\": null,\n    \"kommentar_int\": null,\n    \"kommentar_ekst\": null,\n    \"test_pulje\": null,\n    \"prosedyre\": null,\n    \"felles_oppgave\": null\n}"
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html",
    "href": "statistikkere/jupyter-pyspark.html",
    "title": "Jupyter-pyspark",
    "section": "",
    "text": "Jupyter-pyspark er en tjeneste p√• Dapla Lab som lar brukerne kode i Jupyterlab. Tjenesten kommer med Python, Pyspark og noen vanlige Jupyterlab-extensions ferdig installert. M√•lgruppen for tjenesten er brukere som skal skrive produksjonskode med Pyspark i Jupyterlab.\nSiden tjenesten er ment for produksjonskode s√• er det veldig f√• Python-pakker som er forh√•ndsinstallert. Antagelsen er at brukeren/teamet heller b√∏r installere de pakkene de selv trenger, framfor at det ligger ferdiginstallerte pakker som skal dekke behovet til alle brukere/team i SSB. Det reduserer kompleksitet i tjenesten og dermed sannsynligheten for feilsituasjoner.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#forberedelser",
    "href": "statistikkere/jupyter-pyspark.html#forberedelser",
    "title": "Jupyter-pyspark",
    "section": "Forberedelser",
    "text": "Forberedelser\nF√∏r man starter Jupyter-pyspark b√∏r man ha lest kapitlet om Dapla Lab og satt opp Git- og GitHub-konfigurasjonen under Min konto. Deretter gj√∏r du f√∏lgende:\n\nLogg deg inn p√• Dapla Lab\nUnder Tjenestekatalog trykker du p√• Start-knappen for Jupyter-pyspark\nGi tjenesten et navn\n√Öpne Jupyter-pyspark konfigurasjoner",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#konfigurasjon",
    "href": "statistikkere/jupyter-pyspark.html#konfigurasjon",
    "title": "Jupyter-pyspark",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nF√∏r man √•pner en tjeneste kan man konfigurere hvor mye ressurser man √∏nsker, hvilket team man skal representere, om et GitHub-repo skal klones ved oppstart, og mange andre ting. Valgene man gj√∏r kan ogs√• lagres slik at man √• slipper √• gj√∏re samme jobb senere. Figur¬†1 viser Tjeneste-delen i konfigurasjonen for Jupyter hvor man kan velge hvilken versjon av Jupyter man vil bruke.\n\n\n\n\n\n\nFigur¬†1: Jupyter-pyspark versjon i Dapla Lab\n\n\n\n\nData\nUnder Data-menyen kan man velge hvilket team og tilgangsgruppe man skal representere. Man gj√∏r dette ved √• velge navnet p√• tilgangsgruppen, og denne er alltid p√• formen &lt;teamnavn&gt;-&lt;tilgangsgruppe&gt;. Figur¬†2 viser at brukeren har valgt tilgangsgruppen dapla-felles-developers, dvs. at de representerer tilgangsgruppen developers for teamet dapla-felles.\n\n\n\n\n\n\nFigur¬†2: Detaljert tjenestekonfigurasjon for b√∏ttetilgang i Dapla Lab\n\n\n\nUnder Team og tilgangsgruppe kan brukeren ogs√• velge √• representere tilgangsgruppen data-admins for et team. I de tilfellene er det et krav om brukeren oppgir en skriftlig begrunnelse for hvorfor tilgangen er n√∏dvendig. I tillegg m√• kan de maksimalt aktivere tilgangen i 8 timer.\nFigur¬†3 viser en bruker som aktiverer sin data-admins tilgang for team dapla-felles. Hvis brukeren ikke oppgir en begrunnelse vil de f√• en feilmelding ved oppstart av tjenesten.\n\n\n\n\n\n\nFigur¬†3: Aktivere tilgang til kildedata for data-admins.\n\n\n\n\n\nGit/GitHub\nUnder menyen Git/GitHub kan man konfigurere Git og GitHub slik at det blir lettere √• jobbe med inne i tjenesten. Som standard arves informasjonen som er lagret under Min konto-Git i Dapla Lab. Informasjonen under tjenestekonfigurasjonen blir tilgjengeliggjort som milj√∏variabler i tjenesten. Informasjonen blir ogs√• lagt i $HOME/.netrc slik at man kan benytte ikke trenger √• gj√∏re noe mer for √• jobbe mot GitHub fra tjenesten.\n\n\n\n\n\n\nFigur¬†4: Konfigurasjon av Git og GitHub for Jupyter-pyspark tjenesten i Dapla Lab\n\n\n\nFigur¬†4 viser at brukeren som standard f√•r aktivert Aktiver Git. Dette inneb√¶rer at Git-brukernavn, Git e-post og GitHub-token arves fra brukerkonfigurasjonen. I tillegg s√• opprettes SSBs standard Git-konfigurasjon i ~/.gitconfig.\n\n\nPython/R\nUnder menyen Python/R kan man velge hvilke versjon av R og Python man √∏nsker √• kj√∏re. Man kan velge mellom alle tidligere tilbudte kombinasjoner av R og Python.\nI Figur¬†5 ser vi av navnet py311-spark3.5.3-v4-2024.11.21 at tjenesten som default vil startes versjon 3.11 av Python og og 3.5.3 av Spark. Etter hvert som nye versjoner av Python og Spark kommer, kan disse tilgjengeliggj√∏res i tjenesten, men brukeren kan velge √• starte en eldre versjon av tjenesten.\n\n\n\n\n\n\nFigur¬†5: Konfigurasjon av Git og GitHub for Jupyter-pyspark tjenesten i Dapla Lab\n\n\n\n\n\nRessurser\nUnder menyen Resources kan man velge hvor mye CPU og RAM man √∏nsker i tjenesten, slik som vist i Figur¬†6. Velg s√• lite treng for √•s gj√∏re jobben du skal gj√∏re.\n\n\n\n\n\n\nFigur¬†6: Konfigurasjon av ressurser for Jupyter-pyspark tjenesten i Dapla Lab\n\n\n\n\n\nDiskplass\nSom default f√•r alle som starter en instans av tjenesten en lokal disk p√• 10GB inne i tjenesten. Under Diskplass-menyen kan man velge √• √∏ke st√∏rrelsen p√• disken eller ikke noe disk i det hele tatt. Siden lokal disk i tjenesten hovedsakelig skal benyttes til √• lagre en lokal kopi av koden som lagres p√• GitHub mens man gj√∏r endringer b√∏r ikke st√∏rrelsen p√• disken v√¶re stor. Figur¬†7 viser valgene som kan gj√∏res under Diskplass-fanen.\n\n\n\n\n\n\nFigur¬†7: Konfigurasjon av lokal disk for Jupyter-pyspark tjenesten i Dapla Lab\n\n\n\n\n\nAvansert\nUnder Avansert kan man velge √• ikke tilgjengeliggj√∏re b√∏tter som filsystem inne i tjenesten. Konsekvensen av dette er at man m√• lese og skrive filer ved √• referere til b√∏ttene direkte.\n\n\n\n\n\n\nFigur¬†8: Avansert konfigurasjon for Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#tilgjengelige-jar-filer-og-bruk-i-pyspark",
    "href": "statistikkere/jupyter-pyspark.html#tilgjengelige-jar-filer-og-bruk-i-pyspark",
    "title": "Jupyter-pyspark",
    "section": "Tilgjengelige JAR-filer og bruk i PySpark",
    "text": "Tilgjengelige JAR-filer og bruk i PySpark\nI /jupyter/lib-mappen i Jupyter-pyspark-milj√∏et er flere nyttige JAR-filer tilgjengelige for bruk med PySpark, inkludert st√∏tte for Google Cloud Storage, BigQuery, Avro og Delta Lake. Disse JAR-filene kan inkluderes i PySpark-konfigurasjonen for √• f√• tilgang til og arbeide med data fra disse kildene.\n\nTilgjengelige JAR-filer:\n\ngcs-connector-hadoop.jar: Kobler PySpark til Google Cloud Storage.\nspark-bigquery-with-dependencies_2.12.jar: Kobler PySpark til Google BigQuery.\nspark-avro_2.12.jar: St√∏tte for √• lese og skrive Avro-data.\ndelta-storage.jar og delta-core_2.12.jar: St√∏tte for Delta Lake, som muliggj√∏r ACID-transaksjoner og data versjonering.\n\nLegge til JAR-filer i PySpark:\n\nFor √• bruke disse JAR-filene, konfigurer PySpark med stien til hver fil:\nspark = SparkSession.builder \\\n    .appName(\"Jupyter-pyspark-konfig\") \\\n    .config(\"spark.jars\", \"/jupyter/lib/gcs-connector-hadoop.jar,\"\n                          \"/jupyter/lib/spark-bigquery-with-dependencies_2.12.jar,\"\n                          \"/jupyter/lib/spark-avro_2.12.jar,\"\n                          \"/jupyter/lib/delta-storage.jar,\"\n                          \"/jupyter/lib/delta-core_2.12.jar\") \\\n    .getOrCreate()\n\nEksempler p√• bruk av tilkoblingene:\n\nGoogle Cloud Storage (GCS):\ndf = spark.read.format(\"parquet\").load(\"gs://ditt-bucket-navn/path/to/data\")\nGoogle BigQuery:\ndf = spark.read.format(\"bigquery\") \\\n    .option(\"table\", \"prosjekt_id.dataset_id.tabell_id\") \\\n    .load()\nAvro:\ndf = spark.read.format(\"avro\").load(\"/path/to/avro/files\")\nDelta Lake:\n\nFor √• skrive til en Delta-tabell:\ndf.write.format(\"delta\").save(\"/path/to/delta-table\")\nFor √• lese fra en Delta-tabell:\ndelta_df = spark.read.format(\"delta\").load(\"/path/to/delta-table\")\n\n\n\nMed disse instruksjonene kan brukerne effektivt konfigurere Jupyter-PySpark til √• jobbe med eksterne datakilder og forskjellige dataformater i sitt PySpark-milj√∏.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#hvordan-spark-lokalt-fungerer-og-arbeidsfordeling-p√•-kjerner",
    "href": "statistikkere/jupyter-pyspark.html#hvordan-spark-lokalt-fungerer-og-arbeidsfordeling-p√•-kjerner",
    "title": "Jupyter-pyspark",
    "section": "Hvordan Spark Lokalt Fungerer og Arbeidsfordeling p√• Kjerner",
    "text": "Hvordan Spark Lokalt Fungerer og Arbeidsfordeling p√• Kjerner\nN√•r Spark kj√∏res lokalt, starter det en SparkSession som kj√∏rer p√• en enkelt node (tjenesten din) uten √• involvere en distribuert klynge. Lokalt i Spark kan du kontrollere ressursbruken og fordele arbeidsmengden p√• tilgjengelige CPU-kjerner for √• optimalisere ytelsen.\n\nKj√∏ring i Lokal Modus\nN√•r Spark konfigureres til √• kj√∏re i lokal modus, spesifiseres dette med \"local[N]\", der N representerer antall kjerner som Spark skal bruke. For eksempel: - \"local[*]\": Bruk alle tilgjengelige kjerner p√• maskinen. - \"local[2]\": Bruk 2 kjerner, uavhengig av hvor mange som er tilgjengelige.\nEksempel p√• konfigurasjon:\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"Lokal Spark\") \\\n    .master(\"local[*]\") \\  # Bruker alle tilgjengelige kjerner\n    .getOrCreate()",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#datatilgang",
    "href": "statistikkere/jupyter-pyspark.html#datatilgang",
    "title": "Jupyter-pyspark",
    "section": "Datatilgang",
    "text": "Datatilgang\nMan inspisere dataene fra en terminal inne i tjenesten:\n\n√Öpne en instans av Jupyter-pyspark med data fra b√∏tter\n√Öpne en terminal inne i Jupyter\nG√• til mappen med b√∏ttene ved √• kj√∏re dette fra terminalen cd /buckets\nKj√∏r ls -ahl i terminalen for √• se p√• hvilke b√∏tter som er montert.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#installere-pakker",
    "href": "statistikkere/jupyter-pyspark.html#installere-pakker",
    "title": "Jupyter-pyspark",
    "section": "Installere pakker",
    "text": "Installere pakker\nSiden det nesten ikke er installert noen pakker i tjenesten s√• kan brukeren opprette et ssb-project og installere pakker som vanlig.\nFor √• bygge et eksisterende ssb-project kan brukeren ogs√• bruke ssb-project.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#slette-tjenesten",
    "href": "statistikkere/jupyter-pyspark.html#slette-tjenesten",
    "title": "Jupyter-pyspark",
    "section": "Slette tjenesten",
    "text": "Slette tjenesten\nFor √• slette tjenesten kan man trykke p√• Slette-knappen i Dapla Lab under Mine tjenester. N√•r man sletter en tjeneste s√• slettes hele disken inne i tjenesten, og alle ressurser frigj√∏res. Vi anbefaler at man avslutter heller enn pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#pause-tjenesten",
    "href": "statistikkere/jupyter-pyspark.html#pause-tjenesten",
    "title": "Jupyter-pyspark",
    "section": "Pause tjenesten",
    "text": "Pause tjenesten\nMan kan pause tjenesten ved √• trykke p√• Pause-knappen i Dapla Lab under Mine tjenester. N√•r man pauser, slettes alt p√• den lokale disken som ikke er lagret under $HOME/work. Vi anbefaler at man avslutter heller enn pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#monitorering",
    "href": "statistikkere/jupyter-pyspark.html#monitorering",
    "title": "Jupyter-pyspark",
    "section": "Monitorering",
    "text": "Monitorering\n\n\nMan kan moniterere en instans av Jupyter-pyspark ved √• trykke p√• Jupyter-pyspark-teksten under Mine tjenester i Dapla Lab, slik som vist i Figur¬†9.\nDenne funksjonaliteten er under arbeid og mer informasjon kommer snart.\n\n\n\n\n\n\n\n\n\nFigur¬†9: Monitorering av Jupyter-pyspark i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/transfer-service.html",
    "href": "statistikkere/transfer-service.html",
    "title": "Transfer Service",
    "section": "",
    "text": "Storage Transfer Service1 er en Google-tjeneste for √• flytte data mellom lagringsomr√•der. I SSB bruker vi hovedsakelig tjenesten til √•:\nTjenesten st√∏tter b√•de automatiserte og ad-hoc overf√∏ringer, og den inkluderer et brukergrensesnitt for √• sette opp og administrere overf√∏ringene i Google Cloud Console (GCC).",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Transfer Service"
    ]
  },
  {
    "objectID": "statistikkere/transfer-service.html#tilgangsstyring",
    "href": "statistikkere/transfer-service.html#tilgangsstyring",
    "title": "Transfer Service",
    "section": "Tilgangsstyring",
    "text": "Tilgangsstyring\nTilgangsstyringen til data gjelder ogs√• for overf√∏ringer av data med Transfer Service. Det betyr at du m√• ha tilgang til dataene du skal sette opp overf√∏ringsjobber for. Ved bruk av Transfer Service for overf√∏ring av data mellom bakke og sky s√• er det satt opp en dedikerte mapper for dette i prodsonen. Ogs√• her f√∏lges tilgangsstyringen til dataene, med unntak av at data-admins har permanent tilgang til kildedata som er synkronisert ned til bakken, mens man p√• Dapla m√• de gi seg selv korte, begrunnede tilganger ved behov.\n\n\nP√• Dapla s√• er det opprettet dedikerte b√∏tter for overf√∏ring av data mellom bakke og sky. Disse heter tilsky og frasky. Tanken med disse ‚Äúmellomstasjonene‚Äù for overf√∏ring av data er at de skal beskytte Dapla-team fra √• overskrive data ved en feil. Ved √• ha egne b√∏tter som data blir synkronisert gjennom, s√• legges det opp til at man deretter manuelt3 flytter dataene til riktig b√∏tte.\nMen det er ikke lagt noen sperrer for synkronisere direkte til en annen b√∏tte man har tilgang til. Systembrukeren (se forklaringsboks) som kj√∏rer Transfer Service har tilgang til alle b√∏ttene i prosjektet. Det betyr at en data-admin kan velge √• synkronisere data direkte inn i kildeb√∏tta hvis man mener at det er hensiktsmessig. Det samme gjelder for developers som setter opp dataoverf√∏ringer i standardprosjektet. Men da er det som sagt viktig √• v√¶re bevisst p√• hvordan man setter opp reglene for overskriving av data hvis filene har like navn. Disse opsjonene forklares n√¶rmere senere i kapitlet.\n\n\n\n\n\n\n\n\n\nPersonlig bruker vs systembruker\n\n\n\nN√•r du setter opp en overf√∏ringsjobb med Transfer Service s√• setter du opp en jobb som kj√∏res av en systembruker4 og ikke din egen personlige bruker. Dette er spesielt viktig √• v√¶re klar over n√•r man setter opp automatiserte overf√∏ringsjobber. En konsekves av dette er at automatiske overf√∏ringsjobber vil fortsette √• kj√∏re selv om din tilgang til dataene er midlertidig, siden det er en systembruker som faktisk kj√∏rer jobben.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Transfer Service"
    ]
  },
  {
    "objectID": "statistikkere/transfer-service.html#forberedelser",
    "href": "statistikkere/transfer-service.html#forberedelser",
    "title": "Transfer Service",
    "section": "Forberedelser",
    "text": "Forberedelser\nF√∏rste gang du bruker Transfer Service m√• du sjekke at tjenesten er aktivert for teamet. Transfer Service er en s√•kalt feature som teamet kan skru av og p√• selv. For √• sjekke om den er skrudd p√• g√•r du inn i teamets IaC-repo5 og sjekker filen ./infra/projects.yaml.\n\n\ndapla-example-iac/infra/projects.yaml\n\nteam_uniform_name: dapla-example\n\nprojects:\n  - project_name: dapla-example\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service\n\nI filen over ser du at teamet har skrudd p√• tjenesten i prod-milj√∏et, siden den transfer-service er listet under features. Hvis tjenesten ikke er skrudd p√• kan du lese om hvordan du skrur den p√• i feature-dokumentasjonen.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Transfer Service"
    ]
  },
  {
    "objectID": "statistikkere/transfer-service.html#overf√∏ring-av-data",
    "href": "statistikkere/transfer-service.html#overf√∏ring-av-data",
    "title": "Transfer Service",
    "section": "Overf√∏ring av data",
    "text": "Overf√∏ring av data\n\n\n\n\n\n\nOverf√∏ring av kildedata\n\n\n\nOverf√∏ring av kildedata m√• gj√∏res av en data-admin i teamet som har aktivert sin forh√•ndsgodkjente tilgang til kildedata. Tilgangen aktiveres ved √• g√• inn i JIT-applikasjonen og velge prosjekt-id. Deretter velger du rollene ssb.bucket.write, ssb.buckets.list og storagetransfer.admin, og hvor lenge du √∏nsker tilgangen. Til slutt oppgir du en begrunnelse for hvorfor du trenger tilgangentilgangen og trykker Request access. N√•r du har gjort dette vil du f√• en bekreftelse p√• at tilgangen er aktivert, og det tar ca 1 minutt f√∏r den aktiverte tilgangen er synlig i GCC.\n\n\nGrensesnittet for √• sette opp overf√∏ringsjobber i Transfer Service er tilgjengelig i Google Cloud Console (GCC).\n\n\n\nG√• inn p√• Google Cloud Console i en nettleser.\nSjekk, √∏verst i h√∏yre hj√∏rne, at du er logget inn med din SSB-konto (xxx@ssb.no).\nVelg prosjektet6 som overf√∏ringen skal settes opp under.\nEtter at du har valgt prosjekt kan du s√∏ke etter Storage Transfer i s√∏kefeltet √∏verst p√• siden, og g√• inn p√• siden.\n\n\n\n\n\n\n\n\n\n\nHva er mitt prosjektnavn?\n\n\n\nN√•r det opprettes et Dapla-team, s√• opprettes det flere Google-prosjekter for teamet. N√•r du skal velge hvilket prosjekt du skal jobbe p√• i GCC, s√• f√∏lger de en fast navnestruktur. For eksempel s√• vil et team med navnet dapla-example f√• et standardprosjekt som heter dapla-example-p. Det blir ogs√• opprettet et kildeprosjekt som heter dapla-example-kilde-p.\n\n\n\n\nF√∏rste gang du bruker Storage Transfer m√• man gj√∏re en engangsjobb for √• bruke tjenesten. Dette gj√∏res kun f√∏rste gang din bruker setter opp en jobb, og deretter trenger du ikke √• gj√∏re det flere ganger.\nN√•r du kommer inn p√• siden til Storage Transfer s√• trykker du p√• Set Up Connection. N√•r du trykker p√• denne vil det dukke opp et nytt felt hvor du f√•r valget Create Pub-Sub Resources. Trykk p√• den bl√• Create-knappen, og deretter trykk p√• Close lenger nede. Da er engangsjobben gjort, og du kan begynne √• sette opp overf√∏ringsjobber.\n\nI navigasjonsmenyen til venstre trykk Transfer jobs, og deretter trykk p√• + Create transfer job √∏verst p√• siden for √• opprette en ny overf√∏ringsjobb. Da f√•r du opp bildet som vist i Figur¬†1.\n\n\n\n\n\n\n\nFigur¬†1: Opprett overf√∏ringsjobb i Google Cloud Console.\n\n\n\nVidere vil det variere om man skal overf√∏re data mellom b√∏tter eller mellom Dapla og prodsonen. Under forklarer vi begge fremgangsm√•tene.\n\nProdsonen og Dapla\nOverf√∏ring mellom bakke og sky er en overf√∏ring av data mellom en b√∏tte p√• Dapla og en mappe i prodsonen. Siden tilgangsstyring til kildedata er strengere enn tilgangsstyring til annen data, s√• er det det to litt fremgangsm√•ter for √• sette opp overf√∏ringsjobber for disse.\nSiden stegene er litt forskjellig avhengig av om man skal flytte kildedata eller annen data, s√• deler vi denne delen i to. Figur¬†2 viser hvordan dette er satt opp. Kildeprosjektet p√• Dapla har en tilsky-b√∏tte for √• flytting av data fra prodsonen til Dapla, og den har en frasky-b√∏tte for √• flytte data fra Dapla til prodsonen. Standardprosjektet p√• Dapla har ogs√• en tilsky-b√∏tte for √• flytte data fra prodsonen til Dapla, og den har en frasky-b√∏tte for √• flytte data fra Dapla til prodsonen.\n\n\n\n\n\n\nFigur¬†2: Overf√∏ring av data mellom prodsonen og Dapla.\n\n\n\nVidere viser vi hvordan man overf√∏rer fra Dapla til prodsonen. Overf√∏ring motsatt vei inneb√¶rer bare at man bytter om p√• Source type og Destination type.\n\nI fanen Get started velger du:\n\nSource type: Google Cloud Storage\nDestination type: POSIX filesystem\n\nI fanen Choose a source trykker du p√• Browse, velger hvilken b√∏tte eller ‚Äúundermappe‚Äù i en b√∏tte du skal overf√∏re fra, og trykker Select7.\nI fanen Choose a destination velger du transfer_service_default under Agent pool. Under Destination directory path velger du hvilken undermappe av som filen skal overf√∏res til. Tjenesten vet allerede om du er i kilde- eller standardprosjektet, s√• du trenger kun √• skrive inn frasky/ eller tilsky/ her, og evt. undermappenavn hvis det er aktuelt (f.eks. frasky/data/8). Trykk Next step.\nI fanen Choose when to run job velger du hvor ofte og hvordan jobber skal kj√∏re. Tabell¬†1 viser hvilke valg du kan ta. Trykk Next step.\n\n\n\n\nTabell¬†1: Valg under Choose when to run job\n\n\n\n\n\n\n\n\n\nValg\nFrekvens\n\n\n\n\nRun once\nEngangoverf√∏ringer\n\n\nRun every day\nSynkroniser hver dag\n\n\nRun every week\nSynkroniser hver uke\n\n\nRun with custom frequency\nSynkroniser inntill hver time\n\n\nRun on demand\nSynkroniserer n√•r du manuelt trigger jobben\n\n\n\n\n\n\n\nI fanen Choose settings kan du velge hvordan detaljer knyttet til overf√∏ringen skal h√•ndteres. Tabell¬†2 viser hvilke valg du kan ta.\n\n\n\n\nTabell¬†2: Valg under Choose settings\n\n\n\n\n\n\n\n\n\n\nValg\nUndervalg\nHandling\n\n\n\n\nIdentify your job\n\nBeskriv jobben kort.\n\n\nManifest file\n\nIkke relevant. Bruk default valg.\n\n\nChoose how to handle your data\nMetadata options\nIkke relevant. Bruk default valg.\n\n\n\nWhen to overwrite\nTenk n√∏ye gjennom hva du velger her.\n\n\n\nWhen to delete\nTenk n√∏ye gjennom hva du velger her.\n\n\nChoose how to keep track of transfer progress\nLogging options\nSkru p√• logging.\n\n\n\n\n\n\nValgene When to overwrite og When to delete er det viktig at tenkes n√∏ye gjennom, spesielt ved automatiske synkroniseringer. When to overwrite er spesielt siden det kan f√∏re til data blir overskrevet eller tapt.\n\nTrykk p√• den bl√• Create-knappen for √• opprette overf√∏ringsjobben. Du vil kunne se kj√∏rende jobber under menyen Transfer jobs.\n\n\nMappestrukturen i prodsonen\nMappestrukturen for overf√∏ringer med Transfer Service mellom bakke og sky har en fast struktur som er likt for alle team. Hvis du logger deg inn i terminalen p√• en av Linux-serverne i prodsonen, √•pner du mappen ved √• skrive cd /ssb/cloud_sync. Under denne mappen finner du en mappe for hvert team som har aktivert Transfer Service. Hvis et team for eksempel heter dapla-example s√• vil det v√¶re en mappe som heter dapla-example. Her kan teamet hente og levere data som skal synkroniseres mellom bakke og sky. Videre er det undermapper for kilde- og standardprosjektet til teamet. Det er kun data-admins som har tilgang til kildeprosjektet, og det er kun developers som har tilgang til standardprosjektet. Under finner du en oversikt over hvordan mappene ser ut for et team som heter dapla-example.\n\n\n/ssb/cloud_sync/dapla-example/\n\ndapla-example\n‚îÇ\n‚îú‚îÄ‚îÄ kilde\n‚îÇ   ‚îÇ\n‚îÇ   ‚îÇ‚îÄ‚îÄ tilsky\n‚îÇ   ‚îÇ\n‚îÇ   ‚îî‚îÄ‚îÄ frasky\n‚îÇ\n‚îî‚îÄ‚îÄ standard\n    ‚îÇ\n    ‚îÇ‚îÄ‚îÄ tilsky\n    ‚îÇ\n    ‚îî‚îÄ‚îÄ frasky\n\n\n\n\nB√∏tte til b√∏tte\nOverf√∏ring mellom b√∏tter er en overf√∏ring av data mellom to b√∏tter p√• Dapla. Fremgangsm√•ten er helt likt som beskrevet tidligere, men at du n√• velger Google Cloud Storage som b√•de kilde og destinasjon. Igjen er vi avhengig av at systembrukeren som utf√∏rer jobben har tilgang til begge b√∏ttene som er involvert i overf√∏ringen. Default er at et team kan overf√∏re mellom b√∏tter i kildeprosjektet, og at de kan overf√∏re mellom b√∏tter i standardprosjektet, men aldri mellom de to. Hvis du √∏nsker √• overf√∏re mellom b√∏tter i ditt prosjekt og et annet teams prosjekt, s√• m√• du be det andre teamet om √• gi din systembruker tilgang til dette.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Transfer Service"
    ]
  },
  {
    "objectID": "statistikkere/transfer-service.html#footnotes",
    "href": "statistikkere/transfer-service.html#footnotes",
    "title": "Transfer Service",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI SSB kaller vi tjenesten for Transfer Service, men du kan oppleve at Google kaller den litt forskjellige ting. Den blir omtalt som Storage Transfer Service noen steder, mens i Google Cloud Console blir den omtalt som Data Transfer eller Storage Transfer‚Ü©Ô∏é\nFlytting av data mellom b√∏tter krever at prosjektets Transfer Service har tilgang til begge b√∏ttene.‚Ü©Ô∏é\nMed manuelt menes her at man g√•r inn og flytter filer fra en b√∏tte til en annen. Men det kan ogs√• bety at man flytter data til riktig b√∏tte som en del produksjonskoden sin, som igjen kan kj√∏res automatisk.‚Ü©Ô∏é\nSystembrukere heter Service Accounts p√• engelsk og blir ofte referert til som SA-er i dagligtale.‚Ü©Ô∏é\nDu finner teamets IaC-repo ved √• g√• inn p√• https://github.com/orgs/statisticsnorway/repositories og s√∏ke etter ditt teamnavn og √•pne den som har navnestrukturen teamnavn-iac. For eksempel vil et team som heter dapla-example har et IaC-repo som heter dapla-example-iac.‚Ü©Ô∏é\nDu kan velge prosjekt √∏verst p√• siden, til h√∏yre for teksten Google Cloud. I bildet under ser du at hvordan det ser ut n√•r prosjektet dapla-felles-p er valgt.‚Ü©Ô∏é\nN√•r du skal velge en undermappe i en b√∏tte s√• er grensesnittet litt lite intuitivt. Du kan ikke trykke p√• navnet, men du p√• trykke p√• -tegnet for √• se undermappene.‚Ü©Ô∏é\nN√•r du skal synkronisere fra Dapla til en undermappe i prodsonen, s√• m√• mappen i prodsonen allerede eksisterere. Hvis den ikke gj√∏r det vil jobben feile. Ved synkronsiering fra prodsonen til Dapla trenger ikke undermappen eksistere, siden b√∏tter egentlig ikke har undermapper og filstien fra prodsonen bare blir til filnavnet i b√∏tta.‚Ü©Ô∏é",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Transfer Service"
    ]
  },
  {
    "objectID": "statistikkere/deling-av-data.html",
    "href": "statistikkere/deling-av-data.html",
    "title": "Deling av data",
    "section": "",
    "text": "Dapla-team kan dele data mellom team via s√•kalte delt-b√∏tter. Hvert team kan opprette de delt-b√∏ttene de har behov for, og deretter gi tilgang til grupper i andre team. Opprettelse av b√∏tter skal f√∏lge retningslinjene som er definert her.\n\n\nFor √• opprette delt-b√∏tter m√• man f√∏rst skru p√• featuren shared-buckets for teamet. Det gj√∏res ved at en p√• teamet gj√∏r f√∏lgende:\n\n\n\nG√• til IaC-repoet til teamet p√• Github.\n√Öpne filen ./infra/projects.yaml.\nLegg til en linje med shared-buckets under features i milj√∏et du √∏nsker, slik som vist til h√∏yre.\nOpprette en PR med endringen.\nBe en i gruppa data-admins se over endringen og godkjenne.\nKj√∏r atlantis plan og atlantis apply slik som beskrevet her.\n\n\n\n\n\n\n\nprojects.yaml\n\nteam_uniform_name: dapla-example\n\nprojects:\n  - project_name: dapla-example\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service\n      - shared-buckets\n\n\n\n\n\n\nOpprettelse av delt-b√∏tter gj√∏res i teamets IaC-repo. For √• opprette en delt-b√∏tte m√• man legge til en linje i fila ./infra/projects/&lt;teamnavn&gt;-prod/buckets-shared.yaml. Du legger bare til kortnavnet for b√∏tta, alts√• den delen av b√∏tte-navnet som teamet kan styre selv. For √• opprette en b√∏tte med kortnavn freg, s√• fyller man bare inn f√∏lgende:\n\n\nbuckets-shared.yaml\n\n# Use this file to create new buckets for sharing data easily.\n# Simply add a line with a dash and your new shared bucket's name:\n# - my-shared-data\n# This will create a bucket with the name ssb-arbmark-register-data-delt-my-shared-data-prod\n- freg\n\nDe 5 f√∏rste linjene er bare kommentarer som forteller hvordan man oppretter b√∏ttene. P√• linje 6 ser vi hva som m√•tte fylles ut for √• opprette en b√∏tte med kortnavn freg, som igjen f√•r b√∏ttenavnet:\nssb-&lt;teamnavn&gt;-data-delt-freg-prod\nEtter at endringen er gjort i buckets-shared.yaml, s√• gj√∏r du f√∏lgende:\n\nOpprette en PR p√• repoet med endringen.\nF√• en i gruppen data-admins til √• g√• gjennom og godkjenne.\nKj√∏r atlantis plan og atlantis apply som beskrevet her.\n\n\n\n\n\n\n\nNavngivning av b√∏tter\n\n\n\nNavngivning av b√∏tter f√∏lger RFC 1123. Dvs. at navnet m√• f√∏lge disse reglene:\n\nMax 63 tegn i navnet (gjelder fullt navn p√• b√∏tta)\nkun sm√• bokstaver a-z og sifre 0-9\ntillatt med -\ndet er ikke tillatt med _\n\n\n\n\n\n\nTilgangsstyringen til delt-b√∏tta kan gj√∏res av teamet som eier b√∏tta. Hvem som har tilgang blir definert i fila ./infra/projects/&lt;teamnavn&gt;-prod/iam.yaml i IaC-repoet til teamet. Under ser man strukturen p√• iam.yaml for √• gi tilgang til delt-b√∏tter:\n\n\niam.yaml\n\nbuckets:\n  ssb-dapla-example-data-delt-freg-prod:\n    team-alpha-data-admins:\n    - ssb.bucket.read\n    team-alpha-developers:\n    - ssb.bucket.read\n\n  ssb-dapla-example-data-delt-ameld-prod:\n    team-beta-developers:\n    - ssb.bucket.read\n\nOver ser vi at teamet dapla-example har to delt-b√∏tter:\n\nssb-dapla-example-data-delt-freg-prod\nssb-dapla-example-data-delt-ameld-prod\n\nDen f√∏rste b√∏tta har de gitt lesetilgang til data-admins og developers i team-alpha. Fra linje 8 og nedover ser vi at de har gitt developers i team-beta lesetilgang til ssb-dapla-example-data-delt-ameld-prod.\nN√•r man har gjort endringer i iam.yaml s√• gj√∏r man f√∏lgende:\n\nOpprette en PR med endringen.\nF√• en i gruppen data-admins til √• g√• gjennom endringen og godkjenne.\nKj√∏r atlantis plan og atlantis apply som beskrevet her.",
    "crumbs": [
      "Manual",
      "Data",
      "Deling av data"
    ]
  },
  {
    "objectID": "statistikkere/deling-av-data.html#forberedelser",
    "href": "statistikkere/deling-av-data.html#forberedelser",
    "title": "Deling av data",
    "section": "",
    "text": "For √• opprette delt-b√∏tter m√• man f√∏rst skru p√• featuren shared-buckets for teamet. Det gj√∏res ved at en p√• teamet gj√∏r f√∏lgende:\n\n\n\nG√• til IaC-repoet til teamet p√• Github.\n√Öpne filen ./infra/projects.yaml.\nLegg til en linje med shared-buckets under features i milj√∏et du √∏nsker, slik som vist til h√∏yre.\nOpprette en PR med endringen.\nBe en i gruppa data-admins se over endringen og godkjenne.\nKj√∏r atlantis plan og atlantis apply slik som beskrevet her.\n\n\n\n\n\n\n\nprojects.yaml\n\nteam_uniform_name: dapla-example\n\nprojects:\n  - project_name: dapla-example\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service\n      - shared-buckets",
    "crumbs": [
      "Manual",
      "Data",
      "Deling av data"
    ]
  },
  {
    "objectID": "statistikkere/deling-av-data.html#opprettelse-av-b√∏tte",
    "href": "statistikkere/deling-av-data.html#opprettelse-av-b√∏tte",
    "title": "Deling av data",
    "section": "",
    "text": "Opprettelse av delt-b√∏tter gj√∏res i teamets IaC-repo. For √• opprette en delt-b√∏tte m√• man legge til en linje i fila ./infra/projects/&lt;teamnavn&gt;-prod/buckets-shared.yaml. Du legger bare til kortnavnet for b√∏tta, alts√• den delen av b√∏tte-navnet som teamet kan styre selv. For √• opprette en b√∏tte med kortnavn freg, s√• fyller man bare inn f√∏lgende:\n\n\nbuckets-shared.yaml\n\n# Use this file to create new buckets for sharing data easily.\n# Simply add a line with a dash and your new shared bucket's name:\n# - my-shared-data\n# This will create a bucket with the name ssb-arbmark-register-data-delt-my-shared-data-prod\n- freg\n\nDe 5 f√∏rste linjene er bare kommentarer som forteller hvordan man oppretter b√∏ttene. P√• linje 6 ser vi hva som m√•tte fylles ut for √• opprette en b√∏tte med kortnavn freg, som igjen f√•r b√∏ttenavnet:\nssb-&lt;teamnavn&gt;-data-delt-freg-prod\nEtter at endringen er gjort i buckets-shared.yaml, s√• gj√∏r du f√∏lgende:\n\nOpprette en PR p√• repoet med endringen.\nF√• en i gruppen data-admins til √• g√• gjennom og godkjenne.\nKj√∏r atlantis plan og atlantis apply som beskrevet her.\n\n\n\n\n\n\n\nNavngivning av b√∏tter\n\n\n\nNavngivning av b√∏tter f√∏lger RFC 1123. Dvs. at navnet m√• f√∏lge disse reglene:\n\nMax 63 tegn i navnet (gjelder fullt navn p√• b√∏tta)\nkun sm√• bokstaver a-z og sifre 0-9\ntillatt med -\ndet er ikke tillatt med _",
    "crumbs": [
      "Manual",
      "Data",
      "Deling av data"
    ]
  },
  {
    "objectID": "statistikkere/deling-av-data.html#tilgangsstyring",
    "href": "statistikkere/deling-av-data.html#tilgangsstyring",
    "title": "Deling av data",
    "section": "",
    "text": "Tilgangsstyringen til delt-b√∏tta kan gj√∏res av teamet som eier b√∏tta. Hvem som har tilgang blir definert i fila ./infra/projects/&lt;teamnavn&gt;-prod/iam.yaml i IaC-repoet til teamet. Under ser man strukturen p√• iam.yaml for √• gi tilgang til delt-b√∏tter:\n\n\niam.yaml\n\nbuckets:\n  ssb-dapla-example-data-delt-freg-prod:\n    team-alpha-data-admins:\n    - ssb.bucket.read\n    team-alpha-developers:\n    - ssb.bucket.read\n\n  ssb-dapla-example-data-delt-ameld-prod:\n    team-beta-developers:\n    - ssb.bucket.read\n\nOver ser vi at teamet dapla-example har to delt-b√∏tter:\n\nssb-dapla-example-data-delt-freg-prod\nssb-dapla-example-data-delt-ameld-prod\n\nDen f√∏rste b√∏tta har de gitt lesetilgang til data-admins og developers i team-alpha. Fra linje 8 og nedover ser vi at de har gitt developers i team-beta lesetilgang til ssb-dapla-example-data-delt-ameld-prod.\nN√•r man har gjort endringer i iam.yaml s√• gj√∏r man f√∏lgende:\n\nOpprette en PR med endringen.\nF√• en i gruppen data-admins til √• g√• gjennom endringen og godkjenne.\nKj√∏r atlantis plan og atlantis apply som beskrevet her.",
    "crumbs": [
      "Manual",
      "Data",
      "Deling av data"
    ]
  },
  {
    "objectID": "statistikkere/contribution.html",
    "href": "statistikkere/contribution.html",
    "title": "Bidra til Dapla-manualen",
    "section": "",
    "text": "Alle i SSB kan bidra til denne manualen. Endringer m√• godkjennes av noen i Team Statistikktjenester eller Alex Crozier (akc@ssb.no). Si gjerne i fra at det ligger en PR √• se p√•. Vi trenger bidrag med alt fra spr√•kvask, nye artikler og andre gode initiativer! Har du lyst til √• bidra, men er ikke helt sikker p√• hva du kan bidra med? Ta en titt p√• issues i GitHub-repoet.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Bidra til Dapla-manualen"
    ]
  },
  {
    "objectID": "statistikkere/contribution.html#forutsetninger",
    "href": "statistikkere/contribution.html#forutsetninger",
    "title": "Bidra til Dapla-manualen",
    "section": "Forutsetninger",
    "text": "Forutsetninger\n\nMan trenger basis git kompetanse, det ligger en fin beskrivelse av det p√• Beste Praksis siden fra KVAKK.\nMan trenger en konto p√• Github, det kan man opprette ved √• f√∏lge instruksjonene her.\nMan kan l√¶re seg Markdown (enkel tekst formatering), en fin oversikt finnes her.\nVerkt√∏yet Quarto burde installeres for √• kunne se endringene slik som de ser ut p√• nettsiden. Installasjon instruksjoner finnes her.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Bidra til Dapla-manualen"
    ]
  },
  {
    "objectID": "statistikkere/contribution.html#fremgangsm√•ten",
    "href": "statistikkere/contribution.html#fremgangsm√•ten",
    "title": "Bidra til Dapla-manualen",
    "section": "Fremgangsm√•ten",
    "text": "Fremgangsm√•ten\n\nKlone repoet\n\n\n\nterminal\n\ngit clone https://github.com/statisticsnorway/dapla-manual.git\n\n\nLage en ny gren\nGj√∏re endringen\nKj√∏r f√∏lgende og f√∏lge lenken for √• sjekke at alt ser bra ut p√• nettsiden:\n\n\n\nterminal\n\nquarto preview dapla-manual\n\n\n√Öpne en PR\nBe noen √• gjennomg√• endringen og eventuelt godkjenne den\nMerge PRen\nEtter et par minutter skal din endring v√¶re synlig!\n\n\nTittelblock og nyhetsinnlegg\nI dapla-manualen har vi noen konvensjoner vi f√∏lger. En av de er √• lage en title block som det kalles p√• quarto sin nettside.\nVi ber v√•re bidragsytere om √• lage en lik block med overskrift og datoen artikkelen ble endret. √Öpner man filen til denne siden ser det p√• dette tidspunktet slik ut:\n\n\n\n\n\n\nFigur¬†1: Tittelblock for denne siden.\n\n\n\nManualens egen nyhetssiden skal oppdaters med et innlegg n√•r det kommer nye artikler. Det er ikke n√∏dvendig √• lage en sak om at en side har blitt oppdatert, med mindre endringene er omfattende. Nyhetssiden er i Quarto sitt blog-format. Fremgangsm√•ten er enkel og beskrives i quarto sin artikkel om blogger. Ellers anbefales det √• ta en titt p√• hvordan det gj√∏res i dapla-manualen. Alex Crozier (akc@ssb.no) er ansvarlig for nyhetssiden og kan kontaktes dersom man trenger hjelp.\n\n\nEmbedded notebooks\nQuarto tilbyr √• legge ved (embed) notebooks inn i nettsiden. Dette er en fin m√•te √• dele kode og output p√•. Men det krever at vi tenker gjennom hvor outputen genereres. Siden Dapla-manualen renderes med GitHub-action, s√• √∏nsker vi ikke √• introdusere kompleksiteten det inneb√¶rer √• generere output fra kode her. I tillegg er det mange milj√∏-spesifikke konfigurasjoner som bestemmer hvordan output kan genereres. Derfor anbefaler vi f√∏lgende tiln√¶rming n√•r man skal legge ved en notebook til et kapittel:\n\nLogg deg inn i milj√∏et du √∏nsker √• bruke.\nKlone ned repoet til Dapla-manualen.\nOpprett en notebook i mappen ./dapla-manual/notebooks/notebook-navn.ipynb\nSkriv kode og generer output som du √∏nsker. Husk √• bare bruke data som alle har tilgang til eller de kan generere selv.\nEmbed output i kapitlet du √∏nsker iht til denne beskrivelsen\nP√• toppen av notebooken lager du en raw-celle der du skriver:\n\n\n\nnotebook\n\n---\nfreeze: true\n---\n\n\nKj√∏r denne notebooken fra terminalen med kommandoen:\n\n\n\nterminal\n\nquarto render ./notebooks/&lt;notebook-navn.ipynb&gt; --execute\n\nDette sikrer at vi kan se output etter at boken bygges i GitHub Actions. Husk at notebooks ikke blir strippet for output p√• vanlig m√•te, slik at kun √•pne data skal benyttes.\nSp√∏r Team Statistikktjenester eller Alex (akc@ssb.no) om du lurer p√• noe.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Bidra til Dapla-manualen"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html",
    "href": "statistikkere/jupyter.html",
    "title": "Jupyter",
    "section": "",
    "text": "Jupyter er en tjeneste p√• Dapla Lab som lar brukerne kode i Jupyterlab. Tjenesten kommer med R og Python og noen vanlige Jupyterlab-extensions ferdig installert. M√•lgruppen for tjenesten er brukere som skal skrive produksjonskode i Jupyterlab.\nSiden tjenesten er ment for produksjonskode s√• er det veldig f√• R- og Python-pakker som er forh√•ndsinstallert. Antagelsen er at brukeren/teamet heller b√∏r installere de pakkene de selv trenger, framfor at det ligger ferdiginstallerte pakker som skal dekke behovet til alle brukere/team i SSB. Det reduserer kompleksitet i tjenesten og dermed sannsynligheten for feilsituasjoner.\nFor uerfarne brukere finnes det en egen tjeneste som heter Jupyter-playground. Her er mange av de vanlige R- og Python-pakkene installert og det er opprettet en ferdig kernel som lar brukerne komme i gang fort med koding. Denne tjenesten er ikke tenkt for bruk i produksjonskode.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#forberedelser",
    "href": "statistikkere/jupyter.html#forberedelser",
    "title": "Jupyter",
    "section": "Forberedelser",
    "text": "Forberedelser\nF√∏r man starter Jupyter-tjenesten b√∏r man ha lest kapitlet om Dapla Lab og satt opp Git- og GitHub-konfigurasjonen under Min konto. Deretter gj√∏r du f√∏lgende:\n\nLogg deg inn p√• Dapla Lab\nUnder Tjenestekatalog trykker du p√• Start-knappen for Jupyter\nGi tjenesten et navn\n√Öpne Jupyter konfigurasjoner",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#konfigurasjon",
    "href": "statistikkere/jupyter.html#konfigurasjon",
    "title": "Jupyter",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nF√∏r man √•pner en tjeneste kan man konfigurere hvor mye ressurser man √∏nsker, hvilket team man skal representere, om et GitHub-repo skal klones ved oppstart, og mange andre ting. Valgene man gj√∏r kan ogs√• lagres slik at man √• slipper √• gj√∏re samme jobb senere. Figur¬†1 viser Tjeneste-delen i konfigurasjonen for Jupyter hvor man kan velge hvilken versjon av Jupyter man vil bruke.\n\n\n\n\n\n\nFigur¬†1: Jupyter-versjon i Dapla Lab\n\n\n\n\nData\nUnder Data-menyen kan man velge hvilket team og tilgangsgruppe man skal representere, som igjen bestemmer hvilke data man f√•r tilgang til. Man gj√∏r dette ved √• velge navnet p√• tilgangsgruppen, og denne er alltid p√• formen &lt;teamnavn&gt;-&lt;tilgangsgruppe&gt;. Figur¬†2 viser at brukeren har valgt tilgangsgruppen dapla-felles-developers, dvs. at de representerer tilgangsgruppen developers for teamet dapla-felles.\n\n\n\n\n\n\nFigur¬†2: Detaljert tjenestekonfigurasjon for b√∏ttetilgang i Dapla Lab\n\n\n\nUnder Team og tilgangsgruppe kan brukeren ogs√• velge √• representere tilgangsgruppen data-admins for et team. I de tilfellene er det et krav om brukeren oppgir en skriftlig begrunnelse for hvorfor tilgangen er n√∏dvendig. I tillegg m√• kan de maksimalt aktivere tilgangen i 8 timer.\nFigur¬†3 viser en bruker som aktiverer sin data-admins tilgang for team dapla-felles. Hvis brukeren ikke oppgir en begrunnelse vil de f√• en feilmelding ved oppstart av tjenesten.\n\n\n\n\n\n\nFigur¬†3: Aktivere tilgang til kildedata for data-admins.\n\n\n\nN√•r man √•pner tjeneste, og representerer et team, s√• tilgjengeliggj√∏res det teamets b√∏tter inne i tjenesten under filstien /buckets/. Men et team kan ogs√• ha tilgang til andre sine delt-b√∏tter og √∏nske √• tilgjengliggj√∏re disse ogs√•. Figur¬†4 viser hvordan man spesifiserer hvilke delt-b√∏tter man √∏nsker √• tilgjengeliggj√∏re inne i tjenesten. Man gj√∏r det ved √• spesifisere det tekniske teamnavnet til teamet som eier dataene, og spesifiserer kortnavnet for delt-b√∏tta1.\n\n\n\n\n\n\nFigur¬†4: Konfigurer hvilke delt-b√∏tter fra andre som skal tilgjengeliggj√∏res i tjenesten.\n\n\n\n\n\n\n\n\n\nHvor finner jeg filene inne i tjenesten?\n\n\n\n\n\nB√∏tter som tilgjengeliggj√∏res inne tjenesten finner du alltid under filstien /buckets/ i tjenesten. Under er et eksempel som vil v√¶re vanlig for et statistikkteam:\n\n\nFilsystem\n\n/buckets/\n‚îú‚îÄ‚îÄ produkt/\n‚îÇ   ‚îú‚îÄ‚îÄ inndata/\n‚îÇ   ‚îî‚îÄ‚îÄ klargjorte data/\n‚îú‚îÄ‚îÄ frasky/\n‚îú‚îÄ‚îÄ tilsky/\n‚îú‚îÄ‚îÄ delt-ledstill/\n‚îú‚îÄ‚îÄ delt-freg/\n‚îî‚îÄ‚îÄ shared/\n    ‚îú‚îÄ‚îÄ arbmark-register/\n    ‚îÇ   ‚îî‚îÄ‚îÄ ameld/\n    ‚îî‚îÄ‚îÄ vof/\n        ‚îî‚îÄ‚îÄ rollebasen/\n\nB√∏ttene som eies av teamet (produkt, frasky, tilsky, delt-ledstill og delt-freg) tilgjengeliggj√∏res rett under filstien /buckets/, mens andre team sine delt-b√∏tter tilgjengeliggj√∏res under /buckets/shared/&lt;teamnavn&gt;/&lt;kortnavn for b√∏tte&gt;. Teamets egne delt-b√∏tter f√•r et delt-prefiks slik at en delt-b√∏tte med kortnavn ledstill blir tilgjengeliggjort som delt-ledstill.\n\n\n\nMan kan ogs√• velge √• jobbe direkte mot b√∏ttene, og da trenger man ikke √• tilgjengeliggj√∏re b√∏ttene i filsystemet. Under tjenestekonfigurasjonen Avansert kan man skru av\n\n\nGit/GitHub\nUnder menyen Git/GitHub kan man konfigurere Git og GitHub slik at det blir lettere √• jobbe med inne i tjenesten. Som standard arves informasjonen som er lagret under Min konto-Git i Dapla Lab. Informasjonen under tjenestekonfigurasjonen blir tilgjengeliggjort som milj√∏variabler i tjenesten. Informasjonen blir ogs√• lagt i $HOME/.netrc slik at man kan benytte ikke trenger √• gj√∏re noe mer for √• jobbe mot GitHub fra tjenesten.\n\n\n\n\n\n\nFigur¬†5: Konfigurasjon av Git og GitHub for Jupyter-tjenesten i Dapla Lab\n\n\n\nFigur¬†5 viser at brukeren som standard f√•r aktivert Aktiver Git. Dette inneb√¶rer at Git-brukernavn, Git e-post og GitHub-token arves fra brukerkonfigurasjonen. I tillegg s√• opprettes SSBs standard Git-konfigurasjon i ~/.gitconfig.\n\n\nPython/R\nUnder menyen Python/R kan man velge hvilke versjon av R og Python man √∏nsker √• kj√∏re. Man kan velge mellom alle tidligere tilbudte kombinasjoner av R og Python.\nI Figur¬†6 ser vi av navnet r4.4.0-py311-v55-2024.10.31 at tjenesten som default vil startes versjon 4.4.0 av R og 3.11 for Python. Etterhvert som nye versjoner av R og Python kommer kan disse tilgjengeliggj√∏res i tjenesten, men brukeren kan velge √• starte en eldre versjon av tjenesten.\n\n\n\n\n\n\nFigur¬†6: Konfigurasjon av Git og GitHub for Jupyter-tjenesten i Dapla Lab\n\n\n\n\n\nRessurser\nUnder menyen Resources kan man velge hvor mye CPU og RAM man √∏nsker i tjenesten, slik som vist i Figur¬†7. Velg s√• lite som trengs for √• gj√∏re jobben du skal gj√∏re.\n\n\n\n\n\n\nFigur¬†7: Konfigurasjon av ressurser for Jupyter-tjenesten i Dapla Lab\n\n\n\n\n\nDiskplass\nSom default f√•r alle som starter en instans av Jupyter-tjenesten en lokal disk p√• 10GB inne i tjenesten. Under Diskplass-menyen kan man velge √• √∏ke st√∏rrelsen p√• disken eller ikke noe disk i det hele tatt. Siden lokal disk i tjenesten hovedsakelig skal benyttes til √• lagre en lokal kopi av koden som lagres p√• GitHub mens man gj√∏r endringer b√∏r ikke st√∏rrelsen p√• disken v√¶re stor. Figur¬†8 viser valgene som kan gj√∏res under Diskplass-fanen.\n\n\n\n\n\n\nFigur¬†8: Konfigurasjon av lokal disk for Jupyter-tjenesten i Dapla Lab\n\n\n\n\n\nAvansert\nUnder Avansert kan man velge √• ikke tilgjengeliggj√∏re b√∏tter som filsystem inne i tjenesten. Konsekvensen av dette er at man m√• lese og skrive filer ved √• referere til b√∏ttene direkte.\n\n\n\n\n\n\nFigur¬†9: Avansert konfigurasjon for Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#datatilgang",
    "href": "statistikkere/jupyter.html#datatilgang",
    "title": "Jupyter",
    "section": "Datatilgang",
    "text": "Datatilgang\nSlik kan man inspisere dataene fra en terminal inne i tjenesten:\n\n√Öpne en instans av Jupyter med data fra b√∏tter\n√Öpne en terminal inne i Jupyter\nG√• til mappen med b√∏ttene ved √• kj√∏re dette fra terminalen cd /buckets\nKj√∏r ls -ahl i teminalen for √• se p√• hvilke b√∏tter som er montert.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#installere-pakker",
    "href": "statistikkere/jupyter.html#installere-pakker",
    "title": "Jupyter",
    "section": "Installere pakker",
    "text": "Installere pakker\nSiden det nesten ikke er installert noen pakker i tjenesten s√• kan brukeren opprette et ssb-project og installere pakker som vanlig.\nFor √• bygge et eksisterende ssb-project s√• kan brukeren ogs√• bruke ssb-project.\nFor √• installere R-pakker f√∏lger man beskrivelsen for renv.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#slette-tjenesten",
    "href": "statistikkere/jupyter.html#slette-tjenesten",
    "title": "Jupyter",
    "section": "Slette tjenesten",
    "text": "Slette tjenesten\nFor √• slette tjenesten kan man trykke p√• Slette-knappen i Dapla Lab under Mine tjenester. N√•r man sletter en tjeneste s√• sletter man hele disken inne i tjenesten og frigj√∏r alle ressurser som er reservert. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#pause-tjenesten",
    "href": "statistikkere/jupyter.html#pause-tjenesten",
    "title": "Jupyter",
    "section": "Pause tjenesten",
    "text": "Pause tjenesten\nMan kan pause tjenesten ved √• trykke p√• Pause-knappen i Dapla Lab under Mine tjenester. N√•r man pauser s√• slettes alt p√•den lokale disken som ikke er lagret under $HOME/work. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#monitorering",
    "href": "statistikkere/jupyter.html#monitorering",
    "title": "Jupyter",
    "section": "Monitorering",
    "text": "Monitorering\n\n\nMan kan moniterere en instans av Jupyter ved √• trykke p√• Jupyter-teksten under Mine tjenester i Dapla Lab, slik som vist i Figur¬†10.\nDenne funksjonaliteten er under arbeid og mer informasjon kommer snart.\n\n\n\n\n\n\n\n\n\nFigur¬†10: Monitorering av Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#footnotes",
    "href": "statistikkere/jupyter.html#footnotes",
    "title": "Jupyter",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nKortnavnet til en delt-b√∏tte kan leses ut av Dapla Ctrl eller hentes fra selve b√∏ttenavnet. F.eks. er det kortnavnet til delt-b√∏tta ssb-vof-data-delt-rollebase-fnr-prod bare rollebase-fnr. Det tekniske teamnavnet er vof.‚Ü©Ô∏é",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-botter.html",
    "href": "statistikkere/hva-er-botter.html",
    "title": "Hva er b√∏tter?",
    "section": "",
    "text": "P√• Dapla er det Google Cloud Storage (GCS) som benyttes til √• lagre data og filer. F√∏lgelig er det GCS som erstatter det vi kjente som Linux-stammene i prodsonen tidligere. I SSB har vi v√¶rt vant til √• jobbe med data lagret p√• filsystemer i et Linux-milj√∏1. GCS-b√∏ttene skiller seg fra klassiske filsystemer p√• flere m√•ter, og det er viktig √• v√¶re klar over disse forskjellene. I denne kapitlet vil vi g√• gjennom noen av de viktigste forskjellene og hvordan man gj√∏r vanlige operasjoner mot b√∏tter i GCS.\n\n\nI et Linux- eller Windows-filsystem, som vi har v√¶rt vant til tidligere, s√• er filer og mapper organisert i en hierarkisk struktur p√• et operativsystem (OS). I SSB har OS-ene v√¶rt installert p√• fysiske maskiner som vi vedlikeholder selv.\nEn b√∏tte i GCS er derimot en kj√∏pt tjeneste som lar brukeren lagre alle typer objekter i en container. Man trenger alts√• ikke √• tenke p√• om filene ligger i et hierarki, hvilket operativsystem det kj√∏rer p√•, eller hvor mye diskplass som er tilgjengelig.\nAt data er lagret som objekter i en b√∏tte har noen praktiske implikasjoner for hvordan vi jobber med data i SSB:\n\nMange er vant til √• jobbe direkte med filer i en Linux-terminal eller via systemkall fra spr√•k som SAS, Pyton eller R. For √• gj√∏re det samme i Jupyter mot en b√∏tte, s√• kan vi bruke Python-pakken gcsfs. Se eksempler under.\nN√•r vi bruker Python- eller R-pakker for lese eller skrive data fra b√∏tter, s√• er vi avhengig av at pakkene tilbyr integrasjon mot b√∏tter. Mange pakker gj√∏r det, men ikke alle. For de som ikke gj√∏r det kan vi bruke ofte bruke gcsfs til √• gi oss et python-objekt, som igjen kan brukes av de fleste pakker.\nDet finnes egentlig ikke noen mapper i b√∏tter. I motsetning til et vanlig filsystem s√• er det ikke en hierarkisk mappestruktur i en b√∏tte. Det vil si at alt ligger et sted, men de kan ha lange navn med /-tegn slik at det ligner p√• et klassisk filsystem. Bruker du / i objekt-navnet s√• vil ogs√• Google Cloud Console vise det som mapper, men det er bare for √• gj√∏re det enklere √• forholde seg til. En praktisk konsekvens av dette er f.eks. at man ikke trenger √• opprette en mappe f√∏r man legger et fil/objekt i den. Det er bare en tekststreng som er en del av objekt-navnet.\n\nUnder kommer det en del eksempler p√• hvordan man kan jobbe med objekter i b√∏tter p√• samme m√•te som filer i et filsystem.\n\n\n\nP√• Dapla skal data lagres i b√∏tter. Men n√•r du √•pner Jupyterlab s√• f√•r du ogs√• et ‚Äúlokalt‚Äù eller klassisk filsystem, slik vi definerte det i forrige kapittel. Det er dette filsystemet du ser i filutforskeren i Jupyterlab, slik som vist til venstre i Figur¬†1. Det er ogs√• dette filsystemet du ser n√•r du f.eks. bruker ls-kommandoen i en terminal i Jupyterlab.\n\n\n\n\n\n\nFigur¬†1: Til venstre i bildet ser du filutforskeren i Jupyterlab\n\n\n\nDette filsystemet er ment for √• lagre kode midlertidig mens du jobber med dem. Det er ikke ment for √• lagre data. Det er heller ikke ment som et permanent lagringssted for kode. Permanent lagring av kode skal gj√∏res p√• GitHub. Selv om filene du lagrer der fortsetter √• eksistere for hver gang du logger deg inn i Jupyterlab, s√• b√∏r kode du √∏nsker √• bevare pushes til GitHub f√∏r du avslutter en sesjon i Jupyterlab. Se en mer teknisk beskrivelse av hvordan dette fungerer i boksen under.\n\n\n\n\n\n\nHvem er egentlig denne Jovyan?\n\n\n\nN√•r du logger deg inn i Jupyterlab p√• Dapla, s√• ser du at brukeren din p√• det lokale filsystemet heter jovyan. Grunnen er at det er en Linux-container som kj√∏rer et Jupyterlab Docker-image, og alle bruker denne containeren. Det er derfor vi kaller det et ‚Äúlokalt‚Äù filsystem, fordi det er lokalt i containeren.\nI tillegg er det satt opp persistent volum (PV) og en persistent volume claim (PVC) som er knyttet til denne containeren. Det er dette som gj√∏r at filene du lagrer i Jupyterlab blir lagret mellom hver gang du logger deg inn. I tillegg tas det en backup av filsystemene hver dag, slik at vi kan gjenopprette filene dine hvis noe skulle skje med PV-en.\nLagringsplassen p√• PV-en er begrenset til 10 GB per bruker. Grunnen til at vi ikke tilbyr mer lagringsplass er fordi det kun er kode skal lagres her, og at denne lagringsplassen er relativt dyrt sammenlignet med GCS-b√∏tter. Hvis du jobber i virtuelle milj√∏er og lagrer mange milj√∏er lokalt, kan du oppleve at disken blir full. Les mer her om hvordan man b√∏r h√•ndere dette.\n\n\n\n\n\nTidligere har vi diskutert forskjellene mellom b√∏tter og filsystemer. Mange kjenner hvordan man gj√∏r systemkommandoer2 i klassiske filsystemer fra en terminal eller fra spr√•k som Python, R og SAS. I dette kapitlet viser vi hvordan dette kan gj√∏res mot b√∏tter fra Jupyterlab.\ngcsfs er en Python-pakke som lar deg jobbe med objekter i b√∏tter p√• nesten samme m√•te som filer i et filsystem. For √• kunne gj√∏re det m√• vi f√∏rst sette opp en filsystem-instans som lar oss bruke en b√∏tte som et filsystem. Pakken dapla-toolbelt lar oss gj√∏re det ganske enkelt:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs er n√• et filsystem-versjon av b√∏ttene vi har tilgang til p√• GCS. Vi kan n√• bruk fs til √• gj√∏re typiske operasjoner vi har v√¶rt vant til √• gj√∏re i en Linux-terminal. Se en fullstendig liste over kommandoer her.\nUnder vises noen flere eksempler p√• nyttige funksjoner enn det som ble vist i kapitlet tidligere i boken\n\n\n\n\n\n\nWarning\n\n\n\nSelv om ordet mapper blir brukt i eksemplene under, s√• er det viktig √• huske at det ikke finnes noen mapper i b√∏tter. Det er bare en tekststreng som er en del av objekt-navnet. Men siden vi her jobber med et filsystem-representasjon av b√∏tten, s√• tillater vi oss √• gj√∏re det for √• gj√∏re det enklere √• lese.\n\n\n\n\nfs.glob() lar oss s√∏ke etter filer i b√∏tten. Vi kan bruke *, **, ? og [..] som wildcard for √• finne det vi trenger. Det som returneres er enten en liste eller dictionary avhengig av hva vi velger √• gj√∏re.\nHent en liste over alle filer i en undermappe R_smoke_test i b√∏tta gs://ssb-dapla-felles-data-delt-prod:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/R_smoke_test/*\")\n\nN√•r vi legger til * p√• slutten av filstien s√• returnerer den alle filer i den eksakte undermappen. Men hvis vi √∏nsker √• √• f√• alle filer i alle undermapper, s√• kan vi bruke ** p√• denne m√•ten:\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/R_smoke_test/**\").\nVi kan ogs√• s√∏ke mer avansert ved ved √• bruke ?. ?-tegnet sier at en enkeltkarakter kan v√¶re hva som helst. F.eks. kunne vi skrevet\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/R_smoke_test/**/R_smoke_test_??.csv\")\nfor √• rekursivt liste ut alle filer som starter med R_smoke_test_, etterfulgt av to karakterer av hvilken som helst type, og slutter med .csv.\nHvis vi viste at de to karakterene ikke skulle v√¶re av hvilken som helst karakterer, men det var en liten bokstav, etterfulgt av et tall mellom 2-6, s√• kunne vi brukt [a-z] og [2-6] for √• spesifisere dette. F.eks. kunne vi skrevet:\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/R_smoke_test/**/R_smoke_test_[a-z][2-6].csv\")\nSom vi ser er fs.glob() et verkt√∏y som git oss mye den funksjonaliteten vi er vant til fra Linux-terminalen. Men i tillegg kan vi bruke den til √• hente inn metadataene til de filene/objektene vi f√•r treff p√•, ved √• bruke argumentet detail=True. Her er et eksempel:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\n    \"gs://ssb-dapla-felles-data-produkt-prod/R_smoke_test/**/R_smoke_test_/[a-z][2-6].csv\",\n    detail=True,\n)\n\nMetadataene gir deg da informasjon om filst√∏rrelse, tidspunkt for opprettelse, tidspunkt for siste endring, og hvem som har opprettet og endret filen.\n\n\n\nI noen tilfeller kan det v√¶re nyttig √• sjekke om en fil eksisterer i b√∏tten. Det kan vi gj√∏re med fs.exists():\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.exists(\"gs://ssb-dapla-felles-data-produkt-prod/altinn3/form_dc551844cd74.xml\")\n\nKoden returnerer enten True eller False avhengig av om filen eksisterer eller ikke.\n\n\n\nHvis du lurer p√• hvor mange GB data du har i en b√∏tte, s√• kan du bruke fs.du():\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss p√• b√∏ttene\nfs = FileClient.get_gcs_file_system()\n\ntotal_bytes = fs.du(\n    \"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/\",\n)\n\ntotal_size_gb = total_bytes / (1024**3)\nprint(f\"Total size: {total_size_gb:.3f} GB\")\n\n\n\n\nHvert objekt i b√∏tta har metadata knyttet til seg som kan hentes inn i Jupyter og benyttes.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss p√• b√∏ttene\nfs = FileClient.get_gcs_file_system()\n\nfile = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/form_dc551844cd74.xml\"\n\nfs.info(file)\n\nDette gir deg blant annet informasjon om filst√∏rrelse, tidspunkt for opprettelse, og tidspunkt for siste endring. Hvis du √∏nsker dette for flere filer s√• kan man ogs√• bruke fs.glob(&lt;pattern&gt;, details=True) som vi s√• p√• tidligere.\n\n\n\nfs.ls() brukes for √• gi en liste av filer i et omr√•de. Det kan brukes for b√•de b√∏tter og mapper.\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.ls(\"gs://ssb-dapla-felles-data-produkt-prod/altinn3\")\n\nKoden returnerer en liste.\n\n\n\nfs.open() lar oss √•pne en fil i b√∏tta for lesing og skriving med vanlige Python-funksjoner. Funksjonen returnerer et fil-aktig objekt som kan brukes med vanlige som Python-biblioteker som Pandas og NumPy.\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import FileClient\n\n# Kobler oss p√• b√∏ttene\nfs = FileClient.get_gcs_file_system()\n\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-metrics/number-of-teams.csv\"\n\n# Bruker fs.open() til √• √•pne fil, og deretter leses den med Pandas\nwith fs.open(file_path, \"r\") as file:\n    df = pd.read_csv(file)\ndf\n\nDu kan ogs√• bruke fs.open() til √• skrive til en fil i b√∏tta. Her er et eksempel p√• hvordan man skriver en parquet-fil med Pandas og PyArrow:\n\n\nnotebook\n\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom dapla import FileClient\n\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-metrics/test.parquet\"\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\n# Lager eksempeldata\ndata = {\"A\": [1, 2, 3], \"B\": [4, 5, 6]}\ndf = pd.DataFrame(data)\n\n# Skriver parquet-fil\nwith fs.open(file_path, \"wb\") as file:\n    pq.write_table(pa.Table.from_pandas(df), file)\n\nOver brukte vi wb for √• √•pne den bin√¶re filen for skriving. Hvis du √∏nsker √• lese fra en bin√¶r fil s√• bruker du rb. Skulle du jobbet en ren tekstfil, s√• hadde man brukt w til √• skrive og r til √• lese.\n\n\n\nfs.touch() lar deg opprette en tom fil i b√∏tta, eller oppdatere metadataene til objektet for n√•r den sist ble modifisert.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nfs.touch(\"gs://ssb-dapla-felles-data-produkt-prod/dapla-metrics/test.parquet\")\n\n\n\n\nfs.put() lar deg laste opp en fil fra Jupyter-filsystemet3 til b√∏tta. Husk at filstien til ditt hjemmeomr√•de p√• Jupyter er /home/jovyan/. Her er et eksempel p√• hvordan man kan bruke det p√• enkelt-filer:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"/home/jovyan/custom.scss\"\ndestination = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/test2.csv\"\n\nfs.put(source, destination)\n\nDu kan ogs√• kopiere hele mapper mellom jovyan og b√∏ttene:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"/home/jovyan/sesongjustering/\"\ndestination = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/sesongjustering/\"\n\nfs.put(source, destination, recursive=True)\n\nEt brukstilfellet for √• kopiere mellom b√∏tter og jovyan er ved sesongjustering med Jdemetra+ og JWSACruncher. Siden Jdemetra+ ikke kan lese/skrive fra b√∏tter, s√• m√• vi midlertidig kopiere dataene til jovyan med fs.put() f√∏r vi kan kj√∏re sesongjusteringen. N√•r vi er ferdige med kj√∏ringen kopierer vi dataene tilbake til b√∏tta med fs.get().\n\n\n\nfs.get() gj√∏r det samme som fs.put(), bare motsatt vei. Den kopierer fra en b√∏tte til jovyan.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/sesongjustering/\"\ndestination = \"/home/jovyan/sesongjustering/\"\n\nplaceholder = fs.get(source, destination, recursive=True)\n\nfs.get() returnerer potensielt mye output. Hvis du ikke vil ha det, s√• kan du bare definere det som et objekt. F.eks. placeholder som i eksempelet over.\n\n\n\nfs.mv() lar deg flytte filer og mapper mellom b√∏tter, samt √• gi objekter nye navn.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss p√• b√∏ttene\nfs = FileClient.get_gcs_file_system()\n\nsource = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-metrics/number-of-teams.csv\"\ndestination = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/number-of-teams.csv\"\nfs.mv(source, destination)\n\n\n\n\nfs.copy() lar deg kopiere filer og mapper mellom b√∏tter. I eksempelet under kopierer vi rekursivt:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfrom_path = \"gs://ssb-dapla-felles-data-produkt-prod/ledstill/altinn/2022/11/21/\"\nto_path = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/\"\nfs.copy(from_path, to_path, recursive=True)\n\n\n\n\nfs.rm() lar deg slette filer og mapper i b√∏tta. Her sletter vi en enkeltfil:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfs.rm(\"gs://ssb-dapla-felles-data-produkt-prod/altinn3/number-of-teams.csv\")\n\nOgs√• denne funksjonen tar et recursive-argument hvis du √∏nsker √• slette en hel mappe.",
    "crumbs": [
      "Manual",
      "Data",
      "Hva er b√∏tter?"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-botter.html#b√∏tter-vs-filsystemer",
    "href": "statistikkere/hva-er-botter.html#b√∏tter-vs-filsystemer",
    "title": "Hva er b√∏tter?",
    "section": "",
    "text": "I et Linux- eller Windows-filsystem, som vi har v√¶rt vant til tidligere, s√• er filer og mapper organisert i en hierarkisk struktur p√• et operativsystem (OS). I SSB har OS-ene v√¶rt installert p√• fysiske maskiner som vi vedlikeholder selv.\nEn b√∏tte i GCS er derimot en kj√∏pt tjeneste som lar brukeren lagre alle typer objekter i en container. Man trenger alts√• ikke √• tenke p√• om filene ligger i et hierarki, hvilket operativsystem det kj√∏rer p√•, eller hvor mye diskplass som er tilgjengelig.\nAt data er lagret som objekter i en b√∏tte har noen praktiske implikasjoner for hvordan vi jobber med data i SSB:\n\nMange er vant til √• jobbe direkte med filer i en Linux-terminal eller via systemkall fra spr√•k som SAS, Pyton eller R. For √• gj√∏re det samme i Jupyter mot en b√∏tte, s√• kan vi bruke Python-pakken gcsfs. Se eksempler under.\nN√•r vi bruker Python- eller R-pakker for lese eller skrive data fra b√∏tter, s√• er vi avhengig av at pakkene tilbyr integrasjon mot b√∏tter. Mange pakker gj√∏r det, men ikke alle. For de som ikke gj√∏r det kan vi bruke ofte bruke gcsfs til √• gi oss et python-objekt, som igjen kan brukes av de fleste pakker.\nDet finnes egentlig ikke noen mapper i b√∏tter. I motsetning til et vanlig filsystem s√• er det ikke en hierarkisk mappestruktur i en b√∏tte. Det vil si at alt ligger et sted, men de kan ha lange navn med /-tegn slik at det ligner p√• et klassisk filsystem. Bruker du / i objekt-navnet s√• vil ogs√• Google Cloud Console vise det som mapper, men det er bare for √• gj√∏re det enklere √• forholde seg til. En praktisk konsekvens av dette er f.eks. at man ikke trenger √• opprette en mappe f√∏r man legger et fil/objekt i den. Det er bare en tekststreng som er en del av objekt-navnet.\n\nUnder kommer det en del eksempler p√• hvordan man kan jobbe med objekter i b√∏tter p√• samme m√•te som filer i et filsystem.",
    "crumbs": [
      "Manual",
      "Data",
      "Hva er b√∏tter?"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-botter.html#lokalt-filsystem-p√•-dapla",
    "href": "statistikkere/hva-er-botter.html#lokalt-filsystem-p√•-dapla",
    "title": "Hva er b√∏tter?",
    "section": "",
    "text": "P√• Dapla skal data lagres i b√∏tter. Men n√•r du √•pner Jupyterlab s√• f√•r du ogs√• et ‚Äúlokalt‚Äù eller klassisk filsystem, slik vi definerte det i forrige kapittel. Det er dette filsystemet du ser i filutforskeren i Jupyterlab, slik som vist til venstre i Figur¬†1. Det er ogs√• dette filsystemet du ser n√•r du f.eks. bruker ls-kommandoen i en terminal i Jupyterlab.\n\n\n\n\n\n\nFigur¬†1: Til venstre i bildet ser du filutforskeren i Jupyterlab\n\n\n\nDette filsystemet er ment for √• lagre kode midlertidig mens du jobber med dem. Det er ikke ment for √• lagre data. Det er heller ikke ment som et permanent lagringssted for kode. Permanent lagring av kode skal gj√∏res p√• GitHub. Selv om filene du lagrer der fortsetter √• eksistere for hver gang du logger deg inn i Jupyterlab, s√• b√∏r kode du √∏nsker √• bevare pushes til GitHub f√∏r du avslutter en sesjon i Jupyterlab. Se en mer teknisk beskrivelse av hvordan dette fungerer i boksen under.\n\n\n\n\n\n\nHvem er egentlig denne Jovyan?\n\n\n\nN√•r du logger deg inn i Jupyterlab p√• Dapla, s√• ser du at brukeren din p√• det lokale filsystemet heter jovyan. Grunnen er at det er en Linux-container som kj√∏rer et Jupyterlab Docker-image, og alle bruker denne containeren. Det er derfor vi kaller det et ‚Äúlokalt‚Äù filsystem, fordi det er lokalt i containeren.\nI tillegg er det satt opp persistent volum (PV) og en persistent volume claim (PVC) som er knyttet til denne containeren. Det er dette som gj√∏r at filene du lagrer i Jupyterlab blir lagret mellom hver gang du logger deg inn. I tillegg tas det en backup av filsystemene hver dag, slik at vi kan gjenopprette filene dine hvis noe skulle skje med PV-en.\nLagringsplassen p√• PV-en er begrenset til 10 GB per bruker. Grunnen til at vi ikke tilbyr mer lagringsplass er fordi det kun er kode skal lagres her, og at denne lagringsplassen er relativt dyrt sammenlignet med GCS-b√∏tter. Hvis du jobber i virtuelle milj√∏er og lagrer mange milj√∏er lokalt, kan du oppleve at disken blir full. Les mer her om hvordan man b√∏r h√•ndere dette.",
    "crumbs": [
      "Manual",
      "Data",
      "Hva er b√∏tter?"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-botter.html#systemkommandoer-mot-b√∏ttter",
    "href": "statistikkere/hva-er-botter.html#systemkommandoer-mot-b√∏ttter",
    "title": "Hva er b√∏tter?",
    "section": "",
    "text": "Tidligere har vi diskutert forskjellene mellom b√∏tter og filsystemer. Mange kjenner hvordan man gj√∏r systemkommandoer2 i klassiske filsystemer fra en terminal eller fra spr√•k som Python, R og SAS. I dette kapitlet viser vi hvordan dette kan gj√∏res mot b√∏tter fra Jupyterlab.\ngcsfs er en Python-pakke som lar deg jobbe med objekter i b√∏tter p√• nesten samme m√•te som filer i et filsystem. For √• kunne gj√∏re det m√• vi f√∏rst sette opp en filsystem-instans som lar oss bruke en b√∏tte som et filsystem. Pakken dapla-toolbelt lar oss gj√∏re det ganske enkelt:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs er n√• et filsystem-versjon av b√∏ttene vi har tilgang til p√• GCS. Vi kan n√• bruk fs til √• gj√∏re typiske operasjoner vi har v√¶rt vant til √• gj√∏re i en Linux-terminal. Se en fullstendig liste over kommandoer her.\nUnder vises noen flere eksempler p√• nyttige funksjoner enn det som ble vist i kapitlet tidligere i boken\n\n\n\n\n\n\nWarning\n\n\n\nSelv om ordet mapper blir brukt i eksemplene under, s√• er det viktig √• huske at det ikke finnes noen mapper i b√∏tter. Det er bare en tekststreng som er en del av objekt-navnet. Men siden vi her jobber med et filsystem-representasjon av b√∏tten, s√• tillater vi oss √• gj√∏re det for √• gj√∏re det enklere √• lese.\n\n\n\n\nfs.glob() lar oss s√∏ke etter filer i b√∏tten. Vi kan bruke *, **, ? og [..] som wildcard for √• finne det vi trenger. Det som returneres er enten en liste eller dictionary avhengig av hva vi velger √• gj√∏re.\nHent en liste over alle filer i en undermappe R_smoke_test i b√∏tta gs://ssb-dapla-felles-data-delt-prod:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/R_smoke_test/*\")\n\nN√•r vi legger til * p√• slutten av filstien s√• returnerer den alle filer i den eksakte undermappen. Men hvis vi √∏nsker √• √• f√• alle filer i alle undermapper, s√• kan vi bruke ** p√• denne m√•ten:\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/R_smoke_test/**\").\nVi kan ogs√• s√∏ke mer avansert ved ved √• bruke ?. ?-tegnet sier at en enkeltkarakter kan v√¶re hva som helst. F.eks. kunne vi skrevet\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/R_smoke_test/**/R_smoke_test_??.csv\")\nfor √• rekursivt liste ut alle filer som starter med R_smoke_test_, etterfulgt av to karakterer av hvilken som helst type, og slutter med .csv.\nHvis vi viste at de to karakterene ikke skulle v√¶re av hvilken som helst karakterer, men det var en liten bokstav, etterfulgt av et tall mellom 2-6, s√• kunne vi brukt [a-z] og [2-6] for √• spesifisere dette. F.eks. kunne vi skrevet:\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/R_smoke_test/**/R_smoke_test_[a-z][2-6].csv\")\nSom vi ser er fs.glob() et verkt√∏y som git oss mye den funksjonaliteten vi er vant til fra Linux-terminalen. Men i tillegg kan vi bruke den til √• hente inn metadataene til de filene/objektene vi f√•r treff p√•, ved √• bruke argumentet detail=True. Her er et eksempel:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\n    \"gs://ssb-dapla-felles-data-produkt-prod/R_smoke_test/**/R_smoke_test_/[a-z][2-6].csv\",\n    detail=True,\n)\n\nMetadataene gir deg da informasjon om filst√∏rrelse, tidspunkt for opprettelse, tidspunkt for siste endring, og hvem som har opprettet og endret filen.\n\n\n\nI noen tilfeller kan det v√¶re nyttig √• sjekke om en fil eksisterer i b√∏tten. Det kan vi gj√∏re med fs.exists():\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.exists(\"gs://ssb-dapla-felles-data-produkt-prod/altinn3/form_dc551844cd74.xml\")\n\nKoden returnerer enten True eller False avhengig av om filen eksisterer eller ikke.\n\n\n\nHvis du lurer p√• hvor mange GB data du har i en b√∏tte, s√• kan du bruke fs.du():\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss p√• b√∏ttene\nfs = FileClient.get_gcs_file_system()\n\ntotal_bytes = fs.du(\n    \"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/\",\n)\n\ntotal_size_gb = total_bytes / (1024**3)\nprint(f\"Total size: {total_size_gb:.3f} GB\")\n\n\n\n\nHvert objekt i b√∏tta har metadata knyttet til seg som kan hentes inn i Jupyter og benyttes.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss p√• b√∏ttene\nfs = FileClient.get_gcs_file_system()\n\nfile = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/form_dc551844cd74.xml\"\n\nfs.info(file)\n\nDette gir deg blant annet informasjon om filst√∏rrelse, tidspunkt for opprettelse, og tidspunkt for siste endring. Hvis du √∏nsker dette for flere filer s√• kan man ogs√• bruke fs.glob(&lt;pattern&gt;, details=True) som vi s√• p√• tidligere.\n\n\n\nfs.ls() brukes for √• gi en liste av filer i et omr√•de. Det kan brukes for b√•de b√∏tter og mapper.\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.ls(\"gs://ssb-dapla-felles-data-produkt-prod/altinn3\")\n\nKoden returnerer en liste.\n\n\n\nfs.open() lar oss √•pne en fil i b√∏tta for lesing og skriving med vanlige Python-funksjoner. Funksjonen returnerer et fil-aktig objekt som kan brukes med vanlige som Python-biblioteker som Pandas og NumPy.\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import FileClient\n\n# Kobler oss p√• b√∏ttene\nfs = FileClient.get_gcs_file_system()\n\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-metrics/number-of-teams.csv\"\n\n# Bruker fs.open() til √• √•pne fil, og deretter leses den med Pandas\nwith fs.open(file_path, \"r\") as file:\n    df = pd.read_csv(file)\ndf\n\nDu kan ogs√• bruke fs.open() til √• skrive til en fil i b√∏tta. Her er et eksempel p√• hvordan man skriver en parquet-fil med Pandas og PyArrow:\n\n\nnotebook\n\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom dapla import FileClient\n\nfile_path = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-metrics/test.parquet\"\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\n# Lager eksempeldata\ndata = {\"A\": [1, 2, 3], \"B\": [4, 5, 6]}\ndf = pd.DataFrame(data)\n\n# Skriver parquet-fil\nwith fs.open(file_path, \"wb\") as file:\n    pq.write_table(pa.Table.from_pandas(df), file)\n\nOver brukte vi wb for √• √•pne den bin√¶re filen for skriving. Hvis du √∏nsker √• lese fra en bin√¶r fil s√• bruker du rb. Skulle du jobbet en ren tekstfil, s√• hadde man brukt w til √• skrive og r til √• lese.\n\n\n\nfs.touch() lar deg opprette en tom fil i b√∏tta, eller oppdatere metadataene til objektet for n√•r den sist ble modifisert.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nfs.touch(\"gs://ssb-dapla-felles-data-produkt-prod/dapla-metrics/test.parquet\")\n\n\n\n\nfs.put() lar deg laste opp en fil fra Jupyter-filsystemet3 til b√∏tta. Husk at filstien til ditt hjemmeomr√•de p√• Jupyter er /home/jovyan/. Her er et eksempel p√• hvordan man kan bruke det p√• enkelt-filer:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"/home/jovyan/custom.scss\"\ndestination = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/test2.csv\"\n\nfs.put(source, destination)\n\nDu kan ogs√• kopiere hele mapper mellom jovyan og b√∏ttene:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"/home/jovyan/sesongjustering/\"\ndestination = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/sesongjustering/\"\n\nfs.put(source, destination, recursive=True)\n\nEt brukstilfellet for √• kopiere mellom b√∏tter og jovyan er ved sesongjustering med Jdemetra+ og JWSACruncher. Siden Jdemetra+ ikke kan lese/skrive fra b√∏tter, s√• m√• vi midlertidig kopiere dataene til jovyan med fs.put() f√∏r vi kan kj√∏re sesongjusteringen. N√•r vi er ferdige med kj√∏ringen kopierer vi dataene tilbake til b√∏tta med fs.get().\n\n\n\nfs.get() gj√∏r det samme som fs.put(), bare motsatt vei. Den kopierer fra en b√∏tte til jovyan.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/sesongjustering/\"\ndestination = \"/home/jovyan/sesongjustering/\"\n\nplaceholder = fs.get(source, destination, recursive=True)\n\nfs.get() returnerer potensielt mye output. Hvis du ikke vil ha det, s√• kan du bare definere det som et objekt. F.eks. placeholder som i eksempelet over.\n\n\n\nfs.mv() lar deg flytte filer og mapper mellom b√∏tter, samt √• gi objekter nye navn.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss p√• b√∏ttene\nfs = FileClient.get_gcs_file_system()\n\nsource = \"gs://ssb-dapla-felles-data-produkt-prod/dapla-metrics/number-of-teams.csv\"\ndestination = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/number-of-teams.csv\"\nfs.mv(source, destination)\n\n\n\n\nfs.copy() lar deg kopiere filer og mapper mellom b√∏tter. I eksempelet under kopierer vi rekursivt:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfrom_path = \"gs://ssb-dapla-felles-data-produkt-prod/ledstill/altinn/2022/11/21/\"\nto_path = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/\"\nfs.copy(from_path, to_path, recursive=True)\n\n\n\n\nfs.rm() lar deg slette filer og mapper i b√∏tta. Her sletter vi en enkeltfil:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfs.rm(\"gs://ssb-dapla-felles-data-produkt-prod/altinn3/number-of-teams.csv\")\n\nOgs√• denne funksjonen tar et recursive-argument hvis du √∏nsker √• slette en hel mappe.",
    "crumbs": [
      "Manual",
      "Data",
      "Hva er b√∏tter?"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-botter.html#footnotes",
    "href": "statistikkere/hva-er-botter.html#footnotes",
    "title": "Hva er b√∏tter?",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nEgentlig har vi jobbet med data-filer p√• b√•de Linux- og Windows-filsystemer. Men Linux-stammene har v√¶rt det anbefalte stedet √• lagre datafiler.‚Ü©Ô∏é\nMed systemkommandoer s√• mener vi bash-kommandoer som ls og mv, eller implementasjoner av disse kommandoene i Python, R eller SAS.‚Ü©Ô∏é\nJupyter-milj√∏et har sitt eget filsystem, ofte kalt jovyan. Det er som et vanlig Linux-filsystem, og vil v√¶re det vi omtaler som ‚Äúlokalt‚Äù p√• maskinen din i Jupyter.‚Ü©Ô∏é",
    "crumbs": [
      "Manual",
      "Data",
      "Hva er b√∏tter?"
    ]
  },
  {
    "objectID": "statistikkere/tilgangsstyring.html",
    "href": "statistikkere/tilgangsstyring.html",
    "title": "Tilgangsstyring",
    "section": "",
    "text": "Hvert Dapla-team har sine egne lagringsomr√•der for data som ingen andre har tilgang til, med mindre teamet eksplisitt velger √• dele data med andre team. I tillegg har teamet tilgang til egne ressurser for √• behandle dataene.\nDet er tilgangsgruppen managers som bestemmer hvilke personer som skal ha hvilke roller i et team, og dermed hvilke data de ulike team-medlemmene f√•r tilgang til. Den som jobber med data kan bli plassert i tilgangsgruppene data-admins eller developers. Sistnevnte f√•r tilgang til alle datatilstander utenom kildedata, mens data-admins er forh√•ndsgodkjent til √• ogs√• √• aksessere kildedata ved behov. Dermed er data-admins en priveligert rolle p√• teamet som er forbeholdt noen f√• personer.\n\n\n\n\n\n\nFigur¬†1: Datatilstander som et team sitt medlemmer har ilgang til.\n\n\n\nFigur¬†1 viser hvem som har tilgang til hvilke datatilstander. Som nevnt er data-admins ansett som forh√•ndsgodkjent til √• aksessere kildedata ved behov. M√•ten dette er implementert p√• er at data-admins m√• aktivere denne tilgangen selv, ved √• bruke et JIT-grensesnitt (Just-In-Time Access). Tilgangen krever en begrunnelse og bruken kan l√∏pende monitoreres av managers for teamet.",
    "crumbs": [
      "Manual",
      "Dapla-team",
      "Tilgangsstyring"
    ]
  },
  {
    "objectID": "statistikkere/features.html",
    "href": "statistikkere/features.html",
    "title": "Features",
    "section": "",
    "text": "Under arbeid\n\n\n\nFeatures m√• forel√∏pig skrus p√• av plattformteamene. Ta kontakt med Kundeservice hvis du √∏nsker √• f√• en feature skrudd p√•.\n\n\nEn feature er en GCP-tjeneste som som er satt opp og konfigurert slik at Dapla-team kan ta det i bruk p√• en enkel og selvbetjent m√•te. N√•r man tar i bruk en feature kan man v√¶re sikker p√• at sikkerhet og beste-praksis i SSB er ivaretatt. Et viktig poeng med features er at teamene selv skal kunne skru av og p√• features etter behov.\nForel√∏pig er det tilgjengeliggjort f√∏lgende features p√• Dapla:\n\ndapla-buckets\ndapla-buckets er en feature som gir deg Google Cloud Storage b√∏ttene som statistikkteam skal bruke for √• lagre data i Dapla. Dvs. en b√∏tte for kildedata, en b√∏tte for produkt-data, og en b√∏tte for delt data.\nkildomaten kildomaten er en feature som gir deg tilgang til Kildomaten. Den lar deg automatisere prosessering av data fra kildedata til inndata ved hjelp av Cloud Run.\ntransfer-service\ntransfer-service er en feature som gir deg tilgang til √• overf√∏re data mellom lagringstjenester i Dapla. Den lar deg overf√∏re data mellom b√∏tter, og mellom bakke- og skyplattformen i SSB. Den er bygget p√• GCP-tjenesten Google Transfer Service.\nshared-buckets\nshared-buckets er en feature som lar teamet selv opprette delt-b√∏tter og styre tilganger til disse.\n\n\n\n\n\n\n\n\n\nSkru p√• en feature om gangen.\n\n\n\nHvis du √∏nsker √• skru p√• flere features samtidig, s√• m√• du gj√∏re det i flere PR-er. Atlantis vil ikke klare √• h√•ndtere flere features i samme PR. F√∏lg oppskriften under for hver feature du √∏nsker √• skru p√•.\n\n\nAktivering av en feature for Dapla-teamet ditt er veldig enkelt. Endringen er s√• liten at vi anbefaler √• gj√∏re endringen direkte i GitHubs grensesnitt, uten √• klone repoet f√∏rst. Slik g√•r du frem:\n\nS√∏k opp ditt teams sitt IaC-repoet under https://github.com/statisticsnorway/.\n\n\n\n\nI repoet √•pner du fila ./infra/projects.yaml. Se eksempel for teamet play-obr til h√∏yre.\nFinn ut om du √∏nsker √• skru p√• en feature i test eller prod. Hvis du √∏nsker √• gj√∏re det i prod, s√• skal du legge til en linje under features der env: prod. Hvis du √∏nsker √• gj√∏re det i test, s√• skal du legge til en linje under features der env: test.\nDu kan legge til linjen ved √• trykke p√• -ikonet √∏verst til h√∏yre i fila, endre teksten, og trykke p√• Commit changes. Velg deretter hvilket navn du √∏nsker p√• branchen, og trykk Create pull request.\nI PR-en venter du og ser at atlantis/plan1 har blitt kj√∏rt og f√•r en  til venstre for hver kj√∏ring, slik som vist i Figur¬†1.\n\n\n\n\n./infra/projects.yaml\n\nteam_uniform_name: play-obr\n\nprojects:\n  - project_name: play-obr\n    env: test\n    features:\n      - dapla-buckets\n      - kildomaten\n\n  - project_name: play-obr\n    env: prod\n    features:\n      - dapla-buckets\n\n\n\n\n\n\n\n\n\nFigur¬†1: Atlantis plan planlegger endringen og sjekker om den er klar til gjennomf√∏ring.\n\n\n\n\nHvis alt er i orden s√• ber du en kollega om √• se over endringen og godkjenne hvis alt ser riktig ut. N√•r den er godkjent vil du se et bilde som ligner det du ser i Figur¬†2.\n\n\n\n\n\n\n\nFigur¬†2: Sjekker av PR-er i teamets IaC-repo.\n\n\n\n\nN√•r PR er godkjent av en kollega, og atlantis plan ikke viser noen feilmeldinger, s√• kan du effektuere endringene ved √• atlantis apply i kommentarfeltet til PR-en. Kommentareren vil trigge en kj√∏ring som effektuerer alle endringer p√• plattformen.\nEtter at atlantis apply er kj√∏rt, s√• kan du merge inn branchen til main.\nTil slutt kan du slette branchen.\n\nN√•r dette er gjort s√• endringen effektuert p√• Dapla og klar til bruk. For eksempel, hvis vi hadde aktivert transfer-service i prod for projects.yaml over, s√• ville den etter endringen sett slik ut:\n\n\n./infra/projects.yaml\n\nteam_uniform_name: play-obr\n\nprojects:\n  - project_name: play-obr\n    env: test\n    features:\n      - dapla-buckets\n      - kildomaten\n\n  - project_name: play-obr\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service\n\n\n\n\nFor √• deaktivere en feature som ikke lenger i bruk, s√• f√∏lger du bare beskrivelsen for Aktivere tjeneste over, med det unntak at du n√• fjerner en linje istedenfor √• legge til.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Features"
    ]
  },
  {
    "objectID": "statistikkere/features.html#aktivere-feature",
    "href": "statistikkere/features.html#aktivere-feature",
    "title": "Features",
    "section": "",
    "text": "Skru p√• en feature om gangen.\n\n\n\nHvis du √∏nsker √• skru p√• flere features samtidig, s√• m√• du gj√∏re det i flere PR-er. Atlantis vil ikke klare √• h√•ndtere flere features i samme PR. F√∏lg oppskriften under for hver feature du √∏nsker √• skru p√•.\n\n\nAktivering av en feature for Dapla-teamet ditt er veldig enkelt. Endringen er s√• liten at vi anbefaler √• gj√∏re endringen direkte i GitHubs grensesnitt, uten √• klone repoet f√∏rst. Slik g√•r du frem:\n\nS√∏k opp ditt teams sitt IaC-repoet under https://github.com/statisticsnorway/.\n\n\n\n\nI repoet √•pner du fila ./infra/projects.yaml. Se eksempel for teamet play-obr til h√∏yre.\nFinn ut om du √∏nsker √• skru p√• en feature i test eller prod. Hvis du √∏nsker √• gj√∏re det i prod, s√• skal du legge til en linje under features der env: prod. Hvis du √∏nsker √• gj√∏re det i test, s√• skal du legge til en linje under features der env: test.\nDu kan legge til linjen ved √• trykke p√• -ikonet √∏verst til h√∏yre i fila, endre teksten, og trykke p√• Commit changes. Velg deretter hvilket navn du √∏nsker p√• branchen, og trykk Create pull request.\nI PR-en venter du og ser at atlantis/plan1 har blitt kj√∏rt og f√•r en  til venstre for hver kj√∏ring, slik som vist i Figur¬†1.\n\n\n\n\n./infra/projects.yaml\n\nteam_uniform_name: play-obr\n\nprojects:\n  - project_name: play-obr\n    env: test\n    features:\n      - dapla-buckets\n      - kildomaten\n\n  - project_name: play-obr\n    env: prod\n    features:\n      - dapla-buckets\n\n\n\n\n\n\n\n\n\nFigur¬†1: Atlantis plan planlegger endringen og sjekker om den er klar til gjennomf√∏ring.\n\n\n\n\nHvis alt er i orden s√• ber du en kollega om √• se over endringen og godkjenne hvis alt ser riktig ut. N√•r den er godkjent vil du se et bilde som ligner det du ser i Figur¬†2.\n\n\n\n\n\n\n\nFigur¬†2: Sjekker av PR-er i teamets IaC-repo.\n\n\n\n\nN√•r PR er godkjent av en kollega, og atlantis plan ikke viser noen feilmeldinger, s√• kan du effektuere endringene ved √• atlantis apply i kommentarfeltet til PR-en. Kommentareren vil trigge en kj√∏ring som effektuerer alle endringer p√• plattformen.\nEtter at atlantis apply er kj√∏rt, s√• kan du merge inn branchen til main.\nTil slutt kan du slette branchen.\n\nN√•r dette er gjort s√• endringen effektuert p√• Dapla og klar til bruk. For eksempel, hvis vi hadde aktivert transfer-service i prod for projects.yaml over, s√• ville den etter endringen sett slik ut:\n\n\n./infra/projects.yaml\n\nteam_uniform_name: play-obr\n\nprojects:\n  - project_name: play-obr\n    env: test\n    features:\n      - dapla-buckets\n      - kildomaten\n\n  - project_name: play-obr\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Features"
    ]
  },
  {
    "objectID": "statistikkere/features.html#deaktivere-en-feature",
    "href": "statistikkere/features.html#deaktivere-en-feature",
    "title": "Features",
    "section": "",
    "text": "For √• deaktivere en feature som ikke lenger i bruk, s√• f√∏lger du bare beskrivelsen for Aktivere tjeneste over, med det unntak at du n√• fjerner en linje istedenfor √• legge til.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Features"
    ]
  },
  {
    "objectID": "statistikkere/features.html#footnotes",
    "href": "statistikkere/features.html#footnotes",
    "title": "Features",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nDet som skjer n√•r atlantis plan kj√∏res er at det genereres en detaljert beskrivelse av hvilke endringer som m√• skje p√• plattformen for at teamets feature skal aktiveres. Derfor m√• eventuelle feilmeldinger fra atlantis plan fikses f√∏r man faktiske kan effektuere endringene med atlantis apply. ‚Ü©Ô∏é",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Features"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html",
    "href": "statistikkere/datatilstander.html",
    "title": "Datatilstander",
    "section": "",
    "text": "En datatilstand er et resultat av at et datasett har g√•tt gjennom gitte operasjoner og prosesser (Standardutvalget 2023, 5). Denne siden er ment som en kort innf√∏ring i de forskjellige datatilstandene. Siden er basert p√• det interne dokumentet Datatilstander SSB - 2. utgave. Definisjonene er direkte utdrag fra dette dokumentet. Se interndokumentet for en mer grundig gjennomgang av datatilstander i SSB.\nI SSB skiller vi mellom fem datatilstander:\nAlle datatilstander er obligatoriske bortsett fra inndata. Figur¬†1 viser hvordan de forskjellige datatilstandene henger sammen.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Datatilstander"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#kildedata",
    "href": "statistikkere/datatilstander.html#kildedata",
    "title": "Datatilstander",
    "section": "Kildedata",
    "text": "Kildedata\nKildedata er data lagret slik de ble levert til SSB fra dataeier. Eksempler p√• kildedata er: grunndata, transaksjonsdata, administrative data, statistiske data og aggregerte data og rapporter (Standardutvalget 2023, 7). Kildedata lagres i b√∏tten ssb-&lt;teamnavn&gt;-data-kilde-prod. Les mer om b√∏tter her og lagringsstandarder her.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Datatilstander"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#inndata",
    "href": "statistikkere/datatilstander.html#inndata",
    "title": "Datatilstander",
    "section": "Inndata",
    "text": "Inndata\nInndata er kildedata som er transformert til SSBs standard lagringsformat (Standardutvalget 2023, 8). Denne transformeringer inkluderer blant annet at dataene skal benytte UTF-8 tegnsett. Les mer om SSBs standard lagringsformat her. Inndata kan ogs√• v√¶re andre statistikkers klargjorte data og/eller statistikkdata (Standardutvalget 2023, 8). Inndata er ikke en obligatorisk datatilstand. Inndata lagres i b√∏tten ssb-&lt;teamnavn&gt;-data-produkt-prod.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Datatilstander"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#klargjorte-data",
    "href": "statistikkere/datatilstander.html#klargjorte-data",
    "title": "Datatilstander",
    "section": "Klargjorte data",
    "text": "Klargjorte data\nKlargjorte data er inndata hvor:\n\nvariablene er beregnet gjennom utregninger og koblinger mellom datasett\nn√∏yaktigheten er forbedret\n\nfor eksempel som resultat av editering eller imputering\n\nmetadata med variabeldefinisjoner er lagt til.\n\nEnhver endring som er gjort skal v√¶re sporbare og dokumentert slik at statistikkene skal v√¶re etterpr√∏vbare. Klargjorte date er som regel ikke aggregerte - med mindre dataen vi mottar er aggregert. Med andre ord inneholder klargjorte data oftest enkeltobservasjoner - i likhet med kildedata og inndata (Standardutvalget 2023, 9). Klargjorte data lagres i b√∏tten ssb-&lt;teamnavn&gt;-data-produkt-prod.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Datatilstander"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#statistikk",
    "href": "statistikkere/datatilstander.html#statistikk",
    "title": "Datatilstander",
    "section": "Statistikk",
    "text": "Statistikk\nStatistikk er ‚ÄúTallfestede opplysninger om en gruppe eller et fenomen, og som kommer frem ved en sammenstilling og bearbeidelse av opplysninger om de enkelte enhetene i gruppen eller et utvalg av disse enhetene, eller ved systematisk observasjon av fenomenet‚Äù if√∏lge statistikkloven ¬ß 3a (Standardutvalget 2023, 10). Statistikk lagres i b√∏tten ssb-&lt;teamnavn&gt;-data-produkt-prod.\nStatistikk er ofte aggregerte data eller estimerte st√∏rrelser. Vi skiller mellom ujustert statistikk og justert statistikk. Indekser og sesongjusterte tall er eksempler p√• justert statistikk (Standardutvalget 2023, 10).\nStatistikk kan v√¶re inndata til andre statistikker, og kan dermed inneholde konfidensielle og detaljerte data som ikke publiseres.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Datatilstander"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#utdata",
    "href": "statistikkere/datatilstander.html#utdata",
    "title": "Datatilstander",
    "section": "Utdata",
    "text": "Utdata\nUtdata er statistikk der kravene til konfidensialtet er ivaretatt. Dette er datatilstanden som publiseres. Eksempler inkluderer: statistikkbanktabeller, tabelloppdrag og internasjonal rapportering (Standardutvalget 2023, 11). Utdata lagres i b√∏tten ssb-&lt;teamnavn&gt;-data-produkt-produkt.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Datatilstander"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#metadata-for-datatilstandene",
    "href": "statistikkere/datatilstander.html#metadata-for-datatilstandene",
    "title": "Datatilstander",
    "section": "Metadata for datatilstandene",
    "text": "Metadata for datatilstandene\nDet er forskjellige forventinger til metadata for de ulike datatilstandene. Forskjellene er skildret underdisse punktene:\n\nKildedata\n\nInformasjon p√• datasettniv√• som dataeier, omr√•det dataene omhandler og tidsinformasjon\nMetadata om enkeltvariabler er begrenset til informasjonen dataeier selv avleverer.\n\n\n\nInndata\n\nI utgangspunktet samme som kildedata\n\n\n\nKlargjorte data\n\nVariabeldefinisjoner - beskrivelse av hver enkelt variabel og hvordan den er beregnet\nN√∏yaktighetsforbedrende tiltak som er utf√∏rt\n\n\n\nStatistikk\n\nVariabeldefinisjoner\nHvilke metoder og programmer/kode som er benyttet for √• produsere statistikken\n\n\n\nUtdata\n\nI utgangspunktet samme som for statistikk",
    "crumbs": [
      "Manual",
      "Standarder",
      "Datatilstander"
    ]
  },
  {
    "objectID": "statistikkere/altinn3.html",
    "href": "statistikkere/altinn3.html",
    "title": "Altinn 3",
    "section": "",
    "text": "Frem mot sommeren 2025 skal alle skjema-unders√∏kelser i SSB som gjennomf√∏res p√• Altinn 2 flyttes over til Altinn 3. Skjemaer som flyttes til Altinn 3 vil motta sine data p√• Dapla, og ikke p√• bakken som tidligere. Datafangsten h√•ndteres av Team SUV, mens statistikkseksjonene henter sine data fra Team SUV sitt lagringsomr√•de p√• Dapla. I dette kapitlet beskriver vi n√¶rmere hvordan statistikkseksjonene kan jobbe med Altinn3-data p√• Dapla. Kort oppsummert best√•r det av disse stegene:\n\nStatistikkprodusenten avtaler overf√∏ring av skjema fra Altinn 2 til Altinn 3 med planleggere p√• S821, som koordinerer denne jobben.\nN√•r statistikkprodusentene f√•r beskjed om at Altinn3-skjemaet skal sendes ut til oppgavegiverne, s√• m√• de opprette et Dapla-team.\nN√•r Dapla-teamet er opprettet, og f√∏rste skjema er sendt inn, s√• ber de Team SUV om √• gi statistikkteamet tilgang til dataene som har kommet inn fra Altinn 3. I tillegg ber de om at Team SUV gir tilgang til teamets Transfer Service instans. 1 Merk at det m√• gis separate tilganger for data i staging- og produksjonsmilj√∏.\nStatistikkprodusenten setter opp en automatisk overf√∏ring av skjemadata med Transfer Service, fra Team SUV sitt lagringsomr√•de over til Dapla-teamet sin kildeb√∏tte.\nStatistikkprodusentene kan begynne √• jobbe med dataene i Dapla. Blant annet tilbyr Dapla en automatiseringstjeneste man kan bruke for √• prosessere dataene fra kildedata til inndata2.\n\nUnder forklarer vi mer med mer detaljer hvordan man g√•r frem for gjennomf√∏re steg 4-5 over.\n\n\n\n\n\n\nAnsvar for kildedata\n\n\n\nSelv om Team SUV tar ansvaret for datafangst fra Altinn3, s√• er det statistikkteamet som har ansvaret for langtidslagring av dataene i sin kildeb√∏tte. Det vil si at at statistikkteamet m√• s√∏rge for at data overf√∏res til sin kildeb√∏tte, og at de kan ikke regne med at Team SUV tar vare p√• en backup av dataene.\n\n\n\n\nN√•r skjema-dataene kommer inn fra Altinn 3 til Team SUV sitt lagringsomr√•de, s√• er det en del ting som er verdt √• tenke p√•:\n\nHvordan er filene organisert hos Team SUV?\nFilstrukturen der Team SUV lagrer dataene som kommer inn fra Altinn 3, har en klar struktur. Du kan selv g√• inn √• kikke p√• dataene, enten fra Jupyter eller Google Cloud Console, og bli bedre kjent med strukturen3. Figur¬†1 viser en hvordan en typisk filsti ser ut p√• lagringsomr√•det til Team SUV. Det starter med navnet til b√∏tta som Team SUV har opprettet for skjemaet. Deretter viser den innvitteringsdato, deretter et teknisk navn, og til slutt selve skjemanavnet.\n\n\n\n\n\n\n\nFigur¬†1: Typisk filsti for et Altinn3-skjema.\n\n\n\n\nHvordan organisere dataene i din kildeb√∏tte?\nN√•r vi bruker Transfer Service til √• synkronisere innholdet i Team SUV sitt lagringsomr√•de til Dapla-teamet sitt lagringsomr√•de, s√• er det mest hensiktmessig √• fortsette √• bruke mapppe-strukturen som Team SUV har. Grunnen er at vi ikke kan legge p√• noe logikk som lager en ny struktur. Tjenesten bare kopierer over data. Men du kan kan lage et nytt toppniv√•-mappe som du √∏nsker at dataene skal synkroniseres til. F.eks. at alle Altinn-dataene legger seg inn i en mappe som heter altinn. Det er spesielt nyttig hvis du har flere datakilder som skal ligge i samme kildeb√∏tte.\nUnike skjemanavn\nSkjemanavnet du ser i Figur¬†1 er unike. Dvs. at hvis en oppgavegiver sender inn flere skjemaer p√• samme dag, s√• er fortsatt skjemanavnet unikt. Det er viktig √• v√¶re klar over n√•r man bruker automatiseringstjenesten for kildedata senere. Siden tjenesten trigges per fil, og man er avhengig av unike navn for √• ikke skrive over filer, s√• er det nyttig √• vite at man kan videref√∏re skjemanavnet i overgangen fra kildedata til inndata.\n\n\n\n\nN√•r vi skal overf√∏re filer fra Team SUV sin b√∏tte til v√•r kildeb√∏tte, s√• kan vi gj√∏re det manuelt fra Jupyter som forklart her.. Men det er en bedre l√∏sning √• bruke en tjeneste som gj√∏r dette for deg. Transfer Service er en tjeneste som kan brukes til √• synkronisere innholdet mellom b√∏tter p√• Dapla, samt mellom bakke og sky. N√•r du skal ta i bruk tjenesten for √• overf√∏re data mellom en b√∏tte fra Team SUV sitt prosjekt altinn-data-prod, til en kildedata-b√∏tte i Dapla-teamet ditt, s√• gj√∏r du f√∏lgende:\n\nF√∏lg denne beskrivelsen hvordan man setter opp overf√∏ringsjobber.\nEtter at du har trykket p√• Create Transfer Job velger du Google Cloud Storage p√• begge alternativene under Get Started. Deretter g√•r du videre ved √• klikke p√• Next Step.\nUnder Choose a source s√• skal du velge hvor du skal kopiere data fra. Trykk p√• Browse. I vinduet som dukker opp trykker du p√• üîª-ikonet ved siden av Project ID. I s√∏kevinduet som dukker opp s√∏ker du opp altinn-data-prod og trykker p√• navnet. Da f√•r du listet opp alle b√∏ttene i altinn-data-prod prosjektet. Til slutt trykker du p√• b√∏tta som Team SUV har opprettet for unders√∏kelsen4 og klikker Select til nederst p√• siden. Trykk deretter Next step for √• g√• videre.\nUnder Choose a destination s√• skal du velge hvor du skal kopiere data til. Dette steget er ganske likt som forrige, men du skal n√• velge ditt eget projekt og kildeb√∏tta der. Trykk p√• Browse. I vinduet som dukker opp trykker du p√• üîª-ikonet ved siden av Project ID. I s√∏kevinduet som dukker opp s√∏ker du opp prod-&lt;ditt teamnavn&gt; og trykker p√• navnet. Da f√•r du listet opp alle b√∏ttene i ditt team sitt prosjekt. Velg kildeb√∏tta som har navnet ssb-prod-&lt;teamnavn&gt;-data-kilde. Hvis du √∏nsker √• kopiere data til en undermappe i b√∏tta, s√• trykker du p√• &gt;-ikonet ved b√∏ttenavnet og velger √∏nsket undermappe5. Til slutt trykker du p√• Select til nederst p√• siden. Trykk deretter Next step for √• g√• videre.\nUnder Choose how and when to run job velger du Run with custom frequency og Starting now i Batch-modus. Hvis du √∏nsker √• overf√∏re s√• ofte som mulig, s√• velger du Custom frequency og Repeat every 1 Hours. Til slutt trykker du Next Step nederst p√• siden.\nUnder Choose Settings s√• legger du til en kort beskrivelse av jobben du har opprettet. Under de andre valgene kan du gj√∏re f√∏lgende:\n\nUnder Advanced transfer Options trenger du ikke gj√∏re noen endringer.\nUnder When to overwrite kan du velge If different.\nUnder When to delete kan du velge Never.\nUnder Manifest trenger du ikke huke av.\nUnder Logging options velger du samme som vist i Figur¬†2.\n\n\n\n\n\n\n\n\nFigur¬†2: Valg av opsjoner for logging i Transfer Service\n\n\n\nTil slutt trykker du Create for √• aktivere tjenesten. Den vil da sjekke Team SUV sin b√∏tte hver time og kopiere over alle filer som ikke allerede eksisterer statistikkteamets kildeb√∏tte.\n\n\n\nN√•r du har satt opp Transfer Service til √• kopiere over filer fra Team SUV sin b√∏tte til statistikkteamets kildeb√∏tte, s√• vil det potensielt komme inn nye skjemaer hver time. Disse kan du lese inn i Jupyter og jobbe med hvis du er kildedata-ansvarlig i teamet. Hvis du ikke er det s√• m√• du vente til dataene er tilgjengeliggjort i produkt-b√∏tta til teamet.\nSiden f√• personer innehar rollen som kildedata-ansvarlig s√• er det laget en automatiseringstjeneste som kan bearbeide alle filer som kommer inn i kildeb√∏tta. Den lar deg kj√∏re et python-script p√• alle filer som kommer inn i kildeb√∏tta.\nLes mer om hvordan du kan bruker tjenesten her.\n\n\n\nI denne delen deles noen tips og triks for √• jobbe med Altinn3-dataene p√• Dapla. Fokuset vil v√¶re p√• hvordan du kan lese inn og transformere xml-filer fra skjema-dataene.\n\n\nFor √• se innholdet i en mappe gir det mest mening √• bruke Google Cloud Console. Her kan du se b√•de filer og mapper i et pek-og-klikk grensesnitt. Men du kan ikke se p√• innholdet i filene der. Til det m√• du bruke Jupyter.\nAnta at vi √∏nsker √• liste ut alle som leverte skjema den 10. mars 2023. Da kan vi bruke gcsfs til √• gj√∏re det6:\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/**.xml\")\n\n# Print the list of parquet files\nxml_files\nHer bruker vi fs.glob()-funksjonen fra gcsfs til √• loope gjennom alle undermapper av gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/, og legge alle filer som slutter med .xml til listen xml_files. Dermed har vi et Python-objekt som kun brukes til √• hente inn de filene vi √∏nsker. Vi bruker da det vi vet om mappestrukturen i Team SUV sin b√∏tte som vi s√• tidligere i Figur¬†1.\n\n\n\nNoen ganger kan det v√¶re nyttig √• se en xml-fil direkte. Da kan vi lese den inn i Jupyter og printe den ut. Her er et eksempel p√• hvordan vi kan gj√∏re det, samtidig som vi formatterer den med xml.dom.minidom:\nfrom xml.dom.minidom import parseString\n\nfrom dapla import FileClient\n\n# Kobler oss p√• b√∏ttene\nfs = FileClient.get_gcs_file_system()\n\n# Sett inn filstien din her\nfile = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/form_dc551844cd74.xml\"\n\ndom = parseString(fs.cat_file(file))\npretty_xml = dom.toprettyxml(indent=\"  \")\nprint(pretty_xml)\nVi brukte da fs.cat_file() til √• hente inn innholdet i mappen, gjorde det om til xml-object, og formatterer outputen med xml.dom.minidom. Resultatet vil da se ut noe s√•nt som dette:\n&lt;?xml version=\"1.0\" ?&gt;\n&lt;melding xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" dataFormatProvider=\"SERES\" dataFormatId=\"7079\" dataFormatVersion=\"47315\"&gt;\n  &lt;InternInfo&gt;\n    &lt;raNummer&gt;RA-XXXX&lt;/raNummer&gt;\n    &lt;skjemaVersjon&gt;1.0&lt;/skjemaVersjon&gt;\n    &lt;undersoekelsesNr&gt;XXXXX&lt;/undersoekelsesNr&gt;\n    &lt;visOppgaveByrde&gt;0&lt;/visOppgaveByrde&gt;\n    &lt;visBrukeropplevelse&gt;0&lt;/visBrukeropplevelse&gt;\n    &lt;delregNr&gt;2XXXXX&lt;/delregNr&gt;\n    &lt;periodeFritekst&gt;3. kvartal 2022&lt;/periodeFritekst&gt;\n    &lt;periodeFomDato&gt;2022-07-01&lt;/periodeFomDato&gt;\n    &lt;periodeTomDato&gt;2022-09-30&lt;/periodeTomDato&gt;\n    &lt;periodeType&gt;KVRT&lt;/periodeType&gt;\n    &lt;periodeNummer&gt;3&lt;/periodeNummer&gt;\n    &lt;periodeAAr&gt;2022&lt;/periodeAAr&gt;\n    &lt;periodeDato&gt; &lt;/periodeDato&gt;\n    &lt;enhetsIdent&gt;XXXXXX&lt;/enhetsIdent&gt;\n    &lt;enhetsType&gt;BEDR&lt;/enhetsType&gt;\n    &lt;enhetsOrgNr&gt;XXXXXXXXX&lt;/enhetsOrgNr&gt;\n    &lt;enhetsNavn&gt;STATISTISK SENTRALBYR√Ö &lt;/enhetsNavn&gt;\n    &lt;enhetsGateadresse&gt;Akersveien 26&lt;/enhetsGateadresse&gt;\n    &lt;enhetsPostnr&gt;0177&lt;/enhetsPostnr&gt;\n    &lt;enhetsPoststed&gt;OSLO&lt;/enhetsPoststed&gt;\n    &lt;enhetsAvdeling&gt;AVD XXXX&lt;/enhetsAvdeling&gt;\n    &lt;reporteeOrgNr&gt;XXXXXXXXX&lt;/reporteeOrgNr&gt;\n  &lt;/InternInfo&gt;\n  &lt;Kontakt&gt;\n    &lt;kontaktPersonNavn&gt;OLA NORDMANN&lt;/kontaktPersonNavn&gt;\n    &lt;kontaktPersonEpost&gt;ola@tull.no&lt;/kontaktPersonEpost&gt;\n    &lt;kontaktPersonTelefon&gt; &lt;/kontaktPersonTelefon&gt;\n    &lt;kontaktInfoBekreftet&gt;1&lt;/kontaktInfoBekreftet&gt;\n    &lt;kontaktInfoKommentar&gt;Dette er en et eksempel for Dapla-manualen&lt;/kontaktInfoKommentar&gt;\n  &lt;/Kontakt&gt;\n  &lt;Skjemadata&gt;\n    &lt;ledigeStillinger&gt;75&lt;/ledigeStillinger&gt;\n    &lt;datoPrefill&gt; &lt;/datoPrefill&gt;\n  &lt;/Skjemadata&gt;\n&lt;/melding&gt;\n\n\n\nDe f√¶rreste √∏nsker √• jobbe direkte med XML-filer. Derfor er det nyttig √• kunne transformere XML-filene til et mer brukervennlig format, f.eks. en Pandas Dataframe. Her er et eksempel p√• hvordan vi kan gj√∏re det med Pandas:\n\nimport xml.etree.ElementTree as ET\nimport pandas as pd\nfrom dapla import FileClient\n\ndef single_xml_to_dataframe(file_path: str) -&gt; pd.DataFrame:\n    fs = FileClient.get_gcs_file_system()\n\n    with fs.open(file_path, mode=\"r\") as f:\n        single_xml = f.read()\n\n    root = ET.fromstring(single_xml)\n    intern_info = root.find(\"InternInfo\")\n    kontakt = root.find(\"Kontakt\")\n    skjemadata = root.find(\"Skjemadata\")\n\n    data = []\n    all_tags = set()\n\n    for element in intern_info:\n        all_tags.add(element.tag)\n\n    for element in kontakt:\n        all_tags.add(element.tag)\n\n    for element in skjemadata:\n        all_tags.add(element.tag)\n\n    for tag in all_tags:\n        element = intern_info.find(tag)\n        if element is None:\n            element = kontakt.find(tag)\n        if element is None:\n            element = skjemadata.find(tag)\n        if element is not None:\n            value = element.text\n            data.append(value)\n        else:\n            data.append(None)\n\n    # Include the full path to the XML file in the data list\n    data.append(file_path)\n\n    # Create the DataFrame inside the function\n    df = pd.DataFrame([data], columns=list(all_tags) + [\"pathToXmlFile\"])\n    path_to_xml_file_split = df[\"pathToXmlFile\"].str.split(\"/\")\n    return df\n\n# Run the function\nfile_path = \"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/24/28c34dce4232_e7bed8a4-def5-42dc-b1e2-b9fc66beb769/form_28c34dce4232.xml\"\ndf = single_xml_to_dataframe(file_path=file_path)\nI funksjonen s√• s√∏ker vi etter alle elementer under taggene intern_info, kontakt og skjemadata. Dvs. at vi lager en dataframe med en rad, der vi tar med all data fra xml-filen. Til slutt legger vi til en kolonne med filstien til filen. Det kan v√¶re nyttig senere hvis man g√• tilbake til xml-filen for √• sjekke om konverteringen til en dataframe er riktig.\nFunksjonen single_xml_to_dataframe() kan brukes til √• loope over en liste med filstier (vi viste hvordan man lager en slik liste tidligere i kapitlet). Men et typisk problem som vil oppst√• da er at xml-filene har forskjellig antall elementer i seg, som gir dataframes med ulikt antall kolonner, og derfor vil ikke Pandas vite hvordan man konkatinerer disse. For √• fikse dette m√• du modifisere funksjonen til √• ta h√∏yde for dette.\n\n\n\nHvis vi √∏nsker √• kopiere filer manuelt (dvs. ikke med Transfer Service) fra Team SUV sine b√∏tter til egen kildeb√∏tte, kan vi gj√∏re det fra Jupyter. Vi har tidligere sett hvordan vi kan kopiere enkeltfiler mellom to b√∏tter. Under viser vi hvordan man kan kopiere alle filer under en viss filsti med kommandoen fs.copy():\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfrom_path = \"gs://ssb-prod-arbmark-skjema-data-kilde/ledstill/altinn/2022/11/21/\"\nto_path = \"gs://ssb-prod-arbmark-skjema-data-kilde/ledstill/altinn/test/\"\nfs.copy(from_path, to_path, recursive=True)\nI koden over s√• kopierer vi alle filer under from_path til to_path. Vi bruker recursive=True for √• s√∏rge for at vi kopierer alle filer under from_path.\nI eksempelet over s√• kopierer vi over alt som er from_path og dets undermapper. I tilfellet med Altinn-data s√• ligger det ogs√• pdf-filer av skjemaet som kanskje ikke √∏nsker √• kopiere. I de tilfellene kan vi f√∏rst s√∏ke etter de filene vi √∏nsker √• kopiere og legge de i en liste. Deretter kan vi kopiere over en og en fil fra listen. Denne tiln√¶rmingen er veldig nyttig hvis vi √∏nsker √• filtrere ut filer som ikke er XML-filer, eller vi √∏nsker en annen mappestruktur en den som ligger i from_path. Her er en m√•te vi kan gj√∏re det p√•:\nfrom dapla import FileClient\n\n# Lager en filsystem-instans av GCS\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/**.xml\")\n\n# Stien du √∏nsker √• kopiere til.\n# Koden under foutsetter at du har med gs:// f√∏rst\nto_folder = \"&lt;b√∏ttenavn&gt;/**.xml\"\n\n# Kopierer over filene\nfor file in xml_files:\n    from_bucket = file.split(\"/\")[0]\n    to_bucket = to_folder.split(\"/\")[2]\n    to_path = file.replace(\n        from_bucket, to_bucket\n    )\n    fs.cp(file, to_path)\nI koden over s√• bruker vi fs.glob() og ** til √• s√∏ke rekursivt etter alle xml-filer under filstien gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10. Deretter kopierer vi over filene til egen kildeb√∏tte med fs.cp(). N√•r vi skal kopiere over til en ny b√∏tte m√• vi bestemme oss for hvor filene skal ligge og hvor mye av den gamle filstien vi skal beholde. Anta at vi skal beholde hele mappestrukturen i Team SUV sin b√∏tte. Da kan vi egentlig bare erstatte ra0678-01-altinn-data-prod-e17d-ssb-altinn/ med den nye b√∏tte-navnet, og vi vil f√• den samme strukturen som i Team SUV sin b√∏tte.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn3.html#forberedelse",
    "href": "statistikkere/altinn3.html#forberedelse",
    "title": "Altinn 3",
    "section": "",
    "text": "N√•r skjema-dataene kommer inn fra Altinn 3 til Team SUV sitt lagringsomr√•de, s√• er det en del ting som er verdt √• tenke p√•:\n\nHvordan er filene organisert hos Team SUV?\nFilstrukturen der Team SUV lagrer dataene som kommer inn fra Altinn 3, har en klar struktur. Du kan selv g√• inn √• kikke p√• dataene, enten fra Jupyter eller Google Cloud Console, og bli bedre kjent med strukturen3. Figur¬†1 viser en hvordan en typisk filsti ser ut p√• lagringsomr√•det til Team SUV. Det starter med navnet til b√∏tta som Team SUV har opprettet for skjemaet. Deretter viser den innvitteringsdato, deretter et teknisk navn, og til slutt selve skjemanavnet.\n\n\n\n\n\n\n\nFigur¬†1: Typisk filsti for et Altinn3-skjema.\n\n\n\n\nHvordan organisere dataene i din kildeb√∏tte?\nN√•r vi bruker Transfer Service til √• synkronisere innholdet i Team SUV sitt lagringsomr√•de til Dapla-teamet sitt lagringsomr√•de, s√• er det mest hensiktmessig √• fortsette √• bruke mapppe-strukturen som Team SUV har. Grunnen er at vi ikke kan legge p√• noe logikk som lager en ny struktur. Tjenesten bare kopierer over data. Men du kan kan lage et nytt toppniv√•-mappe som du √∏nsker at dataene skal synkroniseres til. F.eks. at alle Altinn-dataene legger seg inn i en mappe som heter altinn. Det er spesielt nyttig hvis du har flere datakilder som skal ligge i samme kildeb√∏tte.\nUnike skjemanavn\nSkjemanavnet du ser i Figur¬†1 er unike. Dvs. at hvis en oppgavegiver sender inn flere skjemaer p√• samme dag, s√• er fortsatt skjemanavnet unikt. Det er viktig √• v√¶re klar over n√•r man bruker automatiseringstjenesten for kildedata senere. Siden tjenesten trigges per fil, og man er avhengig av unike navn for √• ikke skrive over filer, s√• er det nyttig √• vite at man kan videref√∏re skjemanavnet i overgangen fra kildedata til inndata.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn3.html#transfer-service",
    "href": "statistikkere/altinn3.html#transfer-service",
    "title": "Altinn 3",
    "section": "",
    "text": "N√•r vi skal overf√∏re filer fra Team SUV sin b√∏tte til v√•r kildeb√∏tte, s√• kan vi gj√∏re det manuelt fra Jupyter som forklart her.. Men det er en bedre l√∏sning √• bruke en tjeneste som gj√∏r dette for deg. Transfer Service er en tjeneste som kan brukes til √• synkronisere innholdet mellom b√∏tter p√• Dapla, samt mellom bakke og sky. N√•r du skal ta i bruk tjenesten for √• overf√∏re data mellom en b√∏tte fra Team SUV sitt prosjekt altinn-data-prod, til en kildedata-b√∏tte i Dapla-teamet ditt, s√• gj√∏r du f√∏lgende:\n\nF√∏lg denne beskrivelsen hvordan man setter opp overf√∏ringsjobber.\nEtter at du har trykket p√• Create Transfer Job velger du Google Cloud Storage p√• begge alternativene under Get Started. Deretter g√•r du videre ved √• klikke p√• Next Step.\nUnder Choose a source s√• skal du velge hvor du skal kopiere data fra. Trykk p√• Browse. I vinduet som dukker opp trykker du p√• üîª-ikonet ved siden av Project ID. I s√∏kevinduet som dukker opp s√∏ker du opp altinn-data-prod og trykker p√• navnet. Da f√•r du listet opp alle b√∏ttene i altinn-data-prod prosjektet. Til slutt trykker du p√• b√∏tta som Team SUV har opprettet for unders√∏kelsen4 og klikker Select til nederst p√• siden. Trykk deretter Next step for √• g√• videre.\nUnder Choose a destination s√• skal du velge hvor du skal kopiere data til. Dette steget er ganske likt som forrige, men du skal n√• velge ditt eget projekt og kildeb√∏tta der. Trykk p√• Browse. I vinduet som dukker opp trykker du p√• üîª-ikonet ved siden av Project ID. I s√∏kevinduet som dukker opp s√∏ker du opp prod-&lt;ditt teamnavn&gt; og trykker p√• navnet. Da f√•r du listet opp alle b√∏ttene i ditt team sitt prosjekt. Velg kildeb√∏tta som har navnet ssb-prod-&lt;teamnavn&gt;-data-kilde. Hvis du √∏nsker √• kopiere data til en undermappe i b√∏tta, s√• trykker du p√• &gt;-ikonet ved b√∏ttenavnet og velger √∏nsket undermappe5. Til slutt trykker du p√• Select til nederst p√• siden. Trykk deretter Next step for √• g√• videre.\nUnder Choose how and when to run job velger du Run with custom frequency og Starting now i Batch-modus. Hvis du √∏nsker √• overf√∏re s√• ofte som mulig, s√• velger du Custom frequency og Repeat every 1 Hours. Til slutt trykker du Next Step nederst p√• siden.\nUnder Choose Settings s√• legger du til en kort beskrivelse av jobben du har opprettet. Under de andre valgene kan du gj√∏re f√∏lgende:\n\nUnder Advanced transfer Options trenger du ikke gj√∏re noen endringer.\nUnder When to overwrite kan du velge If different.\nUnder When to delete kan du velge Never.\nUnder Manifest trenger du ikke huke av.\nUnder Logging options velger du samme som vist i Figur¬†2.\n\n\n\n\n\n\n\n\nFigur¬†2: Valg av opsjoner for logging i Transfer Service\n\n\n\nTil slutt trykker du Create for √• aktivere tjenesten. Den vil da sjekke Team SUV sin b√∏tte hver time og kopiere over alle filer som ikke allerede eksisterer statistikkteamets kildeb√∏tte.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn3.html#automatiseringstjeneste-for-kildedata",
    "href": "statistikkere/altinn3.html#automatiseringstjeneste-for-kildedata",
    "title": "Altinn 3",
    "section": "",
    "text": "N√•r du har satt opp Transfer Service til √• kopiere over filer fra Team SUV sin b√∏tte til statistikkteamets kildeb√∏tte, s√• vil det potensielt komme inn nye skjemaer hver time. Disse kan du lese inn i Jupyter og jobbe med hvis du er kildedata-ansvarlig i teamet. Hvis du ikke er det s√• m√• du vente til dataene er tilgjengeliggjort i produkt-b√∏tta til teamet.\nSiden f√• personer innehar rollen som kildedata-ansvarlig s√• er det laget en automatiseringstjeneste som kan bearbeide alle filer som kommer inn i kildeb√∏tta. Den lar deg kj√∏re et python-script p√• alle filer som kommer inn i kildeb√∏tta.\nLes mer om hvordan du kan bruker tjenesten her.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn3.html#tips-og-triks",
    "href": "statistikkere/altinn3.html#tips-og-triks",
    "title": "Altinn 3",
    "section": "",
    "text": "I denne delen deles noen tips og triks for √• jobbe med Altinn3-dataene p√• Dapla. Fokuset vil v√¶re p√• hvordan du kan lese inn og transformere xml-filer fra skjema-dataene.\n\n\nFor √• se innholdet i en mappe gir det mest mening √• bruke Google Cloud Console. Her kan du se b√•de filer og mapper i et pek-og-klikk grensesnitt. Men du kan ikke se p√• innholdet i filene der. Til det m√• du bruke Jupyter.\nAnta at vi √∏nsker √• liste ut alle som leverte skjema den 10. mars 2023. Da kan vi bruke gcsfs til √• gj√∏re det6:\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/**.xml\")\n\n# Print the list of parquet files\nxml_files\nHer bruker vi fs.glob()-funksjonen fra gcsfs til √• loope gjennom alle undermapper av gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/, og legge alle filer som slutter med .xml til listen xml_files. Dermed har vi et Python-objekt som kun brukes til √• hente inn de filene vi √∏nsker. Vi bruker da det vi vet om mappestrukturen i Team SUV sin b√∏tte som vi s√• tidligere i Figur¬†1.\n\n\n\nNoen ganger kan det v√¶re nyttig √• se en xml-fil direkte. Da kan vi lese den inn i Jupyter og printe den ut. Her er et eksempel p√• hvordan vi kan gj√∏re det, samtidig som vi formatterer den med xml.dom.minidom:\nfrom xml.dom.minidom import parseString\n\nfrom dapla import FileClient\n\n# Kobler oss p√• b√∏ttene\nfs = FileClient.get_gcs_file_system()\n\n# Sett inn filstien din her\nfile = \"gs://ssb-dapla-felles-data-produkt-prod/altinn3/form_dc551844cd74.xml\"\n\ndom = parseString(fs.cat_file(file))\npretty_xml = dom.toprettyxml(indent=\"  \")\nprint(pretty_xml)\nVi brukte da fs.cat_file() til √• hente inn innholdet i mappen, gjorde det om til xml-object, og formatterer outputen med xml.dom.minidom. Resultatet vil da se ut noe s√•nt som dette:\n&lt;?xml version=\"1.0\" ?&gt;\n&lt;melding xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" dataFormatProvider=\"SERES\" dataFormatId=\"7079\" dataFormatVersion=\"47315\"&gt;\n  &lt;InternInfo&gt;\n    &lt;raNummer&gt;RA-XXXX&lt;/raNummer&gt;\n    &lt;skjemaVersjon&gt;1.0&lt;/skjemaVersjon&gt;\n    &lt;undersoekelsesNr&gt;XXXXX&lt;/undersoekelsesNr&gt;\n    &lt;visOppgaveByrde&gt;0&lt;/visOppgaveByrde&gt;\n    &lt;visBrukeropplevelse&gt;0&lt;/visBrukeropplevelse&gt;\n    &lt;delregNr&gt;2XXXXX&lt;/delregNr&gt;\n    &lt;periodeFritekst&gt;3. kvartal 2022&lt;/periodeFritekst&gt;\n    &lt;periodeFomDato&gt;2022-07-01&lt;/periodeFomDato&gt;\n    &lt;periodeTomDato&gt;2022-09-30&lt;/periodeTomDato&gt;\n    &lt;periodeType&gt;KVRT&lt;/periodeType&gt;\n    &lt;periodeNummer&gt;3&lt;/periodeNummer&gt;\n    &lt;periodeAAr&gt;2022&lt;/periodeAAr&gt;\n    &lt;periodeDato&gt; &lt;/periodeDato&gt;\n    &lt;enhetsIdent&gt;XXXXXX&lt;/enhetsIdent&gt;\n    &lt;enhetsType&gt;BEDR&lt;/enhetsType&gt;\n    &lt;enhetsOrgNr&gt;XXXXXXXXX&lt;/enhetsOrgNr&gt;\n    &lt;enhetsNavn&gt;STATISTISK SENTRALBYR√Ö &lt;/enhetsNavn&gt;\n    &lt;enhetsGateadresse&gt;Akersveien 26&lt;/enhetsGateadresse&gt;\n    &lt;enhetsPostnr&gt;0177&lt;/enhetsPostnr&gt;\n    &lt;enhetsPoststed&gt;OSLO&lt;/enhetsPoststed&gt;\n    &lt;enhetsAvdeling&gt;AVD XXXX&lt;/enhetsAvdeling&gt;\n    &lt;reporteeOrgNr&gt;XXXXXXXXX&lt;/reporteeOrgNr&gt;\n  &lt;/InternInfo&gt;\n  &lt;Kontakt&gt;\n    &lt;kontaktPersonNavn&gt;OLA NORDMANN&lt;/kontaktPersonNavn&gt;\n    &lt;kontaktPersonEpost&gt;ola@tull.no&lt;/kontaktPersonEpost&gt;\n    &lt;kontaktPersonTelefon&gt; &lt;/kontaktPersonTelefon&gt;\n    &lt;kontaktInfoBekreftet&gt;1&lt;/kontaktInfoBekreftet&gt;\n    &lt;kontaktInfoKommentar&gt;Dette er en et eksempel for Dapla-manualen&lt;/kontaktInfoKommentar&gt;\n  &lt;/Kontakt&gt;\n  &lt;Skjemadata&gt;\n    &lt;ledigeStillinger&gt;75&lt;/ledigeStillinger&gt;\n    &lt;datoPrefill&gt; &lt;/datoPrefill&gt;\n  &lt;/Skjemadata&gt;\n&lt;/melding&gt;\n\n\n\nDe f√¶rreste √∏nsker √• jobbe direkte med XML-filer. Derfor er det nyttig √• kunne transformere XML-filene til et mer brukervennlig format, f.eks. en Pandas Dataframe. Her er et eksempel p√• hvordan vi kan gj√∏re det med Pandas:\n\nimport xml.etree.ElementTree as ET\nimport pandas as pd\nfrom dapla import FileClient\n\ndef single_xml_to_dataframe(file_path: str) -&gt; pd.DataFrame:\n    fs = FileClient.get_gcs_file_system()\n\n    with fs.open(file_path, mode=\"r\") as f:\n        single_xml = f.read()\n\n    root = ET.fromstring(single_xml)\n    intern_info = root.find(\"InternInfo\")\n    kontakt = root.find(\"Kontakt\")\n    skjemadata = root.find(\"Skjemadata\")\n\n    data = []\n    all_tags = set()\n\n    for element in intern_info:\n        all_tags.add(element.tag)\n\n    for element in kontakt:\n        all_tags.add(element.tag)\n\n    for element in skjemadata:\n        all_tags.add(element.tag)\n\n    for tag in all_tags:\n        element = intern_info.find(tag)\n        if element is None:\n            element = kontakt.find(tag)\n        if element is None:\n            element = skjemadata.find(tag)\n        if element is not None:\n            value = element.text\n            data.append(value)\n        else:\n            data.append(None)\n\n    # Include the full path to the XML file in the data list\n    data.append(file_path)\n\n    # Create the DataFrame inside the function\n    df = pd.DataFrame([data], columns=list(all_tags) + [\"pathToXmlFile\"])\n    path_to_xml_file_split = df[\"pathToXmlFile\"].str.split(\"/\")\n    return df\n\n# Run the function\nfile_path = \"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/24/28c34dce4232_e7bed8a4-def5-42dc-b1e2-b9fc66beb769/form_28c34dce4232.xml\"\ndf = single_xml_to_dataframe(file_path=file_path)\nI funksjonen s√• s√∏ker vi etter alle elementer under taggene intern_info, kontakt og skjemadata. Dvs. at vi lager en dataframe med en rad, der vi tar med all data fra xml-filen. Til slutt legger vi til en kolonne med filstien til filen. Det kan v√¶re nyttig senere hvis man g√• tilbake til xml-filen for √• sjekke om konverteringen til en dataframe er riktig.\nFunksjonen single_xml_to_dataframe() kan brukes til √• loope over en liste med filstier (vi viste hvordan man lager en slik liste tidligere i kapitlet). Men et typisk problem som vil oppst√• da er at xml-filene har forskjellig antall elementer i seg, som gir dataframes med ulikt antall kolonner, og derfor vil ikke Pandas vite hvordan man konkatinerer disse. For √• fikse dette m√• du modifisere funksjonen til √• ta h√∏yde for dette.\n\n\n\nHvis vi √∏nsker √• kopiere filer manuelt (dvs. ikke med Transfer Service) fra Team SUV sine b√∏tter til egen kildeb√∏tte, kan vi gj√∏re det fra Jupyter. Vi har tidligere sett hvordan vi kan kopiere enkeltfiler mellom to b√∏tter. Under viser vi hvordan man kan kopiere alle filer under en viss filsti med kommandoen fs.copy():\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfrom_path = \"gs://ssb-prod-arbmark-skjema-data-kilde/ledstill/altinn/2022/11/21/\"\nto_path = \"gs://ssb-prod-arbmark-skjema-data-kilde/ledstill/altinn/test/\"\nfs.copy(from_path, to_path, recursive=True)\nI koden over s√• kopierer vi alle filer under from_path til to_path. Vi bruker recursive=True for √• s√∏rge for at vi kopierer alle filer under from_path.\nI eksempelet over s√• kopierer vi over alt som er from_path og dets undermapper. I tilfellet med Altinn-data s√• ligger det ogs√• pdf-filer av skjemaet som kanskje ikke √∏nsker √• kopiere. I de tilfellene kan vi f√∏rst s√∏ke etter de filene vi √∏nsker √• kopiere og legge de i en liste. Deretter kan vi kopiere over en og en fil fra listen. Denne tiln√¶rmingen er veldig nyttig hvis vi √∏nsker √• filtrere ut filer som ikke er XML-filer, eller vi √∏nsker en annen mappestruktur en den som ligger i from_path. Her er en m√•te vi kan gj√∏re det p√•:\nfrom dapla import FileClient\n\n# Lager en filsystem-instans av GCS\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/**.xml\")\n\n# Stien du √∏nsker √• kopiere til.\n# Koden under foutsetter at du har med gs:// f√∏rst\nto_folder = \"&lt;b√∏ttenavn&gt;/**.xml\"\n\n# Kopierer over filene\nfor file in xml_files:\n    from_bucket = file.split(\"/\")[0]\n    to_bucket = to_folder.split(\"/\")[2]\n    to_path = file.replace(\n        from_bucket, to_bucket\n    )\n    fs.cp(file, to_path)\nI koden over s√• bruker vi fs.glob() og ** til √• s√∏ke rekursivt etter alle xml-filer under filstien gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10. Deretter kopierer vi over filene til egen kildeb√∏tte med fs.cp(). N√•r vi skal kopiere over til en ny b√∏tte m√• vi bestemme oss for hvor filene skal ligge og hvor mye av den gamle filstien vi skal beholde. Anta at vi skal beholde hele mappestrukturen i Team SUV sin b√∏tte. Da kan vi egentlig bare erstatte ra0678-01-altinn-data-prod-e17d-ssb-altinn/ med den nye b√∏tte-navnet, og vi vil f√• den samme strukturen som i Team SUV sin b√∏tte.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn3.html#footnotes",
    "href": "statistikkere/altinn3.html#footnotes",
    "title": "Altinn 3",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nForslag til e-post til Team SUV etter at teamet er opprettet:\nVi har opprettet et Dapla-tema som heter &lt;ditt teamnavn&gt; for √• jobbe med skjema &lt;RA-XXXX&gt;. Kan dere gi oss tilgang til riktig lagringsomr√•de og ogs√• gi v√•r Transfer Service lesetilgang.‚Ü©Ô∏é\nEn typisk prosessering som de fleste vil √∏nske √• gj√∏re er √• konvertere fra xml-formatet det kom p√•, og over til parquet-formatet.‚Ü©Ô∏é\nDu kan g√• inn i Google Cloud Console og s√∏ke opp prosjektet til Team SUV som de bruker for √• dele data. Det heter altinn-data-prod, og du finner b√∏ttene ved √• klikke deg inn p√• Cloud Storage‚Ü©Ô∏é\nB√∏ttenavnet starter alltid med RA-nummeret til unders√∏kelsen.‚Ü©Ô∏é\nAlternativt oppretter du en mappe direkte vinduet ved √• trykke p√• mappe-ikonet med en +-tegn i seg.‚Ü©Ô∏é\nFor √• jobbe mot datat i GCS som i et ‚Äúvanlig‚Äù filsysten kan vi bruke FileClient.get_gcs_file_system() fra dapla-toolbelt.‚Ü©Ô∏é",
    "crumbs": [
      "Manual",
      "Appendix",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/kildomaten.html",
    "href": "statistikkere/kildomaten.html",
    "title": "Kildomaten",
    "section": "",
    "text": "Kildomaten er en tjeneste for √• automatisere overgangen fra kildedata til inndata. Tjenesten lar statistikkere kj√∏re sine egne skript automatisk p√• alle nye filer i kildedatab√∏tta og skrive resultatet til produktb√∏tta. Form√•let med tjenesten er minimere behovet for tilgang til kildedata samtidig som teamet selv bestemmer hvordan transformasjonn til inndata skal foreg√•. Statistikkproduksjon kan da starte i en tilstand der dataminimering og pseudonymisering allerede er gjennomf√∏rt.\nProsessering som skal skje i overgangen fra kildedata til inndata har SSB definert til √• v√¶re:\n\nDataminimering:\nFjerne alle felter som ikke er strengt n√∏dvendig for √• produsere statistikk.\nPseudonymisering:\nPseudonymisering av personidentifiserende data.\nKodeverk:\nLegge p√• standard kodeverk fra for eksempel Klass.\n\nStandardisering:\nTegnsett, datoformat, etc. endres til SSBs standardformat.\n\nUnder forklarer vi n√¶rmere hvordan man bruker tjenesten. Da forutsetter vi at du har et Dapla-team med tjenesten er aktivert. les mer om hvordan du aktiverer tjenester her (lenker her).\n\n\nF√∏r et Dapla-team kan ta i bruk Kildomaten m√• man tjenesten aktivert for teamet. Som standard f√•r alle statistikkteam dette skrudd p√• i prod-milj√∏et som opprettes for teamet. √ònsker du √• aktivere Kildomaten i test-milj√∏et kan dette gj√∏res selvbetjent som en feature. Alternativt kan man opprette en Kundeservice-sak og be om √• hjelp til dette.\n\n\n\nI denne delen bryter vi ned prosessen med √• sette opp Kildomaten i de stegene vi mener det er hensiktsmessig √• gj√∏re det n√•r den settes opp for f√∏rste gang p√• en enkeltkilde. Senere i kapitlet forklarer vi hvordan man setter den opp flere kilder og hvordan man vedlikeholder tjenesten.\nHusk at alle p√• teamet kan gj√∏re det meste av arbeidet her, men det er data-admins som m√• godkjenne at tjenesten rulles ut1.\n\n\nOppsett av Kildomaten gj√∏res i teamets IaC-repo2. N√•r vi skal sette opp Kildomaten-kilde m√• vi gj√∏re gj√∏re endringer i teamets IaC-repo. Man finner teamets IaC-repo ved g√• inn p√• SSBs GitHub-organisasjon og s√∏ke etter repoet som heter &lt;teamnavn&gt;-iac. N√•r du har funnet repoet s√• kan du gj√∏re f√∏lgende:\n\nKlon ned ditt teams Iac-repo git clone &lt;repo-url&gt;\nOpprett en ny branch: git checkout -b add-kildomaten-source\n\n\n\n\nFor at Kildomaten skal fungere s√• m√• det opprettes en bestemt mappestruktur i IaC-repoet til teamet. N√•r et team blir opprettet vil den grunnleggende mappestrukturen i IaC-repoet allerede v√¶re opprettet for prod-milj√∏et til teamet. F.eks. vil mappestrukturen se slik ut for team dapla-example:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n‚îú‚îÄ‚îÄ automation/\n‚îÇ   ‚îî‚îÄ‚îÄ source-data/\n‚îÇ       ‚îú‚îÄ‚îÄ dapla-example-prod/\n‚îÇ           ‚îî‚îÄ‚îÄ README.md\n‚îÇ \n‚îÇ...           \n\nSkal du sette opp Kildomaten i prod-milj√∏et s√• kan du f√∏lge oppskriften som kommer senere i kapitlet uten √• gj√∏re noe mer enda.\nSkal du ogs√• bruke Kildomaten i test-milj√∏et s√• m√• opprette en ny mappe og lage en PR i IaC-repoet til teamet. Da vil strukturen se slik ut:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n‚îú‚îÄ‚îÄ automation/\n‚îÇ   ‚îî‚îÄ‚îÄ source-data/\n‚îÇ       ‚îú‚îÄ‚îÄ dapla-example-prod/\n‚îÇ       ‚îÇ    ‚îî‚îÄ‚îÄ README.md\n‚îÇ       ‚îú‚îÄ‚îÄ dapla-example-test/ \n‚îÇ...           \n\nI mappestrukturen over s√• har vi klargjort den grunnleggende mappestrukturen for √• ta i bruk Kildomaten i prod- og test-milj√∏et. Neste steg blir √• legge de ulike kildene som egne mapper under dapla-example-prod og dapla-example-test. Det viser vi i neste avsnitt.\n\n\n\nKildomaten lar deg prosessere ulike filstier i kildeb√∏tta med ulike python-script. Dette refereres til som at Kildomaten har flere kilder. For √• sette opp en kilde s√• m√• man f√∏lge en definert mappestruktur i IaC-repoet der alle kildene ligger rett under &lt;teamnavn&gt;-prod- eller &lt;teamnavn&gt;-test-mappen. Du kan ikke ha undermapper under en kilde. Du velger selv navnet p√• kildene/mappene i IaC-repoet, og det vil v√¶re navnet p√• kildene i Kildomaten. Senere i kapitlet ser vi at vi m√• bruke navnet for trigge re-kj√∏ring av kilder.\nUnder er et eksempel p√• hvordan det kan se ut for eksempel-teamet dapla-example:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n‚îú‚îÄ‚îÄ automation/\n‚îÇ   ‚îî‚îÄ‚îÄ source-data/\n‚îÇ       ‚îú‚îÄ‚îÄ dapla-example-prod/\n‚îÇ       ‚îÇ    ‚îî‚îÄ‚îÄ altinn\n‚îÇ       ‚îÇ    ‚îî‚îÄ‚îÄ ameld\n‚îÇ       ‚îú‚îÄ‚îÄ dapla-example-test/\n‚îÇ       ‚îÇ    ‚îî‚îÄ‚îÄ altinn\n‚îÇ       ‚îÇ    ‚îî‚îÄ‚îÄ ameld\n‚îÇ       ‚îÇ    ‚îî‚îÄ‚îÄ nudb\n‚îÇ...           \n\nI eksempelet over ser vi at det er opprettet kildene altinn og ameld for b√•de test- og prod-milj√∏et. I tillegg er det i test-milj√∏et kj√∏rende en annen kilde som heter nudb. Hver av disse kildene kan kj√∏re et eget Pyton-script p√• alle filer som skrives til en gitt filsti som man definerer selv.\n\n\n\n\n\n\nAlle kilder p√• samme niv√•\n\n\n\nEt team kan sette opp mange kilder som skal prosesseres med ulike skript. Men Kildomaten tillater bare et niv√• under automation/source-data-&lt;teamnavn&gt;-&lt;milj√∏&gt;/. Det vil si at du ikke kan ha undermapper under en kilde. Det er ogs√• slik at man alltid m√• opprette en mappe for en kilde, selv om du kun har en kilde. F.eks. kan man ikke droppe mappen altinn i eksempelet over.\n\n\n\n\n\nN√•r mappestrukturen er opprettet kan man legge til filene som beskriver hvilke filer som skal prosesseres, og python-skriptet med koden som skal prosessere filene. Det er to filer som m√• eksistere for at tjenesten skal fungere:\n\nEn konfigurasjonsfil\nEt Python-skript\n\nN√•r du har opprettet de skal de ligge p√• denne m√•ten i IaC-repoet:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n‚îú‚îÄ‚îÄ automation/\n‚îÇ   ‚îî‚îÄ‚îÄ source-data/\n‚îÇ       ‚îú‚îÄ‚îÄ dapla-example-prod/\n‚îÇ           ‚îî‚îÄ‚îÄ altinn/\n‚îÇ               ‚îú‚îÄ‚îÄ config.yaml\n‚îÇ               ‚îî‚îÄ‚îÄ process_source_data.py\n‚îÇ...           \n\nUnder forklares hvordan skriver innholdet i de to filene.\n\n\nKildomaten trigges ved at det oppst√•r nye filer i kildeb√∏tta til teamet. Hvorvidt den skal trigges p√• alle filer, eller kun filer i en gitt undermappe, bestemmer brukeren ved √• konfigurere tjenesten i config.yaml. Her kan du ogs√• angi hvor mye ressurser prosesseringen skal f√•.\nHvis vi fortsetter eksempelet v√•rt fra tidligere med dapla-example, s√• kan vi tenkes oss at teamet √∏nsker √• Kildomaten skal trigges p√• alle filer som oppst√•r i kildeb√∏tta under filstien:\nssb-dapla-example-data-kilde-prod/ledstill/altinn/.\nFor √• konfigurere tjenesten i Kildomaten m√• vi legge til en fil som heter config.yaml under automation/source-data/dapla-example-prod/altinn/, slik som vist i forrige kapitel. I dette tilfellet vil den ha innholdet som vist til venstre under:\n\n\n\n\nconfig.yaml\n\nfolder_prefix: ledstill/altinn\nmemory_size: 1  # GiB\n\n\n\n\n\n\nssb-dapla-example-data-kilde-prod/\n\n‚îú‚îÄ‚îÄ ledstill/\n‚îÇ   ‚îî‚îÄ‚îÄ altinn/\n‚îÇ   ‚îî‚îÄ‚îÄ aordningen/\n‚îú‚îÄ‚îÄ sykefra/\n‚îÇ   ‚îî‚îÄ‚îÄ altinn/\n‚îÇ   ‚îî‚îÄ‚îÄ freg/\n‚îÇ...\n       \n\n\n\nMappestrukturen til h√∏yre over viser hvordan vi mappestrukturen ser ut i kildeb√∏tta, mens config.yaml-fila til venstre viser hvordan vi angir at Kildomaten kun skal trigges p√• nye filer som oppst√•r i undermappen ssb-dapla-example-data-kilde-prod/ledstill/altinn/. Vi bruker n√∏kkelen folder_prefix for √• angi hvilken sti i kildeb√∏tta som tjenesten skal trigges p√•. N√∏kkelen memory_size lar brukeren angi hvor mye minne og CPU hver prosessering skal f√•.\n\n\n\n\n\n\nHvor mye minne og CPU er mulig?\n\n\n\nSom standard s√• f√•r hver prosessering med Kildomaten 1 CPU-kjerne og 512MB minne/RAM. Hvis man skal gj√∏re mye prossesering, eller filene som prosesseres er veldig store, kan man sette memory_size opp til max 32GB minne. Antall CPU-kjerner blir satt implisitt ut i fra valg av memory_size iht til begrensningene som Google setter for Cloud Run. Se de n√∏yaktige verdiene som blir satt her.\n\n\n\n\n\n\n\n\n\n\n\nHusk dette n√•r du skriver skriptet ditt\n\n\n\nN√•r du skal skrive et Python-skript for Kildomaten er det spesielt viktig √• huske p√• 2 ting:\n\nSkriptet ditt kommer til √• bli kj√∏rt p√• en-og-en fil.\nSkriptet ditt m√• skrive ut et unikt navn p√• filen som skal skrives til produktb√∏tta, ellers risikerer du at filer blir overskrevet.\nPython-pakkene som kan benyttes er definert her. Ved behov for andre pakker, ta kontakt med Kundeservice.\n\n\n\n\nPython-skriptet er der teamet kan angi hva slags kode som skal kj√∏re p√• hver fil som dukker opp i den angitte mappen i kildeb√∏tta. For at dette skal v√¶re mulig m√• koden f√∏lge disse reglene:\n\nKoden m√• ligge i en fil som heter process_source_data.py.\nKoden m√• pakkes inn i en funksjon som heter main().\n\nmain() er en funksjon med ett parameter som heter source_file som du alltid f√•r av Kildomaten n√•r en fil blir prosessert. N√•r du skriver koden kan man derfor anta at dette parameteret blir gitt til funksjonen, og du kan benytte det inne i main-funksjonen.\nsource_file-parameteret gir main() en ferdig definert filsti til filen som skal prosesseres. For eksempel s√• kan source_file ha verdien\ngs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv hvis filen test20231010.csv dukker opp i ssb-dapla-example-data-kilde-prod/ledstill/altinn/.\nHvis vi fortsetter eksempelet fra tidligere s√• ser mappen i IaC-repoet v√•rt slik ut n√•:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\n‚îú‚îÄ‚îÄ automation/\n‚îÇ   ‚îî‚îÄ‚îÄ source-data/\n‚îÇ       ‚îú‚îÄ‚îÄ dapla-example-prod/\n‚îÇ           ‚îî‚îÄ‚îÄ altinn/\n‚îÇ               ‚îú‚îÄ‚îÄ config.yaml\n‚îÇ               ‚îî‚îÄ‚îÄ process_source_data.py\n‚îÇ\n‚îÇ...         \n\nVi ser n√• at filene config.yaml og process_source_data.py ligger i mappen\nautomation/source-data/dapla-example-prod/ledstill/altinn/. Senere, n√•r vi har rullet ut tjenesten, vil en ny fil i gs://ssb-dapla-exmaple-data-kilde-prod/ledstill/altinn/ trigge Kildomaten og kj√∏re koden i process_source_data.py p√• filen.\nUnder ser du et eksempel p√• hvordan en vanlig kodesnutt kan konverteres til √• kj√∏re i Kildomaten:\n\n\n\n\nVanlig kode\n\nimport dapla as dp\n\n# Stien til filen\nsource_file = \"gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv\"\n\ndf = dp.read_pandas(source_file, file_type=\"csv\")\n\n# Dataminimerer ved √• velge kolonner\ndf2 = df[['col1', 'col2', 'col3']]\n\n# Skriver ut fil til produktb√∏tta\ndp.write_pandas(df2, \n            \"gs://ssb-dapla-example-data-produkt-prod/test20231010.parquet\")\n\n\n\n\n\n\nKode i Kildomaten\n\nimport dapla as dp\n\ndef main(source_file):\n    df = dp.read_pandas(source_file, file_type=\"csv\")\n\n# Dataminimerer ved √• velge kolonner\n    df2 = df[['col1', 'col2', 'col3']]\n\n# Bytter ut \"kilde\" med \"produkt\" i filstien\n    new_path = source_file.replace(\"kilde\", \"produkt\")\n\n# Bytter ut \".csv\" med \".parquet\" i filstien\n    new_path2 = new_path.replace(\".csv\", \".parquet\")\n\n# Skriver ut fil til produktb√∏tta\n    dp.write_pandas(df2, new_path2)\n\n\n\nDe to kodesnuttene over gir det samme resultatet, bare at koden til venstre kj√∏res som vanlig python-kode, mens koden til h√∏yre kj√∏res i Kildomaten. Som vi ser av koden til h√∏yre s√• trenger vi aldri √• hardkode inn filstier i Kildomaten. Tjenesten gir oss filstien, og vi kan bruke den til √• skrive ut filen til produktb√∏tta.\nStrukturen p√• filene som skrives b√∏r tenkes n√∏ye gjennom n√•r man automatiseres prosessering av data. Hvis vi ikke har unike filnavn eller stier s√• kan du risikere at filer blir overskrevet. Typisk er dette allerede tenkt p√• n√•r filer skrives til kildeb√∏tta, s√• hvis man kopierer den strukturen, slik vi gjorde i eksempelet over, s√• vil det ikke v√¶re noe problem.\n\n\n\n\n\n\nHvilke Python-biblioteker kan jeg bruke?\n\n\n\nDu kan kun bruke biblioteker som er forh√•ndsinstallert i Kildomaten. Du kan se en liste over disse bibliotekene her. √ònsker du andre biblioteker s√• m√• du ta kontakt med Kunderservice.\n\n\n\n\n\n\n\n\n\n\n\n\nTilgang til kildedata\n\n\n\nTilgang til kildedata i prod-milj√∏et er det kun gruppen data-admins som kan aktivere ved √• bruke tilgangsstyringsl√∏sningen Just-in-Time Access (JIT). Les mer om hvordan JIT-l√∏sningen fungerer her. √ònsker man √• kunne liste ut innhold fra b√∏tta m√• man aktivere rollen ssb.buckets.list. √ònsker man i tillegg √• lese/skrive til b√∏tta m√• man ogs√• aktivere ssb.bucket.write. Tilgang til kildeb√∏tta i test-milj√∏et krever ikke JIT-tilgang.\n\n\nF√∏r man ruller ut koden i tjenesten er det greit √• teste at alt fungerer som det skal i Jupyterlab. Hvis vi tar utgangspunkt i eksempelet over s√• kan vi teste koden ved √• kj√∏re f√∏lgende nederst i skriptet:\n\n\nprocess_source_data.py\n\nmain(\"gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv\")\n\nN√•r tjenesten er rullet ut s√• vil det v√¶re dette som kj√∏res n√•r en fil dukker opp i gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/. Ved √• kj√∏re det manuelt p√• denne m√•ten f√•r vi sett at ting fungerer som det skal.\nHusk √• fjerne kj√∏ringen av koden f√∏r du ruller ut tjenesten.\n\n\n\n\n\n\nPseudonymisering kan ikke kj√∏res fra en IDE i prod-milj√∏et\n\n\n\nDu kan teste kode fra Jupyter og andre IDE-er i prod-milj√∏et p√• Dapla. Men hvis prosesseringen inneb√¶rer bruk av pseudonymisering, s√• vil den ikke kunne kalles fra programmeringsmilj√∏er som Jupyter. Grunnen til dette er at det ikke er √∏nskelig √• gj√∏re det lett √• se upseudonymisert og pseudonymisert data samtidig. Hvis du √∏nsker √• teste prosesseringen av pseudo-tjenesten, s√• kan du gj√∏re med testdata i test-milj√∏et.\n\n\n\n\n\nFor √• rulle ut tjenesten gj√∏r du f√∏lgende;\n\nPush branchen til GitHub og opprette en pull request.\nPull request m√• godkjennes av en data-admins p√• teamet.\n\n\n\n\nN√•r pull request er godkjent s√• sjekker du om alle tester, planer og utrullinger var vellykket, slik som vist i Figur¬†1.\nHvis alt er vellykket s√• kan du merge branchen inn i main-branchen.\n\n\n\n\n\n\n\n\n\nFigur¬†1: Suksessfulle tester p√• GitHub\n\n\n\n\n\nEtter at du har merget inn i main kan du f√∏lge med p√• utrullingen under Actions-fanen i repoet. N√•r den siste jobben lyser gr√∏nt er Kildomaten rullet ut og klar til bruk.\n\n\n\n\n\n\nVanlige feil ved utrulling\n\n\n\nFor √• gi raskt tilbakemelding p√• noen mulige feilsituasjoner, s√• kj√∏res det enkel validering p√• config.yaml og process_source_data.py n√•r en Pull request er opprettet. F√∏lgende validering gjennomf√∏res:\n\nEr det et python-script kalt process_source_data.py?\nEr det en config.yaml med en gyldig folder_prefix: definert?\nBruker process_source_data.py biblioteker som ikke er installert i Kildomaten?\nHar koden i process_source_data.py feil iht Pyflakes?\n\nDet kan ogs√• forekomme at Atlantis, verkt√∏yet for √• rulle ut endringer fra IaC-repoet til GCP, feiler. Da kan du pr√∏ve √• skrive atlantis plan i kommentarfeltet til pull request‚Äôen, og testene vil kj√∏re p√• nytt. Hvis det fortsatt ikke fungerer s√• kontakter man Dapla kundeservice.\n\n\n\n\n\nN√•r du har rullet ut tjenesten kan teste tjenesten ved at data-admins flytter en fil til en den filstien Kildomaten er satt opp for. I eksempelet vi har brukt tidligere, gj√∏r du f√∏lgende: 1. Aktiverer JIT-tilgangen til kildedata (som beskrevet over). 2. Kopier en fil over til filstien:\ngs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/ 3. Sjekk om filen ble skrevet til √∏nsket mappe i produktb√∏tta.\nDu kan ogs√• sjekke logger og monitorere Kildomaten. Les mer i neste avsnitt.\n\n\n\nN√•r en kilde i Kildomaten er satt opp og ruller ut, kan du monitere tjenesten i Google Cloud Console (GCC) ved √• gj√∏re f√∏lgende:\n\nLogg deg inn med SSB-bruker p√• GCC.\nVelg standardprosjektet i prosjektvelgeren3.\nS√∏k opp Cloud Run i s√∏kefeltet p√• toppen av siden og g√• inn p√• siden.\n\nP√• siden til Cloud Run vil du se en oversikt over alle kilder teamet har kj√∏rende i Kildomaten. De har formen source-&lt;kildenavn&gt;-processor. I eksempelet med team dapla-example tidligere, vil man da se en kilde som heter source-altinn-processor, siden mappen de opprettet under\nautomation/source-data-dapla-example-prod/ i IaC-repoet heter altinn.\nTrykker man seg inn p√• hver enkelt kilde vil man kunne monitorere aktiviteten i kilden, og man kan trykke seg videre for √• se loggene.\n\n\n\n\n\n\nSjekke logger\n\n\n\nDet er anbefalt √• se p√• Kildomaten-loggene i Logs Explorer. Det kan man enkelt gj√∏re ved √• trykke p√• ‚ÄúView in Logs Explorer‚Äù som vist p√• bildet under:\n\n\n\n√Öpne Kildomaten-loggene i Logs Explorer\n\n\n\n\n\n\n\nKildomaten er satt opp for √• kunne prosessere hver kilde i 5 parallelle intanser samtidig. F.eks. hvis det oppst√•r 10 nye filer i en mappe som trigger en Kildomaten-kilde, s√• kan det prosesseres 5 filer samtidig. Ta kontakt med Kundeservice hvis man har behov for ytterlige skalering.\n\n\n\nKildomaten tilbyr e-postvarsling til teamet n√•r tjenesten feiler. Opprett en Kundeservice-sak for √• f√• satt opp e-postvarsling for teamet ditt.\n\n\n\nMan kan sette opp s√• mange kilder man √∏nsker. Men n√•r man setter det opp er det viktig √• huske at alle kildene m√• spesifiseres rett under automation/source-data/&lt;teamnavn&gt;-&lt;milj√∏&gt;/. Her er et eksempel p√• hvordan team dapla-example har to kilder i Kildomaten:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n‚îú‚îÄ‚îÄ automation/\n‚îÇ   ‚îî‚îÄ‚îÄ source-data/\n‚îÇ       ‚îú‚îÄ‚îÄ dapla-example-prod/\n‚îÇ           ‚îî‚îÄ‚îÄ altinn/\n‚îÇ               ‚îú‚îÄ‚îÄ config.yaml\n‚îÇ               ‚îî‚îÄ‚îÄ process_source_data.py\n‚îÇ           ‚îî‚îÄ‚îÄ ledstill/\n‚îÇ               ‚îú‚îÄ‚îÄ config.yaml\n‚îÇ               ‚îî‚îÄ‚îÄ process_source_data.py\n‚îÇ...           \n\nI eksempelet over ser vi det er to kilder: altinn og ledstill. Hver kilde trigges p√• ulike filstier i kildeb√∏tta, og python-koden som kj√∏res kan v√¶re ulik mellom kilder.\n\n\n\nI eksempelet som er brukt i dette kapitlet er Kildomaten satt opp i prodmilj√∏et til teamet. Mens egentlig burde man teste ut nye kilder i teamets test-milj√∏. Kildomaten er ikke satt opp i test-milj√∏et som standard, og derfor m√• det skrus p√• f√∏r man kan anvende det. Teamet kan gj√∏re det selv ved √• f√∏lge denne beskrivelsen, eller de kan ta kontakt med Kundeservice og f√• hjelp til dette.\nEn av de store fordelene med √• sette opp Kildomaten-kilder i test-milj√∏et f√∏r man gj√∏r det i prod-milj√∏et, er at tilgangsstyringen til data er mye mindre streng. Det gj√∏r det lettere for alle i teamet √• utvikle koden som skal benyttes.\nN√•r man skal sette opp Kildomaten i test-milj√∏et s√• f√∏lger det samme oppskrift som vi har vist for prod-milj√∏et over. Den store forskjellen er at test-kilder skal legges under en egen mappe under automation/source-data/ i teamets IaC-repo. Eksempelet under med team dapla-example viser hvordan man kan sette opp kildene altinn og ledstill for b√•de prod- og test-testmilj√∏et:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n‚îú‚îÄ‚îÄ automation/\n‚îÇ   ‚îî‚îÄ‚îÄ source-data/\n‚îÇ       ‚îú‚îÄ‚îÄ dapla-example-prod/\n‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ altinn/\n‚îÇ       ‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ config.yaml\n‚îÇ       ‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ process_source_data.py\n‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ ledstill/\n‚îÇ       ‚îÇ               ‚îú‚îÄ‚îÄ config.yaml\n‚îÇ       ‚îÇ               ‚îî‚îÄ‚îÄ process_source_data.py\n‚îÇ       ‚îú‚îÄ‚îÄ dapla-example-test/\n‚îÇ               ‚îú‚îÄ‚îÄ altinn/\n‚îÇ               ‚îÇ       ‚îú‚îÄ‚îÄ config.yaml\n‚îÇ               ‚îÇ       ‚îî‚îÄ‚îÄ process_source_data.py\n‚îÇ               ‚îî‚îÄ‚îÄ ledstill/\n‚îÇ                       ‚îú‚îÄ‚îÄ config.yaml\n‚îÇ                       ‚îî‚îÄ‚îÄ process_source_data.py\n‚îÇ...           \n\nSom vi ser av mappestrukturen over s√• er det to mapper under\nautomation/source-data:\n\ndapla-example-prod\ndapla-example-test\n\nDisse to mappene angir om det er henholdsvis prod- eller test-milj√∏et vi setter opp kilder for.\n\n\n\nKildomaten er bygget for √• trigge p√• nye filer som oppst√•r i en gitt filsti. Men noen ganger er det n√∏dvendig √• trigge kj√∏ring av alle filer p√• nytt. Noen ganger √∏nsker man kanskje √• kun trigge noen filer for en gitt kilde. Dette kan gj√∏res med en funksjon i Python-pakken dapla-toolbelt.\nF√∏r du kan gj√∏re dette trenger du f√∏lgende informasjon:\n\nproject-id for standardprosjektet. Slik finner du prosjekt-id. Merk at dette ikke er kilde-prosjektet!\nfolder_prefix som du √∏nsker at koden skal trigges p√•. Dette fungerer likt som tidligere forklart for config.yaml, men her har du ogs√• mulighet til √• kunne trigge prosesseringen p√• et undermappe av stien som var satt i kildens config.yaml.\nsource_name er navnet p√• kilden. Navnet p√• kilden i eksempelet med team dapla-example var altinn.\n\nHer er et kodeeksempel som viser hvordan man kan trigge prossesering av alle filer for kilde\n\n\nnotebook\n\nfrom dapla import trigger_source_data_processing\n\nproject_id = \"dapla-example-p\"\nsource_name = \"altinn\"\nfolder_prefix = \"ledstill/altinn/ra0678\"\n\ntrigger_source_data_processing(project_id, source_name, folder_prefix, kuben=True)          \n\nI eksempelet over trigger vi scriptet som er satt opp for kilden altinn til √• kj√∏re p√• alle undermapper av\nssb-dapla-example-data-kilde-prod/ledstill/altinn/ra0678/.\n\n\n\n\nN√•r tjenesten er rullet ut s√• vil den kj√∏re automatisk p√• alle filer som dukker opp i filsti i kildeb√∏tta. Etter hvert vil det v√¶re behov for √• endre p√• skript, endre filstier som skal trigge tjenesten, eller prosessere alle filer p√• nytt. I denne delen forklarer vi hvordan du g√•r frem for √• gj√∏re dette.\n\n\nAlle p√• team kan endre p√• skriptet, men det er data-admins som m√• godkjenne endringene f√∏r de blir rullet ut. For √• endre skriptet gj√∏r du f√∏lgende:\n\nKlon repoet.\nGj√∏r endringene du √∏nsker i en branch.\nPush opp endringene til GitHub og opprett en pull request.\nF√• en data-admins p√• teamet til √• godkjenne endringene.\nN√•r endringene er godkjent s√• kan du merge inn i main-branchen.\n\nEn endring i Python-skriptet krever ikke at tjenesten rulles ut p√• nytt. Derfor er det ikke like mange tester og kj√∏ringer som gj√∏res som n√•r man oppretter en helt ny kilde.\n\n\n\nAlle p√• teamet kan gj√∏re endringer i config.yaml, men det er data-admins som m√• godkjenne endringene f√∏r de blir rullet ut. For √• endre config.yaml gj√∏r du f√∏lgende:\n\nKlon repoet.\nGj√∏r endringene du √∏nsker i en branch.\nPush opp endringene til GitHub og opprett en pull request.\nF√• en data-admins p√• teamet til √• godkjenne endringene.\nN√•r endringene er godkjent s√• kan du merge inn i main-branchen.\n\nEn endring i config.yaml krever at tjenesten rulles ut p√• nytt og tar derfor litt mer tid enn en endring i Python-skriptet.",
    "crumbs": [
      "Manual",
      "Datatjenester"
    ]
  },
  {
    "objectID": "statistikkere/kildomaten.html#forberedelser",
    "href": "statistikkere/kildomaten.html#forberedelser",
    "title": "Kildomaten",
    "section": "",
    "text": "F√∏r et Dapla-team kan ta i bruk Kildomaten m√• man tjenesten aktivert for teamet. Som standard f√•r alle statistikkteam dette skrudd p√• i prod-milj√∏et som opprettes for teamet. √ònsker du √• aktivere Kildomaten i test-milj√∏et kan dette gj√∏res selvbetjent som en feature. Alternativt kan man opprette en Kundeservice-sak og be om √• hjelp til dette.",
    "crumbs": [
      "Manual",
      "Datatjenester"
    ]
  },
  {
    "objectID": "statistikkere/kildomaten.html#sette-opp-tjenesten",
    "href": "statistikkere/kildomaten.html#sette-opp-tjenesten",
    "title": "Kildomaten",
    "section": "",
    "text": "I denne delen bryter vi ned prosessen med √• sette opp Kildomaten i de stegene vi mener det er hensiktsmessig √• gj√∏re det n√•r den settes opp for f√∏rste gang p√• en enkeltkilde. Senere i kapitlet forklarer vi hvordan man setter den opp flere kilder og hvordan man vedlikeholder tjenesten.\nHusk at alle p√• teamet kan gj√∏re det meste av arbeidet her, men det er data-admins som m√• godkjenne at tjenesten rulles ut1.\n\n\nOppsett av Kildomaten gj√∏res i teamets IaC-repo2. N√•r vi skal sette opp Kildomaten-kilde m√• vi gj√∏re gj√∏re endringer i teamets IaC-repo. Man finner teamets IaC-repo ved g√• inn p√• SSBs GitHub-organisasjon og s√∏ke etter repoet som heter &lt;teamnavn&gt;-iac. N√•r du har funnet repoet s√• kan du gj√∏re f√∏lgende:\n\nKlon ned ditt teams Iac-repo git clone &lt;repo-url&gt;\nOpprett en ny branch: git checkout -b add-kildomaten-source\n\n\n\n\nFor at Kildomaten skal fungere s√• m√• det opprettes en bestemt mappestruktur i IaC-repoet til teamet. N√•r et team blir opprettet vil den grunnleggende mappestrukturen i IaC-repoet allerede v√¶re opprettet for prod-milj√∏et til teamet. F.eks. vil mappestrukturen se slik ut for team dapla-example:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n‚îú‚îÄ‚îÄ automation/\n‚îÇ   ‚îî‚îÄ‚îÄ source-data/\n‚îÇ       ‚îú‚îÄ‚îÄ dapla-example-prod/\n‚îÇ           ‚îî‚îÄ‚îÄ README.md\n‚îÇ \n‚îÇ...           \n\nSkal du sette opp Kildomaten i prod-milj√∏et s√• kan du f√∏lge oppskriften som kommer senere i kapitlet uten √• gj√∏re noe mer enda.\nSkal du ogs√• bruke Kildomaten i test-milj√∏et s√• m√• opprette en ny mappe og lage en PR i IaC-repoet til teamet. Da vil strukturen se slik ut:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n‚îú‚îÄ‚îÄ automation/\n‚îÇ   ‚îî‚îÄ‚îÄ source-data/\n‚îÇ       ‚îú‚îÄ‚îÄ dapla-example-prod/\n‚îÇ       ‚îÇ    ‚îî‚îÄ‚îÄ README.md\n‚îÇ       ‚îú‚îÄ‚îÄ dapla-example-test/ \n‚îÇ...           \n\nI mappestrukturen over s√• har vi klargjort den grunnleggende mappestrukturen for √• ta i bruk Kildomaten i prod- og test-milj√∏et. Neste steg blir √• legge de ulike kildene som egne mapper under dapla-example-prod og dapla-example-test. Det viser vi i neste avsnitt.\n\n\n\nKildomaten lar deg prosessere ulike filstier i kildeb√∏tta med ulike python-script. Dette refereres til som at Kildomaten har flere kilder. For √• sette opp en kilde s√• m√• man f√∏lge en definert mappestruktur i IaC-repoet der alle kildene ligger rett under &lt;teamnavn&gt;-prod- eller &lt;teamnavn&gt;-test-mappen. Du kan ikke ha undermapper under en kilde. Du velger selv navnet p√• kildene/mappene i IaC-repoet, og det vil v√¶re navnet p√• kildene i Kildomaten. Senere i kapitlet ser vi at vi m√• bruke navnet for trigge re-kj√∏ring av kilder.\nUnder er et eksempel p√• hvordan det kan se ut for eksempel-teamet dapla-example:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n‚îú‚îÄ‚îÄ automation/\n‚îÇ   ‚îî‚îÄ‚îÄ source-data/\n‚îÇ       ‚îú‚îÄ‚îÄ dapla-example-prod/\n‚îÇ       ‚îÇ    ‚îî‚îÄ‚îÄ altinn\n‚îÇ       ‚îÇ    ‚îî‚îÄ‚îÄ ameld\n‚îÇ       ‚îú‚îÄ‚îÄ dapla-example-test/\n‚îÇ       ‚îÇ    ‚îî‚îÄ‚îÄ altinn\n‚îÇ       ‚îÇ    ‚îî‚îÄ‚îÄ ameld\n‚îÇ       ‚îÇ    ‚îî‚îÄ‚îÄ nudb\n‚îÇ...           \n\nI eksempelet over ser vi at det er opprettet kildene altinn og ameld for b√•de test- og prod-milj√∏et. I tillegg er det i test-milj√∏et kj√∏rende en annen kilde som heter nudb. Hver av disse kildene kan kj√∏re et eget Pyton-script p√• alle filer som skrives til en gitt filsti som man definerer selv.\n\n\n\n\n\n\nAlle kilder p√• samme niv√•\n\n\n\nEt team kan sette opp mange kilder som skal prosesseres med ulike skript. Men Kildomaten tillater bare et niv√• under automation/source-data-&lt;teamnavn&gt;-&lt;milj√∏&gt;/. Det vil si at du ikke kan ha undermapper under en kilde. Det er ogs√• slik at man alltid m√• opprette en mappe for en kilde, selv om du kun har en kilde. F.eks. kan man ikke droppe mappen altinn i eksempelet over.\n\n\n\n\n\nN√•r mappestrukturen er opprettet kan man legge til filene som beskriver hvilke filer som skal prosesseres, og python-skriptet med koden som skal prosessere filene. Det er to filer som m√• eksistere for at tjenesten skal fungere:\n\nEn konfigurasjonsfil\nEt Python-skript\n\nN√•r du har opprettet de skal de ligge p√• denne m√•ten i IaC-repoet:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n‚îú‚îÄ‚îÄ automation/\n‚îÇ   ‚îî‚îÄ‚îÄ source-data/\n‚îÇ       ‚îú‚îÄ‚îÄ dapla-example-prod/\n‚îÇ           ‚îî‚îÄ‚îÄ altinn/\n‚îÇ               ‚îú‚îÄ‚îÄ config.yaml\n‚îÇ               ‚îî‚îÄ‚îÄ process_source_data.py\n‚îÇ...           \n\nUnder forklares hvordan skriver innholdet i de to filene.\n\n\nKildomaten trigges ved at det oppst√•r nye filer i kildeb√∏tta til teamet. Hvorvidt den skal trigges p√• alle filer, eller kun filer i en gitt undermappe, bestemmer brukeren ved √• konfigurere tjenesten i config.yaml. Her kan du ogs√• angi hvor mye ressurser prosesseringen skal f√•.\nHvis vi fortsetter eksempelet v√•rt fra tidligere med dapla-example, s√• kan vi tenkes oss at teamet √∏nsker √• Kildomaten skal trigges p√• alle filer som oppst√•r i kildeb√∏tta under filstien:\nssb-dapla-example-data-kilde-prod/ledstill/altinn/.\nFor √• konfigurere tjenesten i Kildomaten m√• vi legge til en fil som heter config.yaml under automation/source-data/dapla-example-prod/altinn/, slik som vist i forrige kapitel. I dette tilfellet vil den ha innholdet som vist til venstre under:\n\n\n\n\nconfig.yaml\n\nfolder_prefix: ledstill/altinn\nmemory_size: 1  # GiB\n\n\n\n\n\n\nssb-dapla-example-data-kilde-prod/\n\n‚îú‚îÄ‚îÄ ledstill/\n‚îÇ   ‚îî‚îÄ‚îÄ altinn/\n‚îÇ   ‚îî‚îÄ‚îÄ aordningen/\n‚îú‚îÄ‚îÄ sykefra/\n‚îÇ   ‚îî‚îÄ‚îÄ altinn/\n‚îÇ   ‚îî‚îÄ‚îÄ freg/\n‚îÇ...\n       \n\n\n\nMappestrukturen til h√∏yre over viser hvordan vi mappestrukturen ser ut i kildeb√∏tta, mens config.yaml-fila til venstre viser hvordan vi angir at Kildomaten kun skal trigges p√• nye filer som oppst√•r i undermappen ssb-dapla-example-data-kilde-prod/ledstill/altinn/. Vi bruker n√∏kkelen folder_prefix for √• angi hvilken sti i kildeb√∏tta som tjenesten skal trigges p√•. N√∏kkelen memory_size lar brukeren angi hvor mye minne og CPU hver prosessering skal f√•.\n\n\n\n\n\n\nHvor mye minne og CPU er mulig?\n\n\n\nSom standard s√• f√•r hver prosessering med Kildomaten 1 CPU-kjerne og 512MB minne/RAM. Hvis man skal gj√∏re mye prossesering, eller filene som prosesseres er veldig store, kan man sette memory_size opp til max 32GB minne. Antall CPU-kjerner blir satt implisitt ut i fra valg av memory_size iht til begrensningene som Google setter for Cloud Run. Se de n√∏yaktige verdiene som blir satt her.\n\n\n\n\n\n\n\n\n\n\n\nHusk dette n√•r du skriver skriptet ditt\n\n\n\nN√•r du skal skrive et Python-skript for Kildomaten er det spesielt viktig √• huske p√• 2 ting:\n\nSkriptet ditt kommer til √• bli kj√∏rt p√• en-og-en fil.\nSkriptet ditt m√• skrive ut et unikt navn p√• filen som skal skrives til produktb√∏tta, ellers risikerer du at filer blir overskrevet.\nPython-pakkene som kan benyttes er definert her. Ved behov for andre pakker, ta kontakt med Kundeservice.\n\n\n\n\nPython-skriptet er der teamet kan angi hva slags kode som skal kj√∏re p√• hver fil som dukker opp i den angitte mappen i kildeb√∏tta. For at dette skal v√¶re mulig m√• koden f√∏lge disse reglene:\n\nKoden m√• ligge i en fil som heter process_source_data.py.\nKoden m√• pakkes inn i en funksjon som heter main().\n\nmain() er en funksjon med ett parameter som heter source_file som du alltid f√•r av Kildomaten n√•r en fil blir prosessert. N√•r du skriver koden kan man derfor anta at dette parameteret blir gitt til funksjonen, og du kan benytte det inne i main-funksjonen.\nsource_file-parameteret gir main() en ferdig definert filsti til filen som skal prosesseres. For eksempel s√• kan source_file ha verdien\ngs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv hvis filen test20231010.csv dukker opp i ssb-dapla-example-data-kilde-prod/ledstill/altinn/.\nHvis vi fortsetter eksempelet fra tidligere s√• ser mappen i IaC-repoet v√•rt slik ut n√•:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\n‚îú‚îÄ‚îÄ automation/\n‚îÇ   ‚îî‚îÄ‚îÄ source-data/\n‚îÇ       ‚îú‚îÄ‚îÄ dapla-example-prod/\n‚îÇ           ‚îî‚îÄ‚îÄ altinn/\n‚îÇ               ‚îú‚îÄ‚îÄ config.yaml\n‚îÇ               ‚îî‚îÄ‚îÄ process_source_data.py\n‚îÇ\n‚îÇ...         \n\nVi ser n√• at filene config.yaml og process_source_data.py ligger i mappen\nautomation/source-data/dapla-example-prod/ledstill/altinn/. Senere, n√•r vi har rullet ut tjenesten, vil en ny fil i gs://ssb-dapla-exmaple-data-kilde-prod/ledstill/altinn/ trigge Kildomaten og kj√∏re koden i process_source_data.py p√• filen.\nUnder ser du et eksempel p√• hvordan en vanlig kodesnutt kan konverteres til √• kj√∏re i Kildomaten:\n\n\n\n\nVanlig kode\n\nimport dapla as dp\n\n# Stien til filen\nsource_file = \"gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv\"\n\ndf = dp.read_pandas(source_file, file_type=\"csv\")\n\n# Dataminimerer ved √• velge kolonner\ndf2 = df[['col1', 'col2', 'col3']]\n\n# Skriver ut fil til produktb√∏tta\ndp.write_pandas(df2, \n            \"gs://ssb-dapla-example-data-produkt-prod/test20231010.parquet\")\n\n\n\n\n\n\nKode i Kildomaten\n\nimport dapla as dp\n\ndef main(source_file):\n    df = dp.read_pandas(source_file, file_type=\"csv\")\n\n# Dataminimerer ved √• velge kolonner\n    df2 = df[['col1', 'col2', 'col3']]\n\n# Bytter ut \"kilde\" med \"produkt\" i filstien\n    new_path = source_file.replace(\"kilde\", \"produkt\")\n\n# Bytter ut \".csv\" med \".parquet\" i filstien\n    new_path2 = new_path.replace(\".csv\", \".parquet\")\n\n# Skriver ut fil til produktb√∏tta\n    dp.write_pandas(df2, new_path2)\n\n\n\nDe to kodesnuttene over gir det samme resultatet, bare at koden til venstre kj√∏res som vanlig python-kode, mens koden til h√∏yre kj√∏res i Kildomaten. Som vi ser av koden til h√∏yre s√• trenger vi aldri √• hardkode inn filstier i Kildomaten. Tjenesten gir oss filstien, og vi kan bruke den til √• skrive ut filen til produktb√∏tta.\nStrukturen p√• filene som skrives b√∏r tenkes n√∏ye gjennom n√•r man automatiseres prosessering av data. Hvis vi ikke har unike filnavn eller stier s√• kan du risikere at filer blir overskrevet. Typisk er dette allerede tenkt p√• n√•r filer skrives til kildeb√∏tta, s√• hvis man kopierer den strukturen, slik vi gjorde i eksempelet over, s√• vil det ikke v√¶re noe problem.\n\n\n\n\n\n\nHvilke Python-biblioteker kan jeg bruke?\n\n\n\nDu kan kun bruke biblioteker som er forh√•ndsinstallert i Kildomaten. Du kan se en liste over disse bibliotekene her. √ònsker du andre biblioteker s√• m√• du ta kontakt med Kunderservice.\n\n\n\n\n\n\n\n\n\n\n\n\nTilgang til kildedata\n\n\n\nTilgang til kildedata i prod-milj√∏et er det kun gruppen data-admins som kan aktivere ved √• bruke tilgangsstyringsl√∏sningen Just-in-Time Access (JIT). Les mer om hvordan JIT-l√∏sningen fungerer her. √ònsker man √• kunne liste ut innhold fra b√∏tta m√• man aktivere rollen ssb.buckets.list. √ònsker man i tillegg √• lese/skrive til b√∏tta m√• man ogs√• aktivere ssb.bucket.write. Tilgang til kildeb√∏tta i test-milj√∏et krever ikke JIT-tilgang.\n\n\nF√∏r man ruller ut koden i tjenesten er det greit √• teste at alt fungerer som det skal i Jupyterlab. Hvis vi tar utgangspunkt i eksempelet over s√• kan vi teste koden ved √• kj√∏re f√∏lgende nederst i skriptet:\n\n\nprocess_source_data.py\n\nmain(\"gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv\")\n\nN√•r tjenesten er rullet ut s√• vil det v√¶re dette som kj√∏res n√•r en fil dukker opp i gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/. Ved √• kj√∏re det manuelt p√• denne m√•ten f√•r vi sett at ting fungerer som det skal.\nHusk √• fjerne kj√∏ringen av koden f√∏r du ruller ut tjenesten.\n\n\n\n\n\n\nPseudonymisering kan ikke kj√∏res fra en IDE i prod-milj√∏et\n\n\n\nDu kan teste kode fra Jupyter og andre IDE-er i prod-milj√∏et p√• Dapla. Men hvis prosesseringen inneb√¶rer bruk av pseudonymisering, s√• vil den ikke kunne kalles fra programmeringsmilj√∏er som Jupyter. Grunnen til dette er at det ikke er √∏nskelig √• gj√∏re det lett √• se upseudonymisert og pseudonymisert data samtidig. Hvis du √∏nsker √• teste prosesseringen av pseudo-tjenesten, s√• kan du gj√∏re med testdata i test-milj√∏et.\n\n\n\n\n\nFor √• rulle ut tjenesten gj√∏r du f√∏lgende;\n\nPush branchen til GitHub og opprette en pull request.\nPull request m√• godkjennes av en data-admins p√• teamet.\n\n\n\n\nN√•r pull request er godkjent s√• sjekker du om alle tester, planer og utrullinger var vellykket, slik som vist i Figur¬†1.\nHvis alt er vellykket s√• kan du merge branchen inn i main-branchen.\n\n\n\n\n\n\n\n\n\nFigur¬†1: Suksessfulle tester p√• GitHub\n\n\n\n\n\nEtter at du har merget inn i main kan du f√∏lge med p√• utrullingen under Actions-fanen i repoet. N√•r den siste jobben lyser gr√∏nt er Kildomaten rullet ut og klar til bruk.\n\n\n\n\n\n\nVanlige feil ved utrulling\n\n\n\nFor √• gi raskt tilbakemelding p√• noen mulige feilsituasjoner, s√• kj√∏res det enkel validering p√• config.yaml og process_source_data.py n√•r en Pull request er opprettet. F√∏lgende validering gjennomf√∏res:\n\nEr det et python-script kalt process_source_data.py?\nEr det en config.yaml med en gyldig folder_prefix: definert?\nBruker process_source_data.py biblioteker som ikke er installert i Kildomaten?\nHar koden i process_source_data.py feil iht Pyflakes?\n\nDet kan ogs√• forekomme at Atlantis, verkt√∏yet for √• rulle ut endringer fra IaC-repoet til GCP, feiler. Da kan du pr√∏ve √• skrive atlantis plan i kommentarfeltet til pull request‚Äôen, og testene vil kj√∏re p√• nytt. Hvis det fortsatt ikke fungerer s√• kontakter man Dapla kundeservice.\n\n\n\n\n\nN√•r du har rullet ut tjenesten kan teste tjenesten ved at data-admins flytter en fil til en den filstien Kildomaten er satt opp for. I eksempelet vi har brukt tidligere, gj√∏r du f√∏lgende: 1. Aktiverer JIT-tilgangen til kildedata (som beskrevet over). 2. Kopier en fil over til filstien:\ngs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/ 3. Sjekk om filen ble skrevet til √∏nsket mappe i produktb√∏tta.\nDu kan ogs√• sjekke logger og monitorere Kildomaten. Les mer i neste avsnitt.\n\n\n\nN√•r en kilde i Kildomaten er satt opp og ruller ut, kan du monitere tjenesten i Google Cloud Console (GCC) ved √• gj√∏re f√∏lgende:\n\nLogg deg inn med SSB-bruker p√• GCC.\nVelg standardprosjektet i prosjektvelgeren3.\nS√∏k opp Cloud Run i s√∏kefeltet p√• toppen av siden og g√• inn p√• siden.\n\nP√• siden til Cloud Run vil du se en oversikt over alle kilder teamet har kj√∏rende i Kildomaten. De har formen source-&lt;kildenavn&gt;-processor. I eksempelet med team dapla-example tidligere, vil man da se en kilde som heter source-altinn-processor, siden mappen de opprettet under\nautomation/source-data-dapla-example-prod/ i IaC-repoet heter altinn.\nTrykker man seg inn p√• hver enkelt kilde vil man kunne monitorere aktiviteten i kilden, og man kan trykke seg videre for √• se loggene.\n\n\n\n\n\n\nSjekke logger\n\n\n\nDet er anbefalt √• se p√• Kildomaten-loggene i Logs Explorer. Det kan man enkelt gj√∏re ved √• trykke p√• ‚ÄúView in Logs Explorer‚Äù som vist p√• bildet under:\n\n\n\n√Öpne Kildomaten-loggene i Logs Explorer\n\n\n\n\n\n\n\nKildomaten er satt opp for √• kunne prosessere hver kilde i 5 parallelle intanser samtidig. F.eks. hvis det oppst√•r 10 nye filer i en mappe som trigger en Kildomaten-kilde, s√• kan det prosesseres 5 filer samtidig. Ta kontakt med Kundeservice hvis man har behov for ytterlige skalering.\n\n\n\nKildomaten tilbyr e-postvarsling til teamet n√•r tjenesten feiler. Opprett en Kundeservice-sak for √• f√• satt opp e-postvarsling for teamet ditt.\n\n\n\nMan kan sette opp s√• mange kilder man √∏nsker. Men n√•r man setter det opp er det viktig √• huske at alle kildene m√• spesifiseres rett under automation/source-data/&lt;teamnavn&gt;-&lt;milj√∏&gt;/. Her er et eksempel p√• hvordan team dapla-example har to kilder i Kildomaten:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n‚îú‚îÄ‚îÄ automation/\n‚îÇ   ‚îî‚îÄ‚îÄ source-data/\n‚îÇ       ‚îú‚îÄ‚îÄ dapla-example-prod/\n‚îÇ           ‚îî‚îÄ‚îÄ altinn/\n‚îÇ               ‚îú‚îÄ‚îÄ config.yaml\n‚îÇ               ‚îî‚îÄ‚îÄ process_source_data.py\n‚îÇ           ‚îî‚îÄ‚îÄ ledstill/\n‚îÇ               ‚îú‚îÄ‚îÄ config.yaml\n‚îÇ               ‚îî‚îÄ‚îÄ process_source_data.py\n‚îÇ...           \n\nI eksempelet over ser vi det er to kilder: altinn og ledstill. Hver kilde trigges p√• ulike filstier i kildeb√∏tta, og python-koden som kj√∏res kan v√¶re ulik mellom kilder.\n\n\n\nI eksempelet som er brukt i dette kapitlet er Kildomaten satt opp i prodmilj√∏et til teamet. Mens egentlig burde man teste ut nye kilder i teamets test-milj√∏. Kildomaten er ikke satt opp i test-milj√∏et som standard, og derfor m√• det skrus p√• f√∏r man kan anvende det. Teamet kan gj√∏re det selv ved √• f√∏lge denne beskrivelsen, eller de kan ta kontakt med Kundeservice og f√• hjelp til dette.\nEn av de store fordelene med √• sette opp Kildomaten-kilder i test-milj√∏et f√∏r man gj√∏r det i prod-milj√∏et, er at tilgangsstyringen til data er mye mindre streng. Det gj√∏r det lettere for alle i teamet √• utvikle koden som skal benyttes.\nN√•r man skal sette opp Kildomaten i test-milj√∏et s√• f√∏lger det samme oppskrift som vi har vist for prod-milj√∏et over. Den store forskjellen er at test-kilder skal legges under en egen mappe under automation/source-data/ i teamets IaC-repo. Eksempelet under med team dapla-example viser hvordan man kan sette opp kildene altinn og ledstill for b√•de prod- og test-testmilj√∏et:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n‚îú‚îÄ‚îÄ automation/\n‚îÇ   ‚îî‚îÄ‚îÄ source-data/\n‚îÇ       ‚îú‚îÄ‚îÄ dapla-example-prod/\n‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ altinn/\n‚îÇ       ‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ config.yaml\n‚îÇ       ‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ process_source_data.py\n‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ ledstill/\n‚îÇ       ‚îÇ               ‚îú‚îÄ‚îÄ config.yaml\n‚îÇ       ‚îÇ               ‚îî‚îÄ‚îÄ process_source_data.py\n‚îÇ       ‚îú‚îÄ‚îÄ dapla-example-test/\n‚îÇ               ‚îú‚îÄ‚îÄ altinn/\n‚îÇ               ‚îÇ       ‚îú‚îÄ‚îÄ config.yaml\n‚îÇ               ‚îÇ       ‚îî‚îÄ‚îÄ process_source_data.py\n‚îÇ               ‚îî‚îÄ‚îÄ ledstill/\n‚îÇ                       ‚îú‚îÄ‚îÄ config.yaml\n‚îÇ                       ‚îî‚îÄ‚îÄ process_source_data.py\n‚îÇ...           \n\nSom vi ser av mappestrukturen over s√• er det to mapper under\nautomation/source-data:\n\ndapla-example-prod\ndapla-example-test\n\nDisse to mappene angir om det er henholdsvis prod- eller test-milj√∏et vi setter opp kilder for.\n\n\n\nKildomaten er bygget for √• trigge p√• nye filer som oppst√•r i en gitt filsti. Men noen ganger er det n√∏dvendig √• trigge kj√∏ring av alle filer p√• nytt. Noen ganger √∏nsker man kanskje √• kun trigge noen filer for en gitt kilde. Dette kan gj√∏res med en funksjon i Python-pakken dapla-toolbelt.\nF√∏r du kan gj√∏re dette trenger du f√∏lgende informasjon:\n\nproject-id for standardprosjektet. Slik finner du prosjekt-id. Merk at dette ikke er kilde-prosjektet!\nfolder_prefix som du √∏nsker at koden skal trigges p√•. Dette fungerer likt som tidligere forklart for config.yaml, men her har du ogs√• mulighet til √• kunne trigge prosesseringen p√• et undermappe av stien som var satt i kildens config.yaml.\nsource_name er navnet p√• kilden. Navnet p√• kilden i eksempelet med team dapla-example var altinn.\n\nHer er et kodeeksempel som viser hvordan man kan trigge prossesering av alle filer for kilde\n\n\nnotebook\n\nfrom dapla import trigger_source_data_processing\n\nproject_id = \"dapla-example-p\"\nsource_name = \"altinn\"\nfolder_prefix = \"ledstill/altinn/ra0678\"\n\ntrigger_source_data_processing(project_id, source_name, folder_prefix, kuben=True)          \n\nI eksempelet over trigger vi scriptet som er satt opp for kilden altinn til √• kj√∏re p√• alle undermapper av\nssb-dapla-example-data-kilde-prod/ledstill/altinn/ra0678/.",
    "crumbs": [
      "Manual",
      "Datatjenester"
    ]
  },
  {
    "objectID": "statistikkere/kildomaten.html#vedlikehold",
    "href": "statistikkere/kildomaten.html#vedlikehold",
    "title": "Kildomaten",
    "section": "",
    "text": "N√•r tjenesten er rullet ut s√• vil den kj√∏re automatisk p√• alle filer som dukker opp i filsti i kildeb√∏tta. Etter hvert vil det v√¶re behov for √• endre p√• skript, endre filstier som skal trigge tjenesten, eller prosessere alle filer p√• nytt. I denne delen forklarer vi hvordan du g√•r frem for √• gj√∏re dette.\n\n\nAlle p√• team kan endre p√• skriptet, men det er data-admins som m√• godkjenne endringene f√∏r de blir rullet ut. For √• endre skriptet gj√∏r du f√∏lgende:\n\nKlon repoet.\nGj√∏r endringene du √∏nsker i en branch.\nPush opp endringene til GitHub og opprett en pull request.\nF√• en data-admins p√• teamet til √• godkjenne endringene.\nN√•r endringene er godkjent s√• kan du merge inn i main-branchen.\n\nEn endring i Python-skriptet krever ikke at tjenesten rulles ut p√• nytt. Derfor er det ikke like mange tester og kj√∏ringer som gj√∏res som n√•r man oppretter en helt ny kilde.\n\n\n\nAlle p√• teamet kan gj√∏re endringer i config.yaml, men det er data-admins som m√• godkjenne endringene f√∏r de blir rullet ut. For √• endre config.yaml gj√∏r du f√∏lgende:\n\nKlon repoet.\nGj√∏r endringene du √∏nsker i en branch.\nPush opp endringene til GitHub og opprett en pull request.\nF√• en data-admins p√• teamet til √• godkjenne endringene.\nN√•r endringene er godkjent s√• kan du merge inn i main-branchen.\n\nEn endring i config.yaml krever at tjenesten rulles ut p√• nytt og tar derfor litt mer tid enn en endring i Python-skriptet.",
    "crumbs": [
      "Manual",
      "Datatjenester"
    ]
  },
  {
    "objectID": "statistikkere/kildomaten.html#footnotes",
    "href": "statistikkere/kildomaten.html#footnotes",
    "title": "Kildomaten",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI tillegg er det data-admins som m√• teste tjenesten manuelt hvis det gj√∏res p√• skarpe data, siden det kun er data-admins som kan f√• tilgang til de dataene.‚Ü©Ô∏é\nInfrastructure-as-Code (IaC) er repo som definerer alle ressursene til teamet p√• Dapla. Alle Dapla-team har et eget IaC-repo p√• GiHub og du finner det ved √• s√∏ke etter repoet -iac under statisticsnorway.‚Ü©Ô∏é\nStandardprosjektet har navnestrukturen &lt;teamnavn&gt;-p‚Ü©Ô∏é",
    "crumbs": [
      "Manual",
      "Datatjenester"
    ]
  },
  {
    "objectID": "statistikkere/git-og-github.html",
    "href": "statistikkere/git-og-github.html",
    "title": "Git og GitHub",
    "section": "",
    "text": "I SSB bruker vi Git til versjonskontroll av koden v√•r og deler den med andre via GitHub. For √• mestre disse verkt√∏yene er det viktig √• forst√• forskjellen mellom Git og GitHub.\n\n\n\n\n\n\nGit vs.¬†GitHub: Kort fortalt\n\n\n\nGit er et verkt√∏y installert p√• din lokale maskin som sporer endringer i koden din.\nGitHub er en skybasert plattform som fungerer som et felles lagringssystem, der du kan dele og samarbeide med andre om kode.\n\n\nGit og GitHub er viktige verkt√∏y for √• sikre at produksjonssystemene v√•re er trygge og reproduserbare. De gj√∏r det enkelt √• spore endringer og gjennomg√• eller godkjenne hverandres bidrag.\nI dette kapittelet ser vi n√¶rmere p√• Git og GitHub og hvordan de er implementert i SSB. Selv om ssb-project gj√∏r det lettere √• forholde seg til Git og GitHub vil vi dette kapittelet forklare n√¶rmere hvordan det funker uten dette hjelpemiddelet.\n\n\n\n\n\n\nLes videre med interne ressurser\n\n\n\nDet finnes mange gode ressurser p√• huset om versjonsh√•ndtering i tillegg til dette kapittelet. Gruppen Kvalitet i Kode og Koding (KVAKK) har skrevet flere veiledninger p√• confluence, blant annet om Git anbefalt arbeidsflyt eller hvordan man l√∏ser en merge conflict. Se hele katalogen til KVAKK om Git og GitHub ved √• g√• inn p√• deres confluence-omr√•de: Versjonskontroll med Git.\nI tillegg har A200 sitt st√∏tteteam skrevet om hva man b√∏r l√¶re seg og hvordan man g√•r frem for √• l√¶re i confluence-dokumentet Kom i gang med Dapla (statistics-norway.atlassian.net).\nNettsiden learngitbranching.js.org er ogs√• en veldig god ressurs for √• forst√• konseptene.\n\n\n\n\n\n\nGit er en programvare for distribuert versjonsh√•ndtering av filer:\n\nGit tar vare p√• historien til koden din\nAlle som jobber med koden har en kopi av koden hos seg selv (distribuert)\n\nN√•r man √∏nsker √• dele koden med andre laster man opp koden til et felles kodelager p√• GitHub kalt repository (repo). Git versjonsh√•ndterer filene i repoet. Vanligvis versjonsh√•ndteres rene tekstfiler, men git kan ogs√• versjonsh√•ndtere bilder og PDFer.\nGit er installert p√• maskinen du jobber p√• og brukes fra terminalen. Det finnes pek-og-klikk versjoner av Git, blant annet i Jupyterlab og RStudio, men noen situasjoner vil bare kunne l√∏ses i terminalen.\nI SSB anbefaler vi at du starter et nytt Git-prosjekt ved √• benytte ssb-project. Da vil du ikke bare aktivere Git, men du kan ogs√• f√• implementert en del andre gode praksiser for √• holde koden din ryddig, oversiktlig og sikker.\nMen f√∏r vi kan begynne √• bruke Git m√• vi konfigurere v√•r egen bruker for Git, som er forklart i neste seksjon.\nLes mer om Git p√• https://git-scm.com/.\n\n\n\n\n\n\nGit i praksis: Kort fortalt\n\n\n\nMan aktiverer Git p√• en mappe i filsystemet sitt med kommandoen git init n√•r man st√•r i mappen som skal versjonsh√•nderes. Da vil Git versjonsh√•ndtere alle filer som er i den mappen og i eventuelle undermapper. N√•r du s√• gj√∏r endringer p√• en fil i mappen, vil Git registrere endringer. √ònsker du at endringen skal bli et punkt i historikken til prosjektet, s√• m√• du f√∏rst legge til filen i Git med kommandoen git add filnavn. N√•r du har gjort dette, s√• kan du lagre endringen med kommandoen git commit -m \"Din melding her\". N√•r du har gjort dette, s√• vil endringen v√¶re lagret i Git. N√•r du har gjort mange endringer, s√• kan du sende endringene til GitHub med kommandoen git push. N√•r du har gjort dette, s√• vil endringene v√¶re synlige for alle som har tilgang til GitHub-prosjektet.\n\n\n\n\n\nDenne delen er kun gjeldende for gamle jupyter, alts√• jupyter.dapla.ssb.no, og ikke DaplaLab.\n\n\n\n\n\n\nBruk SSB sin konfigurasjon!\n\n\n\nAlle som bruker GitHub i SSB b√∏r bruke SSB sin Git-konfigurasjon. Hvis ikke kan man bl.a. risikere at output fra Notebooks blir pushet til GitHub. Du kan bruke SSBs konfigurasjon ved √• kj√∏re ssb-gitconfig.py i terminalen og svare p√• sp√∏rsm√•lene som dukker opp.\n\n\nFor √• jobbe med Git s√• m√• man konfigurere brukeren sin slik at Git vet hvem som gj√∏r endringer i koden. I praksis betyr det at du m√• ha filen .gitconfig p√• hjemmeomr√•det ditt (f.eks. /home/jovyan/.gitconfig p√• Dapla) med noe grunnleggende informasjon:\n\n\n.gitconfig\n\n# /home/jovyan/.gitconfig p√• Dapla\n[user]\n        name = Ola Nordmann\n        email = &lt;ini&gt;@ssb.no\n\nMed denne konfigurasjonen s√• kan man bruke Git lokalt. Men skal man ogs√• bruke GitHub i SSB, dvs. dele kode med andre, s√• m√• man ogs√• legge til mer informasjon i .gitconfig. Heldigvis er det skrevet et script som gj√∏r dette for deg. For √• f√• anbefalt konfigurasjon for Git s√• kan du kj√∏re f√∏lgende kommando i terminalen:\n\n\nterminal\n\nssb_gitconfig.py\n\nDette scriptet vil sp√∏rre deg om ditt brukernavn i SSB, og s√• vil det opprette en fil som heter .gitconfig i hjemmeomr√•det ditt. Denne filen vil inneholde informasjon om brukernavn, e-post og tilgang til GitHub. I tillegg vil den s√∏rge for at ditt ssb-project ikke sender output fra notebooks til GitHub.\n\n\n\nSelv om det er vanlig √• jobbe i notebooks p√• Dapla, s√• skal all kode i SSB lagres og versjonh√•ndteres som rene tekstfiler i .R- eller .py-filer i prosent-formatet. I praksis har ikke dette s√• stor betydning, siden disse filtypene kan √•pnes som notebooks i verkt√∏y som Jupyterlab og VS Code, og p√• den m√•ten handler det bare om hvilket filformat koden lagres til.\nLes mer om hvordan man lagrer notebooks til rent tekstformat.\n\n\n\nGit er veldig sterkt verkt√∏y med mange muligheter. Det er ikke tenkt at alt skal adresseres i dette kapitlet. Men det er noen vanlig operasjoner som er s√• vanlige at alle som jobber med kode i SSB b√∏r kjenne dem.\nVi har tidligere nevnt at kommandoen for √• aktivere versjonsh√•ndtering med Git p√• en mappe, er git init. Dette gj√∏res ogs√• automatisk n√•r man oppretter et nytt ssb-project.\nVanlige git-kommandoer:\n\ngit status for √• se hvilke endringer Git har oppdaget\ngit add &lt;filnavn&gt; for √• fortelle Git at endringene skal lagres\ngit commit -m \"Din melding her\" for √• gj√∏re endringene om til et punkt i historien til koden din * Hver commit har sin egen unike ID * Flere filer kan samles i en commit\n\nN√•r man utvikler kode s√• gj√∏r man det fra s√•kalte branches1. Hvis vi tenker oss at din eksisterende kodebase er stammen p√• et tre (ofte kalt master eller main), s√• legger Git opp til at man gj√∏r endringer p√• denne koden via branches eller grener av treet. Med andre ord holder vi stammen/master ur√∏rt helt til vi vet at endringen fungerer som den skal. Til slutt merger vi den inn i master. Vi kan opprette en ny branch og g√• inn i den ved √• skrive git checkout -b &lt;branch navn&gt;. Da st√•r du i en branch og kan bruke kommandoer som git add og git commit som vist tidligere.\nVi kan merge endringene fra v√•r branch inn i main ved √• f√∏rst g√• inn i main, git switch main, og deretter skrive git merge &lt;branch navn&gt;. Men typisk vil ikke dette v√¶re fremgangsm√•ten i SSB. N√•r man er forn√∏yd med endringene i en branch, s√• vil man pushe den opp til GitHub, slik at en kollega kan vurdere om den skal merges inn i main. Dermed gj√∏res selve mergen i GitHub-grensenittet. Vi skal se n√¶rmere p√• GitHub i neste kapittel.\n\n\n\n\nGitHub er et nettsted som bl.a. fungerer som v√•rt felles mappesystem for deling av kode. SSB har sin egen organisasjonskonto med navn statisticsnorway. Men selv om SSB har en organisasjonskonto p√• GitHub m√• alle ansatte opprette sin egen brukerprofil, og knytte den mot SSB sin organisasjonskonto. Under forklarer vi hvordan du gj√∏r dette.\n\n\nDette kapittelet er bare relevant hvis man ikke har en GitHub-brukerkonto fra f√∏r. For √• bruke ssb-project-programmet til √• generere et remote repo p√• GitHub m√• du ha en konto. Derfor starter vi med √• gj√∏re dette. Det er en engangsjobb og du trenger aldri gj√∏re det igjen.\n\n\n\n\n\n\nSSB har valgt √• ikke sette opp SSB-brukerne til de ansatte som GitHub-brukere. En viktig √•rsak er at en GitHub-konto ofte regnes som en del av den ansattes CV. For de som aldri har brukt GitHub f√∏r kan det virke fremmed, men det er nok en fordel p√• sikt n√•r alle blir godt kjent med denne arbeidsformen.\n\n\n\nSlik gj√∏r du det:\n\nG√• til https://GitHub.com/\nTrykk Sign up √∏verst i h√∏yre hj√∏rne\nI dialogboksen som √•pnes, se Figur¬†1, skriver du inn din e-postkonto og lager et passord. Dette trenger ikke v√¶re din SSB-bruker og e-post. Hvis du bruker en en personlig e-postkonto er det viktig at du tydeliggj√∏r hvem du er s√• kollegaer vet at du jobber i SSB n√•r de ser aktivitet fra deg.\n\n\n\n\n\n\n\nFigur¬†1: Dialogboks for opprettelse av GitHub-bruker.\n\n\n\nDu har n√• laget en egen GitHub-bruker. I neste steg skal vi knytte denne kontoen til din SSB-bruker.\n\n\n\nHvis du har fullf√∏rt forrige steg s√• har du n√• en GitHub-konto. Hvis du st√•r p√• din profil-side s√• ser den ut som i Figur¬†2.\n\n\n\n\n\n\nFigur¬†2: Et eksempel p√• hjemmeomr√•det til en GitHub-bruker\n\n\n\nDet neste vi m√• gj√∏re er √• aktivere 2-faktor autentisering, siden det er dette som benyttes i SSB. Hvis du st√•r p√• siden i bildet over, s√• gj√∏r du f√∏lgende for √• aktivere 2-faktor autentisering mot GitHub:\n\n\n\nTrykk p√• den lille pilen √∏verst til h√∏yre og velg Settings(se Figur¬†3).\nDeretter velger du Password and authentification i menyen til venstre.\nUnder Two-factor authentication trykker du p√• Enable.\n\n\n\n\n\n\n\n\n\nFigur¬†3: √Öpne settings for din GitHub-bruker.\n\n\n\n\n\n\nFigur¬†4 viser dialogboksen som vises. Velg Enable two-factor authentification.\n\n\n\n\n\n\n\nFigur¬†4: Dialogboks som √•pnes n√•r 2FA skrus p√• f√∏rste gang.\n\n\n\n\nFigur¬†5 viser dialogboksen som vises for √• velge hvordan man skal autentisere seg. Her anbefales det √• velge Set up using an app, slik at du kan bruke Microsoft Authenticator-appen p√• din mobil.\n\n\n\n\n\n\n\nFigur¬†5: Dialogboks for √• velge hvordan man skal autentisere seg med 2FA.\n\n\n\nFigur¬†6 viser QR-koden som vises. Denne skal vi bruke i neste steg.\n\n\n\n\n\n\nFigur¬†6: QR-kode som skannes av Microsoft Authenticator.\n\n\n\n\n\n\nStrekkoden over skal skannes i din Microsoft Authenticator-app p√• mobilen, som vist i Figur¬†7. √Öpne appen, trykk p√• Bekreftede ID-er, og til slutt trykk p√• Skann QR-kode. Deretter skanner du QR-koden fra punkt 5.\nN√•r koden er skannet har du f√•tt opp f√∏lgende bilde p√• appens hovedside (se bilde til h√∏yre). Skriv inn den 6-siffer koden p√• GitHub-siden med QR-koden.\nTil slutt lagrer du Recovery-codes et trygt sted som bare du har tilgang til.\n\n\n\n\n\n\n\n\n\nFigur¬†7: Mobilappen Microsoft authenticator\n\n\n\n\n\nN√• har vi aktivert 2-faktor autentisering for GitHub og er klare til √• knytte v√•r personlige konto til v√•r SSB-bruker p√• SSBs ‚ÄúGitHub organisation‚Äù statisticsnorway.\n\n\n\nI forrige steg aktiverte vi 2-faktor autentisering for GitHub. Det neste vi m√• gj√∏re er √• koble oss til Single Sign On (SSO) for SSB sin organisasjon p√• GitHub:\n\nTrykk p√• lenken https://GitHub.com/orgs/statisticsnorway/sso\nI dialogboksen som dukker opp trykker du p√• Continue, slik som vist i Figur¬†8.\n\n\n\n\n\n\n\nFigur¬†8: Single Sign on (SSO) for SSB sin organisasjon p√• GitHub\n\n\n\nN√•r du har gjennomf√∏rt dette s√• har du tilgang til statisticsnorway p√• GitHub. G√•r du inn p√• denne lenken s√• skal du n√• kunne lese b√•de Public, Private og Internal repoer, slik som vist i Figur¬†9.\n\n\n\n\n\n\nFigur¬†9: Medlemsvisning for SSB sin GitHub-organisasjon.\n\n\n\n\n\n\nN√•r vi skal jobbe med SSB-kode som ligger lagret hos statistcsnorway p√• GitHub, s√• m√• vi autentisere oss. M√•ten vi gj√∏re det p√• er ved √• generere et Personal Access Token (ofte forkortet PAT) som vi oppgir n√•r vi vil hente eller oppdatere kode p√• GitHub. Da sender vi med PAT for √• autentisere oss for GitHub.\n\n\nFor √• lage en PAT som er godkjent mot statisticsnorway s√• gj√∏r man f√∏lgende:\n\nG√• til din profilside p√• GitHub og √•pne Settings slik som ble vist Seksjon¬†1.2.2.\nVelg Developer Settings i menyen til venstre.\nI menyen til venstre velger du Personal Access Token, og deretter Tokens (classic).\nVelg Generate new token og deretter Generate new token (classic).\nUnder Note kan du gi PAT‚Äôen et navn. Velg et navn som er intuitivt for deg. Hvis du skal bruke PAT til √• jobbe mot Dapla, s√• ville jeg ganske enkelt kalt den dapla. Hvis du skal bruke den mot bakkemilj√∏et ville jeg kalt den prodsone eller noe annet som gj√∏r det lett for det skj√∏nne innholdet i ettertid.\nUnder Expiration velger du hvor lang tid som skal g√• f√∏r PAT blir ugyldig. Dette er en avvening mellom sikkerhet og hva som er praktisk. Det anbefales at du velger 365 dager. N√•r PAT utl√∏per m√• du gjenta stegene i dette kapittelet.\nUnder Select scopes velger du repo og workflow slik som vist i Figur¬†10.\n\n\n\n\n\n\n\nFigur¬†10: Gi token et kort og beskrivende navn\n\n\n\n\nTrykk p√• Generate token nederst p√• siden og du f√•r noe lignende det du ser i Figur¬†11.\n\n\n\n\n\n\n\nFigur¬†11: Token som ble generert.\n\n\n\n\nKopier deretter PAT til en midlertidig fil. Grunnen er at du aldri vil se det igjen her etter at vi har gjennomf√∏rt neste steg.\nDeretter trykker du p√• Configure SSO og velger Authorize ved siden statisticsnorway, slik som vist i Figur¬†12. Svar deretter p√• sp√∏rsm√•lene som dukker opp.\n\n\n\n\n\n\n\nFigur¬†12: Autorisering av Token mot SSBs GiHub-organisasjon.\n\n\n\nVi har n√• opprettet en PAT som er godkjent for bruk mot SSB sin kode p√• GitHub. Det betyr at hvis vi vil jobbe med Git p√• SSB sine maskiner i sky eller p√• bakken, s√• m√• vi sendte med dette tokenet for √• f√• lov til √• jobbe med koden som ligger p√• statisticsnorway p√• GitHub.\n\n\n\nDette gjelder gamle Jupyter. Bla lenger ned for √• lese om hvordan man lagrer PAT p√• DaplaLab\nDet er ganske upraktisk √• m√•tte sende med tokenet hver gang vi skal jobbe med GitHub. Vi b√∏r derfor lagre det lokalt der vi jobber, slik at Git automatisk finner det. Det finnes mange m√•ter √• gj√∏re dette p√• og det er ikke bestemt hva som skal v√¶re beste-praksis i SSB. Men en m√•te √• gj√∏re det er via en .netrc-fil. Vi oppretter da en fil som heter .netrc p√• v√•rt hjemmeomr√•de, og legger f√∏lgende informasjon p√• en (hvilken som helst) linje i filen:\n\n\n~/.netrc\n\nmachine GitHub.com login &lt;GitHub-bruker&gt; password &lt;Personal Access Token&gt;\n\nGitHub-bruker er da din personlige bruker og IKKE brukernavnet ditt i SSB. Personal Access Token er det vi lagde i forrige kapittelet.\nEn veldig enkel m√•te √• lagre dette er som f√∏lger. Anta at min personlige GitHub-bruker er SSB-Chad og at min Personal Access Token er blablabla. Da kan jeg gj√∏re f√∏lgende for √• lagre det i .netrc:\n\nG√• inn i Jupyterlab og √•pne en Python-notebook.\nI den f√∏rste kodecellen skriver du:\n\n\n\nterminal\n\n!echo \"machine GitHub.com login SSB-Chad password blablabla\" &gt;&gt; ~/.netrc\n\nAlternativt kan du fjerne utropstegnet og kj√∏re kommandoen direkte i terminalen. Det vil gi samme resultat. Koden over legger til en linje med teksten\nmachine GitHub.com login SSB-Chad password blablabla\ni en .netrc-fil p√• din hjemmeomr√•det, uavhengig av om du har en fra f√∏r eller ikke. Hvis du har en fil fra f√∏r som allerede har et token fra GitHub, ville jeg nok slettet det f√∏r jeg legger en et nytt token.\nHver gang du jobber mot GitHub vil Git sjekke om informasjon om autentisering ligger i denne filen, og bruke den hvis den ligger der.\n\n\n√Ö lagre PAT p√• DaplaLab er enkelt og gj√∏res kun en gang uansett hvor mange tjenester man bruker.\nHer er stegene:\n\nLogg inn p√• https://lab.dapla.ssb.no\nTrykk p√• ‚ÄòMy account‚Äô\nNaviger til Git-fanen\nLim inn token der det st√•r ‚ÄòGit Forge Personal Access Token‚Äô vist i Figur¬†13\n\n\n\n\n\n\n\nFigur¬†13: DaplaLab My account: lagre PAT\n\n\n\n\n\n\n\nI eksempelet over lagde vi en PAT som var gyldig i 90 dager. Dermed vil du ikke kunne jobbe mot GitHub med dette tokenet etter 90 dager. For √• oppdatere tokenet gj√∏r du f√∏lgende:\n\nLag et nytt PAT ved √• repetere Seksjon¬†1.2.4.1.\nI milj√∏et der du skal jobbe med Git og GitHub g√•r du inn i din .netrc og bytter ut token med det nye.\n\nOg med det er du klar til √• jobbe mot statisticsnorway p√• GitHub.",
    "crumbs": [
      "Manual",
      "Kode"
    ]
  },
  {
    "objectID": "statistikkere/git-og-github.html#git",
    "href": "statistikkere/git-og-github.html#git",
    "title": "Git og GitHub",
    "section": "",
    "text": "Git er en programvare for distribuert versjonsh√•ndtering av filer:\n\nGit tar vare p√• historien til koden din\nAlle som jobber med koden har en kopi av koden hos seg selv (distribuert)\n\nN√•r man √∏nsker √• dele koden med andre laster man opp koden til et felles kodelager p√• GitHub kalt repository (repo). Git versjonsh√•ndterer filene i repoet. Vanligvis versjonsh√•ndteres rene tekstfiler, men git kan ogs√• versjonsh√•ndtere bilder og PDFer.\nGit er installert p√• maskinen du jobber p√• og brukes fra terminalen. Det finnes pek-og-klikk versjoner av Git, blant annet i Jupyterlab og RStudio, men noen situasjoner vil bare kunne l√∏ses i terminalen.\nI SSB anbefaler vi at du starter et nytt Git-prosjekt ved √• benytte ssb-project. Da vil du ikke bare aktivere Git, men du kan ogs√• f√• implementert en del andre gode praksiser for √• holde koden din ryddig, oversiktlig og sikker.\nMen f√∏r vi kan begynne √• bruke Git m√• vi konfigurere v√•r egen bruker for Git, som er forklart i neste seksjon.\nLes mer om Git p√• https://git-scm.com/.\n\n\n\n\n\n\nGit i praksis: Kort fortalt\n\n\n\nMan aktiverer Git p√• en mappe i filsystemet sitt med kommandoen git init n√•r man st√•r i mappen som skal versjonsh√•nderes. Da vil Git versjonsh√•ndtere alle filer som er i den mappen og i eventuelle undermapper. N√•r du s√• gj√∏r endringer p√• en fil i mappen, vil Git registrere endringer. √ònsker du at endringen skal bli et punkt i historikken til prosjektet, s√• m√• du f√∏rst legge til filen i Git med kommandoen git add filnavn. N√•r du har gjort dette, s√• kan du lagre endringen med kommandoen git commit -m \"Din melding her\". N√•r du har gjort dette, s√• vil endringen v√¶re lagret i Git. N√•r du har gjort mange endringer, s√• kan du sende endringene til GitHub med kommandoen git push. N√•r du har gjort dette, s√• vil endringene v√¶re synlige for alle som har tilgang til GitHub-prosjektet.\n\n\n\n\n\nDenne delen er kun gjeldende for gamle jupyter, alts√• jupyter.dapla.ssb.no, og ikke DaplaLab.\n\n\n\n\n\n\nBruk SSB sin konfigurasjon!\n\n\n\nAlle som bruker GitHub i SSB b√∏r bruke SSB sin Git-konfigurasjon. Hvis ikke kan man bl.a. risikere at output fra Notebooks blir pushet til GitHub. Du kan bruke SSBs konfigurasjon ved √• kj√∏re ssb-gitconfig.py i terminalen og svare p√• sp√∏rsm√•lene som dukker opp.\n\n\nFor √• jobbe med Git s√• m√• man konfigurere brukeren sin slik at Git vet hvem som gj√∏r endringer i koden. I praksis betyr det at du m√• ha filen .gitconfig p√• hjemmeomr√•det ditt (f.eks. /home/jovyan/.gitconfig p√• Dapla) med noe grunnleggende informasjon:\n\n\n.gitconfig\n\n# /home/jovyan/.gitconfig p√• Dapla\n[user]\n        name = Ola Nordmann\n        email = &lt;ini&gt;@ssb.no\n\nMed denne konfigurasjonen s√• kan man bruke Git lokalt. Men skal man ogs√• bruke GitHub i SSB, dvs. dele kode med andre, s√• m√• man ogs√• legge til mer informasjon i .gitconfig. Heldigvis er det skrevet et script som gj√∏r dette for deg. For √• f√• anbefalt konfigurasjon for Git s√• kan du kj√∏re f√∏lgende kommando i terminalen:\n\n\nterminal\n\nssb_gitconfig.py\n\nDette scriptet vil sp√∏rre deg om ditt brukernavn i SSB, og s√• vil det opprette en fil som heter .gitconfig i hjemmeomr√•det ditt. Denne filen vil inneholde informasjon om brukernavn, e-post og tilgang til GitHub. I tillegg vil den s√∏rge for at ditt ssb-project ikke sender output fra notebooks til GitHub.\n\n\n\nSelv om det er vanlig √• jobbe i notebooks p√• Dapla, s√• skal all kode i SSB lagres og versjonh√•ndteres som rene tekstfiler i .R- eller .py-filer i prosent-formatet. I praksis har ikke dette s√• stor betydning, siden disse filtypene kan √•pnes som notebooks i verkt√∏y som Jupyterlab og VS Code, og p√• den m√•ten handler det bare om hvilket filformat koden lagres til.\nLes mer om hvordan man lagrer notebooks til rent tekstformat.\n\n\n\nGit er veldig sterkt verkt√∏y med mange muligheter. Det er ikke tenkt at alt skal adresseres i dette kapitlet. Men det er noen vanlig operasjoner som er s√• vanlige at alle som jobber med kode i SSB b√∏r kjenne dem.\nVi har tidligere nevnt at kommandoen for √• aktivere versjonsh√•ndtering med Git p√• en mappe, er git init. Dette gj√∏res ogs√• automatisk n√•r man oppretter et nytt ssb-project.\nVanlige git-kommandoer:\n\ngit status for √• se hvilke endringer Git har oppdaget\ngit add &lt;filnavn&gt; for √• fortelle Git at endringene skal lagres\ngit commit -m \"Din melding her\" for √• gj√∏re endringene om til et punkt i historien til koden din * Hver commit har sin egen unike ID * Flere filer kan samles i en commit\n\nN√•r man utvikler kode s√• gj√∏r man det fra s√•kalte branches1. Hvis vi tenker oss at din eksisterende kodebase er stammen p√• et tre (ofte kalt master eller main), s√• legger Git opp til at man gj√∏r endringer p√• denne koden via branches eller grener av treet. Med andre ord holder vi stammen/master ur√∏rt helt til vi vet at endringen fungerer som den skal. Til slutt merger vi den inn i master. Vi kan opprette en ny branch og g√• inn i den ved √• skrive git checkout -b &lt;branch navn&gt;. Da st√•r du i en branch og kan bruke kommandoer som git add og git commit som vist tidligere.\nVi kan merge endringene fra v√•r branch inn i main ved √• f√∏rst g√• inn i main, git switch main, og deretter skrive git merge &lt;branch navn&gt;. Men typisk vil ikke dette v√¶re fremgangsm√•ten i SSB. N√•r man er forn√∏yd med endringene i en branch, s√• vil man pushe den opp til GitHub, slik at en kollega kan vurdere om den skal merges inn i main. Dermed gj√∏res selve mergen i GitHub-grensenittet. Vi skal se n√¶rmere p√• GitHub i neste kapittel.",
    "crumbs": [
      "Manual",
      "Kode"
    ]
  },
  {
    "objectID": "statistikkere/git-og-github.html#github",
    "href": "statistikkere/git-og-github.html#github",
    "title": "Git og GitHub",
    "section": "",
    "text": "GitHub er et nettsted som bl.a. fungerer som v√•rt felles mappesystem for deling av kode. SSB har sin egen organisasjonskonto med navn statisticsnorway. Men selv om SSB har en organisasjonskonto p√• GitHub m√• alle ansatte opprette sin egen brukerprofil, og knytte den mot SSB sin organisasjonskonto. Under forklarer vi hvordan du gj√∏r dette.\n\n\nDette kapittelet er bare relevant hvis man ikke har en GitHub-brukerkonto fra f√∏r. For √• bruke ssb-project-programmet til √• generere et remote repo p√• GitHub m√• du ha en konto. Derfor starter vi med √• gj√∏re dette. Det er en engangsjobb og du trenger aldri gj√∏re det igjen.\n\n\n\n\n\n\nSSB har valgt √• ikke sette opp SSB-brukerne til de ansatte som GitHub-brukere. En viktig √•rsak er at en GitHub-konto ofte regnes som en del av den ansattes CV. For de som aldri har brukt GitHub f√∏r kan det virke fremmed, men det er nok en fordel p√• sikt n√•r alle blir godt kjent med denne arbeidsformen.\n\n\n\nSlik gj√∏r du det:\n\nG√• til https://GitHub.com/\nTrykk Sign up √∏verst i h√∏yre hj√∏rne\nI dialogboksen som √•pnes, se Figur¬†1, skriver du inn din e-postkonto og lager et passord. Dette trenger ikke v√¶re din SSB-bruker og e-post. Hvis du bruker en en personlig e-postkonto er det viktig at du tydeliggj√∏r hvem du er s√• kollegaer vet at du jobber i SSB n√•r de ser aktivitet fra deg.\n\n\n\n\n\n\n\nFigur¬†1: Dialogboks for opprettelse av GitHub-bruker.\n\n\n\nDu har n√• laget en egen GitHub-bruker. I neste steg skal vi knytte denne kontoen til din SSB-bruker.\n\n\n\nHvis du har fullf√∏rt forrige steg s√• har du n√• en GitHub-konto. Hvis du st√•r p√• din profil-side s√• ser den ut som i Figur¬†2.\n\n\n\n\n\n\nFigur¬†2: Et eksempel p√• hjemmeomr√•det til en GitHub-bruker\n\n\n\nDet neste vi m√• gj√∏re er √• aktivere 2-faktor autentisering, siden det er dette som benyttes i SSB. Hvis du st√•r p√• siden i bildet over, s√• gj√∏r du f√∏lgende for √• aktivere 2-faktor autentisering mot GitHub:\n\n\n\nTrykk p√• den lille pilen √∏verst til h√∏yre og velg Settings(se Figur¬†3).\nDeretter velger du Password and authentification i menyen til venstre.\nUnder Two-factor authentication trykker du p√• Enable.\n\n\n\n\n\n\n\n\n\nFigur¬†3: √Öpne settings for din GitHub-bruker.\n\n\n\n\n\n\nFigur¬†4 viser dialogboksen som vises. Velg Enable two-factor authentification.\n\n\n\n\n\n\n\nFigur¬†4: Dialogboks som √•pnes n√•r 2FA skrus p√• f√∏rste gang.\n\n\n\n\nFigur¬†5 viser dialogboksen som vises for √• velge hvordan man skal autentisere seg. Her anbefales det √• velge Set up using an app, slik at du kan bruke Microsoft Authenticator-appen p√• din mobil.\n\n\n\n\n\n\n\nFigur¬†5: Dialogboks for √• velge hvordan man skal autentisere seg med 2FA.\n\n\n\nFigur¬†6 viser QR-koden som vises. Denne skal vi bruke i neste steg.\n\n\n\n\n\n\nFigur¬†6: QR-kode som skannes av Microsoft Authenticator.\n\n\n\n\n\n\nStrekkoden over skal skannes i din Microsoft Authenticator-app p√• mobilen, som vist i Figur¬†7. √Öpne appen, trykk p√• Bekreftede ID-er, og til slutt trykk p√• Skann QR-kode. Deretter skanner du QR-koden fra punkt 5.\nN√•r koden er skannet har du f√•tt opp f√∏lgende bilde p√• appens hovedside (se bilde til h√∏yre). Skriv inn den 6-siffer koden p√• GitHub-siden med QR-koden.\nTil slutt lagrer du Recovery-codes et trygt sted som bare du har tilgang til.\n\n\n\n\n\n\n\n\n\nFigur¬†7: Mobilappen Microsoft authenticator\n\n\n\n\n\nN√• har vi aktivert 2-faktor autentisering for GitHub og er klare til √• knytte v√•r personlige konto til v√•r SSB-bruker p√• SSBs ‚ÄúGitHub organisation‚Äù statisticsnorway.\n\n\n\nI forrige steg aktiverte vi 2-faktor autentisering for GitHub. Det neste vi m√• gj√∏re er √• koble oss til Single Sign On (SSO) for SSB sin organisasjon p√• GitHub:\n\nTrykk p√• lenken https://GitHub.com/orgs/statisticsnorway/sso\nI dialogboksen som dukker opp trykker du p√• Continue, slik som vist i Figur¬†8.\n\n\n\n\n\n\n\nFigur¬†8: Single Sign on (SSO) for SSB sin organisasjon p√• GitHub\n\n\n\nN√•r du har gjennomf√∏rt dette s√• har du tilgang til statisticsnorway p√• GitHub. G√•r du inn p√• denne lenken s√• skal du n√• kunne lese b√•de Public, Private og Internal repoer, slik som vist i Figur¬†9.\n\n\n\n\n\n\nFigur¬†9: Medlemsvisning for SSB sin GitHub-organisasjon.\n\n\n\n\n\n\nN√•r vi skal jobbe med SSB-kode som ligger lagret hos statistcsnorway p√• GitHub, s√• m√• vi autentisere oss. M√•ten vi gj√∏re det p√• er ved √• generere et Personal Access Token (ofte forkortet PAT) som vi oppgir n√•r vi vil hente eller oppdatere kode p√• GitHub. Da sender vi med PAT for √• autentisere oss for GitHub.\n\n\nFor √• lage en PAT som er godkjent mot statisticsnorway s√• gj√∏r man f√∏lgende:\n\nG√• til din profilside p√• GitHub og √•pne Settings slik som ble vist Seksjon¬†1.2.2.\nVelg Developer Settings i menyen til venstre.\nI menyen til venstre velger du Personal Access Token, og deretter Tokens (classic).\nVelg Generate new token og deretter Generate new token (classic).\nUnder Note kan du gi PAT‚Äôen et navn. Velg et navn som er intuitivt for deg. Hvis du skal bruke PAT til √• jobbe mot Dapla, s√• ville jeg ganske enkelt kalt den dapla. Hvis du skal bruke den mot bakkemilj√∏et ville jeg kalt den prodsone eller noe annet som gj√∏r det lett for det skj√∏nne innholdet i ettertid.\nUnder Expiration velger du hvor lang tid som skal g√• f√∏r PAT blir ugyldig. Dette er en avvening mellom sikkerhet og hva som er praktisk. Det anbefales at du velger 365 dager. N√•r PAT utl√∏per m√• du gjenta stegene i dette kapittelet.\nUnder Select scopes velger du repo og workflow slik som vist i Figur¬†10.\n\n\n\n\n\n\n\nFigur¬†10: Gi token et kort og beskrivende navn\n\n\n\n\nTrykk p√• Generate token nederst p√• siden og du f√•r noe lignende det du ser i Figur¬†11.\n\n\n\n\n\n\n\nFigur¬†11: Token som ble generert.\n\n\n\n\nKopier deretter PAT til en midlertidig fil. Grunnen er at du aldri vil se det igjen her etter at vi har gjennomf√∏rt neste steg.\nDeretter trykker du p√• Configure SSO og velger Authorize ved siden statisticsnorway, slik som vist i Figur¬†12. Svar deretter p√• sp√∏rsm√•lene som dukker opp.\n\n\n\n\n\n\n\nFigur¬†12: Autorisering av Token mot SSBs GiHub-organisasjon.\n\n\n\nVi har n√• opprettet en PAT som er godkjent for bruk mot SSB sin kode p√• GitHub. Det betyr at hvis vi vil jobbe med Git p√• SSB sine maskiner i sky eller p√• bakken, s√• m√• vi sendte med dette tokenet for √• f√• lov til √• jobbe med koden som ligger p√• statisticsnorway p√• GitHub.\n\n\n\nDette gjelder gamle Jupyter. Bla lenger ned for √• lese om hvordan man lagrer PAT p√• DaplaLab\nDet er ganske upraktisk √• m√•tte sende med tokenet hver gang vi skal jobbe med GitHub. Vi b√∏r derfor lagre det lokalt der vi jobber, slik at Git automatisk finner det. Det finnes mange m√•ter √• gj√∏re dette p√• og det er ikke bestemt hva som skal v√¶re beste-praksis i SSB. Men en m√•te √• gj√∏re det er via en .netrc-fil. Vi oppretter da en fil som heter .netrc p√• v√•rt hjemmeomr√•de, og legger f√∏lgende informasjon p√• en (hvilken som helst) linje i filen:\n\n\n~/.netrc\n\nmachine GitHub.com login &lt;GitHub-bruker&gt; password &lt;Personal Access Token&gt;\n\nGitHub-bruker er da din personlige bruker og IKKE brukernavnet ditt i SSB. Personal Access Token er det vi lagde i forrige kapittelet.\nEn veldig enkel m√•te √• lagre dette er som f√∏lger. Anta at min personlige GitHub-bruker er SSB-Chad og at min Personal Access Token er blablabla. Da kan jeg gj√∏re f√∏lgende for √• lagre det i .netrc:\n\nG√• inn i Jupyterlab og √•pne en Python-notebook.\nI den f√∏rste kodecellen skriver du:\n\n\n\nterminal\n\n!echo \"machine GitHub.com login SSB-Chad password blablabla\" &gt;&gt; ~/.netrc\n\nAlternativt kan du fjerne utropstegnet og kj√∏re kommandoen direkte i terminalen. Det vil gi samme resultat. Koden over legger til en linje med teksten\nmachine GitHub.com login SSB-Chad password blablabla\ni en .netrc-fil p√• din hjemmeomr√•det, uavhengig av om du har en fra f√∏r eller ikke. Hvis du har en fil fra f√∏r som allerede har et token fra GitHub, ville jeg nok slettet det f√∏r jeg legger en et nytt token.\nHver gang du jobber mot GitHub vil Git sjekke om informasjon om autentisering ligger i denne filen, og bruke den hvis den ligger der.\n\n\n√Ö lagre PAT p√• DaplaLab er enkelt og gj√∏res kun en gang uansett hvor mange tjenester man bruker.\nHer er stegene:\n\nLogg inn p√• https://lab.dapla.ssb.no\nTrykk p√• ‚ÄòMy account‚Äô\nNaviger til Git-fanen\nLim inn token der det st√•r ‚ÄòGit Forge Personal Access Token‚Äô vist i Figur¬†13\n\n\n\n\n\n\n\nFigur¬†13: DaplaLab My account: lagre PAT\n\n\n\n\n\n\n\nI eksempelet over lagde vi en PAT som var gyldig i 90 dager. Dermed vil du ikke kunne jobbe mot GitHub med dette tokenet etter 90 dager. For √• oppdatere tokenet gj√∏r du f√∏lgende:\n\nLag et nytt PAT ved √• repetere Seksjon¬†1.2.4.1.\nI milj√∏et der du skal jobbe med Git og GitHub g√•r du inn i din .netrc og bytter ut token med det nye.\n\nOg med det er du klar til √• jobbe mot statisticsnorway p√• GitHub.",
    "crumbs": [
      "Manual",
      "Kode"
    ]
  },
  {
    "objectID": "statistikkere/git-og-github.html#footnotes",
    "href": "statistikkere/git-og-github.html#footnotes",
    "title": "Git og GitHub",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nBranches kan oversettes til grener p√• norsk. Men i denne boken velger vi √• bruke det engelske ordet branches. Grunnen er at det erfaringsmessig er lettere forholde seg til det engelske ordet n√•r man skal s√∏ke etter informasjon i annen dokumentasjon‚Ü©Ô∏é",
    "crumbs": [
      "Manual",
      "Kode"
    ]
  },
  {
    "objectID": "statistikkere/dashboard.html",
    "href": "statistikkere/dashboard.html",
    "title": "Dash og dashboard",
    "section": "",
    "text": "Hvis en √∏nsker √• lage ett dashbord som et brukergrensesnitt, s√• kan pakken Dash v√¶re et godt alternativ. Dash er ett rammeverk hvor man selv kan bygge opp applikasjoner i form av dashbord p√• en enklere m√•te, og det bygges opp√• javascript pakker som plotly.js og react.js. Det er et produkt ved siden av og helintegrert med plotly, som ogs√• er en annen pakke i Python som gir oss interaktive grafer. Dash er et godt verkt√∏y hvis en √∏nsker et dashbord som brukergrensesnitt for interaktiv visualisering av data. Dash kan kodes i Python og R, men ogs√• Julia og F#.\nI SSB kan man lage dashbord i virtuelle milj√∏er satt opp med ssb-project. For mer om h√•ndtering av pakker i ett virtuelt milj√∏ satt opp med ssb-project kan man se n√¶rmere her.\n\n\nI DaplaLab kan du starte opp ett dashbord ved hjelp av dash pakken enten i vscode-python tjenesten, eller i en notebook i jupyter tjenesten. Det fungerer best √• kj√∏re Dash-apper i en egen fane i nettleseren.\n\nvscode-python scriptjupyter notebookjupyter script\n\n\nHer er et eksempel p√• hvordan man lager en Dash-app i DaplaLab i en vscode tjeneste:\n\nInstaller pandas og dash i et ssb-project.\nOpprett et python-script i prosjektet som f.eks. heter app.py.\nKj√∏r scriptet ved √• kj√∏re f√∏lgende kommando fra terminalen: poetry run python ./app.py\nDeretter kommer det opp et dialog-vinduet hvor du velger Open in browser.\n\nHer er et eksempel p√• script som fungerer i Vscode-python:\n\n\napp.py\n\nfrom dash import Dash, html, dcc, callback, Output, Input\nimport plotly.express as px\nimport pandas as pd\n\n# Creating a sample dataset similar to the Gapminder dataset\ndata = {\n    'country': ['Afghanistan', 'Afghanistan', 'Afghanistan', 'Brazil', 'Brazil', 'Brazil', 'China', 'China', 'China'],\n    'continent': ['Asia', 'Asia', 'Asia', 'South America', 'South America', 'South America', 'Asia', 'Asia', 'Asia'],\n    'year': [1952, 1977, 2007, 1952, 1977, 2007, 1952, 1977, 2007],\n    'lifeExp': [28.801, 39.877, 43.828, 50.917, 59.504, 72.390, 44.000, 63.739, 72.961],\n    'pop': [8425333, 12412311, 31889923, 56602560, 80461570, 190010647, 556263527, 862030000, 1318683096],\n    'gdpPercap': [779.4453145, 786.11336, 974.5803384, 2108.944355, 5718.766744, 9065.800825, 400.448611, 1488.041713, 4959.114854]\n}\n\n# Creating DataFrame\ndf = pd.DataFrame(data)\n\n# Creating the app\nport = 8050 # must be between 1024 and 9999, default is 8050\napp = Dash(\n    __name__,\n    requests_pathname_prefix=f'/proxy/{port}/', \n    serve_locally=True\n)\n\napp.layout = [\n    html.H1(children='Title of Dash App', style={'textAlign':'center'}),\n    dcc.Dropdown(df.country.unique(), 'Canada', id='dropdown-selection'),\n    dcc.Graph(id='graph-content')\n]\n\n@callback(\n    Output('graph-content', 'figure'),\n    Input('dropdown-selection', 'value')\n)\ndef update_graph(value):\n    dff = df[df.country==value]\n    return px.line(dff, x='year', y='pop')\n\nif __name__ == '__main__':\n    app.run(port=port, debug=True)\n\nFor √• stoppe dashbordet fra √• kj√∏re, trykker du i terminalen ctrl + c.\n\n\nHer er et eksempel p√• hvordan man lager en Dash-app i DaplaLab i en jupyter tjeneste:\n\nInstaller pandas og dash i et ssb-project.\nOpprett en notebook i prosjektet som f.eks. heter app.ipynb.\n√Öpne notebooken og kj√∏r kodecellene p√• vanlig m√•te.\n\nHer er et eksempel p√• kode i notebook som fungerer i jupyter:\n\n\napp.ipynb\n\n# %%\n# Notebook cell 1\nfrom dash import Dash, html, dcc, callback, Output, Input\nimport plotly.express as px\nimport pandas as pd\nimport os\n\n# %%\n# Notebook cell 2\n# Creating a sample dataset similar to the Gapminder dataset\ndata = {\n    'country': ['Afghanistan', 'Afghanistan', 'Afghanistan', 'Brazil', 'Brazil', 'Brazil', 'China', 'China', 'China'],\n    'continent': ['Asia', 'Asia', 'Asia', 'South America', 'South America', 'South America', 'Asia', 'Asia', 'Asia'],\n    'year': [1952, 1977, 2007, 1952, 1977, 2007, 1952, 1977, 2007],\n    'lifeExp': [28.801, 39.877, 43.828, 50.917, 59.504, 72.390, 44.000, 63.739, 72.961],\n    'pop': [8425333, 12412311, 31889923, 56602560, 80461570, 190010647, 556263527, 862030000, 1318683096],\n    'gdpPercap': [779.4453145, 786.11336, 974.5803384, 2108.944355, 5718.766744, 9065.800825, 400.448611, 1488.041713, 4959.114854]\n}\n\n# Creating DataFrame\ndf = pd.DataFrame(data)\n\n# Creating the app\nport = 8050 # must be between 1024 and 9999\nservice_prefix = os.getenv('JUPYTERHUB_SERVICE_PREFIX', '/')\ndomain = os.getenv('JUPYTERHUB_HTTP_REFERER', None)\n\napp = Dash(\n    __name__,\n    requests_pathname_prefix=f\"{service_prefix}proxy/{port}/\", \n    serve_locally=True\n)\n\napp.layout = [\n    html.H1(children='Title of Dash App', style={'textAlign':'center'}),\n    dcc.Dropdown(df.country.unique(), 'Canada', id='dropdown-selection'),\n    dcc.Graph(id='graph-content')\n]\n\n@callback(\n    Output('graph-content', 'figure'),\n    Input('dropdown-selection', 'value')\n)\ndef update_graph(value):\n    dff = df[df.country==value]\n    return px.line(dff, x='year', y='pop')\n\nif __name__ == '__main__':\n    # For jupyter_mode, choose between 'external' or 'inline'.\n    # 'jupyterlab' should also be poosible, but doesn't seem to work...\n    app.run(debug=True, jupyter_mode=\"external\", jupyter_server_url=domain, port=port)\n\nFor √• stoppe dashbordet fra √• kj√∏re, restarter du kernelen i jupyterlab: Kernel -&gt; Restart Kernel and Clear Outputs of All Cells...\n\n\nHer er et eksempel p√• hvordan man lager en Dash-app i DaplaLab i en jupyter tjeneste:\n\nInstaller pandas og dash i et ssb-project.\nOpprett et python-script i prosjektet som f.eks. heter app.py.\nKj√∏r scriptet ved √• kj√∏re f√∏lgende kommando fra terminalen: poetry run python app.py\nDeretter dukker det opp en link i terminalen etter teksten ‚ÄòDash is running on‚Äô som du kan trykke p√• for √• f√• opp dashbordet.\n\nHer er et eksempel p√• script som fungerer i jupyter:\n\n\napp.py\n\nfrom dash import Dash, html, dcc, callback, Output, Input\nimport plotly.express as px\nimport pandas as pd\nimport os\n\n# Creating a sample dataset similar to the Gapminder dataset\ndata = {\n    'country': ['Afghanistan', 'Afghanistan', 'Afghanistan', 'Brazil', 'Brazil', 'Brazil', 'China', 'China', 'China'],\n    'continent': ['Asia', 'Asia', 'Asia', 'South America', 'South America', 'South America', 'Asia', 'Asia', 'Asia'],\n    'year': [1952, 1977, 2007, 1952, 1977, 2007, 1952, 1977, 2007],\n    'lifeExp': [28.801, 39.877, 43.828, 50.917, 59.504, 72.390, 44.000, 63.739, 72.961],\n    'pop': [8425333, 12412311, 31889923, 56602560, 80461570, 190010647, 556263527, 862030000, 1318683096],\n    'gdpPercap': [779.4453145, 786.11336, 974.5803384, 2108.944355, 5718.766744, 9065.800825, 400.448611, 1488.041713, 4959.114854]\n}\n\n# Creating DataFrame\ndf = pd.DataFrame(data)\n\n# Creating the app\nport = 8050 # must be between 1024 and 9999, default is 8050\nservice_prefix = os.getenv('JUPYTERHUB_SERVICE_PREFIX', '/')\ndomain = os.getenv('JUPYTERHUB_HTTP_REFERER', None)\nservice = f\"{service_prefix}proxy/{port}/\"\nurl = f\"{domain}{service[1:]}\"\ndefault_host = f\"http://127.0.0.1:{port}{service}\"\n\napp = Dash(\n    __name__,\n    requests_pathname_prefix=service,\n    serve_locally=True\n)\n\napp.layout = [\n    html.H1(children='Title of Dash App', style={'textAlign':'center'}),\n    dcc.Dropdown(df.country.unique(), 'Canada', id='dropdown-selection'),\n    dcc.Graph(id='graph-content')\n]\n\n@callback(\n    Output('graph-content', 'figure'),\n    Input('dropdown-selection', 'value')\n)\ndef update_graph(value):\n    dff = df[df.country==value]\n    return px.line(dff, x='year', y='pop')\n\nif __name__ == '__main__':\n    app.run(proxy = default_host + \"::\" + url, port=port, debug=True)\n\nFor √• stoppe dashbordet fra √• kj√∏re, trykker du i terminalen ctrl + c.\n\n\n\n\n\n\nDiverse som er verdt √• se n√¶rmere p√• n√•r en bygger dashbord applikasjon med Dash. Det f√∏lger med mange gode eksempler for bruk av diverse komponenter i dokumentasjonene under her.\n\nStartsiden til Dash\nLage interaktive grafer i Python med Plotly\nDash Core Components\nDash HTML Components\nDash Bootstrap Components\n\nVel og merke henter denne diverse materialer fra internett, og vil ikke fungere uten tilgang. Det er heller anbefalt √• ha de n√∏dvendige filene lagret lokalt for bruk av denne pakken.\nPakken i seg selv har en fordel i at det er lettere √• bygge opp utseende(layout) i dashbordet ditt, samt andre komponenter som ikke ligger i standard dash pakken.\n\nDash SSB Components\n\nTeam Metadata lager SSB komponenter i Dash, noe Datadoc er lagd med. Dette gir deg muligheten til √• bruke SSB komponentene i dine egne dashbord. Vel og merke er denne pakken fortsatt under utvikling, og ikke alle komponenter er p√• plass.",
    "crumbs": [
      "Manual",
      "Appendix"
    ]
  },
  {
    "objectID": "statistikkere/dashboard.html#eksempel-kode-i-daplalab",
    "href": "statistikkere/dashboard.html#eksempel-kode-i-daplalab",
    "title": "Dash og dashboard",
    "section": "",
    "text": "I DaplaLab kan du starte opp ett dashbord ved hjelp av dash pakken enten i vscode-python tjenesten, eller i en notebook i jupyter tjenesten. Det fungerer best √• kj√∏re Dash-apper i en egen fane i nettleseren.\n\nvscode-python scriptjupyter notebookjupyter script\n\n\nHer er et eksempel p√• hvordan man lager en Dash-app i DaplaLab i en vscode tjeneste:\n\nInstaller pandas og dash i et ssb-project.\nOpprett et python-script i prosjektet som f.eks. heter app.py.\nKj√∏r scriptet ved √• kj√∏re f√∏lgende kommando fra terminalen: poetry run python ./app.py\nDeretter kommer det opp et dialog-vinduet hvor du velger Open in browser.\n\nHer er et eksempel p√• script som fungerer i Vscode-python:\n\n\napp.py\n\nfrom dash import Dash, html, dcc, callback, Output, Input\nimport plotly.express as px\nimport pandas as pd\n\n# Creating a sample dataset similar to the Gapminder dataset\ndata = {\n    'country': ['Afghanistan', 'Afghanistan', 'Afghanistan', 'Brazil', 'Brazil', 'Brazil', 'China', 'China', 'China'],\n    'continent': ['Asia', 'Asia', 'Asia', 'South America', 'South America', 'South America', 'Asia', 'Asia', 'Asia'],\n    'year': [1952, 1977, 2007, 1952, 1977, 2007, 1952, 1977, 2007],\n    'lifeExp': [28.801, 39.877, 43.828, 50.917, 59.504, 72.390, 44.000, 63.739, 72.961],\n    'pop': [8425333, 12412311, 31889923, 56602560, 80461570, 190010647, 556263527, 862030000, 1318683096],\n    'gdpPercap': [779.4453145, 786.11336, 974.5803384, 2108.944355, 5718.766744, 9065.800825, 400.448611, 1488.041713, 4959.114854]\n}\n\n# Creating DataFrame\ndf = pd.DataFrame(data)\n\n# Creating the app\nport = 8050 # must be between 1024 and 9999, default is 8050\napp = Dash(\n    __name__,\n    requests_pathname_prefix=f'/proxy/{port}/', \n    serve_locally=True\n)\n\napp.layout = [\n    html.H1(children='Title of Dash App', style={'textAlign':'center'}),\n    dcc.Dropdown(df.country.unique(), 'Canada', id='dropdown-selection'),\n    dcc.Graph(id='graph-content')\n]\n\n@callback(\n    Output('graph-content', 'figure'),\n    Input('dropdown-selection', 'value')\n)\ndef update_graph(value):\n    dff = df[df.country==value]\n    return px.line(dff, x='year', y='pop')\n\nif __name__ == '__main__':\n    app.run(port=port, debug=True)\n\nFor √• stoppe dashbordet fra √• kj√∏re, trykker du i terminalen ctrl + c.\n\n\nHer er et eksempel p√• hvordan man lager en Dash-app i DaplaLab i en jupyter tjeneste:\n\nInstaller pandas og dash i et ssb-project.\nOpprett en notebook i prosjektet som f.eks. heter app.ipynb.\n√Öpne notebooken og kj√∏r kodecellene p√• vanlig m√•te.\n\nHer er et eksempel p√• kode i notebook som fungerer i jupyter:\n\n\napp.ipynb\n\n# %%\n# Notebook cell 1\nfrom dash import Dash, html, dcc, callback, Output, Input\nimport plotly.express as px\nimport pandas as pd\nimport os\n\n# %%\n# Notebook cell 2\n# Creating a sample dataset similar to the Gapminder dataset\ndata = {\n    'country': ['Afghanistan', 'Afghanistan', 'Afghanistan', 'Brazil', 'Brazil', 'Brazil', 'China', 'China', 'China'],\n    'continent': ['Asia', 'Asia', 'Asia', 'South America', 'South America', 'South America', 'Asia', 'Asia', 'Asia'],\n    'year': [1952, 1977, 2007, 1952, 1977, 2007, 1952, 1977, 2007],\n    'lifeExp': [28.801, 39.877, 43.828, 50.917, 59.504, 72.390, 44.000, 63.739, 72.961],\n    'pop': [8425333, 12412311, 31889923, 56602560, 80461570, 190010647, 556263527, 862030000, 1318683096],\n    'gdpPercap': [779.4453145, 786.11336, 974.5803384, 2108.944355, 5718.766744, 9065.800825, 400.448611, 1488.041713, 4959.114854]\n}\n\n# Creating DataFrame\ndf = pd.DataFrame(data)\n\n# Creating the app\nport = 8050 # must be between 1024 and 9999\nservice_prefix = os.getenv('JUPYTERHUB_SERVICE_PREFIX', '/')\ndomain = os.getenv('JUPYTERHUB_HTTP_REFERER', None)\n\napp = Dash(\n    __name__,\n    requests_pathname_prefix=f\"{service_prefix}proxy/{port}/\", \n    serve_locally=True\n)\n\napp.layout = [\n    html.H1(children='Title of Dash App', style={'textAlign':'center'}),\n    dcc.Dropdown(df.country.unique(), 'Canada', id='dropdown-selection'),\n    dcc.Graph(id='graph-content')\n]\n\n@callback(\n    Output('graph-content', 'figure'),\n    Input('dropdown-selection', 'value')\n)\ndef update_graph(value):\n    dff = df[df.country==value]\n    return px.line(dff, x='year', y='pop')\n\nif __name__ == '__main__':\n    # For jupyter_mode, choose between 'external' or 'inline'.\n    # 'jupyterlab' should also be poosible, but doesn't seem to work...\n    app.run(debug=True, jupyter_mode=\"external\", jupyter_server_url=domain, port=port)\n\nFor √• stoppe dashbordet fra √• kj√∏re, restarter du kernelen i jupyterlab: Kernel -&gt; Restart Kernel and Clear Outputs of All Cells...\n\n\nHer er et eksempel p√• hvordan man lager en Dash-app i DaplaLab i en jupyter tjeneste:\n\nInstaller pandas og dash i et ssb-project.\nOpprett et python-script i prosjektet som f.eks. heter app.py.\nKj√∏r scriptet ved √• kj√∏re f√∏lgende kommando fra terminalen: poetry run python app.py\nDeretter dukker det opp en link i terminalen etter teksten ‚ÄòDash is running on‚Äô som du kan trykke p√• for √• f√• opp dashbordet.\n\nHer er et eksempel p√• script som fungerer i jupyter:\n\n\napp.py\n\nfrom dash import Dash, html, dcc, callback, Output, Input\nimport plotly.express as px\nimport pandas as pd\nimport os\n\n# Creating a sample dataset similar to the Gapminder dataset\ndata = {\n    'country': ['Afghanistan', 'Afghanistan', 'Afghanistan', 'Brazil', 'Brazil', 'Brazil', 'China', 'China', 'China'],\n    'continent': ['Asia', 'Asia', 'Asia', 'South America', 'South America', 'South America', 'Asia', 'Asia', 'Asia'],\n    'year': [1952, 1977, 2007, 1952, 1977, 2007, 1952, 1977, 2007],\n    'lifeExp': [28.801, 39.877, 43.828, 50.917, 59.504, 72.390, 44.000, 63.739, 72.961],\n    'pop': [8425333, 12412311, 31889923, 56602560, 80461570, 190010647, 556263527, 862030000, 1318683096],\n    'gdpPercap': [779.4453145, 786.11336, 974.5803384, 2108.944355, 5718.766744, 9065.800825, 400.448611, 1488.041713, 4959.114854]\n}\n\n# Creating DataFrame\ndf = pd.DataFrame(data)\n\n# Creating the app\nport = 8050 # must be between 1024 and 9999, default is 8050\nservice_prefix = os.getenv('JUPYTERHUB_SERVICE_PREFIX', '/')\ndomain = os.getenv('JUPYTERHUB_HTTP_REFERER', None)\nservice = f\"{service_prefix}proxy/{port}/\"\nurl = f\"{domain}{service[1:]}\"\ndefault_host = f\"http://127.0.0.1:{port}{service}\"\n\napp = Dash(\n    __name__,\n    requests_pathname_prefix=service,\n    serve_locally=True\n)\n\napp.layout = [\n    html.H1(children='Title of Dash App', style={'textAlign':'center'}),\n    dcc.Dropdown(df.country.unique(), 'Canada', id='dropdown-selection'),\n    dcc.Graph(id='graph-content')\n]\n\n@callback(\n    Output('graph-content', 'figure'),\n    Input('dropdown-selection', 'value')\n)\ndef update_graph(value):\n    dff = df[df.country==value]\n    return px.line(dff, x='year', y='pop')\n\nif __name__ == '__main__':\n    app.run(proxy = default_host + \"::\" + url, port=port, debug=True)\n\nFor √• stoppe dashbordet fra √• kj√∏re, trykker du i terminalen ctrl + c.",
    "crumbs": [
      "Manual",
      "Appendix"
    ]
  },
  {
    "objectID": "statistikkere/dashboard.html#aktuell-dokumentasjon",
    "href": "statistikkere/dashboard.html#aktuell-dokumentasjon",
    "title": "Dash og dashboard",
    "section": "",
    "text": "Diverse som er verdt √• se n√¶rmere p√• n√•r en bygger dashbord applikasjon med Dash. Det f√∏lger med mange gode eksempler for bruk av diverse komponenter i dokumentasjonene under her.\n\nStartsiden til Dash\nLage interaktive grafer i Python med Plotly\nDash Core Components\nDash HTML Components\nDash Bootstrap Components\n\nVel og merke henter denne diverse materialer fra internett, og vil ikke fungere uten tilgang. Det er heller anbefalt √• ha de n√∏dvendige filene lagret lokalt for bruk av denne pakken.\nPakken i seg selv har en fordel i at det er lettere √• bygge opp utseende(layout) i dashbordet ditt, samt andre komponenter som ikke ligger i standard dash pakken.\n\nDash SSB Components\n\nTeam Metadata lager SSB komponenter i Dash, noe Datadoc er lagd med. Dette gir deg muligheten til √• bruke SSB komponentene i dine egne dashbord. Vel og merke er denne pakken fortsatt under utvikling, og ikke alle komponenter er p√• plass.",
    "crumbs": [
      "Manual",
      "Appendix"
    ]
  },
  {
    "objectID": "statistikkere/dapla-ctrl.html",
    "href": "statistikkere/dapla-ctrl.html",
    "title": "Dapla Ctrl",
    "section": "",
    "text": "Dapla Ctrl1 er tjeneste for tilgangsstyring p√• Dapla. Form√•let med appen er at det skal v√¶re lett √• f√• oversikt og administrere tilganger knyttet til Dapla-team.\n\n\nAlle som jobber i SSB kan logge seg inn p√• https://dapla-ctrl.intern.ssb.no/ for √• bruke tjenesten.\n\n\n\nAlle SSB-ansatte som logger seg inn i Dapla Ctrl f√•r tilgang til √• se informasjon om Dapla-team og tilganger. I tillegg kan de som er i tilgangsgruppen managers legge til, fjerne og endre medlemmer i teamet de har denne rollen. Seksjonsledere har tilgang til √• opprette nye team.\n\n\n\n\n\n\n\n\n\nFigur¬†1: Bilde av forsiden med oversikt over Mine team i Dapla Ctrl.\n\n\n\nFigur¬†1 viser landingssiden/forsiden som f√∏rst m√∏ter den som logger seg inn i Dapla Ctrl. den. Her f√•r den som er innlogget oversikt over hvilke Dapla-team man er medlem av, og f√∏lgende informasjon om teamene:\n\nTeknisk teamnavn\nTeamets eierseksjon\nAntall teammedlemmer\nManagers for teamet\n\nMan kan ogs√• bytte fane fra Mine team til Alle team for √• se samme informasjon om alle team som finnes p√• Dapla.\n\n\n\n\n\n\n\n\n\n\nFigur¬†2: Bilde av oversikten over et enkelt-team i Dapla Ctrl.\n\n\n\nFra Teamoversikten kan man trykke seg inn p√• et spesifikt team og f√• en oversikt slik som vist i Figur¬†2.\nP√• toppen av siden f√•r man se f√∏lgende informasjon:\n\nteamets visningsnavn\nteamets tekniske kortnavn\neierseksjonens seksjonsleder\neierseksjonens seksjonsnavn\nautonomitetsniv√•et til teamet\n\nVidere ser vi at det er en fane for Teammedlemmer og en for Delte Data. Under fanen for Teammedlemmer ser man f√∏lgende informasjon om alle medlemmene av teamet:\n\nnavn p√• medlem\nhvilken seksjons de jobber p√•\nhvilken tilgangsgruppe de tilh√∏rer p√• teamet\ne-postadresse med brukerens kortnavn\n\nUnder fanen Delte data f√•r man en oversikt over hvilke b√∏tter teamet har opprettet for √• dele data med andre team.\n\n\n\n\n\n\nFigur¬†3: Bilde av oversikten over hvilke b√∏tter teamet har opprettet for √• dele data med andre team.\n\n\n\nFigur¬†3 viser hvilken informasjon man f√•r over teamets delte data. F√∏lgende informasjon vises:\n\nkortnavnet p√• b√∏ttene\ntekniske navnet til b√∏ttene\nhvor mange team som har tilgang\nhvor mange personer som har tilgang2\n\n\n\n\n\n\n\n\n\n\n\nFigur¬†4: Bilde av oversikten over hvilke personer som har tilgang til en delt-b√∏tte.\n\n\n\nFra Teamvisningen kan man velge fanen Delte data og trykke seg p√• en av teamets delte-b√∏tter. Figur¬†4 viser informasjon man f√•r se i denne visningen. P√• toppen av siden f√•r man se kortnavnet til b√∏tta, det tekniske navnet p√• b√∏tta, hvilket team som eier b√∏tta og hvilken eierseksjon teamet har. I tabellen som vises kan man unders√∏ke hvilke personer som har tilgang til b√∏tta og f√• f√∏lgende informasjon om de:\n\nnavn\nhvilken seksjon de jobber p√•\nhvilket team-medelemskap de har tilgang i kraft av\nhvilken tilgangsgruppe de er i p√• teamet de har tilgang i kraft av\n\nFra tabellen kan man velge √• se n√¶rmere p√• personen som har tilgang, f.eks. se hvilke andre tilganger denne personen har, eller man kan se n√¶rmere p√• teamet som personen har tilgang i kraft av. Ved √• unders√∏ke teamet n√¶rmere kommer man inn p√• Teamvisningen som er beskrevet over, mens visning av Teammedlemmer forklares under.\n\n\n\n\n\n\n\n\n\n\nFigur¬†5: Bilde av oversikten over hvilke personer som med i dine team.\n\n\n\nFigur¬†5 viser oversikt over teammedlemmer. √òverst p√• siden kan man velge mellom en fane for Mine teammedlemmer og Alle teammedlemmer. F√∏rstnevnte viser hvilke andre medlemmer som er i de teamene den innloggede er med i, mens sistnevnte viser alle teammedlemmer i SSB3. I tabellen under f√•r man f√∏lgende informasjon om teammedlemmene:\n\nnavn\nhvilken seksjons de jobber p√•\nhvor mange team de er medlem av\nhvor mange team de har tilgangsrollen data-admins\nnavn p√• personens seksjonsleder\n\n\n\n\n\n\n\n\n\n\n\nFigur¬†6: Bilde av oversikten over hvilke team en person er medlem av.\n\n\n\nFigur¬†6 viser hva man ser n√•r g√•r inn p√• en enkeltperson, enten via Teamoversikten eller Teammedlemmer. √òverst p√• siden st√•r navnet til personen, hvorvidt de har arbeidssted i Oslo eller Kongsvinger4, hvilken seksjon de jobber p√• og e-postadressen deres.\nTabellen i Figur¬†6 f√•r man en oversikt over hvilke team personen er medlem av, samt f√∏lgende detaljer:\n\nteamets tekniske kortnavn\nseksjonseier av teamet\nhvilke tilgangsgrupper personen er med i\nhvem som er managers for teamet\n\nVidere kan man g√• videre inn p√• et av teamene og se n√¶rmere p√• hvem som er medlemmer og hvilke data de deler.\n\n\n\n\n\nDet er kun seksjonsledere i SSB som kan opprette et Dapla-team. Hvis en seksjonsleder logger seg inn i Dapla Ctrl s√• vil knappen i Figur¬†7 vises p√• Teamoversikt-siden. Eierseksjonen til et team vil bli definert av hvilken seksjonsleder som oppretter teamet.\n\n\n\n\n\n\n\n\n\nFigur¬†7: Bilde av knappen som vises for seksjonsledere.\n\n\n\n\n\nN√•r man oppretter et team m√• man fylle ut skjemaet i Figur¬†8. Under finner du en oversikt hva som er viktig √• vurdere n√•r man fyller ut de ulike feltene.\n\n\n\n\nVisningsnavn er teamets navn i et lesevennlig format. Navnet b√∏r best√• av et hoveddomenet og et subdomenet. Det er tillatt med sm√•/store bokstaver, mellomrom, √Ü, √ò og √Ö.\nEksempel p√• et hoveddomenet i SSB er Skatt, og under det finnes det subdomener som Person og N√¶ring. Visningsnavnet til teamene er da Skatt Person og Skatt N√¶ring.\nI noen tilfeller gir det ikke mening med et subdomenet og da er det greit √• kun ha et hoveddomenet. Et eksempel p√• et visningsnavn som kun har hoveddomenet er Nasjonalregnskap.\n\n\n\n\n\n\n\n\n\n\nFigur¬†8: Bilde av siden for opprettelse av team.\n\n\n\n\n\n\n\nVelg dette for √• overstyre det automatisk genererte tekniske teamnavnet. Hvis man ikke krysser av denne boksen vil det genereres et teknisk teamnavn basert p√• visningsnavnet. Det kan v√¶re nyttig √• velge dette hvis man har et langt visningsnavn og √∏nsker √• forkorte det genererte tekniske teamnavnet.\n\n\n\nTeamets navn i et maskinvennlig format som bl.a. benyttes i filstier til lagringsb√∏tter. Det er ikke tillatt med mellomrom og norske tegn (√Ü, √ò og √Ö). Navnet kan ikke overskride 17 tegn.\nDet tekniske teamnavnet blir automatisk generert basert p√• visningsnavn hvis man ikke velger Overstyr teknisk navn. Tabell¬†1 viser eksempler p√• visningsnavn der man b√•de har overtyrt det tekniske navnet og ikke.\n\n\n\nTabell¬†1: Eksempler p√• teamnavn\n\n\n\n\n\nVisningsnavn\nTeknisk navn\nOverstyrt\n\n\n\n\nSkatt Person\nskatt-person\nnei\n\n\nSkatt N√¶ring\nskatt-naering\nnei\n\n\nNasjonalregnskap\nnr\nja\n\n\nFinansmarkedsstatistikk\nfinmark\nja\n\n\n\n\n\n\n\n\n\nAlle team tilh√∏rer en seksjon og denne informasjonen ligger lagret i metadataene til teamet. Standard er at seksjonslederen som s√∏ker fyller ut sitt seksjonsnummer her, men det er mulig √• velge andre seksjoner.\n\n\n\nNiv√• av frihet et team har til √• definere sin egen infrastruktur. Statistikkproduserende team er vanligvis i kategorien Managed, dvs. at de kun bruker tjenester som tilbys av plattformen. IT-team vil ofte defineres som Self-Managed fordi dette gir st√∏rre kontroll til teamet. Les mer her.\n\n\n\n\n\n\nHvis man som medlem av tilgangsgruppen managers logger seg inn Dapla Ctrl, vil man f√• tilgang til √• legge til medlemmer i teamet man er i managers-gruppa for. G√•r man inn p√• Teamvisning vil man se knappen i Figur¬†9.\n\n\n\n\n\n\n\n\n\nFigur¬†9: Bilde av knappen som vises for managers.\n\n\n\n\n\nTrykker man p√• knappen s√• f√•r man opp en side for √• legge til nye medlemmer i teamet, slik som vist Figur¬†10. Man kan s√∏ke opp alle ansatte i SSB, og man kan velge √• legge de til i en eller flere tilgangsgrupper. N√•r man har valgt person, og hvilke tilgangsgrupper de skal legges i, s√• avslutter man med √• trykke p√• Legg til medlem for √• effektuere endringen.\n\n\n\n\n\n\nFigur¬†10: Bilde av siden for √• legge til medlemmer.\n\n\n\nDet kan ta mellom 1-2 minutter f√∏r tilgangen er aktivert og klar til bruk.\n\n\n\nHvis man som medlem av tilgangsgruppen managers logger seg inn Dapla Ctrl, vil man f√• tilgang til √• fjerne og endre medlemmer i teamet man er i managers-gruppa for. G√•r man inn p√• Teamvisning s√• vil man se Endre-knapp for hver person i teamet.\n\n\n\n\n\n\nFigur¬†11: Bilde av Teamvisning som vises for personer i tilgangsgruppen managers.\n\n\n\n\n\nAv Figur¬†11 ser vi at hvert medlem i teamet har en Endre-knapp. Trykker man p√• den s√• f√•r man opp bilde som vises i Figur¬†12.\n√ònsker man √• fjerne et medlem fra teamet, s√• kan man bare trykke p√• Fjern fra teamet. Da vil man bli spurt om √• bekrefte at personen skal gjernes, og velger man ok s√• effektureres endringen ila et par minutter.\n√ònsker man endre hvilken tilgangsgruppe en person er med i, s√• gj√∏r man det ved √• enten fjerne eller legge til tilganger som listet under dropdown-menyen for Tilgangsgruppe(r). For eksempel hvis en person ligger som b√•de data-admins og developers, slik som eksempelet i Figur¬†12, s√• trykker man bare p√• X-ikonet for den tilgangen, og til slutt effektuerer man endringen ved √• velge Oppdater tilgang.\n\n\n\n\n\n\n\n\n\nFigur¬†12: Bilde av siden for √• endre eller fjerne medlemmer.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Dapla Ctrl"
    ]
  },
  {
    "objectID": "statistikkere/dapla-ctrl.html#innlogging",
    "href": "statistikkere/dapla-ctrl.html#innlogging",
    "title": "Dapla Ctrl",
    "section": "",
    "text": "Alle som jobber i SSB kan logge seg inn p√• https://dapla-ctrl.intern.ssb.no/ for √• bruke tjenesten.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Dapla Ctrl"
    ]
  },
  {
    "objectID": "statistikkere/dapla-ctrl.html#funksjonalitet",
    "href": "statistikkere/dapla-ctrl.html#funksjonalitet",
    "title": "Dapla Ctrl",
    "section": "",
    "text": "Alle SSB-ansatte som logger seg inn i Dapla Ctrl f√•r tilgang til √• se informasjon om Dapla-team og tilganger. I tillegg kan de som er i tilgangsgruppen managers legge til, fjerne og endre medlemmer i teamet de har denne rollen. Seksjonsledere har tilgang til √• opprette nye team.\n\n\n\n\n\n\n\n\n\nFigur¬†1: Bilde av forsiden med oversikt over Mine team i Dapla Ctrl.\n\n\n\nFigur¬†1 viser landingssiden/forsiden som f√∏rst m√∏ter den som logger seg inn i Dapla Ctrl. den. Her f√•r den som er innlogget oversikt over hvilke Dapla-team man er medlem av, og f√∏lgende informasjon om teamene:\n\nTeknisk teamnavn\nTeamets eierseksjon\nAntall teammedlemmer\nManagers for teamet\n\nMan kan ogs√• bytte fane fra Mine team til Alle team for √• se samme informasjon om alle team som finnes p√• Dapla.\n\n\n\n\n\n\n\n\n\n\nFigur¬†2: Bilde av oversikten over et enkelt-team i Dapla Ctrl.\n\n\n\nFra Teamoversikten kan man trykke seg inn p√• et spesifikt team og f√• en oversikt slik som vist i Figur¬†2.\nP√• toppen av siden f√•r man se f√∏lgende informasjon:\n\nteamets visningsnavn\nteamets tekniske kortnavn\neierseksjonens seksjonsleder\neierseksjonens seksjonsnavn\nautonomitetsniv√•et til teamet\n\nVidere ser vi at det er en fane for Teammedlemmer og en for Delte Data. Under fanen for Teammedlemmer ser man f√∏lgende informasjon om alle medlemmene av teamet:\n\nnavn p√• medlem\nhvilken seksjons de jobber p√•\nhvilken tilgangsgruppe de tilh√∏rer p√• teamet\ne-postadresse med brukerens kortnavn\n\nUnder fanen Delte data f√•r man en oversikt over hvilke b√∏tter teamet har opprettet for √• dele data med andre team.\n\n\n\n\n\n\nFigur¬†3: Bilde av oversikten over hvilke b√∏tter teamet har opprettet for √• dele data med andre team.\n\n\n\nFigur¬†3 viser hvilken informasjon man f√•r over teamets delte data. F√∏lgende informasjon vises:\n\nkortnavnet p√• b√∏ttene\ntekniske navnet til b√∏ttene\nhvor mange team som har tilgang\nhvor mange personer som har tilgang2\n\n\n\n\n\n\n\n\n\n\n\nFigur¬†4: Bilde av oversikten over hvilke personer som har tilgang til en delt-b√∏tte.\n\n\n\nFra Teamvisningen kan man velge fanen Delte data og trykke seg p√• en av teamets delte-b√∏tter. Figur¬†4 viser informasjon man f√•r se i denne visningen. P√• toppen av siden f√•r man se kortnavnet til b√∏tta, det tekniske navnet p√• b√∏tta, hvilket team som eier b√∏tta og hvilken eierseksjon teamet har. I tabellen som vises kan man unders√∏ke hvilke personer som har tilgang til b√∏tta og f√• f√∏lgende informasjon om de:\n\nnavn\nhvilken seksjon de jobber p√•\nhvilket team-medelemskap de har tilgang i kraft av\nhvilken tilgangsgruppe de er i p√• teamet de har tilgang i kraft av\n\nFra tabellen kan man velge √• se n√¶rmere p√• personen som har tilgang, f.eks. se hvilke andre tilganger denne personen har, eller man kan se n√¶rmere p√• teamet som personen har tilgang i kraft av. Ved √• unders√∏ke teamet n√¶rmere kommer man inn p√• Teamvisningen som er beskrevet over, mens visning av Teammedlemmer forklares under.\n\n\n\n\n\n\n\n\n\n\nFigur¬†5: Bilde av oversikten over hvilke personer som med i dine team.\n\n\n\nFigur¬†5 viser oversikt over teammedlemmer. √òverst p√• siden kan man velge mellom en fane for Mine teammedlemmer og Alle teammedlemmer. F√∏rstnevnte viser hvilke andre medlemmer som er i de teamene den innloggede er med i, mens sistnevnte viser alle teammedlemmer i SSB3. I tabellen under f√•r man f√∏lgende informasjon om teammedlemmene:\n\nnavn\nhvilken seksjons de jobber p√•\nhvor mange team de er medlem av\nhvor mange team de har tilgangsrollen data-admins\nnavn p√• personens seksjonsleder\n\n\n\n\n\n\n\n\n\n\n\nFigur¬†6: Bilde av oversikten over hvilke team en person er medlem av.\n\n\n\nFigur¬†6 viser hva man ser n√•r g√•r inn p√• en enkeltperson, enten via Teamoversikten eller Teammedlemmer. √òverst p√• siden st√•r navnet til personen, hvorvidt de har arbeidssted i Oslo eller Kongsvinger4, hvilken seksjon de jobber p√• og e-postadressen deres.\nTabellen i Figur¬†6 f√•r man en oversikt over hvilke team personen er medlem av, samt f√∏lgende detaljer:\n\nteamets tekniske kortnavn\nseksjonseier av teamet\nhvilke tilgangsgrupper personen er med i\nhvem som er managers for teamet\n\nVidere kan man g√• videre inn p√• et av teamene og se n√¶rmere p√• hvem som er medlemmer og hvilke data de deler.\n\n\n\n\n\nDet er kun seksjonsledere i SSB som kan opprette et Dapla-team. Hvis en seksjonsleder logger seg inn i Dapla Ctrl s√• vil knappen i Figur¬†7 vises p√• Teamoversikt-siden. Eierseksjonen til et team vil bli definert av hvilken seksjonsleder som oppretter teamet.\n\n\n\n\n\n\n\n\n\nFigur¬†7: Bilde av knappen som vises for seksjonsledere.\n\n\n\n\n\nN√•r man oppretter et team m√• man fylle ut skjemaet i Figur¬†8. Under finner du en oversikt hva som er viktig √• vurdere n√•r man fyller ut de ulike feltene.\n\n\n\n\nVisningsnavn er teamets navn i et lesevennlig format. Navnet b√∏r best√• av et hoveddomenet og et subdomenet. Det er tillatt med sm√•/store bokstaver, mellomrom, √Ü, √ò og √Ö.\nEksempel p√• et hoveddomenet i SSB er Skatt, og under det finnes det subdomener som Person og N√¶ring. Visningsnavnet til teamene er da Skatt Person og Skatt N√¶ring.\nI noen tilfeller gir det ikke mening med et subdomenet og da er det greit √• kun ha et hoveddomenet. Et eksempel p√• et visningsnavn som kun har hoveddomenet er Nasjonalregnskap.\n\n\n\n\n\n\n\n\n\n\nFigur¬†8: Bilde av siden for opprettelse av team.\n\n\n\n\n\n\n\nVelg dette for √• overstyre det automatisk genererte tekniske teamnavnet. Hvis man ikke krysser av denne boksen vil det genereres et teknisk teamnavn basert p√• visningsnavnet. Det kan v√¶re nyttig √• velge dette hvis man har et langt visningsnavn og √∏nsker √• forkorte det genererte tekniske teamnavnet.\n\n\n\nTeamets navn i et maskinvennlig format som bl.a. benyttes i filstier til lagringsb√∏tter. Det er ikke tillatt med mellomrom og norske tegn (√Ü, √ò og √Ö). Navnet kan ikke overskride 17 tegn.\nDet tekniske teamnavnet blir automatisk generert basert p√• visningsnavn hvis man ikke velger Overstyr teknisk navn. Tabell¬†1 viser eksempler p√• visningsnavn der man b√•de har overtyrt det tekniske navnet og ikke.\n\n\n\nTabell¬†1: Eksempler p√• teamnavn\n\n\n\n\n\nVisningsnavn\nTeknisk navn\nOverstyrt\n\n\n\n\nSkatt Person\nskatt-person\nnei\n\n\nSkatt N√¶ring\nskatt-naering\nnei\n\n\nNasjonalregnskap\nnr\nja\n\n\nFinansmarkedsstatistikk\nfinmark\nja\n\n\n\n\n\n\n\n\n\nAlle team tilh√∏rer en seksjon og denne informasjonen ligger lagret i metadataene til teamet. Standard er at seksjonslederen som s√∏ker fyller ut sitt seksjonsnummer her, men det er mulig √• velge andre seksjoner.\n\n\n\nNiv√• av frihet et team har til √• definere sin egen infrastruktur. Statistikkproduserende team er vanligvis i kategorien Managed, dvs. at de kun bruker tjenester som tilbys av plattformen. IT-team vil ofte defineres som Self-Managed fordi dette gir st√∏rre kontroll til teamet. Les mer her.\n\n\n\n\n\n\nHvis man som medlem av tilgangsgruppen managers logger seg inn Dapla Ctrl, vil man f√• tilgang til √• legge til medlemmer i teamet man er i managers-gruppa for. G√•r man inn p√• Teamvisning vil man se knappen i Figur¬†9.\n\n\n\n\n\n\n\n\n\nFigur¬†9: Bilde av knappen som vises for managers.\n\n\n\n\n\nTrykker man p√• knappen s√• f√•r man opp en side for √• legge til nye medlemmer i teamet, slik som vist Figur¬†10. Man kan s√∏ke opp alle ansatte i SSB, og man kan velge √• legge de til i en eller flere tilgangsgrupper. N√•r man har valgt person, og hvilke tilgangsgrupper de skal legges i, s√• avslutter man med √• trykke p√• Legg til medlem for √• effektuere endringen.\n\n\n\n\n\n\nFigur¬†10: Bilde av siden for √• legge til medlemmer.\n\n\n\nDet kan ta mellom 1-2 minutter f√∏r tilgangen er aktivert og klar til bruk.\n\n\n\nHvis man som medlem av tilgangsgruppen managers logger seg inn Dapla Ctrl, vil man f√• tilgang til √• fjerne og endre medlemmer i teamet man er i managers-gruppa for. G√•r man inn p√• Teamvisning s√• vil man se Endre-knapp for hver person i teamet.\n\n\n\n\n\n\nFigur¬†11: Bilde av Teamvisning som vises for personer i tilgangsgruppen managers.\n\n\n\n\n\nAv Figur¬†11 ser vi at hvert medlem i teamet har en Endre-knapp. Trykker man p√• den s√• f√•r man opp bilde som vises i Figur¬†12.\n√ònsker man √• fjerne et medlem fra teamet, s√• kan man bare trykke p√• Fjern fra teamet. Da vil man bli spurt om √• bekrefte at personen skal gjernes, og velger man ok s√• effektureres endringen ila et par minutter.\n√ònsker man endre hvilken tilgangsgruppe en person er med i, s√• gj√∏r man det ved √• enten fjerne eller legge til tilganger som listet under dropdown-menyen for Tilgangsgruppe(r). For eksempel hvis en person ligger som b√•de data-admins og developers, slik som eksempelet i Figur¬†12, s√• trykker man bare p√• X-ikonet for den tilgangen, og til slutt effektuerer man endringen ved √• velge Oppdater tilgang.\n\n\n\n\n\n\n\n\n\nFigur¬†12: Bilde av siden for √• endre eller fjerne medlemmer.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Dapla Ctrl"
    ]
  },
  {
    "objectID": "statistikkere/dapla-ctrl.html#footnotes",
    "href": "statistikkere/dapla-ctrl.html#footnotes",
    "title": "Dapla Ctrl",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nNavnet Dapla Ctrl er valgt for √• kommunisere at m√•lsetning med appen er √• gi SSB-ere kontroll over tilgangsstyring p√• Dapla p√• en effektiv m√•te.‚Ü©Ô∏é\nAntall personer som har tilgang til en delt-b√∏tte viser hvor mange personer det er som har tilgang fra de teamene som har tilgang. Som regel vil det v√¶re slik at kun noen tilgangsgrupper i et team f√•r tilgang til andre sine delte data, og ikke hele teamet.‚Ü©Ô∏é\nAlle teammedlemmer vil i praksis si alle ansatte i SSB, siden teamet Dapla Felles alltid legger til alle ansatte i SSB. Form√•let med dette er √• la alle ansatte f√• tilgang til testdata i en b√∏tte.‚Ü©Ô∏é\nHvorvidt arbeidssted er Oslo eller Kongsvinger indikeres med henholdsvis O eller K f√∏r seksjonsnummeret.‚Ü©Ô∏é",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Dapla Ctrl"
    ]
  },
  {
    "objectID": "news/posts/2025-02-07-parquet-viewer/index.html",
    "href": "news/posts/2025-02-07-parquet-viewer/index.html",
    "title": "Parquet-utforsker i VS Code",
    "section": "",
    "text": "Sjekk ut den nye Parquet-utforskeren i Vscode-python. Skrive SQL, filtrer,inspiser metadata uten bruk av Python eller R. Les mer i denne blogg-artikkelen."
  },
  {
    "objectID": "news/posts/2025-02-14-cost-in-dapla-lab/index.html",
    "href": "news/posts/2025-02-14-cost-in-dapla-lab/index.html",
    "title": "Estimerte kostnader i Dapla Lab",
    "section": "",
    "text": "Du kan n√• se et estimat for hvor mye din tjeneste koster under Ressurser-fanen i tjenestekonfigurasjonen p√• Dapla Lab. Dette er ikke et n√∏yaktig estimat, men kan gi brukeren en indikasjon p√• hvor mye ressursene vil koste ila en arbeidsdag."
  },
  {
    "objectID": "news/posts/2025-01-22-filinnsamling-moveit/index.html",
    "href": "news/posts/2025-01-22-filinnsamling-moveit/index.html",
    "title": "Nytt kapittel om filinnsamling via MoveIT p√• Dapla",
    "section": "",
    "text": "Mange statistikker mottar data fra eksterne som filer via MoveIT-systemet p√• bakken. Disse filene kan automatisk synkroniseres til kildeb√∏tta p√• Dapla. Les mer om hvordan dette settes opp i dette kapitlet i Dapla-manualen."
  },
  {
    "objectID": "news/posts/2024-11-25-referansegruppe-dapla-lab/index.html",
    "href": "news/posts/2024-11-25-referansegruppe-dapla-lab/index.html",
    "title": "N√•r skrus ‚Äògamle‚Äô jupyter av?",
    "section": "",
    "text": "Under forrige DaplaNytt ble 15. januar 2025 nevt som en mulig sluttdato for ‚Äògamle‚Äô jupyter, alts√• https://jupyter.dapla.ssb.no, og dermed en endelig overgang til Dapla Lab.\nDenne datoen er tentativ. Sluttdato for gamle jupyter vil bli endeliggjort etter m√∏tet med referansegruppen for statistikktjenester 6. desember 2024. Referansegruppen har representanter fra hver statistikkavdeling.\nI mellomtiden ber vi brukere om √• henvende seg til sine respektive st√∏tteteam, n√¶rmeste leder eller medlemmene av referansegruppen for innvendinger eller innspill til n√•r vi kan gjennomf√∏re en endelig overgang til Dapla Lab."
  },
  {
    "objectID": "news/posts/2025-02-09-monter-delt-b√∏tter/index.html",
    "href": "news/posts/2025-02-09-monter-delt-b√∏tter/index.html",
    "title": "Delt-b√∏tter st√∏ttes i Dapla Lab",
    "section": "",
    "text": "Man kan n√• tilgjengeliggj√∏re delt-b√∏tter fra andre team inne i Jupyter, Vscode-python og Rstudio. Les dokumentasjon her, eller se videoen under:"
  },
  {
    "objectID": "news/posts/2024-11-09-data-admins-developers/index.html",
    "href": "news/posts/2024-11-09-data-admins-developers/index.html",
    "title": "Data-admins i developers-gruppa",
    "section": "",
    "text": "Det er bestemt at medlemmer av tilgangsgruppen data-admins ogs√• skal v√¶re medlem av developers-gruppa. Det er p√• denne m√•ten at data-admins ogs√• f√•r tilgang til standardprosjektet til teamet. Dette er en beslutning om arkitekturen rundt tilgangsstyring p√• Dapla, der medlemskap i de to gruppene gir helt forskjellige tilganger. Medlemskap i data-admins-gruppa vil da vanligvis inneb√¶re de samme tilgangene som developers-gruppa, med mindre de aktiverer de forh√•ndsgodkjente tilgangene til kildedata."
  },
  {
    "objectID": "news/posts/2024-12-16-blogpost-pandera/index.html",
    "href": "news/posts/2024-12-16-blogpost-pandera/index.html",
    "title": "Nytt blogginnlegg om Pandera",
    "section": "",
    "text": "Manualens blogg har blitt oppdatert med en post om hvordan validere data i kode ved hjelp av Python pakken Pandera. Les mer her."
  },
  {
    "objectID": "news/posts/2024-11-08-env-vars/index.html",
    "href": "news/posts/2024-11-08-env-vars/index.html",
    "title": "Faste milj√∏variabler i Dapla Lab",
    "section": "",
    "text": "Vi har oppdatert listen over milj√∏variabler som brukere av programmeringsmilj√∏ene (Jupyter, VS Code og RStudio) i Dapla Lab kan forvente √• finne. Form√•let med disse er hovedsakelig √• tilby de som lager Python- eller R-biblioteker en enkel √• sjekke hvilket milj√∏ koden kj√∏res i. Vi har ogs√• noen milj√∏variabler som gj√∏r at de som lager pakker slipper √• hardkode url-er i koden sin."
  },
  {
    "objectID": "news/posts/2025-01-16-manualen-omstrukturert/index.html",
    "href": "news/posts/2025-01-16-manualen-omstrukturert/index.html",
    "title": "Omstrukturering av manualen!",
    "section": "",
    "text": "Finner du ikke favorittartiklene dine? Det er fordi vi har omstrukturert manualen. Vi har delt opp det som tidligere var jobbe-med-kode inn i 3 forksjellige artikler: SSB-project, Pakkeh√•ndtering i Python og Pakkeh√•ndtering i R."
  },
  {
    "objectID": "news/posts/2024-11-26-statbank-dapla-lab/index.html",
    "href": "news/posts/2024-11-26-statbank-dapla-lab/index.html",
    "title": "Dapla Lab st√∏tter Statbank",
    "section": "",
    "text": "Fra og med forrige uke st√∏tter Statbank-pakken lasting fra Dapla Lab. Dette er i forbindelse med ny versjon av Python-pakken dapla-statbank-client.\nLes mer om dette i innlegget til Carl Corneil p√• Viva Engage."
  },
  {
    "objectID": "news/posts/2024-11-08-collector-chapter/index.html",
    "href": "news/posts/2024-11-08-collector-chapter/index.html",
    "title": "Nytt kapittel om Data Collector",
    "section": "",
    "text": "Vi har skrevet et kapittel i Dapla-manualen som beskriver hvordan man bruker Data Collector. Sjekk det ut her."
  },
  {
    "objectID": "news/posts/2025-03-03-dapla-lab-nosuspend/index.html",
    "href": "news/posts/2025-03-03-dapla-lab-nosuspend/index.html",
    "title": "Skru av automatisk pausing for en tjeneste",
    "section": "",
    "text": "Alle tjenester p√• Dapla Lab pauses hver kveld kl. 22:00, slik som forklart i dokumentasjonen. N√• er det mulig √• skru av pausing for enkelttjenester ved √• skrive [nosuspend] p√• slutten av visningsnavnet til tjenesten, slik som vist i videoen under:\n\nDenne funksjonaliteten kan skrus av og p√• etter at tjenesten er startet."
  },
  {
    "objectID": "news/posts/2025-01-14-dapla-ctrl-ny-url/index.html",
    "href": "news/posts/2025-01-14-dapla-ctrl-ny-url/index.html",
    "title": "Dapla Ctrl har f√•tt ny nettadresse",
    "section": "",
    "text": "Nettadressen til Dapla Ctrl er n√• endret fra https://ctrl.dapla.ssb.no/ til https://dapla-ctrl.intern.ssb.no/. Dette er gjort som en del av overgangen til NAIS-plattformen."
  },
  {
    "objectID": "blog/posts/2024-12-16-data-validering-pandera/index.html",
    "href": "blog/posts/2024-12-16-data-validering-pandera/index.html",
    "title": "Intro til Pandera",
    "section": "",
    "text": "Pandera er en python pakke og rammeverk for testing av data - alts√• datavalidering.\nBegrepet testing kan f√∏re til misforst√•elser mellom statistikkere og utviklere: en statistikker vil ofte tenke p√• testing av data, og utvikler p√• testing av kode. Sistnevnte omtales som enhetstesting.\nDet finnes flere rammeverk for testing av kode og datavalidering. N√•r det kommer til Python bruker vi i SSB som oftest Pytest pakken for testing av kode, og Pandera eller Pydantic pakkene for datavalidering. Alle disse pakkene st√•r oppf√∏rt p√• godkjentlista i SSB.\nPandera eller Pydantic? Hvem av dem som b√∏r benyttes avhenger mest av strukturen p√• dataene din. Dersom dataene er semi-strukturert (ofte filformater som json og xml) s√• vil fort Pydantic v√¶re mest aktuell, mens er dataene strukturerte (som en dataframe, eller filformat som csv) s√• vil Pandera v√¶re ett mer naturlig valg. Her vil det gis en intro til Pandera. Vel og merke vil innholdet her dreie seg om grunnleggende bruk, samt forskjellige tips og triks i hvordan det kan brukes, og muligens en bonus til slutt. Mer avanserte temaer, som f.eks. hypotesetesting, er ikke med her.\nMen hvorfor Pandera? Og hvorfor validere data? Siste er enkelt √• besvare og ligger godt integrert i SSBs samfunnsansvar: Vi skal ha god kvalitet i all statistikk, forskning og analyse.\nI den moderniseringsprosessen SSB er i, overgang til Dapla, er det naturlig at dette integreres i kodene v√•re. Det er en anbefaling fra KVAKK anbefaler ogs√• √• kontrollere data for hvert trinn. Da er datavalideringspakker som Pandera h√∏yst aktuell. I tillegg er det en annen anbefaling fra KVAKK, og en ADR vedtatt i SSB, om at kildekode skal v√¶re offentlig tilgjengelig. En eller annen gang skal alts√• produksjonskoden v√•r bli offentlig tilgjengelig. Dette er kanskje mine personlige meninger rundt det, men jeg vil tro at det vil foreligge en stor forventning der ute om at SSB validerer data i kode. Selv om Pandera er relativt nytt st√∏tter den de aller mest brukte dataframe-rammeverkene som er i bruk i SSB, slik som Pandas, Polars, og PySpark."
  },
  {
    "objectID": "blog/posts/2024-12-16-data-validering-pandera/index.html#import-og-testdata",
    "href": "blog/posts/2024-12-16-data-validering-pandera/index.html#import-og-testdata",
    "title": "Intro til Pandera",
    "section": "Import og testdata",
    "text": "Import og testdata\nF√∏rst importerer vi noen biblioteker som vi skal benytte. For √• benytte Pandera pakken m√• det lastes inn til ett virtuelt milj√∏, som vi i SSB benytter ssb-project for;\n\n\nterminal\n\npoetry add pandera\n\nVersjonen av Pandera som benyttes i introduksjonen her er 0.20.4. F√∏lgende pakker f√•r jeg importert deretter;\n\nimport uuid\nfrom typing import Dict\nimport pandas as pd\nimport numpy as np\nimport pandera as pan\nfrom pandera.typing import DataFrame, Series\nfrom pandera.errors import SchemaErrors\n\nJeg lager ogs√• f√∏lgende lekedata vi skal ta for oss i eksemplene;\n\nsize = 6\n\nrandom_data = pd.DataFrame({\n    \"id_nr\": [str(uuid.uuid4()) for _ in range(size)],\n    \"lope_id_nr\": [\"L\" + str(1).zfill(4) for _ in range(size)],\n    \"aar\": np.random.choice(['2023', '2024'], size),\n    \"navn\": np.random.choice(['Ola', 'Kari', 'Per', 'Ida'], size),\n    \"produkt\": np.random.choice(['Eple', 'Gulrot', 'Brokkoli'], size),\n    \"salgsverdi\": np.random.randint(1000, 10000, size),\n    \"vekt\": np.random.randint(500, 5000, size)\n})\n\nrandom_data['kostverdi'] = (\n    random_data['salgsverdi'] * 0.75\n).astype(int)\n\nbad_data = pd.DataFrame({\n    \"id_nr\": [\"random-id1\", \"random-id1\", \"random-id2\",\n              \"random-id2\", \"random-id3\"],\n    \"lope_id_nr\": [\"L0001\", \"L0002\", \"L0001\", \"L0001\", \"0001\"],\n    \"aar\": ['2023', '2023', '2024', '2024', '2024Q1'],\n    \"navn\": ['Ola', 'Ola', 'Per', 'Kari', None],\n    \"produkt\": ['Banan', 'Eple', 'Eple', 'Agurk', 'Eple'],\n    \"salgsverdi\": [5000, 4000, 7000, 3000, 50],\n    \"vekt\": [700, 600, 700, 100, 5],\n    \"kostverdi\": [3500, 2500, 5000, 3100, 55],\n})\n\ndata = pd.concat([random_data, bad_data], ignore_index=True)\ndata\n\n\n\n\n\n\n\n\nid_nr\nlope_id_nr\naar\nnavn\nprodukt\nsalgsverdi\nvekt\nkostverdi\n\n\n\n\n0\n9c2a0f28-830f-4ae3-9400-8d992bcaa4b4\nL0001\n2023\nPer\nEple\n9921\n3698\n7440\n\n\n1\n97a1e3cf-c465-49a9-9d58-28810ff5acd6\nL0001\n2023\nOla\nGulrot\n8806\n994\n6604\n\n\n2\nce06b21d-c416-4a12-9180-71a36a891fb4\nL0001\n2024\nKari\nBrokkoli\n6387\n4145\n4790\n\n\n3\n43903841-4231-4471-b9a5-c8947ca4c985\nL0001\n2023\nKari\nEple\n2813\n4778\n2109\n\n\n4\n76cf98f9-9e6b-42a3-b1d7-d04816b7a1be\nL0001\n2023\nIda\nBrokkoli\n7053\n3606\n5289\n\n\n5\nf78f7396-2554-4f8a-a171-66682628b6db\nL0001\n2024\nKari\nEple\n3221\n2344\n2415\n\n\n6\nrandom-id1\nL0001\n2023\nOla\nBanan\n5000\n700\n3500\n\n\n7\nrandom-id1\nL0002\n2023\nOla\nEple\n4000\n600\n2500\n\n\n8\nrandom-id2\nL0001\n2024\nPer\nEple\n7000\n700\n5000\n\n\n9\nrandom-id2\nL0001\n2024\nKari\nAgurk\n3000\n100\n3100\n\n\n10\nrandom-id3\n0001\n2024Q1\nNone\nEple\n50\n5\n55\n\n\n\n\n\n\n\nDataframen best√•r av f√∏lgende kolonner:\n\nid_nr: identifiseringsnummer\nlope_nr_id: et slags l√∏penummerid\naar: perioden i √•r for gjeldende rad\nnavn: navn p√• enheten (person eller kunde)\nprodukt: produktet det gjelder - la oss si i en frukt og gr√∏nt butikk\nsalgsverdi: sluttverdien varen ble solgt for\nvekt: sluttvekten som ble solgt\nkostverdi: kostnaden tilknyttet innkj√∏p av produktet eller varen.\n\nDet er elementer her som ikke n√∏dvendigvis er fullt realistist med virkeligheten, men sammensetningen av disse kolonnene er mest bygd opp for √• demonstrere mulighetene og fleksibiliteten ved bruk av pandera."
  },
  {
    "objectID": "blog/posts/2024-12-16-data-validering-pandera/index.html#grunnleggende-bruk---schema",
    "href": "blog/posts/2024-12-16-data-validering-pandera/index.html#grunnleggende-bruk---schema",
    "title": "Intro til Pandera",
    "section": "Grunnleggende bruk - schema",
    "text": "Grunnleggende bruk - schema\nFor √• ta i bruk pandera m√• vi definere et schema. Schemaer definerer hvordan dataene forventes at skal se ut, spesielt n√•r det kommer til datatyper.\nMed pandera kan du validere b√•de datatyper og innhold. Det er flere m√•ter √• definere ett schema p√•, men jeg kommer til √• vise den anbefalte m√•ten √• gj√∏re det p√•. Den er ikke n√∏dvendigvis den enkleste, men den er enkel nok, og har likheter til Pydantic.\nEt schema i pandera defineres som f√∏lgende;\n\nclass SchemaValidation1(pan.DataFrameModel):\n    \n    id_nr: Series[str] = pan.Field(unique=True)\n    lope_id_nr: Series[str] = pan.Field(\n        str_startswith='L',\n        str_length={'min_value': 5,\n                    'max_value': 5}\n    )\n    aar: Series[str] = pan.Field(\n        str_length={'min_value': 4,\n                    'max_value': 4}\n    )\n    navn: Series[str] = pan.Field(\n        nullable=False # Default\n    )\n    produkt: Series[str] = pan.Field(\n        isin=['Eple', 'Banan', 'Gulrot', 'Brokkoli']\n    )\n    salgsverdi: Series[int] = pan.Field(ge=1000)\n    vekt: Series[int] = pan.Field(ge=500)\n    kostverdi: Series[int] = pan.Field(gt=700)\n\nS√• hva er det vi har definert her?\nVi har n√• definert et eget Objekt, en class, kalt SchemaValidation1, som arver egenskapene til Pandera sitt objekt DataFrameModel. Mer avansert fra objekt og class verden trenger du ikke √• gj√∏re eller kunne her egentlig, s√• ikke bli skremt med det f√∏rste. Deretter definerer vi kolonnene som vi forventer i dette schemaet. Pandera er bygget p√• typing systemet til Python vel og merke, som enkelt forklart vil si at jeg kan bruke typing-pakkens objekter i definisjonen som han vil bruke til √• validere for, men det gir ogs√• muligheten til √• benytte pythons standardobjekter som str og int i definisjonen. Vi har ogs√• definert regler tilknyttet hver av disse kolonnene som da vil bli validert sammen med datatypene.\n\nid_nr er en Serie (kolonner i pandas dataframe er av datatypen pandas serie) med forventet datatype string (str). Regler som er satt er at innholdet er unikt, alts√• ingen duplikater i de verdiene som ligger i kolonnen.\nlope_id_nr er ogs√• forventet datatype string. Den har 2 regler; at alle verdier starter med ‚ÄòL‚Äô, og at teksten er minimum og maksimum 5 karakterer lang.\naar er forventet √• v√¶re string, med regel om at den er 4 tegn lang.\nnavn er forventet √• v√¶re string, med regel om at det ikke skal v√¶re noen manglende verdier (missing values). Dette er standard for alle regler, s√• det er ikke n√∏dvendig √• notere, men for demonstrasjonens skyld s√• gjorde jeg det her.\nprodukt er forventet √• v√¶re string, med regler om at innholdet er blant verdiene i en gitt liste. I dette tilfellet Eple, Banan, Gulrot, Brokkoli. Kanskje er dette varene butikken selger og har i sortimentet sitt.\nsalgsverdi er forventet √• v√¶re en integer (int). Regel som er satt her er at verdiene er st√∏rre eller lik 1000.\nvekt er forventet √• v√¶re en integer. Regel som er satt her er at verdiene er st√∏rre eller lik 500.\nkostverdi er forventet √• v√¶re en integer. Regel som er satt her er at verdiene er st√∏rre enn 700.\n\nOkei, da har vi definert schemaet. Vi skal bygge videre p√• dette snart. Det finnes mange flere innebygde valideringsregler enn de vi benytter her, og man m√• inn i dokumentasjonen til Pandera for √• se om noe kan passe deg og ditt behov der, men her demonstrerer vi hvertfall noen som sikkert kommer til √• bli brukt.\nFor √• utf√∏re valideringen gj√∏r vi f√∏lgende;\n\ntry:\n    valresult = SchemaValidation1.validate(data, lazy=True)\nexcept SchemaErrors as error:\n    # Rapport av feil utslag i dataframe\n    valresult = error.failure_cases\n    # Dataframe som ble sendt inn\n    errdata = error.data\n    # Antall feil utslag\n    num_errors = error.error_counts\n    # Rapportmeilding av feil utslag i dict\n    error_message = error.message\n\nvalresult\n\n\n\n\n\n\n\n\nschema_context\ncolumn\ncheck\ncheck_number\nfailure_case\nindex\n\n\n\n\n0\nColumn\nid_nr\nfield_uniqueness\nNone\nrandom-id1\n6\n\n\n1\nColumn\nid_nr\nfield_uniqueness\nNone\nrandom-id1\n7\n\n\n2\nColumn\nid_nr\nfield_uniqueness\nNone\nrandom-id2\n8\n\n\n3\nColumn\nid_nr\nfield_uniqueness\nNone\nrandom-id2\n9\n\n\n4\nColumn\nlope_id_nr\nstr_length(5, 5)\n0\n0001\n10\n\n\n5\nColumn\nlope_id_nr\nstr_startswith('L')\n1\n0001\n10\n\n\n6\nColumn\naar\nstr_length(4, 4)\n0\n2024Q1\n10\n\n\n7\nColumn\nnavn\nnot_nullable\nNone\nNone\n10\n\n\n8\nColumn\nprodukt\nisin(['Eple', 'Banan', 'Gulrot', 'Brokkoli'])\n0\nAgurk\n9\n\n\n9\nColumn\nsalgsverdi\ngreater_than_or_equal_to(1000)\n0\n50\n10\n\n\n10\nColumn\nvekt\ngreater_than_or_equal_to(500)\n0\n100\n9\n\n\n11\nColumn\nvekt\ngreater_than_or_equal_to(500)\n0\n5\n10\n\n\n12\nColumn\nkostverdi\ngreater_than(700)\n0\n55\n10\n\n\n\n\n\n\n\nObjektet SchemaValidation1 har en metode validate som vi kan sende inn dataframen som skal valideres opp mot schemaet(som vi arvet fra pandera DataFrameModel objektet). Jeg har satt lazy til True her fordi jeg vil at han skal validere alt og ikke stoppe ved f√∏rste feil han finner. Dersom valideringen feiler har pandera et error objekt SchemaErrors hvor flere nyttige rapporter blir lagd tilgjengelig for oss. Du kan selv legge med flere av dem, men her tar vi for oss dataframen med alle feilmeldingene som dukker opp. Dersom valideringen gikk bra vil du f√• dataframen du sendte inn i retur.\nRapporten vi har f√•tt ut n√• i dataframen valresult har vi flere utslag p√•. Kolonnen id_nr finnes det duplikater blant annet. Kolonnen lope_id_nr er det funnes en som har sl√•tt ut i begge definerte reglene som nevnt tidligere. osv, osv. Denne rapporten har vi kanskje et potensial for √• utnytte videre? Men det f√•r v√¶re opp til den enkelte."
  },
  {
    "objectID": "blog/posts/2024-12-16-data-validering-pandera/index.html#behov-for-flere-kontroller",
    "href": "blog/posts/2024-12-16-data-validering-pandera/index.html#behov-for-flere-kontroller",
    "title": "Intro til Pandera",
    "section": "Behov for flere kontroller",
    "text": "Behov for flere kontroller\nDersom de innebygde mulighetene for validering ikke strekker til kan man definere reglene selv ved √• definere egne metoder med tilh√∏rende decorator (alfakr√∏ll over metoden). Under her definerer jeg SchemaValidation2 som sett bort ifra de nye metodene er nesten helt identisk med SchemaValidation1. Forskjellen er at n√• har kolonnen id_nr kun regelen om at den skal ikke ha manglende verdier i stedet for at det skal unike verdier.\n\nclass SchemaValidation2(pan.DataFrameModel):\n    \n    id_nr: Series[str] = pan.Field(nullable=False) # Default\n    lope_id_nr: Series[str] = pan.Field(\n        str_startswith='L',\n        str_length={'min_value': 5,\n                    'max_value': 5}\n    )\n    aar: Series[str] = pan.Field(\n        str_length={'min_value': 4,\n                    'max_value': 4}\n    )\n    navn: Series[str] = pan.Field(nullable=False) # Default\n    produkt: Series[str] = pan.Field(\n        isin=['Eple', 'Banan', 'Gulrot', 'Brokkoli']\n    )\n    salgsverdi: Series[int] = pan.Field(ge=1000)\n    vekt: Series[int] = pan.Field(ge=500)\n    kostverdi: Series[int] = pan.Field(gt=700)\n\n    # Sjekke at kolonne aar er tekst med tall i seg\n    @pan.check(\"aar\",\n               # Valgfritt, men gir eget navn til regelen enn metodenavnet\n               name=\"str_isdigits\",\n               # Valgfritt, men her kan man styre feilmeldingen\n               error=\"str_not_digits\")\n    def check_isdigits(cls, s: Series[str]) -&gt; Series[bool]:\n        return s.str.isdigit()\n\n    # En metode kan sjekke flere kolonner,\n    # her sjekker vi b√•de kostverdi og salgsverdi.\n    # Validerer at Bananer har b√•de h√∏yere\n    # salgsverdi og kostverdi enn Epler\n    @pan.check(\"kostverdi\", \"salgsverdi\",\n               groupby=\"produkt\",\n               name=\"check_epler_bananer\")\n    def check_groupby(cls, grouped_value: Dict[str, Series[int]]) -&gt; bool:\n        return grouped_value[\"Eple\"].sum() &lt; grouped_value[\"Banan\"].sum()\n\n    # Trenger du √• sjekke mer enn bare en kolonne av gangen?\n    # f.eks. at forholde mellom flere kolonner\n    # har en bestemt regel √• f√∏lge?\n    # Her sjekkes at kombinasjonen for kolonnene\n    # id_nr og lope_id_nr er unike\n    @pan.dataframe_check\n    def unique_combo_idnr_lopeidnr(cls, df: pd.DataFrame) -&gt; Series[bool]:\n        df2 = df.copy()\n        df3 = (\n            df2\n            .groupby(['id_nr', 'lope_id_nr'])\n            .agg({'aar': 'count'})\n            .rename(columns={'aar': 'duplikater'}) == 1\n        ).reset_index()\n        df2 = df2.merge(df3,\n                        on=['id_nr', 'lope_id_nr'],\n                        how='left')\n        return df2['duplikater']\n\nSchemaet SchemaValidation2 har som vi ser n√• 3 metoder;\n\ncheck_isdigits som sjekker at teksten faktisk kun inneholder tall. Her sjekkes kun kolonnen aar.\ncheck_groupby som grupperer verdiene i kolonnen produkt. Det sjekkes her for kolonnene kostverdi og salgsverdi. Den sjekker at summen av bananer er h√∏yere enn summen av epler (for √• gj√∏re noe enkelt og irrelevant).\nde 2 f√∏rste sjekkene kan kun jobbe med en kolonne av gangen, ev. en groupby p√• en annen kolonne med fokus p√• de gjeldende kolonnene en har tenkt √• sjekke for. Den tredje siste sjekken er litt annerledes, for de andre sjekkene har benyttet decoratoren check, mens den siste har dataframe_check. Dette vil si at hele dataframen sendes inn, og her vil du ha full fleksibilitet til √• sjekke det du m√•tte √∏nske p√• tvers av alle kolonner. Viktigste er at det returneres en serie(kolonne) av boolske verdier (True/False). I denne siste sjekken unique_combo_idnr_lopeidnr sjekkes det at kombinasjonen av kolonnene id_nr og lope_id_nr er unike i dataframen.\n\nIgjen kan dataene valideres;\n\ntry:\n    valresult = SchemaValidation2.validate(data, lazy=True)\nexcept SchemaErrors as error:\n    valresult = error.failure_cases\n\nvalresult\n\n\n\n\n\n\n\n\nschema_context\ncolumn\ncheck\ncheck_number\nfailure_case\nindex\n\n\n\n\n14\nDataFrameSchema\nlope_id_nr\nunique_combo_idnr_lopeidnr\n0\nL0001\n8\n\n\n15\nDataFrameSchema\nlope_id_nr\nunique_combo_idnr_lopeidnr\n0\nL0001\n9\n\n\n26\nDataFrameSchema\nkostverdi\nunique_combo_idnr_lopeidnr\n0\n5000\n8\n\n\n25\nDataFrameSchema\nvekt\nunique_combo_idnr_lopeidnr\n0\n100\n9\n\n\n24\nDataFrameSchema\nvekt\nunique_combo_idnr_lopeidnr\n0\n700\n8\n\n\n23\nDataFrameSchema\nsalgsverdi\nunique_combo_idnr_lopeidnr\n0\n3000\n9\n\n\n22\nDataFrameSchema\nsalgsverdi\nunique_combo_idnr_lopeidnr\n0\n7000\n8\n\n\n21\nDataFrameSchema\nprodukt\nunique_combo_idnr_lopeidnr\n0\nAgurk\n9\n\n\n20\nDataFrameSchema\nprodukt\nunique_combo_idnr_lopeidnr\n0\nEple\n8\n\n\n19\nDataFrameSchema\nnavn\nunique_combo_idnr_lopeidnr\n0\nKari\n9\n\n\n18\nDataFrameSchema\nnavn\nunique_combo_idnr_lopeidnr\n0\nPer\n8\n\n\n17\nDataFrameSchema\naar\nunique_combo_idnr_lopeidnr\n0\n2024\n9\n\n\n16\nDataFrameSchema\naar\nunique_combo_idnr_lopeidnr\n0\n2024\n8\n\n\n27\nDataFrameSchema\nkostverdi\nunique_combo_idnr_lopeidnr\n0\n3100\n9\n\n\n13\nDataFrameSchema\nid_nr\nunique_combo_idnr_lopeidnr\n0\nrandom-id2\n9\n\n\n12\nDataFrameSchema\nid_nr\nunique_combo_idnr_lopeidnr\n0\nrandom-id2\n8\n\n\n1\nColumn\nlope_id_nr\nstr_startswith('L')\n1\n0001\n10\n\n\n11\nColumn\nkostverdi\ncheck_epler_bananer\n1\nFalse\nNone\n\n\n10\nColumn\nkostverdi\ngreater_than(700)\n0\n55\n10\n\n\n9\nColumn\nvekt\ngreater_than_or_equal_to(500)\n0\n5\n10\n\n\n8\nColumn\nvekt\ngreater_than_or_equal_to(500)\n0\n100\n9\n\n\n7\nColumn\nsalgsverdi\ncheck_epler_bananer\n1\nFalse\nNone\n\n\n6\nColumn\nsalgsverdi\ngreater_than_or_equal_to(1000)\n0\n50\n10\n\n\n5\nColumn\nprodukt\nisin(['Eple', 'Banan', 'Gulrot', 'Brokkoli'])\n0\nAgurk\n9\n\n\n4\nColumn\nnavn\nnot_nullable\nNone\nNone\n10\n\n\n3\nColumn\naar\nstr_not_digits\n1\n2024Q1\n10\n\n\n2\nColumn\naar\nstr_length(4, 4)\n0\n2024Q1\n10\n\n\n0\nColumn\nlope_id_nr\nstr_length(5, 5)\n0\n0001\n10\n\n\n\n\n\n\n\nDesverre vil sjekker som gjelder hele dataframen registrere flere feil ettersom han sjekker alle kolonner for gjeldende rader. Derimot er fleksibiliteten ganske stor!"
  },
  {
    "objectID": "blog/posts/2024-12-16-data-validering-pandera/index.html#bruk-av-validering-i-funksjonene",
    "href": "blog/posts/2024-12-16-data-validering-pandera/index.html#bruk-av-validering-i-funksjonene",
    "title": "Intro til Pandera",
    "section": "Bruk av validering i funksjonene",
    "text": "Bruk av validering i funksjonene\nOver til et eksempel hvor pandera viser seg som veldig nyttig! La oss si at vi har klargjorte data klart, ihht. datatilstandene, og vi er da klare for √• lage statistikkdata. Det er ikke gitt at l√∏pet er helt rett fram mellom disse datatilstandene, men i dette eksempelet er jobben bare √• f√• aggregert klargjorte data.\nNedenfor her lager jeg klargjorte data av de dataene som vi har jobbet med, og som er korrekte. Lager et tilh√∏rende skjema, som bare arver fra det f√∏rste schemaet vi lagde. Valideringen her vil selvsagt g√• smertefritt igjennom.\n\nklargjort_df = data.head(6)\n\n\nclass KlargjortSchema(SchemaValidation1):\n    pass\n\n\ntry:\n    klargjort_df = KlargjortSchema.validate(klargjort_df, lazy=True)\nexcept SchemaErrors as error:\n    valresult = error.failure_cases\n    raise error\n\nklargjort_df\n\n\n\n\n\n\n\n\nid_nr\nlope_id_nr\naar\nnavn\nprodukt\nsalgsverdi\nvekt\nkostverdi\n\n\n\n\n0\n9c2a0f28-830f-4ae3-9400-8d992bcaa4b4\nL0001\n2023\nPer\nEple\n9921\n3698\n7440\n\n\n1\n97a1e3cf-c465-49a9-9d58-28810ff5acd6\nL0001\n2023\nOla\nGulrot\n8806\n994\n6604\n\n\n2\nce06b21d-c416-4a12-9180-71a36a891fb4\nL0001\n2024\nKari\nBrokkoli\n6387\n4145\n4790\n\n\n3\n43903841-4231-4471-b9a5-c8947ca4c985\nL0001\n2023\nKari\nEple\n2813\n4778\n2109\n\n\n4\n76cf98f9-9e6b-42a3-b1d7-d04816b7a1be\nL0001\n2023\nIda\nBrokkoli\n7053\n3606\n5289\n\n\n5\nf78f7396-2554-4f8a-a171-66682628b6db\nL0001\n2024\nKari\nEple\n3221\n2344\n2415\n\n\n\n\n\n\n\nDeretter definerer vi et eget schema for statistikkdata, med noen tilh√∏rende regler og datatyper;\n\nclass StatistikkSchema(pan.DataFrameModel):\n    \n    aar: Series[pd.CategoricalDtype] = pan.Field(\n        coerce=True, # Vil konvertere datatypene for meg\n        str_length={'min_value': 4,\n                    'max_value': 4})\n    produkt: Series[pd.CategoricalDtype] = pan.Field(\n        coerce=True, # Vil konvertere datatypene for meg\n        isin=['Eple', 'Banan', 'Gulrot', 'Brokkoli'])\n    salgsverdi: Series[int] = pan.Field(ge=0)\n\nS√• over til magien; Pandera schemaene kan innlemmes i hvilken som helst funksjon som har dataframes som input eller output, og det uten at du selv skriver at valideringen skal skje i funksjonen, det skjer automagisk! Og det gj√∏res som f√∏lgende;\n\n# Lazy for at valideringen skal utf√∏res igjennom hele dataframene\n@pan.check_types(lazy=True)\ndef agg_statistikk(\n    df: DataFrame[KlargjortSchema]\n) -&gt; DataFrame[StatistikkSchema]:\n    dff = (\n        df\n        .copy()\n        .groupby(['aar', 'produkt'], as_index=False)\n        .agg({'salgsverdi': 'sum'})\n    )\n    return dff\n\nS√• n√• ved √• bruke funksjonen, vil du ikke f√• lagd statistikkdata uten at b√•de klargjorte data blir validert og godkjent, og at statistikkdata som er p√• vei ut av funksjonen er validert og godkjent. I v√•rt tilfelle skal det g√• fint n√•;\n\nstatistikk_df = agg_statistikk(klargjort_df)\nstatistikk_df\n\n\n\n\n\n\n\n\naar\nprodukt\nsalgsverdi\n\n\n\n\n0\n2023\nBrokkoli\n7053\n\n\n1\n2023\nEple\n12734\n\n\n2\n2023\nGulrot\n8806\n\n\n3\n2024\nBrokkoli\n6387\n\n\n4\n2024\nEple\n3221\n\n\n\n\n\n\n\nMan skal ogs√• kunne validere flere schemaer samtidig ogs√• hvis en √∏nsker det. Alts√• at input til funksjonen sjekkes opp mot flere schema samtidig, eller at output blir det. Det er ikke blitt demonstrert her.\nMed det samme kan vi sjekke datatypene, vi hadde satt at Pandera skulle endre datatypene for oss. B√•de f√∏r og etter;\n\n\nCode\nfrom IPython.display import HTML, display\n\nkdf = pd.DataFrame(klargjort_df.dtypes, columns=['Datatyper'])\nsdf = pd.DataFrame(statistikk_df.dtypes, columns=['Datatyper'])\n\n# Style dataframes\nstyled_df1 = kdf.style.set_caption(\"Klargjorte-data\")\nstyled_df2 = sdf.style.set_caption(\"Statistikk data\")\n\ndisplay(HTML(\nf\"\"\"\n&lt;div style=\"display: flex; justify-content: space-around;\"&gt;\n&lt;div&gt;{styled_df1.to_html()}&lt;/div&gt;\n&lt;div&gt;{styled_df2.to_html()}&lt;/div&gt;\n&lt;/div&gt;\n\"\"\"\n))\n\n\n\n\n\n\n\n\n\nTabell¬†1: Klargjorte-data\n\n\n\n\n\n¬†\nDatatyper\n\n\n\n\nid_nr\nobject\n\n\nlope_id_nr\nobject\n\n\naar\nobject\n\n\nnavn\nobject\n\n\nprodukt\nobject\n\n\nsalgsverdi\nint64\n\n\nvekt\nint64\n\n\nkostverdi\nint64\n\n\n\n\n\n\n\n\n\n\n\n\n\nTabell¬†2: Statistikk data\n\n\n\n\n\n¬†\nDatatyper\n\n\n\n\naar\ncategory\n\n\nprodukt\ncategory\n\n\nsalgsverdi\nint64"
  },
  {
    "objectID": "blog/posts/2024-12-16-data-validering-pandera/index.html#bonus-auto-transformasjon-av-kolonneverdier",
    "href": "blog/posts/2024-12-16-data-validering-pandera/index.html#bonus-auto-transformasjon-av-kolonneverdier",
    "title": "Intro til Pandera",
    "section": "BONUS: Auto-transformasjon av kolonneverdier",
    "text": "BONUS: Auto-transformasjon av kolonneverdier\nPandera har noe som kalles parsers, som gir oss muligheten til √• utf√∏re preprosesseringer p√• dataene f√∏r validering. Dette kan v√¶re flere typer transformasjoner som man b√∏r s√∏rge for er gjort f√∏r valideringen utf√∏res, ev. om transformasjonen bare skal gjennomf√∏res.\nLa oss ta et eksempel med en liten del av dataene vi har jobbet med til n√•, da med data vi vet det ikke skal bli noe problemer med;\n\ndata['dekningsbidrag'] = data['salgsverdi'] - data['kostverdi']\n\ndf = data.head(3).copy()\ndf\n\n\n\n\n\n\n\n\nid_nr\nlope_id_nr\naar\nnavn\nprodukt\nsalgsverdi\nvekt\nkostverdi\ndekningsbidrag\n\n\n\n\n0\n9c2a0f28-830f-4ae3-9400-8d992bcaa4b4\nL0001\n2023\nPer\nEple\n9921\n3698\n7440\n2481\n\n\n1\n97a1e3cf-c465-49a9-9d58-28810ff5acd6\nL0001\n2023\nOla\nGulrot\n8806\n994\n6604\n2202\n\n\n2\nce06b21d-c416-4a12-9180-71a36a891fb4\nL0001\n2024\nKari\nBrokkoli\n6387\n4145\n4790\n1597\n\n\n\n\n\n\n\nJeg beregner her dekningsbidraget for hver observasjon, som da er differansen mellom salgsverdi og kostverdi. Det er mer eller mindre en funksjon som avhenger av disse to variablene, og m√• holdes oppdatert.\nOg la oss n√• si at kostverdien p√• f√∏rste observasjonen ikke skulle v√¶re p√• 75 % av salgsverdi slik vi startet med, men av en eller annen grunn heller skulle v√¶re p√• 85 %. Vi kan editere det inn;\n\ndf.loc[0, ['kostverdi']] = int(round(\n    df.iloc[0]['salgsverdi'] * 0.85, 0)\n                              )\n\ndf\n\n\n\n\n\n\n\n\nid_nr\nlope_id_nr\naar\nnavn\nprodukt\nsalgsverdi\nvekt\nkostverdi\ndekningsbidrag\n\n\n\n\n0\n9c2a0f28-830f-4ae3-9400-8d992bcaa4b4\nL0001\n2023\nPer\nEple\n9921\n3698\n8433\n2481\n\n\n1\n97a1e3cf-c465-49a9-9d58-28810ff5acd6\nL0001\n2023\nOla\nGulrot\n8806\n994\n6604\n2202\n\n\n2\nce06b21d-c416-4a12-9180-71a36a891fb4\nL0001\n2024\nKari\nBrokkoli\n6387\n4145\n4790\n1597\n\n\n\n\n\n\n\nS√• n√• har vi f√•tt korrigert kostverdien p√• f√∏rste observasjon, men dekningsbidraget er fortsatt den samme. Dette kan l√∏ses som en egen funksjon, men hvorfor ikke innlemme det i data valideringen v√•r, da pandera st√∏tter slik transformering. Vi lager f√∏rst et tilh√∏rende schema;\n\nclass ParserSchema(SchemaValidation1):\n    dekningsbidrag: Series[int]\n\n    @pan.check(\"navn\")\n    def is_uppercase(cls, s: Series[str]) -&gt; Series[bool]:\n        return s.str.isupper()\n\n    # konverterer all tekst i kolonnen til √• ha kun store bokstaver\n    @pan.parser(\"navn\")\n    def uppercase(cls, s: Series[str]) -&gt; Series[str]:\n        return s.str.upper()\n\n    # S√∏rger for at dekningsbidrag blir rekalkulert\n    @pan.dataframe_parser\n    def kalkuler_dekningsbidrag(cls, df: pd.DataFrame) -&gt; pd.DataFrame:\n        df['dekningsbidrag'] = df['salgsverdi'] - df['kostverdi']\n        return df\n\nS√• her tar jeg i bruk det aller f√∏rste schema som vi definerte, men legger p√• dekningsbidrag som ikke har noen andre valideringer enn datatype. Med dataene vi har n√• skal det ikke dukke opp noen feil med dette. Jeg legger ved en valideringsregel for navn i dette tilfelle, hvor n√• alt i kolonnen navn skal v√¶re store bokstaver. Vi vet allerede at det ikke er noen store bokstaver der, s√• vi legger inn en metode som har decorator parser som vil transformere dette. I tillegg legger vi til en egen metode med decorator dataframe_parser for √• rekalkulere dekningsbidraget.\nS√• s√•nn sett skulle man kanskje tro at valideringen av kolonnen navn vil kunne sl√• ut i valideringen, men som nevnt s√• kj√∏res transformasjonene f√∏rst f√∏r valideringen. I tillegg, n√•r valideringen g√•r igjennom, vil du f√• dataframen du sendte inn i retur ved utf√∏relsen av valideringen;\n\ntry:\n    valresult = ParserSchema.validate(df, lazy=True)\nexcept SchemaErrors as error:\n    valresult = error.failure_cases\n\nvalresult\n\n\n\n\n\n\n\n\nid_nr\nlope_id_nr\naar\nnavn\nprodukt\nsalgsverdi\nvekt\nkostverdi\ndekningsbidrag\n\n\n\n\n0\n9c2a0f28-830f-4ae3-9400-8d992bcaa4b4\nL0001\n2023\nPER\nEple\n9921\n3698\n8433\n1488\n\n\n1\n97a1e3cf-c465-49a9-9d58-28810ff5acd6\nL0001\n2023\nOLA\nGulrot\n8806\n994\n6604\n2202\n\n\n2\nce06b21d-c416-4a12-9180-71a36a891fb4\nL0001\n2024\nKARI\nBrokkoli\n6387\n4145\n4790\n1597\n\n\n\n\n\n\n\nSom vi n√• ser har valideringen g√•tt fint for seg. Vi ser at alle verdier i kolonnen navn har blitt tekst med kun store bokstaver, og vi ser at dekningsbidraget har blitt rekalkulert s√• det n√• er korrekt!"
  },
  {
    "objectID": "blog/posts/2024-12-16-data-validering-pandera/index.html#oppsummering",
    "href": "blog/posts/2024-12-16-data-validering-pandera/index.html#oppsummering",
    "title": "Intro til Pandera",
    "section": "Oppsummering",
    "text": "Oppsummering\nFor at vi skal kunne produsere og levere statistikk av h√∏y kvalitet er det viktig at vi validerer data l√∏pende i produksjonsl√∏pene v√•re. Store deler av dataene v√•re er strukturerte, ev. tidy om du vil, og da er python pakken Pandera en sterk kandidat √• benytte inn i kodene v√•re. Hvertfall hvis du programmerer i Python. For R s√• er pakken Validate aktuell. Her har vi introdusert generell bruk av Pandera for validering av data; hvordan definere schema og valideringsregler, hvordan validere en dataframe med det, og hvordan det kan tas i bruk i blant annet funksjoner. Trenger du hjelp til √• implementere data validering med Pandera inn i koden din, s√• er St√∏tteteamene mulig √• sp√∏rre, ellers kommer man ikke unna dokumentasjonen til Pandera selv."
  },
  {
    "objectID": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html",
    "href": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html",
    "title": "Ideen bak bloggen",
    "section": "",
    "text": "SSB-ere l√∏ser hele tiden problemer p√• nye m√•ter som andre gjerne skulle nyttiggjort seg av. Spesielt n√•r vi gj√∏r en s√• stor overgang i arbeidsform som overgangen til en ny plattform (Dapla), og vi samtidig skifter mange verkt√∏yene vi har i verkt√∏ykassen v√•r. Av den grunn har vi opprettet denne bloggen. Her vil vi skrive om hvordan vi l√∏ser problemer, hvilke verkt√∏y vi bruker og hvordan vi bruker dem. Vi vil ogs√• skrive om hvordan vi jobber med √• utvikle nye verkt√∏y og hvordan vi jobber med √• utvikle Dapla.\nM√•lsetningen med denne bloggen er at alle i SSB som √∏nsker √• dele noe med andre kan skrive en artikkel og dele i bloggen. Mens Byr√•nettet er kanal for √• dele informasjon med alle i SSB, og Viva Engage en kanal for √• si det du tenker uten s√¶rlig noen formell struktur, er denne bloggen en kanal for √• dele informasjon med andre som jobber med data og teknologi i SSB.\nFordelen med bloggen er at den er tilpasset hvordan statistikkere, forskere og IT-utviklere jobber til daglig. Artiklene kan skrives samme sted som man utvikler kode, og man inkludere output fra kodekj√∏ringer i artikler."
  },
  {
    "objectID": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html#hvordan-skrive-en-artikkel",
    "href": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html#hvordan-skrive-en-artikkel",
    "title": "Ideen bak bloggen",
    "section": "Hvordan skrive en artikkel?",
    "text": "Hvordan skrive en artikkel?\nBloggen er generert med Quarto. Quarto er et rammeverk for √• skrive artikler i markdown. Det er enkelt √• komme i gang med Quarto, og det er enkelt √• skrive artikler i Quarto.\nFor √• skrive en artikkel gj√∏r du f√∏lgende:\n\nSkriv artikkelen som en markdown-fil (.qmd-fil) eller en notebook (.ipynb-fil).\nKlon dapla-manual-internal repoet:\ngit clone https://github.com/statisticsnorway/dapla-manual-internal.git\nOpprett en mappe for artikkelen din i mappen ./dapla-manual-internal/blog/posts/. Gi mappen et navn som beskriver artikkelen din.\nInne mappen legger du din .qmd- eller .ipynb-fil. Eventuelle bilder i artikkelen legges ogs√• i samme mappe.\nOpprette en pull request p√• repoet og noen vil se over artikkelen din og publisere den."
  },
  {
    "objectID": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html#metadata-om-artikkelen",
    "href": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html#metadata-om-artikkelen",
    "title": "Ideen bak bloggen",
    "section": "Metadata om artikkelen",
    "text": "Metadata om artikkelen\nN√•r du skriver artikkelen s√• m√• du starte dokumentet med f√∏lgende metadata:\n\n\nindex.qmd\n\n---\ntitle: Ideen bak bloggen\nsubtitle: Hvorfor vi har opprettet denne bloggen? \ncategories:\n  - Quarto\nauthor:\n  - name: √òyvind Bruer-Skarsb√∏\n    affiliation: \n      - name: Seksjon for dataplattform (724)\n        email: obr@ssb.no\ndate: \"01/11/2024\"\ndate-modified: \"01/11/2024\"\nimage: ../../../images/dapla-long.png\nimage-alt: \"Bilde av Fame-logoen\"\ndraft: false\n---\n\nHusk √• fylle ut alle feltene slik at det blir riktig informasjon for din artikkel. Skriver du en ipynb-fil s√• m√• metadataene ligge i en celle av typen raw.\n√ònsker du √• komme fort igang s√• kan se hvordan denne artikkelen ble skrevet."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html",
    "title": "Fra arkiv til parquet",
    "section": "",
    "text": "I arkivet til SSB ligger data lagret som posisjonerte flatfiler, ogs√• kalt fastbredde-fil eller fixed width file p√• engelsk. I Datadok ligger det spesifisert hvordan du leser inn disse filene fra dat eller txt i arkivet til sas7bdat-formatet, men ikke hvordan man konverterer til Parquet-formatet. I denne artikkelen deler jeg hvordan jeg gikk frem for √• konvertere arkivfiler til Parquet."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#hva-er-en-fastbredde-fil",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#hva-er-en-fastbredde-fil",
    "title": "Fra arkiv til parquet",
    "section": "Hva er en fastbredde-fil?",
    "text": "Hva er en fastbredde-fil?\nEn fastbredde-fil er en fil der hver rad har en fast lengde, og hver kolonne har en fast posisjon. Det er ingen komma eller andre tegn som skiller kolonnene, slik som i en CSV-fil. En fastbredde-fil er en ren tekstfil, dvs. at du kan √•pne den opp i teksteditor og kikke p√• innholdet direkte.\nUnder er et eksempel hvor samme data er lagret b√•de p√• CSV-formatet og som fastbredde-fil:\n\n\n\n\ncsv\n\n012345;;Ola Nordmann;\n345678;Kvinne;Kari Nordmann;\n\n\n\n\n\n\nfastbredde-fil\n\n012345      Ola Nordmann \n345678KvinneKari Nordmann\n\n\n\nI csv-filen over til venstre ser vi at hver kolonne er separert med et semikolon, og at hver rad er separert med et linjeskift. I fastbredde-filen til h√∏yre ser vi at hver kolonne har en fast lengde, den tomme kj√∏nnsvariabelen p√• rad 1 fylles med spaces, hver rad har dermed den samme lengden i antall tegn. I tillegg er det et ekstra mellomrom etter Ola Nordmann ift. Kari Nordmann. Dette er fordi Ola Nordmann er 12 tegn lang, mens Kari Nordmann er 13 tegn lang."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lese-med-pandas",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lese-med-pandas",
    "title": "Fra arkiv til parquet",
    "section": "Lese med Pandas",
    "text": "Lese med Pandas\nVi kan bruke pandas-funksjonen read_fwf() for √• lese inn en fastbredde-fil. Denne funksjonen tar inn en filsti, og en liste med bredder for hver kolonne. I tillegg kan vi spesifisere navn p√• kolonnene, og hvilken datatype kolonnene skal ha og hvordan missing-verdier skal representeres.\nVi er helt avhengig av √• vite bredden p√• hver kolonne for √• kunne lese inn en fastbredde-fil. Dette kan vi finne ut ved √• √•pne filen i en teksteditor og telle/gjette antall tegn i hver kolonne. Alternativt kan vi bruke innlesingsskriptet for SAS som finnes i Datadok, siden breddene er spesifisert der. Under er et ekspempel p√• hvordan vi kan lese inn en fastbredde-fil fra forrige avsnitt1:\n\nimport pandas as pd\nfrom io import StringIO  # N√∏dvendig siden vi sender en streng, ikke en filsti til .read_fwf\ninstring = \"112345      Ola Nordmann \\n345678KvinneKari Nordmann\\n\"\ndf = pd.read_fwf(StringIO(instring),\n                 names=['pers_id', 'kjonn', 'navn'],  # Navngi kolonner\n                 dtype='object',  # Alle kolonnene settes til \"object\"\n                 na_values=['.', ' .'],  # Hvilke karakterer bruker SAS for tom verdi?\n                 widths=[6, 6, 13])  # Tell/regn ut dissa sj√∏l\ndf\n\n\n\n\n\n\n\n\npers_id\nkjonn\nnavn\n\n\n\n\n0\n112345\nNaN\nOla Nordmann\n\n\n1\n345678\nKvinne\nKari Nordmann\n\n\n\n\n\n\n\nKoden over returnerer en Pandas Dataframe i minnet. Den kan vi lett lagre til Parquet-formatet. Men innlesingen m√•tte vi spesifisere en masse detaljer manuelt. Hvis vi skal lese inn mange filer med ulik struktur, s√• er ikke denne fremgangsm√•ten skalerbar. Dette er en fremgangsm√•te for √• lese inn noen f√• filer."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#datadok",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#datadok",
    "title": "Fra arkiv til parquet",
    "section": "Datadok",
    "text": "Datadok\nSom nevnt over s√• finnes det et innlesingsskript for SAS i Datadok. Dette skriptet kan vi bruke til √• lese inn en fastbredde-fil i Python. Vi kan ogs√• bruke det til √• finne breddene p√• hver kolonne. Et slik skript har denne formen:\n\n\ninnlesingsskript.sas\n\nDATA sas_data;\n   INFILE '/ssb/bruker/felles/flatfileksempel_dapla_manual_blogg.dat' MISSOVER PAD LRECL=36;\n   INPUT\n      @1 pers_id 6.0\n      @7 kjonn $CHAR6.0\n      @13 navn $CHAR13.0\n   ;\nRUN;\n\nVi kunne lest av informasjonen her og omsatt innholdet til argumentene read_fwf() trenger. Men fortsatt inneb√¶rer dette potensielt en del manuelt arbeid."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lese-med-saspy",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lese-med-saspy",
    "title": "Fra arkiv til parquet",
    "section": "Lese med saspy",
    "text": "Lese med saspy\nEn annen tiln√¶rming enn √• bruke .read_fwf fra Pandas er √• bruke biblioteket saspy. Dette biblioteket lar oss kj√∏re SAS-kode fra Python, p√• SAS-serverene i prodsonen, og f√• Dataframes tilbake. Vi kan bruke det til √• kj√∏re sas-skript hentet fra Datadok, konvertere til en pandas dataframe, og til slutt skrive til Parquet. I det f√∏lgende antar vi at du jobber i Jupyterlab i prodsonen (sl-jupyter-p), og at du har lagret innlesingsskriptet i en variabel, slik som vist under:\n\n\npython\n\nscript = \"\"\"\nDATA sas_data;\n   INFILE '/ssb/bruker/felles/flatfileksempel_dapla_manual_blogg.dat' MISSOVER PAD LRECL=36;\n   INPUT\n      @1 pers_id 6.0\n      @7 kjonn $CHAR6.0\n      @13 navn $CHAR13.0\n   ;\nRUN;\n\"\"\"\n\nLa oss deretter kj√∏re f√∏lgende kode fra Jupyterlab:\n\n\npython\n\nfrom fagfunksjoner import saspy_session\n\n# Kobler til sas-serverne\nsas = saspy_session()\n\n# Vi bruker tilkoblingen til √• sende inn Datadok-skriptet\nresult = sas.submit(script)\n\n# Lagre sas-loggen i en variabel\nlog = result[\"LOG\"]\n\n# Ber om √• f√• dataframe tilbake\ndf_frasas = sas.sd2df(\"sas_data\", \"work\")\n\n# Lukker koblingen til sas-serverne\nsas._endsas()\n\n# Printer ut datasettet\ndf_frasas\n\nI koden over har vi brukt en pakke som heter ssb-fagfunksjoner for √• opprette koblingen til sas-serveren. Pakken inneholder et overbygg over saspy, og koden over forutsetter at du har lagret passordet ditt p√• en spesiell m√•te2.\n\nDatatyper\nVi har n√• en pandas dataframe med datatyper p√•f√∏rt, men disse er basert p√• den lave mengden datatyper i SAS. Ofte b√∏r det ryddes i datatyper f√∏r man skriver til Parquet. Spesielt b√∏r du tenke p√• f√∏lgende:\n\nCharacter mappes gjerne til object i pandas, ikke den strengere varianten string eller den mer spesifikke string[pyarrow].\nNumeric mappes stort sett til float64 i pandas, vi f√•r som regel ikke heltall direkte Int64 uten videre behandling.\n\nDu kan la Pandas gj√∏re ett nytt fors√∏k p√• √• gjette datatyper ved √• kj√∏re f√∏lgende kode:\n\n\npython\n\ndf_pd_dtypes = df_frasas.convert_dtypes()\ndf_pd_dtypes.dtypes\n\nOm du vil teste min selvskrevne funksjon for √• gjette p√• datatyper s√• ligger den i fellesfunksjons-pakken:\n\n\npython\n\nfrom fagfunksjoner import auto_dtype\ndf_auto = auto_dtype(df_frasas)\ndf_auto.dtypes\n\nSjekk gjerne ut parameteret cardinality_threshold p√• auto_dtype, om du er interessert i √• automatisk sette categorical dtypes.\n\n\nSkalering\nHvis du har mange arkivfiler, med mange forskjellige innlesingsskript, s√• kan du lagre alle skriptene i en mappe, og s√• hente innholdet programmatisk. Her er koden for √©n slik ‚Äúhenting‚Äù.\n\n\npython\n\nsas_script_path = \"/ssb/bruker/felles/flatfileksempel_dapla_manual_blogg.sas\"\nwith open(sas_script_path, \"r\", encoding=\"latin1\") as sas_script:\n    script = sas_script.read().strip()\n    script = \"DATA \" + script.split(\"DATA \")[1] # Forkort ned scriptet til det vi trenger\nprint(script)\n\nHer henter jeg inn et innlesingsskript fra Datadok som jeg har lagret som en tekstfil i en mappe p√• linux-serveren i prodsonen. Deretter gj√∏r jeg den om til et streng-objekt i minnet som kan sendes til saspy-koden som er vist over. Dermed er det bare √• finne en logikk som gj√∏r at du vet hvilket innlesingskript som skal brukes til hvilke arkivfiler (siste valide datadok-script f√∏r datafil oppstod feks), og du kan jobbe veldig effektivt med konvertering. N√•r alt er konvertert kan du f.eks. kj√∏re et script som validerer datatypene p√• tvers av alle √•rganger og filnavn."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lagre-dataframen-til-parquet",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lagre-dataframen-til-parquet",
    "title": "Fra arkiv til parquet",
    "section": "Lagre dataframen til parquet",
    "text": "Lagre dataframen til parquet\nN√• er det veldig lett √• skrive filen til Parquet-formatet.\n\n\npython\n\ndf_auto.to_parquet(\n    \"/ssb/bruker/felles/flatfileksempel_dapla_manual_blogg.parquet\"\n    )"
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#nudb",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#nudb",
    "title": "Fra arkiv til parquet",
    "section": "NUDB",
    "text": "NUDB\nI omleggingen av NUDB (Nasjonal utdanningsdatabase), m√•tte vi konvertere hele arkivet v√•rt p√• 750+ dat-filer.\nDet var √∏nskelig √• slippe √• lagre til sas7bdat i mellom, for √• slippe mye dataduplikasjon og arbeidsprosesser. M√•let v√•rt var pseudonymiserte parquetfiler i sky.\nI stor grad kunne dette arbeidet automatiseres (bortsett fra √• lagre ut innlastingsscript fra gamle datadok). Funksjonene jeg utviklet for dette, ligger stort sett i denne filen:\ngithub.com/utd-nudb/prodsone/konverter_arkiv/archive.py"
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#footnotes",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#footnotes",
    "title": "Fra arkiv til parquet",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\n/n i strengen 112345      Ola Nordmann \\n345678KvinneKari Nordmann\\n betyr linjeskift.‚Ü©Ô∏é\nHvis du √∏nsker kan du bruker ssb-fagfunksjoner til √• lagre passordet ditt i kryptert form. Da kan du lagre passordet i en fil p√• din egen maskin, og slipper √• skrive det inn hver gang du skal koble til SAS. Funksjonen heter fagfunksjoner.prodsone.saspy_ssb.set_password().‚Ü©Ô∏é"
  }
]