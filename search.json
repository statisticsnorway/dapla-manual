[
  {
    "objectID": "utviklere/dokumentere-for-backstage.html",
    "href": "utviklere/dokumentere-for-backstage.html",
    "title": "Hvordan dokumentere systemer for backstage?",
    "section": "",
    "text": "Backstage er en utviklerportal utviklet hos spotify. Vi drifter vår egen instans på NAIS og du kan nå den fra VPN, eller med naisdevice, på backstage.intern.ssb.no. Tech lead gruppen vedlikeholder techradaren vår der, og det er bestemt at alle utviklingsteam hos SSB skal dokumentere sine systemer i backstage sin software katalog.\nÅ dokumentere et system i backstage er ganske enkelt. Alt du trenger er kunnskap om formatet, så kommer du fort igang. Dokumentasjonen skal ligge så nærme koden som mulig så den er enkel å vedlikeholde for utviklerne.\nI dette dokumentet går vi gjennom hvordan backstage fungerer ved å først bli kjent med terminologien. Deretter bruker vi et fiktivt microdata-system som eksempel for hvordan vi dokumenterer de forskjellige entitetene. I bunnen av dokumentet finner du også SSB-spesifikke retningslinjer for backstage dokumenteringen.\nOm du har noen spørsmål som ikke blir besvart i løpet av dette dokumentet, kontakt gjerne techlead gruppen på slack: #tech-lead-forum.\n\n\nFor å dokumentere våre systemer i backstage må vi være kjent med entitietene i backstage sin system modell. Her er en kort beskrivelse av hvordan vi bruker disse i SSB:\n\nUser: Er en enkelt ansatt som hentet fra vår EntraID\nGroup: Er et team som hentet fra vår EntraID f.eks.: microdata-developers\nDomain: Grupperer systemene under domener. Vi har valgt å binde domenene til emnene i veikartet:\n\nformidling\ndapla\nfellesfunksjoner\n\nSystem: En samling software og ressurser som sammen utfører en funksjon\nComponent: Et stykke software i et system\nAPI: Et grensesnitt for kommunikasjon mellom komponenter\nResource: Et stykke fysisk eller virituell infrastruktur for som trengs for å operere en komponent\n\n\n\n\nFor å dokumentere entiteter (systemer, komponenter, api’er og ressurser) må vi til statisticsnorway sin github. Backstage går nemlig gjennom alle repoene vi eier jevnlig og sjekker om nye backstage-definisjoner har blitt postet. Alt man trenger for at backstage skal legge merke til definisjonene dine er å:\n\nSette backstage som topic i repoet. Du gjør dette ved å trykke på tannhjulet ved siden av About på repo siden.\nLegge en backstage.yaml fil i roten av repoet med en gyldig backstage definisjon\n\nLa oss ta for oss et fiktivt microdata-system for å forklare hvordan man dokumenterer alle de forskjellige entitetene.\n\n\nSom sagt tidligere er users og groups hentet fra EntraID, og domenene er allerede definert sentralt. Når vi starter å dokumentere systemet vårt er derfor det første vi må starte med systemet selv.\nEttersom definisjonen av systemet selv ikke hører hjemme i noe spesielt repo, har vi valgt å lage et repo statisticsnorway/microdata-docs der vi lagrer dokumentasjon som tilhører microdatasystemet som helhet. Om vi tagger dette repoet med backstage-taggen, kan vi definere systemet vårt i roten av repoet med en backstage.yaml slik:\napiVersion: backstage.io/v1alpha1\nkind: System\nmetadata:\n  name: microdata\n  title: microdata\n  description: Tilgang på registerdata uten søknadsprosess\n  links:\n    - title: microdata.no\n      url: https://microdata.no\n    - title: data-administrasjon\n      url: https://microdata.no/datastore-admin\n  tags:\n    - on-premises\n    - python\n    - typescript\nspec:\n  owner: microdata-developers\n  domain: formidling\nLa oss se på feltene og hva de betyr:\n\napiVersion: spesifiserer backstage sitt dokumentformat\nkind: Vi ønsker å definere et System\n\n\n\n\nname: Navnet på systemet i kebab-case\ntitle: Menneskeleselig navn på Systemet\ndescription: En kort beskrivelse av systemet\nlinks: En liste med relevante lenker for systemet\ntags: En ustrukturert liste med emnemerker. Se retningslinjene for tags i SSB\n\n\n\n\n\nowner: Gruppen som eier systemet\ndomain: Domenetilhørligheten til systemet\n\nEtter noen minutter, kan vi navigere til backstage websiden og se at systemet vår har dukket opp.\n\n\n\n\nI microdata teamet publiserer vi et python bibliotek til PyPI. Denne pakken brukes av andre komponenter i systemet. La oss gå til github repoet til komponenten statisticsnorway/microdata-tools, markere repoet med backstage som topic, og legge til en backstage.yaml i roten av repoet:\napiVersion: backstage.io/v1alpha1\nkind: Component\nmetadata:\n  name: microdata-tools\n  title: Microdata tools\n  description: |\n    Tools for packaging, encrypting and validating microdata datasets\n  annotations:\n    github.com/project-slug: statisticsnorway/microdata-tools\n  tags:\n    - python\n    - pyarrow\n    - pydantic\nspec:\n  type: library\n  system: microdata\n  owner: microdata-developers\n  lifecycle: production\n  dependencyOf:\n    - component:microdata-job-service\nMange av feltene ligner veldig for å dokumentere en komponent. Dette er en komponent av type library. La oss se på hva feltene betyr:\n\n\n\nname: Navnet på komponenten i kebab-case Se retningslinjene for navn i SSB\ntitle: Menneskeleselig navn på komponenten\ndescription: En kort beskrivelse av komponenten\nannotations: Lokasjonen til komponenten på github\ntags: En ustrukturert liste med emnemerker. Se retningslinjene for tags i SSB\n\n\n\n\n\ntype: Hva slags type komponent dette er Se retningslinjene for type i SSB\nsystem: Systemet denne komponenten er en del av\nowner: Gruppen som eier kompoonenten\nlifecycle: Må være satt til en av:\n\nexperimental: Om komponenten er under utvikling, men ikke blitt prodsatt\nproduction: Om komponenten er prodsatt og i drift\ndeprecated: Om komponenten er markert for nedleggelse\n\ndependencyOf: Her kan du spesifisere hvilke andre entiteter som er avhengige av denne komponenten. Det er viktig å vite at om det finnes noen som bruker dette biblioteket, og marker seg selv som en avhengig av denne, vil det fortsatt registreres av backstage selv om denne avhengigheten ikke er til stedet under dependencyOf-feltet her.\n\n\n\n\n\nVi har et rest-api som kjører on-prem som vi kaller job-service. Job-service eksponerer et rest-api, og er avhengig av microdata-tools som avhengighet. Dette betyr at for å representere job-service må vi bruke to komponenter i backstage-modellen. En komponent for å beskrive tjenesten selv (type: service) og en api definisjon for å beskrive grensesnittet. Vi kan beskrive flere entiteter i samme backstage.yaml ved å putter tre bindestreker på en linje mellom definisjonene. La oss gå til github repoet til komponenten statisticsnorway/microdata-job-service, markere repoet med backstage som topic, og legge til en backstage.yaml i roten av repoet:\napiVersion: backstage.io/v1alpha1\nkind: Component\nLa oss ta for oss et fiktivt microdata-system for å forklare hvordan man dokumenterer alle de forskjellige entitetene.\nmetadata:\n  name: microdata-job-service\n  title:  Job service\n  description: |\n    Lookup service for jobs\n  tags:\n    - python\n    - flask\n    - pymongo\n  annotations:\n    github.com/project-slug: statisticsnorway/microdata-job-service\nspec:\n  type: service\n  system: microdata\n  owner: microdata-developers\n  lifecycle: production\n  providesApis:\n    - job-service-api\n  dependsOn:\n    - component:microdata-tools\n    - resource:microdata-job-db\n---\napiVersion: backstage.io/v1alpha1\nkind: API\nmetadata:\n  name: microdata-job-service-api\n  description: Job service\nspec:\n  type: openapi\n  system: microdata\n  owner: microdata-developers\n  lifecycle: production\n  definition:\n    $text: ./doc/openapi.yaml\nHer er mange av feltene for Component delen like som i sist eksempel, med unntak av:\n\nprovidesApis: Peker på grensesnitt ved en, eller flere, API entiteter denne komponenten eksponerer\ndependsOn: Peker på en eller flere komponenter og ressurser denne tjenesten er avhengig av\n\nFelter man kan ta i bruk som man ikke ser her er også:\n\nConsumesApis: Peker på grensesnitt ved en, eller flere, API entiteter denne komponenten konsumerer\n\nFor API spesifikasjonen som finnes under ---:\n\n\n\nname: Navnet på APIet i kebab-case Se retningslinjene for navn i SSB\ntitle: Menneskeleselig navn på APIet\ndescription: En kort beskrivelse av APIet\n\n\n\n\n\ntype: Hva slags type grensesnitt dette APIet er. Se retningslinjene for type i SSB\nsystem: Systemet dette APIet er en del av\nowner: Gruppen som eier APIet\nlifecycle: Må være satt til en av:\n\nexperimental: Om komponenten er under utvikling, men ikke blitt prodsatt\nproduction: Om komponenten er prodsatt og i drift\ndeprecated: Om komponenten er markert for nedleggelse\n\ndefinition: Om dette er et openapi kan det defineres med openapi formatet i en annen fil. Oppgi path til denne filen i samme repo med $text:\n\nVi får nå se avhengigheter tydelig i grafene som backstage generer. Vi kan også undersøke API definisjonene i backstage websiden ved å gå til “definition” fanen i API ressursen vi har definert.\n\n\n\n\nI microdata.no drifter vi også en mongodb som vi så over at job-service var avhengig av. Mongodb er en database-ressurs. Vi har et repo der vi bygger imaget til denne databasen, men her kunne du lagt ved ressursdefinisjonen sammen med applikasjonen eller i iac-repoet til teamen. Backstage definisjonen bør være så nærme den aktuelle koden som mulig, så tenk pragmatisk på det beste stedet å legge den. I dette tilfellet går vi til github repoet til ressursen statisticsnorway/microdata-job-db, markere repoet med backstage som topic, og legge til en backstage.yaml i roten av repoet:\napiVersion: backstage.io/v1alpha1\nkind: Resource\nmetadata:\n  name: microdata-job-db\n  description: |\n    MongoDB that stores jobs and job information in the microdata platform\n  tags:\n    - mongodb\n  annotations:\n    github.com/project-slug: statisticsnorway/microdata-job-db\nspec:\n  type: database\n  owner: microdata-developers\n  lifecycle: production\n  system: microdata\n\n\n\nname: Navnet på ressursen i kebab-case Se retningslinjene for navn i SSB\ntitle: Menneskeleselig navn på ressursen\ndescription: En kort beskrivelse av ressursen\n\n\n\n\n\ntype: Hva slags type ressurs dette er. Se retningslinjene for type i SSB\nsystem: Systemet denne ressursen er en del av\nowner: Gruppen som eier ressursen\nlifecycle: Må være satt til en av:\n\nexperimental: Om komponenten er under utvikling, men ikke blitt prodsatt\nproduction: Om komponenten er prodsatt og i drift\ndeprecated: Om komponenten er markert for nedleggelse\n\n\nDa har vi dokumentert alle de forskjellige entitetene vi trenger i backstage. Det er viktig at vi opprettholder denne informasjonen for en bedre utvikleropplevelse i SSB. Om du har noen spørsmål angående denne guiden, eller lurer på noe angående backstage; ta kontakt med tech lead gruppen på slack under #tech-lead-forum.\n\n\n\n\n\n\n\nFor å forsikre søkbarhet og god kommunikasjon er det viktig at vi bruker samme språk for å beskrive systemene våre. Det er anbefalt av backstage at organisasjonen tar stilling til bruk av type-feltet. Vi forsøker å holde mengden definisjoner til et minimum, og bruker samme terminologi som NAIS når vi har mulighet. Om du mener det mangler en type i listene her, ta gjerne kontakt med techlead-teamet på slack for diskusjon, eller post en pull request med forslaget til denne dokumentasjonen.\n\n\nFor type på komponenter skal kun en av disse brukes:\n\nservice: For langtlevende tjenester\nlibrary: For biblioteker/pakker som eksponeres på maven/pypi/crates el.\njob: For applikasjoner som er ment å kjøres på et skjema, one-shot eller på en trigger\nwebsite: For applikasjoner som skal eksponeres med browser\n\n\n\n\nFor type på APIer skal kun en av disse typene brukes:\n\nopenapi: Dette gjør at api’et kan dokumenteres med openapi dokumentasjon\n\n\n\n\nFor type på ressurser skal kun en av disse brukes:\n\ndatabase: for alle databaser\nbucket: for bøtter\nqueue: for meldingskøer som pub/sub og kafka\n\n\n\n\n\nPå samme måte ønsker vi at alle tagger sine systemer på en konsistent måte.\n\n\nTags for et system skal BARE inneholde:\n\nHvor systemet kjører. f.eks.: on-premises, bip, nais\nProgrammeringsspråkene brukt i systemet f.eks.: python, kotlin, rust\n\n\n\n\nTags for komponenter skal BARE inneholde:\n\nProgrammeringsspråkene brukt i systemet f.eks.: python, kotlin, rust\nKjerneteknologier brukt i komponenten f.eks.: micronaut, flask, pyarrow\n\n\n\n\nTags for ressurser kan BARE inneholde:\n\nSpesifisering av teknologi. ex.: postgresql, mongodb, pubsub\n\n\n\n\n\nFor at komponenter og ressurser skal kunne kobles sammen og vises korrekt i avheninghetsgrafen, så er vi avhengig av at det er unike tekniske navn på disse på tvers av Systemer i Backstage. Dette gjør vi enklest ved å prefikse med Systemet de tilhører: name: &lt;system&gt;-&lt;navn&gt;.\n\n\n\nMan kan validere at Backstage yaml filer er gyldige vha. følgende kommando:\nnpx @roadiehq/backstage-entity-validator backstage.yaml\n\n\n\n\n\nBackstage\nBackstage Docs: The system model\nADR for bruk av backstage i SBB",
    "crumbs": [
      "Utviklere",
      "Hvordan dokumentere systemer for backstage?"
    ]
  },
  {
    "objectID": "utviklere/dokumentere-for-backstage.html#beskrivelse-av-entiteter-i-backstage",
    "href": "utviklere/dokumentere-for-backstage.html#beskrivelse-av-entiteter-i-backstage",
    "title": "Hvordan dokumentere systemer for backstage?",
    "section": "",
    "text": "For å dokumentere våre systemer i backstage må vi være kjent med entitietene i backstage sin system modell. Her er en kort beskrivelse av hvordan vi bruker disse i SSB:\n\nUser: Er en enkelt ansatt som hentet fra vår EntraID\nGroup: Er et team som hentet fra vår EntraID f.eks.: microdata-developers\nDomain: Grupperer systemene under domener. Vi har valgt å binde domenene til emnene i veikartet:\n\nformidling\ndapla\nfellesfunksjoner\n\nSystem: En samling software og ressurser som sammen utfører en funksjon\nComponent: Et stykke software i et system\nAPI: Et grensesnitt for kommunikasjon mellom komponenter\nResource: Et stykke fysisk eller virituell infrastruktur for som trengs for å operere en komponent",
    "crumbs": [
      "Utviklere",
      "Hvordan dokumentere systemer for backstage?"
    ]
  },
  {
    "objectID": "utviklere/dokumentere-for-backstage.html#dokumentasjon",
    "href": "utviklere/dokumentere-for-backstage.html#dokumentasjon",
    "title": "Hvordan dokumentere systemer for backstage?",
    "section": "",
    "text": "For å dokumentere entiteter (systemer, komponenter, api’er og ressurser) må vi til statisticsnorway sin github. Backstage går nemlig gjennom alle repoene vi eier jevnlig og sjekker om nye backstage-definisjoner har blitt postet. Alt man trenger for at backstage skal legge merke til definisjonene dine er å:\n\nSette backstage som topic i repoet. Du gjør dette ved å trykke på tannhjulet ved siden av About på repo siden.\nLegge en backstage.yaml fil i roten av repoet med en gyldig backstage definisjon\n\nLa oss ta for oss et fiktivt microdata-system for å forklare hvordan man dokumenterer alle de forskjellige entitetene.\n\n\nSom sagt tidligere er users og groups hentet fra EntraID, og domenene er allerede definert sentralt. Når vi starter å dokumentere systemet vårt er derfor det første vi må starte med systemet selv.\nEttersom definisjonen av systemet selv ikke hører hjemme i noe spesielt repo, har vi valgt å lage et repo statisticsnorway/microdata-docs der vi lagrer dokumentasjon som tilhører microdatasystemet som helhet. Om vi tagger dette repoet med backstage-taggen, kan vi definere systemet vårt i roten av repoet med en backstage.yaml slik:\napiVersion: backstage.io/v1alpha1\nkind: System\nmetadata:\n  name: microdata\n  title: microdata\n  description: Tilgang på registerdata uten søknadsprosess\n  links:\n    - title: microdata.no\n      url: https://microdata.no\n    - title: data-administrasjon\n      url: https://microdata.no/datastore-admin\n  tags:\n    - on-premises\n    - python\n    - typescript\nspec:\n  owner: microdata-developers\n  domain: formidling\nLa oss se på feltene og hva de betyr:\n\napiVersion: spesifiserer backstage sitt dokumentformat\nkind: Vi ønsker å definere et System\n\n\n\n\nname: Navnet på systemet i kebab-case\ntitle: Menneskeleselig navn på Systemet\ndescription: En kort beskrivelse av systemet\nlinks: En liste med relevante lenker for systemet\ntags: En ustrukturert liste med emnemerker. Se retningslinjene for tags i SSB\n\n\n\n\n\nowner: Gruppen som eier systemet\ndomain: Domenetilhørligheten til systemet\n\nEtter noen minutter, kan vi navigere til backstage websiden og se at systemet vår har dukket opp.\n\n\n\n\nI microdata teamet publiserer vi et python bibliotek til PyPI. Denne pakken brukes av andre komponenter i systemet. La oss gå til github repoet til komponenten statisticsnorway/microdata-tools, markere repoet med backstage som topic, og legge til en backstage.yaml i roten av repoet:\napiVersion: backstage.io/v1alpha1\nkind: Component\nmetadata:\n  name: microdata-tools\n  title: Microdata tools\n  description: |\n    Tools for packaging, encrypting and validating microdata datasets\n  annotations:\n    github.com/project-slug: statisticsnorway/microdata-tools\n  tags:\n    - python\n    - pyarrow\n    - pydantic\nspec:\n  type: library\n  system: microdata\n  owner: microdata-developers\n  lifecycle: production\n  dependencyOf:\n    - component:microdata-job-service\nMange av feltene ligner veldig for å dokumentere en komponent. Dette er en komponent av type library. La oss se på hva feltene betyr:\n\n\n\nname: Navnet på komponenten i kebab-case Se retningslinjene for navn i SSB\ntitle: Menneskeleselig navn på komponenten\ndescription: En kort beskrivelse av komponenten\nannotations: Lokasjonen til komponenten på github\ntags: En ustrukturert liste med emnemerker. Se retningslinjene for tags i SSB\n\n\n\n\n\ntype: Hva slags type komponent dette er Se retningslinjene for type i SSB\nsystem: Systemet denne komponenten er en del av\nowner: Gruppen som eier kompoonenten\nlifecycle: Må være satt til en av:\n\nexperimental: Om komponenten er under utvikling, men ikke blitt prodsatt\nproduction: Om komponenten er prodsatt og i drift\ndeprecated: Om komponenten er markert for nedleggelse\n\ndependencyOf: Her kan du spesifisere hvilke andre entiteter som er avhengige av denne komponenten. Det er viktig å vite at om det finnes noen som bruker dette biblioteket, og marker seg selv som en avhengig av denne, vil det fortsatt registreres av backstage selv om denne avhengigheten ikke er til stedet under dependencyOf-feltet her.\n\n\n\n\n\nVi har et rest-api som kjører on-prem som vi kaller job-service. Job-service eksponerer et rest-api, og er avhengig av microdata-tools som avhengighet. Dette betyr at for å representere job-service må vi bruke to komponenter i backstage-modellen. En komponent for å beskrive tjenesten selv (type: service) og en api definisjon for å beskrive grensesnittet. Vi kan beskrive flere entiteter i samme backstage.yaml ved å putter tre bindestreker på en linje mellom definisjonene. La oss gå til github repoet til komponenten statisticsnorway/microdata-job-service, markere repoet med backstage som topic, og legge til en backstage.yaml i roten av repoet:\napiVersion: backstage.io/v1alpha1\nkind: Component\nLa oss ta for oss et fiktivt microdata-system for å forklare hvordan man dokumenterer alle de forskjellige entitetene.\nmetadata:\n  name: microdata-job-service\n  title:  Job service\n  description: |\n    Lookup service for jobs\n  tags:\n    - python\n    - flask\n    - pymongo\n  annotations:\n    github.com/project-slug: statisticsnorway/microdata-job-service\nspec:\n  type: service\n  system: microdata\n  owner: microdata-developers\n  lifecycle: production\n  providesApis:\n    - job-service-api\n  dependsOn:\n    - component:microdata-tools\n    - resource:microdata-job-db\n---\napiVersion: backstage.io/v1alpha1\nkind: API\nmetadata:\n  name: microdata-job-service-api\n  description: Job service\nspec:\n  type: openapi\n  system: microdata\n  owner: microdata-developers\n  lifecycle: production\n  definition:\n    $text: ./doc/openapi.yaml\nHer er mange av feltene for Component delen like som i sist eksempel, med unntak av:\n\nprovidesApis: Peker på grensesnitt ved en, eller flere, API entiteter denne komponenten eksponerer\ndependsOn: Peker på en eller flere komponenter og ressurser denne tjenesten er avhengig av\n\nFelter man kan ta i bruk som man ikke ser her er også:\n\nConsumesApis: Peker på grensesnitt ved en, eller flere, API entiteter denne komponenten konsumerer\n\nFor API spesifikasjonen som finnes under ---:\n\n\n\nname: Navnet på APIet i kebab-case Se retningslinjene for navn i SSB\ntitle: Menneskeleselig navn på APIet\ndescription: En kort beskrivelse av APIet\n\n\n\n\n\ntype: Hva slags type grensesnitt dette APIet er. Se retningslinjene for type i SSB\nsystem: Systemet dette APIet er en del av\nowner: Gruppen som eier APIet\nlifecycle: Må være satt til en av:\n\nexperimental: Om komponenten er under utvikling, men ikke blitt prodsatt\nproduction: Om komponenten er prodsatt og i drift\ndeprecated: Om komponenten er markert for nedleggelse\n\ndefinition: Om dette er et openapi kan det defineres med openapi formatet i en annen fil. Oppgi path til denne filen i samme repo med $text:\n\nVi får nå se avhengigheter tydelig i grafene som backstage generer. Vi kan også undersøke API definisjonene i backstage websiden ved å gå til “definition” fanen i API ressursen vi har definert.\n\n\n\n\nI microdata.no drifter vi også en mongodb som vi så over at job-service var avhengig av. Mongodb er en database-ressurs. Vi har et repo der vi bygger imaget til denne databasen, men her kunne du lagt ved ressursdefinisjonen sammen med applikasjonen eller i iac-repoet til teamen. Backstage definisjonen bør være så nærme den aktuelle koden som mulig, så tenk pragmatisk på det beste stedet å legge den. I dette tilfellet går vi til github repoet til ressursen statisticsnorway/microdata-job-db, markere repoet med backstage som topic, og legge til en backstage.yaml i roten av repoet:\napiVersion: backstage.io/v1alpha1\nkind: Resource\nmetadata:\n  name: microdata-job-db\n  description: |\n    MongoDB that stores jobs and job information in the microdata platform\n  tags:\n    - mongodb\n  annotations:\n    github.com/project-slug: statisticsnorway/microdata-job-db\nspec:\n  type: database\n  owner: microdata-developers\n  lifecycle: production\n  system: microdata\n\n\n\nname: Navnet på ressursen i kebab-case Se retningslinjene for navn i SSB\ntitle: Menneskeleselig navn på ressursen\ndescription: En kort beskrivelse av ressursen\n\n\n\n\n\ntype: Hva slags type ressurs dette er. Se retningslinjene for type i SSB\nsystem: Systemet denne ressursen er en del av\nowner: Gruppen som eier ressursen\nlifecycle: Må være satt til en av:\n\nexperimental: Om komponenten er under utvikling, men ikke blitt prodsatt\nproduction: Om komponenten er prodsatt og i drift\ndeprecated: Om komponenten er markert for nedleggelse\n\n\nDa har vi dokumentert alle de forskjellige entitetene vi trenger i backstage. Det er viktig at vi opprettholder denne informasjonen for en bedre utvikleropplevelse i SSB. Om du har noen spørsmål angående denne guiden, eller lurer på noe angående backstage; ta kontakt med tech lead gruppen på slack under #tech-lead-forum.",
    "crumbs": [
      "Utviklere",
      "Hvordan dokumentere systemer for backstage?"
    ]
  },
  {
    "objectID": "utviklere/dokumentere-for-backstage.html#retningslinjer",
    "href": "utviklere/dokumentere-for-backstage.html#retningslinjer",
    "title": "Hvordan dokumentere systemer for backstage?",
    "section": "",
    "text": "For å forsikre søkbarhet og god kommunikasjon er det viktig at vi bruker samme språk for å beskrive systemene våre. Det er anbefalt av backstage at organisasjonen tar stilling til bruk av type-feltet. Vi forsøker å holde mengden definisjoner til et minimum, og bruker samme terminologi som NAIS når vi har mulighet. Om du mener det mangler en type i listene her, ta gjerne kontakt med techlead-teamet på slack for diskusjon, eller post en pull request med forslaget til denne dokumentasjonen.\n\n\nFor type på komponenter skal kun en av disse brukes:\n\nservice: For langtlevende tjenester\nlibrary: For biblioteker/pakker som eksponeres på maven/pypi/crates el.\njob: For applikasjoner som er ment å kjøres på et skjema, one-shot eller på en trigger\nwebsite: For applikasjoner som skal eksponeres med browser\n\n\n\n\nFor type på APIer skal kun en av disse typene brukes:\n\nopenapi: Dette gjør at api’et kan dokumenteres med openapi dokumentasjon\n\n\n\n\nFor type på ressurser skal kun en av disse brukes:\n\ndatabase: for alle databaser\nbucket: for bøtter\nqueue: for meldingskøer som pub/sub og kafka\n\n\n\n\n\nPå samme måte ønsker vi at alle tagger sine systemer på en konsistent måte.\n\n\nTags for et system skal BARE inneholde:\n\nHvor systemet kjører. f.eks.: on-premises, bip, nais\nProgrammeringsspråkene brukt i systemet f.eks.: python, kotlin, rust\n\n\n\n\nTags for komponenter skal BARE inneholde:\n\nProgrammeringsspråkene brukt i systemet f.eks.: python, kotlin, rust\nKjerneteknologier brukt i komponenten f.eks.: micronaut, flask, pyarrow\n\n\n\n\nTags for ressurser kan BARE inneholde:\n\nSpesifisering av teknologi. ex.: postgresql, mongodb, pubsub\n\n\n\n\n\nFor at komponenter og ressurser skal kunne kobles sammen og vises korrekt i avheninghetsgrafen, så er vi avhengig av at det er unike tekniske navn på disse på tvers av Systemer i Backstage. Dette gjør vi enklest ved å prefikse med Systemet de tilhører: name: &lt;system&gt;-&lt;navn&gt;.\n\n\n\nMan kan validere at Backstage yaml filer er gyldige vha. følgende kommando:\nnpx @roadiehq/backstage-entity-validator backstage.yaml",
    "crumbs": [
      "Utviklere",
      "Hvordan dokumentere systemer for backstage?"
    ]
  },
  {
    "objectID": "utviklere/dokumentere-for-backstage.html#lenker",
    "href": "utviklere/dokumentere-for-backstage.html#lenker",
    "title": "Hvordan dokumentere systemer for backstage?",
    "section": "",
    "text": "Backstage\nBackstage Docs: The system model\nADR for bruk av backstage i SBB",
    "crumbs": [
      "Utviklere",
      "Hvordan dokumentere systemer for backstage?"
    ]
  },
  {
    "objectID": "om-dapla.html",
    "href": "om-dapla.html",
    "title": "Om Dapla",
    "section": "",
    "text": "Om Dapla\nDapla står for dataplattform, og er en skybasert løsning for statistikkproduksjon og forskning.\n\n\nHvorfor Dapla?\nSom dataplattform skal Dapla stimulerere til økt kvalitet på statistikk og forskning, samtidig som den gjør organisasjonen mer tilpasningsdyktig i møte med fremtiden.\n\n\nDen nye skybaserte dataplattformen (Dapla) skal bli viktig for å effektivisere arbeids-og produksjonsprosesser, den skal sikre effektiv lagring og gjenfinning av data og metadata, og støtte opp under deling av data på tvers av statistikkområder.\n\nKilde: Langtidsplan for SSB (2022-2024)\n\n\n\nMålet med Dapla er å tilby tjenester og verktøy som lar statistikkprodusenter og forskere produsere resultater på en sikker og effektiv måte."
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Eksempler",
    "section": "",
    "text": "Eksempler\nSe menyen til venstre for eksempler.",
    "crumbs": [
      "Eksempler"
    ]
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html",
    "href": "notebooks/spark/pyspark-intro.html",
    "title": "Introduksjon til PySpark",
    "section": "",
    "text": "Apache Spark er et sterkt verktøy som utvider mulighetsrommet for databehandling med R og Python. Kjernefunksjonaliteten ligger i at den lar deg kjøre en jobb på flere maskiner samtidig, noe som ikke er mulig med klassiske rammeverk som Pandas og Tidyverse. Følgelig er det et rammeverk som blant annet er veldig egnet for å prosessere store datamengder eller gjøre store beregninger. Les om mer om Apache Spark i Dapla-manualen\nI denne notebooken vises noen enkle eksempler på hvordan du kan jobbe med data med PySpark, et Python-grensesnitt mot Spark."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#oppsett",
    "href": "notebooks/spark/pyspark-intro.html#oppsett",
    "title": "Introduksjon til PySpark",
    "section": "Oppsett",
    "text": "Oppsett\nNår du logger deg inn på Dapla kan du velge mellom 2 ferdigoppsatte kernels for å jobbe med PySpark:\n\nPyspark (local)\nPyspark (k8s cluster)\n\nDen første lar deg bruke Spark på en enkeltmaskin, mens den andre lar deg distribuere kjøringen på mange maskiner avhengig av hvor store jobbene er. I eksemplene under brukes Pyspark (local).\n\n# Importer biblioteker\nimport numpy as np\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import date_format, explode, expr, sequence, sum, avg\nfrom pyspark.sql.types import DateType, DoubleType, StructField, StructType\n\n# Initialierer en SparkSession\nspark = (\n    SparkSession.builder.master(\"local[2]\")\n    .appName(\"Dapla-manual-example\")\n    .getOrCreate()\n)\n\nI koden over importerer vi de bibliotekene vi skal bruke under. Grunnen til at vi importerer pyspark.sql er at dette er at Spark SQL er Apache Spark sin modul for å jobbe med strukturerte data. Og som navnet tilsier vil det si at vi kan blande Python og SQL i koden vår. Vi skal nærmere på hvordan man bruke SQL fra PySpark-notebook senere.\nSpark tilbyr også et eget grensesnitt, Spark UI, for å monitorere hva som skjer under en SparkSession. Vi kan bruke følgende kommando for å få opp en lenke til Spark UI i notebooken vår:\n\nspark.sparkContext\n\n\n        \n            SparkContext\n\n            Spark UI\n\n            \n              Version\n                v3.3.1\n              Master\n                local[*]\n              AppName\n                pyspark-shell\n            \n        \n        \n\n\nKlikker du på Spark UI-lenken så tar den deg til dashboard som lar deg monitorere, debugge, optimalisere og forstå kjøringene dine. Det kan være et svært nyttig verktøy i mange tilfeller."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#generere-data",
    "href": "notebooks/spark/pyspark-intro.html#generere-data",
    "title": "Introduksjon til PySpark",
    "section": "Generere data",
    "text": "Generere data\nVi kan begynne med å generere en Spark DataFrame med en kolonne som inneholder månedlige datoer for perioden 2000M1-2023M8.\n\n# Genererer månedlige data\ndates_df = spark.range(1).select(\n    explode(\n        sequence(\n            start=expr(\"date '2000-01-01'\"),\n            stop=expr(\"date '2023-08-01'\"),\n            step=expr(\"interval 1 month\"),\n        )\n    ).alias(\"Date\")\n)\ndates_df.show(5)\n\n+----------+\n|      Date|\n+----------+\n|2000-01-01|\n|2000-02-01|\n|2000-03-01|\n|2000-04-01|\n|2000-05-01|\n+----------+\nonly showing top 5 rows\n\n\n\nEn Spark DataFrame er en distribuert samling av data som er organisert inn i kolonner. Siden Spark lar deg distribuere kjøringer på flere maskiner, er DataFrames optimalisert for å kunne splittes opp slik at de kan brukes på flere maskiner. Med andre er dette ikke det samme som en Pandas dataframe mange kjenner fra før.\nOver genererte vi en datokolonne. For å få litt mer data kan vi også generere 100 kolonner med tidsseriedata og så printer vi de 2 første av disse:\n\n# Genererer random walk data\nschema = StructType(\n    [StructField(f\"serie{i:02d}\", DoubleType(), True) for i in range(100)]\n)\n\ndata = [\n    tuple((10 + np.random.normal(0, 1, 100)).cumsum().tolist())\n    for _ in range(284)  # 284 months from 2000-01 to 2023-08\n]\n\ndata_df = spark.createDataFrame(data, schema=schema)\n\ndata_df.select(\"serie00\", \"serie01\").show(5)\n\n+------------------+------------------+\n|           serie00|           serie01|\n+------------------+------------------+\n|10.410703377184355| 21.06318801110079|\n|10.509249410154466|  19.5674295298024|\n| 9.618310122060274|17.635805093465642|\n| 9.691112692298294|18.593842915949082|\n| 9.903675228685067|20.012215769058564|\n+------------------+------------------+\nonly showing top 5 rows\n\n\n\nTil slutt kan vi joine de to datasettene sammen og lage noen kolonner som viser år, kvartal og måned. Deretter printer vi ut noen av kolonnene med kommandoen show().\n\n\nCode\n# Legger til row index til DataFrame før join med dates_df\ndata_df = data_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n\n# Joiner de to datasettene\ndf = (\n    dates_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n    .join(data_df, \"row_index\")\n    .drop(\"row_index\")\n)\n\n# Legger til år, kvartal og mnd\ndf = df.withColumn(\"Year\", date_format(df.Date, \"yyyy\"))\ndf = df.withColumn(\"Quarter\", expr(\"quarter(Date)\"))\ndf = df.withColumn(\"Month\", date_format(df.Date, \"MM\"))\n\ndf.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows\n\n\n\nOg med det har vi noe data vi kan jobbe med i resten av notebooken."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#skrive-til-parquet",
    "href": "notebooks/spark/pyspark-intro.html#skrive-til-parquet",
    "title": "Introduksjon til PySpark",
    "section": "Skrive til Parquet",
    "text": "Skrive til Parquet\nPySpark tilbyr mange opsjoner ved skriving til parquet-filer som vi kanskje ikke er vant til å forholde oss til med enklere rammeverk som Pandas. Den enkleste måten å skrive ut en fil er som følger:\ndf.write.parquet(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp/timeseries.parquet\"\n)\nDette vil fungere hvis filen ikke finnes fra før. Hvis den finnes fra før så vil den feile. Grunnen er at vi ikke har spesifisert hva vi ønsker at den skal gjøre. Vi kan velge mellom overwrite, append, ignore eller errorifexists. Sistnevnte er også default-oppførsel hvis du ikke ber den gjøre noe annet.\nUnder bruker vi opsjonen overwrite, det vil si at den skriver over en evt eksisterende fil med samme navn.\n\ndf.write.mode(\"overwrite\").parquet(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp/timeseries.parquet\"\n)\n\nVi kan inspisere hva som ble skrevet ved å liste ut innholder i bøtta.\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/temp/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp/',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/_SUCCESS',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/part-00000-b32e7299-0590-4b31-bcc2-dc3d58725529-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/part-00001-b32e7299-0590-4b31-bcc2-dc3d58725529-c000.snappy.parquet']\n\n\nHvis denne parquet-filen hadde vært partisjonert etter en kolonne, så ville det vært egne undermapper med navnestruktur column_name=value som indikerte hva filen er partisjonert på. Siden vi her bruker en maskin og har et lite datasett, valgte Spark å ikke partisjonere."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#lese-inn-parquet",
    "href": "notebooks/spark/pyspark-intro.html#lese-inn-parquet",
    "title": "Introduksjon til PySpark",
    "section": "Lese inn Parquet",
    "text": "Lese inn Parquet\nApache Spark kan lese inn flere parquet-filer samtidig. Syntaxen er like enkel som den for å skrive ut.\n\ndf_ts = spark.read.parquet(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp/timeseries.parquet\"\n)\ndf_ts.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie66\", \"serie55\").show(5)\n\n+----------+----+-------+-----+-----------------+-----------------+\n|      Date|Year|Quarter|Month|          serie66|          serie55|\n+----------+----+-------+-----+-----------------+-----------------+\n|2000-01-01|2000|      1|   01|670.2679830025959|562.4312808525777|\n|2000-02-01|2000|      1|   02|675.4233411662802|562.5168447360121|\n|2000-03-01|2000|      1|   03|687.3412458214908|568.6203957584232|\n|2000-04-01|2000|      2|   04|673.1128047244955|557.4633871253379|\n|2000-05-01|2000|      2|   05| 667.513406101114|561.7766450346327|\n+----------+----+-------+-----+-----------------+-----------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#pyspark-og-sql",
    "href": "notebooks/spark/pyspark-intro.html#pyspark-og-sql",
    "title": "Introduksjon til PySpark",
    "section": "PySpark og SQL",
    "text": "PySpark og SQL\nDu kan også skrive SQL med Spark. For å skrive SQL må vi først lage et temporary view. Under kaller vi viewt for tidsserie.\n\ndf.createOrReplaceTempView(\"tidsserie\")\n\nVi kan deretter skrive en SQL-statement som vi ønsker å kjøre på viewet:\n\nquery = \"SELECT * FROM tidsserie WHERE Year = 2010\"\n\nDeretter kan vi bruke det til å filtrere datasettet:\n\nresult_df = spark.sql(query)\nresult_df.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2010-01-01|2010|      1|   01| 11.26910423907778|21.730128215168268|\n|2010-02-01|2010|      1|   02| 8.722783282690738| 17.46851086792347|\n|2010-03-01|2010|      1|   03|10.376873608348605|20.109386343182802|\n|2010-04-01|2010|      2|   04|11.459959305590926|21.995141825523866|\n|2010-05-01|2010|      2|   05|10.441456792180572| 21.25096473981906|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#aggregering",
    "href": "notebooks/spark/pyspark-intro.html#aggregering",
    "title": "Introduksjon til PySpark",
    "section": "Aggregering",
    "text": "Aggregering\nvi kan aggregere opp enten med PySpark-syntax eller SQL-syntax. Under viser vi begge:\n\nfrom pyspark.sql import functions as F\n\n# Assuming df_ts is your DataFrame\naggregated_df = df_ts.groupBy(\"Quarter\").agg(F.sum(\"serie00\").alias(\"Sum\"),\n                                             F.avg(\"serie00\").alias(\"Average\"),\n                                             F.max(\"serie00\").alias(\"Maximum\"))\n\n# Show the result\naggregated_df.show()\n\n+-------+------------------+------------------+------------------+\n|Quarter|               Sum|           Average|           Maximum|\n+-------+------------------+------------------+------------------+\n|      1|363.95869885234185|10.109963857009497|11.829453550532005|\n|      3|365.68324879453405| 10.15786802207039|12.233378837422391|\n|      4| 342.2334082209804|10.065688477087658|12.210138970053695|\n|      2|  361.991445506568|10.055317930738001|12.276030776082463|\n+-------+------------------+------------------+------------------+\n\n\n\nLa oss gjøre det samme med SQL, men grupperer etter to variabler og sorterer output etterpå.\n\n# Assuming df_ts is your DataFrame\ndf_ts.createOrReplaceTempView(\"temp_table\")\n\n# Now you can run a SQL query\nquery = \"\"\"\n    SELECT\n        Year,\n        Quarter,\n        SUM(serie00) AS Sum,\n        AVG(serie00) AS Average,\n        MAX(serie00) AS Maximum\n    FROM \n        temp_table\n    GROUP BY \n        Year, Quarter\n    ORDER BY\n        YEAR, QUARTER\n\"\"\"\n\naggregated_df_sql = spark.sql(query)\n\n# Show the result\naggregated_df_sql.show(10)\n\n+----+-------+------------------+------------------+------------------+\n|Year|Quarter|               Sum|           Average|           Maximum|\n+----+-------+------------------+------------------+------------------+\n|2000|      1|31.323050432219453|10.441016810739818|11.118293927071951|\n|2000|      2|28.911192473027377| 9.637064157675793| 9.900977410685329|\n|2000|      3|33.670797229797415|11.223599076599138|12.233378837422391|\n|2000|      4|28.094793356286914| 9.364931118762305| 10.32000478359274|\n|2001|      1|31.636678535169537|10.545559511723178|11.367822302191831|\n|2001|      2|29.629770128521507| 9.876590042840503|11.135215930381191|\n|2001|      3| 30.75408440118315| 10.25136146706105|10.723803326978505|\n|2001|      4|30.361048932627902|  10.1203496442093|10.368365984482093|\n|2002|      1|31.184163218551227|10.394721072850409|10.550579652234951|\n|2002|      2|29.128978392451202|   9.7096594641504|10.186365745367246|\n+----+-------+------------------+------------------+------------------+\nonly showing top 10 rows"
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.out.html",
    "href": "notebooks/spark/sparkr-intro.out.html",
    "title": "Introduksjon til SparkR",
    "section": "",
    "text": "Akkurat som PySpark så gir SparkR oss et grensesnitt mot Apache Spark fra R. I denne notebooken viser vi noen eksempler hvordan du gjøre vanlige operasjoner med SparkR."
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.out.html#oppsett",
    "href": "notebooks/spark/sparkr-intro.out.html#oppsett",
    "title": "Introduksjon til SparkR",
    "section": "Oppsett",
    "text": "Oppsett\nEksemplene i notebooken bruker SparkR (k8s cluster) på https://jupyter.dapla.ssb.no/. Det vil si at den kan distribuere kjøringene på flere maskiner i Kubernetes.\n\nspark\n\nJava ref type org.apache.spark.sql.SparkSession id 1"
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.out.html#lese-inn-fil",
    "href": "notebooks/spark/sparkr-intro.out.html#lese-inn-fil",
    "title": "Introduksjon til SparkR",
    "section": "Lese inn fil",
    "text": "Lese inn fil\n\nfile = read.parquet(\"gs://ssb-dapla-felles-data-produkt-prod/temp/timeseries.parquet\")\n\n\nselectedColumns &lt;- select(file, \"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\")\nshowDF(selectedColumns, numRows = 5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.html",
    "href": "notebooks/spark/sparkr-intro.html",
    "title": "Introduksjon til SparkR",
    "section": "",
    "text": "Akkurat som PySpark så gir SparkR oss et grensesnitt mot Apache Spark fra R. I denne notebooken viser vi noen eksempler hvordan du gjøre vanlige operasjoner med SparkR."
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.html#oppsett",
    "href": "notebooks/spark/sparkr-intro.html#oppsett",
    "title": "Introduksjon til SparkR",
    "section": "Oppsett",
    "text": "Oppsett\nEksemplene i notebooken bruker SparkR (k8s cluster) på https://jupyter.dapla.ssb.no/. Det vil si at den kan distribuere kjøringene på flere maskiner i Kubernetes.\n\nspark\n\nJava ref type org.apache.spark.sql.SparkSession id 1"
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.html#lese-inn-fil",
    "href": "notebooks/spark/sparkr-intro.html#lese-inn-fil",
    "title": "Introduksjon til SparkR",
    "section": "Lese inn fil",
    "text": "Lese inn fil\n\nfile = read.parquet(\"gs://ssb-dapla-felles-data-produkt-prod/temp/timeseries.parquet\")\n\n\nselectedColumns &lt;- select(file, \"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\")\nshowDF(selectedColumns, numRows = 5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "statistikkere/metodebibliotek.html",
    "href": "statistikkere/metodebibliotek.html",
    "title": "Metodebiblioteket",
    "section": "",
    "text": "Metodebiblioteket er SSBs bibliotek for statistiske metode funksjoner. Biblioteket finner du her : https://statisticsnorway.github.io/ssb-metodebiblioteket/.\nMetodene er organisert som en liste av funksjoner skrevet i R eller Python. Funksjonene kan kjøres på både på Dapla og i bakke-miljøene. Alle funksjoner er testet av Seksjon for Metode for bruk i produksjon av offisiell statistikk, og alle er brukt i minst ett produksjonsløp eller i SSBs interne metodekurs. Funksjonene er både utviklet internt og hentet fra godkjente eksterne pakker.\nMetodebiblioteket kan søkes i enten via en generell liste eller gjennom steg i prosessmodellen.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Metodebiblioteket"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html",
    "href": "statistikkere/jupyter-pyspark.html",
    "title": "Jupyter-pyspark",
    "section": "",
    "text": "Jupyter-pyspark er en tjeneste på Dapla Lab som lar brukerne kode i Jupyterlab. Tjenesten kommer med Python, Pyspark og noen vanlige Jupyterlab-extensions ferdig installert. Målgruppen for tjenesten er brukere som skal skrive produksjonskode med Pyspark i Jupyterlab.\nSiden tjenesten er ment for produksjonskode så er det veldig få Python-pakker som er forhåndsinstallert. Antagelsen er at brukeren/teamet heller bør installere de pakkene de selv trenger, framfor at det ligger ferdiginstallerte pakker som skal dekke behovet til alle brukere/team i SSB. Det reduserer kompleksitet i tjenesten og dermed sannsynligheten for feilsituasjoner.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#forberedelser",
    "href": "statistikkere/jupyter-pyspark.html#forberedelser",
    "title": "Jupyter-pyspark",
    "section": "Forberedelser",
    "text": "Forberedelser\nFør man starter Jupyter-pyspark bør man ha lest kapitlet om Dapla Lab og satt opp Git- og GitHub-konfigurasjonen under Min konto. Deretter gjør du følgende:\n\nLogg deg inn på Dapla Lab\nUnder Tjenestekatalog trykker du på Start-knappen for Jupyter-pyspark\nGi tjenesten et navn\nÅpne Jupyter-pyspark konfigurasjoner",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#konfigurasjon",
    "href": "statistikkere/jupyter-pyspark.html#konfigurasjon",
    "title": "Jupyter-pyspark",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nKonfigurasjonen av Jupyter-pyspark er nær identisk Jupyter-tjenesten sin konfigurasjon. Se dokumentasjonen for konfigurasjon av Jupyter.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#tilgjengelige-jar-filer-og-bruk-i-pyspark",
    "href": "statistikkere/jupyter-pyspark.html#tilgjengelige-jar-filer-og-bruk-i-pyspark",
    "title": "Jupyter-pyspark",
    "section": "Tilgjengelige JAR-filer og bruk i PySpark",
    "text": "Tilgjengelige JAR-filer og bruk i PySpark\nI /jupyter/lib-mappen i Jupyter-pyspark-miljøet er flere nyttige JAR-filer tilgjengelige for bruk med PySpark, inkludert støtte for Google Cloud Storage, BigQuery, Avro og Delta Lake. Disse JAR-filene kan inkluderes i PySpark-konfigurasjonen for å få tilgang til og arbeide med data fra disse kildene.\n\nTilgjengelige JAR-filer:\n\ngcs-connector-hadoop.jar: Kobler PySpark til Google Cloud Storage.\nspark-bigquery-with-dependencies_2.12.jar: Kobler PySpark til Google BigQuery.\nspark-avro_2.12.jar: Støtte for å lese og skrive Avro-data.\ndelta-storage.jar og delta-core_2.12.jar: Støtte for Delta Lake, som muliggjør ACID-transaksjoner og data versjonering.\n\nLegge til JAR-filer i PySpark:\n\nFor å bruke disse JAR-filene, konfigurer PySpark med stien til hver fil:\nspark = SparkSession.builder \\\n    .appName(\"Jupyter-pyspark-konfig\") \\\n    .config(\"spark.jars\", \"/jupyter/lib/gcs-connector-hadoop.jar,\"\n                          \"/jupyter/lib/spark-bigquery-with-dependencies_2.12.jar,\"\n                          \"/jupyter/lib/spark-avro_2.12.jar,\"\n                          \"/jupyter/lib/delta-storage.jar,\"\n                          \"/jupyter/lib/delta-core_2.12.jar\") \\\n    .getOrCreate()\n\nEksempler på bruk av tilkoblingene:\n\nGoogle Cloud Storage (GCS):\ndf = spark.read.format(\"parquet\").load(\"gs://ditt-bucket-navn/path/to/data\")\nGoogle BigQuery:\ndf = spark.read.format(\"bigquery\") \\\n    .option(\"table\", \"prosjekt_id.dataset_id.tabell_id\") \\\n    .load()\nAvro:\ndf = spark.read.format(\"avro\").load(\"/path/to/avro/files\")\nDelta Lake:\n\nFor å skrive til en Delta-tabell:\ndf.write.format(\"delta\").save(\"/path/to/delta-table\")\nFor å lese fra en Delta-tabell:\ndelta_df = spark.read.format(\"delta\").load(\"/path/to/delta-table\")\n\n\n\nMed disse instruksjonene kan brukerne effektivt konfigurere Jupyter-PySpark til å jobbe med eksterne datakilder og forskjellige dataformater i sitt PySpark-miljø.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#hvordan-spark-lokalt-fungerer-og-arbeidsfordeling-på-kjerner",
    "href": "statistikkere/jupyter-pyspark.html#hvordan-spark-lokalt-fungerer-og-arbeidsfordeling-på-kjerner",
    "title": "Jupyter-pyspark",
    "section": "Hvordan Spark Lokalt Fungerer og Arbeidsfordeling på Kjerner",
    "text": "Hvordan Spark Lokalt Fungerer og Arbeidsfordeling på Kjerner\nNår Spark kjøres lokalt, starter det en SparkSession som kjører på en enkelt node (tjenesten din) uten å involvere en distribuert klynge. Lokalt i Spark kan du kontrollere ressursbruken og fordele arbeidsmengden på tilgjengelige CPU-kjerner for å optimalisere ytelsen.\n\nKjøring i Lokal Modus\nNår Spark konfigureres til å kjøre i lokal modus, spesifiseres dette med \"local[N]\", der N representerer antall kjerner som Spark skal bruke. For eksempel: - \"local[*]\": Bruk alle tilgjengelige kjerner på maskinen. - \"local[2]\": Bruk 2 kjerner, uavhengig av hvor mange som er tilgjengelige.\nEksempel på konfigurasjon:\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"Lokal Spark\") \\\n    .master(\"local[*]\") \\  # Bruker alle tilgjengelige kjerner\n    .getOrCreate()",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#datatilgang",
    "href": "statistikkere/jupyter-pyspark.html#datatilgang",
    "title": "Jupyter-pyspark",
    "section": "Datatilgang",
    "text": "Datatilgang\nMan inspisere dataene fra en terminal inne i tjenesten:\n\nÅpne en instans av Jupyter-pyspark med data fra bøtter\nÅpne en terminal inne i Jupyter\nGå til mappen med bøttene ved å kjøre dette fra terminalen cd /buckets\nKjør ls -ahl i terminalen for å se på hvilke bøtter som er montert.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#installere-pakker",
    "href": "statistikkere/jupyter-pyspark.html#installere-pakker",
    "title": "Jupyter-pyspark",
    "section": "Installere pakker",
    "text": "Installere pakker\nSiden det nesten ikke er installert noen pakker i tjenesten så kan brukeren opprette et ssb-project og installere pakker som vanlig.\nFor å bygge et eksisterende ssb-project kan brukeren også bruke ssb-project.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#slette-tjenesten",
    "href": "statistikkere/jupyter-pyspark.html#slette-tjenesten",
    "title": "Jupyter-pyspark",
    "section": "Slette tjenesten",
    "text": "Slette tjenesten\nFor å slette tjenesten kan man trykke på Slette-knappen i Dapla Lab under Mine tjenester. Når man sletter en tjeneste så slettes hele disken inne i tjenesten, og alle ressurser frigjøres. Vi anbefaler at man avslutter heller enn pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#pause-tjenesten",
    "href": "statistikkere/jupyter-pyspark.html#pause-tjenesten",
    "title": "Jupyter-pyspark",
    "section": "Pause tjenesten",
    "text": "Pause tjenesten\nMan kan pause tjenesten ved å trykke på Pause-knappen i Dapla Lab under Mine tjenester. Når man pauser, slettes alt på den lokale disken som ikke er lagret under $HOME/work. Vi anbefaler at man avslutter heller enn pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-pyspark.html#monitorering",
    "href": "statistikkere/jupyter-pyspark.html#monitorering",
    "title": "Jupyter-pyspark",
    "section": "Monitorering",
    "text": "Monitorering\n\n\nMan kan moniterere en instans av Jupyter-pyspark ved å trykke på Jupyter-pyspark-teksten under Mine tjenester i Dapla Lab, slik som vist i Figur 1.\nDenne funksjonaliteten er under arbeid og mer informasjon kommer snart.\n\n\n\n\n\n\n\n\n\nFigur 1: Monitorering av Jupyter-pyspark i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-pyspark"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-model.html",
    "href": "statistikkere/datadoc-model.html",
    "title": "Datamodell",
    "section": "",
    "text": "I dette kapitlet beskrives informasjonselementene i Datadoc. Siden noen gjelder for datasett og andre gjelder for variabler, så er kapitlet delt inn etter disse.\nFor hvert informasjonselement angis det om informasjonen er obligatorisk✅ for gitte datatilstander. Siden inndata ikke er en obligatorisk datatilstand, og kildedata ikke er mulig å dokumentere ennå, så angis de ikke. Men hvis man lagrer inndata i en statistikkproduksjon, så er de samme feltene obligatorisk som for klargjorte data.\nDet angis også hva det internasjonale🌐 navnet er, det er dette navnet som benyttes i Datadoc-filene som genereres. Noen informasjonselementer er kun relevant for noen typer av data, og de er derfor kun obligatorisk hvis man har denne typen data, f.eks. er bruksrestriksjoner kun obligatorisk å fylle ut dersom datasettet har slike, og “måleenhet” kun obligatorisk for kvantitative1 variabler. Dette angis med ⚠️. Felter som er valgfrie, angis ved (valgfritt).\n✅ = obligatorisk, ⚠️ = hvis relevant, 🌐 = internasjonalt",
    "crumbs": [
      "Manual",
      "Metadata",
      "Datadoc",
      "Datamodell"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-model.html#datasett",
    "href": "statistikkere/datadoc-model.html#datasett",
    "title": "Datamodell",
    "section": "Datasett",
    "text": "Datasett\n\nNavn\n🌐 name\nklargjorte data ✅ | statistikkdata ✅ | utdata ✅\nAngir navnet til datasettet. Navnet skal være forståelig for mennesker (ikke kun forkortelser) slik at det er søkbart. Navnet skal fylles ut på bokmål eller nynorsk. Det er valgfritt om en også vil fylle ut på den andre norske målformen og engelsk.\n\n\n\n\n\n\nTipEksempler\n\n\n\n\n\nEksempel 1:\n\nLevekårsundersøkelsen\n\n\n\n\n\n\n\nBeskrivelse\n🌐 description\nklargjorte data ✅ | statistikkdata ✅ | utdata ✅\nBeskriver kort innholdet i datasettet.\n\n\n\n\n\n\nTipEksempler\n\n\n\n\n\nEksempel 1:\n\nInneholder data om bruk av helsetjenester (Levekårsundersøkelsen).\n\n\n\n\n\n\n\nVerdivurdering\n🌐 assessment\nklargjorte data ✅ | statistikkdata ✅ | utdata ✅\nUtledes automatisk basert på datasettets datatilstand, og informasjon om datatilstand hentes fra filstien (se Navnestandarden) til den dokumenterte filen.\nTabell 1 viser sammenhengen mellom datatilstander og verdivurdering.\n\n\n\nTabell 1: Sammenheng mellom datatilstand og verdivurdering\n\n\n\n\n\nDatatilstand\nVerdivurdering\n\n\n\n\nKildedata\nSensitiv\n\n\nInndata\nSkjermet\n\n\nKlargjorte data\nSkjermet\n\n\nStatistikkdata\nSkjermet\n\n\nUtdata\nÅpen\n\n\n\n\n\n\n\n\n\nBruksrestriksjoner\n🌐 use_restrictions\nklargjorte data ⚠️ | statistikkdata ⚠️\nAngis om datasettet har bruksrestriksjoner. Et datasett kan fint ha null, én eller flere bruksrestriksjoner. Dersom datasettet har ingen brukrestriksjoner så kan feltet stå tomt, og i json-utskriften vil verdien i dette feltet da vises som null.\nBruksrestriksjoner er oppgitt som en liste hvor hvert element har to felt som beskrevet under.\n\nBruksrestriksjonstype\nFølgende verdier er tillatt for datasett med bruksrestriksjoner:\n\nSletting/anonymisering\n🌐 DELETION_ANONYMIZATION\nDatasettet er hentet inn til SSB med forutsetning om at opplysningene skal slettes eller anonymiseres innen et bestemt tidspunkt. F.eks. har samtykkebaserte surveydata alltid krav om anonymisering eller sletting innen et gitt tidspunkt.\n\n\nBehandlingsbegrensninger\n🌐 PROCESS_LIMITATIONS\nDatasettet er hentet inn til SSB med forutsetning om at opplysningene kun prosesseres på en forhåndsdefinert og begrenset måte. Et datasett kan f.eks. være hentet inn med begrensninger knyttet til hvilke andre data den kan kobles sammen med.\n\n\nSekundærbruksrestriksjoner\n🌐 SECONDARY_USE_RESTRICTIONS\nDatasettet er hentet inn under forutsetning av at opplysningene utelukkende benyttes av SSB til utvikling, utarbeiding eller formidling av offisiell statistikk. SSB kan ikke gi tilgang til opplysningene for utarbeiding av statistiske resultater og analyser, herunder forskning, jf. statikkloven § 14..\n\n\n\n\nBruksrestriksjonsdato\n🌐 use_restriction_date\nAngis kun dersom bruksrestriksjonen har en “tiltaksdato”. En tiltaksdato kan f.eks. være at et datasett skal slettes eller anonymiseres på en gitt dato. Noen datasett med Bruksrestriksjon vil ikke ha en slik dato, f.eks. vil en behandlingsbegrensning normalt være permanent/tidsuavhengig, og da skal ikke dette feltet fylles ut. I json-utskriften vil verdien i dette feltet da vises som null.\nDato må være på formatet YYYY-MM-DD iht ISO 8601 Date and time format.\n\n\n\n\n\n\nTipEksempler\n\n\n\n\n\nEksempel 1:\n\n2024-12-31\n\n\n\n\n\n\n\n\nDatatilstand\n🌐 dataset_state\nklargjorte data ✅ | statistikkdata ✅ | utdata ✅\nUtledes automatisk fra filstien hvis datasettet som dokumenteres er lagret iht Navnestandarden, men kan overstyres. Datasettets datatilstand er en av følgende:\n\nKILDEDATA\n🌐 SOURCE_DATA\n\n\nINNDATA\n🌐 INPUT_DATA\n\n\nKLARGJORTE DATA\n🌐 PROCESSED_DATA\n\n\nSTATISTIKK\n🌐 STATISTICS\n\n\nUTDATA\n🌐 OUTPUT_DATA\n\n\n\n\nStatus\n🌐 dataset_status\nklargjorte data ✅ | statistikkdata ✅ | utdata ✅\nAngir hvor metadataene er i livssyklusen. Følgende kategorier kan velges:\n\nUtkast\n🌐 DRAFT\nArbeid med data og metadata pågår, dvs. ikke delt internt eller eksternt.\n\n\nIntern\n🌐 INTERNAL\nMetadata er godkjent for intern bruk, men data med verdivurdering “skjermet” kan kun brukes/deles med ansatte med tjenstlig behov for tilgang\n\n\nEkstern\n🌐 EXTERNAL\nMetadata er godkjent for både intern og eksternt bruk, men data kan kun deles med alle eksterne hvis datatilstanden er “utdata” (verdivurdering=åpen). Andre datatilstander kan deles f.eks. med forskere etter søknad og godkjenning.\n\n\nUtgått\n🌐 DEPRECATED\nUtgått, avsluttet eller erstattet av noe annet.\n\n\n\n\nPopulasjon\n🌐 population_description\nklargjorte data ✅ | statistikkdata ✅ | utdata ✅\nAngir populasjonen som datasettet dekker(fritekst).\n\n\n\n\n\n\nTipEksempler\n\n\n\n\n\nEksempel 1:\n\nAlle bosatte personer\n\nEksempel 2:\n\nAlle studenter ved universitet og høgskole\n\nEksempel 3:\n\nAlle aktive bedrifter innenfor bergverksdrift, olje- og gassutvinning, industri og kraftforsyning\n\nEksempel 4:\n\nAlle museer som er åpne for publikum, og som har minst et fast lønnet årsverk\n\nEksempel 5:\n\nAlle personer bosatt i Norge som har hatt ett eller flere sykehusopphold eller behandling\n\nEksempel 6:\n\nAlle verdipapirfond som har konsesjon fra Finanstilsynet\n\n\n\n\nDersom noen av variablene i datasettet har en annen populasjon, er dette dokumentert under Populasjon for den aktuelle variabelen.\n\n\n\nVersjon\n🌐 version\nklargjorte data ✅ | statistikkdata ✅ | utdata ✅\nUtledes automatisk dersom datasettet følger navnestandarden. Dersom datasettet ikke følger navnestandarden (det er f.eks. ikke et krav at kildedata skal gjøre det), settes versjonsnummeret manuelt. Les mer om versjoner i Dapla-manualen.\n\n\n\nVersjonsbeskrivelse\n🌐 version_description\nklargjorte data ✅ | statistikkdata ✅ | utdata ✅\nKort beskrivelse av årsaken til at en ny versjon ble laget. For versjon 1 kan en bare skrive Opprinnelig versjon.\n\n\n\nInneholder data f.o.m.\n🌐 contains_data_from\nklargjorte data ✅ | statistikkdata ✅ | utdata ✅\nUtledes automatisk dersom datasettet følger navnestandarden. Dersom datasettet ikke følger navnestandarden (det er f.eks. ikke et krav at kildedata skal gjøre det), settes denne datoen manuelt. Dersom variablene i datasettet inneholder data med ulike startdatoer, settes den eldste datoen her. Under variabler-fanen kan en sette korrekt Inneholder data f.o.m. for variabler som avviker fra datoen som settes her.\nDato må være på formatet YYYY-MM-DD iht ISO 8601 Date and time format.\n\n\n\nInneholder data t.o.m.\n🌐 contains_data_until\nklargjorte data ✅ | statistikkdata ✅ | utdata ✅\nUtledes automatisk dersom datasettet følger navnestandarden. Dersom datasettet ikke følger navnestandarden (det er f.eks. ikke et krav at kildedata skal gjøre det), settes denne datoen manuelt. Dersom variablene i datasettet inneholder data med ulike sluttdatoer, settes den nyeste datoen her. Under variabler-fanen kan en sette korrekt «Inneholder data t.o.m.» for variabler som avviker fra datoen som settes her.\nDato må være på formatet YYYY-MM-DD iht ISO 8601 Date and time format.\n\n\n\nStatistikkområdet\n🌐 subject_field\nklargjorte data ✅ | statistikkdata ✅ | utdata ✅\nAngir det primære statistikkområdet som datasettet tilhører.\nGyldige verdier er definert i en Klass kodeliste.\n\n\n\nNøkkelord\n🌐 keyword\nklargjorte data (valgfritt) | statistikkdata (valgfritt) | utdata (valgfritt)\nBeskriver datasettet vha. nøkkelord. Disse kan brukes i søk. Nøkkelordene må legges inn som en kommaseparert streng.\n\n\n\n\n\n\nTipEksempler\n\n\n\n\n\nEksempel 1:\n\nbefolkning, skatt, arbeidsledighet\n\n\n\n\n\n\n\nGeografisk dekningsområde\n🌐 spatial_coverage_description\nklargjorte data ✅ | statistikkdata ✅ | utdata ✅\nOppgi datasettets geografiske dekningsområde. Norge er satt som standard.",
    "crumbs": [
      "Manual",
      "Metadata",
      "Datadoc",
      "Datamodell"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-model.html#variabler",
    "href": "statistikkere/datadoc-model.html#variabler",
    "title": "Datamodell",
    "section": "Variabler",
    "text": "Variabler\n\nNavn\n🌐 name\nklargjorte data ✅ | statistikkdata ✅ | utdata ✅\nAngir navnet til variabelen. Navnet skal være forståelig for mennesker (ikke kortnavn/teknisk navn) slik at det er søkbart. Navnet skal fylles ut på bokmål eller nynorsk.\nEt eksempel på navn er Personidentifikator. Det tilhørende kortnavnet (navnet i datasettet) vil være pers_id.\nDe fleste variabler skal dokumenters i Vardef, og dermed kunne pekes til fra Datadoc-editor. Men hvis datasettet inneholder noen variabler som kun brukes i dette datasettet, og dermed ikke skal gjenbrukes, trenger de ikke å dokumenteres i Vardef . Det kan f.eks. dreie seg om variabler som brukes i en spesiell beregning eller kontroll. Disse dokumenteres da i Datadoc-editor. Navnet skal dokumenteres i dette feltet, mens definisjonen og eventuelt tilhørende kodeverk skal dokumenteres hhv. i feltet Kommentar og Kodeverkets URI.\n\n\n\n\n\n\nNoteSammenkobling med Vardef\n\n\n\nNår Vardef kommer i produksjon må Navn-feltet bare fylles ut dersom Definisjons-URI-feltet ikke er utfylt. Dersom det er utfylt, vil navnefeltet automatisk fylles med navnet til Vardef-variabelen som det lenkes til. Det er også mulig å endre Vardef-navnet i Datadoc-editor dersom det er relevant.\n\n\n\n\n\nDefinisjons-URI\n🌐 definition_uri\nklargjorte data ⚠️ | statistikkdata ⚠️ | utdata ⚠️\nAngir lenken til relevant variabel(definisjon) i Vardef. For variabler som ikke er definert i Vardef skal definisjonen til variabelen dokumenteres under feltet Kommentar i Datadoc.\n\n\n\n\n\n\nImportantIkke obligatorisk enda\n\n\n\nSelv om Vardef er i produksjon på Dapla, så vil det i 2025 foregå arbeid med å migrere variabeldefinisjoner fra Vardok til Vardef. I denne perioden er det ikke et krav å fylle inn dette feltet.\n\n\n\n\n\nEr personopplysning\n🌐 is_personal_data\nklargjorte data ✅\nAngir om variabelen er en personopplysning eller ikke. Personopplysninger er alle opplysninger som entydig kan knyttes til en fysisk person (f.eks. fødselsnummer, ulike adresser og bankkontonummer). Se flere eksempler på personopplysninger i liste laget av PAPIS-prosjektet:\nPAPIS - Variabelliste.docx\nNæringsdata og enkeltpersonforetak (ENK) blir ikke regnet som personopplysninger.\n\n\n\nEnhetstype\n🌐 unit_type\nklargjorte data ✅ | statistikkdata ✅ | utdata ✅\nAngir enhetstypen («objektet») som variabelen inneholder informasjon om. Eksempler på enhetstyper er «person», «foretak» og «bruksenhet».\nI noen tilfeller vil dataene være aggregerte allerede når de kommer til SSB. Da vil det ikke være logisk å snakke om enhetstyper, og en kan velge kategorien «aggregert» som verdi i Enhetstype-feltet. Denne kategorien vil også ofte være det aktuelle valget for datatilstandene statistikk og utdata da disse ofte vil bestå av aggregerte data. Et unntak her vil være Kostra, som også har aggregerte data, men der disse er aggregert på kommune- eller fylkes nivå, og det vil da være naturlig å bruke enhetstypen «kommune(forvaltning)» eller «fylke(forvaltning)».\nKodeliste i Klass for tillatte verdier.\n\n\n\nMåleenhet\n🌐 measurement_unit\nklargjorte data ⚠️ | statistikkdata ⚠️ | utdata ⚠️\nAngir måleenhet dersom variabelen er kvantitativ2, f.eks. kroner eller tonn.\nKlass-kodeliste for tillatte måleeneheter.\n\n\n\nMultiplikasjonsfaktor\n🌐 multiplication_factor\nklargjorte data (valgfritt) | statistikkdata (valgfritt) | utdata (valgfritt)\nAngir multiplikasjonsfaktor der denne brukes sammen med måleenheten, f.eks. hvis det er store tall i datasettet. En kan f.eks. velge multiplikasjonsfaktor 1000, og måleenhet «kroner», slik at verdiene vises i 1000 kroner.\n\n\n\nVariabelens rolle\n🌐 variable_role\nklargjorte data ✅ | statistikkdata ✅ | utdata ✅\nAngir variabelens rolle i datasettet. Gyldige verdier/roller er:\n\nIDENTIFIKATOR\n🌐 IDENTIFIER\nIdentifiserer de ulike enhetene i et datasett, f.eks. fødselsnummer som identifiserer personer og organisasjonsnummer som identifiserer foretak.\n\n\nMÅLEVARIABEL\n🌐 MEASURE\nBeskriver det vi måler, dvs. egenskaper som sivilstand eller omsetning.\n\n\nSTARTTID\n🌐 START_TIME\nBeskriver startdato for hendelser som har et forløp, eller måletidspunkt for tverrsnittdata.\n\n\nSTOPPTID\n🌐 STOP_TIME\nBeskriver stoppdato for hendelser som har et forløp.\n\n\nATTRIBUTT\n🌐 ATTRIBUTE\nBenyttes dersom en ønsker å utvide datasettet med informasjon knyttet til gitte variabler, f.eks. vedrørende datakvalitet eller editering.\n\n\n\n\nKodeverkets URI\n🌐 classification_uri\nklargjorte data ⚠️ | statistikkdata ⚠️ | utdata ⚠️\nAngir lenke (URI) til gyldig kodeverk (klassifikasjon eller kodeliste) i Klass. Dette feltet vil sjelden benyttes siden variabelen i all hovedsak vil knyttes til tilhørende kodeverk via relevant variabeldefinisjon i Vardef. Unntaksvis kan den imidlertid knyttes direkte til Klass via dette feltet i tilfeller der variabelen ikke defineres i Vardef (se nærmere info under feltet «Navn»). Via dette feltet kan en også lenke til en variant av et kodeverk dersom en ønsker å spesifisere kodeverket som er knyttet til variabeldefinisjonen i Vardef (eksempelvis variabelen “næring” som i Vardef er knyttet til Standard for næringsgruppering (SN). For variabelen i datasettet kan en da f.eks. lenke til en variant av SN som kun inkluderer næringene som er aktuelle for egen statistikk, hvis dette er ønskelig).\n\n\n\nKommentar\n🌐 comment\nklargjorte data (valgfritt) | statistikkdata (valgfritt) | utdata (valgfritt)\nBeskriver to ulike situasjoner.\n\nDette feltet brukes vanligvis til å legge inn ytterligere informasjon om en variabel, f.eks. dersom en ønsker å utdype definisjonen i Vardef-variabelen en har lenket til.\nFeltet skal brukes i de sjeldne tilfellene der en variabel ikke må dokumenteres i Vardef (da er feltet ikke valgfritt). En kan nemlig unnlate å dokumentere en variabel i Vardef dersom den kun brukes i ett dataset (se mer info under Navn). I et slikt tilfelle må variabelen defineres i Kommentar-feltet. (NB variabler som kun brukes i ett datasett, kan godt dokumenters i Vardef i stedet for i dette kommentar-feltet dersom en ønsker det).\n\n\n\n\nDatakilde\n🌐 data_source\nklargjorte data ✅ | statistikkdata ✅ | utdata ✅\nAngir kilden til dataene (på etats-/organisasjonsnivå). Denne velges fra menyen.\nTillate verdier er definert i en Klass kodeliste.\n\n\n\nTemporalitetstype\n🌐 temporality_type\nklargjorte data ✅ | statistikkdata ✅ | utdata ✅\nAngir tidsdimensjonen i variabelen. Følgende verdier er tillatt:\n\nFast\n🌐 FIXED\nData med verdier som ikke endres over tid, f.eks. fødselsdato.\n\n\nTverrsnitt\n🌐 STATUS\n«Tverrsnitt» er data som er målt på et bestemt tidspunkt.\n\n\nAkkumulert\n🌐 ACCUMULATED\ner data som er samlet over en viss tidsperiode, f.eks. inntekt gjennom et år.\n\n\nHendelse/forløp\n🌐 EVENT\n«Hendelse/forløp» registrerer tidspunkt og tidsperiode for ulike hendelser/tilstander, f.eks. (skifte av) bosted.\n\n\n\n\nPopulasjon\n🌐 population_description\nklargjorte data ✅ | statistikkdata ✅ | utdata ✅\nAngis vanligvis på datasettnivå, men dersom datasettet består av variabler med ulike populasjoner, kan populasjonen dokumenteres på variabelnivå i dette feltet.\n\n\n\n\n\n\nTipEksempler\n\n\n\n\n\nEksempel 1:\n\nAlle (bosatte) personer i Norge per 31.12.2024\n\nEksempel 2:\n\nAlle foretak i Oslo per 01.03.2025\n\n\n\n\n\n\n\nFormat\n🌐 format\nklargjorte data (valgfritt) | statistikkdata (valgfritt) | utdata (valgfritt)\nAngir en ytterligere presisering av datatype i tilfellene der det er relevant. Kan inneholde verdienes format (fysisk format eller regulært uttrykk) i maskinlesbar form i forbindelse med validering, f.eks. ISO 8601 som datoformat.\n\n\n\nInneholder data f.o.m.\n🌐 contains_data_from\nklargjorte data ✅ | statistikkdata ✅ | utdata ✅\nAngis vanligvis på datasettenivå, men dersom variablene i datasettet inneholder data med ulike startdatoer, kan startdato dokumenteres på variabelnivå i dette feltet.\nDato må være på formatet YYYY-MM-DD iht ISO 8601 Date and time format.\n\n\n\nInneholder data t.o.m.\n🌐 contains_data_until\nklargjorte data ✅ | statistikkdata ✅ | utdata ✅\nAngis vanligvis på datasettenivå, men dersom variablene i datasettet inneholder data med ulike sluttdatoer, kan sluttdato dokumenteres på variabelnivå i dette feltet.\nDato må være på formatet YYYY-MM-DD iht ISO 8601 Date and time format.\n\n\n\nDatatype\n🌐 data_type\nklargjorte data ✅ | statistikkdata ✅ | utdata ✅\nAngir variabelens Datadoc-datatype. Utledes automatisk basert på PyArrow-datatype når Parquet-filen blir lest inn av Datadoc. Den som dokumenterer kan manuelt endre den utledede Datadoc-dataypen. Følgende er verdier er tillat:\n\nTEKST\n🌐 STRING\nDatadoc-datatype STRING utledes automatisk dersom variabelen har en av de følgende PyArrow-datatypene: | string | large_string | bytes |\n\n\nHELTALL\n🌐 INTEGER\nDatadoc-datatype INTEGER utledes automatisk dersom variabelen har en av de følgende PyArrow-datatypene: | int8 | int16 | int32 | int64 | uint8 | uint16 | uint32 | uint64 |\n\n\nDESIMALTALL\n🌐 FLOAT\nDatadoc-datatype FLOAT utledes automatisk dersom variabelen har en av de følgende PyArrow-datatypene: | float16 | float32 | float64 |\n\n\nDATOTID\n🌐 DATETIME\nDatadoc-datatype DATETIME utledes automatisk dersom variabelen har en av de følgende PyArrow-datatypene: | timestamp('s') | timestamp('ms') | timestamp('us') | timestamp('ns') | date32() | date64() | time32('s') |\n\n\nBOOLSK\n🌐 BOOLEAN\nDatadoc-datatype BOOLEAN utledes automatisk dersom variabelen har følgende PyArrow-datatype: | bool |\n\n\n\n\nDataelementsti\n🌐 data_element_path\nklargjorte data ⚠️ | statistikkdata ⚠️ | utdata ⚠️\nAngis kun for hierarkiske datasett (JSON) hvor det er nødvendig å oppgi sti til dataelementet. “dot-notasjon” (JsonPath- lignende syntaks) brukes til å peke til variabelen (dataelementet). Dette er en generisk/teknologinøytral måte å peke til elementer i både JSON, XML og andre hierarkiske datastrukturer.\n\n\n\nUgyldige verdier\n🌐 invalid_value_description\nklargjorte data (valgfritt) | statistikkdata (valgfritt) | utdata (valgfritt)\nBeskriver ugyldige verdier som inngår i variabelen (fritekstfelt). Et eksempel kan være variabelen «organisasjonsnummer» hvis en vet at noen av verdiene knyttet til enpersonsforetak mangler et siffer i fødselsnummeret eller egentlig er passnummer.\n\n\n\nKortnavn\n🌐 short_name\nklargjorte data ✅ | statistikkdata ✅ | utdata ✅\nUtledes automatisk. Kortnavnet vil være det samme som navnet som brukes på variabelen i datasettet.\n\n\n\nId\n🌐 id\nklargjorte data ✅ | statistikkdata ✅ | utdata ✅\nUtledes automatisk. Dette er en unik SSB-identifikator for variabelen, og denne maskingenereres av Datadoc.\n\n\n\nPseudonymiseringstidspunkt\n🌐 pseudonymization_time\nklargjorte data ⚠️\nAngir tidspunkt for når variabelen ble pseudonymisert.\nDato må være på formatet YYYY-MM-DD iht ISO 8601 Date and time format.\n\n\n\nStabil identifikator type\n🌐 stable_identifier_type\nklargjorte data ⚠️\nAngir om variabelen er transformert til en stabil identifikator før pseudonymisering, og hvilken identifikator som ble benyttet. F.eks. er en vanlig transformasjon i SSB at man konverterer fødselsnummer til SNR før pseudosnymisering.\n\n\n\nStabil identifikator versjon\n🌐 stable_identifier_version\nklargjorte data ⚠️\nAngir hvilken versjon av stabil identifikator som variabelen ble transformert til før pseudonymisering.\n\n\n\nKrypteringsalgoritme\n🌐 encryption_algorithm\nklargjorte data ⚠️\nAngir krypteringsalgoritmen som ble benyttet for pseudonymisering. F.eks. TINK-FPE eller DAEAD.\n\n\n\nKrypteringsnøkkel referanse\n🌐 encryption_key_reference\nklargjorte data ⚠️\nAngir navn eller referanse til krypteringsnøkkelen som er benyttet for pseudonymisering.\n\n\n\nKrypteringsalgoritme-parametre\n🌐 encryption_algorithm_parameters\nklargjorte data ⚠️\nAngir eventuelle krypteringsalgoritme-parametre som ble benyttet utover encryption_key_reference.",
    "crumbs": [
      "Manual",
      "Metadata",
      "Datadoc",
      "Datamodell"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-model.html#footnotes",
    "href": "statistikkere/datadoc-model.html#footnotes",
    "title": "Datamodell",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nKvantitative variabler har verdier som er tall, i motsetning til kvalitative variabler som henter sine verdier fra en klassifikasjon eller kodeliste. Inntekt og alder er eksempler på kvantitative variabler.↩︎\nKvantitative variabler har verdier som er tall. Inntekt og Alder er eksempler på kvantitative variabler.↩︎",
    "crumbs": [
      "Manual",
      "Metadata",
      "Datadoc",
      "Datamodell"
    ]
  },
  {
    "objectID": "statistikkere/datadoc.html",
    "href": "statistikkere/datadoc.html",
    "title": "Datadoc",
    "section": "",
    "text": "Datadoc er SSBs system for dokumentasjon av datasett på Dapla. Hensikten er at alle datasett i de obligatoriske datatilstandene (klargjorte data, statistikk og utdata) skal dokumenteres ett sted, og informasjonen skal knyttes tett til tilhørende datasett. Den obligatoriske datatilstanden kildedata skal også dokumenters i Datadoc, men her vil det kreves mindre detaljert informasjon enn for de tre andre tilstandene. Kravene til dokumentasjon er større for klargjorte data, statistikk og utdata siden dette er datatilstander som skal kunne deles med andre1.\nI Datadoc skal både informasjon om alle kolonner, og informasjon om datasettet som helhet, dokumenteres. Informasjon om datasettet kan f.eks. være en beskrivelse av hva datasettet inneholder, hvilket Dapla-team som eier det, om det inneholder personopplysninger, og om det er bruksrestriksjoner knyttet til dataene. I tillegg vil en del felter bli maskingenerert, f.eks. identifikator, filsti og hvilke datoer datasettet inneholder data fra og til. Hele modellen for datasett finnes her: DataDoc - Krav til dokumentasjon av datasett på Dapla - Metadata på DAPLA - Confluence.\nInformasjon om enkeltkolonner (ofte omtalt som variabelforekomster) i datasettet skal også dokumenteres. Denne informasjonen skal bl.a. inneholde en beskrivelse av variabelen. Dette skal primært gjøres ved at en lenker til tilhørende variabeldefinisjon i Vardef. I tillegg til beskrivelsen, skal bl.a. måleenhet (hvis det er en kvantitativ variabel), datatype og (dersom variabelen er en personopplysning) om verdiene er pseudonymisert, dokumenteres. Hele modellen for variabelforekomster finne her: Variabelforekomst - Metadata på DAPLA - Confluence.\nFigur 1 viser et datasett der en av variabelforekomstene er sivst. Den er lenket opp til definisjonen av sivilstand i Vardef, som igjen er lenket opp til kodeverket som beskriver kategoriene i «Standard for sivilstand» i Klass.",
    "crumbs": [
      "Manual",
      "Metadata",
      "Datadoc"
    ]
  },
  {
    "objectID": "statistikkere/datadoc.html#footnotes",
    "href": "statistikkere/datadoc.html#footnotes",
    "title": "Datadoc",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nUtdata er åpne data, mens klargjorte data og statistikkdata kan deles med brukere som har rett på tilgang.↩︎",
    "crumbs": [
      "Manual",
      "Metadata",
      "Datadoc"
    ]
  },
  {
    "objectID": "statistikkere/kildomaten.html",
    "href": "statistikkere/kildomaten.html",
    "title": "Kildomaten",
    "section": "",
    "text": "Kildomaten er en tjeneste for å automatisere deler av overgangen fra kildedata til inndata.\nTjenesten lar statistikkere kjøre sine egne skript automatisk på alle nye filer i kildedatabøtta og skrive resultatet til produktbøtta. Formålet med tjenesten er minimere behovet for tilgang til kildedata samtidig som teamet selv bestemmer hvordan transformasjonn til inndata skal foregå. Statistikkproduksjon kan da starte i en tilstand der dataminimering og pseudonymisering allerede er gjennomført.\nAnbefalt prosessering som bør skje i Kildomaten er:\nEn forutestning for å bruke kildomaten er å ha et Dapla-team.",
    "crumbs": [
      "Manual",
      "Datatjenester"
    ]
  },
  {
    "objectID": "statistikkere/kildomaten.html#forberedelser",
    "href": "statistikkere/kildomaten.html#forberedelser",
    "title": "Kildomaten",
    "section": "Forberedelser",
    "text": "Forberedelser\nFør et Dapla-team kan ta i bruk kildomaten må tjenesten være aktivert for teamet. Som standard får alle statistikkteam dette skrudd på i prod-miljøet som opprettes for teamet. Ønsker du å aktivere kildomaten i test-miljøet kan dette gjøres selvbetjent som en feature. Alternativt kan man opprette en Kundeservice-sak og be om å hjelp til dette.",
    "crumbs": [
      "Manual",
      "Datatjenester"
    ]
  },
  {
    "objectID": "statistikkere/kildomaten.html#sette-opp-tjenesten",
    "href": "statistikkere/kildomaten.html#sette-opp-tjenesten",
    "title": "Kildomaten",
    "section": "Sette opp tjenesten",
    "text": "Sette opp tjenesten\nI denne delen bryter vi ned prosessen med å sette opp kildomaten i de stegene vi mener er hensiktsmessige. Senere i artikkelen forklarer vi hvordan man setter opp flere kilder og hvordan man vedlikeholder tjenesten.\nHusk at alle på teamet kan gjøre det meste av arbeidet her, men det er data-admins som må godkjenne at tjenesten rulles ut1.\n\nKlone IaC-repoet\nOppsett av kildomaten gjøres i teamets IaC-repo2. Når vi skal sette opp kildomaten må vi gjøre endringer i teamets IaC-repo. Man finner teamets IaC-repo ved gå inn på SSBs GitHub-organisasjon og søke etter repoet som heter &lt;teamnavn&gt;-iac. Når du har funnet repoet så kan du gjøre følgende:\n\nKlon teamets IaC-repo git clone &lt;repo-url&gt;\nOpprett en ny branch: git switch -c add-kildomaten-source\n\n\n\nMappestruktur i IaC-repo\nFor at kildomaten skal fungere må det opprettes en bestemt mappestruktur i IaC-repoet til teamet.F.eks. vil mappestrukturen for prod-miljøet se slik ut for team dapla-example:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n├── automation/\n│   └── source-data/\n│       ├── dapla-example-prod/\n│           └── README.md\n│\n│...\n\nSkal du sette opp kildomaten i prod-miljøet kan du følge oppskriften som kommer senere i kapitlet uten å gjøre noe mer enda.\nSkal du også bruke kildomaten i test-miljøet må du opprette en ny mappe og lage en PR i IaC-repoet til teamet. Da vil strukturen se slik ut:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n├── automation/\n│   └── source-data/\n│       ├── dapla-example-prod/\n│       │    └── README.md\n│       ├── dapla-example-test/\n│...\n\nI mappestrukturen over har vi klargjort den grunnleggende mappestrukturen for å ta i bruk kildomaten i prod- og test-miljøet. Neste steg blir å legge de ulike kildene som egne mapper under dapla-example-prod og dapla-example-test. Det viser vi i neste avsnitt.\n\n\nFlere kilder\nKildomaten lar deg prosessere ulike filstier i kildebøtta med ulike python-script. Dette refereres til som at Kildomaten har flere kilder. For å sette opp en kilde må man følge en definert mappestruktur i IaC-repoet der alle kildene ligger rett under &lt;teamnavn&gt;-prod- eller &lt;teamnavn&gt;-test-mappen. Du kan ikke ha undermapper under en kilde. Du velger selv navnet på kildene/mappene i IaC-repoet, og det vil være navnet på kildene i Kildomaten. Senere i kapitlet ser vi at vi må bruke navnet for trigge re-kjøring av kilder.\nUnder er et eksempel på hvordan det kan se ut for eksempel-teamet dapla-example:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n├── automation/\n│   └── source-data/\n│       ├── dapla-example-prod/\n│       │    └── altinn\n│       │    └── ameld\n│       ├── dapla-example-test/\n│       │    └── altinn\n│       │    └── ameld\n│       │    └── nudb\n│...\n\nI eksempelet over ser vi at det er mapper i IaC-repoet for kildene altinn og ameld for både test- og prod-miljøet. I tillegg har test-miljøet en mappe for kilden nudb. Hver av disse kildene kan kjøre et eget Pyton-script på alle filer som skrives til en gitt filsti som man definerer selv.\n\n\n\n\n\n\nImportantAlle kilder på samme nivå\n\n\n\nEt team kan sette opp forskjellige skript for forkskjellige kilder men kildomaten tillater bare ett nivå under automation/source-data-&lt;teamnavn&gt;-&lt;miljø&gt;/. Det vil si at du ikke kan ha undermapper under en kilde. Det er også slik at man alltid må opprette en mappe for en kilde, selv om du kun har en kilde. F.eks. kan man ikke droppe mappen altinn i eksempelet over.\n\n\n\n\nKonfigurasjon og skript\nNår mappestrukturen er opprettet kan man legge til konfigurasjonsfilen som beskriver hvilke filer som skal prosesseres, og python-skriptet med koden som skal prosessere filene. Det er to filer for hver datakilde:\n\nEn konfigurasjonsfil\nEt Python-skript\n\nHer ser du hvordan IaC-repoet ser ut med én kilde (altinn):\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n├── automation/\n│   └── source-data/\n│       ├── dapla-example-prod/\n│           └── altinn/\n│               ├── config.yaml\n│               └── process_source_data.py\n│...\n\n\nKonfigurasjonsfil\nKildomaten trigges ved at det oppstår nye filer i kildebøtta til teamet. Hvorvidt den skal trigges på alle filer, eller kun filer i en gitt undermappe, bestemmer brukeren ved å konfigurere tjenesten i config.yaml. Her kan du også angi hvor mye ressurser prosesseringen skal få.\nHvis vi fortsetter eksempelet vårt fra tidligere med dapla-example kan vi tenke oss at teamet ønsker at kildomaten skal trigges på alle filer som oppstår i kildebøtta under filstien: ssb-dapla-example-data-kilde-prod/ledstill/altinn/.\nFor å konfigurere tjenesten i kildomaten må vi legge til en fil som heter config.yaml under automation/source-data/dapla-example-prod/altinn/ slik som vist tidligere. I dette tilfellet vil den ha innholdet som vist til venstre under:\n\n\n\n\nconfig.yaml\n\nfolder_prefix: ledstill/altinn\nmemory_size: 1  # GiB\n\n\n\n\n\n\nssb-dapla-example-data-kilde-prod/\n\n├── ledstill/\n│   └── altinn/\n│   └── aordningen/\n├── sykefra/\n│   └── altinn/\n│   └── freg/\n│...\n\n\n\nMappestrukturen i kildebøtta er illustrert til høyre, mens config.yaml-fila til venstre viser hvordan vi angir at kildomaten kun skal trigges på nye filer som oppstår i undermappen ssb-dapla-example-data-kilde-prod/ledstill/altinn/. Vi bruker nøkkelen folder_prefix for å angi hvilken sti i kildebøtta som tjenesten skal trigges på. Nøkkelen memory_size lar brukeren angi hvor mye minne og CPU hver prosessering skal få. Vil du at alle filer skal trigge kildomaten? Da setter du folder_prefix til \"\".\n\n\n\n\n\n\nNoteHvor mye minne og CPU er mulig?\n\n\n\n\n\nSom standard får hver prosessering med kildomaten 1 CPU-kjerne og 512MB minne/RAM. Hvis man skal gjøre mye prossesering, eller filene som prosesseres er veldig store, kan man sette memory_size opp til max 32GB minne. Antall CPU-kjerner blir satt implisitt ut i fra valg av memory_size iht til begrensningene som Google setter for Cloud Run. Se de nøyaktige verdiene som blir satt her.\n\n\n\n\n\nPython-skript\n\n\n\n\n\n\nImportantHusk dette når du skriver skriptet ditt\n\n\n\n\nSkriptet ditt kommer til å bli kjørt på en-og-en fil.\nSkriptet ditt må skrive ut et unikt navn på filen som skal skrives til produktbøtta, ellers risikerer du at filer blir overskrevet.\nPython-pakkene som kan benyttes er definert her. Ved behov for andre pakker, ta kontakt med Kundeservice.\nBøttemontering gjelder ikke for kildomaten, så man må bruke pakken gcfs når man gjør filoperasjoner i kildomaten.\n\n\n\nPython-skriptet er der teamet angir hva slags kode som skal kjøre på hver fil som dukker opp i den angitte mappen i kildebøtta. For at dette skal være mulig må koden følge disse reglene:\n\nKoden må ligge i en fil som heter process_source_data.py.\nKoden må pakkes inn i en funksjon som heter main().\n\n\n\n\n\n\n\nImportantIkke bruk sys.exit eller lignende\n\n\n\nSkriptet ditt kjøres som en del av et større program, og bruk av exit-funksjoner vil hindre resten av Kildomaten i å virke som den skal og kan forårsake uprosesserte filer og uendelige omstarter. Hvis du har brukt sys.exit i skriptet ditt, vil dette oppdages i pull request-en og du vil bli hindret fra å rulle ut koden.\nHvis du vil stoppe prosesseringen pga. en feil bør du bruke exceptions. Kildomaten forstår da at noe har gått galt i prosesseringen og vil i tillegg varsle teamet på e-post dersom dette har blitt konfigurert.\n\n\nmain() er en funksjon med ett parameter som heter source_file som du alltid får av kildomaten når en fil blir prosessert. Når man skriver koden kan man derfor anta at dette parameteret blir gitt til funksjonen, og du kan benytte det inne i main-funksjonen.\nsource_file-parameteret gir main() en ferdig definert filsti til filen som skal prosesseres. For eksempel kan source_file ha verdien gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv hvis filen test20231010.csv dukker opp i ssb-dapla-example-data-kilde-prod/ledstill/altinn/.\nHvis vi fortsetter eksempelet fra tidligere ser mappen i IaC-repoet vårt slik ut nå:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\n├── automation/\n│   └── source-data/\n│       ├── dapla-example-prod/\n│           └── altinn/\n│               ├── config.yaml\n│               └── process_source_data.py\n│\n│...\n\nVi ser nå at filene config.yaml og process_source_data.py ligger i mappen automation/source-data/dapla-example-prod/ledstill/altinn/. Senere, når vi har rullet ut tjenesten, vil en ny fil i gs://ssb-dapla-exmaple-data-kilde-prod/ledstill/altinn/ trigge kildomaten og kjøre koden i process_source_data.py på filen.\nUnder ser du et eksempel på hvordan en vanlig kodesnutt kan konverteres til å kjøre i kildomaten:\n\n\n\n\nVanlig kode\n\nimport pandas as pd\n\n# Stien til filen\nsource_file = \"gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv\"\n\ndf = pd.read_csv(source_file)\n\n# Dataminimerer ved å velge kolonner\ndf2 = df[['col1', 'col2', 'col3']]\n\n# Skriver ut fil til produktbøtta\ndf2.to_parquet(\"gs://ssb-dapla-example-data-produkt-prod/test20231010.parquet\")\n\n\n\n\n\n\nKode i Kildomaten\n\nimport pandas as pd\n\ndef main(source_file):\n    df = pd.read_csv(source_file)\n\n# Dataminimerer ved å velge kolonner\n    df2 = df[['col1', 'col2', 'col3']]\n\n# Bytter ut \"kilde\" med \"produkt\" i filstien\n    new_path = source_file.replace(\"kilde\", \"produkt\")\n\n# Bytter ut \".csv\" med \".parquet\" i filstien\n    new_path2 = new_path.replace(\".csv\", \".parquet\")\n\n# Skriver ut fil til produktbøtta\n    df2.to_parquet(new_path2)\n\n\n\nDe to kodesnuttene over gir det samme resultatet, bare at koden til venstre kjøres som vanlig python-kode, mens koden til høyre kjøres i kildomaten. Som vi ser av koden til høyre så trenger vi aldri å hardkode inn filstier i Kildomaten. Tjenesten gir oss filstien, og vi kan bruke den til å skrive ut filen til produktbøtta.\nStrukturen på filene som skrives bør tenkes nøye gjennom når man automatiseres prosessering av data. Hvis vi ikke har unike filnavn eller stier kan du risikere at filer blir overskrevet. Typisk er dette allerede tenkt på når filer skrives til kildebøtta, så hvis man kopierer den strukturen, slik vi gjorde i eksempelet over, så vil det ikke være noe problem.\n\n\n\nTest koden\nFør man ruller ut koden i tjenesten er det greit å teste at alt fungerer som det skal i Dapla Lab. Hvis vi tar utgangspunkt i eksempelet over kan vi teste koden ved å kjøre følgende nederst i skriptet i en data-admins-tjeneste:\n\n\nprocess_source_data.py\n\nmain(\"gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv\")\n\nMerk at siden man i en data-admins-tjeneste kun har tilgang til kildebøtta, og ikke produktbøtta, så vil kjøringen feile når den prøver å skrive resultatet til en fil i produktbøtta.\nNår tjenesten er rullet ut vil det være dette som kjøres når en fil dukker opp i gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/. Ved å kjøre det manuelt på denne måten får vi sett at ting fungerer som det skal.\nHusk å fjerne kjøringen av koden før du ruller ut tjenesten.\n\n\n\n\n\n\nNotePseudonymisering kan ikke kjøres fra en IDE i prod-miljøet\n\n\n\nDu kan teste kode fra Jupyter og andre IDE-er i prod-miljøet på Dapla med mindre prosesseringen innebærer bruk av pseudonymisering. Grunnen til dette er at det ikke er ønskelig å gjøre det lett å se upseudonymisert og pseudonymisert data samtidig. Hvis man ønsker å teste prosesseringen av pseudo-tjenesten kan man gjøre med testdata i test-miljøet.\n\n\n\n\nRull ut tjenesten\nFor å rulle ut tjenesten gjør du følgende;\n\nPush branchen til GitHub og opprette en pull request.\nPull request må godkjennes av en data-admins på teamet.\n\n\n\n\nNår pull request er godkjent så sjekker du om alle tester, planer og utrullinger var vellykket, slik som vist i Figur 1.\nHvis alt er vellykket så kan du merge branchen inn i main-branchen.\n\n\n\n\n\n\n\n\n\nFigur 1: Suksessfulle tester på GitHub\n\n\n\n\n\nEtter at du har merget inn i main kan du følge med på utrullingen under Actions-fanen i repoet. Når den siste jobben lyser grønt er kildomaten rullet ut og klar til bruk.\n\n\n\n\n\n\nNoteVanlige feil ved utrulling\n\n\n\nFor å gi raskt tilbakemelding på noen mulige feilsituasjoner kjøres det enkel validering på config.yaml og process_source_data.py når en Pull request er opprettet. Følgende validering gjennomføres:\n\nHeter python-skriptet process_source_data.py?\nFinnes en config.yaml med gyldig folder_prefix:?\nBruker process_source_data.py biblioteker som ikke er installert i kildomaten?\nHar koden i process_source_data.py feil iht. Pyflakes?\nBruker du sys.exit i koden?\n\nDet kan også forekomme at Atlantis, verktøyet for å rulle ut endringer fra IaC-repoet til GCP, feiler. Da kan du prøve å skrive atlantis plan i kommentarfeltet til pull request-en, og testene vil kjøre på nytt. Hvis det fortsatt ikke fungerer kontakter man Dapla kundeservice.\n\n\n\n\nTest tjenesten\nNår du har rullet ut tjenesten kan du teste tjenesten ved at data-admins flytter en fil til den filstien kildomaten er satt opp for. I eksempelet vi har brukt tidligere, gjør du følgende: 1. Åpne en tjeneste i Dapla Lab med teamets data-admins-gruppe valgt i Data-menyen. 2. Kopier en fil over til filstien: gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/ 3. Sjekk om filen ble skrevet til ønsket mappe i produktbøtta.\nDu kan også sjekke logger og monitorere kildomaten.\n\n\nMonitorering og logging\nNår en kilde i kildomaten er satt opp og ruller ut kan tjenesten monitoreres i Google Cloud Console (GCC) ved å gjøre følgende:\n\nLogg deg inn med SSB-bruker på GCC.\nVelg standardprosjektet i prosjektvelgeren3.\nSøk opp Cloud Run i søkefeltet på toppen av siden og gå inn på siden.\nVelg Worker Pools i menyen til venstre.\n\nPå siden til Cloud Run Worker Pools vil du se en oversikt over alle kilder teamet har kjørende i kildomaten. De har formen source-&lt;kildenavn&gt;-processor. I eksempelet med team dapla-example tidligere, vil man da se en kilde som heter source-altinn-processor, siden mappen de opprettet under automation/source-data/dapla-example-prod/ i IaC-repoet heter altinn.\nTrykker man seg inn på hver enkelt kilde vil man kunne monitorere aktiviteten i kilden, og man kan trykke seg videre for å se loggene.\n\n\n\n\n\n\nNoteSjekke logger\n\n\n\nDet er anbefalt å se på Kildomaten-loggene i Logs Explorer. Det kan man enkelt gjøre ved å trykke på “View in Logs Explorer” som vist på bildet under:\n\n\n\nÅpne Kildomaten-loggene i Logs Explorer\n\n\n\n\n\nSjekk prosesseringsstatus\nKildomaten lar deg sjekke om filprosessering har feilet, blitt vellykket eller forstatt pågår. Kodeeksempelet fordrer at man har installert python-pakken google-cloud-storage.\n\n\nsjekk-status-kildomaten.py\n\nfrom google.cloud import storage\n\nteam = \"prisstat\" # skriv inn teamnavn\nenv = \"prod\"\nstatus = \"failed\" # let etter filer som har failed status. evt. done/processing for andre resultater\nsource_name = \"ra2900\" # refererer til denne mappen: https://github.com/statisticsnorway/prisstat-iac/tree/main/automation/source-data/prisstat-prod/ra2900\nsource_name = source_name.lower().replace(\"_\", \"-\")\nprefix = \"ra2900\"\n\n\nclient = storage.Client()\nbucket = client.bucket(f\"ssb-{team}-data-kilde-{env}\")\n\nresults = []\nfor blob in bucket.list_blobs(prefix=prefix):\n  if blob.metadata.get(f\"source-{source_name}-status\", \"\") == status:\n    results.append(blob.name)\n\nprint(results) # skriver ut filnavn for filer hvor prosessering feilet\n\n\n\n\nSkalering\nKildomaten er satt opp for å kunne prosessere hver kilde i opptil 100 parallelle instanser samtidig. Ta kontakt med Kundeservice hvis man opplever problemer med kapasiteten til Kildomaten.\n\n\nVarsling på e-post\nKildomaten tilbyr e-postvarsling til teamet når tjenesten feiler. Opprett en Kundeservice-sak for å få satt opp e-postvarsling for teamet ditt.\nVarslene sendes dersom det oppstår en feilmelding under prosesseringen. Det betyr at dersom du vil trigge et varsel fra koden din, så har du to muligheter:\n\n\nvarsling.py\n\n# 1. Bruk en exception.\n#   I dette tilfellet vil prosesseringen stanse og filen markeres som feilet.\ndef main(source_file):\n    raise Exception(\"Kunne ikke prosessere fil pga..\")\n\n# 2. Log en feilmelding manuelt\n#  I dette tilfellet vil ikke prosesseringen stanse. Hvis ingen flere feil (exceptions) skjer,\n#  så markeres filen som prosessert.\nimport logging\ndef main(source_file):\n  logging.error(\"Ikke-kritisk feil\")\n\nSom det er hintet til i koden så anbefales alternativ 1 for uhåndterbare feil hvor prosesseringen ikke kan fortsette, mens alternativ 2 passer bra for feil som ikke hindrer prosessering, men som man vil bli varlset om allikevel.\n\n\nFlere kilder\nMan kan sette opp så mange kilder man ønsker. Men når man setter det opp er det viktig å huske at alle kildene må spesifiseres rett under automation/source-data/&lt;teamnavn&gt;-&lt;miljø&gt;/. Her er et eksempel på hvordan team dapla-example har to kilder i kildomaten:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n├── automation/\n│   └── source-data/\n│       ├── dapla-example-prod/\n│           └── altinn/\n│               ├── config.yaml\n│               └── process_source_data.py\n│           └── ledstill/\n│               ├── config.yaml\n│               └── process_source_data.py\n│...\n\nI eksempelet over ser vi det er to kilder: altinn og ledstill. Hver kilde trigges på ulike filstier i kildebøtta, og python-koden som kjøres kan være ulik mellom kilder.\n\n\nTest-miljø\nI eksempelet som er brukt i denne artiklen er kildomaten satt opp i prodmiljøet til teamet. Man bør egentlig teste ut nye kilder i teamets test-miljø. Kildomaten er ikke satt opp i test-miljøet som standard, og derfor må det skrus på før man kan anvende det. Teamet kan gjøre det selv ved å følge denne beskrivelsen eller ta kontakt med kundeservice og få hjelp til dette.\nEn av de store fordelene med å sette opp kildomaten-kilder i test-miljøet før man gjør det i prod-miljøet er at tilgang til funksjonalitet i tjenester som pseudo-tjenesten er bredere. Det gjør det lettere for alle i teamet å utvikle koden som skal benyttes.\nNår man skal sette opp kildomaten i test-miljøet følger det samme oppskrift som vi har vist for prod-miljøet over. Den store forskjellen er at test-kilder skal legges under en egen mappe under automation/source-data/ i teamets IaC-repo. Eksempelet under med team dapla-example viser hvordan man kan sette opp kildene altinn og ledstill for både prod- og test-testmiljøet:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\n├── automation/\n│   └── source-data/\n│       ├── dapla-example-prod/\n│       │       ├── altinn/\n│       │       │       ├── config.yaml\n│       │       │       └── process_source_data.py\n│       │       └── ledstill/\n│       │               ├── config.yaml\n│       │               └── process_source_data.py\n│       ├── dapla-example-test/\n│               ├── altinn/\n│               │       ├── config.yaml\n│               │       └── process_source_data.py\n│               └── ledstill/\n│                       ├── config.yaml\n│                       └── process_source_data.py\n│...\n\nSom vi ser av mappestrukturen over er det to mapper under automation/source-data:\n\ndapla-example-prod\ndapla-example-test\n\nDisse to mappene angir om det er henholdsvis prod- eller test-miljøet vi setter opp kilder for.\n\n\nTrigge kilde manuelt\nKildomaten er bygget for å trigge på nye filer som oppstår i en gitt filsti. Men noen ganger er det nødvendig å trigge kjøring av filer for en gitt kilde på nytt. Dette kan gjøres med en funksjon i Python-pakken dapla-toolbelt.\n\n\n\n\n\n\nCautionLogg inn som data-admins i Dapla Lab\n\n\n\nTrigging av kilder manuelt kan kun gjøres av data-admins på Dapla Lab. På Dapla Lab må man representere en av gruppene når man logger seg inn.\n\n\nFør du kan gjøre dette trenger du følgende informasjon:\n\nproject-id for standardprosjektet. Slik finner du prosjekt-id. Merk at dette ikke er kilde-prosjektet!\nfolder_prefix er mappen i kilde-bøtten som du ønsker at koden skal trigges på. Dette fungerer likt som tidligere forklart for config.yaml, men her har du også mulighet til å kunne trigge prosesseringen på en undermappe av stien som var satt i kildens config.yaml.\nsource_name er navnet på mappen i IaC-repoet hvor kilden du skal trigge er definert. Navnet på kilden i eksempelet med team dapla-example var altinn.\n\nHvis man under folder_prefix legger inn en hel filsti med filnavn og filending, så trigges Kildomaten kun på denne ene fila.\nHer er et kodeeksempel som viser hvordan man kan trigge prossesering av alle filer for kilde\n\n\nnotebook\n\nfrom dapla import trigger_source_data_processing\n\nproject_id = \"dapla-example-p\"\nsource_name = \"altinn\"\nfolder_prefix = \"ledstill/altinn/ra0678\"\n\ntrigger_source_data_processing(project_id, source_name, folder_prefix, kuben=True)\n\nI eksempelet over trigger vi scriptet som er satt opp for kilden altinn til å kjøre på alle undermapper av ssb-dapla-example-data-kilde-prod/ledstill/altinn/ra0678/.\nHer er et kodeeksempel som viser hvordan man kan trigge prosessering av spesifike filer i en kilde\n\n\nnotebook\n\nfrom dapla import trigger_source_data_processing\n\nproject_id = \"dapla-example-p\"\nsource_name = \"altinn\"\nfile_paths = [\n    'ledstill/altin/ra0678/data1.parquet',\n    'ledstill/altin/ra0678/data2.parquet',\n    'ledstill/altin/ra0678/data3.parquet'\n]\nfor file in file_paths:\n    trigger_source_data_processing(project_id, source_name, file, kuben=True)\n\nI eksempelet over trigger vi scriptet som er satt opp for kilden altinn til å kjøre på de tre spesifike filene ssb-dapla-example-data-kilde-prod/ledstill/altinn/ra0678/data1.parquet, ssb-dapla-example-data-kilde-prod/ledstill/altinn/ra0678/data2.parquet og ssb-dapla-example-data-kilde-prod/ledstill/altinn/ra0678/data3.parquet.",
    "crumbs": [
      "Manual",
      "Datatjenester"
    ]
  },
  {
    "objectID": "statistikkere/kildomaten.html#vedlikehold",
    "href": "statistikkere/kildomaten.html#vedlikehold",
    "title": "Kildomaten",
    "section": "Vedlikehold",
    "text": "Vedlikehold\nNår tjenesten er rullet ut så vil den kjøre automatisk på alle filer som dukker opp i filsti i kildebøtta. Etter hvert vil det være behov for å endre på skript, endre filstier som skal trigge tjenesten, eller prosessere alle filer på nytt. I denne delen forklarer vi hvordan du går frem for å gjøre dette.\n\nEndre skript\nAlle på team kan endre på skriptet, men det er data-admins som må godkjenne endringene før de blir rullet ut. For å endre skriptet gjør du følgende:\n\nKlon repoet.\nGjør endringene du ønsker i en branch.\nPush opp endringene til GitHub og opprett en pull request.\nFå en data-admins på teamet til å godkjenne endringene.\nNår endringene er godkjent så kan du merge inn i main-branchen.\n\nEn endring i Python-skriptet krever ikke at tjenesten rulles ut på nytt. Derfor er det ikke like mange tester og kjøringer som gjøres som når man oppretter en helt ny kilde.\n\n\nEndre config.yaml\nAlle på teamet kan gjøre endringer i config.yaml, men det er data-admins som må godkjenne endringene før de blir rullet ut. For å endre config.yaml gjør du følgende:\n\nKlon repoet.\nGjør endringene du ønsker i en branch.\nPush opp endringene til GitHub og opprett en pull request.\nFå en data-admins på teamet til å godkjenne endringene.\nNår endringene er godkjent så kan du merge inn i main-branchen.\n\nEn endring i config.yaml krever at tjenesten rulles ut på nytt og tar derfor litt mer tid enn en endring i Python-skriptet.\n\n\nMetadata\nFor å hindre dobbeltprosessering og forbedre ytelse bruker Kildomaten Cloud Storage sin innebygde metadatafunksjonalitet for å sette noen markører på kildedatafilene. Disse metadatane kan du hente ut f.eks. via dette scriptet:\n\n\nsjekk-metadata.py\n\nfrom google.cloud import storage\n\nteam = \"dapla-example\"\nenv = \"prod\"\nfilename = \"my-folder/my-file.parquet\"\n\nclient = storage.Client()\nbucket = client.bucket(f\"ssb-{team}-data-kilde-{env}\")\nblob = bucket.get_blob(filename)\n\nprint(blob.metadata)\n\nEt eksempel på hvordan denne metadataen er som følger:\nsource-my-source-processor-generation: 2309419879432\nsource-my-source-processor-status: done\nStatus-feltet forteller deg om filen er ferdigprosessert, under arbeid eller om prosesseringen feilet. Verdien for dette feltet vil da være, henholdsvis, done, processing eller failed. Merk at filer som er markert som failed fremdeles kan bli plukket opp igjen, siden Kildomaten vil forsøke å prosessere filen 10 ganger før den gir opp.\nGeneration-feltet sier akkurat hvilken utgave av filen status-feltet gjelder. Dette feltet er nødvendig ettersom filens metadata ikke blir slettet når en ny utgave blir lastet opp, og Kildomaten må holde styr på hva den faktisk har prosessert. Du kan sjekke at dette er den nåværende utgaven av filen ved å sammenligne metadatafeltets verdi med filens generation-felt. Hvis vi utvider eksempelet ovenfor:\n\n\nsjekk-metadata.py\n\n# ...\nmetadata_generation = int(blob.metadata[\"source-my-source-processor-generation\"])\nprint(metadata_generation == blob.generation)",
    "crumbs": [
      "Manual",
      "Datatjenester"
    ]
  },
  {
    "objectID": "statistikkere/kildomaten.html#footnotes",
    "href": "statistikkere/kildomaten.html#footnotes",
    "title": "Kildomaten",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI tillegg er det data-admins som må teste tjenesten manuelt hvis det gjøres på skarpe data, siden det kun er data-admins som kan få tilgang til de dataene.↩︎\nEt Infrastructure-as-Code (IaC)-repo er et GitHub-repo som definerer alle ressursene til teamet på Dapla. Alle Dapla-team har et eget IaC-repo på GiHub med navnet -iac under statisticsnorway↩︎\nStandardprosjektet har navnestrukturen &lt;teamnavn&gt;-p↩︎",
    "crumbs": [
      "Manual",
      "Datatjenester"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-botter.html",
    "href": "statistikkere/hva-er-botter.html",
    "title": "Hva er bøtter?",
    "section": "",
    "text": "På Dapla er det Google Cloud Storage (GCS) som benyttes til å lagre data. Bøtter erstatter altså Linux-stammene vi har brukt i produksjonssonen.\nI denne artikelen går vi gjennom noen av de viktigste forskjellene og hvordan man gjør vanlige operasjoner mot bøtter i GCS.",
    "crumbs": [
      "Manual",
      "Data",
      "Hva er bøtter?"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-botter.html#bøtter-vs-filsystemer",
    "href": "statistikkere/hva-er-botter.html#bøtter-vs-filsystemer",
    "title": "Hva er bøtter?",
    "section": "Bøtter vs filsystemer",
    "text": "Bøtter vs filsystemer\nI et Linux- eller Windows-filsystem er filer og mapper organisert i en hierarkisk struktur på et operativsystem (OS). I SSB har OS-ene vært installert på fysiske maskiner som vi vedlikeholder selv.\nEn bøtte i GCS er derimot en kjøpt tjeneste som lar brukeren lagre alle typer objekter i en container. Man trenger altså ikke å tenke på om filene ligger i et hierarki, hvilket operativsystem det kjører på, eller hvor mye diskplass som er tilgjengelig.\n\n\n\n\n\n\nNoteDet finnes egentlig ingen mapper i bøtter\n\n\n\nI motsetning til et vanlig filsystem er det ikke en hierarkisk mappestruktur i en bøtte. Det vil si at alt ligger et sted, men de kan ha lange navn med /-tegn slik at det ligner på et klassisk filsystem. Bruker du / i objekt-navnet så vil også Google Cloud Console vise det som mapper, men det er bare for å gjøre det enklere å forholde seg til. En praktisk konsekvens av dette er eksempelvis at man ikke trenger å opprette en mappe før man legger et fil/objekt i den. Det er bare en tekststreng som er en del av objekt-navnet.",
    "crumbs": [
      "Manual",
      "Data",
      "Hva er bøtter?"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-botter.html#lokalt-filsystem-på-dapla-lab",
    "href": "statistikkere/hva-er-botter.html#lokalt-filsystem-på-dapla-lab",
    "title": "Hva er bøtter?",
    "section": "Lokalt filsystem på Dapla lab?",
    "text": "Lokalt filsystem på Dapla lab?\nPå Dapla skal data lagres i bøtter, men når du åpner en tjeneste fra Dapla lab får du også et lokalt filsystem Figur 1. Det er også dette filsystemet du ser når du f.eks. bruker ls-kommandoen i en terminal i JupyterLab, VSCode og RStudio.\n\n\n\n\n\n\nFigur 1: Til venstre i bildet ser du filutforskeren i Jupyterlab\n\n\n\nDette filsystemet er ment for å lagre kode midlertidig mens du jobber med dem. Det er ikke ment for å lagre data. Det er heller ikke ment som et permanent lagringssted for kode. Permanent lagring av kode skal gjøres på GitHub. Selv om filene du lagrer der fortsetter å eksistere før du sletter tjenesten bør kode du ønsker å bevare pushes til GitHub før du avslutter en sesjon i Jupyterlab.\n\n\n\n\n\n\nWarning\n\n\n\nAlt arbeid bør skje i området $HOME/work. Alle filer utenfor dette området vil slettes når en tjeneste pauses.",
    "crumbs": [
      "Manual",
      "Data",
      "Hva er bøtter?"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-botter.html#systemkommandoer-mot-bøtter",
    "href": "statistikkere/hva-er-botter.html#systemkommandoer-mot-bøtter",
    "title": "Hva er bøtter?",
    "section": "Systemkommandoer mot bøtter",
    "text": "Systemkommandoer mot bøtter\nMan kan i likeht med vanlige filsystemer gjøre operasjoner mot bøtter fra Python og R utover det å lese og skrive data. Man kan lage mapper, slette og flytte filer. I forrige artikkel - jobbe med data finner du kodeeksempler for hvordan du kan både lese og skrive forskjellige filtyper fra bøtter og gjøre forskjellige systemkommandoer.\n\nBøttemontering med Cloud Storage FUSE\nMed unntak av noen begrensninger kan man behandle bøttene som et vanlig filsystem når man jobber fra Dapla. Dette er fordi Dapla lab tjenestene i standardkatalogen er bøttemontert ved hjelp av Google Cloud sin tjeneste FUSE.\nBøttemonteringen er en abstraksjon over bøttene og har derfor noen begrensninger. For eksempel støttes ikke samtidighet, i tillegg til at noe metadata ikke blir bevart. Bøttemonteringen er forøvrig tregere enn et lokalt filsystem. Les mer om begrensningene i Gwoogle sin dokumentasjon om FUSE/bøttemontering.",
    "crumbs": [
      "Manual",
      "Data",
      "Hva er bøtter?"
    ]
  },
  {
    "objectID": "statistikkere/vardef-model.html",
    "href": "statistikkere/vardef-model.html",
    "title": "Datamodell",
    "section": "",
    "text": "For hvert informasjonselement angis det om informasjonen er obligatorisk✅ Det angis også hva det internasjonale🌐 navnet er, det er dette navnet som benyttes i Vardef. Noen informasjonselementer er kun relevante for noen variabler, og de er derfor kun obligatoriske hvis man har denne typen variabler, f.eks. er «Kodeverkets URI» kun obligatorisk å fylle ut dersom det er en kvalitativ variabel . Dette angis med ⚠️. Felter som er valgfrie, angis ved (valgfritt).\nDet angis også om informasjonsfeltet kan endres (patches) etter publisering. Publisering gjelder her både intern og ekstern publisering.",
    "crumbs": [
      "Manual",
      "Metadata",
      "Vardef",
      "Datamodell"
    ]
  },
  {
    "objectID": "statistikkere/vardef-model.html#footnotes",
    "href": "statistikkere/vardef-model.html#footnotes",
    "title": "Datamodell",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nfnr er et reservert kortnavn for variabelen fødselsnummer.↩︎",
    "crumbs": [
      "Manual",
      "Metadata",
      "Vardef",
      "Datamodell"
    ]
  },
  {
    "objectID": "statistikkere/dapla-ctrl.html",
    "href": "statistikkere/dapla-ctrl.html",
    "title": "Dapla Ctrl",
    "section": "",
    "text": "Dapla Ctrl1 er tjeneste for tilgangsstyring på Dapla. Formålet med appen er at det skal være lett å få oversikt og administrere tilganger knyttet til Dapla-team.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Dapla Ctrl"
    ]
  },
  {
    "objectID": "statistikkere/dapla-ctrl.html#innlogging",
    "href": "statistikkere/dapla-ctrl.html#innlogging",
    "title": "Dapla Ctrl",
    "section": "Innlogging",
    "text": "Innlogging\nAlle som jobber i SSB kan logge seg inn på https://dapla-ctrl.intern.ssb.no/ for å bruke tjenesten.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Dapla Ctrl"
    ]
  },
  {
    "objectID": "statistikkere/dapla-ctrl.html#funksjonalitet",
    "href": "statistikkere/dapla-ctrl.html#funksjonalitet",
    "title": "Dapla Ctrl",
    "section": "Funksjonalitet",
    "text": "Funksjonalitet\nAlle SSB-ansatte som logger seg inn i Dapla Ctrl får tilgang til å se informasjon om Dapla-team og tilganger. I tillegg kan de som er i tilgangsgruppen managers legge til, fjerne og endre medlemmer i teamet de har denne rollen. Seksjonsledere har tilgang til å opprette nye team.\n\nTeamoversikt\n\n\n\n\n\n\n\nFigur 1: Bilde av forsiden med oversikt over Mine team i Dapla Ctrl.\n\n\n\nFigur 1 viser landingssiden/forsiden som først møter den som logger seg inn i Dapla Ctrl. den. Her får den som er innlogget oversikt over hvilke Dapla-team man er medlem av, og følgende informasjon om teamene:\n\nTeknisk teamnavn\nTeamets eierseksjon\nAntall teammedlemmer\nManagers for teamet\n\nMan kan også bytte fane fra Mine team til Alle team for å se samme informasjon om alle team som finnes på Dapla.\n\n\nTeamvisning\n\n\n\n\n\n\n\nFigur 2: Bilde av oversikten over et enkelt-team i Dapla Ctrl.\n\n\n\nFra Teamoversikten kan man trykke seg inn på et spesifikt team og få en oversikt slik som vist i Figur 2.\nPå toppen av siden får man se følgende informasjon:\n\nteamets visningsnavn\nteamets tekniske kortnavn\neierseksjonens seksjonsleder\neierseksjonens seksjonsnavn\nautonomitetsnivået til teamet\n\nVidere ser vi at det er en fane for Teammedlemmer og en for Delte Data. Under fanen for Teammedlemmer ser man følgende informasjon om alle medlemmene av teamet:\n\nnavn på medlem\nhvilken seksjons de jobber på\nhvilken tilgangsgruppe de tilhører på teamet\ne-postadresse med brukerens kortnavn\n\nUnder fanen Delte data får man en oversikt over hvilke bøtter teamet har opprettet for å dele data med andre team.\n\n\n\n\n\n\nFigur 3: Bilde av oversikten over hvilke bøtter teamet har opprettet for å dele data med andre team.\n\n\n\nFigur 3 viser hvilken informasjon man får over teamets delte data. Følgende informasjon vises:\n\nkortnavnet på bøttene\ntekniske navnet til bøttene\ntype delt-bøtte\nhvor mange team som har tilgang\nhvor mange personer som har tilgang2\n\n\n\nDelte data\n\n\n\n\n\n\n\nFigur 4: Bilde av oversikten over hvilke personer som har tilgang til en delt-bøtte.\n\n\n\nFra Teamvisningen kan man velge fanen Delte data og trykke seg på en av teamets delte-bøtter. Figur 4 viser informasjon man får se i denne visningen. På toppen av siden får man se kortnavnet til bøtta, det tekniske navnet på bøtta, hvilket team som eier bøtta og hvilken eierseksjon teamet har. I tabellen som vises kan man undersøke hvilke personer som har tilgang til bøtta og få følgende informasjon om de:\n\nnavn\nhvilken seksjon de jobber på\nhvilket team-medelemskap de har tilgang i kraft av\nhvilken tilgangsgruppe de er i på teamet de har tilgang i kraft av\n\nFra tabellen kan man velge å se nærmere på personen som har tilgang, f.eks. se hvilke andre tilganger denne personen har, eller man kan se nærmere på teamet som personen har tilgang i kraft av. Ved å undersøke teamet nærmere kommer man inn på Teamvisningen som er beskrevet over, mens visning av Teammedlemmer forklares under.\n\n\nTeammedlemmer\n\n\n\n\n\n\n\nFigur 5: Bilde av oversikten over hvilke personer som med i dine team.\n\n\n\nFigur 5 viser oversikt over teammedlemmer. Øverst på siden kan man velge mellom en fane for Mine teammedlemmer og Alle teammedlemmer. Førstnevnte viser hvilke andre medlemmer som er i de teamene den innloggede er med i, mens sistnevnte viser alle teammedlemmer i SSB3. I tabellen under får man følgende informasjon om teammedlemmene:\n\nnavn\nhvilken seksjons de jobber på\nhvor mange team de er medlem av\nhvor mange team de har tilgangsrollen data-admins\nnavn på personens seksjonsleder\n\n\n\nMedlemsvisning\n\n\n\n\n\n\n\nFigur 6: Bilde av oversikten over hvilke team en person er medlem av.\n\n\n\nFigur 6 viser hva man ser når går inn på en enkeltperson, enten via Teamoversikten eller Teammedlemmer. Øverst på siden står navnet til personen, hvorvidt de har arbeidssted i Oslo eller Kongsvinger4, hvilken seksjon de jobber på og e-postadressen deres.\nTabellen i Figur 6 får man en oversikt over hvilke team personen er medlem av, samt følgende detaljer:\n\nteamets tekniske kortnavn\nseksjonseier av teamet\nhvilke tilgangsgrupper personen er med i\nhvem som er managers for teamet\n\nVidere kan man gå videre inn på et av teamene og se nærmere på hvem som er medlemmer og hvilke data de deler.\n\n\nOpprette team\n\n\nDet er kun seksjonsledere i SSB som kan opprette et Dapla-team. Hvis en seksjonsleder logger seg inn i Dapla Ctrl så vil knappen i Figur 7 vises på Teamoversikt-siden. Eierseksjonen til et team vil bli definert av hvilken seksjonsleder som oppretter teamet.\n\n\n\n\n\n\n\n\n\nFigur 7: Bilde av knappen som vises for seksjonsledere.\n\n\n\n\n\nNår man oppretter et team må man fylle ut skjemaet i Figur 8. Under finner du en oversikt hva som er viktig å vurdere når man fyller ut de ulike feltene.\n\n\n\nVisningsnavn\nVisningsnavn er teamets navn i et lesevennlig format. Navnet bør bestå av et hoveddomenet og et subdomenet. Det er tillatt med små/store bokstaver, mellomrom, Æ, Ø og Å.\nEksempel på et hoveddomenet i SSB er Skatt, og under det finnes det subdomener som Person og Næring. Visningsnavnet til teamene er da Skatt Person og Skatt Næring.\nI noen tilfeller gir det ikke mening med et subdomenet og da er det greit å kun ha et hoveddomenet. Et eksempel på et visningsnavn som kun har hoveddomenet er Nasjonalregnskap.\n\n\n\n\n\n\n\n\n\n\nFigur 8: Bilde av siden for opprettelse av team.\n\n\n\n\n\n\nOverstyr teknisk navn\nVelg dette for å overstyre det automatisk genererte tekniske teamnavnet. Hvis man ikke krysser av denne boksen vil det genereres et teknisk teamnavn basert på visningsnavnet. Det kan være nyttig å velge dette hvis man har et langt visningsnavn og ønsker å forkorte det genererte tekniske teamnavnet.\n\n\nTeknisk teamnavn\nTeamets navn i et maskinvennlig format som bl.a. benyttes i filstier til lagringsbøtter. Det er ikke tillatt med mellomrom og norske tegn (Æ, Ø og Å). Navnet kan ikke overskride 17 tegn.\nDet tekniske teamnavnet blir automatisk generert basert på visningsnavn hvis man ikke velger Overstyr teknisk navn. Tabell 1 viser eksempler på visningsnavn der man både har overtyrt det tekniske navnet og ikke.\n\n\n\nTabell 1: Eksempler på teamnavn\n\n\n\n\n\nVisningsnavn\nTeknisk navn\nOverstyrt\n\n\n\n\nSkatt Person\nskatt-person\nnei\n\n\nSkatt Næring\nskatt-naering\nnei\n\n\nNasjonalregnskap\nnr\nja\n\n\nFinansmarkedsstatistikk\nfinmark\nja\n\n\n\n\n\n\n\n\nEierseksjon\nAlle team tilhører en seksjon og denne informasjonen ligger lagret i metadataene til teamet. Standard er at seksjonslederen som søker fyller ut sitt seksjonsnummer her, men det er mulig å velge andre seksjoner.\n\n\nAutonomitetsnivå\nNivå av frihet et team har til å definere sin egen infrastruktur. Statistikkproduserende team er vanligvis i kategorien Managed, dvs. at de kun bruker tjenester som tilbys av plattformen. IT-team vil ofte defineres som Self-Managed fordi dette gir større kontroll til teamet. Les mer her.\n\n\n\nLegge til medlemmer\n\n\nHvis man som medlem av tilgangsgruppen managers logger seg inn Dapla Ctrl, vil man få tilgang til å legge til medlemmer i teamet man er i managers-gruppa for. Går man inn på Teamvisning vil man se knappen i Figur 9.\n\n\n\n\n\n\n\n\n\nFigur 9: Bilde av knappen som vises for managers.\n\n\n\n\n\nTrykker man på knappen så får man opp en side for å legge til nye medlemmer i teamet, slik som vist Figur 10. Man kan søke opp alle ansatte i SSB, og man kan velge å legge de til i en eller flere tilgangsgrupper. Når man har valgt person, og hvilke tilgangsgrupper de skal legges i, så avslutter man med å trykke på Legg til medlem for å effektuere endringen.\n\n\n\n\n\n\nFigur 10: Bilde av siden for å legge til medlemmer.\n\n\n\nDet kan ta mellom 1-2 minutter før tilgangen er aktivert og klar til bruk.\n\n\nEndre eller fjerne medlemmer\nHvis man som medlem av tilgangsgruppen managers logger seg inn Dapla Ctrl, vil man få tilgang til å fjerne og endre medlemmer i teamet man er i managers-gruppa for. Går man inn på Teamvisning så vil man se Endre-knapp for hver person i teamet.\n\n\n\n\n\n\nFigur 11: Bilde av Teamvisning som vises for personer i tilgangsgruppen managers.\n\n\n\n\n\nAv Figur 11 ser vi at hvert medlem i teamet har en Endre-knapp. Trykker man på den så får man opp bilde som vises i Figur 12.\nØnsker man å fjerne et medlem fra teamet, så kan man bare trykke på Fjern fra teamet. Da vil man bli spurt om å bekrefte at personen skal gjernes, og velger man ok så effektureres endringen ila et par minutter.\nØnsker man endre hvilken tilgangsgruppe en person er med i, så gjør man det ved å enten fjerne eller legge til tilganger som listet under dropdown-menyen for Tilgangsgruppe(r). For eksempel hvis en person ligger som både data-admins og developers, slik som eksempelet i Figur 12, så trykker man bare på X-ikonet for den tilgangen, og til slutt effektuerer man endringen ved å velge Oppdater tilgang.\n\n\n\n\n\n\n\n\n\nFigur 12: Bilde av siden for å endre eller fjerne medlemmer.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Dapla Ctrl"
    ]
  },
  {
    "objectID": "statistikkere/dapla-ctrl.html#footnotes",
    "href": "statistikkere/dapla-ctrl.html#footnotes",
    "title": "Dapla Ctrl",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nNavnet Dapla Ctrl er valgt for å kommunisere at målsetning med appen er å gi SSB-ere kontroll over tilgangsstyring på Dapla på en effektiv måte.↩︎\nAntall personer som har tilgang til en delt-bøtte viser hvor mange personer det er som har tilgang fra de teamene som har tilgang. Som regel vil det være slik at kun noen tilgangsgrupper i et team får tilgang til andre sine delte data, og ikke hele teamet.↩︎\nAlle teammedlemmer vil i praksis si alle ansatte i SSB, siden teamet Dapla Felles alltid legger til alle ansatte i SSB. Formålet med dette er å la alle ansatte få tilgang til testdata i en bøtte.↩︎\nHvorvidt arbeidssted er Oslo eller Kongsvinger indikeres med henholdsvis O eller K før seksjonsnummeret.↩︎",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Dapla Ctrl"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html",
    "href": "statistikkere/jupyter-playground.html",
    "title": "Jupyter-playground",
    "section": "",
    "text": "Jupyter-playground er en tjeneste på Dapla Lab som er ment for nybegynnere og andre som vil komme raskt i gang med koding i Jupyterlab. Den har mange likheter med Jupyter-tjenesten på Dapla Lab med den forskjellen at mange flere pakker og extensions er ferdig installert i Jupyter-playground. Tjenesten har både R og Python installert.\nSiden tjenesten er ment for opplæring og utforskning så er det ikke anbefalt å bygge produksjonskode fra denne tjenesten. Grunnen til det er at det er flere avhengigheter mellom programvare enn nødvendig, noe som skaper mer komplisert kode enn nødvendig. Derimot er det et ideelt sted for å lære seg R eller Python siden man slipper kompleksiteten med å installere sine egne pakker og forholde seg til ssb-project. For de som skal utvikle produksjonskode anbefales det at koden heller utvikles fra Jupyter-tjenesten.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#forberedelser",
    "href": "statistikkere/jupyter-playground.html#forberedelser",
    "title": "Jupyter-playground",
    "section": "Forberedelser",
    "text": "Forberedelser\nFør man starter Jupyter-playground-tjenesten bør man ha lest kapitlet om Dapla Lab og satt opp Git- og GitHub-konfigurasjonen under Min konto. Deretter gjør du følgende:\n\nLogg deg inn på Dapla Lab\nUnder Tjenestekatalog trykker du på Start-knappen for Jupyter\nGi tjenesten et navn\nÅpne Jupyter konfigurasjoner",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#konfigurasjon",
    "href": "statistikkere/jupyter-playground.html#konfigurasjon",
    "title": "Jupyter-playground",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nKonfigurasjonen av Jupyter-playground er identisk som for Jupyter-tjenesten. Se dokumentasjonen for konfigurasjon av Jupyter.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#datatilgang",
    "href": "statistikkere/jupyter-playground.html#datatilgang",
    "title": "Jupyter-playground",
    "section": "Datatilgang",
    "text": "Datatilgang\nSlik kan man inspisere dataene fra en terminal inne i tjenesten:\n\nÅpne en instans av Jupyter med data fra bøtter\nÅpne en terminal inne i Jupyter\nGå til mappen med bøttene ved å kjøre dette fra terminalen cd /buckets\nKjør ls -ahl i teminalen for å se på hvilke bøtter som er montert.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#installere-pakker",
    "href": "statistikkere/jupyter-playground.html#installere-pakker",
    "title": "Jupyter-playground",
    "section": "Installere pakker",
    "text": "Installere pakker\nSiden det nesten ikke er installert noen pakker i tjenesten så kan brukeren opprette et ssb-project og installere pakker som vanlig.\nFor å bygge et eksisterende ssb-project så kan brukeren også bruke ssb-project.\nFor å installere R-pakker følger man beskrivelsen for renv.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#slette-tjenesten",
    "href": "statistikkere/jupyter-playground.html#slette-tjenesten",
    "title": "Jupyter-playground",
    "section": "Slette tjenesten",
    "text": "Slette tjenesten\nFor å slette tjenesten kan man trykke på Slette-knappen i Dapla Lab under Mine tjenester. Når man sletter en tjeneste så sletter man hele disken inne i tjenesten og frigjør alle ressurser som er reservert. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#pause-tjenesten",
    "href": "statistikkere/jupyter-playground.html#pause-tjenesten",
    "title": "Jupyter-playground",
    "section": "Pause tjenesten",
    "text": "Pause tjenesten\nMan kan pause tjenesten ved å trykke på Pause-knappen i Dapla Lab under Mine tjenester. Når man pauser så slettes alt påden lokale disken som ikke er lagret under $HOME/work. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/jupyter-playground.html#monitorering",
    "href": "statistikkere/jupyter-playground.html#monitorering",
    "title": "Jupyter-playground",
    "section": "Monitorering",
    "text": "Monitorering\n\n\nMan kan moniterere en instans av Jupyter ved å trykke på Jupyter-teksten under Mine tjenester i Dapla Lab, slik som vist i Figur 1.\nDenne funksjonaliteten er under arbeid og mer informasjon kommer snart.\n\n\n\n\n\n\n\n\n\nFigur 1: Monitorering av Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter-playground"
    ]
  },
  {
    "objectID": "statistikkere/klass.html",
    "href": "statistikkere/klass.html",
    "title": "Klass",
    "section": "",
    "text": "Klass er SSBs system for dokumentasjon av kodeverk1. Hensikten er at alle kodeverk skal dokumenteres og oppdateres ett sted (av ansvarlig seksjon), og gjenbrukes av alle som har behov for dem. Ansvarlig seksjon vil da sikre at alle får tak i gyldig versjon (eller en tidligere versjon dersom det er det brukeren trenger).\nAlle2 kodeverk som brukes i SSBs statistikkproduksjon skal dokumenteres i Klass. Hvert kodeverk har en eierseksjon, og det er eierseksjonen som har ansvar for å dokumentere, og senere oppdatere, kodeverket. Klass viser hvilke kategorier de ulike kodeverkene inneholder på en gitt dato, eller i løpet av en gitt tidsperiode, hvordan kategoriene endrer seg over tid (f.eks. endring i kommuneinndelingen Standard for kommuneinndeling). I tillegg kan Klass vise mappingen mellom to ulike kodeverk (f.eks mellom politidistrikt og kommuner Standard for politidistrikt). En kan også lage en variant av et kodeverk, der en f.eks. aggregerer ulike koder i det opprinnelige kodeverket (se f.eks. Standard for næringsgruppering (SN) som er en variant der kodene i Standard for næringsgruppering er blitt aggregert). Kodeverkene kan gjenbrukes av alle som trenger dem, både SSB-ansatte og eksterne brukere. Mange av kodeverkene i Klass, f.eks. Standard for kommuneinndeling og Standard for næringsgruppering, gjenbrukes i stor grad både av interne og eksterne brukere.\nDet kan refereres til Klass både fra Vardef og Datadoc.\nKodeverkene kan hentes ut via et API eller lastes ned i csv-format. API-er det mest fleksible å bruke. Her kan en f.eks. hente ut kodeverket slik det var på en bestemt dato, i et bestemt tidsrom, hvilke endringer som har skjedd innen et visst tidsrom og korrespondansetabeller som viser mappingen mellom to ulike kodeverk. Det finnes også veiledning til bruk av Klass i statistikkproduksjonen (sas, R og Python) her: Bruk av Klass i statistikkproduksjonen.",
    "crumbs": [
      "Manual",
      "Metadata",
      "Klass"
    ]
  },
  {
    "objectID": "statistikkere/klass.html#footnotes",
    "href": "statistikkere/klass.html#footnotes",
    "title": "Klass",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nKodeverk er en fellesbetegnelse for klassifikasjoner og kodelister. En klassifikasjon er et mer «formelt» kodeverk som oppfyller flere betingelser, bl.a. at den er normativ og uttømmende. Alle klassifikasjoner i SSB har navn som begynner med Standard for, f.eks. Standard for næringsgruppering eller Standard for kommuneinndeling. Ei kodeliste er ofte skreddersydd for en bestemt statistikk, er ikke normativ og trenger ikke være uttømmende.↩︎\nDet finnes kodeverk som ikke kan legges inn i Klass fordi de ikke oppfyller kravene som stilles i den internasjonale modellen som Klass bygger på.↩︎",
    "crumbs": [
      "Manual",
      "Metadata",
      "Klass"
    ]
  },
  {
    "objectID": "statistikkere/kvakk.html",
    "href": "statistikkere/kvakk.html",
    "title": "Kvalitet i kode og koding (KVAKK)",
    "section": "",
    "text": "KVAKK er en tverrfaglig gruppe som utarbeider regler, anbefalinger og veiledninger for god prakis for jobbing med kode og koding. Gruppa har mandat fra DM og består av representanter fra alle statistikkavdelinger, forskning, metode og IT.\nGruppa jobber temabasert og publiserer regler, anbefalinger og veiledninger for hvert tema under beste praksis området på Confluence. Regler skal følges, med mindre man har en dokumentert begrunnelse på hvorfor regelen avvikes. Følgende regler er fastsatt i SSB:\n\nAll produksjonskode skal være under versjonskontroll i GitHub\nKildekode i GitHub skal ikke inneholde ukrypterte passord eller hemmeligheter\nGit-klienter skal konfigureres slik at resultat fra kjøringer i Jupyter Notebooks ikke lagres på GitHub\nAlle biblioteker skal ha en eier\nBruk SSB-mal for PyPI-biblioteker når du skal lage et python-bibliotek\nGrensesnittet til biblioteket skal være dokumentert\nAlle biblioteker skal ha tilhørende tester\nAll produksjonskode skal lagres i et format som støtter kodeanalyse\n\nAnbefalinger er god praksis som de fleste bør følge. Disse finner du på KVAKKs sider om regler, anbefalinger og veiledninger. Du finner også hjelpeartikler som for eksempel artikkelen om hvordan man håndterer hemmeligheter og passord i git.\nDu finner mer informasjon om KVAKK, hvem som er med og arbeidet deres på KVAKK-siden.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Kvalitet i kode og koding (KVAKK)"
    ]
  },
  {
    "objectID": "statistikkere/dapla-statbank-client.html",
    "href": "statistikkere/dapla-statbank-client.html",
    "title": "dapla-statbank-client",
    "section": "",
    "text": "dapla-statbank-client er en Python-pakke som lar SSB-ere laste inn og hente ut data fra Statistikkbanken fra Dapla Lab1. Pakken tilbyr ulik funksjonalitet som gjør det enkelt å jobbe mot Statistikkbanken:",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-statbank-client"
    ]
  },
  {
    "objectID": "statistikkere/dapla-statbank-client.html#tilgangsstyring",
    "href": "statistikkere/dapla-statbank-client.html#tilgangsstyring",
    "title": "dapla-statbank-client",
    "section": "Tilgangsstyring",
    "text": "Tilgangsstyring\nTilgang til funksjonalitet i Statistikkbanken styres av brukernavn (“lastebruker”) og passord (“lastepassord”) som brukeren må oppgi. Tilgang til data som skal lastes inn i banken krever at man representerer teamet som har tilgang til dataene når man starter Dapla Lab.\nStatistikkbanken har en prod- og testdatabase. Brukere har tilgang til å publisere til begge deler fra Dapla Lab sitt prod-miljø. Fra Daplas testmiljø, kan man kun publisere til Statistikkbankens testdatabase.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-statbank-client"
    ]
  },
  {
    "objectID": "statistikkere/dapla-statbank-client.html#forberedelser",
    "href": "statistikkere/dapla-statbank-client.html#forberedelser",
    "title": "dapla-statbank-client",
    "section": "Forberedelser",
    "text": "Forberedelser\nFor å bruke dapla-statbank-client i Dapla Lab eller prodsonen må man opprette et ssb-project og installere pakken på følgende måte:\n\n\nTerminal\n\npoetry add dapla-statbank-client",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-statbank-client"
    ]
  },
  {
    "objectID": "statistikkere/dapla-statbank-client.html#funksjonalitet",
    "href": "statistikkere/dapla-statbank-client.html#funksjonalitet",
    "title": "dapla-statbank-client",
    "section": "Funksjonalitet",
    "text": "Funksjonalitet\nI denne delen beskrives den viktigste funksjonalitet som tilbys i dapla-statbank-client. Det finnes også noe funksjonalitet som kun er beskrevet her.\n\nFilbeskrivelse\nMan kan hente ut filbeskrivelsen til en statistikkbanktabell fra Statistikkbanken med følgende kode:\n\n\nNotebook\n\nfrom statbank import StatbankClient\nstat_client = StatbankClient()  # Ber om lastebruker og lastepassord\n\ntabell = \"06339\"\nfilbeskrivelse = stat_client.get_description(tableid=tabell)\nprint(filbeskrivelse)\n\nI eksempelet over henter vi filbeskrivelsen til statistikkbanktabell 06339 og printer den ut. Beskrivelse-objektet inneholder alle formelle krav til dataene som kan lastes inn i denne tabellen.\n\n\nLaste tabeller\nMan kan laste data inn i Statistikkbankens databaser med dapla-statbank-client. Dataene som skal lastes inn må sendes inn som en dictionary med deltabellnavn som nøkler, og Pandas dataframes som verdier. Under er et eksempel på at en tabell lastes for publisering 1. januar 2050:\n\n\nNotebook\n\nfrom statbank import StatbankClient\n\nstat_client = StatbankClient(\n    date=\"2050-01-01\",\n    overwrite=True,\n    approve=2\n)\n\nstat_client.transfer({\"deltabellfilnavn.dat\" : df_06399},\n                     \"06339\")\n\nI eksempelet over ser vi også at vi sender inn parametere, om vi ønsker at eksisterende tabeller skal overskrives, og at godkjenning skal skje Just-in-Time. I tillegg har vi spesifiser en publiseringsdato (merk at statbanken kun godtar publiseringer en viss antall måneder frem i tid).\nUnder er et eksempel på lasting av flere deltabeller samtidig.\n\n\nNotebook\n\nfrom statbank import StatbankClient\nstat_client = StatbankClient(\n    date=\"2050-01-01\",\n    overwrite=True,\n    approve=2\n)\n\ndata_07495 = {\"kargrs01fylker1.dat\" : df_07495_fylker,\n              \"kargrs01landet1.dat\" : df_07495_landet,}\n\nstat_client.transfer(data_07495, tableid=\"07495\")\n\n\nAvrunding av desimaltall\nMetoden round_data() lar brukeren avrunde desimaltall iht filbeskrivelsen til en statistikkbanktabell. Dette kan være nyttig siden, antall desimaler som skal beholdes ligger markert i filbeskrivelsen, og R og Python håndterer avrunding annerledes enn det mange forventer (se boks under).\nI eksempelet under har vi en dataframe med navn data_07495 som inneholder data som skal lastes i Statistikkbanken. For å avrunde denne og lagre endringene, så lagrer vi resultatet tilbake til den opprinnelige dictionaryen’n. Ved dags dato er det smart å runde av ETTER man validerer, siden validate() ikke tar høyde for at dataene kan være avrundet allerede.\n\n\nNotebook\n\nfrom statbank import StatbankClient\nstat_client = StatbankClient()\n\n# Hente filbeskrivelse\ntabell = \"07495\"\nfilbeskrivelse = stat_client.get_description(tableid=tabell)\n\n# Pakk datasettet i dictionaryen (navnet på deltabellen ligger i filbeskrivelsen)\ndata_07495 = filbeskrivelse.transferdata_template(df_07495)\n\n# For å ta vare på endringene, så må du skrive tilbake til datasettet i dictionaryen\ndata_07495 = filbeskrivelse.round_data(data_07495)\n\n\n\n\n\n\n\nImportantAvrunding i Python og R\n\n\n\nAvrunding av desimaltall i Python og R gjøres mot nærmeste partall. Dvs. at 2,5 blir avrundet til 2, og 1,5 blir også avrundet til 22. Dette er ulikt hvordan tall blir avrundet i SAS og Excel, der 2,5 blir til 3, og 1,5 blir til 2. Bruker man round_data() i dapla-statbank-client så er det sistnevnte metode som benyttes.\n\n\n\n\nLaste til test\nStatistikkbanken har et test-miljø som kan benyttes hvis man ønsker. Det er åpnet for å laste til Statistikkbankens test-miljøet fra både Dapla Lab prod og Dapla Lab test.\nHvis man ønsker å laste til Statistikkbankens test-miljø fra Dapla Lab prod, så må man eksplisitt angi dette i argumententet til StatbankClient(use_db='TEST'). Det er derimot ikke mulig å laste til Statistikkbankens prod-miljø fra Dapla Lab test.\n\n\n\nValidering\nMetoden validate() lar brukeren validere en dataframe mot en filbeskrivelsen. Dette er nyttig siden man kan få tilbakemelding om eventuelle feil før man sender over data til Statistikkbanken. Spesielt er dette interessant under utvikling av en statistikkbanklevering i nytt kodespråk, hvor validate() kan gi deg tilbakemelding på hva du har gjort feil hittils med formen på dataene.\n\n\nNotebook\n\nfrom statbank import StatbankClient\nstat_client = StatbankClient()\n\n# Hente filbeskrivelse\ntabell = \"07495\"\nfilbeskrivelse = stat_client.get_description(tableid=tabell)\n\n# Pakk deltabeller i en dict før validering\ndata_07495 = filbeskrivelse.transferdata_template(df_07495)\n\n# Vi tar ikke vare på valideringene, men de kan raise errors, eller printes ut.\nfilbeskrivelse.validate(data_07495)\n\n\n\nPubliserte tabeller\nMan kan hente ut publiserte data fra Statistikkbankens åpne API med dapla-statbank-client. apidata_all() henter all data i tabellen, uten å spesifisere noe filter:\n\n\nNotebook\n\nfrom statbank import apidata_all\n\ndf_06339 = apidata_all(\"06339\", include_id=True)\n\nHvis man spesifiserer include_id=True så fletter funksjonen id-kolonner (variabel-koder) for klassifikasjonsvariablene etter “label-kolonner”.\nMan kan også spesifisere akkurat hvilken informasjon man ønsker fra en tabell, dette “query” objektet kan man kopiere ut av statstikkbankens nettsider, etter å ha filtrert en tabell så ligger det under “API-spørring for denne tabellen” i det andre feltet. Dette kan du copy-paste ut og assigne til variabelen “query” her. I følgende kodebit henter vi også fra intern statbank (upublisert?) ved å spesifisere hele adressen, ikke kun numerisk statbank-id.\n\n\nNotebook\n\nfrom statbank import apidata\n\nquery = {'query': [{'code': 'Region', 'selection': {'filter': 'vs:Landet', 'values': ['0']}}, {'code': 'Alder', 'selection': {'filter': 'vs:AldGrupp19', 'values': ['000', '001', '002', '003', '004', '005', '006', '007', '008', '009', '010', '011', '012', '013', '014', '015', '016', '017', '018', '019', '020', '021', '022', '023', '024', '025', '026', '027', '028', '029', '030', '031', '032', '033', '034', '035', '036', '037', '038', '039', '040', '041', '042', '043', '044', '045', '046', '047', '048', '049', '050', '051', '052', '053', '054', '055', '056', '057', '058', '059', '060', '061', '062', '063', '064', '065', '066', '067', '068', '069', '070', '071', '072', '073', '074', '075', '076', '077', '078', '079', '080', '081', '082', '083', '084', '085', '086', '087', '088', '089', '090', '091', '092', '093', '094', '095', '096', '097', '098', '099', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119+']}}, {'code': 'Statsbrgskap', 'selection': {'filter': 'vs:Statsborgerskap', 'values': ['000']}}, {'code': 'Tid', 'selection': {'filter': 'item', 'values': ['2022']}}], 'response': {'format': 'json-stat2'}}\n\ndf_folkemengde = apidata(\"https://i.ssb.no/pxwebi/api/v0/no/prod_24v_intern/START/be/be01/folkemengde/Rd0002Aa\",\n                         query,\n                         include_id = True\n)\n\n\n\nIntern statistikkbank\nSSB-brukere kan hente data fra SSBs interne Statistikkbank:\n\n\nNotebook\n\nfrom statbank import apidata\n\nuri = \"https://i.ssb.no/pxwebi/api/v0/no/prod_24v_intern/START/be/be01/folkemengde/Rd0002Aa\"\n\nquery = {'query': [{'code': 'Region', 'selection': {'filter': 'vs:Landet', 'values': ['0']}}, {'code': 'Alder', 'selection': {'filter': 'vs:AldGrupp19', 'values': ['000', '001', '002', '003', '004', '005', '006', '007', '008', '009', '010', '011', '012', '013', '014', '015', '016', '017', '018', '019', '020', '021', '022', '023', '024', '025', '026', '027', '028', '029', '030', '031', '032', '033', '034', '035', '036', '037', '038', '039', '040', '041', '042', '043', '044', '045', '046', '047', '048', '049', '050', '051', '052', '053', '054', '055', '056', '057', '058', '059', '060', '061', '062', '063', '064', '065', '066', '067', '068', '069', '070', '071', '072', '073', '074', '075', '076', '077', '078', '079', '080', '081', '082', '083', '084', '085', '086', '087', '088', '089', '090', '091', '092', '093', '094', '095', '096', '097', '098', '099', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119+']}}, {'code': 'Statsbrgskap', 'selection': {'filter': 'vs:Statsborgerskap', 'values': ['000']}}, {'code': 'Tid', 'selection': {'filter': 'item', 'values': ['2022']}}], 'response': {'format': 'json-stat2'}}\n\ndf_folkemengde = apidata(uri, query, include_id = True)",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-statbank-client"
    ]
  },
  {
    "objectID": "statistikkere/dapla-statbank-client.html#footnotes",
    "href": "statistikkere/dapla-statbank-client.html#footnotes",
    "title": "dapla-statbank-client",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPakken fungerer også på treningsarenaen til Dapla; Jupyter i prodsonen↩︎\nGrunnen til at R og Python runder av desimaltall nærmeste partall er at det reduserer bias i en retning for en kolonne. Dersom alle tall rundes opp vil summen av en kolonne *dras oppover**. ↩︎",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-statbank-client"
    ]
  },
  {
    "objectID": "statistikkere/vardef-toolbelt.html",
    "href": "statistikkere/vardef-toolbelt.html",
    "title": "Vardef",
    "section": "",
    "text": "Vardef er SSBs system for dokumentasjon av variabler. Vardef består av et sentralt datalager som man kan interagere med via et API. Statistikere og forskere i SSB kan interagere med systemet gjennom Vardef-delen av Python-pakken dapla-toolbelt-metadata.\nBrukerdokumentasjonen for Vardef er skrevet i notebooks som er ferdig installert i tjenesten Vardef-forvaltning i Dapla Lab. Det er opprette egne notebooks for typiske arbeidsoppgaver man ønsker å gjøre i Vardef, og brukeren kan kjøre disse uten å skrive kode selv. I notebooks-ene finner man ferdigskrevet kode som bruker dapla-toolbelt-metadata for å jobbe med Vardef. Hvis man ønsker å se innholdet i notebooks uten å starte tjenesten Vardef-forvaltning, så kan gå inn på lenkene til høyre på denne siden.\nPå denne siden dokumenteres hvordan notebooks’ene i Vardef-forvaltning kan benyttes for å gjøre ulike arbeidsoppgaver knyttet til Vardef.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata",
      "Vardef"
    ]
  },
  {
    "objectID": "statistikkere/vardef-toolbelt.html#forberedelser",
    "href": "statistikkere/vardef-toolbelt.html#forberedelser",
    "title": "Vardef",
    "section": "Forberedelser",
    "text": "Forberedelser\nFor å benytte Datadoc-delen av dapla-toolbelt-metadata må man først installere pakken i et ssb-project:\n\n\nTerminal\n\npoetry add dapla-toolbelt-metadata",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata",
      "Vardef"
    ]
  },
  {
    "objectID": "statistikkere/vardef-toolbelt.html#funksjonalitet",
    "href": "statistikkere/vardef-toolbelt.html#funksjonalitet",
    "title": "Vardef",
    "section": "Funksjonalitet",
    "text": "Funksjonalitet\nFor benytte seg av Vardef-delen av dapla-toolbelt-metadata så må man først importere modulen:\n\n\nNotebook\n\nfrom dapla_metadata.variable_definitions import Vardef\n\nKodeeksempler er samlet i notebooks som er ferdiginstallert i Dapla Lab tjenesten Vardef-forvaltning. Se eksemplene her.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata",
      "Vardef"
    ]
  },
  {
    "objectID": "statistikkere/git-arbeidsflyt.html",
    "href": "statistikkere/git-arbeidsflyt.html",
    "title": "Anbefalt arbeidsflyt med Git",
    "section": "",
    "text": "Å lære seg å bruke git handler om å implementere nye rutiner i arbeidshverdagen. Vi kaller dette for anbefalt arbeidsflyt. Arbeidsflyten er noe man gjennomfører hyppig iløpet av en arbeidsdag om man jobber med kode. Vi beskriver derfor anbefalt git arbeidsflyt fra man begynner på en ny oppgave til den er avsluttet og merget til main.\nDenne siden er en inspirert av Beste Praksis sin confluence-artikkel Hvordan er Git anbefalt arbeidsflyt?.\n\n\n\n\n\n\nTipUsikker på hva Git og GitHub er?\n\n\n\nLes vår artikkel om Git og GitHub. Der kan du lære hva verktøyene er og hvorfor vi bruker de i SSB.\n\n\n\nArbeidsflyt\n\n1. Oppdater ditt lokale repo\n\n1.1 Sørg for at repoet ikke har ulagrede endringer\n\n\nterminal\n\ngit status # Viser hvilke filer som er endret, nye eller klare for commit\n\nVil du lagre endringene gjør du som beskrevet i punkt 4. Hvis ikke kjører du følgende:\n\n\nterminal\n\ngit reset --hard HEAD # forkaster endringer\n\n\n\n1.2 Oppdater lokal versjon av main\n\n\nterminal\n\ngit switch main # bytt til main\ngit pull # oppdaterer\n\n\n\n\n2. Lag ny gren (branch)\nI vårt eksempel skal vi omstrukturere denne delen av manualen. Vi kaller derfor grenen omstrukturere-git-github\n\n\nterminal\n\ngit switch -c omstrukturere-git-github # -c står for create\n\n\n\n3. Gjør endringer i repoet/koden\n\n\n4. Lagre kode og oppdatere repoet på GitHub\nDette er hoveddelen av arbeidsflyten og en prosess man vil gjenta flere ganger før man er ferdig med en gren.\n\n\nterminal\n\ngit status          # Viser hvilke filer som er endret, nye eller klare for commit\ngit add &lt;filnavn&gt;   # For alle filer du vil ha med i committen\ngit commit -m\"&lt;tekst som beksriver endringer&gt;\" \ngit push            # Sender endringene til GitHub\n\n\n\n\n\n\n\nTipTips for git add\n\n\n\ngit add . legger til alle filer! Da slipper man å skrive hvert filnavn.\nMan kan også legge til alt nytt innhold i en mappe ved å skrive git add &lt;mappenavn&gt;.\nOm du vil legge til alle filer av en filtype kan man skrive git add *&lt;filtype&gt;. For eksempel: git add *.py.\n\n\n\n\n5. Gjenta steg 3 og 4 til oppgaven er ferdig\n\n\n6. Sjekk for merge-konflikt: Merge inn endringer fra main til din gren\nDette gjør vi fordi det er lettere å løse mergekonflikter lokalt enn på GitHub. Sjekk først at du har en ‘ren’ git status slik vist i punkt 1.1.\n\n\nterminal\n\ngit fetch # Synkroniserer origin-biten av ditt lokale repo med github\ngit merge origin/main # evt. origin/master om hovedgrena heter master\ngit push\n\nOm du får en merge-konflikt løser du dette slik det er beskrevet i siste del av denne artikkelen: Hvordan løse en merge-konflikt.\n\n\n7. Opprett en pull request på GitHub\n\n7.1 Finn repoet på GitHub\n\n\n7.2 Opprett ny pull request\nHar du nylig pushet kan du trykke på Compare & pull request der den lilla pilen viser. Eventuelt kan man trykke på pull requets slik den grønne boblen viser.\n\n\n\nGitHub: opprett PR\n\n\n\n\n7.3 Beskriv endringene\n\n\n\nGitHub: PR beskrivelse\n\n\n\n\n7.4 Be en kollega om se over\nFor eksempel ved å bruke GitHub sin request review funksjon. Eller sende melding på teams\n\n\n\nGitHub: Request Review\n\n\n\n\n7.5 Merge og slette gren\nNår en kollega har godkjent endringene vil merge-knappen bli grønn slik bildet nedenfor viser.\n\n\n\nGitHub: Godkjent PR\n\n\nEtter å ha merget vil man få muligheten til å slette grenen enkelt ved å trykke på den grå knappen siste figur viser.\n\n\n\nPR: Slett gren\n\n\n\n\n\nArbeidsflyt og repo illustrert\n\n\n\n\nHvordan løse en merge-konflikt\nHvis to personer har endret samme linje i en fil vet ikke Git hvilken av endringene den skal velge. Dette kalles en merge-konflikt. I vårt tilfelle vil dette skje om endringene har skjedd i main etter at vi lagde grenen vår.\nOm en slik konflikt oppstår må man manuelt velge hvilken versjon man ønsker. Følg denne veiledningen (confluence) for å løse en merge-konflikt.",
    "crumbs": [
      "Manual",
      "Kode",
      "Anbefalt arbeidsflyt med Git"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html",
    "href": "statistikkere/gcc.html",
    "title": "Anbefalt arbeidsflyt med Git",
    "section": "",
    "text": "Google Cloud Console (GCC) er et web-basert grensesnitt for å administrere ressurser og tjenester på Google Cloud Platform (GCP). Alle i SSB kan logge seg inn i GCC med sin SSB-bruker. Dapla-team har sjelden mulighet til å opprette nye ressurser fra dette grensesnittet, siden vi ønsker at det skal gjøres med kode. Men det er likevel et nyttig verktøy for å se på ressurser og gjøre endringer på eksisterende ressurser. I SSB bruker bruker vi GCC hovedsakelig til følgende:",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Anbefalt arbeidsflyt med Git"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html#innlogging",
    "href": "statistikkere/gcc.html#innlogging",
    "title": "Anbefalt arbeidsflyt med Git",
    "section": "Innlogging",
    "text": "Innlogging\nFor å logge inn i GCC så gjør du følgende:\n\nÅpne Google Cloud Console i en nettleser.\nLogg in med din SSB-bruker.\n\nHvis du også har en privat Google-konto som benyttes i samme nettleser, må du noen ganger passe på at du er logget inn med riktig konto. Dette kan du sjekke ved å trykke på profilbildet ditt øverst til høyre i GCC. Hvis du ikke er logget inn med riktig konto, så trykker du på Logg ut og logger inn på nytt med riktig konto.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Anbefalt arbeidsflyt med Git"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html#prosjektvelger",
    "href": "statistikkere/gcc.html#prosjektvelger",
    "title": "Anbefalt arbeidsflyt med Git",
    "section": "Prosjektvelger",
    "text": "Prosjektvelger\nEtter at du har logget deg på med din SSB-bruker, så må du velge hvilket av ditt teams prosjekter du ønsker å jobbe med. Dette gjør du ved å trykke på prosjektvelgeren øverst til venstre på siden. Vidoen under viser hvordan du velger et prosjekt og lister ut hvilke bøtter som finnes i prosjektet.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Anbefalt arbeidsflyt med Git"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html#filutforsker",
    "href": "statistikkere/gcc.html#filutforsker",
    "title": "Anbefalt arbeidsflyt med Git",
    "section": "Filutforsker",
    "text": "Filutforsker\nFor å utforske bøtter og filer i et Dapla-team sitt Google-prosjekt så kan man bruke Cloud Storage-grensesnittet i GCC. For å bruke denne funksjonaliteten gjør du følgende:\n\nBruk prosjektvelgeren til å velge ønsket prosjekt.\nDeretter søker du opp Google Storage i søkefeltet øverst på siden.\n\nDa får du en oversikt over alle bøttene i prosjektet. Velg ønsker bøtte for å utforske innholdet i bøtta.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Anbefalt arbeidsflyt med Git"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html#logger",
    "href": "statistikkere/gcc.html#logger",
    "title": "Anbefalt arbeidsflyt med Git",
    "section": "Logger",
    "text": "Logger\nKommer snart.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Anbefalt arbeidsflyt med Git"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html#transfer-service",
    "href": "statistikkere/gcc.html#transfer-service",
    "title": "Anbefalt arbeidsflyt med Git",
    "section": "Transfer Service",
    "text": "Transfer Service\nLes mer om hvordan man bruker Transfer Service her.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Anbefalt arbeidsflyt med Git"
    ]
  },
  {
    "objectID": "statistikkere/vardef.html",
    "href": "statistikkere/vardef.html",
    "title": "Vardef",
    "section": "",
    "text": "Vardef er SSBs system for dokumentasjon av variabler. Hensikten med Vardef er at alle variabler i SSB skal dokumenteres og oppdateres ett sted (av ansvarlig Dapla-team) og gjenbrukes av alle som har behov for dem.\nVardef skal inneholde all nødvendig informasjon om en variabel. Mens Datadoc dokumenterer variabelforekomstene, dvs. enkeltvariablene i et datasett1, skal Vardef dokumentere de mer overordnede beskrivelsene av en variabel. I Vardef er det variabler som gjenbrukes, enten på tvers i SSB, eller flere ganger over tid i samme statistikk, som skal dokumenteres. Et eksempel er f.eks. variabelen «organisasjonsnummer» som vil være definert i Vardef, slik at beskrivelsen kan gjenbrukes i alle datafiler der variabelforekomsten organisasjonsnummer inngår. I SSB har ofte organisasjonsnummer ulike navn i ulike datasett , men når alle variablene er knyttet til samme Vardef-variabel, kan brukerne likevel forstå at det er samme variabel.\nI Datadoc har en mulighet for å presisere en variabel som refereres til i Vardef. Vi kan f.eks. ha variabelen «Yrkesinntekt» som brukes både i datasett A og B. Selve definisjonen hentes da fra Vardef og er dermed den samme i begge datasett. Men så kan det være presiseringer en må gjøre i Datadoc for hver variabelforekomst, f.eks. at yrkesinntekt i datasett A måles i kroner, mens den i datasett B måles i 1000 kroner.\nDersom Vardef-variabelen er en kvalitativ variabel (har verdier som hentes fra et kodeverk), skal variabelen referere til tilhørende kodeverk i Klass. F.eks. skal variabelen «Sivilstand» som har definisjonen «Variabelen viser en persons stilling ihht ekteskapslovgivningen», referere til «Standard for sivilstand» i Klass som viser hvilke verdier (kategorier) variabelen kan anta.",
    "crumbs": [
      "Manual",
      "Metadata",
      "Vardef"
    ]
  },
  {
    "objectID": "statistikkere/vardef.html#footnotes",
    "href": "statistikkere/vardef.html#footnotes",
    "title": "Vardef",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nDette er noe vi ønsker å unngå på Dapla, det er best for brukerne om samme variabelforekomst har samme navn i alle datasett der den brukes. Dette vil ikke være noe krav, men dersom variabelforekomstnavnet er det samme som kortnavnet til tilhørende variabel i Vardef, vil variabelforekomsten i Datadoc kunne lenkes maskinelt til riktig variabeldefinisjon i Vardef. Dermed slipper en mye manuelt arbeid.↩︎",
    "crumbs": [
      "Manual",
      "Metadata",
      "Vardef"
    ]
  },
  {
    "objectID": "statistikkere/altinn-sfu.html",
    "href": "statistikkere/altinn-sfu.html",
    "title": "System for utvalgsadministrasjon (SFU)",
    "section": "",
    "text": "Denne kapitlet beskriver hvordan man kan bruke dapla-suv-tools pakken for å hente utvalg og enhetsinformasjon fra SFU i Dapla-miljøet.\nDet finnes fortsatt avhengigheter til bakkesystemene for å kunne kjøre et fullstendig produksjonsløp på Dapla. SU-V har laget integrasjoner mellom bakke- og skyløsninger der det er behov for dette. Administrasjon av utvalg og enheter skjer fortsatt fra SFU på bakke.",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "SFU"
    ]
  },
  {
    "objectID": "statistikkere/altinn-sfu.html#forberedelser",
    "href": "statistikkere/altinn-sfu.html#forberedelser",
    "title": "System for utvalgsadministrasjon (SFU)",
    "section": "Forberedelser",
    "text": "Forberedelser\nFor å kunne hente enhetsinformasjon fra SFU må brukeren din ha tilgang til delregisteret i riktig miljø (DB1T/DB1P). Ta kontakt med Kundeservice dersom du opplever problemer med tilganger.\nI tillegg må dapla-suv-tools være installert i en tjeneste på Dapla Lab:\n\n\nterminal\n\npoetry add dapla-suv-tools",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "SFU"
    ]
  },
  {
    "objectID": "statistikkere/altinn-sfu.html#funksjonalitet",
    "href": "statistikkere/altinn-sfu.html#funksjonalitet",
    "title": "System for utvalgsadministrasjon (SFU)",
    "section": "Funksjonalitet",
    "text": "Funksjonalitet\nUnder finner du eksempler på funksjonalitet som tilbys i dapla-suv-tools for å jobbe med SFU fra Dapla Lab.\n\nHente utvalg fra SFU\nFor å hente utvalg fra SFU, bruk metoden get_utvalg_from_sfu i SuvClient. Sørg for at du oppgir riktig delregisternummer og RA-nummer for utvalget. Dersom utvalget er delt inn i puljer kan du oppgi pulje som parameter.\n\n\nnotebook\n\nclient = SuvClient()\n\nresponse = client.get_utvalg_from_sfu(\n    delreg_nr=49430224,\n    ra_nummer='RA-0666A3',\n    pulje='2'\n)\n\nprint(json.dumps(response, indent=4))\n\n\n\nHente enhetsinformasjon fra SFU\nFor å hente enhetsinformajon fra SFU, bruk metoden get_enhet_from_sfu i SuvClient. Sørg for at du oppgir riktig delregisternummer og organisajonsnummer.\n\n\nnotebook\n\nclient = SuvClient()\n\nresponse = client.get_enhet_from_sfu(\n    delreg_nr=49430224,\n    orgnr='123456789' \n)\n\nprint(json.dumps(response, indent=4))\n\n\n\n\n\n\n\nCautionVis output\n\n\n\n\n\n{\n    \"delreg_nr\": 49430224,\n    \"ident_nr\": \"A3TF0019\",\n    \"orgnr\": \"123456789\",\n    \"enhets_type\": \"FRTK\",\n    \"foretak\": \"A3TF0019\",\n    \"orgnr_foretak\": \"123456789\",\n    \"flv\": \"0\",\n    \"navn1\": \"MITT REGNSKAP\",\n    \"navn2\": null,\n    \"navn3\": null,\n    \"f_adresse1\": \"Testvegen 19\",\n    \"f_adresse2\": null,\n    \"f_adresse3\": null,\n    \"f_postnr\": \"0019\",\n    \"f_poststed\": \"OSLO\",\n    \"maalform\": null,\n    \"kontaktperson\": \"OLA NORDMANN\",\n    \"kont_telefon\": \"12121212\",\n    \"kont_mobiltlf\": null,\n    \"kont_epost\": null,\n    \"h_var1_n\": null,\n    \"h_var2_n\": null,\n    \"h_var3_n\": null,\n    \"h_var1_a\": null,\n    \"h_var2_a\": null,\n    \"h_var3_a\": null,\n    \"utvalgsstatus\": null,\n    \"pulje_nr\": 2,\n    \"vedtak_tvmulkt\": \"N\",\n    \"sendt_si\": \"N\",\n    \"status\": null,\n    \"org_form\": null,\n    \"sn07_1\": \"17.120\",\n    \"str_kode\": null,\n    \"viktig_enhet\": null,\n    \"kommentar_int\": null,\n    \"kommentar_ekst\": null,\n    \"test_pulje\": null,\n    \"prosedyre\": null,\n    \"felles_oppgave\": null\n}",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "SFU"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-editor.html",
    "href": "statistikkere/datadoc-editor.html",
    "title": "Datadoc-editor",
    "section": "",
    "text": "Datadoc editor er et grafisk grensesnitt for å dokumentere datasett og variablene som utgjør datasettet.\nFormålet med tjenesten er å tilby et lett-å-bruke grensesnitt som hovedsakelig vil benyttes første gang man dokumenterer en type datasett.\nSiden løpende statistikkproduksjon ofte innebærer at nye data legges til data fra tidligere perioder, uten at strukturen i datasett endres, så tilbys det også et annet verktøy som lar brukeren programmatisk gjenbruke metadata fra en tidligere periode. Les mer om Python-pakken dapla-toolbelt-metadata.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Datadoc-editor"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-editor.html#forberedelser",
    "href": "statistikkere/datadoc-editor.html#forberedelser",
    "title": "Datadoc-editor",
    "section": "Forberedelser",
    "text": "Forberedelser\nFør man starter Datadoc-editor-tjenesten bør man ha lest kapitlet om Dapla Lab. Deretter gjør du følgende:\n\nLogg deg inn på Dapla Lab\nUnder Tjenestekatalog trykker du på Start-knappen for Datadoc-editor\nGi tjenesten et navn\nÅpne Datadoc-editor konfigurasjoner og gjør ønskede konfigurasjoner (se neste kapittel).\nTrykk Start igjen for å åpne tjenesten.\n\nDatadoc editor bruker ca. 1 minutt på starte og etter det klart for dokumentere datasett.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Datadoc-editor"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-editor.html#konfigurasjon",
    "href": "statistikkere/datadoc-editor.html#konfigurasjon",
    "title": "Datadoc-editor",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nFør man starter Datadoc editor bør man konfigurere tjenesten. Dette er spesielt viktig siden du bare kan representere et Dapla-team for hver Datadoc editor man starter. I tjenestekonfigurasjonen til Datadoc er det to nedtrekksmenyer: Data og Tjeneste.\n\nData\nUnder Data kan man velge Team og tilgangsgruppe. I denne menyen får du listet alle team og tilgangsgrupper du er med i. Listen vises på formen &lt;daplateam&gt;-&lt;tilgangsgruppe&gt;.\nFigur 1 viser tilfellet der det er valgt å representere tilgangsgruppen developers i teamet Dapla Felles, derav dapla-felles-developers. Dette er standardvalget.\n\n\n\n\n\n\nFigur 1: Data-menyen i tjenestekonfigurasjonen for Datadoc editor.\n\n\n\nDatadoc editor støtter for øyeblikket ikke kildedata selv om man kan velge begrunnelse og tilgangvarighet fra konfigurasjons fanen.\n\n\nTjeneste\nUnder menyen Tjeneste kan man velge versjon av tjenesten. Det vil være svært sjelden at brukere trenger å endre på noe her. Som standard åpnes alltid siste versjon av tjenesten.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Datadoc-editor"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-editor.html#datatilgang",
    "href": "statistikkere/datadoc-editor.html#datatilgang",
    "title": "Datadoc-editor",
    "section": "Datatilgang",
    "text": "Datatilgang\nNår man starter en Datadoc-editor tjeneste må man på forhånd velge hvilket team og tilgangsgruppe man skal representere, som forklart i forrige del.\n\n\n\n\n\n\nWarningdata-admins ikke tilgjengelig enda\n\n\n\nDet er ikke mulig å velge andre tilgangsgrupper enn developers for øyeblikket. Av den grunn kan man ikke bruke Datadoc editor til å dokumentere kildedata enda.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Datadoc-editor"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-editor.html#funksjonalitet",
    "href": "statistikkere/datadoc-editor.html#funksjonalitet",
    "title": "Datadoc-editor",
    "section": "Funksjonalitet",
    "text": "Funksjonalitet\n\nÅpne datasett\nFør man kan benytte Datadoc editor, må man åpne et Datasett. Det gjøres enkelt ved å lime inn stien til datasettet i Filsti tekstboksen (Punkt 1 i Figur 2) og trykke på Åpne fil knappen (Punkt 2 i Figur 2).\nDatadoc editor benytter brukerens innloggingsopplysninger for å aksessere data. Det betyr at man i utgangspunktet har tilgang til de samme filene som ellers på Dapla.\n\n\n\n\n\n\nWarning\n\n\n\nMan må inkludere gs:// på begynnelsen av stien når man jobber med et datasett i en bøtte.\n\n\n\n\n\n\n\n\nNote\n\n\n\nMan kan finne filstien gjennom Google konsollet eller ved å benytte Dapla toolbelt\n\n\n\n\n\n\n\n\nFigur 2: Input-feltet for å oppgi filsti i Datadoc editor.\n\n\n\n\nVellykket åpning\nEtter at man har trykket på Åpne fil knappen bør man se meldingen vist i Figur 3\n\n\n\n\n\n\nFigur 3: Meldingen at det var velykket å åpne datasettet.\n\n\n\n\n\nVellykket åpning med advarsel\nHvis man åpner et datasett som ikke følger navnestandarden, vil det komme en advarsel (Figur 4). Det er fortsatt fullt mulig å bruke Datadoc editor for å dokumentere datasettet, men ikke like mye metadata kan utledes automatisk (TODO: lenke til seksjonen om utledning).\n\n\n\n\n\n\nNote\n\n\n\nDette kan være en fin anledning til å justere på navngivning og strukturen i teamets bøtter slik at alt følger navnestandarden. Det er en lenke til navnestandarden i meldingen.\n\n\n\n\n\n\n\n\nFigur 4: Meldingen at datasettet ikke følger navnestandarden.\n\n\n\n\n\nFeil ved åpning\nHvis Datadoc editor ikke klarer å åpne datasettet vises en rød error melding (Figur 5). Som oftest forårsakes dette av at filen ikke finnes (skrivefeil) eller fordi man ikke har tilgang til filen.\n\n\n\n\n\n\nFigur 5: Meldingen at det var en feil ved åpning av datasettet.\n\n\n\n\n\nÅpne et datasett når metadatadokument eksisterer\nHvis et metadatadokument eksisterer, er det denne informasjonen som lastes inn. Det utledes ingenting fra datasettet.\n\n\n\nUtledet informasjon\nInformasjon som kan utledes vil bli fylt inn når du åpner datasettet. Informasjonen hentes enten fra filstien eller settes inn som en default verdi (*). Det er mulig å korrigere informasjonen i ettertid. Følgende felter blir forsøkt utledet:\nDatasett:\n\nVerdivurdering\nStatus (*)\nDatatilstand\nVersjon\nStatistikkområde\nInneholder data f.o.m.\nInneholder data t.o.m.\nGeografisk dekningsområde (*)\n\nVariabler:\n\nKortnavn\nDatatype\n\n\n\nDokumentere datasett-metadata\nDokumentasjon av datasettet som helthet gjøres i datasettfanen i Datadoc editor.\nAlle felter har en ordforklaring  du kan trykke på. Her vil du få en kort forklaring til hva som skal stå i feltet.\nFlere felter har verdilister hvor mange er hentet fra KLASS, mens noen er fritekstfelter. For noen av fritekstfeltene gjøres det en sjekk av innholdet og du vil få en feilmelding hvis kriteriene ikke er oppfylt.\n\nObligatorisk\nAlt som står under obligatorisk må fylles inn.\n\n\nAnbefalt\nAnbefalte felter er frivillig å fylle ut.\n\n\nMaskingenerert\nFeltene her genereres automatisk og kan ikke redigeres. De er kun med til informasjon.\n\n\n\nDokumentere variabelforekomst-metadata\nDokumentasjon av variabelforekomster for et datasett kan gjøres i variabelfanen i Datadoc editor. Her vil man se en liste av alle kortnavnene til variabelforekomstene i datasettet. Ved å trykke seg inn på et av kortnavnene kan man dokumentere de obligatoriske og anbefalte feltene for en variabelforekomst.\n\nArv mellom datasett og variabelforekomst fanen\nFor å forenkle dokumentasjonen av variabelforekomster vil noen felt arve verdiene som blir satt i datasettfanen. Dette gjelder følgende felter:\n\nDatakilde\nPopulasjon\nTemporalitetstype\nInneholder data f.o.m\nInneholder data t.o.m\n\nDet er mulig å redigere vediene i variabelforekomst fanen etter en verdi er satt i datasettfanen. Hvis disse feltene blir endret i datasettfanen senere, vil de alltid overskrive det som er satt i variabelforekomst fanen.\n\n\nSøk i variabelforekomster\nDet er mulig å søke gjennom variabelforekomstene sine kortnavn. Dette filtrerer på listen over variabelforekomster.\n\n\n\n\n\n\nFigur 6: Søk gjennom kortnavn til variabelforekomster\n\n\n\n\n\n\nLagre metadata\n\n\n\n\n\n\nWarningViktig informasjon\n\n\n\nDatadoc editor mellomlagrer ikke utfylt metadata.\nPass på å lagre metadataene ofte ved å trykke lagre og legg merke til om du får en bekreftelse på at metadataene er lagret.\n\n\n\nVed lagring\nNår du trykker Lagre metadata knappen vil du få en bekreftelse på vellykket lagring. \nHvis ikke alle obligatoriske felt er utfylt vil du få opp en advarsel for datasett og variabelforekomstene. Advarselen for datasett viser en liste over hvilke felt som mangler. For variabelforekomster vises både variabelens kortnavn og manglende felt.\nNår du fyller ut de manglende obligatoriske feltene må du lagre på nytt og advarslene vil forsvinne når alle obligatoriske felt er fylt ut.\nVed lagring gjøres det også en sjekk på om variabel kortnavene avviker for navnestandarden for variabelnavn. Om det finnes avvik vil disse kortnavene vises i en gul advarsel boks. Navnestandarden for variabelkortnavn er som følger:\n\nAlfanumerisk begrenset til a-z (kun små bokstaver)\n0-9\n_ (understrek).\n\n\n\nMetadata filen\nNår du trykker på Lagre metadata knappen i Datadoc editor skrives alle metadata til en fil i samme mappe (katalog) som datafilen. Dette er en JSON-fil med nesten samme navn som datafilen. Navnekonvensjonen for metadatafilen er\n&lt;navn på datasettfilen uten endelse&gt;__DOC.json\nEksempelvis vil Datadoc lagre metadata i filen skattedata_p2022_v1__DOC.json hvis datafilen har navnet skattedata_p2022_v1.parquet.\nFordelen med å benytte en JSON-fil til å lagre metadata er at denne filen kan kopieres og flyttes like enkelt som selve datafilen. JSON-filer er strukturerte tekstfiler som kan leses av både maskiner (Python/R) og av mennesker (åpnes i en tekst-editor).\nSe et eksempel på JSON metadata-fil lagret av DataDoc.\n\n\n\nModifisere metadata\nØnsker du å endre eller legge til metadata, åpner du et datasett slik som beskrevet i Åpne et datasett. Da vil innholdet fra metadata-filen leses inn i Datadoc editor og kan redigeres videre. Endringene blir lagret når man trykker Lagre metadata.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Datadoc-editor"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-editor.html#kildekode",
    "href": "statistikkere/datadoc-editor.html#kildekode",
    "title": "Datadoc-editor",
    "section": "Kildekode",
    "text": "Kildekode\nKildekoden til Datadoc editor er offentlig tilgjengelig på Github: https://github.com/statisticsnorway/datadoc",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Datadoc-editor"
    ]
  },
  {
    "objectID": "statistikkere/appendix/spark.html",
    "href": "statistikkere/appendix/spark.html",
    "title": "Apache Spark",
    "section": "",
    "text": "Koding i R og Python har historisk sett foregått på en enkelt maskin og vært begrenset av minnet (RAM) og prosessorkraften på maskinen. For bearbeiding av små og mellomstore datasett er det sjelden et problem på kjøre på en enkelt maskin. Populære pakker som dplyr for R, og pandas for Python, blir ofte brukt i denne typen databehandling. I senere år har det også kommet pakker som er optimalisert for å kjøre kode parallelt på flere kjerner på en enkelt maskin, skrevet i minne-effektive språk som Rust og C++.\nMen selv om man kommer langt med å kjøre kode på en enkelt maskin, vil enkelte oppgaver kreve mer minne og prosessorkraft enn dette. For større datasett, eller store beregninger, kan det være nyttig å bruke et rammeverk som kan kjøre kode parallelt på flere maskiner. Et slikt rammeverk er Apache Spark.\nApache Spark er et rammeverk for å kjøre kode parallelt på flere maskiner. Det er bygget for å håndtere store datasett og store beregninger. Det er derfor et nyttig verktøy for å løse problemer som er for store for å kjøre på en enkelt maskin. Men det finnes også andre bruksområder som er nyttige for Apache Spark. Her er noen eksempler:\nDette er noen av bruksområdene der Spark kan løse problemer som er for store for å kjøre på en enkelt maskin med for eksempel Pandas eller dplyr.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Apache Spark"
    ]
  },
  {
    "objectID": "statistikkere/appendix/spark.html#spark-på-dapla",
    "href": "statistikkere/appendix/spark.html#spark-på-dapla",
    "title": "Apache Spark",
    "section": "Spark på Dapla",
    "text": "Spark på Dapla\nDapla kjører på et Kubernetes-kluster og er derfor er et svært egnet sted for å kjøre kode parallelt på flere maskiner. Jupyter på Dapla har også en flere klargjorte kernels for å kjøre kode i Apache Spark. Denne koden vil kjøre på et eget kluster av maskiner som er dedikert til Apache Spark, slik som vist i Figur 1.\n\n\n\n\n\n\n\n\n\n\n\n(a) PySpark på kubernetes\n\n\n\n\n\n\n\n\n\n\n\n(b) PySpark på 1 maskin\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) SparkR på kubernetes\n\n\n\n\n\n\n\n\n\n\n\n(d) SparkR på 1 maskin\n\n\n\n\n\n\n\nFigur 1: Ferdigkonfigurerte kernels for Spark på Dapla.\n\n\n\nFigur 1 (a) og Figur 1 (c) kan velges hvis du ønsker å bruke Spark for å kjøre store jobber på flere maskiner, for henholdsvis Python- og R-grensesnittene for Spark.\nFigur 1 (b) og Figur 1 (d) bør du velge hvis du ønsker å bruke Spark av andre grunner enn å kjøre store jobber på flere maskiner. For eksempel hvis du ønsker å bruke en av de mange pakker som er bygget på Spark, eller hvis du ønsker å bruke Spark til å lese og skrive data fra Dapla.\nHvis du ønsker å sette opp et eget virtuelt miljø for å kjøre Spark, så kan du bruke ssb-project. Se ssb-project for mer informasjon.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Apache Spark"
    ]
  },
  {
    "objectID": "statistikkere/appendix/spark.html#spark-i-og-python",
    "href": "statistikkere/appendix/spark.html#spark-i-og-python",
    "title": "Apache Spark",
    "section": "Spark i  og Python",
    "text": "Spark i  og Python\nSpark er implementert i programmeringsspråket Scala. Men det tilbys også mange grensesnitt for å bruke Spark fra andre språk. De mest populære grensesnittene er PySpark for Python og SparkR for R. Disse grensesnittene er bygget på Spark, og gir tilgang til Spark-funksjonalitet fra Python og R.\n\nPySpark\nPySpark er et Python-grensesnitt for Apache Spark. Det er en Python-pakke som gir tilgang til Spark-funksjonalitet fra Python. Det er enkelt å bruke, og har mange av de samme funksjonene som Pandas.\nUnder ser du datasettet som benyttes for i vedlagt notebook pyspark-intro.ipynb. Den viser hvordan man kan gjøre vanlige databehandling med PySpark. I eksempelet brukes kernel som vist i Figur 1 (b).\n\n\n\nCode\n# Legger til row index til DataFrame før join med dates_df\ndata_df = data_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n\n# Joiner de to datasettene\ndf = (\n    dates_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n    .join(data_df, \"row_index\")\n    .drop(\"row_index\")\n)\n\n# Legger til år, kvartal og mnd\ndf = df.withColumn(\"Year\", date_format(df.Date, \"yyyy\"))\ndf = df.withColumn(\"Quarter\", expr(\"quarter(Date)\"))\ndf = df.withColumn(\"Month\", date_format(df.Date, \"MM\"))\n\ndf.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows\n\n\n\nSource: Introduksjon til PySpark\nDet finnes også et Pandas API/grensesnitt mot Spark. Målet med en er å gjøre overgangen fra Pandas til Spark lettere for nybegynneren. Men hvis man skal gjøre litt mer avansert databehandling anbefales det at man bruker PySpark direkte og ikke Pandas API-et.\n\n\nSparkR\nSparkR er et R-grensesnitt for Apache Spark. Det er en R-pakke som gir tilgang til Spark-funksjonalitet fra R. Det er enkelt å bruke, og har mange av de samme funksjonene som dplyr. Se eksempel i notebook under:\n\n\nselectedColumns &lt;- select(file, \"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\")\nshowDF(selectedColumns, numRows = 5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows\n\n\nSource: Introduksjon til SparkR",
    "crumbs": [
      "Manual",
      "Appendix",
      "Apache Spark"
    ]
  },
  {
    "objectID": "statistikkere/appendix/spark.html#lakehouse-arkitektur",
    "href": "statistikkere/appendix/spark.html#lakehouse-arkitektur",
    "title": "Apache Spark",
    "section": "Lakehouse-arkitektur",
    "text": "Lakehouse-arkitektur\n\n\n\n\n\n\nWarning\n\n\n\nI denne delen viser vi hvordan funksjonalitet som kan bli relevant for SSB å benytte seg av i fremtiden. Men det er fortsatt under testing og ta det i betraktning før man eventuelt implementerer dette i produksjon.\n\n\nEn av utvidelsene som er laget rundt Apache Spark er den såkalte Lakehouse-arkitekturen. Kort fortalt kan den dekke behovene som et klassisk datavarehus har tatt seg av tidligere. I kontekst av SSB sine teknologier kan det også benyttes som et databaselag over Parquet-filer i bøtter. Det finnes flere open source løsninger for dette, men mest aktuelle er:\n\nDelta Lake\nApache Hudi\nApache Iceberg\n\nI det følgende omtaler vi hovedsakelig egenskapene til Delta Lake, men alle rammeverkene har mange av de samme egenskapene. Delta Lake kan også benyttes på Dapla nå.\nSentrale egenskaper ved Delta Lake er:\n\nACID-transactions som sikrer data-integritet og stabilitet, også når det skjer feil.\nMetadata som bli håndtert akkurat som all annen data og er veldig skalebar. Den støtter også egendefinert metadata.\nSchema Enforcement and Evolution sikrer at skjemaet til dataene blir håndhevet, og den tillater også den kan endres over tid.\nTime travel sikrer at alle versjoner av dataene er blitt lagret, og at du kan gå tilbake til tidligere versjoner av en fil.\nAudit history sikrer at du kan få full oversikt over hvilke operasjoner som utført på dataene.\nInserts, updates and deletes betyr at du lettere kan manipulere data enn hva som er tilfellet med vanlige Parquet-filer.\nIndexing er støttes for forbedre spørringer mot store datamengder.\n\nI vedlagt notebook deltalake-intro.ipynb finner du blant annet eksempler på hvordan du legger til følgende metadata i spesifikk versjon av en fil:\n\n\n\nCode\n# Lagrer egendefinert metadata i en json-fil\nimport json\n\nmetadata = {\n    \"comment\": \"Kontaktet oppgavegiver og kranglet!\",\n    \"manueltEditert\": \"True\",\n    \"maskineltEditert\": \"False\",\n}\nmetadata\n\n\n{'comment': 'Kontaktet oppgavegiver og kranglet!',\n 'manueltEditert': 'True',\n 'maskineltEditert': 'False'}\n\n\nSource: Introduksjon til Delta Lake",
    "crumbs": [
      "Manual",
      "Appendix",
      "Apache Spark"
    ]
  },
  {
    "objectID": "statistikkere/appendix/contribution.html",
    "href": "statistikkere/appendix/contribution.html",
    "title": "Hvordan bidra til Dapla-manualen",
    "section": "",
    "text": "Alle i SSB kan bidra til denne manualen. Endringer må godkjennes av noen i Team Statistikktjenester eller Alex Crozier (akc@ssb.no). Si gjerne i fra at det ligger en PR å se på. Vi trenger bidrag med alt fra språkvask, nye artikler og andre gode initiativer! Har du lyst til å bidra, men er ikke helt sikker på hva du kan bidra med? Ta en titt på issues i GitHub-repoet.\nGjør du nyhetsverdige endringer må du også lage et nyhetsinnlegg i nyhetsdelen av manualen. Mer om det lenger nede.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Hvordan bidra til Dapla-manualen"
    ]
  },
  {
    "objectID": "statistikkere/appendix/contribution.html#forutsetninger",
    "href": "statistikkere/appendix/contribution.html#forutsetninger",
    "title": "Hvordan bidra til Dapla-manualen",
    "section": "Forutsetninger",
    "text": "Forutsetninger\n\nMan trenger basis git kompetanse, det ligger en fin beskrivelse av det på Beste Praksis siden fra KVAKK.\nMan trenger en konto på Github, det kan man opprette ved å følge instruksjonene her.\nMan kan lære seg Markdown (enkel tekst formatering), en fin oversikt finnes her.\nVerktøyet Quarto burde installeres for å kunne se endringene slik som de ser ut på nettsiden. Installasjon instruksjoner finnes her.\n\n\nTittelblock\nI dapla-manualen har vi noen konvensjoner vi følger. En av de er å lage en title block som det kalles på quarto sin nettside.\nVi ber våre bidragsytere om å lage en lik block med overskrift og datoen artikkelen ble endret. Åpner man filen til denne siden ser det på dette tidspunktet slik ut:\n\n\n\n\n\n\nFigur 1: Tittelblock for denne siden.\n\n\n\n\n\n\n\n\n\nTipInterne lenker\n\n\n\nNår man skal lenke til andre artikler i manualen bør gjør man det slik: [artikkel om ssb-project](../ssb-project.qmd)\nLegg merke til at vi lenker til .qmd-filen, ikke den genererte .html-filen, ei heller manual.dapla.ssb.no\n\n\n\n\nNyhetsinnlegg\nManualens egen nyhetssiden skal oppdaters med et innlegg en nyhetsverdig endring på Dapla skjer - for eksempel når manualen får en ny artikkel. Det er ikke nødvendig å lage en sak om at en side har blitt oppdatert, med mindre endringene er omfattende.\nNyhetssiden er i Quarto sitt blog-format. Fremgangsmåten er enkel og beskrives i quarto sin artikkel om blogger. Ellers anbefales det å ta en titt på hvordan det gjøres i dapla-manualen. Alex Crozier (akc@ssb.no) er ansvarlig for nyhetssiden og kan kontaktes dersom man trenger hjelp.\n\n\n\n\n\n\nTipHvordan skrive nyhetsinnlegg?\n\n\n\nUtover det tekniske er det særlig tre ting å tenke på:\nKategoriser innlegget. Om du skriver en ny artikkel bør du kategorisere innlegget med ‘dapla-manual’, slik det har blitt gjort i dette nyhetsinnlegget.\nBruk emojier for å kategorisere For å gjøre det enda lettere for lesere bruker vi emojier i starten av overskriften som sier noe om nyhetens natur. Trykk win + . for å få opp emojier. Her er emojiene vi tar i bruk og deres betydning:\n📄 Ny dokumentasjon (f.eks artikkel i manualen) 🚀 Endringer på Dapla lab 🐍 Python 🐍📚 Pythonpakke ❗ Driftsmelding 🥳 Ny lansering\nEr du usikker på hvilken emoji du skal bruke kan det fint droppes - eller så kan man ta i bruk nye emojier.\nSkriv kort og ha en forklarende overskrift\n\n\n\n\nEmbedded notebooks\nQuarto tilbyr å legge ved (embed) notebooks inn i nettsiden. Dette er en fin måte å dele kode og output på. Men det krever at vi tenker gjennom hvor outputen genereres. Siden Dapla-manualen renderes med GitHub-action, så ønsker vi ikke å introdusere kompleksiteten det innebærer å generere output fra kode her. I tillegg er det mange miljø-spesifikke konfigurasjoner som bestemmer hvordan output kan genereres. Derfor anbefaler vi følgende tilnærming når man skal legge ved en notebook til et kapittel:\n\nLogg deg inn i miljøet du ønsker å bruke.\nKlone ned repoet til Dapla-manualen.\nOpprett en notebook i mappen ./dapla-manual/notebooks/notebook-navn.ipynb\nSkriv kode og generer output som du ønsker. Husk å bare bruke data som alle har tilgang til eller de kan generere selv.\nEmbed output i kapitlet du ønsker iht til denne beskrivelsen\nPå toppen av notebooken lager du en raw-celle der du skriver:\n\n\n\nnotebook\n\n---\nfreeze: true\n---\n\n\nKjør denne notebooken fra terminalen med kommandoen:\n\n\n\nterminal\n\nquarto render ./notebooks/&lt;notebook-navn.ipynb&gt; --execute\n\nDette sikrer at vi kan se output etter at boken bygges i GitHub Actions. Husk at notebooks ikke blir strippet for output på vanlig måte, slik at kun åpne data skal benyttes.\nSpør Team Statistikktjenester eller Alex (akc@ssb.no) om du lurer på noe.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Hvordan bidra til Dapla-manualen"
    ]
  },
  {
    "objectID": "statistikkere/appendix/contribution.html#teknisk-remgangsmåte",
    "href": "statistikkere/appendix/contribution.html#teknisk-remgangsmåte",
    "title": "Hvordan bidra til Dapla-manualen",
    "section": "Teknisk remgangsmåte",
    "text": "Teknisk remgangsmåte\n\nKlone repoet\n\n\n\nterminal\n\ngit clone https://github.com/statisticsnorway/dapla-manual.git\n\n\nLage en ny gren\nGjøre endringen\nKjør følgende og følge lenken for å sjekke at alt ser bra ut på nettsiden:\n\n\n\nterminal\n\nquarto preview dapla-manual\n\n\nÅpne en PR\nBe noen å gjennomgå endringen og eventuelt godkjenne den\nMerge PRen\nEtter et par minutter skal din endring være synlig!",
    "crumbs": [
      "Manual",
      "Appendix",
      "Hvordan bidra til Dapla-manualen"
    ]
  },
  {
    "objectID": "statistikkere/appendix/dashboard.html",
    "href": "statistikkere/appendix/dashboard.html",
    "title": "Dash og dashboard",
    "section": "",
    "text": "Hvis en ønsker å lage ett dashbord som et brukergrensesnitt, så kan pakken Dash være et godt alternativ. Dash er ett rammeverk hvor man selv kan bygge opp applikasjoner i form av dashbord på en enklere måte, og det bygges oppå javascript pakker som plotly.js og react.js. Det er et produkt ved siden av og helintegrert med plotly, som også er en annen pakke i Python som gir oss interaktive grafer. Dash er et godt verktøy hvis en ønsker et dashbord som brukergrensesnitt for interaktiv visualisering av data. Dash kan kodes i Python og R, men også Julia og F#.\nI SSB kan man lage dashbord i virtuelle miljøer satt opp med ssb-project. For mer om håndtering av pakker i et virtuelt miljø satt opp med ssb-project kan man se nærmere på artikkelen vår om ssb-project.",
    "crumbs": [
      "Manual",
      "Appendix"
    ]
  },
  {
    "objectID": "statistikkere/appendix/dashboard.html#eksempel-kode-i-daplalab",
    "href": "statistikkere/appendix/dashboard.html#eksempel-kode-i-daplalab",
    "title": "Dash og dashboard",
    "section": "Eksempel kode i DaplaLab",
    "text": "Eksempel kode i DaplaLab\nI DaplaLab kan du starte opp ett dashbord ved hjelp av dash pakken enten i vscode-python tjenesten, eller i en notebook i jupyter tjenesten. Det fungerer best å kjøre Dash-apper i en egen fane i nettleseren.\n\nvscode-python scriptjupyter notebookjupyter script\n\n\nHer er et eksempel på hvordan man lager en Dash-app i DaplaLab i en vscode tjeneste:\n\nInstaller pandas og dash i et ssb-project.\nOpprett et python-script i prosjektet som f.eks. heter app.py.\nKjør scriptet ved å kjøre følgende kommando fra terminalen: poetry run python ./app.py\nDeretter kommer det opp et dialog-vinduet hvor du velger Open in browser.\n\nHer er et eksempel på script som fungerer i Vscode-python:\n\n\napp.py\n\nfrom dash import Dash, html, dcc, callback, Output, Input\nimport plotly.express as px\nimport pandas as pd\n\n# Creating a sample dataset similar to the Gapminder dataset\ndata = {\n    'country': ['Afghanistan', 'Afghanistan', 'Afghanistan', 'Brazil', 'Brazil', 'Brazil', 'China', 'China', 'China'],\n    'continent': ['Asia', 'Asia', 'Asia', 'South America', 'South America', 'South America', 'Asia', 'Asia', 'Asia'],\n    'year': [1952, 1977, 2007, 1952, 1977, 2007, 1952, 1977, 2007],\n    'lifeExp': [28.801, 39.877, 43.828, 50.917, 59.504, 72.390, 44.000, 63.739, 72.961],\n    'pop': [8425333, 12412311, 31889923, 56602560, 80461570, 190010647, 556263527, 862030000, 1318683096],\n    'gdpPercap': [779.4453145, 786.11336, 974.5803384, 2108.944355, 5718.766744, 9065.800825, 400.448611, 1488.041713, 4959.114854]\n}\n\n# Creating DataFrame\ndf = pd.DataFrame(data)\n\n# Creating the app\nport = 8050 # must be between 1024 and 9999, default is 8050\napp = Dash(\n    __name__,\n    requests_pathname_prefix=f'/proxy/{port}/', \n    serve_locally=True\n)\n\napp.layout = [\n    html.H1(children='Title of Dash App', style={'textAlign':'center'}),\n    dcc.Dropdown(df.country.unique(), 'Canada', id='dropdown-selection'),\n    dcc.Graph(id='graph-content')\n]\n\n@callback(\n    Output('graph-content', 'figure'),\n    Input('dropdown-selection', 'value')\n)\ndef update_graph(value):\n    dff = df[df.country==value]\n    return px.line(dff, x='year', y='pop')\n\nif __name__ == '__main__':\n    app.run(port=port, debug=True)\n\nFor å stoppe dashbordet fra å kjøre, trykker du i terminalen ctrl + c.\n\n\nHer er et eksempel på hvordan man lager en Dash-app i DaplaLab i en jupyter tjeneste:\n\nInstaller pandas og dash i et ssb-project.\nOpprett en notebook i prosjektet som f.eks. heter app.ipynb.\nÅpne notebooken og kjør kodecellene på vanlig måte.\n\nHer er et eksempel på kode i notebook som fungerer i jupyter:\n\n\napp.ipynb\n\n# %%\n# Notebook cell 1\nfrom dash import Dash, html, dcc, callback, Output, Input\nimport plotly.express as px\nimport pandas as pd\nimport os\n\n# %%\n# Notebook cell 2\n# Creating a sample dataset similar to the Gapminder dataset\ndata = {\n    'country': ['Afghanistan', 'Afghanistan', 'Afghanistan', 'Brazil', 'Brazil', 'Brazil', 'China', 'China', 'China'],\n    'continent': ['Asia', 'Asia', 'Asia', 'South America', 'South America', 'South America', 'Asia', 'Asia', 'Asia'],\n    'year': [1952, 1977, 2007, 1952, 1977, 2007, 1952, 1977, 2007],\n    'lifeExp': [28.801, 39.877, 43.828, 50.917, 59.504, 72.390, 44.000, 63.739, 72.961],\n    'pop': [8425333, 12412311, 31889923, 56602560, 80461570, 190010647, 556263527, 862030000, 1318683096],\n    'gdpPercap': [779.4453145, 786.11336, 974.5803384, 2108.944355, 5718.766744, 9065.800825, 400.448611, 1488.041713, 4959.114854]\n}\n\n# Creating DataFrame\ndf = pd.DataFrame(data)\n\n# Creating the app\nport = 8050 # must be between 1024 and 9999\nservice_prefix = os.getenv('JUPYTERHUB_SERVICE_PREFIX', '/')\ndomain = os.getenv('JUPYTERHUB_HTTP_REFERER', None)\n\napp = Dash(\n    __name__,\n    requests_pathname_prefix=f\"{service_prefix}proxy/{port}/\", \n    serve_locally=True\n)\n\napp.layout = [\n    html.H1(children='Title of Dash App', style={'textAlign':'center'}),\n    dcc.Dropdown(df.country.unique(), 'Canada', id='dropdown-selection'),\n    dcc.Graph(id='graph-content')\n]\n\n@callback(\n    Output('graph-content', 'figure'),\n    Input('dropdown-selection', 'value')\n)\ndef update_graph(value):\n    dff = df[df.country==value]\n    return px.line(dff, x='year', y='pop')\n\nif __name__ == '__main__':\n    # For jupyter_mode, choose between 'external' or 'inline'.\n    # 'jupyterlab' should also be poosible, but doesn't seem to work...\n    app.run(debug=True, jupyter_mode=\"external\", jupyter_server_url=domain, port=port)\n\nFor å stoppe dashbordet fra å kjøre, restarter du kernelen i jupyterlab: Kernel -&gt; Restart Kernel and Clear Outputs of All Cells...\n\n\nHer er et eksempel på hvordan man lager en Dash-app i DaplaLab i en jupyter tjeneste:\n\nInstaller pandas og dash i et ssb-project.\nOpprett et python-script i prosjektet som f.eks. heter app.py.\nKjør scriptet ved å kjøre følgende kommando fra terminalen: poetry run python app.py\nDeretter dukker det opp en link i terminalen etter teksten ‘Dash is running on’ som du kan trykke på for å få opp dashbordet.\n\nHer er et eksempel på script som fungerer i jupyter:\n\n\napp.py\n\nfrom dash import Dash, html, dcc, callback, Output, Input\nimport plotly.express as px\nimport pandas as pd\nimport os\n\n# Creating a sample dataset similar to the Gapminder dataset\ndata = {\n    'country': ['Afghanistan', 'Afghanistan', 'Afghanistan', 'Brazil', 'Brazil', 'Brazil', 'China', 'China', 'China'],\n    'continent': ['Asia', 'Asia', 'Asia', 'South America', 'South America', 'South America', 'Asia', 'Asia', 'Asia'],\n    'year': [1952, 1977, 2007, 1952, 1977, 2007, 1952, 1977, 2007],\n    'lifeExp': [28.801, 39.877, 43.828, 50.917, 59.504, 72.390, 44.000, 63.739, 72.961],\n    'pop': [8425333, 12412311, 31889923, 56602560, 80461570, 190010647, 556263527, 862030000, 1318683096],\n    'gdpPercap': [779.4453145, 786.11336, 974.5803384, 2108.944355, 5718.766744, 9065.800825, 400.448611, 1488.041713, 4959.114854]\n}\n\n# Creating DataFrame\ndf = pd.DataFrame(data)\n\n# Creating the app\nport = 8050 # must be between 1024 and 9999, default is 8050\nservice_prefix = os.getenv('JUPYTERHUB_SERVICE_PREFIX', '/')\ndomain = os.getenv('JUPYTERHUB_HTTP_REFERER', None)\nservice = f\"{service_prefix}proxy/{port}/\"\nurl = f\"{domain}{service[1:]}\"\ndefault_host = f\"http://127.0.0.1:{port}{service}\"\n\napp = Dash(\n    __name__,\n    requests_pathname_prefix=service,\n    serve_locally=True\n)\n\napp.layout = [\n    html.H1(children='Title of Dash App', style={'textAlign':'center'}),\n    dcc.Dropdown(df.country.unique(), 'Canada', id='dropdown-selection'),\n    dcc.Graph(id='graph-content')\n]\n\n@callback(\n    Output('graph-content', 'figure'),\n    Input('dropdown-selection', 'value')\n)\ndef update_graph(value):\n    dff = df[df.country==value]\n    return px.line(dff, x='year', y='pop')\n\nif __name__ == '__main__':\n    app.run(proxy = default_host + \"::\" + url, port=port, debug=True)\n\nFor å stoppe dashbordet fra å kjøre, trykker du i terminalen ctrl + c.",
    "crumbs": [
      "Manual",
      "Appendix"
    ]
  },
  {
    "objectID": "statistikkere/appendix/dashboard.html#aktuell-dokumentasjon",
    "href": "statistikkere/appendix/dashboard.html#aktuell-dokumentasjon",
    "title": "Dash og dashboard",
    "section": "Aktuell dokumentasjon",
    "text": "Aktuell dokumentasjon\nDiverse som er verdt å se nærmere på når en bygger dashbord applikasjon med Dash. Det følger med mange gode eksempler for bruk av diverse komponenter i dokumentasjonene under her.\n\nStartsiden til Dash\nLage interaktive grafer i Python med Plotly\nDash Core Components\nDash HTML Components\nDash Bootstrap Components\n\nVel og merke henter denne diverse materialer fra internett, og vil ikke fungere uten tilgang. Det er heller anbefalt å ha de nødvendige filene lagret lokalt for bruk av denne pakken.\nPakken i seg selv har en fordel i at det er lettere å bygge opp utseende(layout) i dashbordet ditt, samt andre komponenter som ikke ligger i standard dash pakken.\n\nDash SSB Components\n\nTeam Metadata lager SSB komponenter i Dash, noe Datadoc er lagd med. Dette gir deg muligheten til å bruke SSB komponentene i dine egne dashbord. Vel og merke er denne pakken fortsatt under utvikling, og ikke alle komponenter er på plass.",
    "crumbs": [
      "Manual",
      "Appendix"
    ]
  },
  {
    "objectID": "statistikkere/appendix/kartdata.html",
    "href": "statistikkere/appendix/kartdata.html",
    "title": "Kartdata",
    "section": "",
    "text": "Det er tilrettelagt kartdata i Dapla til analyse, statistikkproduksjon og visualisering. Det er de samme dataene som er i GEODB i vanlig produksjonssone. Kartdataene er lagret på ssb-areal-data-delt-kart-prod. De er lagret som parquetfiler i standardprojeksjonen vi bruker i SSB (UTM sone 33N) hvis ikke annet er angitt. Det er også SSBs standard-rutenett i ulike størrelser samt Eurostats rutenett over Norge.\nI tillegg ligger det noe testdata i fellesbøtta her: ssb-dapla-felles-data-produkt-prod/GIS/testdata",
    "crumbs": [
      "Manual",
      "Appendix",
      "Kartdata"
    ]
  },
  {
    "objectID": "statistikkere/appendix/kartdata.html#python",
    "href": "statistikkere/appendix/kartdata.html#python",
    "title": "Kartdata",
    "section": "Python",
    "text": "Python\n\nGIS-pakker og lesing/skriving av geodata\nGeopandas er et Python-bibliotek som integrerer Pandas med GIS-analysering og kartlegging. Geopandas er bygd opp rundt GeoDataFrames, som er en pandas DataFrame med geografisk informasjon. Disse kan lastes inn direkte hvis man har geografiske filer, eller man kan konvertere fra DataFrames med koordinat-kolonner (se eksempel under).\nGeopandas kan blant annet brukes til å kartlegge dataene, beregne avstander og labe variabler for nærmiljø ved å koble datasett sammen basert på geografisk overlapp.\nMan kan også bruke SSB-pakken sgis til å for eksempel beregne reisetider langs veg og lage publiseringsverdige kart.\nPakkene kan installeres i Dapla sånn her:\n\n\nterminal\n\npoetry add geopandas\npoetry add ssb-sgis\n\nEksempel på lesing/skriving av kartdata\nEn rekke kartdata for hele landet til analyse- og visualiseringsformål ligger i delt-bøtta “kart”. For eksempel kommuneflater til analyse eller statistikk:\n\n\nnotebook\n\nimport geopandas as gpd\nkommuner = gpd.read_parquet(\"gs://ssb-areal-data-delt-kart-prod/analyse_data/klargjorte-data/2025/ABAS_kommune_flate_p2025_v1.parquet\")\n\nOg kommunedata ment for visualisering. Disse dataene er generalisert slik at de egner seg for å lage kart, men blir unøyaktige til statistikk:\n\n\nnotebook\n\nkommuner = gpd.read_parquet(\"gs://ssb-areal-data-delt-kart-prod/visualisering_data/klargjorte-data/2025/parquet/N5000_kommune_flate_p2025_v1.parquet\")\n\nStørre filer er lagret som partisjonerte parquet-filer. Disse leses best med sgis siden geopandas ikke støtter partisjonerte filer ennå.\n\n\nnotebook\n\nimport sgis as sg\narealbruk = sg.read_geopandas(\"ssb-areal-data-delt-kart-prod/analyse_data/klargjorte-data/2025/SSB_arealbruk_flate_p2025_v1.parquet\")\n\nMan kan også gjøre vanlige tabell-filer geografiske hvis man har koordinat-kolonner. For eksempel situasjonsuttak fra Virksomhets- og foretaksregisteret (VoF):\n\n\nnotebook\n\nimport dapla as dp\n\nVOFSTI = \"ssb-vof-data-delt-stedfesting-prod/klargjorte-data/parquet\"\nvof_df = dp.read_pandas(\n    f\"{VOFSTI}/stedfesting-situasjonsuttak_p2023-01_v1.parquet\"\n)\nvof_gdf = gpd.GeoDataFrame(\n    vof_df, \n    geometry=gpd.points_from_xy(\n        vof_df[\"y_koordinat\"],\n        vof_df[\"x_koordinat\"],\n    ),\n    crs=25833,\n)\nvof_gdf\n\n\n\ngeopandas-eksempler i Dapla\nEksempel på å lage kart\nMan kan lage tematiske kart med sgis.ThematicMap. Her er et eksempel på et enkelt kart over arealet i kommuner.\n\n\nnotebook\n\nimport sgis as sg\nkommuner = sg.read_geopandas(\"ssb-areal-data-delt-kart-prod/visualisering_data/klargjorte-data/2025/parquet/N5000_kommune_flate_p2025_v1.parquet\")\nkommuner[\"km2\"] = kommuner.area / 1_000_000\n\nm = sg.ThematicMap(\n    kommuner, \n    column=\"km2\", \n    size=15,\n    title=\"Areal i kommunene\",\n    legend_kwargs=dict(title=\"Kvadratkilometer\"),\n)\nm.plot()\n\nSe flere kart-eksempler her.\nEksempel på avstandsberegning\nAvstand i luftlinje finner man enkelt med sjoin_nearest. Sånn her finner man avstand fra boligbygg (1000 tilfeldige) i Oslo til nærmeste butikkbygg.\n\n\nnotebook\n\ntestdatasti = \"gs://ssb-dapla-felles-data-produkt-prod/GIS/testdata\"\nboligbygg = sg.read_geopandas(f\"{testdatasti}/noen_boligbygg_oslo.parquet\")\nbutikkbygg = sg.read_geopandas(f\"{testdatasti}/butikkbygg_oslo.parquet\")\n\nboligbygg_med_butikkavstand = boligbygg.sjoin_nearest(butikkbygg, distance_col=\"meter_til_butikk\")\n\nFor å finne avstand eller reisetid langs veier, kan man gjøre nettverksanalyse med sgis. Man må først klargjøre vegnettet og bestemme regler for beregningen(e):\n\n\nnotebook\n\nveger = sg.read_geopandas(f\"{testdatasti}/veger_oslo.parquet\")\nveger_klargjort = (\n    sg.get_connected_components(veger)\n    .loc[lambda x: x[\"connected\"] == 1]\n    .pipe(sg.make_directed_network_norway, dropnegative=True)\n)\n\nregler = sg.NetworkAnalysisRules(directed=True, weight=\"minutes\")\n\nnwa = sg.NetworkAnalysis(network=veger_klargjort, rules=regler)\n\nSå kan man beregne reisetider fra boligbygg til butikkbygg:\n\n\nnotebook\n\navstander = nwa.od_cost_matrix(boligbygg, butikkbygg)\n\nKorteste reisetid per bolig kan kobles på som kolonne i boligdataene sånn her:\n\n\nnotebook\n\nboligbygg[\"minutter_til_butikk\"] = avstander.groupby(\"origin\")[\"minutes\"].min()\n\nUndersøk resultatene i interaktivt kart:\n\n\nnotebook\n\nsg.explore(boligbygg, butikkbygg, \"minutter_til_butikk\")\n\nSe mer informasjon og eksempler i dokumentasjonen.\nOg flere nettverksanalyse-eksempler her.\nEksempel på geografisk kobling\nDatasett kan kobles basert på geografisk overlapp med geopandas.sjoin. Denne fungerer akkurat som pandas.merge, bare at det kobles basert på geometrien.\nKodesnutten under returnerer én kommuneflate for hvert bygg som overlapper, med kolonner fra begge datasettene. Inner-join er default, men med left-join beholder man også geometriene som ikke overlapper.\n\n\nnotebook\n\nkommuner = sg.read_geopandas(f\"{testdatasti}/enkle_kommuner.parquet\")\nkommuner_med_boliginfo = kommuner.sjoin(boligbygg, how=\"left\")\n\nMed motsatt rekkefølge, får man ett boligbygg per kommune den overlapper med. Siden byggene er punkter, vil hvert bygg havne i maksimalt én kommune.\n\n\nnotebook\n\nboligbygg_med_kommuneinfo = boligbygg.sjoin(kommuner, how=\"left\")\n\nKonvertere DataFrame til GeoDataFrame\nEn pandas.DataFrame med koordinat-kolonner, kan konverteres til GeoDataFrame sånn her:\n\n\nnotebook\n\nimport pandas as pd\ndf = pd.DataFrame({\"x\": [10.8, 10.7, 10.9], \"y\": [59.9, 60, 59.85]})\n\ngdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.x, df.y), crs=4326)\n\nFor å beregne avtand i meter og kunne koble med annen geodata i Dapla, må man ha UTM-koordinater (hvis man ikke hadde det fra før):\n\n\nnotebook\n\ngdf = gdf.to_crs(25833)\n\nSe også geopandas’ dokumentasjon for mer utfyllende informasjon.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Kartdata"
    ]
  },
  {
    "objectID": "statistikkere/appendix/kartdata.html#r",
    "href": "statistikkere/appendix/kartdata.html#r",
    "title": "Kartdata",
    "section": "R",
    "text": "R\nDen viktigste GIS-pakken i R er sf. Pakken er bygd opp rundt sf-objekter, som er data.frames med geografisk informasjon. Man kan gjøre standard tidyverse-opersjoner på sf-objektene, pluss GIS-operasjoner.\nSindre Mikael Haugen har lagd funksjoner for å lese og skrive blant annet geodata i Dapla. For å få geodata, setter man parametret ‘sf’ til TRUE:\n\n\nnotebook\n\nlibrary(fellesr)\n\nveger = read_SSB(\"ssb-dapla-felles-data-produkt-prod/GIS/testdata/veger_oslo.parquet\", sf = TRUE)\n\nHaugen har også lagd en pakke for å gjøre nettverksanalyse, som også lar deg geokode adresser, altså å finne adressenes koordinater.\n\n\nnotebook\n\nlibrary(GISSB)\n\nLite eksempel på kobling basert geografisk sammenfall (enkle kommuner er egentlig til visualisering). Her får man ett bygg per kommune som overlapper (som maksimalt er én kommune siden dette er bygningspunkter):\n\n\nnotebook\n\nlibrary(sf)\n\nboligbygg = read_SSB(\"ssb-dapla-felles-data-produkt-prod/GIS/testdata/noen_boligbygg_oslo.parquet\", sf = TRUE)\nenkle_kommuner = read_SSB(\"ssb-dapla-felles-data-produkt-prod/GIS/testdata/enkle_kommuner.parquet\", sf = TRUE)\n\nboligbygg_med_kommuneinfo = st_join(boligbygg, enkle_kommuner)\n\nMed motsatt rekkefølge, får man én kommuneflate per bolig som overlapper:\n\n\nnotebook\n\nkommuner_med_boliginfo = st_join(enkle_kommuner, boligbygg)",
    "crumbs": [
      "Manual",
      "Appendix",
      "Kartdata"
    ]
  },
  {
    "objectID": "statistikkere/opprette-dapla-team.html",
    "href": "statistikkere/opprette-dapla-team.html",
    "title": "Opprette Dapla-team",
    "section": "",
    "text": "For å komme i gang med å opprette et Dapla-team trengs det en oversikt over teamets medlemmer og hvilke tilgangsgrupper medlemmene skal være med i. Det trengs også informasjon om hvilke Dapla-tjenester som er aktuelle for teamet å ta i bruk. Derfor har det blitt opprettet en egen veileder for dette kalt Dapla Start.\n\n\n\n\n\n\nGå til Dapla Start for starte bestilling av et nytt Dapla-team.\n\n\n\nNår teamet er opprettet får alle medlemmene tilgang til sitt eget prosjekt i Google Cloud Platform (GCP), som er SSBs leverandør av skytjenester. Videre får hvert prosjekt et sett med tjenester og tilganger som knyttes til teamet. Det opprettes også datalagringsområder (kalt bøtter) som bare kan aksesseres av brukere som er med i teamets tilgangsgrupper.\nDapla-teamet vil også få sin egen gruppe i SSBs Active Directory slik at medlemskapet i gruppen kan administreres av Kundeservice."
  },
  {
    "objectID": "statistikkere/editeringsrammeverk.html",
    "href": "statistikkere/editeringsrammeverk.html",
    "title": "Editeringsrammeverket",
    "section": "",
    "text": "Editeringsrammeverket er et Python-bibliotek som hjelper deg lage en editeringsapplikasjon for å se gjennom dataene dine og hvis nødvendig korrigere feilaktige verdier. Rammeverket er utviklet i SSB.\nMålet er at rammeverket skal være fleksibelt og gjenbrukbart. Det vi kaller variabelvelgeren er en sentral del av rammeverket. Kort fortalt koordinerer den skjermbildene i applikasjonen og sørger for at alt henger sammen.\nRammeverket krever noe montering. Dette er grunnen til at det heter Editeringsrammeverk og ikke Editeringsløsning.",
    "crumbs": [
      "Manual",
      "Data",
      "Editeringsrammeverket"
    ]
  },
  {
    "objectID": "statistikkere/editeringsrammeverk.html#hva-er-editeringsrammeverket",
    "href": "statistikkere/editeringsrammeverk.html#hva-er-editeringsrammeverket",
    "title": "Editeringsrammeverket",
    "section": "",
    "text": "Editeringsrammeverket er et Python-bibliotek som hjelper deg lage en editeringsapplikasjon for å se gjennom dataene dine og hvis nødvendig korrigere feilaktige verdier. Rammeverket er utviklet i SSB.\nMålet er at rammeverket skal være fleksibelt og gjenbrukbart. Det vi kaller variabelvelgeren er en sentral del av rammeverket. Kort fortalt koordinerer den skjermbildene i applikasjonen og sørger for at alt henger sammen.\nRammeverket krever noe montering. Dette er grunnen til at det heter Editeringsrammeverk og ikke Editeringsløsning.",
    "crumbs": [
      "Manual",
      "Data",
      "Editeringsrammeverket"
    ]
  },
  {
    "objectID": "statistikkere/editeringsrammeverk.html#gjenbruk-som-hovedprinsipp",
    "href": "statistikkere/editeringsrammeverk.html#gjenbruk-som-hovedprinsipp",
    "title": "Editeringsrammeverket",
    "section": "Gjenbruk som hovedprinsipp",
    "text": "Gjenbruk som hovedprinsipp\nRammeverket tilrettelegger for gjenbruk på to måter:\n\nModulbasert Rammeverket inneholder moduler/komponenter som kan gjenbrukes. Disse krever litt ulike grader av tilpasning, alt fra null tilpasning til at dataene dine må settes opp på en spesifikk måte.\nStandardisert oppsett for kode Man må følge et standard oppsett for hvordan koden kan struktureres. Se contributing.md for detaljer.\n\nDisse prinsippene bidrar til å redusere personavhengighet og samle mest mulig vedlikeholdsarbeid på ett sted. I tillegg kan en felles struktur gjøre det enklere å lære av hverandre og utvide med mer felles funksjonalitet.\nRammeverket er ikke låst til en spesifikk lagringsteknologi eller datastruktur: Så lenge du kan få dataene dine inn i en pandas dataframe kan du bruke rammeverket.\nMen noen moduler forutsetter foreløpig en spesifikk datastruktur lagret i eimerdb.",
    "crumbs": [
      "Manual",
      "Data",
      "Editeringsrammeverket"
    ]
  },
  {
    "objectID": "statistikkere/editeringsrammeverk.html#hva-trenger-jeg-å-kunne",
    "href": "statistikkere/editeringsrammeverk.html#hva-trenger-jeg-å-kunne",
    "title": "Editeringsrammeverket",
    "section": "Hva trenger jeg å kunne?",
    "text": "Hva trenger jeg å kunne?\nDu må ha litt python kompetanse, men hvor komfortabel du må være med python avhenger av hvor kompliserte behov du har.\nGrovt sett kan kompetansekravet deles i tre nivåer.\nNivå 1: Gjenbruk av ferdigbygde moduler er tilstrekkelig. Nivå 2: Noen skreddersydde tabeller, figurer eller skjermbilder kreves. Nivå 3: Du har behov for skreddersydd funksjonalitet og egne moduler.\nKompleksiteten reduseres om du:\n\nkan bruke eimerdb for å lagre dataene dine.\nhar enkle data:\n\nfå variabler\nflat struktur\ningen repeterende verdier (hver variabel kun forekommer 1 gang for hver observasjon)\n\nkan følge anbefalt datastruktur.\nkan gjenbruke eksisterende moduler.\n\nKompleksitet økes om:\n\nsamme variabel kan ha mellom 0 og uendelig antall verdier for samme enhet\n\nEks. en bedrift kan ha gjort 0 eller mange investeringer og rapporterer opplysninger for hver enkelt investering, som skal behandles enkeltvis.\n\ndatene har flere nivåer.\ndataene har kompliserte/sammensatte sammenhenger mellom variabler.\ndu har behov for skreddersydde og sammensatte moduler",
    "crumbs": [
      "Manual",
      "Data",
      "Editeringsrammeverket"
    ]
  },
  {
    "objectID": "statistikkere/editeringsrammeverk.html#hvordan-kan-jeg-ta-det-i-bruk",
    "href": "statistikkere/editeringsrammeverk.html#hvordan-kan-jeg-ta-det-i-bruk",
    "title": "Editeringsrammeverket",
    "section": "Hvordan kan jeg ta det i bruk?",
    "text": "Hvordan kan jeg ta det i bruk?\nDet er noen forutsetninger før du kan sette igang med rammeverket. Det viktigste er at du må forberede dataene dine slik at de er klare for editering: altså at maskinelle/automatiske rettinger skal være gjort og dataene er strukturert på en måte som gjør at det er lett å bearbeide.\nEksempelkoden viser hvordan man tar i bruk rammeverket for å editere en dataframe. Begynn med å installere pakken ssb-dash-framework og eventuelle andre pakker som mangler. Kodeeksemplene nedenfor er også tilgjengelig som en egen notebook: editering_demo.ipynb.\n\nImporter pakker\nfrom ssb_dash_framework import main_layout\nfrom ssb_dash_framework import app_setup\nfrom ssb_dash_framework import set_variables\nfrom ssb_dash_framework import EditingTableTab # Tab modulen\nfrom ssb_dash_framework import EditingTableWindow # Vindu modulen\nfrom ssb_dash_framework import apply_edits\n\nimport pandas as pd\nimport numpy as np\n\nimport os\nimport uuid\nfrom datetime import datetime, timezone\n\nfrom fagfunksjoner import next_version_path\n\n\nLes inn data\nfilsti_data = '/buckets/produkt/editering/editering_test_v1.parquet'\n\ndf = pd.read_parquet(\n    filsti_data\n)\n\nfilsti_logg = '/buckets/produkt/editering/logg/prosessdata/prosess-editering_test_v1.jsonl'\ntry: os.makedirs('/buckets/produkt/editering/logg/prosessdata') # lag mappe for logg \nexcept: pass\n\n\nKlargjøre data\nLøsningen avhenger av at man har en uuid-kolonne i dataframen. Dette er for å forsikre at riktig rad blir endret.\nif 'uuid' not in df.columns: df['uuid'] = [uuid.uuid4() for _ in range(len(df))]\n\n\nKjør app\n\nApp setup\nport = 8070\nservice_prefix = os.getenv(\"JUPYTERHUB_SERVICE_PREFIX\", \"/\")\ndomain = os.getenv(\"JUPYTERHUB_HTTP_REFERER\", None)\ntheme = \"cosmo\" # sjekk ut flere temaer: https://www.dash-bootstrap-components.com/docs/themes/explorer/ \napp = app_setup(port, service_prefix, theme)\napp_timestamp = datetime.now()\n\n\nVelg variabler\nset_variables([\"orgnr\",\"aar\"]) # Dette gjør at orgnr og aar er tilgjengelig i applikasjonen din.\nstart_verdier = { # Valgfritt å ha med, men kan være praktisk for brukervennlighet. Puttes inn i main_layout() funksjonen.\n    \"orgnr\": \"971526920\", \n    \"aar\": \"2020\"\n}\n\n# Definer get_data funkjson\ndef get_data(orgnr,aar):\n    return df[(df['orgnr'] == orgnr) & (df['aar']==aar)]\n    \nenhetstabell = EditingTableTab(\n    label=\"Eksempel 1: Fiktiv data\",\n    inputs=[\"orgnr\", \"aar\"], # evt. start_verdier.keys()\n    states=[],\n    get_data_func=get_data,\n    update_table_func=lambda x, *_: x ,\n    output=\"aar\",\n    log_filepath = filsti_logg,\n    \n)\n\ntab_list = [\n    enhetstabell,\n]\nwindow_list = []\n\napp.layout = main_layout(window_list, tab_list, default_values = start_verdier, )\n\nif __name__ == \"__main__\":\n    app.run(\n        port=port,\n        jupyter_server_url=domain,\n        jupyter_mode=\"tab\",\n    )\nEditingTableTab() tar imot parameteret justify_edit. Standardverdien er True. Dersom man ikke vil gi begrunnelse når man editerer setter man den til False. Vær vi må spore endringer i dataene. Dette innebærer at vi må loggføre endringene - blant annet med begrunnelse for editering.\n\n\nGjør endringer i datasettet\n\n\n\nPåfør endringer fra logg\nlogg = pd.read_json(filsti_logg, lines = True)\n\nprint(logg)\n\ndf_editert = apply_edits(df, filsti_logg, app_timestamp)\n\n\nSjekk hva som har blitt endret\n\npd.concat([df,df_editert]).drop_duplicates(keep=False)\n\n\nLagre det editerte datasettet\nfilsti_data_neste_versjon = next_version_path(filsti_data)\nprint(filsti_data_neste_versjon)\ndf.to_parquet(filsti_data_neste_versjon)",
    "crumbs": [
      "Manual",
      "Data",
      "Editeringsrammeverket"
    ]
  },
  {
    "objectID": "statistikkere/editeringsrammeverk.html#hvor-er-dokumentasjonen",
    "href": "statistikkere/editeringsrammeverk.html#hvor-er-dokumentasjonen",
    "title": "Editeringsrammeverket",
    "section": "Hvor er dokumentasjonen?",
    "text": "Hvor er dokumentasjonen?\nNedenfor er en oversikt over dokumentasjonen for Editeringsrammeverket. Det er strukturert slik at dokumentasjon som er tett knyttet til selve koden er i repoet, mens dokumentasjon om hvordan du tilrettelegger data o.l. er på Confluence.\nFor konkret bruk av biblioteket og koden er det best å se på dokumentasjonen i repoet og i koden, enten ved å se på denne siden eller se direkte i repoet github repoet. Hvis du sjekker i repoet, ta gjerne en tur innom docs/ mappen.\nVi har demoer som du kan se på, og som kan kjøres med dapla-felles tilgangen i Dapla Lab. Du kan finne demoene her.\nHvis du vil teste hvordan det er å lage en editeringsapplikasjon så kan du bruke de samme dataene som demoene bruker.\nVi har skrevet en veiledning for hvordan du kan tilpasse Altinn 3 data til å passe editeringsrammeverket som du kan se på Confluence.\n\nNoen moduler forutsetter at dataene dine er i eimerdb og følger foreslått datastruktur.\nOm du ikke har en Altinn 3 undersøkelse kan veiledningen fortsatt være nyttig for å se en foreslått struktur på dataene.\nStrukturen på dataene kan følges uavhengig av om du har dataene dine i parquet filer, eimerdb eller andre lagringsteknologier.",
    "crumbs": [
      "Manual",
      "Data",
      "Editeringsrammeverket"
    ]
  },
  {
    "objectID": "statistikkere/editeringsrammeverk.html#er-editeringsrammeverket-ferdig",
    "href": "statistikkere/editeringsrammeverk.html#er-editeringsrammeverket-ferdig",
    "title": "Editeringsrammeverket",
    "section": "Er Editeringsrammeverket ferdig?",
    "text": "Er Editeringsrammeverket ferdig?\nVi mener at Editeringsrammeverket er klart til at det kan tas i bruk og at det er fleksibelt nok til å være nyttig for mange som skal legge over statistikken sin til Dapla.\nVi mener også at strukturen vi har lagt opp til gjør det mulig å utvide funksjonalitet, lage egne moduler, skreddersy og tilpasse i stor nok grad til at de aller fleste kan tilpasse det til egne behov. - Det kan kreve litt ekstra innsats, men det vil kreve mer innsats å starte fra ingenting enn å starte fra rammeverket. - Om ny funksjonalitet legges til i rammeverket vil du ha lettere for å gjenbruke om du allerede har tilpasset deg kodestrukturen i editeringsrammeverket.\nSamtidig kan vi nok aldri si at rammeverket er ferdig. Det vil alltid være muligheter til å utvide, lage ny funksjonalitet, nye moduler og rom for forbedring. Behov kan forandre seg over tid, nye kan dukke opp og gamle behov kan bli irrelevante.\nHvis du ønsker å bidra ønskes det velkommen, enten om det er å fikse på dokumentasjonen eller lage hele moduler.",
    "crumbs": [
      "Manual",
      "Data",
      "Editeringsrammeverket"
    ]
  },
  {
    "objectID": "statistikkere/editeringsrammeverk.html#hvor-kan-jeg-få-hjelp",
    "href": "statistikkere/editeringsrammeverk.html#hvor-kan-jeg-få-hjelp",
    "title": "Editeringsrammeverket",
    "section": "Hvor kan jeg få hjelp?",
    "text": "Hvor kan jeg få hjelp?\nDet er supert om du stiller spørsmål og deler erfaringer eller tips på Viva kanalen vår.\nFor hjelp kan du også kontakte støtteteamene.",
    "crumbs": [
      "Manual",
      "Data",
      "Editeringsrammeverket"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-toolbelt.html",
    "href": "statistikkere/datadoc-toolbelt.html",
    "title": "Datadoc",
    "section": "",
    "text": "Datadoc er SSBs system for dokumentasjon av datasett. Datadoc lagrer dokumentasjonen i et strukturert format ved siden av dataene. Man kan jobbe programmatisk med metadataene til Datadoc med Datadoc-delen av dapla-toolbelt-metadata.\nFørste gang man skal dokumentere et datasett i Datadoc så er det anbefalt å bruke det grafiske grensesnittet i Datadoc-editor. I løpende produksjon er det dermed anbefalt å benytte en programmatisk tilnærming gjennom Datadoc-delen av Python-pakken dapla-toolbelt-metadata.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata",
      "Datadoc"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-toolbelt.html#forberedelser",
    "href": "statistikkere/datadoc-toolbelt.html#forberedelser",
    "title": "Datadoc",
    "section": "Forberedelser",
    "text": "Forberedelser\nFor å benytte Datadoc-delen av dapla-toolbelt-metadata må man først installere pakken i et ssb-project:\n\n\nTerminal\n\npoetry add dapla-toolbelt-metadata",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata",
      "Datadoc"
    ]
  },
  {
    "objectID": "statistikkere/datadoc-toolbelt.html#funksjonalitet",
    "href": "statistikkere/datadoc-toolbelt.html#funksjonalitet",
    "title": "Datadoc",
    "section": "Funksjonalitet",
    "text": "Funksjonalitet\nUnder finner du hvilken funksjonalitet som tilbys for Datadoc i dapla-toolbelt-metadata.\n\nKopiere fra forrige periode uten endringer\nHvis man har dokumentert datasett for periode t med Datadoc-editor, så kan man programmatisk dokumentere periode t+1 ved å benytte Datadoc-klassen i dapla-toolbelt-metadata. Det forutsetter at det ikke er noen endringer i kolonnene i datasettet, og at eneste endring er at nye observasjoner. Da kan man dokumentere den nye perioden på følgende måte:\n\n\nNotebook\n\nfrom dapla_metadata.datasets import Datadoc\n\nmeta = Datadoc(\n1    dataset_path=\"gs://ssb-dapla-felles-data-produkt-prod/datadoc/sykefratot/klargjorte-data/person_testdata_p2022_v1.parquet\",\n2    metadata_document_path=\"gs://ssb-dapla-felles-data-produkt-prod/datadoc/sykefratot/klargjorte-data/person_testdata_p2021_v1__DOC.json\",\n)\n3meta.write_metadata_document()\n\n\n1\n\ndataset_path angir det nye datasettet som skal dokumenteres.\n\n2\n\nmetadata_document_path angir sti til tidligere periodes metadata.\n\n3\n\nwrite_metadata_document er kommandoen som produserer de nye metadataene og skriver de til filen gs://ssb-dapla-felles-data-produkt-prod/datadoc/sykefratot/klargjorte-data/person_testdata_p2022_v1__DOC.json.\n\n\nDet veldig viktig at man ikke bruker denne metoden hvis det er endringer i hvilke kolonner som finnes i datasettet eller andre større endringer. Metoden over antar at den eneste informasjonen som har endret seg er den som kan leses ut av filstien. Ved større endringer i selve dataene bør man heller gjøre en manuell gjennomgang av metadataene med Datadoc-editor, eller bruke metoden som beskrives i neste avsnitt.\n\n\nKopiere fra forrige periode med endringer\nHvis det har skjedd noen endringer i datasettet ditt, men mange av kolonnene har matchende navn og likt innhold, så kan man lage et nytt metadatadokument basert metadataene til en annen fil.\n\n\nNotebook\n\nfrom dapla_metadata.datasets import Datadoc\n\nmeta = Datadoc(\n    dataset_path=\"gs://ssb-dapla-felles-data-produkt-prod/datadoc/sykefratot/klargjorte-data/dataset-to-document_p2022_v1.parquet\",\n    metadata_document_path=\"gs://ssb-dapla-felles-data-produkt-prod/existing-metadata__DOC.json\",\n    errors_as_warnings=True,\n)\n\nI koden over angir vi det nye datasettet som skal dokumenteres i dataset_path=. Deretter angir vi filstien til metadatadokumentet i metadata_document_path= som det delvis skal kopieres metadata fra. Til slutt angir vi at feilmeldinger skal behandles som advarsler med argumentet errors_as_warnings=True.\nMed dette utgangspunktet kan man deretter gjøre endringer på evt. nye eller eksisterende kolonner ved å enten bruke [Datadoc-editor], eller en programmatisk tilnærming som forklart senere i kapitlet.\n\n\nOpprette metadata for ny fil\nDet er i de fleste tilfeller anbefalt å opprette metadata for en ny fil med Datadoc-editor fordi den har et er mer brukervennlig grensesnitt. Men i noen tilfeller kan det være nyttig å opprette et metadatadokument for Datadoc programmatisk.\n\n\nNotebook\n\nfrom dapla_metadata.datasets import Datadoc\n\nmeta = Datadoc(\n    dataset_path = \"/buckets/produkt/datadoc/brukertest/3/sykefratot/klargjorte-data/person_testdata_p2021-12-31_p2022-12-31_v1.parquet\",\n)\n\nmeta.write_metadata_document()\n\nI koden over så genererer vi et metadatadokument for en gitt parquet-fil og skriver den til samme filstien som filen som dokumenteres med write_metadata_document(). Metdatadokumentet blir i eksempelet over skrevet til:\n/buckets/produkt/datadoc/brukertest/3/sykefratot/klargjorte-data/person_testdata_p2021-12-31_p2023-12-31_v1__DOC.json\nVed generering av metadataene blir all informasjon som kan genereres automatisk, faktisk generert. Dette fungerer på samme måte som når man åpner et udokumentert datasett i Datadoc-editor. I boksen under ser man et eksempel på json-filen som blir generert ved å kjøre koden over.\n\n\n\n\n\n\nCautionEksempel på et generert metadatadokument i json-format\n\n\n\n\n\n{\n  \"document_version\": \"0.0.1\",\n  \"datadoc\": {\n    \"percentage_complete\": 66,\n    \"document_version\": \"4.0.0\",\n    \"dataset\": {\n      \"short_name\": \"person_testdata\",\n      \"assessment\": \"PROTECTED\",\n      \"dataset_status\": \"DRAFT\",\n      \"dataset_state\": \"PROCESSED_DATA\",\n      \"name\": null,\n      \"description\": null,\n      \"data_source\": null,\n      \"population_description\": null,\n      \"version\": \"1\",\n      \"version_description\": null,\n      \"unit_type\": null,\n      \"temporality_type\": null,\n      \"subject_field\": null,\n      \"keyword\": null,\n      \"spatial_coverage_description\": [\n        {\n          \"languageCode\": \"nb\",\n          \"languageText\": \"Norge\"\n        },\n        {\n          \"languageCode\": \"nn\",\n          \"languageText\": \"Noreg\"\n        },\n        {\n          \"languageCode\": \"en\",\n          \"languageText\": \"Norway\"\n        }\n      ],\n      \"contains_personal_data\": false,\n      \"use_restriction\": null,\n      \"use_restriction_date\": null,\n      \"custom_type\": null,\n      \"id\": \"a7be0245-8d58-48b2-9158-3df338406cc7\",\n      \"owner\": \"dapla-felles\",\n      \"file_path\": \"/buckets/produkt/datadoc/brukertest/3/sykefratot/klargjorte-data/person_testdata_p2021-12-31_p2023-12-31_v1.parquet\",\n      \"metadata_created_date\": \"2025-04-09T18:10:26.185913Z\",\n      \"metadata_created_by\": \"obr@ssb.no\",\n      \"metadata_last_updated_date\": \"2025-04-09T18:10:26.185349Z\",\n      \"metadata_last_updated_by\": \"obr@ssb.no\",\n      \"contains_data_from\": \"2021-12-31\",\n      \"contains_data_until\": \"2023-12-31\"\n    },\n    \"variables\": [\n      {\n        \"short_name\": \"fnr\",\n        \"data_element_path\": null,\n        \"name\": null,\n        \"data_type\": \"STRING\",\n        \"variable_role\": \"MEASURE\",\n        \"definition_uri\": null,\n        \"is_personal_data\": \"NOT_PERSONAL_DATA\",\n        \"data_source\": null,\n        \"population_description\": null,\n        \"comment\": null,\n        \"temporality_type\": null,\n        \"measurement_unit\": null,\n        \"multiplication_factor\": null,\n        \"format\": null,\n        \"classification_uri\": null,\n        \"special_value\": null,\n        \"invalid_value_description\": null,\n        \"custom_type\": null,\n        \"id\": \"cf8a0c60-4a62-4a93-bbbe-143053b3bf5f\",\n        \"contains_data_from\": \"2021-12-31\",\n        \"contains_data_until\": \"2023-12-31\"\n      },\n      {\n        \"short_name\": \"sivilstand\",\n        \"data_element_path\": null,\n        \"name\": null,\n        \"data_type\": \"STRING\",\n        \"variable_role\": \"MEASURE\",\n        \"definition_uri\": null,\n        \"is_personal_data\": \"NOT_PERSONAL_DATA\",\n        \"data_source\": null,\n        \"population_description\": null,\n        \"comment\": null,\n        \"temporality_type\": null,\n        \"measurement_unit\": null,\n        \"multiplication_factor\": null,\n        \"format\": null,\n        \"classification_uri\": null,\n        \"special_value\": null,\n        \"invalid_value_description\": null,\n        \"custom_type\": null,\n        \"id\": \"f25d96d6-4a49-472d-b1f0-4805a1546daf\",\n        \"contains_data_from\": \"2021-12-31\",\n        \"contains_data_until\": \"2023-12-31\"\n      },\n      {\n        \"short_name\": \"bostedskommune\",\n        \"data_element_path\": null,\n        \"name\": null,\n        \"data_type\": \"STRING\",\n        \"variable_role\": \"MEASURE\",\n        \"definition_uri\": null,\n        \"is_personal_data\": \"NOT_PERSONAL_DATA\",\n        \"data_source\": null,\n        \"population_description\": null,\n        \"comment\": null,\n        \"temporality_type\": null,\n        \"measurement_unit\": null,\n        \"multiplication_factor\": null,\n        \"format\": null,\n        \"classification_uri\": null,\n        \"special_value\": null,\n        \"invalid_value_description\": null,\n        \"custom_type\": null,\n        \"id\": \"cf139119-4a97-4b1f-a793-a2501285c81b\",\n        \"contains_data_from\": \"2021-12-31\",\n        \"contains_data_until\": \"2023-12-31\"\n      },\n      {\n        \"short_name\": \"inntekt\",\n        \"data_element_path\": null,\n        \"name\": null,\n        \"data_type\": \"INTEGER\",\n        \"variable_role\": \"MEASURE\",\n        \"definition_uri\": null,\n        \"is_personal_data\": \"NOT_PERSONAL_DATA\",\n        \"data_source\": null,\n        \"population_description\": null,\n        \"comment\": null,\n        \"temporality_type\": null,\n        \"measurement_unit\": null,\n        \"multiplication_factor\": null,\n        \"format\": null,\n        \"classification_uri\": null,\n        \"special_value\": null,\n        \"invalid_value_description\": null,\n        \"custom_type\": null,\n        \"id\": \"c9357b37-ff1a-4f83-a332-a5239aaa3cf6\",\n        \"contains_data_from\": \"2021-12-31\",\n        \"contains_data_until\": \"2023-12-31\"\n      },\n      {\n        \"short_name\": \"bankinnskudd\",\n        \"data_element_path\": null,\n        \"name\": null,\n        \"data_type\": \"INTEGER\",\n        \"variable_role\": \"MEASURE\",\n        \"definition_uri\": null,\n        \"is_personal_data\": \"NOT_PERSONAL_DATA\",\n        \"data_source\": null,\n        \"population_description\": null,\n        \"comment\": null,\n        \"temporality_type\": null,\n        \"measurement_unit\": null,\n        \"multiplication_factor\": null,\n        \"format\": null,\n        \"classification_uri\": null,\n        \"special_value\": null,\n        \"invalid_value_description\": null,\n        \"custom_type\": null,\n        \"id\": \"38f67ddf-bd07-4bdb-8ea0-411b0aaf511e\",\n        \"contains_data_from\": \"2021-12-31\",\n        \"contains_data_until\": \"2023-12-31\"\n      },\n      {\n        \"short_name\": \"dato\",\n        \"data_element_path\": null,\n        \"name\": null,\n        \"data_type\": \"STRING\",\n        \"variable_role\": \"MEASURE\",\n        \"definition_uri\": null,\n        \"is_personal_data\": \"NOT_PERSONAL_DATA\",\n        \"data_source\": null,\n        \"population_description\": null,\n        \"comment\": null,\n        \"temporality_type\": null,\n        \"measurement_unit\": null,\n        \"multiplication_factor\": null,\n        \"format\": null,\n        \"classification_uri\": null,\n        \"special_value\": null,\n        \"invalid_value_description\": null,\n        \"custom_type\": null,\n        \"id\": \"33353e56-7f5f-409f-915a-644949137228\",\n        \"contains_data_from\": \"2021-12-31\",\n        \"contains_data_until\": \"2023-12-31\"\n      }\n    ]\n  },\n  \"pseudonymization\": null\n}\n\n\n\n\n\nEndre enkeltfelt\nI noen tilfeller kan det være nyttig å endre enkeltelementer i en metadatadokument på en programmatisk. F.eks. hvis det eneste som endrer seg ved hver periode er informasjonen i et felt, så kan man kopiere inn forrige periodes metadatadokument, og deretter endre verdien til det ene feltet som har endret seg. Det er også nyttig i de tilfellene der man ønsker å opprette metadata for ny fil.\nI kodeeksempelet under så ønsker vi å oppdatere feltet multiplication_factor siden det ikke har noen verdi fra før. Først henter vi inn metadataene til en filen i objektet meta.\n\n\nNotebook\n\nfrom dapla_metadata.datasets import Datadoc\n# Leser inn metadataene\nmeta = Datadoc(\n    dataset_path=\"/buckets/produkt/datadoc/brukertest/3/sykefratot/klargjorte-data/person_testdata_p2021-12-31_p2023-12-31_v1.parquet\",\n)\n\n#Oppdaterer verdien\nmeta.variables_lookup[\n    \"inntekt\"\n].multiplication_factor = 1000  # Variable expressed in thousands of kroner\n\n#Skriver de nye metadataene til dokumentet\nmeta.write_metadata_document()\n\nEtter at vi har hentet inn metadataene i koden over, så oppdaterer vi verdien til multiplication_factor = 1000, og til slutt skriver vi tilbake til metadatadokumentet som er lagret sammen med datasettet.\nNoen informasjonelementer i metadatadokumentet lagres som lister av dictionaries. F.eks. så lagres elementet spatial_coverage_description (Geografisk dekningsområde på norsk) på denne måten siden den er flerspråklig. Denne har en litt mer kompleks syntaks for oppdatering:\n\n\nNotebook\n\nfrom dapla_metadata import datadoc_model as model\nfrom dapla_metadata.datasets import Datadoc\n\n# Leser inn metadataene\nmeta = Datadoc(\n    dataset_path=\"/buckets/produkt/datadoc/brukertest/3/sykefratot/klargjorte-data/person_testdata_p2021-12-31_p2023-12-31_v1.parquet\",\n)\n\n#Oppdaterer verdien\nmeta.dataset.spatial_coverage_description = model.LanguageStringType(\n    root=[\n        model.LanguageStringTypeItem(languageCode=\"nb\", languageText=\"Test persondata\"),\n        model.LanguageStringTypeItem(languageCode=\"nn\", languageText=\"Testar persondata\"),\n        model.LanguageStringTypeItem(languageCode=\"en\", languageText=\"Test of persondata\")\n    ]\n)\n\n#Skriver de nye metadataene til dokumentet\nmeta.write_metadata_document()\n\nI koden over ser vi at oppdatering av flerspråklige informasjonselementer gjøres med å først indentifisere feltet vi ønsker å endre, meta.dataset.spatial_coverage_description. Dette kan leses som at vi ønsker å endre meta-objektet, under dataset-delen, og feltet spatial_coverage_description. Deretter kommer en syntax som er lik for alle flerspråklige felt.\nI boksen under finnes flere eksempler på hvordan man endrer informasjon i enkeltfelter.\n\n\n\n\n\n\nCautionFlere eksempler på hvordan man endrer enkeltfelter\n\n\n\n\n\n\n\nNotebook\n\nfrom dapla_metadata import datadoc_model as model\nfrom dapla_metadata.datasets import Datadoc\n\n# Importerer/genererer et metadataobjekt\nmeta = Datadoc(\n    dataset_path=\"resources/sykefratot/klargjorte-data/person_testdata_p2022_v1.parquet\",\n)\n\n# Endre \"name\" på datasettnivå\nmeta.dataset.name = model.LanguageStringType(\n    root=[\n        model.LanguageStringTypeItem(languageCode=\"nb\", languageText=\"Test persondata\"),\n        model.LanguageStringTypeItem(languageCode=\"nn\", languageText=\"Test persondata\"),\n        model.LanguageStringTypeItem(\n            languageCode=\"en\", languageText=\"Test personal data\"\n        ),\n    ],\n)\n\n# Endre \"description\" på datasettnivå (flerspråklig)\nmeta.dataset.description = model.LanguageStringType(\n    root=[\n        model.LanguageStringTypeItem(\n            languageCode=\"nb\",\n            languageText=\"Data er kun for test formål\",\n        ),\n        model.LanguageStringTypeItem(\n            languageCode=\"nn\",\n            languageText=\"Data er kun for test formål\",\n        ),\n        model.LanguageStringTypeItem(\n            languageCode=\"en\",\n            languageText=\"For testing purposes only\",\n        ),\n    ],\n)\n\n# Endre \"data_source\" på datasettnivå\nmeta.dataset.data_source = (\n    \"23\"  # Refers to code in https://www.ssb.no/klass/klassifikasjoner/712\n)\n\n#Endre \"use_restriction\" på dataettsnivp\nmeta.dataset.use_restriction = model.UseRestriction.PROCESS_LIMITATIONS\n\n# Endre \"name\" på variabelnivå for variabelen \"inntekt\" (flerspråklig)\nmeta.variables_lookup[\"inntekt\"].name = model.LanguageStringType(\n    root=[\n        model.LanguageStringTypeItem(languageCode=\"nb\", languageText=\"Inntekt\"),\n        model.LanguageStringTypeItem(languageCode=\"nn\", languageText=\"Inntekt\"),\n        model.LanguageStringTypeItem(\n            languageCode=\"en\",\n            languageText=\"Income\",\n        ),\n    ],\n)\n\n# Endre \"multiplication_factor\" på variabelnivå for variabelen \"inntekt\"\nmeta.variables_lookup[\n    \"inntekt\"\n].multiplication_factor = 1000  # Variable expressed in thousands of kroner\n\n# # Endre \"is_personal_data\" på variabelnivå for variabelen \"inntekt\"\nmeta.variables_lookup[\n    \"inntekt\"\n].is_personal_data = model.IsPersonalData.NOT_PERSONAL_DATA\n\n\n\n\n\n\nLese ut informasjon\nSiden metadataene som er laget med Datadoc lagres ved siden av filen som dokumenteres som en json-fil, så kan man lese inn hele filen i en notebook og printe den ut hvis man ønsker det:\n\n\nNotebook\n\nimport json\n\nfile_path = \"/buckets/produkt/datadoc/brukertest/10/sykefratot/klargjorte_data/person_testdata_p2021_v1__DOC.json\"\n\nwith open(file_path, \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\nprint(json.dumps(data, indent=2, ensure_ascii=False))\n\n\n\n\n\n\n\nCautionOutput fra eksempelfil\n\n\n\n\n\n{\n  \"document_version\": \"0.0.1\",\n  \"datadoc\": {\n    \"percentage_complete\": 66,\n    \"document_version\": \"4.0.0\",\n    \"dataset\": {\n      \"short_name\": \"person_testdata\",\n      \"assessment\": \"PROTECTED\",\n      \"dataset_status\": \"DRAFT\",\n      \"dataset_state\": \"PROCESSED_DATA\",\n      \"name\": null,\n      \"description\": null,\n      \"data_source\": null,\n      \"population_description\": null,\n      \"version\": \"1\",\n      \"version_description\": null,\n      \"unit_type\": null,\n      \"temporality_type\": null,\n      \"subject_field\": null,\n      \"keyword\": null,\n      \"spatial_coverage_description\": [\n        {\n          \"languageCode\": \"nb\",\n          \"languageText\": \"Test persondata\"\n        },\n        {\n          \"languageCode\": \"nn\",\n          \"languageText\": \"Testar persondata\"\n        },\n        {\n          \"languageCode\": \"en\",\n          \"languageText\": \"Test of persondata\"\n        }\n      ],\n      \"contains_personal_data\": false,\n      \"use_restriction\": null,\n      \"use_restriction_date\": null,\n      \"custom_type\": null,\n      \"id\": \"a7be0245-8d58-48b2-9158-3df338406cc7\",\n      \"owner\": \"dapla-felles\",\n      \"file_path\": \"/buckets/produkt/datadoc/brukertest/3/sykefratot/klargjorte-data/person_testdata_p2021-12-31_p2023-12-31_v1.parquet\",\n      \"metadata_created_date\": \"2025-04-09T18:10:26.185913Z\",\n      \"metadata_created_by\": \"obr@ssb.no\",\n      \"metadata_last_updated_date\": \"2025-04-09T18:58:28.808974Z\",\n      \"metadata_last_updated_by\": \"obr@ssb.no\",\n      \"contains_data_from\": \"2021-12-31\",\n      \"contains_data_until\": \"2023-12-31\"\n    },\n    \"variables\": [\n      {\n        \"short_name\": \"fnr\",\n        \"data_element_path\": null,\n        \"name\": null,\n        \"data_type\": \"STRING\",\n        \"variable_role\": \"MEASURE\",\n        \"definition_uri\": null,\n        \"is_personal_data\": \"NOT_PERSONAL_DATA\",\n        \"data_source\": null,\n        \"population_description\": null,\n        \"comment\": null,\n        \"temporality_type\": null,\n        \"measurement_unit\": null,\n        \"multiplication_factor\": null,\n        \"format\": null,\n        \"classification_uri\": null,\n        \"special_value\": null,\n        \"invalid_value_description\": null,\n        \"custom_type\": null,\n        \"id\": \"cf8a0c60-4a62-4a93-bbbe-143053b3bf5f\",\n        \"contains_data_from\": \"2021-12-31\",\n        \"contains_data_until\": \"2023-12-31\"\n      },\n      {\n        \"short_name\": \"sivilstand\",\n        \"data_element_path\": null,\n        \"name\": null,\n        \"data_type\": \"STRING\",\n        \"variable_role\": \"MEASURE\",\n        \"definition_uri\": null,\n        \"is_personal_data\": \"NOT_PERSONAL_DATA\",\n        \"data_source\": null,\n        \"population_description\": null,\n        \"comment\": null,\n        \"temporality_type\": null,\n        \"measurement_unit\": null,\n        \"multiplication_factor\": null,\n        \"format\": null,\n        \"classification_uri\": null,\n        \"special_value\": null,\n        \"invalid_value_description\": null,\n        \"custom_type\": null,\n        \"id\": \"f25d96d6-4a49-472d-b1f0-4805a1546daf\",\n        \"contains_data_from\": \"2021-12-31\",\n        \"contains_data_until\": \"2023-12-31\"\n      },\n      {\n        \"short_name\": \"bostedskommune\",\n        \"data_element_path\": null,\n        \"name\": null,\n        \"data_type\": \"STRING\",\n        \"variable_role\": \"MEASURE\",\n        \"definition_uri\": null,\n        \"is_personal_data\": \"NOT_PERSONAL_DATA\",\n        \"data_source\": null,\n        \"population_description\": null,\n        \"comment\": null,\n        \"temporality_type\": null,\n        \"measurement_unit\": null,\n        \"multiplication_factor\": null,\n        \"format\": null,\n        \"classification_uri\": null,\n        \"special_value\": null,\n        \"invalid_value_description\": null,\n        \"custom_type\": null,\n        \"id\": \"cf139119-4a97-4b1f-a793-a2501285c81b\",\n        \"contains_data_from\": \"2021-12-31\",\n        \"contains_data_until\": \"2023-12-31\"\n      },\n      {\n        \"short_name\": \"inntekt\",\n        \"data_element_path\": null,\n        \"name\": null,\n        \"data_type\": \"INTEGER\",\n        \"variable_role\": \"MEASURE\",\n        \"definition_uri\": null,\n        \"is_personal_data\": \"NOT_PERSONAL_DATA\",\n        \"data_source\": null,\n        \"population_description\": null,\n        \"comment\": null,\n        \"temporality_type\": null,\n        \"measurement_unit\": null,\n        \"multiplication_factor\": 1000,\n        \"format\": null,\n        \"classification_uri\": null,\n        \"special_value\": null,\n        \"invalid_value_description\": null,\n        \"custom_type\": null,\n        \"id\": \"c9357b37-ff1a-4f83-a332-a5239aaa3cf6\",\n        \"contains_data_from\": \"2021-12-31\",\n        \"contains_data_until\": \"2023-12-31\"\n      },\n      {\n        \"short_name\": \"bankinnskudd\",\n        \"data_element_path\": null,\n        \"name\": null,\n        \"data_type\": \"INTEGER\",\n        \"variable_role\": \"MEASURE\",\n        \"definition_uri\": null,\n        \"is_personal_data\": \"NOT_PERSONAL_DATA\",\n        \"data_source\": null,\n        \"population_description\": null,\n        \"comment\": null,\n        \"temporality_type\": null,\n        \"measurement_unit\": null,\n        \"multiplication_factor\": null,\n        \"format\": null,\n        \"classification_uri\": null,\n        \"special_value\": null,\n        \"invalid_value_description\": null,\n        \"custom_type\": null,\n        \"id\": \"38f67ddf-bd07-4bdb-8ea0-411b0aaf511e\",\n        \"contains_data_from\": \"2021-12-31\",\n        \"contains_data_until\": \"2023-12-31\"\n      },\n      {\n        \"short_name\": \"dato\",\n        \"data_element_path\": null,\n        \"name\": null,\n        \"data_type\": \"STRING\",\n        \"variable_role\": \"MEASURE\",\n        \"definition_uri\": null,\n        \"is_personal_data\": \"NOT_PERSONAL_DATA\",\n        \"data_source\": null,\n        \"population_description\": null,\n        \"comment\": null,\n        \"temporality_type\": null,\n        \"measurement_unit\": null,\n        \"multiplication_factor\": null,\n        \"format\": null,\n        \"classification_uri\": null,\n        \"special_value\": null,\n        \"invalid_value_description\": null,\n        \"custom_type\": null,\n        \"id\": \"33353e56-7f5f-409f-915a-644949137228\",\n        \"contains_data_from\": \"2021-12-31\",\n        \"contains_data_until\": \"2023-12-31\"\n      }\n    ]\n  },\n  \"pseudonymization\": null\n}\n\n\n\nMan kan lese ut metadata fra Datadoc-dokumenterte datasett med dapla-toolbelt-metadata. I eksempelet under henter vi ut metadataene til en fil og printer ut informasjonen på datasettnivå:\n\n\nNotebook\n\nfrom pprint import pprint\nfrom dapla_metadata.datasets import Datadoc\n\nmeta = Datadoc(\n    dataset_path=\"/buckets/produkt/datadoc/brukertest/10/sykefratot/klargjorte_data/person_testdata_p2022_v1.parquet\",\n)\n\npprint(vars(meta.dataset))\n\n\n\n\n\n\n\nCautionOutput fra eksempelfil\n\n\n\n\n\n{'assessment': 'PROTECTED',\n 'contains_data_from': datetime.date(2021, 12, 31),\n 'contains_data_until': datetime.date(2023, 12, 31),\n 'contains_personal_data': False,\n 'custom_type': None,\n 'data_source': None,\n 'dataset_state': 'PROCESSED_DATA',\n 'dataset_status': 'DRAFT',\n 'description': None,\n 'file_path': '/buckets/produkt/datadoc/brukertest/3/sykefratot/klargjorte-data/person_testdata_p2021-12-31_p2023-12-31_v1.parquet',\n 'id': UUID('a7be0245-8d58-48b2-9158-3df338406cc7'),\n 'keyword': None,\n 'metadata_created_by': 'obr@ssb.no',\n 'metadata_created_date': datetime.datetime(2025, 4, 9, 18, 10, 26, 185913, tzinfo=TzInfo(UTC)),\n 'metadata_last_updated_by': 'obr@ssb.no',\n 'metadata_last_updated_date': datetime.datetime(2025, 4, 9, 18, 10, 26, 185349, tzinfo=TzInfo(UTC)),\n 'name': None,\n 'owner': 'dapla-felles',\n 'population_description': None,\n 'short_name': 'person_testdata',\n 'spatial_coverage_description': LanguageStringType(root=[LanguageStringTypeItem(languageCode='nb', languageText='Norge'), LanguageStringTypeItem(languageCode='nn', languageText='Noreg'), LanguageStringTypeItem(languageCode='en', languageText='Norway')]),\n 'subject_field': None,\n 'temporality_type': None,\n 'unit_type': None,\n 'use_restriction': None,\n 'use_restriction_date': None,\n 'version': '1',\n 'version_description': None}\n\n\n\nOver hentet vi ut fra datasets-delen av dokumentet med Python sin innebygde funksjon vars() for å få output’en mer lesevennlig. Vi kan også printe ut informasjon fra variables-delen av dokumentet:\n\n\nNotebook\n\nfrom pprint import pprint\nfrom dapla_metadata.datasets import Datadoc\n\nmeta = Datadoc(\n    dataset_path=\"/buckets/produkt/datadoc/brukertest/3/sykefratot/klargjorte-data/person_testdata_p2021-12-31_p2023-12-31_v1.parquet\",\n)\n\n# Formattere en pen print\nfor var in meta.variables:\n    pprint(vars(var))\n    print(\"-\" * 60)\n\n\n\n\n\n\n\nCautionOutput fra eksempelfil\n\n\n\n\n\n{'classification_uri': None,\n 'comment': None,\n 'contains_data_from': datetime.date(2021, 12, 31),\n 'contains_data_until': datetime.date(2023, 12, 31),\n 'custom_type': None,\n 'data_element_path': None,\n 'data_source': None,\n 'data_type': 'STRING',\n 'definition_uri': None,\n 'format': None,\n 'id': UUID('cf8a0c60-4a62-4a93-bbbe-143053b3bf5f'),\n 'invalid_value_description': None,\n 'is_personal_data': 'NOT_PERSONAL_DATA',\n 'measurement_unit': None,\n 'multiplication_factor': None,\n 'name': None,\n 'population_description': None,\n 'short_name': 'fnr',\n 'special_value': None,\n 'temporality_type': None,\n 'variable_role': 'MEASURE'}\n------------------------------------------------------------\n{'classification_uri': None,\n 'comment': None,\n 'contains_data_from': datetime.date(2021, 12, 31),\n 'contains_data_until': datetime.date(2023, 12, 31),\n 'custom_type': None,\n 'data_element_path': None,\n 'data_source': None,\n 'data_type': 'STRING',\n 'definition_uri': None,\n 'format': None,\n 'id': UUID('f25d96d6-4a49-472d-b1f0-4805a1546daf'),\n 'invalid_value_description': None,\n 'is_personal_data': 'NOT_PERSONAL_DATA',\n 'measurement_unit': None,\n 'multiplication_factor': None,\n 'name': None,\n 'population_description': None,\n 'short_name': 'sivilstand',\n 'special_value': None,\n 'temporality_type': None,\n 'variable_role': 'MEASURE'}\n------------------------------------------------------------\n{'classification_uri': None,\n 'comment': None,\n 'contains_data_from': datetime.date(2021, 12, 31),\n 'contains_data_until': datetime.date(2023, 12, 31),\n 'custom_type': None,\n 'data_element_path': None,\n 'data_source': None,\n 'data_type': 'STRING',\n 'definition_uri': None,\n 'format': None,\n 'id': UUID('cf139119-4a97-4b1f-a793-a2501285c81b'),\n 'invalid_value_description': None,\n 'is_personal_data': 'NOT_PERSONAL_DATA',\n 'measurement_unit': None,\n 'multiplication_factor': None,\n 'name': None,\n 'population_description': None,\n 'short_name': 'bostedskommune',\n 'special_value': None,\n 'temporality_type': None,\n 'variable_role': 'MEASURE'}\n------------------------------------------------------------\n{'classification_uri': None,\n 'comment': None,\n 'contains_data_from': datetime.date(2021, 12, 31),\n 'contains_data_until': datetime.date(2023, 12, 31),\n 'custom_type': None,\n 'data_element_path': None,\n 'data_source': None,\n 'data_type': 'INTEGER',\n 'definition_uri': None,\n 'format': None,\n 'id': UUID('c9357b37-ff1a-4f83-a332-a5239aaa3cf6'),\n 'invalid_value_description': None,\n 'is_personal_data': 'NOT_PERSONAL_DATA',\n 'measurement_unit': None,\n 'multiplication_factor': 1000,\n 'name': None,\n 'population_description': None,\n 'short_name': 'inntekt',\n 'special_value': None,\n 'temporality_type': None,\n 'variable_role': 'MEASURE'}\n------------------------------------------------------------\n{'classification_uri': None,\n 'comment': None,\n 'contains_data_from': datetime.date(2021, 12, 31),\n 'contains_data_until': datetime.date(2023, 12, 31),\n 'custom_type': None,\n 'data_element_path': None,\n 'data_source': None,\n 'data_type': 'INTEGER',\n 'definition_uri': None,\n 'format': None,\n 'id': UUID('38f67ddf-bd07-4bdb-8ea0-411b0aaf511e'),\n 'invalid_value_description': None,\n 'is_personal_data': 'NOT_PERSONAL_DATA',\n 'measurement_unit': None,\n 'multiplication_factor': None,\n 'name': None,\n 'population_description': None,\n 'short_name': 'bankinnskudd',\n 'special_value': None,\n 'temporality_type': None,\n 'variable_role': 'MEASURE'}\n------------------------------------------------------------\n{'classification_uri': None,\n 'comment': None,\n 'contains_data_from': datetime.date(2021, 12, 31),\n 'contains_data_until': datetime.date(2023, 12, 31),\n 'custom_type': None,\n 'data_element_path': None,\n 'data_source': None,\n 'data_type': 'STRING',\n 'definition_uri': None,\n 'format': None,\n 'id': UUID('33353e56-7f5f-409f-915a-644949137228'),\n 'invalid_value_description': None,\n 'is_personal_data': 'NOT_PERSONAL_DATA',\n 'measurement_unit': None,\n 'multiplication_factor': None,\n 'name': None,\n 'population_description': None,\n 'short_name': 'dato',\n 'special_value': None,\n 'temporality_type': None,\n 'variable_role': 'MEASURE'}\n------------------------------------------------------------\n\n\n\nNå som vi har sett alle informasjonselementer i metadatadokumentet, så kan vi velge hente ut spesifikke elementer som vi er interessert i. Under leses variabelen inntekt inn og all informasjon printes ut:\n\n\nNotebook\n\nfrom pprint import pprint\nfrom dapla_metadata.datasets import Datadoc\n\nmeta = Datadoc(\n    dataset_path=\"/buckets/produkt/datadoc/brukertest/3/sykefratot/klargjorte-data/person_testdata_p2021-12-31_p2023-12-31_v1.parquet\",\n)\n\ninntekt = meta.variables_lookup[\"inntekt\"]\n\npprint(vars(inntekt))\n\n\n\n\n\n\n\nCautionOutput fra eksempelfil\n\n\n\n\n\n{'classification_uri': None,\n 'comment': None,\n 'contains_data_from': datetime.date(2021, 12, 31),\n 'contains_data_until': datetime.date(2023, 12, 31),\n 'custom_type': None,\n 'data_element_path': None,\n 'data_source': None,\n 'data_type': 'INTEGER',\n 'definition_uri': None,\n 'format': None,\n 'id': UUID('c9357b37-ff1a-4f83-a332-a5239aaa3cf6'),\n 'invalid_value_description': None,\n 'is_personal_data': 'NOT_PERSONAL_DATA',\n 'measurement_unit': None,\n 'multiplication_factor': 1000,\n 'name': None,\n 'population_description': None,\n 'short_name': 'inntekt',\n 'special_value': None,\n 'temporality_type': None,\n 'variable_role': 'MEASURE'}\n\n\n\nVidere kan vi hente ut verdien til feltet multiplication_factor for variabelen inntekt med følgende kode:\n\n\nNotebook\n\nfrom dapla_metadata.datasets import Datadoc\n\nmeta = Datadoc(\n    dataset_path=\"/buckets/produkt/datadoc/brukertest/3/sykefratot/klargjorte-data/person_testdata_p2021-12-31_p2023-12-31_v1.parquet\",\n)\n\nmeta.variables_lookup[\n    \"inntekt\"\n].multiplication_factor\n\nVidere kan vi hente Dapla-teamet som eier datasettet ved hente ut verdien til feltet owner i dataset-delen av dokumentet:\n\n\nNotebook\n\nfrom dapla_metadata.datasets import Datadoc\n\nmeta = Datadoc(\n    dataset_path=\"/buckets/produkt/datadoc/brukertest/3/sykefratot/klargjorte-data/person_testdata_p2021-12-31_p2023-12-31_v1.parquet\",\n)\n\nmeta.dataset.owner",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata",
      "Datadoc"
    ]
  },
  {
    "objectID": "statistikkere/delomaten.html",
    "href": "statistikkere/delomaten.html",
    "title": "Delomaten",
    "section": "",
    "text": "WarningTjenesten er under utvikling\n\n\n\nDelomaten under utvikling og er ikke tilgjengelig for Dapla-team enda. Tjenesten vil lanseres ila høsten 2025.\nDelomaten er en tjeneste for å automatisere deling av data mellom seksjoner og team som har forskjellige datatilganger av hensyn til behov og personvern. Dette gjøres ved at tjenesten pseudonymiserer eller depseudonymiserer felt i datasett før den lagres i en spesiell deltbøtte som det delende teamet ikke har lesetilgang til. Tjenesten leser paruqet datafiler og datadoc metadata fra teamets produktbøtte, prosesserer dem og deretter skriver dem til en mappe i ‘delomat’ deltbøtte. Datadoc metadatafiler for dataen er påbudt. Hvis data filer blir flyttet til kildemappen uten tilhørende metadatafil innen 5 minutter vil delomaten jobben feile.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Delomaten"
    ]
  },
  {
    "objectID": "statistikkere/delomaten.html#forberedelser",
    "href": "statistikkere/delomaten.html#forberedelser",
    "title": "Delomaten",
    "section": "Forberedelser",
    "text": "Forberedelser\nFør et Dapla-team kan ta i bruk delomaten må tjenesten være aktivert for teamet. Dette kan gjøres selvbetjent som en feature hvor både shared-buckets og delomaten må skrus på. Alternativt kan man opprette en Kundeservice-sak og be om å hjelp til dette.\nTil slutt oppretter man en ‘delomat’ deltbøtte.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Delomaten"
    ]
  },
  {
    "objectID": "statistikkere/delomaten.html#sette-opp-tjenesten",
    "href": "statistikkere/delomaten.html#sette-opp-tjenesten",
    "title": "Delomaten",
    "section": "Sette opp tjenesten",
    "text": "Sette opp tjenesten\nTjenesten lar statistikere konfigurere hvilke felt som skal pseudonymiseres/depseudonymiseres før dataen blir lagret i deltbøtten gjennom en .yaml fil. I likhet med kildomaten oppretter man kilder under automation mappen i teamets IAC repo. For delomaten ligger disse under automation/shared-data. Delomaten forventer en config.yaml fil per kilde. F.eks. hvis man skal opprette en delomaten-jobb i prod-miljøet som heter ‘forbruk’ oppretter man filen automation/shared-data/mitt-dapla-team-prod/forbruk/config.yaml.\n\nKonfigurasjonsfil\nKonfigurasjonsfilen inneholder oppskriften på hvilke felt som skal pseudonymiseres/depseudonymiseres og med hvilken algoritme som skal brukes.\n\n\nautomation/shared-data/mitt-dapla-team-prod/forbruk/config.yaml\n\nshared_bucket: \"delomatentest\"\nsource_folder_prefix: \"forbruk/\"\ndestination_folder: \"forbruk\"\nmemory_size: 3\npseudo:\n  - name: task_one\n    columns: [ \"fnr\", \"snr\" ]\n    pseudo_operation: \"PSEUDO\"\n    encryption:\n      algorithm: \"sid_mapping\"\n      sid_snapshot_date: \"2025-07-07\"\n\nI dette eksempelet definerer vi en delomaten-jobb som vil pseudonymisere fnr og snr feltene ved bruk av sid mapping og katalogdato 2025-07-07. Det er også mulig å spesifisere forskjellige algoritmer for forskjellige felt.\n\n\nautomation/shared-data/mitt-dapla-team-prod/ledstill/config.yaml\n\nshared_bucket: \"delomatentest\"\nsource_folder_prefix: \"ledstill/\"\ndestination_folder: \"ledstill\"\nmemory_size: 2\npseudo:\n  - name: task_one\n    columns: [ \"fnr\"]\n    pseudo_operation: \"DEPSEUDO\"\n    encryption:\n      algorithm: \"sid_mapping\"\n      sid_snapshot_date: \"2025-07-07\"\n  - name: task_two\n    columns: [ \"fornavn\"]\n    pseudo_operation: \"DEPSEUDO\"\n    encryption:\n      algorithm: \"default\"\n\nHer depseudonymiserer vi fnr kolonnen med sid mapping algoritmen, mens fornavn blir depseudonymisert med standardalgoritmen DAEAD. Konfigurasjonsfilen blir validert når du oppretter en pull request på GitHub. Der vil du få eventuelle feilmeldinger om du har feilkonfigurert delomaten.\n\n\n\nDelomaten test feil\n\n\nHvis du så trykker deg inn på workflowen som feilet vil du få en beskrivende feilmelding markert i rødt som f.eks:\n\n\n\nDelomaten feilmelding",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Delomaten"
    ]
  },
  {
    "objectID": "statistikkere/metadata.html",
    "href": "statistikkere/metadata.html",
    "title": "Metadata",
    "section": "",
    "text": "Metadata er informasjonen som trengs for å produsere, finne, forstå og gjenbruke data. For å bruke et datasett riktig må vi forstå hva det inneholder. Vi må vite hva de ulike variablene representerer, for eksempel om variabelen inntekt i datasett A er den samme som inntekt i datasett B og derfor kan sammenliknes. Vi trenger også innsikt i hva kodene i datasettet betyr, som for eksempel hva sivilstandskode 2 representerer. I tillegg er det viktig å vite om datasettet inneholder personopplysninger, slik at vi kan oppfylle krav fra Datatilsynet.\nAlt dette er eksempler på metadata som er tett knyttet til selve dataene. I tillegg har SSB mange andre typer metadata, som kvalitetsindikatorer og prosessbeskrivelser. Her velger vi imidlertid å fokusere på de metadataene som er nødvendige for å kunne bruke et datasett riktig.\nSSB har i flere år hatt systemer for å dokumentere «datanære» metadata. Vardok dokumenterer variabler, som definisjoner og eierseksjoner. Klass håndterer kodeverk, inkludert klassifikasjoner og kodelister. Datadok fokuserer på dokumentasjon av datasett, for eksempel variabler, variabeltyper og antall desimaler.\nSSB valgte å dokumentere metadata i flere separate systemer, både fordi andre statistikkbyråer hadde erfart at ett samlet system ble for stort og komplekst, og fordi SSB allerede hadde en eksisterende løsning, Datadok, som skulle videreføres. I stedet for å lage ett enkelt system, ble strategien å utvikle flere systemer som skulle fungere sammen som en helhet. Dette førte til integrasjoner mellom systemene, som for eksempel en kobling mellom Vardok og Klass. En variabel som Næring i Vardok har en lenke til Standard for næringsgruppering i Klass. I tillegg er Vardok koblet til Statistikkbanken og MetaDB (NUDB og FD-Trygd) for utveksling av variabeldefinisjoner. Datadok er også koblet til Vardok, slik at variablene i Datadok kan referere til tilhørende variabeldefinisjoner fra Vardok.\nVerken Datadok eller Vardok kan brukes på Dapla. Derfor er en ny løsning for dokumentasjon av datasett, Datadoc, allerede implementert på Dapla. En ny løsning for dokumentasjon av variabeldefinisjoner, Vardef, er under utvikling. Når Vardef er ferdigstilt, skal variabler fra Vardok flyttes dit, og alle fremtidige oppdateringer vil skje i Vardef. Klass kan derimot brukes fra Dapla, og det er derfor ennå ikke besluttet om og når denne løsningen skal flyttes eller eventuelt implementeres på nytt på Dapla.\nPå Dapla skal det etter hvert også implementeres en datakatalog som skal være en portal inn til metadataløsningene. Via datakatalogen skal en kunne søke i alle disse metadataene.\n\n\n\n\n\n\nFigur 1: Metadata på Dapla",
    "crumbs": [
      "Manual",
      "Metadata"
    ]
  },
  {
    "objectID": "statistikkere/altinn-dapla-suv-tools.html",
    "href": "statistikkere/altinn-dapla-suv-tools.html",
    "title": "dapla-suv-tools",
    "section": "",
    "text": "dapla-suv-tools er en python pakke med en samling verktøy for integrering med SUV-plattformen. Pakken tilbyr verktøy for skjema administrasjon, bygging av prefill og utsending av skjema på Altinn 3 plattformen.\nDokumentasjon av pakken ligger her. Denne gir en teknisk innføring som du kan følge og kopiere kode fra. Noe demokode ligger også i repoet og kan være ett godt utgangspunkt.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-suv-tools"
    ]
  },
  {
    "objectID": "statistikkere/altinn-dapla-suv-tools.html#installasjon",
    "href": "statistikkere/altinn-dapla-suv-tools.html#installasjon",
    "title": "dapla-suv-tools",
    "section": "Installasjon",
    "text": "Installasjon\n\nPip\n\n\nTerminal\n\npip install dapla-suv-tools\n\n\n\nPoetry\n\n\nTerminal\n\npoetry add dapla-suv-tools",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-suv-tools"
    ]
  },
  {
    "objectID": "statistikkere/altinn-dapla-suv-tools.html#opprette-klient",
    "href": "statistikkere/altinn-dapla-suv-tools.html#opprette-klient",
    "title": "dapla-suv-tools",
    "section": "Opprette klient",
    "text": "Opprette klient\nFor å kunne bruke pakken må du importere klienten:\n\n\nnotebook\n\nfrom dapla_suv_tools.suv_client import SuvClient\n\nclient = SuvClient()",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-suv-tools"
    ]
  },
  {
    "objectID": "statistikkere/altinn-dapla-suv-tools.html#paginering",
    "href": "statistikkere/altinn-dapla-suv-tools.html#paginering",
    "title": "dapla-suv-tools",
    "section": "Paginering",
    "text": "Paginering\nFor å sette opp paginering må du importere PaginationInfo\n\n\nnotebook\n\nfrom dapla_suv_tools.pagination import PaginationInfo\n\nPaginering brukes for å hente data i mindre deler spesielt når et datasett er stort. Dette bidrar til å redusere belastningen både på klient og server, og gir bedre ytelse. Side- og størrelsesparametere sendes som en del av forespørselen.\n\n\n\n\n\n\nWarningMaksimal størrelse\n\n\n\nMaksimal tillatt størrelse per side er 100 i alle forespørsler. Hvis man angir en høyere verdi vil forespørselen feile eller bli begrenset til 100 poster per side.\n\n\n\nEnkel bruk av paginering\nI dette eksempelet brukes paginering for å hente en spesifikk side med et gitt antall elementer.\n\n\nnotebook\n\np_info = PaginationInfo(page=1, size=5)\n\nresult = client.get_skjema_by_ra_nummer(\n    ra_nummer=\"RA-0666A3\", pagination_info=p_info\n)\n\nHer hentes den første siden (page=1) med 5 elementer per side (size=5).\n\n\nHente alle data med paginering\nDette eksempelet viser hvordan man kan hente alle data ved å iterere gjennom flere sider.\n\n\nnotebook\n\npage = 1\nsize = 100\nall_records = []\n\nwhile True:\n    p_info = PaginationInfo(page=page, size=size)\n    \n    response = client.get_utvalg_from_sfu(\n        delreg_nr=49430324,\n        ra_nummer='RA-0666A3',\n        pagination_info=p_info\n    )\n    \n    records = response\n    all_records.extend(records)\n    \n    if len(records) &lt; size:\n        break\n    \n    page += 1\n\nprint(f\"Totalt antall poster: {len(all_records)}\")\n)\n\nDette sikrer at alle poster hentes når datasettet går over flere sider.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-suv-tools"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-data.html",
    "href": "statistikkere/jobbe-med-data.html",
    "title": "Jobbe med data",
    "section": "",
    "text": "I denne artikkelen viser vi hvordan man leser og skriver forskjellige filtyper på Dapla med Python og R. Vi har skrevet egne artikler for deling av data, flytting av data mellom bøtter (transfer service), pseudonymisering. Vi har også skrevet om kartdata og datavalidering med pandera.",
    "crumbs": [
      "Manual",
      "Data"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-data.html#lese-og-skrive-data-på-dapla",
    "href": "statistikkere/jobbe-med-data.html#lese-og-skrive-data-på-dapla",
    "title": "Jobbe med data",
    "section": "Lese og skrive data på Dapla",
    "text": "Lese og skrive data på Dapla\nVi bruker Python og R når vi skal lese, skrive og bearbeide data. Dette må gjøres fra en av Dapla lab-tjenestene (for eksempel jupyter). Dette er fordi Dapla lab er koblet opp mot Google Cloud Storage.\nTidligere har vi vært avhengige av Python-pakken dapla-toolbelt og R-pakken fellesR. Nå trenger man ikke ta spesielle hensyn: man leser og skriver data som om man jobber lokalt. Man bruker altså pandas for Python og arrow for R.\n\nBøttemontering og teamtilhørighet\nFor å få tilgang til data på Google Cloud må man kjøre Python/R fra Dapla lab. Første steg er altså å starte en instans av VSCode, Jupyter eller RStudio fra Dapla lab.\nNår man starter tjenesten må man være obs på hvilket team man representerer i den tjenesten. På dapla styres datatilgang etter Dapla team. Du må derfor velge riktig dapla team under tjenestekonfigurasjonen. Tjenestekonfigurasjonen kan du lese om i blant annet jupyter-artikkelen vår.\n\n\n\n\n\n\nFigur 1: Detaljert tjenestekonfigurasjon for bøttetilgang i Dapla Lab\n\n\n\nMan må naturligvis vite hvor dataene ligger, men vi trenger ikke hele filstien! Dapla lab er bøttemontert: det vil si at vi kan forenkle filstier til buckets/{bøttenavn}/{filnavn} (gs://ssb-dapla-felles-data-produkt-prod er ikke lenger nødvendig å ha i filstien).\n\n\n\n\n\n\nTipEksempeldata i Dapla Felles\n\n\n\nDapla Felles er et team der alle i SSB er med i developers-gruppa. Kode-eksemplene finnes for både R og Python, og du kan velge hvilken du skal se ved å trykke på den arkfanen du er interessert i.\n\n\n\n\nLese og skrive filer\nUnder finner du eksempler på hvordan du kan lese og skrive data på Dapla. Kode-eksemplene finnes for både R og Python og du kan velge hvilken du skal se ved å trykke på den arkfanen du er interessert i.\n\nParquet\n\nPython \n\n\n\n\nnotebook\n\nimport pandas as pd\n\nfile_path = \"/buckets/produkt/datadoc/brukertest/10/sykefratot/klargjorte-data/person_testdata_p2021_v1.parquet\"\n\n# Les med pd.read_parquet()\ndf = pd.read_parquet(file_path)\n\n# Skriv med to_parquet()\ndf.to_parquet(file_path )\n\n\n\n\n\nnotebook\n\nlibrary(arrow)\n\nfile_path = \"/buckets/produkt/datadoc/brukertest/10/sykefratot/klargjorte-data/person_testdata_p2021_v1.parquet\"\n\n# Les med arrow::read_parquet()\nperson_testdata &lt;- arrow::read_parquet(file_path)\n\n# Skriv med arrow::write_parquet()\narrow::write_parquet(person_testdata, file_path)\n\nVi kan også filtrere hvilke variabler vi ønsker å lese inn ved å spesifisere parameter col_select. For eksempel:\nperson_testdata &lt;- arrow::read_parquet(file_path,\n                                       col_select = c(\"fnr\", \"sivilstand\"))\nKartdata lagret som .parquet kan leses inn ved å kombinere funksjonen open_dataset fra pakken arrow og read_sf_dataset fra pakken sfarrow.\nlibrary(arrow)\nlibrary(sfarrow)\nlibrary(tidyverse)\n\ndata &lt;- arrow::open_dataset(\"/buckets/produkt/GIS/Kart/2023/ABAS_grunnkrets_flate_2023/ABAS_grunnkrets_flate_2023.parquet\") %&gt;%\n  dplyr::filter(KOMMUNENR == \"0301\") %&gt;%\n  sfarrow::read_sf_dataset()\n\n\n\n\n\nTekstfiler\n\nPython \n\n\n\n\nnotebook\n\nimport pandas as pd\n\nfile_path = \"/buckets/produkt/dapla-manual-examples/purchases.csv\"\n\n# Les med pd.read_csv()\ndf = pd.read_csv(file_path, sep=';')\n\n# Skriv med df.to_csv()\ndf.to_csv(file_path)\n\n\n\nnotebook\n\nimport pandas as pd\n\nfile_path = \"/buckets/produkt/dapla-manual-examples/test.json\"\n\n# Les med pd.read_json()\ndf = pd.read_json(file_path)\n\n# Skriv med df.to_json()\ndf.to_json(file_path)\n\n\n\n\n\nnotebook\n\nfile_path = \"/buckets/produkt/dapla-manual-examples/purchases.csv\"\n\n# Les med read.csv2() \ndt_1987 &lt;- read.csv2(file_path) #bruk read.csv2 for semikolonseparerte filer\n# OBS: bruk read.csv() for kommaseparerte filer\n\n# Skriv med write.csv2()\nwrite.csv2(dt_1987, file_path)\n\n\n\nnotebook\n\nlibrary(jsonlite)\n\nfile_path = \"/buckets/produkt/dapla-manual-examples/test.json\"\n\n# Les med jsonlite::fromJSON()\ndata &lt;- jsonlite::fromJSON(file_path)\n\n# Skriv med jsonlite::write_json()\njsonlite::write_json(data, file_path)\n\n\n\n\n\n\nxlsx\n\nPython \n\n\n\n\nnotebook\n\nimport pandas as pd\n\nfile_path = \"/buckets/produkt/dapla-manual-examples/test.xlsx\"\n\n# Les med pd.read_excel()\ndf = pd.read_excel(file_path)\n\n# Skriv med df.to_excel()\ndf.to_excel(file_path)\n\n\n\nXLSX-filer kan leses inn med funksjonen read.xlsx fra pakken openxlsx.\n\n\nnotebook\n\nlibrary(openxlsx)\n\nfile_path = \"/buckets/produkt/dapla-manual-examples/purchases.xlsx\"\n\n# Les med openxlsx::read.xlsx()\ndata &lt;- openxlsx::read.xlsx(file_path)\n\n# Skriv med openxlsx::write.xlsx()\nopenxlsx::write.xlsx(data, file_path)\n\n\n\n\n\n\nSAS\nHer er et eksempel på hvordan man leser inn en sas7bdat-fil på Dapla som har blitt generert i prodsonen.\n\nPython \n\n\n\n\nnotebook\n\nimport dapla as dp\n\nfile_path = \"/buckets/produkt/dapla-manual-examples/statbank_ledstill.sas7bdat\"\n\n# Les med pd.read_sas()\npd.read_sas(file_path, format=\"sas7bdat\", encoding=\"latin1\")\n\n\n\nFor å lese sas7bdat-filer i R kan man bruke funksjonen read_sas fra pakken haven (som ligger i tidyverse).\n\n\nnotebook\n\nlibrary(tidyverse)\n\n# Filsti\nfile_path = \"/buckets/produkt/dapla-manual-examples/statbank_ledstill.sas7bdat\"\n\n# Les med read_sas()\ndata &lt;- haven::read_sas(file_path)\n\n\n\n\n\n\n\n\n\n\nNoteFull filsti ikke lenger nødvendig!\n\n\n\nLegg merke til at det ikke lenger er nødvendig med full filsti. I tilfellene vist over ville man tidligere skrevet filstiene på denne måten: gs://ssb-dapla-felles-data-produkt-prod/dapla-metrics/test.parquet\nNå skjønner maskinen at den skal lete under ssb-dapla-felles fordi dette er teamet vi valgte under tjenestekonfigurasjon\n\n\nSå enkelt er det! Men…\nNår vi jobber med data på dapla må vi ta stilling til og følge obligatoriske standarder, for eksempel navnestandarden. Versjonering av datasett er viktig i denne sammenheng. Å jobbe med data krever i praksis mer enn de tekniske ferdighetene til å lese, behandle og skrive data.\n\n\n\n\n\n\nTipHva nå? Hva gjør du med dataene?\n\n\n\nFor å lære hvordan du bruker R og Python til databehandling må man foreløpig ut av manualen. Vi anbefaler at du bruker deler av dokumentet Kom i gang med Dapla skrevet av A200 støtteteam og boken R for Data Science.",
    "crumbs": [
      "Manual",
      "Data"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-data.html#filbehandling",
    "href": "statistikkere/jobbe-med-data.html#filbehandling",
    "title": "Jobbe med data",
    "section": "Filbehandling",
    "text": "Filbehandling\n\nSlette filer\nÅ slette filer fra lagringsområdet kan gjøres på flere måter. Man kan gjøre det med pek-og-klikk i Google Cloud Console eller med Python og R:\n\nPython \n\n\n\n\nnotebook\n\nimport os\n\nfile_path = \"fil/sti.parquet\"\n\nos.remove(file_path)\n\n\n\nFunksjonen file.remove kan brukes til å slette data på lagringsområdet.\n\n\nnotebook\n\n# Skriv inn full filsti til filen som skal slettes\nfile_path = \"fil/sti.parquet\"\n\nfile.remove(file_path)\n\n\n\n\n\n\nKopiere filer\n\nPython \n\n\n\n\nnotebook\n\nimport shutil\n\n# Path to folders\nfrom_folder = \"/buckets/produkt/felles/veiledning/python/eksempler/purchases\"\nto_folder = \"/buckets/produkt/felles/veiledning/python/eksempler\"\n\n# Copy file\nshutil.copy(f\"{from_folder}/data.parquet\",\n      f\"{to_folder}/data_copy.parquet\")\n\nDette fungerer også for å kopiere filer mellom bøtter.\n\n\n\n\nConsole\n\n# Skriv inn filnavn på filen som skal flytte og navn på bøtta hvor filen ligger. \nfile_name = \"purchases.csv\"\nbucket_path_gammel = \"/buckets/produkt/dapla_manual-examples/\"\n\n#Skriv inn ny (hvis ikke bøtta finnes må den opprettes - se Opprette mapper under)\nbucket_path_ny = \"/buckets/produkt/dapla_manual-examples/ny_mappe/\"\n\n#Hvis du bare vil endre plassering skriver du:\nfile.copy(from = paste0(bucket_path_gammel,file_name), to = paste0(bucket_path_ny,file_name))\n\n#Hvis du også vil endre navnet på filen kan du skrive:\nnew_name = \"innkjoep.csv\"\nfile.copy(from = paste0(bucket_path_gammel,file_name), to = paste0(bucket_path_ny,new_name))\n\nMerk at funksjonen file.copy() bevarer den opprinnelige filen. Hvis du heller ønsker å flytte filen (dvs å kopiere og slette den opprinnelige filen) velg file.rename().\n\n\n\n\n\nFlytte (klippe/lime) filer\n\nPython \n\n\n\n\nnotebook\n\nimport shutil\n\n# Path to folders\nfrom_folder = \"/buckets/produkt/felles/veiledning/python/eksempler/purchases\"\nto_folder = \"/buckets/produkt/felles/veiledning/python/eksempler\"\n\n# Copy file\nshutil.move(f\"{from_folder}/data.parquet\",\n      f\"{to_folder}/data.parquet\")\n\n\n\n\n\nConsole\n\n# Skriv inn filnavn på filen som skal flyttes og navn på bøtta hvor filen ligger nå. \nfile_name = \"purchases.csv\"\nbucket_path_gammel = \"/buckets/produkt/dapla_manual-examples/\"\n\n#Skriv inn navn på den nye plasseringen (hvis ikke bøtta finnes må den opprettes - se Opprette mapper under)\nbucket_path_ny = \"/buckets/produkt/dapla_manual-examples/ny_mappe/\"\n\n#Hvis du bare vil endre plassering skriver du:\nfile.rename(from = paste0(bucket_path_gammel,file_name), to = paste0(bucket_path_ny,file_name))\n\n#Hvis du også vil endre navnet på filen kan du skrive:\nnew_name = \"innkjoep.csv\"\nfile.rename(from = paste0(bucket_path_gammel,file_name), to = paste0(bucket_path_ny,new_name))\n\nMerk at funksjonen file.rename() sletter den opprinnelige filen. Hvis du bare vil kopiere uten å slette den opprinnelige filen velg file.copy().\n\n\n\n\n\nOpprette mapper\nSelv om bøtter ikke har mapper med en hierarkisk struktur slik man er kjent med fra klassiske filsystemer, så kan man opprette det som ser ut som mapper i objektnavnet. I realiteten blir bare / oppfattet som en del av navnet på objektet. Skulle du likevel ønske å opprette dette så kan du gjøre det følgende måte:\n\nPython \n\n\n\n\nnotebook\n\nimport os\n\n#Path to folder\nnew_folder_path = '/buckets/produkt/felles/veiledning/python/eksempler/testmappe/'\n\n# Create folder\nos.mkdir(new_folder_path)\n\n\n\n\n\nConsole\n\n# Skriv inn navnet på den nye mappen. \nny_mappe = \"/buckets/produkt/dapla_manual-examples/testmappe/\"\ndir.create(ny_mappe)\n\n\n\n\n\n\n\n\n\n\nTipDet finnes mange flere systemkommandoer!\n\n\n\nUtover de som er vist i denne artiklene finnes det en rekke forskjellige ysstemkommandoer man kan foreta seg fra Python og R, for eksempel som os.listdir eller `list.dirs() i R.\nPoenget her er at siden Dapla lab har bøttemontering kan du som bruker utnytte deg av funksjonalitetet som om du jobber på din lokale maskin!",
    "crumbs": [
      "Manual",
      "Data"
    ]
  },
  {
    "objectID": "statistikkere/index.html",
    "href": "statistikkere/index.html",
    "title": "Hurtigstart",
    "section": "",
    "text": "Velkommen til hurtigstart-siden! Her presenterer vi manualen og setter deg i gang på 1-2-3!\nDenne manualen tar sikte på å gi SSB-ansatte mulighet til å ta i bruk Dapla uten hjelp fra eksperter. Manualen inneholder artikler om blant annet kodespråkene Python og R, Git, Dapla Lab, Metadata og mye mer. I tillegg har manualen en blogg og en nyhetsside.\nUnder hjelp-fanen øverst på nettsiden finner du blant annet siden FAQ hvor ofte stilte spørsmål besvares. I denne boken omtaler vi den gamle produksjonssonen, ofte kalt prodsonen, som bakke, og det nye skymiljøet Google Cloud som sky. Det er ikke helt presist men duger for formålene i denne boken.\n\nKom i gang med Dapla - Steg for steg!\nHolder du på med omleggingen av produksjonssystemet ditt, eller skal du begynne å jobbe med Dapla? Les vår artikkel Overgang til Dapla - steg for steg!.\n\n\nLes om…\n\nDapla Nyheter - Hva skjer på Dapla om dagen?\nVi har vår egen nyhetsside Dapla Nyheter. Bruk den for å holde deg oppdatert på hva som skjer på Dapla!\n\n\nHva er Dapla Team?\nStatistikkproduksjon på Dapla foregår med Dapla-team som midtpunktet. Vi anbefaler at alle begynner med å lese artikkelen Hva er Dapla-team!\n\n\nHvor er dataene våre? I bøtter!\nDataene våre lagres på Google Cloud platform i det som kalles bøtter. Bøttene er basert på de obligatoriske datatilstandene. Les artikkelen Hva er bøtter? og artikkelen om datatilstander\n\n\nDapla lab - arbeidsbenken vår\nDapla lab er der vi finner verktøy og tjenester som Jupyter, RStudio og Datadoc-editor. Les hovedartikkelen om Dapla-lab. Det finnes også artikler for hver tjeneste vi har i Dapla lab.\n\n\nGit, GitHub og malen vår SSB-project\nGit og GitHub brukes for å lagre (og versjonshåndtere) koden vår. SSB-project brukes som mal for GitHub-repoer og for å håndtere Pythonpakker. Her finner du artikler om Git og GitHub, SSB-project (oversikt) og SSB-project for pakkehåndtering i Pyton. For pakkehåndtering i R brukes renv.\n\n\n\n\n\n\nNoteVi trenger bidragsytere!\n\n\n\nDapla er i konstant utvikling og det er manualen og! Derfor trenger vi flere bidragsytere til å fjerne utdatert informasjon, forbedre eksisterende artikler og skrive nye.\nKunne du tenkt deg å bidra? Les om hvordan du kan bidra i denne artikkelen i appendiksen. Har du lyst til å bidra, men er ikke helt sikker på hva du kan bidra med? Ta en titt på issues i GitHub-repoet.\n\n\nKommentarer og ønsker vedrørende boken tas imot med åpne armer. Dette kan gjøres ved å lage en issue i GitHub-repoet.\nGod fornøyelse😁",
    "crumbs": [
      "Manual",
      "Kom i gang"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard.html",
    "href": "statistikkere/navnestandard.html",
    "title": "Navnestandard",
    "section": "",
    "text": "Data i de permanente datatilstandene inndata, klargjorte data, statistikkdata og utdata skal lagres i Google Cloud Storage (GCS) bøtter og følge en definert navnestandard. Standarden gjelder for både statistikkprodukter og dataprodukter (se forklaringsboks under). Navnestandarden beskrevet i dette kapitlet er derfor gjeldende for alle data som lagres i bøtter i standardprosjektet, som f.eks. produktbøtta og delt-bøttene.\nDatatilstanden kildedata omfattes ikke av navnestandarden. Grunnen er at kildedata mottas av SSB i mange former/strukturer og de deles sjelden med andre team. De unike egenskapene til kildedata er også grunnen til at de ikke har samme krav til dokumentasjon i metadatasystemene heller.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Navnestandard"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard.html#mappestruktur",
    "href": "statistikkere/navnestandard.html#mappestruktur",
    "title": "Navnestandard",
    "section": "Mappestruktur",
    "text": "Mappestruktur\nNavnestandarden for lagring av data innfører obligatoriske mapper som alle statistikk- og dataprodukter må følge, samt valgfrie deler hvor teamet selv kan bestemme sin mappestruktur.\n\nObligatoriske mapper\nIfølge navnestandarden skal følgende mappenivåer alltid eksistere først i en lagringsbøtte:\n\nStatistikkproduktets eller dataproduktets kortnavn\nDatatilstand\n\nAnta at det er team som heter dapla-example som har produserer statistikkproduktene ledstill og sykefra. I tillegg produserer de et dataprodukt som heter ameld. Deres mappestruktur i produktbøtta vil da se slik ut:\n\n\nObligatoriske mapper\n\nssb-dapla-example-data-produkt-prod/  \n└─ ledstill/  \n   ├── inndata/\n   ├── klargjorte-data/\n   ├── statistikk/\n   └── utdata/\n└─ sykefra/  \n   ├── inndata/\n   ├── klargjorte-data/\n   ├── statistikk/\n   └── utdata/\n└─ ameld_data/  \n   ├── inndata/\n   ├── klargjorte-data/\n   ├── statistikk/\n   └── utdata/                    \n\n\n\nValgfrie mapper\nDe to første mappenivåene er bestemt og obligatoriske. Teamene kan likevel opprette egendefinerte mapper der det er behov. Det kan gjøres i følgende tilfeller:\n\nTeamet ønsker å organisere dataene i undermapper for hver datatilstand.\nTeamet trenger å lagre temporære data.\n\nDet er anbefalt å lage en temp-mappe på første nivå etter bøttenavn, men det er også tillatt å opprette temp-mapper andre steder i mappe-hierarkiet, f.eks. ../inndata/temp/ eller ../klargjorte-data/temp/.\n\nTeamet utfører oppdrag og ønsker et eget sted å lagre data knyttet til dette. Det kan kun gjøres i en oppdrag-mappe på første nivå etter bøttenavn.\n\nUnder er et nytt eksempel i produktbøtta for team dapla-example, men nå har de kun statistikkproduktet ledstill, en temp-mappe og en oppdrag-mappe. I tillegg så ønsker de å skille mellom data som er produsert på Dapla og data som er migrert fra tidligere plattform. De gjør det ved å opprette de egendefinerte mappene on-prem og dapla for hver datatilstand.\n\n\nObligatoriske og egendefinerte mapper\n\nssb-dapla-example-data-produkt-prod/  \n└─ ledstill/  \n   ├── inndata/\n       ├── on-prem/\n       ├── dapla/\n   ├── klargjorte-data/\n       ├── on-prem/\n       ├── dapla/\n   ├── statistikk/\n       ├── on-prem/\n       ├── dapla/\n   └── utdata/\n       ├── on-prem/\n       ├── dapla/\n└─ temp/\n└─ oppdrag/                     \n\n\n\n\n\n\n\nNoteMappe for oppdrag\n\n\n\nNår man oppretter en mappe for oppdrag så er det viktig å kunne knytte dataene til et Websak-saksnummer. Det er derfor anbefalt at det opprettes en undermappe med saksnummeret eller at saksnummeret er med i filnavnet.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Navnestandard"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard.html#filnavn",
    "href": "statistikkere/navnestandard.html#filnavn",
    "title": "Navnestandard",
    "section": "Filnavn",
    "text": "Filnavn\nFilnavn skal ha en fast struktur som inneholder: en kort beskrivelse, periode, versjon og filtype, slik som vist i Figur 1.\n\n\n\n\n\n\nFigur 1: De ulike delene av et standardisert filnavn\n\n\n\nEksempelet i Figur 1 har varehandel som kort beskrivelse, dataene er gyldige for 2018-Q1, versjon er 1 og filtypen er parquet. I tillegg ser vi at periodeangivelse alltid skal prefixes med p og versjon med v. Elementene i filnavnet skal skilles med understrek.\nDet er også verdt å merke seg at mellomrom og særnorske bokstaver som æ, ø og å ikke forekommer i filnavnet. Følgende alfanumeriske tegn kan benyttes i fil- og mappenavn:\n\na-z og A-Z2.\n0-9\nBruk bindestrek -, eller understrek _, og ikke mellomrom.\n\nTabell 1 viser en mer inngående beskrivelse av hva som inngår i de ulike delene av et filnavn.\n\n\n\nTabell 1: Autonomitet og tilganger i IaC-repo\n\n\n\n\n\nElement\nForklaring\n\n\n\n\nKort beskrivelse\nKort tekst som forklarer datasettets innhold, f.eks. “varehandel” eller “personinntekt”. Bruk bindestrek hvis beskrivelsen består av flere ord, f.eks. “grensehandel-imputert” eller “framskrevne-befolkningsendringer”.\n\n\nPeriode - inneholder data f.o.m. dato\nDatasettet inneholder data fra og med dato/tidspunkt. I filnavnet må perioden prefikses med “_p”, eksempel “_p2022” eller “_p2022-01-01”. Se også gyldige formater for periode (dato/tidspunkt)\n\n\nPeriode - inneholder data t.o.m. dato\nDatasettet inneholder data til og med dato/tidspunkt. Denne brukes ved behov, eksempelvis for datasett som inneholder forløpsdata eller datasett med flere perioder/årganger.\n\n\nVersjon\nVersjon av datasettet. I filnavnet må versjonsnummeret prefikses med “_v”, eksempel “v1”, “v2” eller “v3”.\n\n\nFiltype\nFilendelse som sier noen om filtypen, f.eks. “.json”, “.csv”, “.xml” eller “.parquet”.\n\n\n\n\n\n\n\nEksempler på gyldige filnavn\nUnder finner du et utvalg eksempler på gyldige filnavn for ulike tidsspenn.\n\n\n\n\n\n\n\nTidsspenn\nEksempel på gyldige filnavn\n\n\n\n\nÉn årgang med data\nflygende-objekter_p2019_v1.parquet\n\n\nTo årganger med data\nufo-observasjoner_p2019_p2020_v1.parquet\n\n\nFra 2019 til 2050\nframskrevne-befolkningsendringer_p2019_p2050_v1.parquet\n\n\nFra 01.01.2022 til 31.12.2022\nsykepenger_p2022-01-01_p2022-12-31_v1.parquet\n\n\nTverrsnittsdata (status) per 01.10.2022\nutanningsnivaa_p2022-10-01_v1.parquet\n\n\nOktober, november og desember 2022\ngrensehandel-imputert_p2022-10_p2022-12_v1.parquet\n\n\nUke-nummer 1\nomsetning_p2020-W01_v1.parquet\n\n\nUke-nummer 15\nomsetning_p2020-W15_v1.parquet\n\n\nFørste bimester i 2022\nskipsanloep_p2022-B1_v1.parquet\n\n\nFørste kvartal i 2018 (quarter)\npensjon_p2018-Q1_v1.parquet\n\n\nFørste tertial i 2022\nnybilreg_p2022-T1_v1.parquet\n\n\nFørste halvår i 2022\npersoninntekt_p2022-H1_v1.parquet\n\n\nKvartalene 1, 2, 3 og 4 i 2018\nvarehandel_p2018-Q1_p2018-Q4_v1.parquet\n\n\nDato 31.12.2024 og tid 23:59:30.000\nskjema_p2024-12-31T23-59-30.000_v1.parquet",
    "crumbs": [
      "Manual",
      "Standarder",
      "Navnestandard"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard.html#partisjonerte-data",
    "href": "statistikkere/navnestandard.html#partisjonerte-data",
    "title": "Navnestandard",
    "section": "Partisjonerte data",
    "text": "Partisjonerte data\nTeam som partisjonerer sine filer ved lagring skal fortsatt følge navnestandarden. Det som endrer seg er at filtype ikke blir en del av filnavnet, men heller kommer inn under partisjoneringen. Anta at team dapla-example partisjonerer et datasett i datatilstand inndata som heter skjema_p2018_p2020_v1. Anta også at de partisjonerer dataene med hensyn på kolonnen aar. Da vil de i henhold til navnestandarden opprette denne strukturen:\n\n\nMappestruktur partisjonert data\n\nssb-dapla-example-data-produkt-prod/  \n└─ ledstill/  \n   ├── inndata/\n        └── skjema_p2018_p2020_v1\n            └── aar=2018\n                └── data.parquet\n            └── aar=2019\n                └── data.parquet\n            └── aar=2020\n                └── data.parquet         \n   ├── klargjorte-data/\n   ├── statistikk/\n   └── utdata/                 \n\n\nEksempel: Produktbøtte for team dapla-example\nAnta at det er team som heter dapla-example med statistikkproduktene ledstill og sykefra, og de har et dataprodukt med kortnavnet ameld. Teamet har følgende mappestruktur i produktbøtta:\n\n\nProduktbøtta: ledstill, sykefra og ameld\n\nssb-dapla-example-data-produkt-prod/\n└─ ledstill/  \n    ├── inndata/\n    │   ├── skjema_p2024-Q1_v1.parquet\n    │   ├── skjema_p2024-Q2_v1.parquet\n    │   └── skjema_p2024-Q2_v2.parquet\n    ├── klargjorte-data/\n    │   ├── editert_p2024-Q1_v1.parquet\n    │   └── editert_p2024-Q2_v1.parquet\n    ├── statistikk/\n    │   ├── aggregert_p2024-Q1_v1.parquet\n    │   └── aggregert_p2024-Q2_v1.parquet        \n    └── utdata/\n    │   ├── statbank_p2024-Q1_v1.parquet\n    │   └── statbank_p2024-Q2_v1.parquet   \n    │\n└─ sykefra/  \n    ├── inndata/\n    │   ├── egenmeldt_p2024-Q1_v1.parquet\n    │   ├── egenmeldt_p2024-Q2_v1.parquet\n    │   ├── legemeldt_p2024-Q1_v1.parquet\n    │   └── legemeldt_p2024-Q2_v1.parquet\n    ├── klargjorte-data/\n    │   ├── sykefravaer_p2024-Q1_v1.parquet\n    │   └── sykefravaer_p2024-Q2_v1.parquet\n    ├── statistikk/\n    │   ├── aggregert_p2024-Q1_v1.parquet\n    │   └── aggregert_p2024-Q2_v1.parquet\n    └── utdata/\n    │   ├── statbank_p2024-Q1_v1.parquet\n    │   └── statbank_p2024-Q2_v1.parquet\n    │\n└─ ameld_data/  \n    ├── inndata/\n    │   ├── ameldingen_p2024-11_v1.parquet\n    │   └── ameldingen_p2024-12_v1.parquet\n    └── klargjorte-data/\n    │   ├── ameldingen_p2024-11_v1.parquet\n    │   └── ameldingen_p2024-12_v1.parquet",
    "crumbs": [
      "Manual",
      "Standarder",
      "Navnestandard"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard.html#versjonering-av-datasett",
    "href": "statistikkere/navnestandard.html#versjonering-av-datasett",
    "title": "Navnestandard",
    "section": "Versjonering av datasett",
    "text": "Versjonering av datasett\nVersjonering er obligatorisk når man jobber med data på dapla. Hovedgrunnen til at vi versjonerer er for å dekke kravet om uforanderlighet og etterprøvbarehet: at data-konsumenter (menneske eller maskin) skal ha kontroll på endringer. Derfor skal et datasett som er brukt i statistikkproduksjon aldri slettes - det skal opprettes en ny versjon av datasettet. Les mer om prinsippet om uforanderlighet av data på confluence-siden til IT-Arkitektur.\nKort fortalt innebærer versjonering av data at datasettene har versjonsnummer før filendelsen. For eksempel: framskrevne-befolkningsendringer_p2019_p2050_v1.parquet\n\n\n\n\n\n\nNoteUnntak til versjonering: nyeste versjon og temporære data\n\n\n\nNyeste versjon kan lagres uten versjonsnummer. Dette er for at man enkelt skal kunne lese inn siste versjon av et datasett (ved å utelate versjonssuffiks). I tilegg trenger man ikke versjonere temporære data.\n\n\n\nNår skal man lagre ny versjon?\nFølgende hendelser skaper ny versjon av et datasett:\n\nReberegninger av data med nye metoder.\nKorrigeringer av verdier.\nOservasjoner legges til eller fjernes.\nOppdatert eller erstattet kodeverk.\nVariabler fjernes eller legges til.\n\nHvis det gjøres vesentlige endringer (mange variabler) så bør det vurderes om dette er et helt nytt datasett.\n\nAndre strukturendringer, f.eks. bytte av datatyper eller formater.\n\nMed andre ord: enhver endring skaper en ny versjon!\n\n\nVersjonering i praksis\nFor hver versjon som oppstår av datasettet opprettes det en ny fysisk fil hvor versjonsnummeret økes med en. Alle gamle versjoner av et datasett skal også eksistere i mappen.\nEtterhvert som man får flere versjoner av et datasett kan det se slik ut:\n\n\nMappe med flere versjoner av et datasett\n\nssb-prod-team-personstatistikk-data-produkt-prod/  \n└── befolkningsframskrivinger/  \n    └── klargjorte-data/  \n        ├── framskrevne-befolkningsendringer_p2019_p2050.parquet  \n        ├── framskrevne-befolkningsendringer_p2019_p2050_v1.parquet  \n        ├── framskrevne-befolkningsendringer_p2019_p2050_v2.parquet  \n        └── framskrevne-befolkningsendringer_p2019_p2050_v3.parquet\n\nEksempelet over viser at siste versjon av en fil kan lagres med og uten versjonsnummer for å gjøre det lettere å lese inn nyeste versjon.\n\nEksempelkode med pakken ssb-fagfunksjoner\n\nPython \n\n\n\n\nPython kode fra SSB-fagfunksjoner for finne neste filversjon\n\n# importer funksjonen next_version_path() fra ssb-fagfunksjoner\nfrom fagfunksjoner import next_version_path\n\nfilsti = 'gs://ssb-prod-team-personstatistikk-data-produkt-prod/befolkningsframskrivninger/klargjorte-data/framskrevne-befolkningsendringer_p2019_p2050.parquet'\n\nny_filsti = next_version_path(filsti)\n\nprint(ny_filsti)\n# vil returnere:\n# 'gs://ssb-prod-team-personstatistikk-data-produkt-prod/befolkningsframskrivninger/klargjorte-data/framskrevne-befolkningsendringer_p2019_p2050_v4.parquet'\n\nssb-fagfunksjoner har også følgende funksjoner for å gjøre versjonering lettere:\n\nget_fileversions() # Retrieves a list of file versions matching a specified pattern.\nlatest_version_number() # Function for finding latest version in use for a file.\nlatest_version_path() # Finds the path to the latest version of a specified file.\n\n\n\n\n\nR-kode fra fellesr for finne neste filversjon\n\nlibrary(fellesr)\nfil = \"/buckets/produkt/dapla-manual-examples/testdata_p2025-Q2_v1.parquet\"\nny_filsti = lag_versjonert_filsti(fil, versjon = \"ny\")\nprint(ny_filsti)\n\n# Vil returnere: \"/buckets/produkt/dapla-manual-examples/testdata_p2025-Q2_v2.parquet\"\n\nfellesr har også følgende funksjoner for å gjøre versjonering lettere: * finn_filversjoner(fil) # Skaffer en liste over filversjoner som matcher filnavn (KOMMER). * finn_versjon(lag_versjonert_filsti(fil, versjon = \"siste\")) # Nøstet funksjon som finner nummeret til en riktig versjonert fil. * lag_versjonert_filsti(fil, versjon = \"siste\") # Finner stien til den siste versjonen av en spesifisert fil.\n\n\n\n\n\n\n\n\n\nImportantPermanente data skal ikke endres eller slettes\n\n\n\nTidligere delte/publiserte data skal ikke slettes eller overskrives!\nDet må derfor lagres fysiske filer for hver versjon av datasettet. Dette er viktig for at SSB skal oppfylle krav om etterprøvbarhet av statistikkene.\n\n\n\n\nVersjon 0: Deling av data som ikke har oppnådd stabil tilstand\nHvis det er behov for å dele data som fortsatt er under innsamling eller pågående klargjøring gjøres dette ved å bruke versjonsnummer 0 i filnavnet.\nDette versjonsnummeret skal kun brukes midlertidig fram til datasettet oppnår stabil tilstand. Ved stabil tilstand byttes versjonsnummer for datasettet til 1 eller høyere.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Navnestandard"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard.html#footnotes",
    "href": "statistikkere/navnestandard.html#footnotes",
    "title": "Navnestandard",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nLes mer om hvordan man henter ut informasjon fra API-et til Statistikkregisteret i denne blogg-artikkelen.↩︎\nDet er anbefalt at æ, ø og å erstattes med ae, oe og aa, f.eks. naering, oekonomi eller levekaar.↩︎",
    "crumbs": [
      "Manual",
      "Standarder",
      "Navnestandard"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html",
    "href": "statistikkere/maskinporten-guardian.html",
    "title": "Maskinporten",
    "section": "",
    "text": "Maskinporten er en tjeneste fra Digdir som gir sikker autentisering og tilgangskontroll for datautveksling mellom virksomheter.\nTilgangsstyring for data administeres via et webgrensesnitt (Samarbeidsportalen) En virksomhet som deler data (API-tilbyder) definerer såkalte API scopes, og velger hvilke andre virksomheter (API-konsumenter) som skal ha tilgang til disse. API-konsumenter kan på sin side selv opprette en eller flere Maskinporten-klienter og gir de tilgang til API scopes som er delt med virksomheten.\nLes mer om Maskinporten her.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#hva-gjør-maskinporten-guardian",
    "href": "statistikkere/maskinporten-guardian.html#hva-gjør-maskinporten-guardian",
    "title": "Maskinporten",
    "section": "Hva gjør Maskinporten Guardian?",
    "text": "Hva gjør Maskinporten Guardian?\nUtveksling av data fra en API-tilbyder gjøres ved å inkludere et sikkerhetstoken som hentes fra Maskinporten på vegne av virksomheten man representerer (f. eks SSB). Hvilke data (API scopes) et sikkerhetstoken har tilgang til er knyttet til Maskinporten-klienten. For å hente et sikkerhetstoken for en klient, kreves det at man autentiserer seg som virksomhet. Dette gjøres ved å signere forespørsler til Maskinporten ved bruk av et virksomhetssertifikat. Det er her Maskinporten Guardian kommer inn i bildet.\nMaskinporten Guardian har tilgang til SSBs virksomhetssertifikat og kan dermed signere forespørsler mot Maskinporten for å hente ut sikkerhetstokens.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#komme-igang",
    "href": "statistikkere/maskinporten-guardian.html#komme-igang",
    "title": "Maskinporten",
    "section": "Komme igang",
    "text": "Komme igang\nHvis en skal ta i bruk et API som er beskyttet av Maskinporten er det noen steg som må gjøres i forkant:\n\n\n\n\n\nflowchart TD\n    A[API-tilbyder gir SSB tilgang til API via Samarbeidsportalen]\n    A --&gt; B[M2M-teamet hos SSB oppretter Maskinporten-klienter]\n    B --&gt; C[Keycloak-klienter for Maskinporten Guardian opprettes]\n\n\n\n\n\n\n\nAPI-tilbyderen må gi SSB tilgang til et API scope. API-tilbydere kan gjøre dette via Samarbeidsportalen hos Digdir. Fra API-tilbyderen får du også annen informasjon om API-ene, som:\n\n\nURL-er som skal benyttes for å snakke med API-et\nNavn på API scopes\nOm det finnes testadata\nDokumentasjon for API-endepunktene\n\n\nNår data er delt av en API-tilbyder, og en har navnet på API scopes, kan M2M-teamet hos SSB kontaktes for å få opprettet Maskinporten-klienter, én pr miljø (f. eks prod og test). De må vite hvilke API scopes og miljøer (test/prod) som skal benyttes. M2M-teamet vil gi deg ID-er (f. eks 12345678-9abc-def0-1234-567890abcdef) for klientene som er blitt opprettet.\n\n\n\n\n\n\n\nDu kan anse ID for en Maskinporten-klient som et slags brukernavn, og behandle dette deretter. Slike ID-er er ikke sensitive i seg selv, men de bør allikevel ikke ligge i åpne git-repoer (SSB-private repoer er OK). Det er heller ingen grunn til å behandle disse som hemmeligheter. Det anbefales at man opererer med Maskinporten-klienter som ekstern konfigurasjon til koden, f. eks på samme måte som man behandler URL-er til API-ene.\n\n\n\n\nNår du har ID for Maskinporten-klienten(e), er neste steg å få opprettet en Keycloak-bruker for Maskinporten Guardian. Se Opprette en Maskinporten Guardian M2M-bruker. Ta kontakt med oss på Dapla via Pureservice dersom du trenger hjelp til dette. Client ID og client secret for Keycloak-brukeren kan hentes fra Secret Manager (mer om det lenger ned). Medlemmer av Dapla-teamet har tilgang til å hente disse.\n\nOm noen på teamet trenger personlig tilgang til API-ene så må det konfigureres i Maskinporten Guardian. Da må vi vite hvilke personer som skal ha denne tilgangen. Les mer om forskjellen på M2M og personlig tilgang lenger ned.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#administrasjon-av-api-integrasjoner-i-ssb",
    "href": "statistikkere/maskinporten-guardian.html#administrasjon-av-api-integrasjoner-i-ssb",
    "title": "Maskinporten",
    "section": "Administrasjon av API-integrasjoner i SSB",
    "text": "Administrasjon av API-integrasjoner i SSB\nDet er M2M-teamet i SSB som administrerer det formelle i forbindelse med integrasjoner mot eksterne API-er. For å få opprettet nye Maskinporten-klienter er det dette teamet man kontakter. De må vite hvilke miljøer (test og/eller prod) og hvilke API scopes som skal benyttes. Hver Maskinporten-klient som opprettes identifiseres av en ID (f. eks 12345678-9abc-def0-1234-567890abcdef), som du vil få tilsendt.\nLegg merke til at det er SSB selv som oppretter og administrerer Maskinporten-klienter. Det gjøres i Samarbeidsportalen hos Difi. Klientene knyttes til API scope(s) (som er delt med SSB av API-tilbyderen). Du kan lese mer om hvordan M2M-teamet administrerer Maskinporten-klienter her.\n\n\n\n\n\n\nAlle med en SSB-epostadresse kan registerere en personlig bruker i Digdir Samarbeidsportalen for å få innsyn i hvilke API-integrasjoner som finnes.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#hvordan-maskinporten-guardian",
    "href": "statistikkere/maskinporten-guardian.html#hvordan-maskinporten-guardian",
    "title": "Maskinporten",
    "section": "Hvordan bruker jeg Maskinporten Guardian?",
    "text": "Hvordan bruker jeg Maskinporten Guardian?\nMaskinporten Guardian er tilgjengelig fra alle SSB og NAIS sine IP-adresser, og kan nås på:\n\nProd: https://guardian.intern.ssb.no\nTest: https://guardian.intern.test.ssb.no\n\nMaskinporten Guardian sine endepunkter er selv beskyttet av Keycloak. To typer brukere støttes:\n\nMaskin-til-maskin (M2M) - Systembruker knyttet til en gitt Maskinporten-klient. For å opptre på vegne av en M2M-bruker autentiserer man seg med en Keycloak client secret. Denne hemmeligheten er lagret i Google Secret Manager og kun Service Accounts eller Dapla-grupper med tilgang kan hente den ut.\nPersonlig - Din egen SSB-bruker. Man kan f. eks bruke dapla-toolbelt for å hente ut sitt personlige Keycloak-token. I tillegg til å autentisere deg må din bruker være autorisert til å gjøre oppslag på vegne av en Maskinporten-klient. Dette styres i konfigurasjonen til Maskinporten Guardian.\n\n\n\n\n\n\n\nMan skal i hovedsak kun anvende M2M-brukere for datautveksling mot API-er som er beskyttet av Maskinporten. Personlige brukere skal kun brukes unntaksvis for enkeltoppslag (f. eks ved feilsøking) mot API-er eller for utvikling og test.\n\n\n\nLegg merke til at Maskinporten Guardian kun er tilgjengelig fra interne SSB-adresser. Bruk følgende URL-er: * Prod: https://guardian.intern.ssb.no * Test: https://guardian.intern.test.ssb.no",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#systemskisse",
    "href": "statistikkere/maskinporten-guardian.html#systemskisse",
    "title": "Maskinporten",
    "section": "Systemskisse",
    "text": "Systemskisse\nFølgende gir en oversikt over hvordan systemer henger sammen. En API-konsument kan f. eks være et Dapla-team som ønsker å hente data, mens en API-tilbyder er en ekstern virksomhet som tilbyr data via Maskinporten. Det er noen forskjeller i flyt avhengig av om Maskinporten Guardian aksesseres med systembruker (M2M) eller personlig bruker.\n\n\n\n\n\n\nFigur 2\n\n\n\n\nFlyt: M2M\n\nAPI-konsumenten henter sin Keycloak client secret for en gitt Maskinporten-klient fra Secret Manager.\nAPI-konsumenten henter et Keycloak sikkerhetstoken ved å bruke client secret fra steg 1.\nAPI-konsumenten henter et Maskinporten sikkerhetstoken fra Maskinporten Guardian ved å bruke Keycloak sikkerhetstoken fra steg 2. En kan alternativt angi andre API scopes enn det som er standard for Maskinporten-klienten, men dette er vanligvis ikke nødvendig.\nMaskinporten Guardian bruker SSB sitt virksomhetssertifikat til å signere en forespørsel om å hente et sikkerhetstoken fra Maskinporten (ref steg 3)\nAPI-konsumenten henter data fra den eksterne API-tilbyderen ved å bruke sikkerhetstoken fra Maskinporten.\n\n\n\nFlyt: Personlig bruker\n\n\n\n\n\n\nSteg 1 og 2 gjelder kun for M2M-brukere. Dersom man aksesserer Maskinporten Guardian med personlig bruker så hentes Keycloak-tokenet f. eks ved hjelp av AuthClient i dapla-toolbelt.\n\n\n\n\nAPI-konsumenten henter et Maskinporten sikkerhetstoken fra Maskinporten Guardian ved å bruke sitt personlige Keycloak sikkerhetstoken. Det må angis hvilken Maskinporten-klient og hvilke API scopes Maskinporten sikkerhetstokenet skal gjelde for. Den personlige brukeren må på forhånd være autorisert (ref Maskinporten Guardian sin tilgangskonfigurasjon) til å kunne hente sikkerthetstokens for Maskinporten-klienten.\nMaskinporten Guardian bruker SSB sitt virksomhetssertifikat til å signere en forespørsel om å hente et sikkerhetstoken fra Maskinporten (ref steg 3)\nAPI-konsumenten henter data fra den eksterne API-tilbyderen ved å bruke sikkerhetstoken fra Maskinporten.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#opprette-en-maskinporten-guardian-m2m-bruker",
    "href": "statistikkere/maskinporten-guardian.html#opprette-en-maskinporten-guardian-m2m-bruker",
    "title": "Maskinporten",
    "section": "Opprette en Maskinporten Guardian M2M-bruker",
    "text": "Opprette en Maskinporten Guardian M2M-bruker\n\n\n\n\n\n\nNote\n\n\n\nDette avsnittet inneholder tekniske instrukser ment for deg som er kjent med Git og som f. eks jobber i et Self-managed Dapla-team. Ta kontakt med oss via Pureservice så hjelper vi deg gjerne med dette.\n\n\nFor å kunne opptre på vegne av Maskinporten-klienten uavhengig av din personlige bruker, må man opprette en Keycloak systembruker. Det gjøres ved å åpne en Pull Request (konfigurasjon som gjennomgås av en tekniker) til keycloak-iac der du angir informasjon som ID for Maskinporten-klient, API scopes og hvem som skal ha tilgang. Legg merke til at du må opprette en klient pr miljø (test og prod). Du kan se bort fra play-miljøet.\n\nEksempelkonfigurasjon\namends \".../pkl/MaskinportenGuardianClient.pkl\"\n\napi_shortname = \"Kort API-beskrivelse (maks 32 tegn)\"\nmaskinporten_client_id = \"12345678-9abc-def0-1234-567890abcdef\"\nmaskinporten_audience = \"https://maskinporten.no/\"\nmaskinporten_default_scopes {\n  \"foo:data1\"\n  \"foo:data2\"\n}\ncredentials_access {\n  \"group:play-foeniks-data-admins@groups.ssb.no\"\n  \"serviceaccount:foo-sa@play-foeniks-p-ab.iam.gserviceaccount.com\"\n}\n\n\n\n\n\n\nNote\n\n\n\nI test skal maskinporten_audience ha verdien https://test.maskinporten.no/. I prod skal det være https://maskinporten.no/ (merk: skråstrek på slutten er viktig)\n\n\nPull Requesten må godkjennes og behandles av en Dapla platformutvikler. Når dette er gjort blir det opprettet en Keycloak-klient, og hemmeligheten som kan brukes for å hente ut sikkerhetstokens for denne klienten er tilgjengelig i Secret Manager.\nSe følgende dokumentasjon for mer informasjon:\n\nDetaljert beskrivelse av konfigurasjonsmuligeter.\nGenerell beskrivelse av hvordan man oppretter en Keycloak-klient\nKeycloak client credentials\n\nTa kontakt med Kundeservice hvis du har spørsmål eller trenger ei hand å halde i.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten"
    ]
  },
  {
    "objectID": "statistikkere/maskinporten-guardian.html#kodeeksempler",
    "href": "statistikkere/maskinporten-guardian.html#kodeeksempler",
    "title": "Maskinporten",
    "section": "Kodeeksempler",
    "text": "Kodeeksempler\nFølgende viser Python kodeeksempler for hvordan man kan hente ut et Maskinporten sikkerhetstoken.\n\nFlyt: M2M\n\"\"\"\nRetrieve maskinporten M2M access token for querying an external API.\n\nThe following code example expects a toml config file to exist named\n&lt;team_uniform_name&gt;-&lt;env&gt;-maskinporten-m2m.toml (e.g. play-foeniks-prod-maskinporten-m2m.toml)\n\nwith contents such as:\n\nkeycloak_url = \"https://auth.test.ssb.no\"\nkeycloak_clients_gcp_project_id = \"keycloak-clients-&lt;p|t&gt;-??\"\nguardian_url = \"https://guardian.intern.test.ssb.no\"\n\n[my-api]\n    maskinporten_client_id = \"12345678-9abc-def0-1234-567890abcdef\"\n\"\"\"\nimport os\nimport re\nimport requests\nimport toml\n\n\nteam_uniform_name = \"play-foeniks\"\napi_name = \"my-api\" # identifies the API in the config.toml file\n\n# Check which environment we're running and load corresponding config\ndapla_env = os.getenv(\"DAPLA_ENVIRONMENT\").lower()\nconfig = toml.load(f\"{team_uniform_name}-{dapla_env}-maskinporten-m2m.toml\")\n\n# Get Maskinporten Guardian M2M Keycloak client credentials from Google Secret Manager\n# The secret's name is deduced from the team name and maskinporten client id\nname = f\"{team_uniform_name}-ssb-maskinporten-{config[api_name]['maskinporten_client_id']}-credentials\"\nsecret = get_secret_version(project_id=config['keycloak_clients_gcp_project_id'],\n                            shortname=name)\n\n# The credentials are stored as yaml. Here we simply use a regex to parse. \nkeycloak_client_id = re.search(r'\"client_id\": \"(.*)\"', secret).group(1)\nkeycloak_client_secret = re.search(r'\"client_secret\": \"(.*)\"', secret).group(1)\n\n# Get Keycloak access token\n# This token includes custom claims with values such as maskinporten_default_scopes\nresponse = requests.post(f\"{config['keycloak_url']}/realms/ssb/protocol/openid-connect/token\",\n    headers={\n        \"Content-type\": \"application/x-www-form-urlencoded\",\n    },\n    auth=(keycloak_client_id, keycloak_client_secret),\n    data={\"grant_type\": \"client_credentials\"}\n)\nkeycloak_access_token = response.json()['access_token']\n\n# Get Maskinporten access token from Maskinporten Guardian (using the Keycloak token from above)\n# Note that you can specify custom scopes in the request body if you need to. Using defaults defined in the client config if not specified.\nrequest_body={}\nresponse = requests.post(f\"{config['guardian_url']}/maskinporten/access-token\",\n            headers={\n                \"Authorization\": f\"Bearer {keycloak_access_token}\",\n                \"Content-type\": \"application/json\",\n            },\n            json=request_body,\n        )\nmaskinporten_access_token = response.json()['accessToken']\n\n# Then use the maskinporten access token to query the external API...\nStøttefunksjon for å hente ut secrets fra Secret Manager\nfrom dapla import AuthClient\nfrom google.cloud import secretmanager\n\ndef get_secret_version(project_id, shortname, version_id='latest'):\n    \"\"\"\n    Access the payload for a given secret version.\n    The user's google credentials are used to authorize that the user have permission\n    to access the secret_id.\n    \n    Args:\n    - project_id (str): ID of the Google Cloud project where the secret is stored.\n    - shortname (str): Name (not full path) of the secret in Secret Manager.\n    - version_id (str, optional): The version of the secret to access. Defaults to 'latest'.\n\n    Returns:\n    - str: The payload of the secret version as a UTF-8 decoded string.\n    \"\"\"\n    client = secretmanager.SecretManagerServiceClient(credentials=AuthClient.fetch_google_credentials())\n    secret_name = f\"projects/{project_id}/secrets/{shortname}/versions/{version_id}\"\n    response = client.access_secret_version(name=secret_name)\n    return response.payload.data.decode(\"UTF-8\")\n\n\nFlyt: Personlig bruker\n\"\"\"\nRetrieve maskinporten personal access token for querying an external API.\n\nThe following code example expects a toml config file to exist named\n&lt;team_uniform_name&gt;-&lt;env&gt;-maskinporten-pers.toml (e.g. play-foeniks-prod-maskinporten-pers.toml)\n\nwith contents such as:\n\nguardian_url = \"https://guardian.intern.test.ssb.no\"\n\n[my-api]\n    maskinporten_client_id = \"12345678-9abc-def0-1234-567890abcdef\"\n    scopes = [\"some:scope1\", \"some:scope2\"]\n\"\"\"\nimport os\nimport requests\nimport toml\nfrom dapla import AuthClient\n\nteam_uniform_name = \"play-foeniks\"\napi_name = \"my-api\" # identifies the API in the config.toml file\n\n# Check which environment we're running and load corresponding config\ndapla_env = os.getenv(\"DAPLA_ENVIRONMENT\").lower()\nconfig = toml.load(f\"{team_uniform_name}-{dapla_env}-maskinporten-pers.toml\")\n\n# Get Keycloak access token\nkeycloak_access_token = AuthClient.fetch_personal_token()\n\n# Get Maskinporten access token from Maskinporten Guardian (using the personal Keycloak token from above)\nrequest_body = {\n  \"maskinportenClientId\": config[api_name]['maskinporten_client_id'],\n  \"scopes\": config[api_name]['scopes']\n}\nresponse = requests.post(f\"{config['guardian_url']}/maskinporten/access-token\",\n            headers={\n                \"Authorization\": f\"Bearer {keycloak_access_token}\",\n                \"Content-type\": \"application/json\",\n            },\n            json=request_body\n        )\n\nmaskinporten_access_token = response.json()['accessToken']\n\n# Finally, use the maskinporten access token to query the external API\nprint(maskinporten_access_token)",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Maskinporten"
    ]
  },
  {
    "objectID": "statistikkere/dapla-toolbelt-metadata.html",
    "href": "statistikkere/dapla-toolbelt-metadata.html",
    "title": "dapla-toolbelt-metadata",
    "section": "",
    "text": "dapla-toolbelt-metadata er en Python-pakke for å jobbe med metadatasystemene på Dapla. Pakken gir brukeren et Python-grensesnitt for å jobbe mot Datadoc og Vardef.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata"
    ]
  },
  {
    "objectID": "statistikkere/dapla-toolbelt-metadata.html#forberedelser",
    "href": "statistikkere/dapla-toolbelt-metadata.html#forberedelser",
    "title": "dapla-toolbelt-metadata",
    "section": "Forberedelser",
    "text": "Forberedelser\ndapla-toolbelt-metadata kan installeres i tjenester på Dapla Lab. Siden det er en Python-pakke så må den installeres i en tjeneste der Python er installert. Deretter gjør du følgende:\n\nÅpne en tjeneste på Dapla Lab med Python installert. Før du åpner tjenesten må du velge å representere team og tilgangsgruppe som har tilgang til dataene som skal dokumenteres.\nInstaller pakken i et ssb-project på følgende måte:\n\n\n\nTerminal\n\npoetry add dapla-toolbelt-metadata\n\nEtter det er du klar for bruke funksjonaliteten i dapla-toolbelt-metadata i en notebook.\n\n\n\n\n\n\nCautionDapla Lab og dapla-toolbelt-metadata\n\n\n\nI Dapla Lab velger man hvilket team og tilgangsgruppe man skal representere før man åpner en tjeneste. Hvis man f.eks. skal dokumentere et datasett i datatilstanden inndata for et team som heter dapla-felles, så må man logge seg inn i en tjeneste som dapla-felles-developers, hvis ikke har man ikke tilgang til datasettet.\nI tillegg så benytter Vardef bl.a. informasjon om hvilket team og tilgangsgruppe du logget deg inn som, for definere hvilket team som blir eier av en nyopprettet variabeldefinisjon. På samme måte bruker Vardef denne informasjonen til å avgjøre om en bruker har tilgang til å endre en definisjon. F.eks. vil en bruker som logger seg inn som dapla-felles-developers og oppretter en ny definisjon, så vil dapla-felles stå som eier av definisjonen og det vil kun være medlemmer av dette teamet som kan gjøre endringer i definisjonen.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata"
    ]
  },
  {
    "objectID": "statistikkere/deling-av-data.html",
    "href": "statistikkere/deling-av-data.html",
    "title": "Deling av data",
    "section": "",
    "text": "Dapla-team kan dele data mellom team via såkalte delt-bøtter. Hvert team kan opprette de delt-bøttene de har behov for, og deretter gi tilgang til grupper i andre team. Opprettelse av bøtter skal følge retningslinjene som er definert her.\nMan kan også dele data ved hjelp av automatiseringstjensten delomaten.",
    "crumbs": [
      "Manual",
      "Data",
      "Deling av data"
    ]
  },
  {
    "objectID": "statistikkere/deling-av-data.html#forberedelser",
    "href": "statistikkere/deling-av-data.html#forberedelser",
    "title": "Deling av data",
    "section": "Forberedelser",
    "text": "Forberedelser\nFor å opprette delt-bøtter må man først skru på featuren shared-buckets for teamet. Det gjøres ved at en på teamet gjør følgende:\n\n\n\nGå til IaC-repoet til teamet på Github.\nÅpne filen ./infra/projects.yaml.\nLegg til en linje med shared-buckets under features i miljøet du ønsker, slik som vist til høyre.\nOpprette en PR med endringen.\nBe en i gruppa data-admins se over endringen og godkjenne.\nKjør atlantis plan og atlantis apply slik som beskrevet her.\n\n\n\n\n\n\n\nprojects.yaml\n\nteam_uniform_name: dapla-example\n\nprojects:\n  - project_name: dapla-example\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service\n      - shared-buckets",
    "crumbs": [
      "Manual",
      "Data",
      "Deling av data"
    ]
  },
  {
    "objectID": "statistikkere/deling-av-data.html#typer-av-delt-bøtter",
    "href": "statistikkere/deling-av-data.html#typer-av-delt-bøtter",
    "title": "Deling av data",
    "section": "Typer av delt-bøtter",
    "text": "Typer av delt-bøtter\nDet finnes to typer av delt-bøtter som team kan opprette:\n\nStandard\nStandard er en type delt-bøtte som teamet selv har lese- og skriverettigheter til bøtta. Dvs. at den er ment for deling av data som det delende team også har tilgang til i produkt-bøtta.\nDelomat\nDelomat er en type delt-bøtte som kan benyttes med Delomaten-tjenesten. Denne benyttes når delende teamet skal dele personidentifiserende informasjon (PII) på en annen måte enn de selv har tilgang til. F.eks. hvis et team som jobber med pseudonymisert PII, men må dele en versjon av datasettet hvor PII er i klartekst.\n\nHvilken type delt-bøtte som skal opprettes vil avgjøres av om delende og konsumerende behandler PII likt eller ikke.",
    "crumbs": [
      "Manual",
      "Data",
      "Deling av data"
    ]
  },
  {
    "objectID": "statistikkere/deling-av-data.html#opprettelse-av-bøtte",
    "href": "statistikkere/deling-av-data.html#opprettelse-av-bøtte",
    "title": "Deling av data",
    "section": "Opprettelse av bøtte",
    "text": "Opprettelse av bøtte\nOpprettelse av delt-bøtter gjøres i teamets IaC-repo. For å opprette en delt-bøtte må man legge til en linje i fila ./infra/projects/&lt;teamnavn&gt;-prod/buckets-shared.yaml. Du legger bare til kortnavnet for bøtta, altså den delen av bøtte-navnet som teamet kan styre selv, og hvilken type bøtte som ønskes.\n\n\nbuckets-shared.yaml\n\nversion: kuben/v1\nkind: SharedBuckets\nbuckets:\n- name: ameld\n  type: standard\n- name: ledstill\n  type: standard\n- name: ameld-pii\n  type: delomat\n\nI eksempelet over så opprettes to standardbøtter med navn ameld og ledstill. I tillegg opprettes bøtta ameld-pii for å dele en versjon av ameld der PII er i klartekst. Sistnevnte må deles via Delomaten, siden delende team ikke skal ha tilgang til disse dataene.\nDe fulle bøttenavnene som blir opprettet fra eksempelet over vil være:\n\nssb--data-delt-ameld-prod\nssb--data-delt-ledstill-prod\nssb--data-delt-delomat-ameld-pii-prod\n\nEtter at endringen er gjort i buckets-shared.yaml, så gjør du følgende:\n\nOpprette en PR på repoet med endringen.\nFå en i gruppen data-admins til å gå gjennom og godkjenne.\nKjør atlantis plan og atlantis apply som beskrevet her.\n\n\n\n\n\n\n\nWarningNavngivning av bøtter\n\n\n\nNavngivning av bøtter følger RFC 1123. Dvs. at navnet må følge disse reglene:\n\nMax 63 tegn i navnet (gjelder fullt navn på bøtta)\nkun små bokstaver a-z og sifre 0-9\ntillatt med -\ndet er ikke tillatt med _",
    "crumbs": [
      "Manual",
      "Data",
      "Deling av data"
    ]
  },
  {
    "objectID": "statistikkere/deling-av-data.html#tilgangsstyring",
    "href": "statistikkere/deling-av-data.html#tilgangsstyring",
    "title": "Deling av data",
    "section": "Tilgangsstyring",
    "text": "Tilgangsstyring\nTilgangsstyring kan skje i samme steg som opprettelse av bøtta, men her beskrives det som et eget steg for de tilfellene der man ønsker å gi nye tilganger til eksisterende delt-bøtter.\nTilganger til delt-bøtter kan gis av teamet som eier bøtta. Hvem som har tilgang blir definert i fila ./infra/projects/&lt;teamnavn&gt;-prod/buckets-shared.yaml i IaC-repoet til teamet. Det er den samme filen som hvor selve bøtta ble opprettet.\nHvem som skal ha tilgang angis under sharedWith for hver bøtte, og dette gir tilgangsgruppen som oppgis lesetilgang til filene i bøtta.\n\n\nbuckets-shared.yaml\n\nversion: kuben/v1\nkind: SharedBuckets\nbuckets:\n- name: ameld\n  type: standard\n  sharedWith:\n  - play-obr-b-developers\n- name: ledstill\n  type: standard\n  sharedWith:\n  - play-obr-b-developers  \n- name: ameld-pii\n  type: delomat\n  sharedWith:\n  - dapla-felles-developers\n\nOver ser vi at teamet dapla-example har tre delt-bøtter:\n\nssb-dapla-example-data-delt-ameld-prod\nssb-dapla-example-data-delt-ledstill-prod\nssb-dapla-example-data-delt-delomat-ameld-pii-prod\n\nDe to første bøttene har de gitt lese- og skrivetilgang til developers i play-obr-b. Den tredje bøtten er en delomat der data-admins i dapla-felles har lesetilgang (uten at det delende teamet har det).\nNår man har gjort endringer i iam.yaml så gjør man følgende:\n\nOpprette en PR med endringen.\nFå en i gruppen data-admins til å gå gjennom endringen og godkjenne.\nKjør atlantis plan og atlantis apply som beskrevet her.",
    "crumbs": [
      "Manual",
      "Data",
      "Deling av data"
    ]
  },
  {
    "objectID": "blog/posts/2025-06-10-hurtigguide-vardef/index.html",
    "href": "blog/posts/2025-06-10-hurtigguide-vardef/index.html",
    "title": "Hurtigguide til migrering fra Vardok til Vardef",
    "section": "",
    "text": "I denne artikkelen gis det en kjapp veiledning for migrerering av en variabeldefinisjon fra Vardok til Vardef. Mer detaljerte ressurser finnes her:"
  },
  {
    "objectID": "blog/posts/2025-06-10-hurtigguide-vardef/index.html#finn-fram-vardok-id",
    "href": "blog/posts/2025-06-10-hurtigguide-vardef/index.html#finn-fram-vardok-id",
    "title": "Hurtigguide til migrering fra Vardok til Vardef",
    "section": "1 Finn fram Vardok-ID",
    "text": "1 Finn fram Vardok-ID\nAlle seksjonsledere har fått tilsendt en oversikt over hvilke variabeldefinisjoner i Vardok som din seksjon er ansvarlig for. I oversikten finner man også ID-ene til variabeldefinisjonene, og man må vite Vardok-ID’en til variabeldefinisjonen man skal migrere før man begynner."
  },
  {
    "objectID": "blog/posts/2025-06-10-hurtigguide-vardef/index.html#avgjør-eierskap",
    "href": "blog/posts/2025-06-10-hurtigguide-vardef/index.html#avgjør-eierskap",
    "title": "Hurtigguide til migrering fra Vardok til Vardef",
    "section": "2 Avgjør eierskap",
    "text": "2 Avgjør eierskap\nDiskuter før migreringen hvilket Dapla-team på seksjonen som skal “eie” variabeldefinisjonen. Det er kun ett team som kan være ansvarlig, og man definerer hvilket team det skal være når man velger team og tilgangsgruppe ved oppstart av Vardef-forvaltning (forklart i neste steg)."
  },
  {
    "objectID": "blog/posts/2025-06-10-hurtigguide-vardef/index.html#starte-vardef-forvaltning",
    "href": "blog/posts/2025-06-10-hurtigguide-vardef/index.html#starte-vardef-forvaltning",
    "title": "Hurtigguide til migrering fra Vardok til Vardef",
    "section": "3 Starte Vardef-forvaltning",
    "text": "3 Starte Vardef-forvaltning\n\nSelve migreringen gjøres fra en tjeneste som heter Vardef-forvaltning i Dapla Lab. Ønsker man å teste litt hvordan systemet fungerer uten fare for å gjøre noe galt, så kan man åpne tjenesten i test-miljøet på Dapla Lab: https://lab.dapla-test.ssb.no/. Her jobber man mot en test-versjon av Vardef-basen som blir slettet med jevne mellomrom, og ingen informasjon blir eksponert for eksterne. Ønsker man å migrere “på ekte”, så logger seg man inn i prodmiljøet: https://lab.dapla.ssb.no/."
  },
  {
    "objectID": "blog/posts/2025-06-10-hurtigguide-vardef/index.html#selve-migrering",
    "href": "blog/posts/2025-06-10-hurtigguide-vardef/index.html#selve-migrering",
    "title": "Hurtigguide til migrering fra Vardok til Vardef",
    "section": "4 Selve migrering",
    "text": "4 Selve migrering\nInne tjenesten Vardef-forvaltning så ligger det ferdige notebooks for vanlige arbeidsflyter i Vardef. For å migrere fra Vardok til Vardef så åpner du notebooken migrerer_variabeldefinisjoner.ipynb under mappen ./variable_definitions/.\nFor å migrere variabeldefinisjonen med ID’en 10 i Vardok, så gjør du følgende:\n\nKjør første kodecelle\nI andre kodecelle så fyller du inn Vardok-ID i variabelen min_id_liste. Under har jeg fylt inn 10:\n\n\n\nNotebook\n\nmin_id_liste = [10]\nprint(f\"✅ {len(min_id_liste)} vil bli migrert\")\n\n\nDeretter kjører du cellen.\nNår du kjører den neste cellen så skjer selve migreringen. Hvis den vellykket så får du følgende beskjed i notebooken: INFO: ✅ Successfully migrated variable definition 'wkapinnt' with ID 'LlT6N-cz', bare at kortnavn og ID er unikt for din variabel. VIKTIG: Husk kortnavnet siden du trenger det i neste steg.\n\nOg med det så er variabelen migrert fra Vardok til Vardef. Variabeldefinisjonen har nå fått utledet alle informasjon automatisk som er mulig å utlede, og resten må brukeren fylle inn selv. Husk at definisjonen nå er lagret i databasen til Vardef, men med status UTKAST. Dvs. at man kan endre på all informasjon og den er ikke publisert enda. For å publisere må man redigere innholdet i utkastet, og deretter kjøre en annen notebook som gjør selve publiseringen. Dette er forklart under."
  },
  {
    "objectID": "blog/posts/2025-06-10-hurtigguide-vardef/index.html#rediger-innhold",
    "href": "blog/posts/2025-06-10-hurtigguide-vardef/index.html#rediger-innhold",
    "title": "Hurtigguide til migrering fra Vardok til Vardef",
    "section": "5 Rediger innhold",
    "text": "5 Rediger innhold\nFor å redigere innholdet i et utkast i Vardef så kan man kjøre notebooken rediger_utkast_variabeldefinisjon.ipynb. Da gjør man følgende:\n\nÅpne notebooken rediger_utkast_variabeldefinisjon.ipynb\nKjør gjennom de to første cellene og skriv inn kortnavnet til den migrerte variabelen (ble printet ut når du migrerte).\nEtter kjøringen over ble det skrevet en yaml-fil med innholdet i variabeldefinisjonen. Her kan du gjøre endringer direkte i fila.\nNår du er ferdig med å gjøre endringer så kan du kjøre den siste kodecellen for å lagre arbeidet ditt i et nytt utkast. Husk at du bør lagre ofte og man kan redigere utkast så mange ganger man vil.\n\nNår en variabeldefinisjon er migrert fra Vardok til et utkast i Vardef, så kan man begynne å redigere innholdet. Det er obligatorisk å gå gjennom all informasjon og kvalitetssikre innholdet. Hvis det er behov for å avklare innholdet med andre i SSB, så skal dette gjøres før den publiseres. Når en variabel først er publsiert eksternt i Vardef så kan den ikke endres, kun opprette nye gyldighetsperioder eller gjøre mindre endringer."
  },
  {
    "objectID": "blog/posts/2025-06-10-hurtigguide-vardef/index.html#publiser",
    "href": "blog/posts/2025-06-10-hurtigguide-vardef/index.html#publiser",
    "title": "Hurtigguide til migrering fra Vardok til Vardef",
    "section": "6 Publiser",
    "text": "6 Publiser\nNår man er ferdig med redigere innholdet i variabeldefinisjonen og har publisert det til utkast, så kan man bruke notebooks til å publisere internt (publiser_variabeldefinisjon_internt.ipynb), eller publisere eksternt (publiser_variabeldefinisjon_eksternt.ipynb).\n🥳Gratulerer! Du har nå migrert en variabeldefinisjon fra Vardok til Vardef."
  },
  {
    "objectID": "blog/posts/2025-06-12-dapla-auth-client/index.html",
    "href": "blog/posts/2025-06-12-dapla-auth-client/index.html",
    "title": "Separering av autentisering i Dapla: Introduksjon av dapla-auth-client",
    "section": "",
    "text": "I dette blogginnlegget presenterer vi endringen der autentiseringsklassen er flyttet ut fra dapla-toolbelt og inn i en egen Python-pakke kalt dapla-auth-client. Hensikten er å gjøre det mulig å installere og bruke autentisering isolert, uten å måtte laste inn hele verktøykassen i dapla-toolbelt."
  },
  {
    "objectID": "blog/posts/2025-06-12-dapla-auth-client/index.html#innledning",
    "href": "blog/posts/2025-06-12-dapla-auth-client/index.html#innledning",
    "title": "Separering av autentisering i Dapla: Introduksjon av dapla-auth-client",
    "section": "",
    "text": "I dette blogginnlegget presenterer vi endringen der autentiseringsklassen er flyttet ut fra dapla-toolbelt og inn i en egen Python-pakke kalt dapla-auth-client. Hensikten er å gjøre det mulig å installere og bruke autentisering isolert, uten å måtte laste inn hele verktøykassen i dapla-toolbelt."
  },
  {
    "objectID": "blog/posts/2025-06-12-dapla-auth-client/index.html#hvorfor-separere-autentisering",
    "href": "blog/posts/2025-06-12-dapla-auth-client/index.html#hvorfor-separere-autentisering",
    "title": "Separering av autentisering i Dapla: Introduksjon av dapla-auth-client",
    "section": "Hvorfor separere autentisering?",
    "text": "Hvorfor separere autentisering?\n\nRedusere avhengigheter\nMange brukere trenger kun autentiseringsbiten for å hente tokens til Google Cloud eller Keycloak. Tidligere måtte man installere og importere hele dapla-toolbelt, selv om man bare trengte én liten del. Ved å splitte ut en egen pakke blir installasjonen lettere og mer strømlinjeformet:\n [project]\n dependencies = [\n     \"dapla-auth-client &gt;=1.0.0\",\n ]\nModularitet og enklere vedlikehold\nVed å holde autentiseringsflyten adskilt fra resten av dapla-toolbelt, kan vi lansere feilfikser og nye funksjoner for OAuth2/Keycloak uten å påvirke de andre modulene.\nLettvektskode for sluttbruker\nÅ importere dapla-toolbelt dro med seg en rekke avhengigheter. Med dapla-auth-client får man kun det som trengs for autentisering, og slipper unødvendige pakker.\nSkalerbarhet for fremtidige utvidelser\nEn dedikert autentiseringspakke gir oss frihet til å legge til flere Identity Providers eller nye token-flows senere uten å måtte endre i dapla-toolbelt."
  },
  {
    "objectID": "blog/posts/2025-06-12-dapla-auth-client/index.html#overgang-fra-gammelt-til-nytt-bruk",
    "href": "blog/posts/2025-06-12-dapla-auth-client/index.html#overgang-fra-gammelt-til-nytt-bruk",
    "title": "Separering av autentisering i Dapla: Introduksjon av dapla-auth-client",
    "section": "Overgang fra gammelt til nytt bruk",
    "text": "Overgang fra gammelt til nytt bruk\n\nGammelt oppsett\nfrom google.cloud import storage\nfrom dapla import AuthClient\n\nstorage_client = storage.Client(\n    credentials=AuthClient.fetch_google_credentials()\n)\nHer krevde vi eksplisitt innlasting av credentials via AuthClient.fetch_google_credentials() fra dapla-toolbelt. Det medførte at hele dapla-toolbelt ble installert, med alle avhengigheter.\n\n\nNytt oppsett\nfrom google.cloud import storage\n\nstorage_client = storage.Client()\nNå håndteres Google-autentisering automatisk via Application Default Credentials (ADC). Vi trenger ikke lenger å kalle AuthClient.fetch_google_credentials(), og vi slipper å importere hele dapla-toolbelt."
  },
  {
    "objectID": "blog/posts/2025-06-12-dapla-auth-client/index.html#installere-og-ta-i-bruk-dapla-auth-client",
    "href": "blog/posts/2025-06-12-dapla-auth-client/index.html#installere-og-ta-i-bruk-dapla-auth-client",
    "title": "Separering av autentisering i Dapla: Introduksjon av dapla-auth-client",
    "section": "Installere og ta i bruk dapla-auth-client",
    "text": "Installere og ta i bruk dapla-auth-client\n\nInstaller pakken\n\npoetry add dapla-auth-client\n\nBytt import\n\nDersom du tidligere importerte slik:\nfrom dapla import AuthClient\nbytter du til:\nfrom dapla_auth_client import AuthClient\n\nFjern eksplisitte kall til Google‐credentials\n\nBytt ut:\nstorage_client = storage.Client(\n    credentials=AuthClient.fetch_google_credentials()\n)\nmed:\nstorage_client = storage.Client()\nFor funksjonen fetch_personal_token() for Keycloak i Dapla Lab, importerer du tilsvarende fra dapla_auth_client, men API-et er identisk:\nfrom dapla_auth_client import AuthClient\n\ntoken = AuthClient.fetch_personal_token()\nDe deprecated-metodene (fetch_google_credentials(), fetch_google_token(), osv.) vil fortsatt være tilgjengelige i dapla-auth-client en stund til, men gir advarsler ved bruk og fjernes i kommende versjoner. Vi anbefaler å migrere til ADC-mønsteret så snart som mulig.\nFor dere som bruker dapla-toolbelt og ikke har behov for å bruke AuthClient direkte, er det eneste dere trenger å gjøre å oppgradere dapla-toolbelt til nyeste versjon. Vi tar oss av alt det tekniske i bakgrunnen, slik at overgangen blir sømløs for dere."
  },
  {
    "objectID": "blog/posts/2025-06-12-dapla-auth-client/index.html#oppsummering",
    "href": "blog/posts/2025-06-12-dapla-auth-client/index.html#oppsummering",
    "title": "Separering av autentisering i Dapla: Introduksjon av dapla-auth-client",
    "section": "Oppsummering",
    "text": "Oppsummering\nVed å skille autentiseringsklassen ut i dapla-auth-client:\n\nFår brukerne en lettere installasjon når de kun trenger autentisering.\nUnngår man unødvendige avhengigheter fra dapla-toolbelt.\nGjør vi koden mer modulær og enkel å vedlikeholde.\nKan man bruke standard storage.Client() eller lignende funksjoner for Google Cloud, uten å tenke på credentials-henting.\n\nLykke til med migreringen!"
  },
  {
    "objectID": "blog/posts/2024-07-11-pseudo-testdata/index.html",
    "href": "blog/posts/2024-07-11-pseudo-testdata/index.html",
    "title": "Pseudonymisering med testdata",
    "section": "",
    "text": "Den strenge tilgangsstyringen til pseudonymiseringsfunksjonaliteten på Dapla gjør at det er vanskelig for brukere å bli kjent med funksjonaliteten ved bruk av produksjonsdata. Derfor bør alle som jobber med dette starte med å bruke testdata og jobbe i test-miljøet på Dapla."
  },
  {
    "objectID": "blog/posts/2024-07-11-pseudo-testdata/index.html#importere",
    "href": "blog/posts/2024-07-11-pseudo-testdata/index.html#importere",
    "title": "Pseudonymisering med testdata",
    "section": "Importere",
    "text": "Importere\nFørst så importerer vi noen biblioteker som vi skal benytte. Du kan benytte standard-kernelen som heter python3, for der er biblioteket som pseudonymiserer, dapla-toolbelt-pseudo, tilgjengelig.\n\nimport json\n\nimport dapla as dp\nimport pandas as pd\nfrom dapla_pseudo import Depseudonymize, Pseudonymize\nfrom dapla_pseudo.constants import MapFailureStrategy\nfrom dapla_pseudo.utils import convert_to_date\nfrom IPython.display import JSON\n\nVersjonen av dapla-toolbelt-pseudo er 2.1.2.\nDataene vi skal bruke syntetiske fødselsnummer fra testversjonen SNR-katalogen. På den måten får vi også testet pseudonymiseringen via SNR-katalogen som er veldig vanlig i SSB. Denne SNR-katalogen ligger som en fil i en bøtte som alle i SSB har tilgang til.\n\npath = \"gs://ssb-dapla-felles-data-produkt-test/freg/snr_kat.csv\"\n\ndf = dp.read_pandas(path, file_format=\"csv\", dtype={\"fnr\": str, \"fnr_date\": str})\n\ndf.head(n=5).style.set_table_styles([\n    {'selector': 'th', 'props': [('text-align', 'left')]}\n])\n\n\n\nTabell 1: Syntetisk versjon av SNR-katalogen\n\n\n\n\n\n\n\n\n \nfnr\ncurrent_fnr\nsnr\ncurrent_snr\nfnr_date\ncurrent_fnr_date\n\n\n\n\n0\n16890249063\n16890249063\n026mxd3\n026mxd3\n20201222\n20201222\n\n\n1\n15854996565\n15854996565\n34qm7pt\n34qm7pt\n20201222\n20201222\n\n\n2\n27871547810\n27871547810\n53uxelp\n53uxelp\n20201222\n20201222\n\n\n3\n50889200399\n50889200399\nf35lbnf\nf35lbnf\n20201222\n20201222\n\n\n4\n22919199052\n22919199052\nc2hxvdv\nc2hxvdv\n20201222\n20201222\n\n\n\n\n\n\n\n\nFra Tabell 1 ser vi at datasettet inkluderer en del kolonner. For utforsking av pseudonymiseringsfunksjonalitet så trenger vi kun fnr-kolonnen."
  },
  {
    "objectID": "blog/posts/2024-07-11-pseudo-testdata/index.html#forberedelse-av-datasettet",
    "href": "blog/posts/2024-07-11-pseudo-testdata/index.html#forberedelse-av-datasettet",
    "title": "Pseudonymisering med testdata",
    "section": "Forberedelse av datasettet",
    "text": "Forberedelse av datasettet\nLa oss kun beholde fnr-kolonnen og kopiere den en ny kolonne slik at vi enklere kan sammenligne før og etter pseudonymisering. I tillegg kutter jeg antall rader til 10, siden vi ikke trenger noe mer for formålet her.\n\ndf2 = df.head(n=10)\ndf3 = df2[[\"fnr\"]]\ndf4 = df3.copy()\ndf4['fnr_original'] = df4['fnr']\n\ndf4.head(n=5).style.set_table_styles([\n    {'selector': 'th', 'props': [('text-align', 'left')]}\n])\n\n\n\nTabell 2: Nedstrippet versjon av SNR-katalogen\n\n\n\n\n\n\n\n\n \nfnr\nfnr_original\n\n\n\n\n0\n16890249063\n16890249063\n\n\n1\n15854996565\n15854996565\n\n\n2\n27871547810\n27871547810\n\n\n3\n50889200399\n50889200399\n\n\n4\n22919199052\n22919199052\n\n\n\n\n\n\n\n\nHvis du ønsker å teste hvordan krypteringsalgoritmene fungerer med kolonner som inneholder navn, så kan vi generere noe data med et også.\n\nfornavn = [\n    \"Jo\",\n    \"Hans-August\",\n    \"Nils\",\n    \"Eva\",\n    \"Lars\",\n    \"Øyvind\",\n    \"Kenneth\",\n    \"Johnny\",\n    \"Rupinder\",\n    \"Nicolas\",\n]\netternavn = [\n    \"Nordman\",\n    \"Karlsen\",\n    \"Nordmann\",\n    \"Karlsen\",\n    \"Carlsen\",\n    \"Nordmann\",\n    \"Carlsen\",\n    \"Nordmann\",\n    \"Karlsen\",\n    \"Normann\",\n]\n\ndf4['fornavn'] = fornavn\ndf4['etternavn'] = etternavn\ndf5 = df4.copy()\n\nTil slutt legger vi på noen ugyldige fødselsnummer slik at vi får testet hvordan algoritmene håndterer dette.\n\nnew_row1 = pd.DataFrame(\n    [\n        {\n            \"fnr\": \"99999999999\",\n            \"fnr_original\": \"99999999999\",\n            \"fornavn\": \"Michael\",\n            \"etternavn\": \"Norman\",\n        }\n    ]\n)\ndf6 = pd.concat([df5, new_row1], ignore_index=True)\n\nnew_row2 = pd.DataFrame(\n    [{\"fnr\": \"XX\", \"fnr_original\": \"XX\", \"fornavn\": \"Ola Glenn\", \"etternavn\": \"Gåås\"}]\n)\ndf7 = pd.concat([df6, new_row2], ignore_index=True)\n\nnew_row3 = pd.DataFrame(\n    [\n        {\n            \"fnr\": \"X8b7k28\",\n            \"fnr_original\": \"X8b7k28\",\n            \"fornavn\": \"Lars\",\n            \"etternavn\": \"Gaas\",\n        }\n    ]\n)\ndf8 = pd.concat([df7, new_row3], ignore_index=True)\n\ndf8.head(n=5).style.set_table_styles([\n    {'selector': 'th', 'props': [('text-align', 'left')]}\n])\n\n\n\nTabell 3: Datasett for å teste pseudonymiseringsfunksjonalitet\n\n\n\n\n\n\n\n\n \nfnr\nfnr_original\nfornavn\netternavn\n\n\n\n\n0\n16890249063\n16890249063\nJo\nNordman\n\n\n1\n15854996565\n15854996565\nHans-August\nKarlsen\n\n\n2\n27871547810\n27871547810\nNils\nNordmann\n\n\n3\n50889200399\n50889200399\nEva\nKarlsen\n\n\n4\n22919199052\n22919199052\nLars\nCarlsen\n\n\n\n\n\n\n\n\nTabell 3 viser datasettet vi skal bruke til å teste med."
  },
  {
    "objectID": "blog/posts/2024-07-11-pseudo-testdata/index.html#pseudonymisering",
    "href": "blog/posts/2024-07-11-pseudo-testdata/index.html#pseudonymisering",
    "title": "Pseudonymisering med testdata",
    "section": "Pseudonymisering",
    "text": "Pseudonymisering\nNå kan vi begynne å leke med dataene. Det første vi kan gjøre er å pseudonymisere med den mest vanlige algoritmen som benyttes i produksjon: Papis-nøkkelen.\n\nresult = (\n    Pseudonymize.from_pandas(df8)\n    .on_fields(\"fnr\")\n    .with_stable_id()\n    .run()\n)\nresult.to_pandas()\n\nUnexpected length of metadata: 2\n\n\n\n\nTabell 4: Pseudonymiserer med Papis-algoritmen\n\n\n\n\n\n\n\n\n\n\nfnr\nfnr_original\nfornavn\netternavn\n\n\n\n\n0\nBnQe23u\n16890249063\nJo\nNordman\n\n\n1\nI1mQmBP\n15854996565\nHans-August\nKarlsen\n\n\n2\neVbGYLy\n27871547810\nNils\nNordmann\n\n\n3\nO4jegSM\n50889200399\nEva\nKarlsen\n\n\n4\n6JhhRKi\n22919199052\nLars\nCarlsen\n\n\n5\nvygqpLq\n66821775168\nØyvind\nNordmann\n\n\n6\nJWzitL8\n13824498614\nKenneth\nCarlsen\n\n\n7\noIhVngD\n46927100797\nJohnny\nNordmann\n\n\n8\n0y8qqUZ\n16907699157\nRupinder\nKarlsen\n\n\n9\nFNnp5AL\n10920998203\nNicolas\nNormann\n\n\n10\n4yI2BlkviaI\n99999999999\nMichael\nNorman\n\n\n11\nXX\nXX\nOla Glenn\nGåås\n\n\n12\ntKHXmUl\nX8b7k28\nLars\nGaas\n\n\n\n\n\n\n\n\n\n\nI Tabell 4 ser vi at kolonnen fnr har blitt pseudonymisert. Det er også verdt å legge merke til at kolonnen ikke endrer navn. Grunnen til at lengden på verdiene som er pseudonymiserte er på 7 tegn for de opprinnelige fødselsnummerne, er at det først skjer en oversetting fra fnr til snr før det pseudonymiseres, og snr-nummerserien er på 7 tegn. Med andre ord så preserverer algoritmen lengden på snr-nummeret siden det er dette som pseudonymiseres.\nDet er også verdt å merke seg at verdier som er kortere enn 4 i lengde, f.eks. XX i rad 11, ikke blir pseudonymisert i det hele tatt. Verdier som er 4 eller lengre, vil bli pseudonymisert selv om de ikke fikk treff i SNR-katalogen.\n\nMetadata\nDet genereres også 2 metadata-objekter ved pseudonymisering. Disse er:\n\nresult.datadoc\nresult.metadata\n\nLa oss se nærmere på de:\n\ndata = json.loads(result.datadoc)\ndisplay(data)\n\n{'document_version': '0.0.1',\n 'pseudonymization': {'document_version': '0.1.0',\n  'pseudo_variables': [{'short_name': 'fnr',\n    'data_element_path': 'fnr',\n    'data_element_pattern': '/fnr',\n    'stable_identifier_type': 'FREG_SNR',\n    'stable_identifier_version': '2023-08-31',\n    'encryption_algorithm': 'TINK-FPE',\n    'encryption_key_reference': 'papis-common-key-1',\n    'encryption_algorithm_parameters': [{'keyId': 'papis-common-key-1'},\n     {'strategy': 'skip'}]}]}}\n\n\nDette er metadata som skal integreres i Datadoc etter hvert.\nLa oss se på den andre typen metadata:\n\ndisplay(result.metadata)\n\n{'logs': ['No SID-mapping found for fnr 999********',\n  'No SID-mapping found for fnr X8b****'],\n 'metrics': {'MAPPED_SID': 10, 'FPE_LIMITATION': 1, 'MISSING_SID': 2}}\n\n\nHer ser vo at 10 felt fikk treff i SNR-katalogen, 1 felt var for kort for algoritmen, og 2 felt fikk ikke treff SNR-katalogen. Vi får også se litt fødselsnummeret til de 2 som ikke fikk treff."
  },
  {
    "objectID": "blog/posts/2024-07-11-pseudo-testdata/index.html#depseudonymisering",
    "href": "blog/posts/2024-07-11-pseudo-testdata/index.html#depseudonymisering",
    "title": "Pseudonymisering med testdata",
    "section": "Depseudonymisering",
    "text": "Depseudonymisering\nLa oss ta vare på den pseudonymiserte kolonnen og så depseudonymisere og se om resultatet blir riktig:\n\nresult2 = result.to_pandas()\nresult2['pseudo_fnr'] = result2['fnr']\n\nresult_df = (\n    Depseudonymize.from_pandas(result2)         \n    .on_fields(\"fnr\")                              \n    .with_stable_id()                              \n    .run()                                         \n    .to_pandas() \n)\n\nresult_df.style.set_table_styles([\n    {'selector': 'th', 'props': [('text-align', 'left')]}\n])\n\n\n\nTabell 5: Depseudonymisering av fødselsnummer\n\n\n\n\n\n\n\n\n \nfnr\nfnr_original\nfornavn\netternavn\npseudo_fnr\n\n\n\n\n0\n16890249063\n16890249063\nJo\nNordman\nBnQe23u\n\n\n1\n15854996565\n15854996565\nHans-August\nKarlsen\nI1mQmBP\n\n\n2\n27871547810\n27871547810\nNils\nNordmann\neVbGYLy\n\n\n3\n50889200399\n50889200399\nEva\nKarlsen\nO4jegSM\n\n\n4\n22919199052\n22919199052\nLars\nCarlsen\n6JhhRKi\n\n\n5\n66821775168\n66821775168\nØyvind\nNordmann\nvygqpLq\n\n\n6\n13824498614\n13824498614\nKenneth\nCarlsen\nJWzitL8\n\n\n7\n46927100797\n46927100797\nJohnny\nNordmann\noIhVngD\n\n\n8\n16907699157\n16907699157\nRupinder\nKarlsen\n0y8qqUZ\n\n\n9\n10920998203\n10920998203\nNicolas\nNormann\nFNnp5AL\n\n\n10\n99999999999\n99999999999\nMichael\nNorman\n4yI2BlkviaI\n\n\n11\nXX\nXX\nOla Glenn\nGåås\nXX\n\n\n12\nX8b7k28\nX8b7k28\nLars\nGaas\ntKHXmUl\n\n\n\n\n\n\n\n\nTabell 5 viser at depseudonymiseringen returnerer de opprinnelige fødselsnummerne.\nVidere kan man utforske å pseudonymisere navn ved bruk av ulike algoritmer."
  },
  {
    "objectID": "blog/posts/2024-11-17-lonboard/index.html",
    "href": "blog/posts/2024-11-17-lonboard/index.html",
    "title": "Lonboard",
    "section": "",
    "text": "Lonboard1 er et bibliotek for å vise kart i Jupyter notebooks. Lonboard er bygget på Deck.gl, et GPU akselerert, høytytende, kartvisualiseringsbibliotek for store data. Lonboard er bygget med Anywidget."
  },
  {
    "objectID": "blog/posts/2024-11-17-lonboard/index.html#hvorfor-lonboard",
    "href": "blog/posts/2024-11-17-lonboard/index.html#hvorfor-lonboard",
    "title": "Lonboard",
    "section": "Hvorfor Lonboard?",
    "text": "Hvorfor Lonboard?\nGeopandas’ explore-metode og explore-funksjonen i ssb-sgis-pakken bruker kartvisningsbiblioteket Folium. Hver gang et kart skal vises med Folium, blir kartdataene konvertert til GeoJSON-formatet, som deretter sendes ukomprimert fra Jupyter-serveren til nettleseren. Dette kan føre til lang overføringstid og potensielt høyt minneforbruk i nettleseren, spesielt når man forsøker å vise store datasett, som landsdekkende grunnkretser, tettsteder eller postnummerområder.\nLonboard håndterer store datamengder bedre ved å overføre data mellom serveren og nettleseren i Parquet-format i stedet for GeoJSON, som så leses av Deck.gl. Siden Parquet tilfeldigvis er SSBs standard lagringsformat, kan denne overføringen skje med minimal datakonvertering.\nIkke alle i SSB jobber med Pandas, og for disse brukerne kan Lonboard visualisere tabeller fra DuckDB og PyArrow, i tillegg til Geopandas-tabeller.\nHvor kommer Anywidget inn? Anywidget er et rammeverk for å lage widgets. En widgets lar ta med interaktive komponenter inn i en Jupyter notebook, slik som en datovelger eller en filtrerbar og sorterbar tabell. Litt slik som Dash lar deg gjøre, men rett i Jupyter notebook. Å lage sin egen widget til Juypter er en ganske komplisert afære. Det krever at du pakker både Python kode og Javascript kode. Det krever at du må skrive kode for å støtte alle miljøer som kan kjøre Jupyter notbooks, slik som VScode, Jupyterlab eller Google Colab. Anywidget forenkler prosessen veldig, og sørger for at widget din virker på alle plattformer. Den forenkler kommunikasjonen mellom Python og Javascript siden, så det blir enklere å lage interaktivitet.\nAnywidget er nå installert i Jupyter på Dapla Lab, slik at alle kan prøve ut både Lonboard, samt andre pakker som bygger på Anywidget. Et eksempel på en slik annen pakke er Vega-altair"
  },
  {
    "objectID": "blog/posts/2024-11-17-lonboard/index.html#eksempel-med-grunnkretser-i-innlandet-fylke",
    "href": "blog/posts/2024-11-17-lonboard/index.html#eksempel-med-grunnkretser-i-innlandet-fylke",
    "title": "Lonboard",
    "section": "Eksempel med grunnkretser i Innlandet fylke",
    "text": "Eksempel med grunnkretser i Innlandet fylke\nUnder er eksempel på hvordan man visualisere grunnkretsene i Innlandet fylke i en Jupyterlab Notebook på Dapla Lab. Gjør følgende først:\n\nStart en Jupyter på Dapla Lab.\nOpprett et ssb-project\nInstaller nødvendige pakker poetry add geopandas lonboard numpy mapclassify matplotlib libpysal\nÅpne en ny notebook med kernelen som ble opprettet av ssb-project.\n\nI den nyopprettede notebooken kan deretter hente inn data om grunngretser i Innlandet fylke og visualisere de med Lonboard.\n\nimport geopandas as gpd\nimport lonboard\nfrom lonboard import basemap\nfrom mapclassify import greedy\nfrom matplotlib import colormaps\nimport numpy as np\n\ngrunnkretser = gpd.read_file(\n    \"https://nedlasting.geonorge.no/geonorge/Basisdata/Grunnkretser/GML/Basisdata_34_Innlandet_25833_Grunnkretser_GML.zip\",\n    layer=\"Grunnkrets\",\n    engine=\"pyogrio\",\n    columns=[\"grunnkretsnummer\", \"grunnkretsnavn\", \"kommunenummer\"],\n)\n\ngrunnkretser.head()\n\ncolor = greedy(grunnkretser, strategy=\"balanced\", balance=\"centroid\").map(\n    colormaps[\"Set2\"].colors.__getitem__\n)\ncolor = (np.stack(color.to_numpy()) * 255).astype(np.uint8)\n\ngrunnkretser_wgs84 = grunnkretser.to_crs(4326)\nlayer = lonboard.PolygonLayer.from_geopandas(\n    grunnkretser_wgs84,\n    opacity=0.2,\n    line_miter_limit=1,\n    line_width_units = \"pixels\",\n    get_fill_color=color,\n    get_line_color=[255,255,255],\n    auto_highlight=True,\n)\n\nkart = lonboard.Map(\n    [layer],\n    basemap_style=basemap.CartoBasemap.DarkMatterNoLabels,\n    _height=500,\n)\n\nkart\n\n\n\n\n\n\n    \n    Lonboard export\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLonboard har i motsetning til Folium muligheten for interaktivitet. Hvis du velger en avgrensningsboks i kartet over, men knappen oppe i høyre hjørne, så blir det utvalet tilgjengleig i Python som kart.selected_bounds\nif kart.selected_bounds:\n    xmin, ymin, xmax, ymax = kart.selected_bounds\n    utvalg = grunnkretser_wgs84.cx[xmin:xmax, ymin:ymax]\n    print(f\"Det er {len(utvalg)} grunnkretser i utvalget\")\nelse:\n  print(\"Du har ikke gjort et utvalg.\")\nDu kan også utvikle din egen widget, men selvom Anywidget forenkler prossessen mye, krever dette fremdeles ferdigheter i både Python og Javascript, så det er kansje ikke for alle."
  },
  {
    "objectID": "blog/posts/2024-11-17-lonboard/index.html#footnotes",
    "href": "blog/posts/2024-11-17-lonboard/index.html#footnotes",
    "title": "Lonboard",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nNavnet er et ordspill. Et «longboard», er en type raskt skateboard, et «deck» er den delen av skateboardet du står på. «Lon» en mye brukt forkortelse for «longitude», lengdegrad.↩︎"
  },
  {
    "objectID": "blog/posts/2025-04-04-laste-til-statbank/index.html",
    "href": "blog/posts/2025-04-04-laste-til-statbank/index.html",
    "title": "Hvordan laste mange tabeller i Statistikkbanken?",
    "section": "",
    "text": "Filbeskrivelsene kan lagres som json med en metode, og åpnes med en annen (uten å bruke passord). Om man skal jobbe med mange statbanktabeller i ett løp, så kan en ok inndeling av notebooks være:\n\nEnkelt notebook for å hente alle filbeskrivelser (krever lastepassord)\nEtt notebook per statbanktabell som omformer fra “statistikkfil” til det som skal sendes til statbanken. Kan bruke filbeskrivelsen til avrunding, validering. (krever ikke lastepassord)\nEnkelt notebook for å sende alle tabellene til statbanken (krever lastepassord)\n\nMan kan i teorien lagre filbeskrivelses-jsonfilene hvor man vil, de er ikke å anse som sensitive, men de kan utløpe på dato når metadata i statbanken blir oppdatert. Det kan derfor anbefales at du lagrer dem i Dapla-lab instansen din, dvs. i /work utenfor git-repoet du har klonet. De vil da bli slettet når du sletter tjenesten din, og det kan være ønskelig så de må hentes på nytt neste gang.\nEtt eksempel på dette ligger i Vgogjen-produksjonsløpet.\nFørst oppretter du en notebook for å hente alle filbeskrivelser:\n\n\nnotebook\n\n# Notebook for å hente alle filbeskrivelser\nclient = StatbankClient()  # Krever passord\nfor tab_id in config.statbanktabell_ider:\n    filbesk = client.get_description(tab_nr)\n    filbesk.to_json(f\"~/work/filbesk/{tab_nr}.json\")  # Sørg for at mappe finnes først kanskje\n\nDeretter kan du opprette en notebook per statistikkbanktabell. Under er et eksempel på hvordan en slik notebook kan se ut:\n\n\nnotebook\n\n# Notebooks for alle statbanktabeller \ntab_nr = \"12958\"\nfilbesk = StatbankClient.read_description_json(f\"filbesk/{tab_nr}.json\")  # Krever ikke passord\ndata = filbesk.transferdata_template(tab_ut)\nfilbesk.validate(data)\ndata = filbesk.round_data(data)\nfor datfil in filbesk.subtables.keys():\n    path = (path_root + \"VG_gjforing/\" + datfil).replace(\".dat\", \".parquet\")\n    data[datfil].to_parquet(path)\n\nTil slutt oppretter du en egen notebook som laster alle tabellene til Statistikkbanken:\n\n\nnotebook\n\n# Notebook for å laste alle tabellene \nclient = StatbankClient(date=config.publiseringsdato)  # Krever passord\nfor tab_id in config.statbanktabell_ider:\n    filbesk = client.read_description_json(f\"~/work/filbesk/{tab_nr}.json\")\n    data = {}\n    for datfil in filbesk.subtables.keys():\n        fil = (path_root + \"VG_gjforing/\" + datfil).replace(\".dat\", \".parquet\")\n        data[datfil] = pd.read_parquet(fil)\n        client.transfer(data, tabell)"
  },
  {
    "objectID": "blog/posts/2025-04-25-function-use/index.html",
    "href": "blog/posts/2025-04-25-function-use/index.html",
    "title": "Bruk av funksjoner",
    "section": "",
    "text": "Mange er godt i gang med å kode produksjonsløpene sine i Python og R. Mye virker og er bra, men fortsatt er det mange steder lite bruk av funksjoner. Gruppen for Kvalitet i Kode og Koding (KVAKK) i SSB ønsker derfor med denne artikkelen å oppfordre til økt bruk av funksjoner i statistikkproduksjon."
  },
  {
    "objectID": "blog/posts/2025-04-25-function-use/index.html#hvorfor-bruke-funksjoner",
    "href": "blog/posts/2025-04-25-function-use/index.html#hvorfor-bruke-funksjoner",
    "title": "Bruk av funksjoner",
    "section": "Hvorfor bruke funksjoner?",
    "text": "Hvorfor bruke funksjoner?\nProfesjonell kode, bortsett fra i skript, skrives nesten utelukkende innenfor funksjoner1 og klasser. Noen av årsakene til dette er:\n\nGjenbruk: Ting som gjøres flere ganger bør skilles ut til funksjoner for å unngå duplisert kode. Da kan retting og forbedring gjøres ett sted, i stedet for flere steder.\nMuliggjør automatisert testing: Kode utenfor funksjoner er det vanskelig å lage automatiserte tester for. Kode du ønsker å teste automatisk bør derfor ligge i funksjoner. På sikt bør vi erstatte mest mulig av manuell testing med automatiske tester.\nEnklere å forstå og forklare: Når koden er organisert i små funksjoner med tydelige navn, blir det lettere for deg og kollegaene dine å forstå hva koden gjør, og hver bit blir mindre kompleks. Tenk på funksjoner som «byggeklosser»: én funksjon gjør én ting, og navnet på funksjonen forteller tydelig hva denne tingen er. Det blir som å skrive en god innholdsfortegnelse i et dokument, slik at du raskt ser hva hvert avsnitt handler om.\nAutomatisert kjøring av mange notebooks: Bygger du opp koden din med funksjoner så kan du automatisere kjøring av alt sammen, uten å bruke Papermill, som har problemer med ytelse."
  },
  {
    "objectID": "blog/posts/2025-04-25-function-use/index.html#hvordan-endre-fra-vanlig-notebook-til-bruk-av-funksjoner",
    "href": "blog/posts/2025-04-25-function-use/index.html#hvordan-endre-fra-vanlig-notebook-til-bruk-av-funksjoner",
    "title": "Bruk av funksjoner",
    "section": "Hvordan endre fra “vanlig” notebook til bruk av funksjoner?",
    "text": "Hvordan endre fra “vanlig” notebook til bruk av funksjoner?\nKVAKK har laget en beskrivelse som viser hvordan du kan gå fra en “vanlig” notebook, hvor koden kjøres fra topp til bunn, uten bruk av funksjoner, og automatisert ved bruk av Papermill, og til en tilsvarende notebook med bruk av funksjoner.\nSe Hvordan automatisere Jupyter notebooks ved bruk av funksjoner."
  },
  {
    "objectID": "blog/posts/2025-04-25-function-use/index.html#verktøy-for-å-sjekke-funksjonsbruk",
    "href": "blog/posts/2025-04-25-function-use/index.html#verktøy-for-å-sjekke-funksjonsbruk",
    "title": "Bruk av funksjoner",
    "section": "Verktøy for å sjekke funksjonsbruk",
    "text": "Verktøy for å sjekke funksjonsbruk\nTech-coachene på seksjon IT-partner har laget et skript, analyze_function_use.py, du kan bruke til å sjekke hvor mye bruk av funksjoner det er i python-koden i et repo. Det gir deg andelen av koden som ligger utenfor funksjoner, samt antall funksjoner.\nSkriptet har ingen avhengigheter, så du kan kjøre det uten å installere noe ekstra.\nSkriptet analyserer python-filer i et clonet GitHub-repo. For hver fil teller den opp antall kodelinjer som er innenfor en funksjonsblokk (eller metode, som er det navnet som brukes på funksjoner som hører til en klasse), og også totalt antall kodelinjer i filen. Dette summerer den for alle filer i repoet og beregner andelen av koden som er utenfor funksjonsblokker. Den dropper blanke linjer, kommentarer og filer som ligger tests-katalogen. I tillegg teller den opp antall funksjoner i repoet."
  },
  {
    "objectID": "blog/posts/2025-04-25-function-use/index.html#eksempler",
    "href": "blog/posts/2025-04-25-function-use/index.html#eksempler",
    "title": "Bruk av funksjoner",
    "section": "Eksempler",
    "text": "Eksempler\nEt par eksempler på statistikk-repoer som er gode til å bruke funksjoner er stat-bygganlprod og stat-mnd-el på seksjon 422. Ta gjerne en titt på disse. Den første har kun 4 % av koden utenfor funksjoner og er liten, mens den andre har 634 funksjoner og er en større statistikk.\nTa gjerne også en titt på tech-coach-stat-repoet, som er en eksempelstatistikk som tech-coach’ene har begynt på, og som viser mye god kodepraksis."
  },
  {
    "objectID": "blog/posts/2025-04-25-function-use/index.html#footnotes",
    "href": "blog/posts/2025-04-25-function-use/index.html#footnotes",
    "title": "Bruk av funksjoner",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nMed funksjoner menes her også metoder i klasser.↩︎"
  },
  {
    "objectID": "news/posts/2025-04-06-update-statbank-client/index.html",
    "href": "news/posts/2025-04-06-update-statbank-client/index.html",
    "title": "Dokumentasjon av lasting til Statistikkbanken er oppdatert",
    "section": "",
    "text": "Kapitlet om dapla-statbank-client i Dapla-manualen er skrevet om og url-en er endret til: https://manual.dapla.ssb.no/statistikkere/dapla-statbank-client.html.\nI tillegg har Carl Corneil skrevet et blogginnlegg hvor han beskriver en optimal fremgangsmåte for å laste til flere statistikkbanktabeller i et produksjonsløp."
  },
  {
    "objectID": "news/posts/2024-11-26-statbank-dapla-lab/index.html",
    "href": "news/posts/2024-11-26-statbank-dapla-lab/index.html",
    "title": "Dapla Lab støtter Statbank",
    "section": "",
    "text": "Fra og med forrige uke støtter Statbank-pakken lasting fra Dapla Lab. Dette er i forbindelse med ny versjon av Python-pakken dapla-statbank-client.\nLes mer om dette i innlegget til Carl Corneil på Viva Engage."
  },
  {
    "objectID": "news/posts/2024-12-03-refresh-buckets/index.html",
    "href": "news/posts/2024-12-03-refresh-buckets/index.html",
    "title": "refresh-buckets kommando i Dapla Lab",
    "section": "",
    "text": "Nå kan man kjøre kommandoen refresh-buckets fra terminalen i en tjeneste på Dapla Lab for å oppdatere visningen av en bøttene i filsystemet. Dette kan være nyttig hvis det blir opprettet mapper med dapla-toolbelt eller Kildomaten, og ikke via buckets-mappen i filsystemet. Les mer i kapitlet om Dapla Lab."
  },
  {
    "objectID": "news/posts/2025-04-11-ssb-altinn-python-fix/index.html",
    "href": "news/posts/2025-04-11-ssb-altinn-python-fix/index.html",
    "title": "Ny versjon av ssb-altinn-python",
    "section": "",
    "text": "Ny release av ssb-altinn-python v0.4.12 er sluppet i Kildomaten. Pakken håndterer nå problemet med at UTC-datoformat i feltet ALTINNTIDSPUNKTLEVERT ble trunkert når det sluttet på en eller flere nuller, som igjen forårsaket at utflating av XML’er stoppet/feilet ved konvertering til Oslo-tid."
  },
  {
    "objectID": "news/posts/2025-03-09-epost-dapla-lab/index.html",
    "href": "news/posts/2025-03-09-epost-dapla-lab/index.html",
    "title": "E-postvarslinger om “gamle” tjenester fra Dapla Lab",
    "section": "",
    "text": "Fra og med mandag 10. mars 2025 implementeres automatisk utsending av e-post til brukere som har tjenester som ble startet for mer enn 7 dager siden. Formålet med varslingen er å bevisstgjøre brukerne på viktigheten av å slette tjenester med jevne mellomrom, og starte en ny slik at man kjører på siste versjon.\n\n\n\n\n\n\nNoteEksempel på e-post\n\n\n\nHei Nordmann, Ola\nDu har en eller flere tjenester i Dapla Lab som ble startet for mer enn 7 dager siden. Se oversikt i tabellen under.\nVi anbefaler deg å slette tjenester som ble startet for mer enn 7 dager siden. Når tjenesten slettes, og du senere starter den opp igjen, får du med deg eventuelle forbedringer som er gjort i tjenesten.\n\n\n\n\n\nTjeneste\n\n\nNavn\n\n\nAntall dager\n\n\n\n\n\n\nvscode-python\n\n\nvscode-etlev\n\n\n25\n\n\n\n\nvscode-python\n\n\nvscode-detlev\n\n\n8\n\n\n\n\nHusk å pushe kode til GitHub, og ta vare på andre filer som ligger i tjenestens filsystem, før du sletter. Vær oppmerksom på alle filer på hjemmeområdet ditt slettes når tjenesten slettes.\nLes mer om hvordan man bruker Git og GitHub i SSB."
  },
  {
    "objectID": "news/posts/2025-06-02-datadoc-model/index.html",
    "href": "news/posts/2025-06-02-datadoc-model/index.html",
    "title": "Nytt kapittel om Datadoc",
    "section": "",
    "text": "Det er skrevet et nytt kapittel i Dapla-manualen som gir en mer detaljert beskrivelse av informasjonselementene i Datadoc. Dette kan være nyttig både for produsenter og konsumenter av informasjon i Datadoc. Figur 1 viser beskrivelsen av Navn-feltet.\n\n\n\n\n\n\nFigur 1: Visning av en beskrivelse fra det nye kapitlet."
  },
  {
    "objectID": "news/posts/2025-04-02-ssb-project-dapla-lab/index.html",
    "href": "news/posts/2025-04-02-ssb-project-dapla-lab/index.html",
    "title": "Enklere med ssb-project i Dapla Lab",
    "section": "",
    "text": "Mange brukere har etterspurt muligheten for å kunne angi et repo og at ssb-project build skal kjøres ved oppstart av nye Dapla Lab tjenester. Vi har nå lagt til støtte for dette i tjenestekonfigurasjonen. Brukere kan be om at ssb-project build kjøres ved oppstart, og brukeren vil da komme inn i en ny tjeneste med ferdiginstallerte biblioteker og opprettet kernel. Oppstartstiden for tjenesten blir litt lengre for de som benytter seg av dette.\nEn stor gevinst med denne endringen er at det vil gjør det lettere å slette tjenester jevnlig istedenfor å pause de, siden man kan lagre konfigurasjonen og komme til “dekka bord” neste gang man starter en ny tjeneste. Se eksempel på dette i videoen under.\nDokumentasjon finner du her."
  },
  {
    "objectID": "news/posts/2025-01-14-dapla-ctrl-ny-url/index.html",
    "href": "news/posts/2025-01-14-dapla-ctrl-ny-url/index.html",
    "title": "Dapla Ctrl har fått ny nettadresse",
    "section": "",
    "text": "Nettadressen til Dapla Ctrl er nå endret fra https://ctrl.dapla.ssb.no/ til https://dapla-ctrl.intern.ssb.no/. Dette er gjort som en del av overgangen til NAIS-plattformen."
  },
  {
    "objectID": "news/posts/2025-04-01-daplanytt-mar25/index.html",
    "href": "news/posts/2025-04-01-daplanytt-mar25/index.html",
    "title": "DaplaNytt-møte 1.4.2025",
    "section": "",
    "text": "DaplaNytt for mars 2025 ble avholdt som Teams-møte 1. april 2025.\nSe opptaket her (intern lenke).\nPresentasjonen finner du her.\n\n\n\nSkjembilde fra Teams-møtet DaplaNytt."
  },
  {
    "objectID": "news/posts/2025-04-10-datadoc-update/index.html",
    "href": "news/posts/2025-04-10-datadoc-update/index.html",
    "title": "Oppdatert dokumentasjon for Datadoc",
    "section": "",
    "text": "Dokumentasjonen for Datadoc-delen av dapla-toolbelt-metadata er skrevet om og inkluderer nå flere eksempler på hvordan man jobbe programmatisk med metadata.\nLes kapitlet her."
  },
  {
    "objectID": "news/posts/2025-03-17-bucket-list/index.html",
    "href": "news/posts/2025-03-17-bucket-list/index.html",
    "title": "Tilgang til å liste bøttenavn på Dapla",
    "section": "",
    "text": "Fra og med mandag 17. mars 2025 har alle SSB-brukere fått tilgangen bucket.list på alle GCS-bøtter på Dapla. Det betyr at de kan se navnene til bøttene, men ikke innholdet i bøttene. Formålet med å tillate dette er at alle skal kunne undersøke hvilke bøtter et team har via Google Cloud Console, noe som kan være nyttige i flere sammenhenger."
  },
  {
    "objectID": "news/posts/2025-06-09-vardef-lansering/index.html",
    "href": "news/posts/2025-06-09-vardef-lansering/index.html",
    "title": "Lansering av Vardef",
    "section": "",
    "text": "Onsdag 11. juni lanseres SSBs nye system for variabeldokumentasjon: Vardef. Systemet blir en sentral del av SSBs fremtidige metadataløsning, og alle inviteres til lanseringen i morgen.\nLes mer om arrangementet i denne saken."
  },
  {
    "objectID": "news/posts/2025-03-31-dapla-lab-metrics-mar25/index.html",
    "href": "news/posts/2025-03-31-dapla-lab-metrics-mar25/index.html",
    "title": "Aktivitet på Dapla Lab i mars 2025",
    "section": "",
    "text": "Fom. 15. februar 2025 har Dapla Lab vært arbeidsbenken for SSB-ere som jobber med data på Dapla. I dette viser vi noen interessante tall om hvor mange som jobber på plattformen.\nFigur 1 viser antall maksimalt antall unike brukere av Dapla Lab ila en dag. Figuren er en oversikt over perioden 25. februar tom. 29. mars 2025.\n\n\n\n\n\n\n\n\n\n\nFigur 1: Maks antall daglige brukere på Dapla Lab\n\n\n\n\n\n\nFigur 2 viser antall unike brukere ila dagen på Dapla Lab. Hver linje i figuren er en dag i perioden 25. februar tom. 29. mars 2025. Dermed gir figuren et bilde av hvilket tidspunkt på dagen brukerne jobber. Den 27. mars 2025 endret vi tidspunktet for automatisk pausing av tjenester fra kl 22.00 til 17.00, og dette er vist figuren ved at dager etter 26. mars vises med grønne linjer og resten med grå linjer. Av figuren ser vi dermed at det ganske få som jobber på Dapla Lab etter kl 17.00.\n\n\n\n\n\n\n\n\n\n\nFigur 2: Antall brukere gjennom dagen"
  },
  {
    "objectID": "news/posts/2025-06-23-vardef-publishing/index.html",
    "href": "news/posts/2025-06-23-vardef-publishing/index.html",
    "title": "Publisering i Vardef er midlertidig sperret",
    "section": "",
    "text": "Inntil eierskap av kortnavn er avklart vil det ikke være mulig å publisere variabeldefinisjoner i Vardef.\nDu kan fortsatt:\n\nopprette nye variabeldefinisjoner som utkast\nmigrere variabeldefinisjoner fra Vardok til Vardef\nredigere utkast\nteste publisering i Dapla Lab Test\n\nVi gir beskjed så snart publisering er tilgjengelig igjen."
  },
  {
    "objectID": "news/posts/2025-09-08-datadoc-model-breaking-changes/index.html",
    "href": "news/posts/2025-09-08-datadoc-model-breaking-changes/index.html",
    "title": "Breaking Changes i Datadoc-modellen",
    "section": "",
    "text": "dapla-toolbelt-metadata v0.9.0 slippes ut. Det inneholder noen Breaking Changes i strukturen av modellen. De alle fleste kommer ikke til å merke endringen dersom eksisterende Datadoc ...__DOC.json filer blir oppgradert når de er åpnet og Datadoc-editor blir oppdatert i takt."
  },
  {
    "objectID": "news/posts/2025-09-08-datadoc-model-breaking-changes/index.html#introduksjon",
    "href": "news/posts/2025-09-08-datadoc-model-breaking-changes/index.html#introduksjon",
    "title": "Breaking Changes i Datadoc-modellen",
    "section": "",
    "text": "dapla-toolbelt-metadata v0.9.0 slippes ut. Det inneholder noen Breaking Changes i strukturen av modellen. De alle fleste kommer ikke til å merke endringen dersom eksisterende Datadoc ...__DOC.json filer blir oppgradert når de er åpnet og Datadoc-editor blir oppdatert i takt."
  },
  {
    "objectID": "news/posts/2025-09-08-datadoc-model-breaking-changes/index.html#hvorfor-gjør-vi-endringene",
    "href": "news/posts/2025-09-08-datadoc-model-breaking-changes/index.html#hvorfor-gjør-vi-endringene",
    "title": "Breaking Changes i Datadoc-modellen",
    "section": "Hvorfor gjør vi endringene?",
    "text": "Hvorfor gjør vi endringene?\nEndringene er gjort for å:\n\nKunne i større grad knytte metadata til variablene slik at vi beskriver variablene mer nøyaktig.\nLegge til rette for å ha flere bruksrestriksjoner knyttet til et datasett samtidig."
  },
  {
    "objectID": "news/posts/2025-09-08-datadoc-model-breaking-changes/index.html#hvem-blir-rammet",
    "href": "news/posts/2025-09-08-datadoc-model-breaking-changes/index.html#hvem-blir-rammet",
    "title": "Breaking Changes i Datadoc-modellen",
    "section": "Hvem blir rammet?",
    "text": "Hvem blir rammet?\nDe som bruker dapla-toolbelt-metadata progammatisk og har kode som refererer til følgende metadata-felt eller enum kommer til å måtte oppdatere koden sin:\nmeta.dataset.use_restriction         # Ny datastruktur\nmeta.dataset.use_restriction_date    # Ny datastruktur\n\nmodel.UseRestriction.PROCESS_LIMITATIONS  # Nytt navn\n\nmeta.dataset.contains_personal_data  # Kun tilgjengelig på variabel-nivå\nmeta.dataset.unit_type               # Kun tilgjengelig på variabel-nivå\nmeta.dataset.temporality_type        # Kun tilgjengelig på variabel-nivå\nmeta.dataset.data_source             # Kun tilgjengelig på variabel-nivå"
  },
  {
    "objectID": "news/posts/2025-09-08-datadoc-model-breaking-changes/index.html#hvordan-skal-koden-se-ut-nå",
    "href": "news/posts/2025-09-08-datadoc-model-breaking-changes/index.html#hvordan-skal-koden-se-ut-nå",
    "title": "Breaking Changes i Datadoc-modellen",
    "section": "Hvordan skal koden se ut nå?",
    "text": "Hvordan skal koden se ut nå?\n\nBruksrestriksjoner\nmeta.dataset.use_restrictions = [\n    model.UseRestrictionItem(\n        model.UseRestrictionType.PROCESS_LIMITATIONS, date(2025, 12, 31)\n    ),\n    model.UseRestrictionItem(\n        model.UseRestrictionType.DELETION_ANONYMIZATION, None\n    ),\n]\n\n\nFelt flyttet til variabel-nivå\nmy_variable = meta.variables_lookup[\n    \"my_short_name\"\n]\nmy_variable.contains_personal_data = True\nmy_variable.unit_type = \"20\"\nmy_variable.temporality_type = model.TemporalityTypeType.ACCUMULATED\nmy_variable.data_source = \"23\""
  },
  {
    "objectID": "news/posts/2025-09-08-datadoc-model-breaking-changes/index.html#hva-med-datadoc-editor",
    "href": "news/posts/2025-09-08-datadoc-model-breaking-changes/index.html#hva-med-datadoc-editor",
    "title": "Breaking Changes i Datadoc-modellen",
    "section": "Hva med Datadoc-editor",
    "text": "Hva med Datadoc-editor\nDatadoc-editor oppdateres til å reflektere disse endringene. Det kommer informasjon om dette i et eget innlegg etterhvert."
  },
  {
    "objectID": "news/posts/2025-04-24-update-shared-buckets/index.html",
    "href": "news/posts/2025-04-24-update-shared-buckets/index.html",
    "title": "Endring i opprettelse og administrasjon av delt-bøtter",
    "section": "",
    "text": "Vi ruller nå ut en endring i hvordan brukere kan opprette og gi tilganger til delt-bøtter. Den nye fremgangsmåten er dokumentert i Dapla-manualen. Endringen vil skje gradvis de neste dagene, og vil fullføres ila et par dager.\nÅrsaken til endringen er at Dapla nå skal støtte en ny type delt-bøtte kalt Delomat. Denne bøttetypen er spesielt konfigurert for en ny tjeneste for datadeling som utvikles med navnet Delomaten.\nEn direkte konsekvens av endringen er at brukere ikke lenger trenger å forholde seg iam.yaml for å gi tilganger slik som tidligere. Både opprettelse av bøtter og tilgangstyring skjer nå i samme fil shared-buckets.yaml. Figur 1 viser hvordan den nye strukturen ser ut.\nUtrullingen av den nye funksjonaliteten medfører ikke at brukere trenger å gjøre noe, siden alle tidligere shared-buckets.yaml og iam.yaml blir migrert inn til det nye formatet. Hvis noen skulle oppleve endringer i tilganger til delt-bøtter, så kan man kontakte Kundeservice for å få hjelp.\n\n\n\n\n\n\nFigur 1: Bilde av det nye oppsettet for delt-bøtter."
  },
  {
    "objectID": "news/posts/2024-12-23-standarder-chapter/index.html",
    "href": "news/posts/2024-12-23-standarder-chapter/index.html",
    "title": "Nytt kapittel om Standarder og standardutvalget",
    "section": "",
    "text": "Vi har skrevet et kapittel i Dapla-manualen som gir et overblikk over standardutvalget og standardene på Dapla. Sjekk det ut her."
  },
  {
    "objectID": "news/posts/2025-04-14-pausing-dapla-lab/index.html",
    "href": "news/posts/2025-04-14-pausing-dapla-lab/index.html",
    "title": "Nye tidspunkter for pausing på Dapla Lab",
    "section": "",
    "text": "Alle tjenester på Dapla Lab blir automatisk pauset kl 17.00 hver dag, med mindre man har meldt seg ut av pausingen. Basert på erfaring ser vi at det startes en del tjenester etter kl 17, og siden disse ikke pauses automatisk, så kan de stå og kjøre til kl 17 dagen etter hvis brukeren glemmer å pause disse. Dette kan medføre unødvendige ressursbruk og kostnader for SSB.\nDerfor innfører vi en ny automatisk rutine for pausing av tjenester på Dapla Lab. I tillegg til pausingen som skjer kl 17 hver, så vil vi fom. 14. april 2025 pause tjenester også hver hele time i tidsintervallet 18.00-05.00. Den nye rutinen vil fortsatt respektere de som meldt seg ut av pausingen. De som har meldt seg ut av pausingen må selv ta ansvar for å manuelt pause tjenesten når den ikke er i bruk.\n\n\n\n\n\n\nFigur 1: Bilde av tjeneste som er pauset."
  },
  {
    "objectID": "news/posts/2025-03-20-stabilitet-dapla-lab/index.html",
    "href": "news/posts/2025-03-20-stabilitet-dapla-lab/index.html",
    "title": "Endringer for økt stabilitet på Dapla Lab",
    "section": "",
    "text": "Etter overgangen til Dapla Lab februar 2025 har vi gjort erfaringer som tilsa at vi burde endre hvordan tjenestene er satt opp. Dette burde resultere i at brukere opplever færre avbrudd når de jobber i en tjeneste som Jupyter, og at det blir færre problemer knyttet til konkurerende minnebruk. Ofte er disse vurderingene en avveining mellom opplevd stabilitet og fleksibilitet vs kostnader for SSB. Det optimalet oppsettet er derfor noe vi må kontinuerlig justere etter hvert som vi får erfaring."
  },
  {
    "objectID": "news/posts/2025-03-20-stabilitet-dapla-lab/index.html#færre-avbrudd",
    "href": "news/posts/2025-03-20-stabilitet-dapla-lab/index.html#færre-avbrudd",
    "title": "Endringer for økt stabilitet på Dapla Lab",
    "section": "Færre avbrudd",
    "text": "Færre avbrudd\nBrukere som har jobbet i Jupyter, VS Code eller RStudio har rapportert at de plutselig kan oppleve avbrudd og at tjenesten ikke kan brukes på noen minutter. En refresh av nettsiden etter noen minutter har fikset problemet, men det oppleves ustabilt for brukeren. Vi har nå gjort endringer i den underliggende plattformen slik at dette ikke skal skje. Årsaken til avbruddene har vært at brukere har blitt omplassert på nye maskiner for å spare ressurser. Vi har nå økt toleransen for at brukere skal bli omplassert på ny maskin og dermed bør problemet forsvinne. For de som er interessert i mer detaljer så kan de lese neste avsnitt.\nNår en tjeneste startes i Dapla Lab bestemmer Kubernetes1 hvilken underliggende maskin (også kalt node) tjenesten skal starte på. Hvis det er for lite kapasitet vil Kubernetes starte en ny maskin. Denne skaleringen er grunnen til at oppstart av en tjeneste noen ganger kan ta ekstra tid. Skulle det vise seg at vi har flere maskiner enn nødvendig vil de overflødige maskinene bli skrudd av. Ved å kun kjøre det minste nødvendige antallet med maskiner får vi en mer kostnadseffektiv drift. En konsekvens av dette har vært at tjenester plutselig kan oppleve et midlertidig avbrudd mens tjenesten blir flyttet fra en maskin til en annen. Etter tilbakemelding fra brukerne har vi endret denne praksisen, og lar nå tjenestene kjøre videre på samme maskin, selv om det er ledig kapasitet på andre maskiner."
  },
  {
    "objectID": "news/posts/2025-03-20-stabilitet-dapla-lab/index.html#mindre-konkurrerende-minnebruk",
    "href": "news/posts/2025-03-20-stabilitet-dapla-lab/index.html#mindre-konkurrerende-minnebruk",
    "title": "Endringer for økt stabilitet på Dapla Lab",
    "section": "Mindre konkurrerende minnebruk",
    "text": "Mindre konkurrerende minnebruk\n\nEnkelte brukere har opplevd at tjenesten i Dapla Lab kræsjer pga. minnebruk selv om minnebruken er mindre enn det som ble forespurt av brukeren. Dette skyldes at det er forskjell på forespurte og tilgjengelige ressurser i Kubernetes. Vi har nå endret oppsettet i Dapla Lab slik at brukerne alltid får det som forespørres, og derfor vil mangel på ressurser ikke forekomme lenger. Resultatet for brukeren er en mer stabil tjeneste og bedre brukeropplevelse. De som er interessert i flere detaljer rundt dette, kan lese avsnittet under.\nNår man som bruker i Dapla Lab konfigurerer en tjeneste med CPU og Minne/RAM, så blir det satt føringer som må overholdes av Kubernetes. Den ene føringen er en forespørsel - mengden CPU/minne man er garantert å få - og den andre er grensen/taket. Hvis man går over taket vil Kubernetes “drepe” tjenesten og starte den på nytt.\nI “tradisjonelle” applikasjoner (tenk f.eks. Klass sitt API) gir forespørsler og grenser mening, grunnet naturlig variasjon i belastning og trafikk, og at applikasjonene er designet for å kunne feile, starte på nytt ofte og kjøre flere instanser i parallel. Hvis den om og om igjen går over grensen sin tyder dette på en feil i applikasjonen (f.eks. minnelekkasje).\nKubernetes benytter seg av det forespurte minnet når den velger hvilken maskin tjenesten eller applikasjonen skal kjøre på. Det vil si at det er mulig å totalt ha en høyere minnegrense enn hva som er tilgjengelig på maskinen. Et eksempel på dette er hvis tjeneste A har minneforespørsel/-grense på 100GB/150GB, og tjeneste B har tilsvarende, men maskinen bare har 256GB med minne tilgjengelig. Da er vi i et scenario der maskinen ikke har nok minne hvis både tjeneste A og tjeneste B bruker opp til minnegrensen sin. I dette tilfellet vil en tjeneste på maskinen bli drept og startet på nytt. Merk at dette kan være en tilfeldig tjeneste på maskinen, ikke nødvendigvis den som bruker mest minne.\nVi har sett at det for tjenester på Dapla Lab ikke gir mening å differensiere på forespørsel og grense, som i tradisjonelle applikasjoner. Derfor er det nå innført en endring der forespørselen er lik grensen.Dette håper vi gjør det mer forutsigbart for brukeren hva de kan forvente av tilgjengelig minne, samtidig som stabiliteten for alle brukerne ved at “minnetunge” brukere ikke påvirker andre sine tjenester."
  },
  {
    "objectID": "news/posts/2025-03-20-stabilitet-dapla-lab/index.html#nye-maskiner",
    "href": "news/posts/2025-03-20-stabilitet-dapla-lab/index.html#nye-maskiner",
    "title": "Endringer for økt stabilitet på Dapla Lab",
    "section": "Nye maskiner",
    "text": "Nye maskiner\nDet å velge korrekte maskiner er ikke nødvendigvis en enkel oppgave, da generell bruk av maskinen må veies opp mot kostnaden ved den. Kostnadsdriveren for maskinene Dapla Lab og tjenestene kjører på er minne. Den nysgjerrige kan ta en titt på pris-oversikten her:\nhttps://cloud.google.com/compute/all-pricing?hl=nb.\nPå Dapla Lab benyttes maskintype n2 (standard og highmem). Ved å monitorere og observere bruken av ressurser gjør vi nå også endringer i hvilke underliggende maskiner som kan kjøres opp. Fremover vil vi kjøre opp færre store maskiner enn flere små (fordi det finnes en basiskostnad for hver ekstra maskin), samtidig som vi nå innfører en maskintype som ikke har like mye minne som andre (n2-standard-80). Det vil være kostnadseffektivt for dem som trenger mye CPU men ikke nødvendigvis masse minne.\nDette vil være noe brukerne ikke merker eller vil ha et forhold til."
  },
  {
    "objectID": "news/posts/2025-03-20-stabilitet-dapla-lab/index.html#footnotes",
    "href": "news/posts/2025-03-20-stabilitet-dapla-lab/index.html#footnotes",
    "title": "Endringer for økt stabilitet på Dapla Lab",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nKubernetes er systemet som brukes under panseret for å skalere opp og ned ressurser i Dapla Lab. Les mer om Kubernetes her.↩︎"
  },
  {
    "objectID": "news/posts/2025-10-21-whodat-lansering/index.html",
    "href": "news/posts/2025-10-21-whodat-lansering/index.html",
    "title": "Lansering av FNR-letingstjenesten",
    "section": "",
    "text": "dapla-toolbelt-whodat er en Python-pakke som gir statistikere mulighet for å søke etter fødselsnummer basert på hjelpeopplysninger - for eksempel navn, adresse eller kommunenummer."
  },
  {
    "objectID": "news/posts/2025-10-21-whodat-lansering/index.html#hvorfor-er-dette-nyttig",
    "href": "news/posts/2025-10-21-whodat-lansering/index.html#hvorfor-er-dette-nyttig",
    "title": "Lansering av FNR-letingstjenesten",
    "section": "Hvorfor er dette nyttig?",
    "text": "Hvorfor er dette nyttig?\nFunksjonaliteten kan hjelpe statistikere å beholde rader som ellers hadde blitt forkastet, fordi man har manglende eller feil identifikator i datasettet. dapla-toolbelt-whodat er, bak kulissene, et tynt lag rundt Skatteetatens folkeregister-søk."
  },
  {
    "objectID": "news/posts/2025-10-21-whodat-lansering/index.html#hvordan-bruker-jeg-dette",
    "href": "news/posts/2025-10-21-whodat-lansering/index.html#hvordan-bruker-jeg-dette",
    "title": "Lansering av FNR-letingstjenesten",
    "section": "Hvordan bruker jeg dette?",
    "text": "Hvordan bruker jeg dette?\nPakken er tilgjengelig i Kildomaten, og skal brukes med kildedata. Man kan lese dokumentasjonen her, og ellers ta kontakt med Team Statistikktjenester gjennom Kundeservice for øvrige spørsmål eller tilbakemeldinger."
  },
  {
    "objectID": "news/posts/2025-04-22-dapla-metrics-shared-buckets/index.html",
    "href": "news/posts/2025-04-22-dapla-metrics-shared-buckets/index.html",
    "title": "Statistikk over datadeling på Dapla",
    "section": "",
    "text": "Ett år etter at Dapla-team kunne opprette delt-bøtter selvbetjent på Dapla, så er det tilsammen opprettet 118 bøtter per 22. April 2025. 83 av disse er i prod-miljøet til teamene, mens 35 er i test-miljøet.\nAv de 171 teamene totalt på Dapla, så er det 33 av disse som har opprettet en eller flere delt-bøtter.\nTabell 1 viser en oversikt over alle delt-bøtter som er opprettet1, dens kortnavn, hvilket team som er eier den, hvilket miljø den finnes i, og hvilken seksjon som er ansvarlig for teamet.\nTabell 1 viser mer informasjon enn hva som er tilgjengelig i dag i Dapla Ctrl. Grunnen er at det i Tabell 1 også inkluderes delt-bøtter i test-miljøet, samt at Dapla Ctrl kun viser delt-bøtter hvor andre team faktisk har blitt gitt tilgang. I fremtiden vil denne informasjonen kunne inspiseres i en datakatalog, med mye mer detaljert informasjon om dataene som deles.\nFigur 1 viser at det er avdeling 200 som har opprettet flest bøtter, i alt 52 delt-bøtter. Deretter følger avdeling 300 med 26 bøtter.\nAv Figur 2, som viser hvilke 10 team som har flest delt-bøtter, så ser vi at 48 av de 52 bøttene i avdeling 200 kommer fra team Kostra. Vi ser også at team arbmark-register har opprettet 9 av totalt 26 bøtter i avdeling 300.\nFigur 1: Antall delt-bøtter per avdeling og miljø\nFigur 2: Antall delt-bøtter per team og miljø"
  },
  {
    "objectID": "news/posts/2025-04-22-dapla-metrics-shared-buckets/index.html#footnotes",
    "href": "news/posts/2025-04-22-dapla-metrics-shared-buckets/index.html#footnotes",
    "title": "Statistikk over datadeling på Dapla",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nHelt presist viser tabellen alle delt-bøtter som er opprettet gjennom shared-buckets-featuren.↩︎"
  },
  {
    "objectID": "news/posts/2024-11-08-collector-chapter/index.html",
    "href": "news/posts/2024-11-08-collector-chapter/index.html",
    "title": "Nytt kapittel om Data Collector",
    "section": "",
    "text": "Vi har skrevet et kapittel i Dapla-manualen som beskriver hvordan man bruker Data Collector. Sjekk det ut her."
  },
  {
    "objectID": "news/posts/2025-10-14-datadoc-update/index.html",
    "href": "news/posts/2025-10-14-datadoc-update/index.html",
    "title": "Viktige endringer i Datadoc",
    "section": "",
    "text": "Tidligere annonserte vi at det er gjort større endringer i Datadoc-modellen, dvs. endringer hvilken informasjon vi ønsker om våre datasett og variabler. Endringene ble implementert fom. v0.0.9 av dapla-toolbelt-metadata. Nå er det også gjort tilsvarende tilpasninger og endringer i Datadoc-editor."
  },
  {
    "objectID": "news/posts/2025-10-14-datadoc-update/index.html#hva-er-endret",
    "href": "news/posts/2025-10-14-datadoc-update/index.html#hva-er-endret",
    "title": "Viktige endringer i Datadoc",
    "section": "Hva er endret?",
    "text": "Hva er endret?\nFølgende endringer er gjort i Datadoc-modellen og Datadoc-editor:\n\nfeltene Datakilde og Temporalitetstype kunne tidligere dokumenteres både på datasett- og variabelnivå. Etter endringen kan de bare dokumenteres på variabelnivå.\nfeltene Enhetstype og Inneholder personopplysninger kunne tidligere dokumenteres på datasettnivå. Etter endringen kan de bare dokumenteres på variabelnivå. Feltet Inneholder personopplysninger har også endret navn til Er personopplysning.\nimplementert ny funksjonalitet i Datadoc-editor for å lettere kunne gi samme verdi for alle variabler. Dette er et tiltak for å redusere arbeidsmengden ved dokumentasjon av data. Denne funksjonaliteten innføres for feltene Enhetstype, Datakilde, Temporalitetstype, Variabelens rolle, Måleenhet og Multiplikasjonsfaktor.\nimplementert en forenklet måte å fylle inn informasjon om pseudonymisering i Datadoc-editor.\nimplementert mulighet for å velge flere bruksrestriksjoner for et datasett."
  },
  {
    "objectID": "news/posts/2025-10-14-datadoc-update/index.html#hvorfor-er-endringene-gjort",
    "href": "news/posts/2025-10-14-datadoc-update/index.html#hvorfor-er-endringene-gjort",
    "title": "Viktige endringer i Datadoc",
    "section": "Hvorfor er endringene gjort?",
    "text": "Hvorfor er endringene gjort?\nErfaringer fra bruk av Datadoc-editor, og behov knyttet til pseudonymisering, gjør at det nå er gjort noen endringer i modellen for Datadoc, og dermed også i Datadoc-editor. Dette er gjort for å forenkle innlegging av data, gi mer detaljert informasjon på variabelnivå og for å dokumentere hvilken pseudonymiseringsalgoritme som er brukt."
  },
  {
    "objectID": "news/posts/2025-10-14-datadoc-update/index.html#hva-betyr-dette-for-meg",
    "href": "news/posts/2025-10-14-datadoc-update/index.html#hva-betyr-dette-for-meg",
    "title": "Viktige endringer i Datadoc",
    "section": "Hva betyr dette for meg?",
    "text": "Hva betyr dette for meg?\nEndringene betyr ingenting for brukere som skal dokumentere et datasett for første gang etter oppdateringen.\nBrukere som allerede har dokumentert datasett blir påvirket av endringene og må oppdatere eksisterende Datadoc-dokumentasjon til siste versjon av modellen. Et allerede dokumentert datasett vil automatisk oppdateres neste gang du åpner det i Datadoc-editor eller bruker dapla-toolbelt-metadata. Den automatiske oppdateringen innebærer at følgende endringer gjøres i dokumentasjonen din:\n\nEnhetstype\nVed oppdateringen til den nye modellen, vil enhetstypen som er satt på datasettnivå (her var det bare mulig å velge én enhetstype), automatisk settes som enhetstype på hver variabel. Hvis datasettet inneholder ulike enhetstyper så må dette endres for hver varabel\nInneholder personopplysninger\nVed oppdatering til den nye modellen vil de som tidligere hadde fylt inn at datasettet inneholder personopplysninger få denne verdien på alle variabler i den nye modellen. Hvis noen av variablene ikke inneholder personopplysninger må brukeren endre dette for hver variabel.\nPseudonymisering\nAlle som har dokumenterer data som inkluderer pseudonymiserte variabler bør oppdatere til ny modell og dokumentere hvordan pseudonymiseringen er gjennomført. Dette er enklest å gjøre fra Datadoc-editor siden den kun lar deg velge mellom noen få alternativer, mens en programmatisk utfylling med dapla-toolbelt-metadata innebærer at man må kjenne til mange flere detaljer.\n\n\nProgrammatisk oppdatering\nI noen tilfeller kan det være enklere å oppdatere til den nye modellen programmatisk og ikke med Datadoc-editor. Under er et eksempel på hvordan man programmatisk oppdaterer en dokumentert enkeltfil:\n\n\nNotebook\n\nfrom pathlib import Path\nfrom dapla_metadata.datasets.core import Datadoc\n\n# Angi stien til metadatadokumentet\nmeta_path = \"/path/to/metadata.json\"\n\nmeta = Datadoc(metadata_document_path=meta_path)\nmeta.write_metadata_document()\nprint(f\"Upgraded {Path(meta_path).stem} to v{meta.container.datadoc.document_version}\")\n\n\n\nNy funksjonalitet i Datadoc-editor\nSom nevnt tidligere, så er det nå mulig sette like verdier på visse felt for alle variabler. Dette er implementert under Variabler i Datadoc-editor med en egen fane for Alle variabler, slik som vist på Figur 1.\n\n\n\n\n\n\nFigur 1: Ny funksjonalitet for sette verdier som skal gjelde for alle variabler\n\n\n\nEndringen er gjort for å gjøre det enklere å dokumentere data. Man kan da sette en verdi for alle variabler, og deretter endre på enkeltvariabler som skal ha en egen verdi. Dette kan være spesielt nyttig for de som har datasett med mange variabler."
  },
  {
    "objectID": "news/posts/2025-04-30-challenges-gcsfuse/index.html",
    "href": "news/posts/2025-04-30-challenges-gcsfuse/index.html",
    "title": "Deling av data og /buckets",
    "section": "",
    "text": "Etter noen måneder med erfaring med tilgjengeliggjøring av bøtter som filsystem i Dapla Lab, har vi erfart at funksjonaliteten i noen tilfeller har noen utfordringer.\nEn utfordring oppstår når team skriver data til delt-bøtter direkte via Transfer Service, uten å logge seg inn på Dapla Lab etterpå. Hvis det delende teamet aldri åpner en tjeneste på Dapla Lab etter at dataene er delt, så vil ikke de dataene vises for konsumentene av dataene under /buckets/ i Dapla Lab.\nÅrsaken er at opprettelsen av mapper krever skrivetilgang til bøtta, og det har ikke konsumenten av delte-data. Konsumentene vil kunne se dataene via gs://-stien, men ikke /buckets/-stien. Hvis data derimot deles ved at de produseres direkte fra en tjeneste på Dapla Lab, så vil ikke dette være en problemstilling.\nVi jobber med å løse dette problemet, og finner forhåpentligvis en løsning innen kort tid."
  },
  {
    "objectID": "news/posts/2025-04-28-function-use/index.html",
    "href": "news/posts/2025-04-28-function-use/index.html",
    "title": "Blogginnlegg om bruk av funksjoner",
    "section": "",
    "text": "Arne Sørli har skrevet et blogginnlegg som oppfordrer til mer bruk av funksjoner i statistikkproduksjonskode. Innlegget sier litt om hvorfor, hvordan man kan gjøre det og kommer med noen eksempler. Det har også et skript du kan bruke til å sjekke hvor mye bruk av funksjoner det er i ditt repo.\nLes innlegget her."
  },
  {
    "objectID": "news/posts/2025-04-07-altinn-to-isee/index.html",
    "href": "news/posts/2025-04-07-altinn-to-isee/index.html",
    "title": "Mer dokumentasjon om Altinn 3 i Dapla-manualen",
    "section": "",
    "text": "Team SU-V har skrevet flere nye kapitler med brukerdokumentasjon for brukere som jobber med Altinn 3 på Dapla. Kapitlene er samlet i en egen del av manualen, i tillegg så Python-pakken dapla-suv-tools dokumentert under Datatjenester.\nMagnus Theodor Engh har også skrevet et blogginnlegg som gir en detaljert beskrivelse av hvordan man kan integrere et datafangst med Altinn 3 og ISEE."
  },
  {
    "objectID": "news/posts/2025-01-22-filinnsamling-moveit/index.html",
    "href": "news/posts/2025-01-22-filinnsamling-moveit/index.html",
    "title": "Nytt kapittel om filinnsamling via MoveIT på Dapla",
    "section": "",
    "text": "Mange statistikker mottar data fra eksterne som filer via MoveIT-systemet på bakken. Disse filene kan automatisk synkroniseres til kildebøtta på Dapla. Les mer om hvordan dette settes opp i dette kapitlet i Dapla-manualen."
  },
  {
    "objectID": "news/posts/2025-05-06-kildomat-migration/index.html",
    "href": "news/posts/2025-05-06-kildomat-migration/index.html",
    "title": "Driftsbrudd for Kildomaten mandag 12. mai 2025",
    "section": "",
    "text": "I forbindelse med at pseudonymiseringstjenesten på Dapla skal migreres over til NAIS mandag den 12. mai 2025, så vil det bli en driftsstans i Kildomaten mellom 15.00-21.00 den dagen. Les mer i denne saken på Viva Engage."
  },
  {
    "objectID": "news/posts/2025-04-26-kildomaten-docs/index.html",
    "href": "news/posts/2025-04-26-kildomaten-docs/index.html",
    "title": "Oppdatert beskrivelse av manuell trigging av Kildomaten-jobber",
    "section": "",
    "text": "Dapla-manualen er nå oppdatert med en bedre beskrivelse av hvordan man kan trigge Kildomaten manuelt. Brukere må logge seg inn som developers for å manuelt trigge en Kildomaten-jobb. Les mer i dokumentasjonen."
  },
  {
    "objectID": "news/posts/2025-05-01-dapla-lab-groups/index.html",
    "href": "news/posts/2025-05-01-dapla-lab-groups/index.html",
    "title": "Ny informasjon under Mine tjenester i Dapla Lab",
    "section": "",
    "text": "I dag er det lagt til visning av hvilken tilgangsgruppe som er valgt for åpnede tjenester i Dapla Lab. Det vil gjøre det lettere å vite hvilke data man tilgang til i ulike tjenester. Du finner under informasjonen under Group i tjenesteikonet under Mine tjenester. Informasjonen vises både når tjenesten er aktiv og når den er pauset."
  },
  {
    "objectID": "news/posts/2025-02-05-daplanytt-jan25/index.html",
    "href": "news/posts/2025-02-05-daplanytt-jan25/index.html",
    "title": "DaplaNytt-møte 5.2.2025",
    "section": "",
    "text": "DaplaNytt for januar 2025 ble avholdt som Teams-møte 5. februar 2025.\nSe opptaket her (intern lenke).\n\n\n\nSkjembilde fra Teams-møte"
  },
  {
    "objectID": "news/posts/2025-06-07-ssb-altinn-python/index.html",
    "href": "news/posts/2025-06-07-ssb-altinn-python/index.html",
    "title": "Ny artikkel - ssb-altinn-python",
    "section": "",
    "text": "Vi har skrevet ny artikkel om python-pakken ssb-altinn-python. Trykk her for å lese den!"
  },
  {
    "objectID": "news/posts/2025-06-10-datadoc-update/index.html",
    "href": "news/posts/2025-06-10-datadoc-update/index.html",
    "title": "Ny versjon av dapla-toolbelt-metadata",
    "section": "",
    "text": "Vi har sluppet en ny versjon av dapla-toolbelt-metadata som tar i bruk en ny versjon av Datadoc modellen. Dette medfører at de som har brukt dapla-toolbelt-metadata programmatisk kan komme til å måtte gjøre kode endringer. Altså man trenger kun å gjøre endringer om man bruker dapla-toolbelt-metadata programmatisk og man bruker pseudonymisering.\nHovedforskjellen fra tidligere er at vi har flyttet pseudonymiserings informasjonen fra ved siden av datasettet, til å ligge under hver enkelt variabel. Hver variabel har nå et felt pseudonymisering som inneholder informasjon om hvordan den variabelen er pseudonymisert."
  },
  {
    "objectID": "news/posts/2024-11-11-dapla-lab-source-data/index.html",
    "href": "news/posts/2024-11-11-dapla-lab-source-data/index.html",
    "title": "Kildedata tilgjengelig fra Dapla Lab",
    "section": "",
    "text": "Brukere i tilgangsgruppen data-admins kan nå aksessere kildedata fra tjenestene på Dapla Lab. Kravene for tilgangen er de samme som før:\nSom tidligere må de da velge hvilket team og tilgangsgruppe de ønsker å starte tjenesten som, og deretter oppgi begrunnelse og lengde på tilgang. Dette gjøres fra tjenestekonfigurasjonen i Dapla Lab, slik som vist i Figur 1."
  },
  {
    "objectID": "news/posts/2024-11-11-dapla-lab-source-data/index.html#begrunnelse-gis-i-dapla-lab",
    "href": "news/posts/2024-11-11-dapla-lab-source-data/index.html#begrunnelse-gis-i-dapla-lab",
    "title": "Kildedata tilgjengelig fra Dapla Lab",
    "section": "Begrunnelse gis i Dapla Lab",
    "text": "Begrunnelse gis i Dapla Lab\nNår en bruker tidligere har aksessesert kildedata fra “gamle” Jupyter (https://jupyter.dapla.ssb.no/), så ble en JIT-applikasjon for å få midlertidig tilgang. Denne tilnærmingen vil fortsette å fungere for det “gamle” mijøet inntil det avvikles til fordel for Dapla Lab. Men på Dapla Lab kan begrunnelse og tidspunkt angis direkte i tjenestekonfigurasjonen."
  },
  {
    "objectID": "news/posts/2024-11-11-dapla-lab-source-data/index.html#feil-ved-manglende-begrunnelse",
    "href": "news/posts/2024-11-11-dapla-lab-source-data/index.html#feil-ved-manglende-begrunnelse",
    "title": "Kildedata tilgjengelig fra Dapla Lab",
    "section": "Feil ved manglende begrunnelse",
    "text": "Feil ved manglende begrunnelse\nHvis man prøver å starte en tjeneste som data-admins uten å oppgi en begrunnelse, så vil man få feilmeldingen vist i Figur 2. Denne feilmeldingen er lite forklarende og er noe vi ønsker å forbedre etter hvert.\n\n\n\n\n\n\nFigur 2: Feilmelding ved manglende begrunnelse i Dapla Lab."
  },
  {
    "objectID": "news/posts/2024-11-29-kvakk-chapter/index.html",
    "href": "news/posts/2024-11-29-kvakk-chapter/index.html",
    "title": "Nytt kapittel om Kvalitet i kode og koding (KVAKK)",
    "section": "",
    "text": "Vi har skrevet et kapittel i Dapla-manualen om Kvalitet i kode og koding (KVAKK). Sjekk det ut her.\nKapittelet er foreløpig kort og henviser til confluence. Vi har planer om å skrive mer utfyllende om KVAKK i manualen."
  },
  {
    "objectID": "news/posts/2025-01-15-overgang-dapla-lab/index.html",
    "href": "news/posts/2025-01-15-overgang-dapla-lab/index.html",
    "title": "Overgang til Dapla Lab",
    "section": "",
    "text": "Innen 15. februar må alle brukere av dagens Jupyter-miljø flytte over til Dapla Lab, SSBs nye arbeidsbenk for staistikkproduksjon og forskning. Innen fristen må kode, filer og kjøringer flyttes fra det gamle miljøet over til Dapla Lab.\nMigreringsguide\nhttps://statistics-norway.atlassian.net/wiki/spaces/DAPLA/pages/4398678345/Migreringsguide+for+Dapla+Lab\nDokumentasjon for Dapla Lab\nhttps://manual.dapla.ssb.no/statistikkere/dapla-lab.html\nDokumentasjon for tjenestene på Dapla Lab\nhttps://manual.dapla.ssb.no/statistikkere/jupyter.html"
  },
  {
    "objectID": "news/posts/2025-03-03-dapla-lab-nosuspend/index.html",
    "href": "news/posts/2025-03-03-dapla-lab-nosuspend/index.html",
    "title": "Skru av automatisk pausing for en tjeneste",
    "section": "",
    "text": "Alle tjenester på Dapla Lab pauses hver kveld kl. 22:00, slik som forklart i dokumentasjonen. Nå er det mulig å skru av pausing for enkelttjenester ved å skrive [nosuspend] på slutten av visningsnavnet til tjenesten, slik som vist i videoen under:\n\nDenne funksjonaliteten kan skrus av og på etter at tjenesten er startet."
  },
  {
    "objectID": "news/posts/2025-03-18-dapla-lab-conf-locked-input-fields/index.html",
    "href": "news/posts/2025-03-18-dapla-lab-conf-locked-input-fields/index.html",
    "title": "Forbedring av konfigurasjon i Dapla Lab",
    "section": "",
    "text": "Den 18. mars 2025 rullet vi ut en endring i tjenestekonfigurasjonene til alle tjenester på Dapla Lab. Under Data-fanen er ikke lenger mulig å starte en tjeneste som data-admins uten å oppgi en begrunnelse. I tillegg vises feltene for Begrunnelse og Varighet nå kun hvis man har valgt data-admins under Team og tilgangsgruppe.\nFigur 1 viser hvordan tjenestekonfigurasjonen ser ut etter endringen. Figur 1 (a) viser hvordan Data-fanen ser ut hvis man velger å representere developers, mens Figur 1 (b) viser valgene hvis man velger data-admins.\n\n\n\n\n\n\n\n\n\n\n\n(a) developers\n\n\n\n\n\n\n\n\n\n\n\n(b) data-admins\n\n\n\n\n\n\n\nFigur 1: Ulik visning av informasjon basert på om man velger developers eller data-admins"
  },
  {
    "objectID": "news/posts/2025-09-11-r-in-vscode/index.html",
    "href": "news/posts/2025-09-11-r-in-vscode/index.html",
    "title": "R-støtte i VS Code",
    "section": "",
    "text": "Tjenesten vscode-python i Dapla Lab har nå støtte for å kjøre R-kode. Bakgrunnen for at vi tilbyr R-støtte er at mange brukere ønsker å kunne kjøre metodebiblioteker som kun er tilgjengelig i R.\nSelv om R nå er tilgjengelig i VS Code, er det fortsatt en tjeneste som hovedsakelig er spisset mot Python-kode. Man kan derfor ikke forvente at tjenesten har like god støtte for R-arbeidsflyter som man finner i RStudio.\nKom gjerne med tilbakemeldinger hvis det er noe som ikke fungerer optimalt med denne utvidelsen."
  },
  {
    "objectID": "news/posts/2025-09-11-r-in-vscode/index.html#r-støtte-i-vs-code",
    "href": "news/posts/2025-09-11-r-in-vscode/index.html#r-støtte-i-vs-code",
    "title": "R-støtte i VS Code",
    "section": "",
    "text": "Tjenesten vscode-python i Dapla Lab har nå støtte for å kjøre R-kode. Bakgrunnen for at vi tilbyr R-støtte er at mange brukere ønsker å kunne kjøre metodebiblioteker som kun er tilgjengelig i R.\nSelv om R nå er tilgjengelig i VS Code, er det fortsatt en tjeneste som hovedsakelig er spisset mot Python-kode. Man kan derfor ikke forvente at tjenesten har like god støtte for R-arbeidsflyter som man finner i RStudio.\nKom gjerne med tilbakemeldinger hvis det er noe som ikke fungerer optimalt med denne utvidelsen."
  },
  {
    "objectID": "news/posts/2025-06-23-startup-script/index.html",
    "href": "news/posts/2025-06-23-startup-script/index.html",
    "title": "Personliggjør dine tjenester på Dapla Lab",
    "section": "",
    "text": "Tjenestene på Dapla Lab støtter nå at brukere kan kjøre sitt eget oppstartsskript når en tjeneste startes. Det gir brukere mulighet til å definere sine egne farger, snarveier, osv. Skriptet må være et bash-script og det må ligger lagret i et repo på www.github.com/statisticsnorway. Brukeren kan deretter referere til skriptet under Avansert i tjenestekonfigurasjonen på Dapla Lab, slik som vist Figur 1.\n\n\n\n\n\n\nFigur 1: Avansert-fanen til en tjeneste på Dapla Lab\n\n\n\nEksempelt i Figur 1 viser en bruker som ber om å få kjørt et skript (personal-init.sh) som ligger lagret i https://github.com/statisticsnorway/obr-test.\nOppstartsskript er et kraftfullt verktøy som lar brukere tilpasse tjenesten sin på mange forskjellige måter. Generelt sett kan man gjør alt man kan gjøre i en terminal inne i tjenesten, bare at det ved oppstart.\nUnder er et eksempel på et oppstartsskript hvor det gjøres noen endringer i $HOME/.bashrc og theme i Jupyter defineres.\n\n\n\n\n\n\nNoteEksempel på et oppstartskript\n\n\n\n\n\ndemo-script.sh\n\n#!/bin/bash\n\n# Update .bashrc with environment variables and aliases\necho \"\" &gt;&gt; \"$HOME/.bashrc\"\necho \"# Opprettet av mitt personlige startupscript:\" &gt;&gt; \"$HOME/.bashrc\"\necho \"export TEST=true\" &gt;&gt; \"$HOME/.bashrc\"\necho \"alias gs='git status'\" &gt;&gt; \"$HOME/.bashrc\"\necho \"alias ll='ls -alF'\" &gt;&gt; \"$HOME/.bashrc\"\n\n# Set Jupyter-theme via settings file\nTHEME_NAME=\"JupyterLab Dark\"\nSETTINGS_DIR=\"$HOME/work/.jupyter/config/lab/user-settings/@jupyterlab/apputils-extension\"\nSETTINGS_FILE=\"$SETTINGS_DIR/themes.jupyterlab-settings\"\n\nmkdir -p \"$SETTINGS_DIR\"\n\ncat &gt; \"$SETTINGS_FILE\" &lt;&lt;EOF\n{\n    // Theme set from init-script\n    \"theme\": \"$THEME_NAME\"\n}\nEOF\n\necho \"[Init Script] JupyterLab theme set to '$THEME_NAME'\"\n\n\n\nHvis oppstartsskriptet feiler, eller man er interessert i hva som ble kjørt, så kan man undersøke logg-filen til oppstartskriptet inne i tjenesten: $HOME/personal_init_script.log."
  },
  {
    "objectID": "news/posts/2025-03-05-daplanytt-feb25/index.html",
    "href": "news/posts/2025-03-05-daplanytt-feb25/index.html",
    "title": "DaplaNytt-møte 5.3.2025",
    "section": "",
    "text": "DaplaNytt for februar 2025 ble avholdt som Teams-møte 5. mars 2025.\nSe opptaket her (intern lenke).\nPresentasjonen finner du her.\n\n\n\nSkjembilde fra Teams-møtet DaplaNytt."
  },
  {
    "objectID": "news/posts/2024-11-08-env-vars/index.html",
    "href": "news/posts/2024-11-08-env-vars/index.html",
    "title": "Faste miljøvariabler i Dapla Lab",
    "section": "",
    "text": "Vi har oppdatert listen over miljøvariabler som brukere av programmeringsmiljøene (Jupyter, VS Code og RStudio) i Dapla Lab kan forvente å finne. Formålet med disse er hovedsakelig å tilby de som lager Python- eller R-biblioteker en enkel å sjekke hvilket miljø koden kjøres i. Vi har også noen miljøvariabler som gjør at de som lager pakker slipper å hardkode url-er i koden sin."
  },
  {
    "objectID": "news/posts/2025-10-13-dapla-toolbelt-utfasing/index.html",
    "href": "news/posts/2025-10-13-dapla-toolbelt-utfasing/index.html",
    "title": "Utfasing av dapla-toolbelt",
    "section": "",
    "text": "Team Statistikktjenester kommer til å fjerne støtte for Python-pakken dapla-toolbelt 1. februar 2026."
  },
  {
    "objectID": "news/posts/2025-10-13-dapla-toolbelt-utfasing/index.html#hva-endres",
    "href": "news/posts/2025-10-13-dapla-toolbelt-utfasing/index.html#hva-endres",
    "title": "Utfasing av dapla-toolbelt",
    "section": "Hva endres?",
    "text": "Hva endres?\nEtter 1. februar vil vi ikke lenger støtte bruk av dapla-toolbelt på Dapla Lab. Noe av funksjonaliteten i dapla-toolbelt fjernes, mens noe videreføres inn i nye Python-pakker."
  },
  {
    "objectID": "news/posts/2025-10-13-dapla-toolbelt-utfasing/index.html#hva-betyr-dette-for-meg",
    "href": "news/posts/2025-10-13-dapla-toolbelt-utfasing/index.html#hva-betyr-dette-for-meg",
    "title": "Utfasing av dapla-toolbelt",
    "section": "Hva betyr dette for meg?",
    "text": "Hva betyr dette for meg?\nHvis du bruker dapla-toolbelt i koden din i dag så må du erstatte dette før 1. februar 2026. Noe kan erstattes allerede nå, mens annen funksjonalitet vil erstattes gradvis de neste månedene. Tabell 1 gir en oversikt over hvilken funksjonalitet som erstattes med hva:\n\n\n\nTabell 1: Funksjonalitet i dapla-toolbelt og hvor det flyttes.\n\n\n\n\n\ndapla-toolbelt\nEtter endring\n\n\n\n\nread_pandas(), write_pandas()\nBytt til standard-pakker som Pandas, Polars, PyArrow, etc.\n\n\nCollectorClient\nNy Python-pakke kommer snart\n\n\nGuardianClient\nNy Python-pakke kommer snart\n\n\nAuthClient\ndapla-auth-client\n\n\nget_secret_version()\nSkal flyttes til Python-pakken ssb-fagfunksjoner\n\n\nFileClient\ngcsfs\n\n\ntrigger_source_data_processing\nNy Python-pakke kommer snart\n\n\n\n\n\n\nAv Tabell 1 ser vi at funksjonaliteten i read_pandas(), write_pandas(), AuthClient og FileClient kan erstattes med nye alternativene nå. Resten av funksjonaliteten er under utvikling og det vil annonseres når disse er klare for erstatning.\n\ndapla-toolbelt i Kildomaten\nBrukere som benytter dapla-toolbelt fra Kildomaten kan erstatte dette med en gang. I Kildomaten er det read_pandas(), write_pandas() og FileClient som benyttes og disse kan erstattes nå."
  },
  {
    "objectID": "news/posts/2025-10-13-dapla-toolbelt-utfasing/index.html#kan-jeg-få-hjelp",
    "href": "news/posts/2025-10-13-dapla-toolbelt-utfasing/index.html#kan-jeg-få-hjelp",
    "title": "Utfasing av dapla-toolbelt",
    "section": "Kan jeg få hjelp?",
    "text": "Kan jeg få hjelp?\nJa. Ta kontakt med avdelingens støtteteam eller tech-coacher ved behov for støtte."
  },
  {
    "objectID": "news/posts/2025-10-13-dapla-toolbelt-utfasing/index.html#hvorfor-gjøres-endringen",
    "href": "news/posts/2025-10-13-dapla-toolbelt-utfasing/index.html#hvorfor-gjøres-endringen",
    "title": "Utfasing av dapla-toolbelt",
    "section": "Hvorfor gjøres endringen?",
    "text": "Hvorfor gjøres endringen?\nEndringen gjøres fordi mye av funksjonaliteten i pakken ikke lenger er relevant, og vi ønsker å flytte gjenværende funksjonalitet inn i mindre Python-pakker. Fordelen med dette er følgende:\n\nmindre forvaltning\nfærre avhengigheter i ssb-project\nenklere migrering fra prodsonen til Dapla\nmindre SSB-spesifikk dokumentasjon"
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "Spørsmål og svar",
    "section": "",
    "text": "Hvordan finner jeg et Google-prosjekt sin prosjekt-ID?\nProsjekt-ID-en til et Google-prosjekt er en unik identifikator som brukes til å identifisere prosjektet i Google Cloud Platform. Prosjekt-ID-en er en streng som består av små bokstaver, tall og bindestrek. Prosjekt-ID-en er ikke det samme som prosjektnavnet, som kan inneholde store bokstaver og mellomrom.\nDu finner prosjekt-ID ved logge deg inn på GCC, åpne prosjektvelgeren, søk opp ditt prosjekt, og så ser du det i høyre kolonne, slik som vist i denne sladdete kolonnen i Figur 1.\n\n\n\n\n\n\nFigur 1: Prosjektvelgeren i Google Cloud Console\n\n\n\n\n\nHvordan får jeg slettet et GitHub-repo under statisticsnorway?\nHovedregelen er at vi arkiverer repoer istedenfor å slette. Det skyldes at vi kan trenge å ettergå historikken i repoer ved et senere tidspunkt. Arkivering av repoer kan du gjøre selv under Settings i repoet.\nI de tilfellene der du mener at det gir mest mening å slette repoet, så må dette gjøres av en Github-administrator. Da sender du en henvendelse til Kundeservice og ber om at repoet slettes. Husk å oppgi navnet på repoet du ønsker å få slettet.\n\n\nHvordan løser jeg feilmeldinger knyttet til at data rate exceeded i Jupyter?\nNår du mottar følgende melding i Jupyter:\n\nFeilmelding:\nIOPub data rate exceeded.\nThe Jupyter server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--ServerApp.iopub_data_rate_limit`.\n\nCurrent values:\nServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\nServerApp.rate_limit_window=3.0 (secs)\n\nbetyr det at mengden data som sendes fra jupyter-kernelen til jupyterlab-frontend overskrider den tillatte grensen. Selv om det er mulig å justere ServerApp.iopub_data_rate_limit og ServerApp.rate_limit_window for å endre denne grensen, ønsker vi ikke dette. Å endre disse verdiene kan ha en negativ påvirkning på Jupyterlab sin ytelse.\n\nHer er noen løsningsforslag:\n\nReduser datamengden: Prøv å redusere datamengden du prøver å vise. Hvis du for eksempel viser en stor pandas dataframe, kan du vise kun toppradene med df.head() eller et tilfeldig utvalg med df.sample(10).\nLegg til forsinkelse: Bruk time.sleep()-funksjonen i Python for å legge til en pause mellom hver utskrift. Dette kan spre utdataene over en lengre tidsperiode, noe som kan hjelpe med å unngå å overskride datagrensen.\nSkriv til en fil: I stedet for å skrive utdata direkte i Jupyter, kan du vurdere å skrive dataene til en fil. Dette omgår IOPub-datahastighetsgrensen, og du kan se gjennom dataene i ettertid.\nUnngå utskrift: Hvis du kun trenger å utføre beregninger eller operasjoner på dataene, vurder å gjøre det uten å skrive ut resultatene i Jupyter.\n\n\n\n\nHvordan kan jeg gjenopprette filer fra bøtter?\nFremgangsmåten for gjenoppretting av filer er beskrevet i Backup-kapitlet.\n\n\nHvordan sjekker jeg om Kildomaten er tilgjengelig for mitt team?\n\n\nDu kan sjekke om teamet ditt har tilgang til Kildomaten ved å gå inn i teamets IaC-repo. Du finner IaC-repoet ved å søke etter team-navnet på statisticsnorway på Github. Repoet er det som heter &lt;teamnavn&gt;-iac. Inne i repoet går du inn filen infra/projects.yaml.\nI eksemplet til høyre ser vi hvordan filen ser ut for det fiktive teamet dapla-example. For dette teamet ser vi at Kildomaten er skrudd på i prod-miljøet til teamet, men ikke test-miljøet.\n\n\n\n\n\ndapla-example-iac/infra/projects.yaml\n\nteam_uniform_name: dapla-example\n\nprojects:\n  - project_name: dapla-example\n    env: test\n    features:\n      - dapla-buckets\n\n  - project_name: dapla-example\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service\n      - kildomaten\n\n\n\n\n\nHvordan kan jeg se innholdet i mitt team sine bøtter?\nFor å se innholdet i ditt teams sine bøtter kan du enten gjøre dette med kode fra Jupyter, eller via Google Cloud Console (GCC). Les mer om hvordan man kan benytte GCC i SSB.\nHusk at det er kun data-admins som skal kunne se innhold i kildeprosjektet til teamet. For å gjøre det må de aktivere en tilgang i JIT-applikasjonen.\n\n\nAtlantis feiler når jeg skrur på en feature. Hva gjør jeg?\nNår man skrur på en feature på Dapla, så er det verktøyet Atlantis som ruller ut infrastrukturen på GCP. I mange tilfeller kan det hjelpe å skrive atlantis plan i kommentarfeltet til PR’en, slik at Atlantis får kjørt planen en gang til. Hvis den da ikke viser noen feil, så kan du skrive atlantis apply i kommentarfeltet og endringene vil bli effektuert. Får du fortsatt feilmeldinger etter å ha kjørt atlantis plan på nytt, så bør du kontakte Kundeservice for hjelp.\n\n\nJeg får meldingen Account already exists når jeg prøver å logge meg inn på Dapla. Hvordan løser jeg det?\n\n\nDet hender at man får melding om at Account already exists når man logger seg inn på en Dapla-tjeneste. For å løse dette gjør man følgende:\n\nVelg Add to existing account som vist i Figur 2.\nVelg Google slik som vist i Figur 3.\n\nI Figur 3 skal du IKKE fylle inn Username or email og Password. Du skal kun trykke på Google-knappen.\nMeldingen om at Account already exists forekommer svært sjelden, og typisk skjer det første gang man logger seg inn på en tjeneste.\n\n\n\n\n\n\n\n\n\nFigur 2\n\n\n\n\n\n\n\n\n\nFigur 3\n\n\n\n\n\n\n\nHvorfor viser git status endringer når jeg ikke har endret filene?\nDette kan noen ganger forekomme med notebooks eller ipynb-filer fordi de inneholder metadata som kan bli endret over tid. Siden ipynb-filer er et json-basert format, som inkluderer metadata som oppdaterer seg i bakgrunnen, så kan en endring i strukturen til metadataene føre til endringer i den underliggende json-fila selv om man ikke har gjort endringer i ipynb-fila. Endringene blir deretter plukket opp av git som følger med på endringer i filene.\nEn mulig løsning er å sjekke inn endringene i kodebasen.\nI noen tilfeller viser git status at det har skjedd endringer i filer, men git diff viser ingen endring. I disse tilfellene kan det være ulike problemer i bakgrunnen. Se denne oversikten over problemer og tilhørende løsninger.\n\n\nHvordan setter jeg opp 2FA med Authenticator-appen mot GitHub etter at jeg har byttet telefon?\nFørste gang man begynner å jobbe med kode i SSBs GitHub-organisasjon statisticsnorway, så følger man denne oppskriften fra Dapla-manualen. Men hvis man bytter telefon eller lignende, så må man sette opp dette på nytt.\nProsessen for å bytte 2FA-metode for autentisering er beskrevet i GitHub sin dokumentasjon. Gjør følgende:\n\nLogg deg inn med din bruker på https://github.com/.\nKlikk på din profil, øverst til høyre på siden, og velg Settings\nI menyen til venstre velger du Password and authentication\nSkroll ned til Two-factor-authentication\nUnder Two-factor-methods vil du se Authenticator app (Configured) hvis du har brukt dette tidligere.\nTrykk Edit\nSkann QR-koden som dukker opp med Authenticator-appen på din nye mobil.\nFølg instruksjonene og lagre autentiseringsmetoden når du er ferdig.\n\n\n\nHvorfor får jeg ikke opprettet mapper fra Google Cloud Console (GCC)?\nI mars 2024 ble det gjort sikkerhetstiltak på Dapla som innebar at funksjonaliteten Download, Upload og Create folder i GCC ble skrudd av. Hvis man prøver å opprette en mappe på den måten vil man få en feilemelding som starter med\nRequest is prohibited by organization's policy. vpcServiceControlsUniqueIdentifier:\nHvis man skal opprette mapper i en bøtte så må man gjøre det med dapla-toolbelt, eller gjøre det fra en tjeneste som Jupyter i Dapla Lab. Sistnevnte metode betyr at man åpner tjenesten som det teamet og tilgangsgruppen som har tilgang til ønsket bøtte, og deretter kan man opprette mapper med mkdir i terminalen.\n\n\nHvor finner jeg dokumentasjonen til Dapla Lab tjenester der?\nHer finner du Dokumentasjonen for Dapla Lab.\nDokumentasjon for tjenestene finner du i Dapla-manualen under Datatjenester/Dapla Lab tjenester.\n\n\nHvorfor får jeg feilmelding i Dapla Lab etter å ha vært borte fra datamaskinen min en stund?\nI de fleste tilfeller vil dette løses seg ved at laster inn siden på nytt eller refresher siden.\nGrunnen til at det skjer er at Dapla Lab bare er tilgjengelig for de som er logget på SSBs nettverk. Hvis du forlater maskinen med Dapla Lab åpnet i nettleseren over lang tid, så kan det være at du har falt ut av SSBs nettverk og Dapla Lab ikke oppdager at du er logget inn igjen.\n\n\nHva betyr det at tjenesten min er suspendert?\n\nAt en tjeneste er suspendert betyr at tjenesten er satt på pause. I praksis vil det si at ressursene, f.eks. CPU og RAM, som tjenesten har reservert har blitt frigjort. Man kan starte tjenesten på nytt ved å trykke på Play-knappen i bildet til høyre, og man starter opp tjenesten på nytt med de samme ressursene. Alt som ble lagret i tjenestens lokale filsystem under /home/onyxia/work/blir også gjennopprettet, mens resten blir borte. Derfor bør alltid kode lagres under work-mappen.\n\n\nKan jeg jobbe med kildedata fra Dapla Lab?\nJa. Medlemmer av gruppen data-admins kan gi seg selv midlertidig tilgang til kildedata. Dette gjøres ved å bruke Just-in-Time Access (JIT) applikasjonen: https://jitaccess.dapla.ssb.no. Les manual-artikkelen om JIT for en veiledning.\n\n\nHvorfor kan jeg ikke jobbe med dataene til alle team jeg er med i fra tjenestene i Dapla Lab?\nMange ansatte i SSB er med i flere Dapla-team, og hvert av brukernes teammedlemskap gir ofte tilgang til data iht til formaålet for teamet. hver av teamene har tilgang til sine data og potensielt andres delt data. For å unngå for brede tilganger til data så innfører vi at en bruker må bestemme seg for hvilket team og tilgangsgruppe de skal representere før de starter arbeid med data i et programmeringsmiljø. Dette gjør at vi kan være sikker på at ingen kan koble data på tvers av team.\n\n\nHvordan jobber jeg med notebooks i VS Code?\nFor å jobbe med notebooks i VS Code kan du gjøre følgende:\n\nÅpne en ny Vscode-python\nÅpne en terminal under File/Terminal/New terminal\nOpprett et ssb-project som vanlig under mappen /home/onyxia/work/ ved å skrive ssb-project create &lt;prosjektnavn&gt;.\n\nDersom du allerede har et ssb-project, skriver du cd &lt;prosjektnavn&gt; etterfulgt av ssb-project build. Da blir prosjektets tilhørende pakker installert i en kernel med samme navn som prosjektet.\n\nGå inn i mappen med cd &lt;prosjektnavn&gt;\nInstaller ønskede pakker poetry add pandas, f.eks. Pandas.\nVelg prosjektmappen i filutforskeren ved å gå til File/Open folder/ og åpne prosjektmappen i boksen som vises.\nOpprett en notebook-fil og åpne den.\nTrykk Refresh på hele nettsiden for å få opp visningen av den nyopprettede kernels.\nØverst til høyre i Notebooken kan du trykke på Select kernel og utforskeren velger du Jupyter kernels og deretter den som har samme navn som prosjektet ditt.\nTil slutt kan du teste at riktig Python-installasjon er valgt ved å importere pakken du akkurat installerte.\n\nDu må kjøre en ssb-project build, som beskrevet over, hver gang en instans av Jupyter, eller VSCode blir startet. Hvis du isteden åpner en pauset (suspendert) instans, vil du kunne fortsette arbeidet fra forrige kjøring uten å måtte kjøre ssb-project build.\n\n\nHvordan kjører man en Dash-app?\n\nFra vscode-python tjenesten\nDet fungerer best å kjøre Dash-apper i en egen fane i nettleseren. Inline-visning av figurer fungerer ikke enda.\nHer er et eksempel på hvordan man lager en Dash-app i DaplaLab i en vscode tjeneste:\n\nInstaller pandas og dash i et ssb-project.\nOpprett et python-script i prosjektet som f.eks. heter app.py.\nKjør scriptet ved å kjøre følgende kommando fra terminalen: poetry run python ./app.py\nDeretter kommer det opp et dialog-vinduet hvor du velger Open in browser.\n\nHer er et eksempel på script som fungerer i Vscode-python:\n\n\napp.py\n\nfrom dash import Dash, html, dcc, callback, Output, Input\nimport plotly.express as px\nimport pandas as pd\n\n# Creating a sample dataset similar to the Gapminder dataset\ndata = {\n    'country': ['Afghanistan', 'Afghanistan', 'Afghanistan', 'Brazil', 'Brazil', 'Brazil', 'China', 'China', 'China'],\n    'continent': ['Asia', 'Asia', 'Asia', 'South America', 'South America', 'South America', 'Asia', 'Asia', 'Asia'],\n    'year': [1952, 1977, 2007, 1952, 1977, 2007, 1952, 1977, 2007],\n    'lifeExp': [28.801, 39.877, 43.828, 50.917, 59.504, 72.390, 44.000, 63.739, 72.961],\n    'pop': [8425333, 12412311, 31889923, 56602560, 80461570, 190010647, 556263527, 862030000, 1318683096],\n    'gdpPercap': [779.4453145, 786.11336, 974.5803384, 2108.944355, 5718.766744, 9065.800825, 400.448611, 1488.041713, 4959.114854]\n}\n\n# Creating DataFrame\ndf = pd.DataFrame(data)\n\n# Creating the app\napp = Dash(\n    __name__,\n    requests_pathname_prefix='/proxy/8050/', \n    serve_locally=True\n)\n\napp.layout = [\n    html.H1(children='Title of Dash App', style={'textAlign':'center'}),\n    dcc.Dropdown(df.country.unique(), 'Canada', id='dropdown-selection'),\n    dcc.Graph(id='graph-content')\n]\n\n@callback(\n    Output('graph-content', 'figure'),\n    Input('dropdown-selection', 'value')\n)\ndef update_graph(value):\n    dff = df[df.country==value]\n    return px.line(dff, x='year', y='pop')\n\nif __name__ == '__main__':\n    app.run(debug=True)\n\n\n\nFra jupyter tjenesten\nDet fungerer best å kjøre Dash-apper i en egen fane i nettleseren, men inline-visning skal også være mulig i notebook.\nHer er et eksempel på hvordan man lager en Dash-app i DaplaLab i en jupyter tjeneste:\n\nInstaller pandas og dash i et ssb-project.\nOpprett en notebook i prosjektet som f.eks. heter app.ipynb.\n\nHer er et eksempel på kode som fungerer i jupyter:\n\n\napp.ipynb\n\n# %%\n# Notebook cell 1\nfrom dash import Dash, html, dcc, callback, Output, Input\nimport plotly.express as px\nimport pandas as pd\n\n# %%\n# Notebook cell 2\n# Creating a sample dataset similar to the Gapminder dataset\ndata = {\n    'country': ['Afghanistan', 'Afghanistan', 'Afghanistan', 'Brazil', 'Brazil', 'Brazil', 'China', 'China', 'China'],\n    'continent': ['Asia', 'Asia', 'Asia', 'South America', 'South America', 'South America', 'Asia', 'Asia', 'Asia'],\n    'year': [1952, 1977, 2007, 1952, 1977, 2007, 1952, 1977, 2007],\n    'lifeExp': [28.801, 39.877, 43.828, 50.917, 59.504, 72.390, 44.000, 63.739, 72.961],\n    'pop': [8425333, 12412311, 31889923, 56602560, 80461570, 190010647, 556263527, 862030000, 1318683096],\n    'gdpPercap': [779.4453145, 786.11336, 974.5803384, 2108.944355, 5718.766744, 9065.800825, 400.448611, 1488.041713, 4959.114854]\n}\n\n# Creating DataFrame\ndf = pd.DataFrame(data)\n\n# Creating the app\nport = 8050 # must be between 1024 and 9999\nservice_prefix = os.getenv('JUPYTERHUB_SERVICE_PREFIX', '/')\ndomain = os.getenv('JUPYTERHUB_HTTP_REFERER', None)\n\napp = Dash(\n    __name__,\n    requests_pathname_prefix=f\"{service_prefix}proxy/{port}/\", \n    serve_locally=True\n)\n\napp.layout = [\n    html.H1(children='Title of Dash App', style={'textAlign':'center'}),\n    dcc.Dropdown(df.country.unique(), 'Canada', id='dropdown-selection'),\n    dcc.Graph(id='graph-content')\n]\n\n@callback(\n    Output('graph-content', 'figure'),\n    Input('dropdown-selection', 'value')\n)\ndef update_graph(value):\n    dff = df[df.country==value]\n    return px.line(dff, x='year', y='pop')\n\nif __name__ == '__main__':\n    # For jupyter_mode, choose between 'external' or 'inline'.\n    # 'jupyterlab' should also be poosible, but doesn't seem to work...\n    app.run(jupyter_mode=\"external\", jupyter_server_url=domain, port=port)\n\n\n\n\nHvilke miljøvariabler er tilgjengelig i utviklingstjenestene?\nDette er miljøvariablene som alltid vil være tilgjengelig i Jupyterlab, Rstudio og VSCode:\n\n\n\n\n\n\n\nMiljøvariabel\nForklaring\n\n\n\n\nDAPLA_ENVIRONMENT\nHvilket dapla-lab miljø du kjører i f.eks PROD, TEST eller DEV\n\n\nDAPLA_REGION\nHvilket dapla miljø du kjører i f.eks. DAPLA_LAB eller BIP\n\n\nDAPLA_USER\nEpostaddressen til den innloggede brukeren f.eks. obr@ssb.no\n\n\nDAPLA_SERVICE\nDapla tjenesten som er i bruk f.eks. JUPYTERLAB eller VS_CODE\n\n\nSTAT_TEMPLATE_DEFAULT_REFERENCE\nHvilken versjon av ssb-project templatet som skal brukes f.eks. 1.1.8\n\n\nPSEUDO_SERVICE_URL\nURL til pseudotjenesten for bruk til testing\n\n\nSTATBANK_BASE_URL\nURL til statistikkbanken\n\n\nSTATBANK_ENCRYPT_URL\nURL til ‘statbank-authenticator’ sitt krypteringsendepunkt\n\n\nSTATBANK_TEST_BASE_URL\nURL til statistikkbanken (i test)\n\n\nSTATBANK_TEST_ENCRYPT_URL\nURL til ‘statbank-authenticator (i test)’ sitt krypteringsendepunkt\n\n\n\n\n\nHvorfor får jeg feilmelding når jeg prøver å slette en delt-bøtte? Jeg får melding om at Error trying to delete bucket  objects without ‘force destroy’ set to true når jeg prøver å slette en delt-bøtte ved å fjerne den fra shared-buckets.yaml\nEn delt-bøtte kan kun slettes hvis den er helt tom for data. Det betyr at:\n\nAlle filer og mapper må være slettet.\nSiden filer versjoneres i delt-bøtter må også alle gamle versjoner av filer også være slettet (såkalt non-current objekter).\n\nHvis man har prøvd å slette en delt-bøtte ved fjerne bøtten fra shared-buckets.yaml, kjøre atlantis apply og deretter fått feilmeldingen Error trying to delete bucket &lt;bøttenavn&gt; objects without 'force destroy' set to true, så betyr det at det fortsatt ligger data i bøtta. Da kan du gjøre følgende for å få slettet bøtta:\n\nLegg til delt-bøtta i buckets-shared.yaml igjen og kjør atlantis apply etter å ha fått godkjenning.\nKjør følgende Python-script i en tjeneste på Dapla Lab som er startet med teamets developers-gruppe og google-cloud-storage er installert:\n\n\n\nNotebook\n\nimport google.cloud.storage as storage\n\nclient = storage.Client()\n\nfor blob in client.list_blobs(\"FULLT NAVN PÅ DELT-BØTTE\", versions=True):\n    blob.delete()\n\n\nFjern delt-bøtta fra buckets-shared.yaml igjen og kjør atlantis apply etter godkjenning.\n\nFungerer ikke det så kan man ta kontakt med Kundeservice og få hjelp.\n\n\nHvordan kan jeg pause en regelmessig jobb i Transfer Service?\nMan kan pause en regelmessig jobb med Transfer Service fra Google Cloud Console i nettleseren.\n\nÅpne Consolet\nVelge riktig Google prosjekt, slik som vist Figur 4\n\n\n\n\n\n\n\nFigur 4: Prosjektvelger i Consolet\n\n\n\n\nSøk opp Transfer Jobs i søkefeltet øverst på siden.\nVelg hvilken Transfer Service jobb som skal endres.\nVelg Disable job som vist i Figur 5\n\n\n\n\n\n\n\nFigur 5: Valg for Transfer Service jobber.\n\n\n\nFor å aktivere jobben kan man følge stegene over, men da velge Enable job i siste steg.\n\n\nHvordan søker jeg om åpning for utgående trafikk i Dapla Lab (åpning av APIer for å hente data)?\nHvis du trenger å hente data fra eksterne kilder til bruk i statistikkproduksjonen (f.eks. APIer), må du søke om tilgang til dette hos Team M2M. Dette gjøres ved å melde sak til Kundeservice. Se veiledning nedenfor.\nFra Byrånettet velg Arbeidsverktøy og Meld sak til kundeservice. Logg deg inn ved å velge Logg inn med federert autentisering. Inne løsningen velger du deretter Dapla helpdesk. Figur 6 viser menyen som vises.\n\n\n\n\n\n\nFigur 6: Visning av en Kundeservice-sak\n\n\n\nFra menyen velger du Søknad for utgående trafikk i DaplaLab, fyller ut skjemaet som vises og trykker på «Send».\nTeam M2M vil så godkjenne eller avslå søknaden. Hvis søknaden godkjennes sendes saken videre til Kundeservice for effektuering. Hvis avslag så får du en begrunnelse for dette fra Team M2M.\n\n\nHvorfor fungerer det ikke med å melde seg ut av automatiske pausingen av tjenester på Dapla Lab?\nHvis man opplever at en tjeneste fortsatt pauses selv om man har endret visningsnavnet til tjenesten til å inkludere [nosuspend], så kan det skyldes at man faktisk ikke har lagret det nye navnet. For å lagre det nye navnet så må man trykke Enter etter at man skrevet inn [nosuspend]. Gjør man ikke dette kan det tilsynelatende virke som om du har endret navnet selv om det ikke lagret. Du kan verifisere at navnet er riktig ved refreshe siden og se at navnet nå er riktig."
  },
  {
    "objectID": "news/posts/2025-10-09-vardef-workshop/index.html",
    "href": "news/posts/2025-10-09-vardef-workshop/index.html",
    "title": "Workshop om Vardef",
    "section": "",
    "text": "Velkommen til workshop om migrering fra Vardok til Vardef. I workshopen presenterer vi kort hvordan man kan migrere fra Vardok til Vardef. Deretter vil plattformutviklere og støtteteam for informasjonsforvaltning kunne bistå med ressurser til å hjelpe brukere med konkrete spørsmål de måtte ha knyttet til overgangen til Vardef. Man kan også ta med deg PC-en din og få hjelp til praktisk migrering fra Vardok til Vardef.\nOslo: 27. oktober 12:00-15:00 Kursrom Utdanning (lenke til arrangement)\nKongsvinger: 28. oktober 10:00-13:00 Prosjektrom Mulighetsrommet 713 (lenke til arrangement)"
  },
  {
    "objectID": "news/posts/2025-09-25-daplanytt-sep25/index.html",
    "href": "news/posts/2025-09-25-daplanytt-sep25/index.html",
    "title": "DaplaNytt-møte 25.9.2025",
    "section": "",
    "text": "DaplaNytt fra september 2025 ble avholdt som Teams-møte 25. september 2025. Se opptaket her.\nSlides finner du her."
  },
  {
    "objectID": "news/posts/2024-11-09-data-admins-developers/index.html",
    "href": "news/posts/2024-11-09-data-admins-developers/index.html",
    "title": "Data-admins i developers-gruppa",
    "section": "",
    "text": "Det er bestemt at medlemmer av tilgangsgruppen data-admins også skal være medlem av developers-gruppa. Det er på denne måten at data-admins også får tilgang til standardprosjektet til teamet. Dette er en beslutning om arkitekturen rundt tilgangsstyring på Dapla, der medlemskap i de to gruppene gir helt forskjellige tilganger. Medlemskap i data-admins-gruppa vil da vanligvis innebære de samme tilgangene som developers-gruppa, med mindre de aktiverer de forhåndsgodkjente tilgangene til kildedata."
  },
  {
    "objectID": "news/posts/2025-03-25-dapla-lab-pausing-time/index.html",
    "href": "news/posts/2025-03-25-dapla-lab-pausing-time/index.html",
    "title": "Nytt tidspunkt for automatisk pausing på Dapla Lab",
    "section": "",
    "text": "Alle tjenester på Dapla Lab blir pauset kl 22.00 hver dag. I forbindelse med at vi nå tilbyr brukere å melde seg ut av den automatiske pausingen ved behov, så endrer vi nå tidspunktet for pausing fra 22.00 til 17.00 fra og med torsdag 27. mars.\n\n\n\n\n\n\nFigur 1: Bilde av tjeneste som er pauset."
  },
  {
    "objectID": "news/posts/2025-05-13-dapladag-2025/index.html",
    "href": "news/posts/2025-05-13-dapladag-2025/index.html",
    "title": "Årets Dapla-dager er i gang",
    "section": "",
    "text": "Onsdag forrige uke ble årets Dapladag gjennomført i SSBs lokaler i Oslo. Her kunne ansatte få med seg Frokost med Geir, ulike presentasjoner om Dapla for ulike målgrupper, ny runde med Dapla-grunnkurs og avslutning med pizzafest og quiz. I morgen, onsdag 14. mai 2025, arrangeres Dapla-dagen i Kongsvinger.\nSe året program og påmelding her."
  },
  {
    "objectID": "news/posts/2024-11-11-jupyter-pyspark-chapter/index.html",
    "href": "news/posts/2024-11-11-jupyter-pyspark-chapter/index.html",
    "title": "Nytt kapittel om Jupyter-pyspark",
    "section": "",
    "text": "Vi har skrevet et kapittel i Dapla-manualen om hvordan man bruker Jupyter-pyspark i Dapla Lab. Sjekk det ut her."
  },
  {
    "objectID": "news/posts/2024-11-25-endring-i-tjenestekonfig/index.html",
    "href": "news/posts/2024-11-25-endring-i-tjenestekonfig/index.html",
    "title": "Endring i tjenestekonfigurasjon på Dapla Lab",
    "section": "",
    "text": "Tjenestekonfigurasjonen i Dapla Lab har endret seg. Dette er altså menyen man får opp når man starter opp en ny tjeneste, for eksempel en ny instans av Jupyter eller VSCode. Parameterne man kan konfigurere er derimot uendret. Følgende artikler i manualen har derfor blitt oppdatert: jupyter, jupyter-pyspark, jupyter-playground, VSCode-python, RStudio, Datadoc, JDemetra og artikkelen om Dapla Lab.\n\n\n\n\n\n\nFigur 1: Tjenestekonfigurasjon for jupyter"
  },
  {
    "objectID": "news/posts/2025-05-07-datadoc-editor-update/index.html",
    "href": "news/posts/2025-05-07-datadoc-editor-update/index.html",
    "title": "Ny versjon av Datadoc-editor",
    "section": "",
    "text": "I den nye versjonen varsles brukeren når det avdekkes avvik mellom datasettet og eksisterende metadata slik at man kan korrigere metadataene.\nEn liten feil rapportert av Østhus, Andreas er blitt fikset av Corneil, Carl Franklin: Vi nå kun ta hensyn til primær plassering av en statistikk i et statistikkområde når det utledes mens før tok vi det første vi fant, som førte til merarbeid på korrigering i en del tilfeller. Fasiten for å utlede dette er det som statistikkbanken benytter for kategorisering av statistikk og den ligger her for de som er interessert: www.ssb.no/xp/_/service/mimir/subjectStructurStatistics\nI tillegg har vi migrert prosjektet til uv fra Poetry. Det er ikke noe brukerne skal merke, men vi kan melde om en stor forbedring på utvikleropplevelsen.\nRelease Notes: https://github.com/statisticsnorway/datadoc-editor/releases/tag/v1.0.13"
  },
  {
    "objectID": "news/posts/2025-10-17-jupytext-bug/index.html",
    "href": "news/posts/2025-10-17-jupytext-bug/index.html",
    "title": "Fjerning av Jupytext-extension i VS Code",
    "section": "",
    "text": "Team Statistikktjenester har besluttet at Jupytext-extension vi har hatt i VS Code på Dapla Lab må avinstalleres. Årsaken er at noen brukere opplever at den sletter kode de har skrevet. Problemet skyldes trolig at extension ikke har blitt oppdatert på flere år.\nVi forsøkte en annen extension som erstatning, men dette medførte nye utfordringer. Det ble derfor besluttet at vi fjerner disse extension’ene inntill problemene er fikset."
  },
  {
    "objectID": "news/posts/2025-10-17-jupytext-bug/index.html#hva-betyr-dette-for-meg",
    "href": "news/posts/2025-10-17-jupytext-bug/index.html#hva-betyr-dette-for-meg",
    "title": "Fjerning av Jupytext-extension i VS Code",
    "section": "Hva betyr dette for meg?",
    "text": "Hva betyr dette for meg?\nBrukere av VS Code vil ikke lenger kunne synkronisere notebooks og py-filer med en Jupytext-extension."
  },
  {
    "objectID": "news/posts/2025-04-25-update-dapla-ctrl/index.html",
    "href": "news/posts/2025-04-25-update-dapla-ctrl/index.html",
    "title": "Dapla Ctrl viser nå type delt-bøtte",
    "section": "",
    "text": "Ifm. innføring av en ny type delt-bøtte på Dapla, så har Dapla Ctrl blitt oppdatert slik at den også viser hvilken type en delt-bøtte det er. Dette viser både på Oversiktssiden for teamets bøtter slik som vist i Figur 1, og på siden for en enkelt delt-bøtte som vist i Figur 2.\n\n\n\n\n\n\nFigur 1: Oversiktssiden som viser alle delt-bøttene til et team\n\n\n\n\n\n\n\n\n\nFigur 2: Siden som viser hvem som har tilgang til en gitt delt-bøtte"
  },
  {
    "objectID": "news/posts/2024-12-11-nais-platform/index.html",
    "href": "news/posts/2024-12-11-nais-platform/index.html",
    "title": "Tar i bruk Navs applikasjonsplattform",
    "section": "",
    "text": "I 2025 skal SSB for fullt ta i bruk Navs applikasjonsplattform, NAIS.\n\nMed Nais er mange gjennomtenkte valg allerede tatt. Det blir lettere for applikasjonsutviklere i SSB å gå fra idé til implementering, samtidig som vi slipper utvikling og vedlikehold som ellers måtte gjøre selv.\n\nLes mer i denne SSB-interne saken."
  },
  {
    "objectID": "news/posts/2025-05-14-kildomat-check/index.html",
    "href": "news/posts/2025-05-14-kildomat-check/index.html",
    "title": "Validere at filer er prosessert i Kildomaten",
    "section": "",
    "text": "Ifm. driftstansen, og driftsproblemene i etterkant, i Kildomaten fra 12. mai kl. 15.00 til 14. mai kl. 13.00 så kan det være team som ønsker å sjekke om det kom inn filer i kildebøtta som ikke trigget Kildomaten. I denne artikkelen så finner man eksempelkode for å gjøre denne sjekken."
  },
  {
    "objectID": "news/posts/2025-05-14-kildomat-check/index.html#sjekke-kildebøtta",
    "href": "news/posts/2025-05-14-kildomat-check/index.html#sjekke-kildebøtta",
    "title": "Validere at filer er prosessert i Kildomaten",
    "section": "Sjekke kildebøtta",
    "text": "Sjekke kildebøtta\nDet første man bør gjøre er å sjekke om det, i tidsintevallet for driftsstansen, har blitt skrevet filer til mapper i kildebøtta som vanligvis trigger Kildomaten-jobber. Har det ikke skjedd blitt skrevet filer, så trenger man ikke å gjøre noe mer.\nSjekken kan gjøres av en data-admins på teamet på følgende måte:\n\nÅpne Jupyter på Dapla Lab\nUnder tjenestekonfigurasjonen for Jupyter velger du å representere data-admins for det aktuelle teamet\nÅpne en notebook og velg en kernel som har Python-pakken gcsfs installert. Hvis du ikke har en slik kernel kan du opprette et ssb-project fra terminalen med kommandoen ssb-project create test, gå inn i mappen og installere pakken ved å skrive poetry add gcsfs. Da vil kernelen du skal velge hete test.\nKjør denne koden etter at du har endret verdiene til variablene source_bucket og source_folder:\n\n\n\n\nNotebook\n\nfrom datetime import datetime\nfrom gcsfs import GCSFileSystem\nimport pytz\n\nutc = pytz.UTC\n\nsource_bucket = \"gs://ssb-dapla-felles-kilde-prod\"\nsource_folder = \"freg\"\n\nfilter_start_date = datetime(year=2025, month= 5, day=12, hour=15)\nfilter_end_date = datetime(year=2025, month=5, day=14, hour=13)\n\n\n\ndef is_file_modified_on_date(file_path: str) -&gt; bool:\n    \"\"\"\n    Returns True if file at file path was modified on the date \"filter_date\", returns False otherwise\n    \"\"\"\n    file_info = GCSFileSystem().info(path=file_path)\n    modified_date: datetime | None = file_info.get(\"mtime\")\n    if not isinstance(modified_date, datetime):\n        return False\n\n    return modified_date &lt; utc.localize(\n        filter_end_date\n    ) and modified_date &gt; utc.localize(filter_start_date)\n\n\nsource_files = GCSFileSystem().find(\n    path=f\"{source_bucket}/{source_folder}\", maxdepth=None, withdirs=False\n)\nsource_files_at_date = [f for f in source_files if is_file_modified_on_date(f) is True]\n\nprint(f\"Source files found between date {str(filter_start_date)} and {str(filter_end_date)}:\")\nprint(source_files_at_date)\n\nHvis man kjører koden over og den returnerer en tom liste så har man verifisert at det ikke ble skrevet noen filer til mappen under driftsstansen. Hvis koden returnerer en liste med filnavn så bør man undersøke om filene ble prosessert av Kildomaten eller ikke."
  },
  {
    "objectID": "news/posts/2025-05-14-kildomat-check/index.html#verifisere-produktbøtta",
    "href": "news/posts/2025-05-14-kildomat-check/index.html#verifisere-produktbøtta",
    "title": "Validere at filer er prosessert i Kildomaten",
    "section": "Verifisere produktbøtta",
    "text": "Verifisere produktbøtta\nNår man har en liste med filnavn som ble skrevet til kildebøtta i tidsperioden med driftsstans, så må man verifisere om filene ble skrevet til produktbøtta som forventet. Hvordan man gjør dette vil avhenge av logikken i teamets kildomaten-skript. Hvis Kildomaten-skriptet skriver en fil til produktbøtta med samme filnavn som er brukt i kildebøtta, så kan man verfisere at samme fil også finne i kildebøtta. Hvis Kildomaten-skriptet endrer navnet, så må man først gjenskape det nye navnet, og deretter verifisere.\nFor å verifisere om filen er prosessert må man gjøre følgende:\n\nhente ut liste med filer fra forrige steg (krever innloggin som data-admins i Dapla Lab)\nåpne en ny tjeneste i Dapla Lab som developers og kopiere over listen fra tjenesten man åpnet som data-admins.\nåpne en notebook og velg en kernel som har Python-pakken gcsfs installert. Hvis du ikke har en slik kernel kan du opprette et ssb-project fra terminalen med kommandoen ssb-project create test, gå inn i mappen og installere pakken ved å skrive poetry add gcsfs. Da vil kernelen du skal velge hete test.\nKjør koden under, der source_files_at_date er listen du kopierte over fra Sjekke kildebøtta-delen. I tillegg må du oppgi hvilken bøtte som skal sjekkes under bucket, og hvilken undermappe i bøtta som det skal søkes gjennom i folder.\n\nfrom gcsfs import GCSFileSystem\n\n# Denne listen inneholder filnavnene vi identifiserte tidligere\nsource_files_at_date = [\n  'ssb-etlev-data-kilde-prod/kildetilgang/access-grant_abc.json',  \n  'ssb-etlev-data-kilde-prod/kildetilgang/access-grant_def.json',]\n\n# Disse variablene må brukeren oppgi selv  \nbucket = \"gs://my-bucket-name\"\nfolder = \"my/folder/name\"\n\n\ndef find_missing_files(bucket_name: str, folder_name: str, source_files: list[str]) -&gt; list[str]:\n    file_names = [f.split(sep=\"/\")[-1].split(sep=\".\")[0] for f in source_files]\n\n    fs_prefix = \"\" if \"gs://\" in bucket_name else \"gs://\"\n    fs_path = f\"{fs_prefix}{bucket_name}/{folder_name}\"\n    fs = GCSFileSystem()\n\n    missing_files = []\n\n    for i, f in enumerate(file_names):\n        found = fs.glob(f\"{fs_path}/**/{f}.*\")\n        if len(found) == 0:\n            source_file = f\"gs://{source_files[i]}\" if \"gs://\" not in source_files[i] else source_files[i]\n            missing_files.append(source_file)\n    \n    return missing_files\n\nfind_missing_files(\n  bucket_name=bucket,\n  folder_name=folder,\n  source_files=source_files_at_date)\nFunksjonen over returnerer alle filer den ikke finner på den oppgitte stien, og som derfor ikke er prosessert av Kildomaten. I kapitlet som heter Trigge prosessering manuelt finner du en beskrivelse av hvordan man trigger prosesseringen av disse filene manuelt."
  },
  {
    "objectID": "news/posts/2025-05-14-kildomat-check/index.html#fra-altinn3-til-isee",
    "href": "news/posts/2025-05-14-kildomat-check/index.html#fra-altinn3-til-isee",
    "title": "Validere at filer er prosessert i Kildomaten",
    "section": "Fra Altinn3 til ISEE",
    "text": "Fra Altinn3 til ISEE\nStatistikkproduksjoner som har datafangst fra Altinn3 og synkroniserer filer prosessert med Kildomaten ned til ISEE, må selv hente ut en liste over hvilke filer som er lastet til ISEE. En fremgangsmåte er sammenligne hvilke filer som innkvittert i SFU med hva som ligger i ISEE-databasen. Disse kan identifiseres ved deres unike referansenummer i SFU. Hvis man har en liste med referansenummer som finnes i SFU, men ikke i ISEE, så kan søke etter disse filene i kildebøtta.\nTa kontakt med Kundeservice dersom man ønsker hjelp til dette."
  },
  {
    "objectID": "news/posts/2025-05-14-kildomat-check/index.html#trigge-prosessering-manuelt",
    "href": "news/posts/2025-05-14-kildomat-check/index.html#trigge-prosessering-manuelt",
    "title": "Validere at filer er prosessert i Kildomaten",
    "section": "Trigge prosessering manuelt",
    "text": "Trigge prosessering manuelt\nHvis koden i forrige avsnitt returnerte en liste med filer, så kan man manuelt trigge prosessering av disse filene med Kildomaten.\n\n\n\n\n\n\nImportantLogg deg inn som data-admins for trigge Kildomaten manuelt\n\n\n\nFor å trigge Kildomaten manuelt må man være logget inn som data-admins i en Dapla Lab tjeneste. Det betyr at man kan liste ut alle filer som ble skrevet i et gitt tidspunkt, og trigge prosesseringen av disse i samme notebook eller script. Men man må oppgi project_id for standardprosjektet (ikke kildeprosjektet) i koden under, noe som kan være forvirrende for brukeren. Dette er planlagt endret slik at det blir lettere bruke fremover.\n\n\nKoden i forrige avsnitt returnerte en liste med alle filer som heter source_files_at_date, og vi kan loope over disse og trigge prosesseringen. For å gjøre dette må du først fylle inn riktig verdier for project-id og source_name.\n\nproject-id for standardprosjektet. Slik finner du prosjekt-id. Merk at dette ikke er kilde-prosjektet!\nsource_name er navnet på mappen i iac-repoet hvor kilden du skal trigge er definert. Navnet på kilden i eksempelet med team dapla-example var altinn.\n\n\n\nNotebook\n\nfrom dapla import trigger_source_data_processing\n\n# Brukeren fyller inn her\nproject_id = \"\"\nsource_name = \"\"\n\n# Fjerner bøttenavn fra filstier\nstripped_paths = [path.split('/', 1)[1] if '/' in path else path for path in source_files_at_date]\n\n# Trigger prosessering for alle filer i listen\nfor file in stripped_paths:\n    trigger_source_data_processing(project_id, source_name, file, kuben=True)"
  },
  {
    "objectID": "news/posts/2025-05-21-kildomaten-instances/index.html",
    "href": "news/posts/2025-05-21-kildomaten-instances/index.html",
    "title": "Økt ytelse i Kildomaten",
    "section": "",
    "text": "Kildomaten vil nå som standard prosessere 100 filer parallelt, mot 5 tidligere. Endringen vil spesielt forbedre ytelsen for team som mottar svært mange filer kildebøtta som skal prosesseres med en Kildomaten-jobb. Antall parallelle prosesser kan også økes over 100 som er standard, men vi ønsker da at teamet tar kontakt med Kundeservice slik at vi kan få erfaringer med hvordan andre systemer, som pseudonymisering, håndterer dette.\nForbedringen i ytelse påvirker i hovedsak Kildoamten sin evne til å prosessere mange små/mellomstore filer, og ikke hvordan den håndterer store filer. Team Statistikktjenester jobber kommer fremover til iverksette tiltak å øke kapasiteten på prosessering av store filer også.\nLes oppdatert dokumentasjon for Kildomaten her."
  },
  {
    "objectID": "news/posts/2025-01-08-metodebib-chapter/index.html",
    "href": "news/posts/2025-01-08-metodebib-chapter/index.html",
    "title": "Nytt kapittel om Metodebiblioteket",
    "section": "",
    "text": "Vi har skrevet et kapittel i Dapla-manualen om metodebiblioteket. Sjekk det ut her."
  },
  {
    "objectID": "news/posts/2025-03-18-klass-web-upgrade/index.html",
    "href": "news/posts/2025-03-18-klass-web-upgrade/index.html",
    "title": "Nytt utseende for Klass-siden på ssb.no",
    "section": "",
    "text": "Den 18. mars 2025 ble nettsiden som lar brukere utforske i kodeverk i Klass på www.ssb.no/klass oppdatert. Brukere vil merke at utseende nå er oppdatert og mer i tråd med utseendet på resten av www.ssb.no. I tillegg har selve applikasjonen blitt oppgradert og migrert til å kjøre på SSBs nye applikasjonsplattform NAIS.\nFigur 1 viser hvordan den nye nettsiden ser ut.  \n\n\n\n\n\n\nFigur 1: Den nye nettsiden på ssb.no for Klass."
  },
  {
    "objectID": "news/posts/2025-09-04-daplanytt-aug25/index.html",
    "href": "news/posts/2025-09-04-daplanytt-aug25/index.html",
    "title": "DaplaNytt-møte 4.9.2025",
    "section": "",
    "text": "DaplaNytt fra august 2025 ble avholdt som Teams-møte 4. septemeber 2025. Se opptaket her.\nSlides finner du her."
  },
  {
    "objectID": "news/posts/2025-06-12-auth-client-blogginnlegg/index.html",
    "href": "news/posts/2025-06-12-auth-client-blogginnlegg/index.html",
    "title": "Blogginnlegg om dapla-auth-client",
    "section": "",
    "text": "Autentiseringsklassen for dapla er flyttet fra python-pakken dapla-toolbelt til dapla-auth-client. Les mer om dette i denne blogg-artikkelen."
  },
  {
    "objectID": "news/posts/2025-01-16-manualen-omstrukturert/index.html",
    "href": "news/posts/2025-01-16-manualen-omstrukturert/index.html",
    "title": "Omstrukturering av manualen!",
    "section": "",
    "text": "Finner du ikke favorittartiklene dine? Det er fordi vi har omstrukturert manualen. Vi har delt opp det som tidligere var jobbe-med-kode inn i 3 forksjellige artikler: SSB-project, Pakkehåndtering i Python og Pakkehåndtering i R."
  },
  {
    "objectID": "news/posts/2025-09-15-dapla-toolbelt-metadata-forbedret-errormeldinger/index.html",
    "href": "news/posts/2025-09-15-dapla-toolbelt-metadata-forbedret-errormeldinger/index.html",
    "title": "Forbedret errormeldinger i dapla-toolbelt-metadata",
    "section": "",
    "text": "dapla-toolbelt-metadata v0.9.2 er sluppet ut. I denne versjonen forbedres errormeldinger som gis når man programmatisk produserer metadata for en ny periode slik at det er enklere å forstå hva det er som avviker."
  },
  {
    "objectID": "news/posts/2025-09-15-dapla-toolbelt-metadata-forbedret-errormeldinger/index.html#introduksjon",
    "href": "news/posts/2025-09-15-dapla-toolbelt-metadata-forbedret-errormeldinger/index.html#introduksjon",
    "title": "Forbedret errormeldinger i dapla-toolbelt-metadata",
    "section": "",
    "text": "dapla-toolbelt-metadata v0.9.2 er sluppet ut. I denne versjonen forbedres errormeldinger som gis når man programmatisk produserer metadata for en ny periode slik at det er enklere å forstå hva det er som avviker."
  },
  {
    "objectID": "news/posts/2025-09-15-dapla-toolbelt-metadata-forbedret-errormeldinger/index.html#hvorfor-gjør-vi-endringene",
    "href": "news/posts/2025-09-15-dapla-toolbelt-metadata-forbedret-errormeldinger/index.html#hvorfor-gjør-vi-endringene",
    "title": "Forbedret errormeldinger i dapla-toolbelt-metadata",
    "section": "Hvorfor gjør vi endringene?",
    "text": "Hvorfor gjør vi endringene?\nNår man lager metadata for en ny periode av en statistikk bygger det på en antagelse om at det er ingen strukturelle endringer i datasettet. Hvis det er det så kan metadataene være feilaktig og det kan være et brudd på Daplas system for versjonering av datasett.\ndapla-toolbelt-metadata gjør en rekke sjekker når man produserer metadataen for den nye perioden og returnerer en feilmelding dersom det er oppdaget noe avvik. Sjekkene utført på variabler i et datasett var relativt enkle og selv om de plukket opp at det var noe avvik, så kommunisert de ikke hva avviket var på en god måte.\ndapla-toolbelt-metadata nå detekterer og tydelig kommuniserer alle disse ulike avvik mellom eksisterende metadata og nytt datasett:\n\nDatasettet har flere variabler.\nDatasettet har færre variabler.\nDatasettet har et variabel med et annet navn.\nDatasettet har ulik rekkefølge av variabler.\nDatasettet har variabler med annen datatype.\n\nDersom man er bevisst om risikoen for å produsere feilaktig metadata så kan man slå av errorne slik."
  },
  {
    "objectID": "news/posts/2025-09-15-dapla-toolbelt-metadata-forbedret-errormeldinger/index.html#eksempel-på-en-av-de-nye-feilmeldingene",
    "href": "news/posts/2025-09-15-dapla-toolbelt-metadata-forbedret-errormeldinger/index.html#eksempel-på-en-av-de-nye-feilmeldingene",
    "title": "Forbedret errormeldinger i dapla-toolbelt-metadata",
    "section": "Eksempel på en av de nye feilmeldingene",
    "text": "Eksempel på en av de nye feilmeldingene\n\n\n\n\n\n\nFigur 1: Eksempel av error på avvik mellom nytt datasett og eksisterende metadata."
  },
  {
    "objectID": "news/posts/2025-01-29-dapla-lab-dagene/index.html",
    "href": "news/posts/2025-01-29-dapla-lab-dagene/index.html",
    "title": "Åpen dag om Dapla Lab i Oslo og Kongsvinger",
    "section": "",
    "text": "10. og 11. februar arrangerer vi åpen dag om Dapla Lab for å hjelpe brukere med overgangen til Dapla Lab. Utviklere, tech-coacher og støtteteam stiller fysisk for å hjelpe deg med å flytte kode, filer, sette opp Git- og GitHub-konfigurasjon, eller svare på spørsmål/innspill man måtte ha ifm. overgangen. Vi håper at så mange som mulig tar turen innom for å få hjelp eller bare slå av en prat.\nArrangement i Oslo 10. februar kl. 12 i Auditoriet\nArrangement i Kongsvinger 11. februar kl. 12 i A005"
  },
  {
    "objectID": "news/posts/2025-04-29-change-data-admins/index.html",
    "href": "news/posts/2025-04-29-change-data-admins/index.html",
    "title": "Endring i tilganger for data-admins",
    "section": "",
    "text": "Ifm med utrulling av en ny type delt-bøtte ble det oppdaget at noen team hadde en feil i tilgangene for data-admins. Dette ble rettet i sammenheng med utrulling av de nye delt-bøttene. I praksis vil data-admins merke dette ved at de ikke lenger har tilgang til produkt-bøtta når de logger seg som data-admins i Dapla Lab."
  },
  {
    "objectID": "news/posts/2025-09-08-editeringsrammeverk/index.html",
    "href": "news/posts/2025-09-08-editeringsrammeverk/index.html",
    "title": "Ny artikkel om Editeringsrammeverket",
    "section": "",
    "text": "Editeringsrammeverket er nå klart for bruk. Det er et Python-bibliotek for å forenkle jobben med å sette opp applikasjoner for kvalitetssikring og editering av data. Artikkelen finner du her.\nFunksjonalitet er delt inn i ‘moduler’ som du kan legge inn enkeltvis og tilpasse til dine egne behov.\nVi har laget ‘byggekloss’ moduler som er svært fleksible og kan tilpasses til å vise det aller meste. Vi har også skreddersydde moduler som er rettet mot skjemabaserte utvalgsundersøkelser for å forenkle overgangen til Dapla for undersøkelser som bruker Altinn 3.\nPå siden i Dapla Manualen får du en oversikt over hva rammeverket er og overordnet informasjon, med henvisninger til mer detaljert dokumentasjon."
  },
  {
    "objectID": "news/posts/2025-02-14-cost-in-dapla-lab/index.html",
    "href": "news/posts/2025-02-14-cost-in-dapla-lab/index.html",
    "title": "Estimerte kostnader i Dapla Lab",
    "section": "",
    "text": "Du kan nå se et estimat for hvor mye din tjeneste koster under Ressurser-fanen i tjenestekonfigurasjonen på Dapla Lab. Dette er ikke et nøyaktig estimat, men kan gi brukeren en indikasjon på hvor mye ressursene vil koste ila en arbeidsdag."
  },
  {
    "objectID": "news/posts/2024-11-25-referansegruppe-dapla-lab/index.html",
    "href": "news/posts/2024-11-25-referansegruppe-dapla-lab/index.html",
    "title": "Når skrus ‘gamle’ jupyter av?",
    "section": "",
    "text": "Under forrige DaplaNytt ble 15. januar 2025 nevt som en mulig sluttdato for ‘gamle’ jupyter, altså https://jupyter.dapla.ssb.no, og dermed en endelig overgang til Dapla Lab.\nDenne datoen er tentativ. Sluttdato for gamle jupyter vil bli endeliggjort etter møtet med referansegruppen for statistikktjenester 6. desember 2024. Referansegruppen har representanter fra hver statistikkavdeling.\nI mellomtiden ber vi brukere om å henvende seg til sine respektive støtteteam, nærmeste leder eller medlemmene av referansegruppen for innvendinger eller innspill til når vi kan gjennomføre en endelig overgang til Dapla Lab."
  },
  {
    "objectID": "news/posts/2024-12-16-blogpost-pandera/index.html",
    "href": "news/posts/2024-12-16-blogpost-pandera/index.html",
    "title": "Nytt blogginnlegg om Pandera",
    "section": "",
    "text": "Manualens blogg har blitt oppdatert med en post om hvordan validere data i kode ved hjelp av Python pakken Pandera. Les mer her."
  },
  {
    "objectID": "news/posts/2025-06-24-dapla-toolbelt-update/index.html",
    "href": "news/posts/2025-06-24-dapla-toolbelt-update/index.html",
    "title": "Oppdater til versjon 4 av dapla-toolbelt",
    "section": "",
    "text": "Team Statistikktjenester har sluppet versjon 4.0.0 av dapla-toolbelt. Den største endringen i versjon 4 er at AuthClient-delen av dapla-toolbelt nå er fjernet fra dapla-toolbelt og skilt ut i en egen pakke som heter dapla-auth-client. Funksjonene i dapla-toolbelt vil fortsatt fungerere som før, men dapla-toolbelt benytter seg av en ny pakke og måte for autentisering mot NAIS. Alle brukere bør derfor oppdatere til versjon 4 ved første anledning.\nFordelen med å skille ut autentisering i en egen pakke er at brukere som kun trengte dapla-toolbelt for autententisering mot NAIS, ikke lenger trenger å importere alle avhengighetene til dapla-toolbelt til sitt prosjekt.\nFormålet med å fortsatt tilby AuthClient i dapla-toolbelt er for å autentisere brukere mot Maskinporten Guardian og Data Collector på NAIS.\nDu kan lese dokumentasjonen til dapla-auth-client her, i tillegg er det skrevet et blogg-artikkel om endringen."
  },
  {
    "objectID": "news/posts/2024-11-14-kildedata-datadoc/index.html",
    "href": "news/posts/2024-11-14-kildedata-datadoc/index.html",
    "title": "Kildedata og Datadoc",
    "section": "",
    "text": "Datadoc støtter ikke dokumentasjon av datasett i datatilstanden kildedata enda. Det vil støttes på sikt, men foreløpig er det ikke funksjonalitet for dette.\nI Datadoc-applikasjonen i Dapla Lab får man nå valget med å logge seg inn i tjenesten som data-admins, og dermed en teknisk mulighet til å dokumentere kildedata. Siden dette ikke er støttet enda, så ber vi alle om å ikke gjøre dette. Det er ingen grunn til å aktivere sin kildedatatilgang når dette ikke støttes.\nLes mer i dokumentasjonen til Datadoc."
  },
  {
    "objectID": "news/posts/2025-04-24-config-files-in-template/index.html",
    "href": "news/posts/2025-04-24-config-files-in-template/index.html",
    "title": "Mal for statistikk-repoer oppdatert med konfigurasjonsfiler",
    "section": "",
    "text": "Det er lurt å samle felles innstillinger og verdier som koden din trenger i en konfigurasjonsfil, i stedet for at de er spredt rundt omkring i koden.\nMalen for statistikk-repoer er nå oppdatert med eksempel og kode for hvordan man kan bruke slike konfigurasjonsfiler. Konfigurasjonsfilen er en tekstfil som lagres i TOML-format, og som leses inn med biblioteket DynaConf. Se README-filen for ytterligere beskrivelse og konkrete eksempler.\nDere kan oppdatere deres egne repoer med de nye endringene i malen ved å følge denne beskrivelsen. Ta kontakt med tech-coacher eller støtteteam hvis dere ønsker hjelp til oppdateringen."
  },
  {
    "objectID": "news/posts/2025-08-19-kildomaten-update/index.html",
    "href": "news/posts/2025-08-19-kildomaten-update/index.html",
    "title": "Oppgradering av Kildomaten 26.august",
    "section": "",
    "text": "Tirsdag 26. august oppgraderer vi Kildomaten for alle Dapla-team. Formålet er å øke stabiliteten og gi en bedre brukeropplevelse."
  },
  {
    "objectID": "news/posts/2025-08-19-kildomaten-update/index.html#dette-blir-nytt",
    "href": "news/posts/2025-08-19-kildomaten-update/index.html#dette-blir-nytt",
    "title": "Oppgradering av Kildomaten 26.august",
    "section": "Dette blir nytt",
    "text": "Dette blir nytt\n\nIngen timeout-grense ved prosessering\nMulighet til å prosessere betydelig flere filer samtidig sammenlignet med tidligere\nTeam som mottar svært mange filer i kildebøtta samtidig vil ikke lenger oppleve problemer/ustabilitet\nMer detaljert logging\nEnklere oversikt over om en kildefil er prosessert eller ikke"
  },
  {
    "objectID": "news/posts/2025-08-19-kildomaten-update/index.html#nedetid",
    "href": "news/posts/2025-08-19-kildomaten-update/index.html#nedetid",
    "title": "Oppgradering av Kildomaten 26.august",
    "section": "Nedetid",
    "text": "Nedetid\nOppgraderingen medfører ca. 15 minutters nedetid per team i tidsrommet kl. 09–15. I denne perioden vil ikke Kildomaten prosessere nye filer fra kildebøtta, og team kan ikke gjøre endringer i sitt IaC-repo. Etter oppgraderingen vil Kildomaten fortsette å prosessere filer som ikke allerede er prosessert. Brukere og team trenger ikke å gjøre noe i forbindelse med oppgraderingen."
  },
  {
    "objectID": "news/posts/2025-08-19-kildomaten-update/index.html#viktig-for-team-med-isee-synkronisering-til-bakken",
    "href": "news/posts/2025-08-19-kildomaten-update/index.html#viktig-for-team-med-isee-synkronisering-til-bakken",
    "title": "Oppgradering av Kildomaten 26.august",
    "section": "Viktig for team med ISEE-synkronisering til bakken",
    "text": "Viktig for team med ISEE-synkronisering til bakken\nTeam som synkroniserer data fra produktbøtta ned til bakken og videre til ISEE bør være oppmerksomme på at enkelte filer kan bli prosessert på nytt. Dette kan føre til dubletter i ISEE. Hvis dette er en utfordring, anbefaler vi å pause synkroniseringen i oppgraderingsperioden."
  },
  {
    "objectID": "news/posts/2025-08-19-kildomaten-update/index.html#tidskritiske-publiseringer",
    "href": "news/posts/2025-08-19-kildomaten-update/index.html#tidskritiske-publiseringer",
    "title": "Oppgradering av Kildomaten 26.august",
    "section": "Tidskritiske publiseringer",
    "text": "Tidskritiske publiseringer\nDersom teamet ditt har publiseringer som er tidskritiske ift oppgraderingsvinduet, ta kontakt med Kundeservice for å avtale et annet tidspunkt."
  },
  {
    "objectID": "news/posts/2025-02-07-parquet-viewer/index.html",
    "href": "news/posts/2025-02-07-parquet-viewer/index.html",
    "title": "Parquet-utforsker i VS Code",
    "section": "",
    "text": "Sjekk ut den nye Parquet-utforskeren i Vscode-python. Skrive SQL, filtrer,inspiser metadata uten bruk av Python eller R. Les mer i denne blogg-artikkelen."
  },
  {
    "objectID": "news/posts/2025-10-20-editeringsrammeverk-update/index.html",
    "href": "news/posts/2025-10-20-editeringsrammeverk-update/index.html",
    "title": "Endringer i editeringsrammeverket og tilhørende artikkel",
    "section": "",
    "text": "Vi har videreutviklet editeringsrammeverket (python-pakken ssb-dash-framework) og tilhørende artikkel i manualen."
  },
  {
    "objectID": "news/posts/2025-10-20-editeringsrammeverk-update/index.html#hva-betyr-dette-for-meg",
    "href": "news/posts/2025-10-20-editeringsrammeverk-update/index.html#hva-betyr-dette-for-meg",
    "title": "Endringer i editeringsrammeverket og tilhørende artikkel",
    "section": "Hva betyr dette for meg?",
    "text": "Hva betyr dette for meg?\nDet betyr at du nå kan editere dataframes på Dapla uten å bruke EimerDB!"
  },
  {
    "objectID": "news/posts/2025-04-04-dapla-toolbelt-meta-python/index.html",
    "href": "news/posts/2025-04-04-dapla-toolbelt-meta-python/index.html",
    "title": "dapla-toolbelt-metadata støtter igjen Python 3.10",
    "section": "",
    "text": "I dapla-toolbelt-metadata v0.5.0 ble støtte for Python 3.10 fjernet. Det gjorde at pakken ikke var kompatibel med mange prosjekter skapt vha. ssb-project. Endringen skapte problemer for flere prosjekter og derfor er støtte for Python 3.10 nå støttet i dapla-toolbelt-metadata v0.6.2."
  },
  {
    "objectID": "news/posts/2025-04-11-bug-dapla-lab-admins/index.html",
    "href": "news/posts/2025-04-11-bug-dapla-lab-admins/index.html",
    "title": "Forbedring av kildedatatilgang i Dapla Lab",
    "section": "",
    "text": "Når man har startet en tjeneste på Dapla Lab som data-admins, så har er tilgangen til data alltid tidsbegrenset. Tidligere har man ikke kunne starte tjenesten igjen når tilgangen har gått ut. Dette har skapt utfordringer for brukere som ikke har pushet koden til GitHub.\nNå vi gjort endriner i Dapla Lab slik at tjenesten også kan startes opp etter at datatilgangen har utgått. Formålet med dette er å la brukere pushe kode til GitHub, og det er ønskelig at brukere sletter disse tjenestene etter at det er gjort."
  },
  {
    "objectID": "news/posts/2025-05-20-dapla-overgang-101/index.html",
    "href": "news/posts/2025-05-20-dapla-overgang-101/index.html",
    "title": "Ny artikkel - Hvordan gjennomføre overgang til Dapla?",
    "section": "",
    "text": "Vi har skrevet en artikkel om hvordan man bør gjennomføre overgangen til Dapla. Les artikkelen her."
  },
  {
    "objectID": "news/posts/2025-03-24-dapla-lab-tmux/index.html",
    "href": "news/posts/2025-03-24-dapla-lab-tmux/index.html",
    "title": "Støtte for langtkjørende jobber på Dapla Lab",
    "section": "",
    "text": "Brukere som har behov for å kjøre jobber på Dapla Lab som tar mer enn 12 timer, har opplevd at kjøringene avbrytes etter noen timer uten noen åpenbar grunn. Feilen har også oppstått når brukeren har lukket nettleser-fanen eller maskinen til brukeren har gått i dvale, selv om dette ikke i teorien skulle påvirke kjøringen. Årsaken virker å være IDE-ene (Jupyter, VS Code og RStudio) pauser jobbene når de blir pauset.\nFor å unngå denne oppførselen er terminal-verktøyet tmux installert i Jupyter, VS Code og RStudio. tmux er terminal multiplexer som lar brukeren kjøre og organisere mange terminal-sesjoner i et vindu. En av fordelene med tmux i denne sammenhengen er at den lar brukeren kjøre prosesser selv om terminalen lukkes. Derfor anbefaler vi at brukere som skal kjøre kode over lang tid (&gt;12h) kjører koden i en tmux-session.\nDu kan kjøre ditt Python-script ved å kjøre følgende:\n\n\nTerminal\n\ntmux new -s minsesjon\n\nI koden over startes en session med navn minsesjon. For å aktivere et shell for ditt ssb-project, slik at din Python-installasjon kan kalles på med python-kommandoen i en terminal, så kan du kjøre kommandoen:\n\n\nTerminal\n\npoetry shell\n\nNå kan du kjøre scriptet ditt på følgende måte:\n\n\nTerminal\n\npython mittskript.py\n\nDeretter kan lukke PC-en, eller nettleser-fanen, og være trygg på at skriptet ditt vil kjøre til det er ferdig. Husk at alle tjenester pauses kl 22 hver kveld, så hvis du ikke ønsker det må du også følge denne oppskriften.\ntmux operer med konsepter som session, window og panes. Over startet vi en session med navnet minsesjon. Dette er et komplett arbeidsmiljø hvor man kan ha flere windows og splitte de opp i panes.\nEn av styrkene til tmux er at man kan kjøre prosesser i bakgrunnen. Derfor er det også viktig å kjenne til at en session kan være attached, at brukeren kan se sesjonen i terminalen, og detached, som betyr at den kan kjøre i bakgrunnen mens brukeren jobber med noe annet. Du kan liste ut dine sesjoner ved å skrive følgende i en terminal:\n\n\nTerminal\n\ntmux ls\n\nI eksempelet over så kan vi koble til (attach) til vår sesjon ved å skrive:\n\n\nTerminal\n\ntmux attach -t minsesjon\n\nDu vet at din sesjon er attached ved at du ser en grønn statusbar nederst med navnet til sesjonen, slik som vist Figur 1.\n\n\n\n\n\n\nFigur 1: Statusbar for en tmux-sesjon\n\n\n\nDu kan koble fra en sesjon (detach) ved å skrive CTRL B og deretter d, eller skrive følgende i terminalen:\n\n\nTerminal\n\ntmux detach -s minsesjon\n\nProsessen vil deretter kjøre i sesjonen, samtidig som du kan jobbe med andre ting i tjenesten."
  },
  {
    "objectID": "news/posts/2025-04-24-transfer-service-onprem/index.html",
    "href": "news/posts/2025-04-24-transfer-service-onprem/index.html",
    "title": "Beslutning om Transfer Service fra testmiljøet på Dapla til bakken",
    "section": "",
    "text": "Mange brukere har bedt om å kunne overføre data fra et Dapla-team sitt test-miljø til bakken. Dette er spesielt relevant for team har datafangst fra Altinn 3 på Dapla, men som skal prosessere dataene på bakken.\nDet er nå besluttet at det ikke skal åpnes for å kunne overføre data fra test-miljø på Dapla til bakken. Årsaken er at det ikke finnes noe test-miljøet på bakken, og derfor lager dette en åpning for å overføre data mellom prod- og test-miljøene på Dapla. En slik åpning er problematisk og derfor er det nå besluttet at dette ikke skal være mulig.\nLøsningen for team som har datafangst med Altinn3, og får test-data fra team SU-V, blir derfor å bruke Transfer Service fra prod-miljøet til teamet for å overføre til bakken. Team SU-V har nå gitt Transfer Service i prod-miljøet til alle team med datafangst fra Altinn 3 tilgang til test-bøtta for data, slik som vist i Figur 1.\nTeam som allerede har fått satt opp muligheten til å overføre data fra test-miljøet til bakken, vil bli kontaktet av team Skyinfra for endre dette til løsningen vist i Figur 1.\n\n\n\n\n\n\n\n\nflowchart LR \n\n    subgraph suv-altinn-data-t\n        id_suv_bucket_t[(ssb-suv-altinn-raXXXX-YY-test)]\n    end\n\n    subgraph ssb-teamnavn-kilde-t\n        id_stat_kilde_bucket_t[(data-kilde)]\n    end\n    id_suv_bucket_t &lt;-- Transfer service read/write --&gt; id_stat_kilde_bucket_t\n\n    subgraph ssb-teamnavn-t\n        id_stat_produkt_bucket_t[(data-produkt)]\n    end\n    id_stat_kilde_bucket_t -- Kildomaten --&gt; id_stat_produkt_bucket_t\n\n    subgraph suv-altinn-data-p\n        id_suv_bucket_p[(ssb-suv-altinn-raXXXX-YY-prod)]\n    end\n\n    subgraph ssb-teamnavn-kilde-p\n        id_stat_kilde_bucket_p[(data-kilde&lt;br&gt;/altinn/prod/)]\n        id_stat_kilde_bucket_pt[(data-kilde&lt;br&gt;/altinn/test/)]\n    end\n    id_suv_bucket_p &lt;-- Transfer service read/write --&gt; id_stat_kilde_bucket_p\n    id_suv_bucket_t -- Transfer service read --&gt; id_stat_kilde_bucket_pt\n\n    subgraph ssb-teamnavn-p\n        id_stat_produkt_bucket_p[(data-produkt&lt;br&gt;/inndata/altinn/)]\n        id_stat_produkt_bucket_pt[(data-produkt&lt;br&gt;/temp/test/altinn/)]\n\n        id_stat_kilde_bucket_p -- Kildomaten --&gt; id_stat_produkt_bucket_p\n        id_stat_kilde_bucket_pt -- Kildomaten --&gt; id_stat_produkt_bucket_pt\n\n        id_stat_frasky_bucket_p[(frasky&lt;br&gt;/altinn/prod)]\n        id_stat_frasky_bucket_pt[(frasky&lt;br&gt;/altinn/test)]\n\n        id_stat_produkt_bucket_p -- Transfer service --&gt; id_stat_frasky_bucket_p\n        id_stat_produkt_bucket_pt -- Transfer service --&gt; id_stat_frasky_bucket_pt\n    end\n\n    subgraph linux on-prem\n        id_cloudsync_p[(cloud_sync&lt;br&gt;/frasky/altinn/prod)]\n        id_cloudsync_pt[(cloud_sync&lt;br&gt;/frasky/altinn/test)]\n\n        id_stat_frasky_bucket_p -- Transfer service --&gt; id_cloudsync_p\n        id_stat_frasky_bucket_pt -- Transfer service --&gt; id_cloudsync_pt\n\n        id_stammekat_p[(stammekatalog&lt;br&gt;/wk24/datafangst/gSdv/)]\n        id_stammekat_pt[(stammekatalog&lt;br&gt;/test/wk24/datafangst/gSdv/)]\n\n        id_cloudsync_p -- MoveIT --&gt; id_stammekat_p \n        id_cloudsync_pt -- MoveIT --&gt; id_stammekat_pt \n    end\n\n    subgraph Oracle DB1P\n        id_isee_p[(ISEE Prod)]\n    end\n    id_stammekat_p -- crontab --&gt; id_isee_p\n\n    subgraph Oracle DB1T\n        id_isee_t[(ISEE Test)]\n    end\n    id_stammekat_pt -- crontab --&gt; id_isee_t\n\n\n\n\n\n\n\n\n\nFigur 1: Figur som viser dataflyt for statistikkproduksjon på Dapla og Dapla/ISEE."
  },
  {
    "objectID": "news/posts/2025-06-06-new-cluster/index.html",
    "href": "news/posts/2025-06-06-new-cluster/index.html",
    "title": "Viktig melding om endringer i Dapla Lab",
    "section": "",
    "text": "Tirsdag 10. juni 2025 kl 15.00 vil team Skyinfrastruktur gjennomføre en endring i testmiljøet til Dapla Lab (https://lab.dapla-test.ssb.no/). Endringen innebærer at brukere må slette alle eksisterende tjenester de har kjørende før kl. 15.00. Før tjenesten slettes så må man lagre alt man ønsker å beholde, siden dette også slettes når tjenesten slettes. F.eks. skal kode lagres på GitHub og data lagres i bøtter.\nEndringen gjennomføres ila av noen minutter, og man kan derfor starte opp en ny tjeneste og fortsette arbeidet noen minutter etter kl 15.00. Du kan verifisere at det er klart for å starte nye tjenester ved å se etter teksten Test Ny øverst til venstre i Dapla Lab, slik som vist i Figur 1.\nHvis man ikke har mulighet til å slette tjenesten kl 15.00, så kan man i en periode få tilgang til gamle tjenester på følgende url: https://legacy-lab.dapla-test.ssb.no/.\nTa kontakt med Kundeservice dersom man har spørsmål eller trenger hjelp til noe."
  },
  {
    "objectID": "news/posts/2025-06-06-new-cluster/index.html#endring-i-prod",
    "href": "news/posts/2025-06-06-new-cluster/index.html#endring-i-prod",
    "title": "Viktig melding om endringer i Dapla Lab",
    "section": "Endring i prod",
    "text": "Endring i prod\nTilsvarende endring vil gjennomføres i prodmiljøet til Dapla Lab den 17. juni kl. 15.00, der gamle tjenester kan nåes på url-en https://legacy-lab.dapla.ssb.no/ i en periode etter endringen er gjort."
  },
  {
    "objectID": "news/posts/2025-10-21-datadoc-eksterne-referanser/index.html",
    "href": "news/posts/2025-10-21-datadoc-eksterne-referanser/index.html",
    "title": "Hvordan refererer til eksterne systemer fra Datadoc",
    "section": "",
    "text": "Før i Datadoc-editor så oppga man URLer i feltene “Definisjon URI” og “Kodeverkets URI”. Nå oppgir man kun en ID i disse feltene, som refererer til en Variabeldefinisjon i Vardef eller et Kodeverk i Klass. UIet i Datadoc-editor har oppdatert instruksjoner og verdiene valideres.\n\n\n\nDet nye feltet for Variabeldefinisjon ID.\n\n\nDisse endringene tilgjengeliggjøres i dapla-toolbelt-metadata v0.9.8 og Datadoc-editor v1.3.0."
  },
  {
    "objectID": "news/posts/2025-10-21-datadoc-eksterne-referanser/index.html#hva-er-endret",
    "href": "news/posts/2025-10-21-datadoc-eksterne-referanser/index.html#hva-er-endret",
    "title": "Hvordan refererer til eksterne systemer fra Datadoc",
    "section": "",
    "text": "Før i Datadoc-editor så oppga man URLer i feltene “Definisjon URI” og “Kodeverkets URI”. Nå oppgir man kun en ID i disse feltene, som refererer til en Variabeldefinisjon i Vardef eller et Kodeverk i Klass. UIet i Datadoc-editor har oppdatert instruksjoner og verdiene valideres.\n\n\n\nDet nye feltet for Variabeldefinisjon ID.\n\n\nDisse endringene tilgjengeliggjøres i dapla-toolbelt-metadata v0.9.8 og Datadoc-editor v1.3.0."
  },
  {
    "objectID": "news/posts/2025-10-21-datadoc-eksterne-referanser/index.html#hvorfor-er-endringene-gjort",
    "href": "news/posts/2025-10-21-datadoc-eksterne-referanser/index.html#hvorfor-er-endringene-gjort",
    "title": "Hvordan refererer til eksterne systemer fra Datadoc",
    "section": "Hvorfor er endringene gjort?",
    "text": "Hvorfor er endringene gjort?\nÅ kunne oppgi en vilkårlig URL hadde en liten fordel i og med at det er en veldig fleksibel løsning. Men det har flere ulemper, særlig med tanke på forvaltning av metadata over tid. Fordi:\n\nMan kunne legge inn en URL som har ingenting med Metadata å gjøre. Dette kunne etterhvert blitt forvirrende eller i verste fall farlig hvis man blir lurt til å åpne visse lenker. Nå skal vi kunne garantere at slike lenker sender en kun til kjente systemer.\nDet kan være flere ulike URLer som refererer til en ressurs, f.eks for Klass er det ulike URLer for Nettsiden og APIet. Det er viktig at vi kan kontrollere hvilken som er brukt i en viss kontekst.\nDet gir mulighet for datavalidering slik at kvaliteten i metadata er ivaretatt.\nDet åpner for bedre funksjonalitet etterhvert, som f.eks automatisk søk og kobling til variabeldefinisjoner."
  },
  {
    "objectID": "news/posts/2025-10-21-datadoc-eksterne-referanser/index.html#hva-betyr-dette-for-meg",
    "href": "news/posts/2025-10-21-datadoc-eksterne-referanser/index.html#hva-betyr-dette-for-meg",
    "title": "Hvordan refererer til eksterne systemer fra Datadoc",
    "section": "Hva betyr dette for meg?",
    "text": "Hva betyr dette for meg?\nHvis man har metadata dokumenter liggene med URLer i disse feltene så har vi lagt inn funksjonalitet til å oppgradere informasjon. Her er det to tilfeller:\n\nVi kjenner igjen formen av URLen til å være en referanse til Vardef eller Klass også omformer vi den til den nyere format.\nVi kjenner ikke igjen formen.\n\nI dette tilfellet endrer vi ikke på URLen\nVi skriver en loggmelding med instruksjoner til å ta kontakt med Team Metadata\nVi har særlig lyst til å høre fra dere dersom dere mener man skulle kunne refererer til et annet metadata system (f.eks en Eurostat kodeverk). Da må vi legge til støtte for det også.\nURLen må gjøres noe med før eventuell publisering til datakatalogen\n\n\nHvis man ikke har URLer i metadata dokumentene sine, er det bare å dokumentere utifra instruksjonene i Datadoc-editor som vanlig.\n\nProgrammatisk oppdatering\nI noen tilfeller kan det være enklere å oppdatere til den nye modellen programmatisk og ikke med Datadoc-editor. Under er et eksempel på hvordan man programmatisk oppdaterer en dokumentert enkeltfil:\n\n\nNotebook\n\nfrom pathlib import Path\nfrom dapla_metadata.datasets.core import Datadoc\n\n# Angi stien til metadatadokumentet\nmeta_path = \"/path/to/metadata.json\"\n\nmeta = Datadoc(metadata_document_path=meta_path)\nmeta.write_metadata_document()\nprint(f\"Upgraded {Path(meta_path).stem} to v{meta.container.datadoc.document_version}\")\n\nHvis vi kjenner ikke igjen URLen så blir det sendt en varsel slik som dette:\nThe URL is not in a supported format for field 'definition_uri' of variable 'fnr'. URL: 'https://www.vg.no'. Please contact Team Metadata if this URL should be supported."
  },
  {
    "objectID": "news/posts/2025-02-09-monter-delt-bøtter/index.html",
    "href": "news/posts/2025-02-09-monter-delt-bøtter/index.html",
    "title": "Delt-bøtter støttes i Dapla Lab",
    "section": "",
    "text": "Man kan nå tilgjengeliggjøre delt-bøtter fra andre team inne i Jupyter, Vscode-python og Rstudio. Les dokumentasjon her, eller se videoen under:"
  },
  {
    "objectID": "news/posts/2025-04-01-validate-standards/index.html",
    "href": "news/posts/2025-04-01-validate-standards/index.html",
    "title": "Nye funksjoner for å validere navnestandard",
    "section": "",
    "text": "Team Metadata har nå lansert funksjonalitet i dapla-toolbelt-metadata for å sjekke om filene i bøttene følger navnestandarden. Metoden check_naming_standard kan validere alt fra en enkel filsti til en hel bøtte og returnerer lister med eventuelle brudd på navnestandarden. I tillegg er det mulig å hente ut en enkel oppsummering med generate_validation_report. Det er laget en notebook med eksempler https://github.com/statisticsnorway/dapla-toolbelt-metadata/blob/main/demo/standards/navnestandard.ipynb."
  },
  {
    "objectID": "news/posts/2025-04-07-altinn-part/index.html",
    "href": "news/posts/2025-04-07-altinn-part/index.html",
    "title": "Blogginnlegg om Altinn 3 og ISEE",
    "section": "",
    "text": "Magnus Theodor Engh har skrevet et blogginnlegg som forklarer hvordan man kan integrere datafangst med Altinn 3 på Dapla, med lasting til ISEE i prodsonen. Les innlegget her."
  },
  {
    "objectID": "news/posts/2024-11-20-daplanytt-nov24/index.html",
    "href": "news/posts/2024-11-20-daplanytt-nov24/index.html",
    "title": "DaplaNytt-møte 19.11.2024",
    "section": "",
    "text": "DaplaNytt for november 2024 ble avholdt som Teams-møte 19. november 2024.\nSe opptaket her (intern lenke)."
  },
  {
    "objectID": "news/posts/2024-11-08-endring-cpu-ram/index.html",
    "href": "news/posts/2024-11-08-endring-cpu-ram/index.html",
    "title": "Endring i default-maskinkraft på Dapla Lab",
    "section": "",
    "text": "Førstkommende mandag gjør vi en endring i hvor mye RAM og CPU programmeringsmiljøene på Dapla Lab har som default. Vi endrer antall millicores (M) CPU fra 2000M til 200M, og mengden RAM fra 8GB til 4GB. Dvs. at brukeren aktivt må øke ressursene i tjenestekonfigurasjonen (se Figur 1) før oppstart hvis de ønsker mer enn dette.\nÅrsaken til at vi gjør endringen er at vi har observert at flere brukere reserverer ressurser som ikke benyttes, og at dette skaper unødvendige kostnader for SSB.\nEndringen gjelder tjenestene Jupyter, Jupyter-playground, Vscode og Rstudio.\n\n\n\n\n\n\nFigur 1: Bilde av Ressurser-fanen under tjenestekonfigurasjonen til Jupyter"
  },
  {
    "objectID": "blog/posts/2025-04-07-altinn3-til-isee/index.html",
    "href": "blog/posts/2025-04-07-altinn3-til-isee/index.html",
    "title": "Fra Altinn 3 til ISEE",
    "section": "",
    "text": "Her skal vi gå gjennom det du trenger for å få i gang lasting av Altinn 3 skjemaer til ISEE, steg for steg.\nMåten vi gjør det ved å gå gjennom hvordan prosessen er satt opp for hagebruksundersøkelsen, så har du et konkret eksempel å se på underveis. Det kan stort sett kopieres så lenge du endrer filstier og navn fra hagebruk til å passe egen undersøkelse\nVi kommer til å være innom Transfer service og Kildomaten, men det er ikke krav om forkunnskaper for å komme gjennom denne veiledningen.\nNår prosessen er satt opp for en undersøkelse så vil det i de fleste tilfeller ikke være behov for å endre på det, så det er vanligvis en engangsjobb å få på plass.\nEksemplene som vises er for å laste produksjonsdata, dersom du ønsker å sende ned skjemaer fra test-altinn (tt02) til DB1T kan du se på tegningen av den overordnede flyten.\nProsessen er identisk, men filstiene er ulike. Se skjermbildet for flyten nedenfor for eksempel på oppsettet.\nDet anbefales å sette opp test-løpet, så du slipper å lage rot i DB1P."
  },
  {
    "objectID": "blog/posts/2025-04-07-altinn3-til-isee/index.html#overblikk-over-prosessen.",
    "href": "blog/posts/2025-04-07-altinn3-til-isee/index.html#overblikk-over-prosessen.",
    "title": "Fra Altinn 3 til ISEE",
    "section": "Overblikk over prosessen.",
    "text": "Overblikk over prosessen.\nI prosessen er det en del overføringer som må settes opp, men de kjøres automatisk når det først er i gang. Stegene vi skal gjennom er:\n1. Transfer service fra datamottak til din kildebøtte\n2. Kildomaten\n3. Transfer service fra din produktbøtte til frasky-bøtten\n4. Transfer service fra din frasky-bøtte til bakken.\n5. Opprette mappestruktur på stammen\n6. MoveIT og lasting til ISEE\nTestløpet krever at du gjør alle stegene dobbelt opp. Én gang for prod, én gang for test. Det er viktig å ikke blande disse løpene i hverandre, og at filstiene viser til en prod-mappe eller test-mappe. Se flytdiagrammet nedenfor for konkrete filstier.\n\n\n\n\n\n\nImportantOverføring mellom suv-altinn-data-t og prod\n\n\n\nFor å sette opp testløpet kan det foreløpig være nødvendig å kontakte kundeservice for å åpne på tilganger, slik at du kan overføre filer fra suv-altinn-data-t, datamottaket hvor skjemaer fra test-altinn / tt02 havner, til prod bøtten din.\nTil vanlig så har du kun tilgang til suv-altinn-data-t fra test-teamet, som fungerer på Dapla, men siden test-teamet ikke kan overføre filer til bakken så er det ikke mulig å teste innlasting til ISEE med skjemaer fra tt02 uten å gå via prod bøtten. Derfor trenger du at det åpnes for dette.\n\n\n\nFlytdiagram for prosessen\n\n\n\nFlytdiagram for prosessen\n\n\nFor informasjon om hvordan mappestrukturen i bøttene dine skal være, se navnestandarden."
  },
  {
    "objectID": "blog/posts/2025-04-07-altinn3-til-isee/index.html#transfer-service-fra-datamottak-til-din-kildebøtte.",
    "href": "blog/posts/2025-04-07-altinn3-til-isee/index.html#transfer-service-fra-datamottak-til-din-kildebøtte.",
    "title": "Fra Altinn 3 til ISEE",
    "section": "Transfer service fra datamottak til din kildebøtte.",
    "text": "Transfer service fra datamottak til din kildebøtte.\nDet første du skal gjøre er å sette opp transfer service fra datamottaket til din kildebøtte. Du kommer til å være godt kjent med å sette opp transfer service når vi er ferdige med dette, og det er en del detaljer som må stemme. Så det er bare å holde hodet kaldt, dobbeltsjekke at alt stemmer, prøve seg frem og be om hjelp hvis du står fast.\nSiden det første steget involverer kildebøtta så trenger du å være data-admin på teamet ditt (du kan sjekke rollene dine i dapla ctrl) og søke om midlertidig tilgang via just in time (JIT) tjenesten.\nFor å komme seg til vinduet hvor du kan sette opp en transfer service går du først inn på denne lenken: https://console.cloud.google.com/\nDeretter skal du klikke på en knapp øverst i skjermbildet på venstre side, til høyre for GoogleCloud logoen og få opp menyen hvor du velger hvilket prosjekt du skal jobbe med\nHer skal du velge ditt team, du kan se teamnavnet i dapla-ctrl. Du skal inn på prosjektet som har ditt-teamnavn-kilde-p i navnet sitt. Hvis du ikke ser prosjektet ditt, sjekk at du har valgt ssb.no som organisasjon under “Select a resource”.\n\n\n\nI dette tilfellet er det primaer-j-skjema-kilde-p som er riktig valg.\n\n\nNår du er inne på prosjektet ditt kan du klikke i søkefeltet på toppen av siden, søke på “Storage transfer” og velge den.\nNå skal vi inn på “Transfer jobs” og klikke på “CREATE TRANSFER JOB”.\nNoter deg hvordan du fant denne menyen, for litt senere i prosessen skal vi inn på et annet prosjekt og sette opp flere transfer jobs.\nDatamottaket ligger i team SUV sitt dapla team som heter “suv-altinn-data-p”, og du kan finne skjemaene i bøtter med navnestrukturen ssb-suv-altinn-raXXXX-01-prod. Merk at 01 viser til versjonen av skjema, så hvis du får en ny versjon av skjemaet ditt vil den ha et annet versjonsnummer.\nHagebruksundersøkelsen har RA-nummeret 0571, så datamottaket sitt bøttenavn vil i dette tilfellet være ssb-suv-altinn-ra0571-01-prod.\nMappestrukturen i datamottaket er slik at det først er en mappe for året skjemaet ble mottatt, deretter for måneden, deretter for dagen, og her finner du hver enkelt innsending. I transfer job henviser vi bare til den øverste mappen, den fører automatisk med seg alt som ligger lenger “nede” i mappestrukturen.\nSe på bildene for å se hvordan filstier og innstillingene for jobben skal se ut.\nDet er lurt å skrive tydelig i “Description” feltet før du trykker “CREATE” hvilken undersøkelse det gjelder og hva transfer jobben gjør. Det sparer andre i teamet (og deg selv) forvirring senere.\nEt lite tips er å la denne jobben kjøre ofte, siden det vil gjøre at skjemaene behandles fortløpende i kildomaten istedenfor å komme i store overføringer. Andre overføringer trenger ikke kjøre mer enn en gang om dagen.\nI denne jobben skal ikke filer slettes.\n\nGet startedChoose a sourceChoose a destinationChoose when to run jobChoose settings"
  },
  {
    "objectID": "blog/posts/2025-04-07-altinn3-til-isee/index.html#kildomaten",
    "href": "blog/posts/2025-04-07-altinn3-til-isee/index.html#kildomaten",
    "title": "Fra Altinn 3 til ISEE",
    "section": "Kildomaten",
    "text": "Kildomaten\nNå skal vi gå løs på den mest tekniske delen av oppsettet. Her blir det nok mye prøving og feiling for å få alt til å fungere, så ikke mist motet.\nKildomaten er en tjeneste som automatiserer bearbeiding av data som kommer inn i kildebøtta. Etter at den er satt opp så vil den automatisk bearbeide filer som havner i mappen den er stilt inn til å følge med på. Den vil enkelt forklart bearbeide hvert enkelt skjema hver for seg, og sende videre til produkt bøtta.\nFor mer informasjon om kildomaten kan du se veiledning for kildomaten her.\nI første omgang holder vi det enkelt og setter opp rammen som vi senere skal fylle ut.\nDe to filene nedenfor skal opprettes og plasseres i mappen dapla-team-iac/automation/source-data/dapla-team-prod/hagebruk/\n\n\nconfig.yaml\n\nfolder_prefix: hagebruk/altinn\nmemory_size: 1\n\n\n\nprocess_source_data.py\n\nimport dapla as dp\nfrom altinn import isee_transform, create_isee_filename\n\ntags = [\"SkjemaData\", \"Kontakt\", \"Brukeropplevelse\"]\n\nmapping = {\n    \"Altinn3 navn\": \"ISEE navn\"\n}\n\ndef main(source_file):\n    if source_file.endswith(\".xml\"):\n        fil = isee_transform(source_file, mapping, tag_list=tags) # transformerer XML'ene fra kildedatabøtta til pandas dataframe med riktig ISEE-feltnavn\n        fil[\"FELTNAVN\"] = fil[\"FELTNAVN\"].str[:25].str.upper()\n        if \"altinn/test/\" in source_file:\n            dp.write_pandas(df = fil, gcs_path = f\"ssb-primaer-j-skjema-data-produkt-prod/hagebruk/temp/test/altinn/{create_isee_filename(source_file)}\", file_format=\"csv\", index=False, sep=\";\")\n        elif \"altinn/prod/\" in source_file:\n            dp.write_pandas(df = fil, gcs_path = f\"ssb-primaer-j-skjema-data-produkt-prod/hagebruk/inndata/altinn/{create_isee_filename(source_file)}\", file_format=\"csv\", index=False, sep=\";\")\n        else: \n            raise ValueError (f'source_filen sitt navn inneholder verken altinn/test/ eller altinn/prod/: {source_file}')\n\nFilstiene i process_source_data.py må du endre nå, mappingen kan du utsette til du er ferdig med å sette opp resten av overføringene.\nDet er vanskelig å sjekke om mappingen din stemmer uten å ha forsøkt innlasting til ISEE, så det kan være greit å ordne resten av løpet først.\nMapping brukes for å omkode nytt variabelnavn fra Altinn 3 til ISEE feltnavn og er nødvendig for at dataene dine skal bli synlige i ISEE.\nDet er viktig at filen som lages av kildomaten ikke har feltnavn som er lengre enn 25 tegn, er en csv fil og at det er semikolon som er skilletegnet. Dette er tatt høyde for i malen over."
  },
  {
    "objectID": "blog/posts/2025-04-07-altinn3-til-isee/index.html#transfer-service-fra-din-produkt-bøtte-til-frasky-bøtten",
    "href": "blog/posts/2025-04-07-altinn3-til-isee/index.html#transfer-service-fra-din-produkt-bøtte-til-frasky-bøtten",
    "title": "Fra Altinn 3 til ISEE",
    "section": "Transfer service fra din produkt bøtte til frasky bøtten",
    "text": "Transfer service fra din produkt bøtte til frasky bøtten\nHer skal vi starte med å flytte filene som kildomaten vår lager til bakken. Før filene kan sendes til bakken, så skal de innom frasky-bøtta. Det betyr at flyttingen ned til bakken involverer to transfer jobs.\nDisse jobbene trenger ikke kjøre mer enn én gang om dagen, men pass på at jobben som flytter fra produkt til frasky kjører før jobben som flytter filer til bakken. Hva du kan sette på tidspunkt er vist på “Choose when to run job” bildene for hver jobb.\nFørst skal du skifte hvilket prosjekt du jobber i. Nå skal vi være i teamet som heter ditt-teamnavn-p, altså det som ikke har kilde i navnet sitt. I dette eksempelet er riktig team primaer-j-skjema-p.\nDeretter skal transfer jobs settes opp på samme måte, se på skjermbildene for å se hvordan det skal se ut.\nOBS! Det er viktig at filene slettes fra kilden etter flytting. Hvis det ikke slettes, så overføres samme skjema flere ganger og skaper enormt mange dubletter i ISEE.\n\nGet startedChoose a sourceChoose a destinationChoose when to run jobChoose settings"
  },
  {
    "objectID": "blog/posts/2025-04-07-altinn3-til-isee/index.html#transfer-service-fra-din-frasky-bøtte-til-bakken",
    "href": "blog/posts/2025-04-07-altinn3-til-isee/index.html#transfer-service-fra-din-frasky-bøtte-til-bakken",
    "title": "Fra Altinn 3 til ISEE",
    "section": "Transfer service fra din frasky bøtte til bakken",
    "text": "Transfer service fra din frasky bøtte til bakken\nI dette steget skal du fortsatt være i “ditt-teamnavn-p” prosjektet.\nNå flytter vi filene ned til bakken. Måten mottaket på bakken er lagt opp er at det finnes en mappe på linux for ditt dapla team under området cloud_sync.\nDet vil ha filstien cloud_sync/team-navn/…\nOBS! Det er viktig at filene slettes fra kilden etter flytting. Hvis det ikke slettes, så overføres samme skjema flere ganger og skaper enormt mange dubletter i ISEE.\nMåten du skal sette opp transfer service kan du se her:\n\nGet startedChoose a sourceChoose a destinationChoose when to run jobChoose settings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNår du har satt opp transfer service så skal du ned på bakken og opprette noen mapper på linux. Samme sted som stammene ligger så finnes også et område som heter cloud_sync, det er dit vi skal nå.\nDitt dapla team vil ha en egen mappe i cloud_sync som du skal finne.\n\n\nPå bakken er det ett mottak for kildedata (mappen som heter kilde) og ett for annen data (mappen som heter standard). Det er “standard” vi skal bruke. Det vil si at vi skal forholde oss til cloud_sync/ditt-teamnavn/standard/frasky.\nI den mappen skal du opprette mappen “altinn” og den skal inneholde to undermapper, “prod” og “test”.\n\n\n\n\n\n\nMappestruktur på cloud_sync"
  },
  {
    "objectID": "blog/posts/2025-04-07-altinn3-til-isee/index.html#opprette-mappestruktur-på-stammen",
    "href": "blog/posts/2025-04-07-altinn3-til-isee/index.html#opprette-mappestruktur-på-stammen",
    "title": "Fra Altinn 3 til ISEE",
    "section": "Opprette mappestruktur på stammen",
    "text": "Opprette mappestruktur på stammen\n\n\nFor at innlasting til ISEE skal fungere så trengs det noen spesifikke mapper på stammen.\nDette er nødvendig for at den automatiske lastejobben skal fungere, og så lenge du følger dette oppsettet så burde selve innlastingen gå fint.\nI skjermbildet vises det oppsett som inkluderer mappene som trengs for test-løpet. Dette løpet må settes opp separat fra prod-løpet.\nSe skjermbildet med overordnet flyt øverst på siden for å se forskjellen på filstiene.\n\n\n\n\n\n\nMappestruktur på stammen"
  },
  {
    "objectID": "blog/posts/2025-04-07-altinn3-til-isee/index.html#moveit-og-lasting-til-isee",
    "href": "blog/posts/2025-04-07-altinn3-til-isee/index.html#moveit-og-lasting-til-isee",
    "title": "Fra Altinn 3 til ISEE",
    "section": "MoveIT og lasting til ISEE",
    "text": "MoveIT og lasting til ISEE\nNå som vi har satt opp mappestrukturene som trengs så har vi en positiv overraskelse, det neste steget er enkelt for deg som statistikkansvarlig.\nFørst skal du opprette skjemaet i SFU og ISEE. Det skal hete RA-XXXXA3, hvor XXXX er ditt RA-nummer.\nDeretter skal du sende en mail til kundeservice. I den skal du: - si at du trenger en MoveIt overføring fra cloudsync/standard/undersøkelse/altinn/prod/ og til din stamme sin wk24/datafangst/gSdv/ mappe. - at du trenger opplasting fra datafangst/gSdv mappen din til ISEE.\nDu kan ta utgangspunkt i malen nedenfor. Hvis du ikke setter opp test-løpet så kan du fjerne de bitene.\n\nHei!\nTrenger to moveit jobber og innlasting til ISEE fra stammen.\nJeg skal overføre RA-XXXXA3 til ISEE, delregisternummer 123456\nMoveIt som flytter fra: /ssb/cloud_sync/primaer-j-skjema/standard/frasky/hagebruk/test/altinn/ Til: /ssb/stamme02/jordbruk/sbhagebruk/test/wk24/datafangst/gSdv\nTest jobben burde kjøre 20 minutter over hver time så det rekker å bli med på lastingen til DB1T.\nMoveIt som flytter fra: /ssb/cloud_sync/primaer-j-skjema/standard/frasky/hagebruk/prod/altinn/ Til: /ssb/stamme02/jordbruk/sbhagebruk/wk24/datafangst/gSdv\nBegge MoveIt jobbene må slette filer fra cloud_sync når det er flyttet.\nTrenger også at det opprettes lastejobb fra wk24 og test/wk24 til DB1P og DB1T.\n\nOBS! Det er viktig at filene slettes fra kilden etter flytting. Hvis det ikke slettes, så overføres samme skjema flere ganger og skaper enormt mange dubletter i ISEE.\nNår disse jobbene er satt opp så får du data inn i ISEE!"
  },
  {
    "objectID": "blog/posts/2025-04-07-altinn3-til-isee/index.html#prosessen-er-satt-opp-hva-nå",
    "href": "blog/posts/2025-04-07-altinn3-til-isee/index.html#prosessen-er-satt-opp-hva-nå",
    "title": "Fra Altinn 3 til ISEE",
    "section": "Prosessen er satt opp, hva nå?",
    "text": "Prosessen er satt opp, hva nå?\nDu burde nå få data lastet inn i tabellene ISEE er bygd på, som er et godt stykke men likevel er det en del jobb igjen.\nDet er mange detaljer som skal stemme for at dataene som lastes inn skal dukke opp riktig i ISEE. Hvis mappingen ikke stemmer så vil ikke dataene dine bli synlige i ISEE, selv om de er lastet inn i systemet.\nDu må sjekke at kodelister eksisterer for variablene dine, at disse matcher variabelverdiene som kommer fra Altinn 3 og at eventuelle endringer i Altinn 3 skjemaet er hensyntatt i skjemavisningen i ISEE. Ved behov kan du opprette nye kodelister som passer Altinn 3 dataene dine i klass-forvaltning.\nVær oppmerksom på at ofte vil noen kodelister som benyttes i Altinn 3 skjemaet skille seg fra det som ble brukt i altinn2, som vil kreve noen endringer i skjemabyggeren.\nDet er mulig å abonnere på rapporter fra ISEE hvis du går inn på Administrasjon -&gt; Oversikt datafangst -&gt; Datafangstrapporter -&gt; Legg inn epostadressen din.\nDu kan også få varsler om noe skulle gå galt underveis i lastingen til ISEE.\nHer er det noen ressurser som kan være til hjelp med å ferdigstille opplegget ditt:\nDokumentasjon for ssb-altinn-python pakken:\n\nhttps://statistics-norway.atlassian.net/wiki/spaces/altinnpython/overview\nhttps://statisticsnorway.github.io/ssb-altinn-python/\n\nISEE dokumentasjon:\n\nhttps://statistics-norway.atlassian.net/wiki/spaces/ISEE/pages/3958932070/Brukerdokumentasjon"
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html",
    "title": "Fra arkiv til parquet",
    "section": "",
    "text": "I arkivet til SSB ligger data lagret som posisjonerte flatfiler, også kalt fastbredde-fil eller fixed width file på engelsk. I Datadok ligger det spesifisert hvordan du leser inn disse filene fra dat eller txt i arkivet til sas7bdat-formatet, men ikke hvordan man konverterer til Parquet-formatet. I denne artikkelen deler jeg hvordan jeg gikk frem for å konvertere arkivfiler til Parquet."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#hva-er-en-fastbredde-fil",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#hva-er-en-fastbredde-fil",
    "title": "Fra arkiv til parquet",
    "section": "Hva er en fastbredde-fil?",
    "text": "Hva er en fastbredde-fil?\nEn fastbredde-fil er en fil der hver rad har en fast lengde, og hver kolonne har en fast posisjon. Det er ingen komma eller andre tegn som skiller kolonnene, slik som i en CSV-fil. En fastbredde-fil er en ren tekstfil, dvs. at du kan åpne den opp i teksteditor og kikke på innholdet direkte.\nUnder er et eksempel hvor samme data er lagret både på CSV-formatet og som fastbredde-fil:\n\n\n\n\ncsv\n\n012345;;Ola Nordmann;\n345678;Kvinne;Kari Nordmann;\n\n\n\n\n\n\nfastbredde-fil\n\n012345      Ola Nordmann \n345678KvinneKari Nordmann\n\n\n\nI csv-filen over til venstre ser vi at hver kolonne er separert med et semikolon, og at hver rad er separert med et linjeskift. I fastbredde-filen til høyre ser vi at hver kolonne har en fast lengde, den tomme kjønnsvariabelen på rad 1 fylles med spaces, hver rad har dermed den samme lengden i antall tegn. I tillegg er det et ekstra mellomrom etter Ola Nordmann ift. Kari Nordmann. Dette er fordi Ola Nordmann er 12 tegn lang, mens Kari Nordmann er 13 tegn lang."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lese-med-pandas",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lese-med-pandas",
    "title": "Fra arkiv til parquet",
    "section": "Lese med Pandas",
    "text": "Lese med Pandas\nVi kan bruke pandas-funksjonen read_fwf() for å lese inn en fastbredde-fil. Denne funksjonen tar inn en filsti, og en liste med bredder for hver kolonne. I tillegg kan vi spesifisere navn på kolonnene, og hvilken datatype kolonnene skal ha og hvordan missing-verdier skal representeres.\nVi er helt avhengig av å vite bredden på hver kolonne for å kunne lese inn en fastbredde-fil. Dette kan vi finne ut ved å åpne filen i en teksteditor og telle/gjette antall tegn i hver kolonne. Alternativt kan vi bruke innlesingsskriptet for SAS som finnes i Datadok, siden breddene er spesifisert der. Under er et ekspempel på hvordan vi kan lese inn en fastbredde-fil fra forrige avsnitt1:\n\nimport pandas as pd\nfrom io import StringIO  # Nødvendig siden vi sender en streng, ikke en filsti til .read_fwf\ninstring = \"112345      Ola Nordmann \\n345678KvinneKari Nordmann\\n\"\ndf = pd.read_fwf(StringIO(instring),\n                 names=['pers_id', 'kjonn', 'navn'],  # Navngi kolonner\n                 dtype='object',  # Alle kolonnene settes til \"object\"\n                 na_values=['.', ' .'],  # Hvilke karakterer bruker SAS for tom verdi?\n                 widths=[6, 6, 13])  # Tell/regn ut dissa sjøl\ndf\n\n\n\n\n\n\n\n\npers_id\nkjonn\nnavn\n\n\n\n\n0\n112345\nNaN\nOla Nordmann\n\n\n1\n345678\nKvinne\nKari Nordmann\n\n\n\n\n\n\n\nKoden over returnerer en Pandas Dataframe i minnet. Den kan vi lett lagre til Parquet-formatet. Men innlesingen måtte vi spesifisere en masse detaljer manuelt. Hvis vi skal lese inn mange filer med ulik struktur, så er ikke denne fremgangsmåten skalerbar. Dette er en fremgangsmåte for å lese inn noen få filer."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#datadok",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#datadok",
    "title": "Fra arkiv til parquet",
    "section": "Datadok",
    "text": "Datadok\nSom nevnt over så finnes det et innlesingsskript for SAS i Datadok. Dette skriptet kan vi bruke til å lese inn en fastbredde-fil i Python. Vi kan også bruke det til å finne breddene på hver kolonne. Et slik skript har denne formen:\n\n\ninnlesingsskript.sas\n\nDATA sas_data;\n   INFILE '/ssb/bruker/felles/flatfileksempel_dapla_manual_blogg.dat' MISSOVER PAD LRECL=36;\n   INPUT\n      @1 pers_id 6.0\n      @7 kjonn $CHAR6.0\n      @13 navn $CHAR13.0\n   ;\nRUN;\n\nVi kunne lest av informasjonen her og omsatt innholdet til argumentene read_fwf() trenger. Men fortsatt innebærer dette potensielt en del manuelt arbeid."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lese-med-saspy",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lese-med-saspy",
    "title": "Fra arkiv til parquet",
    "section": "Lese med saspy",
    "text": "Lese med saspy\nEn annen tilnærming enn å bruke .read_fwf fra Pandas er å bruke biblioteket saspy. Dette biblioteket lar oss kjøre SAS-kode fra Python, på SAS-serverene i prodsonen, og få Dataframes tilbake. Vi kan bruke det til å kjøre sas-skript hentet fra Datadok, konvertere til en pandas dataframe, og til slutt skrive til Parquet. I det følgende antar vi at du jobber i Jupyterlab i prodsonen (sl-jupyter-p), og at du har lagret innlesingsskriptet i en variabel, slik som vist under:\n\n\npython\n\nscript = \"\"\"\nDATA sas_data;\n   INFILE '/ssb/bruker/felles/flatfileksempel_dapla_manual_blogg.dat' MISSOVER PAD LRECL=36;\n   INPUT\n      @1 pers_id 6.0\n      @7 kjonn $CHAR6.0\n      @13 navn $CHAR13.0\n   ;\nRUN;\n\"\"\"\n\nLa oss deretter kjøre følgende kode fra Jupyterlab:\n\n\npython\n\nfrom fagfunksjoner import saspy_session\n\n# Kobler til sas-serverne\nsas = saspy_session()\n\n# Vi bruker tilkoblingen til å sende inn Datadok-skriptet\nresult = sas.submit(script)\n\n# Lagre sas-loggen i en variabel\nlog = result[\"LOG\"]\n\n# Ber om å få dataframe tilbake\ndf_frasas = sas.sd2df(\"sas_data\", \"work\")\n\n# Lukker koblingen til sas-serverne\nsas._endsas()\n\n# Printer ut datasettet\ndf_frasas\n\nI koden over har vi brukt en pakke som heter ssb-fagfunksjoner for å opprette koblingen til sas-serveren. Pakken inneholder et overbygg over saspy, og koden over forutsetter at du har lagret passordet ditt på en spesiell måte2.\n\nDatatyper\nVi har nå en pandas dataframe med datatyper påført, men disse er basert på den lave mengden datatyper i SAS. Ofte bør det ryddes i datatyper før man skriver til Parquet. Spesielt bør du tenke på følgende:\n\nCharacter mappes gjerne til object i pandas, ikke den strengere varianten string eller den mer spesifikke string[pyarrow].\nNumeric mappes stort sett til float64 i pandas, vi får som regel ikke heltall direkte Int64 uten videre behandling.\n\nDu kan la Pandas gjøre ett nytt forsøk på å gjette datatyper ved å kjøre følgende kode:\n\n\npython\n\ndf_pd_dtypes = df_frasas.convert_dtypes()\ndf_pd_dtypes.dtypes\n\nOm du vil teste min selvskrevne funksjon for å gjette på datatyper så ligger den i fellesfunksjons-pakken:\n\n\npython\n\nfrom fagfunksjoner import auto_dtype\ndf_auto = auto_dtype(df_frasas)\ndf_auto.dtypes\n\nSjekk gjerne ut parameteret cardinality_threshold på auto_dtype, om du er interessert i å automatisk sette categorical dtypes.\n\n\nSkalering\nHvis du har mange arkivfiler, med mange forskjellige innlesingsskript, så kan du lagre alle skriptene i en mappe, og så hente innholdet programmatisk. Her er koden for én slik “henting”.\n\n\npython\n\nsas_script_path = \"/ssb/bruker/felles/flatfileksempel_dapla_manual_blogg.sas\"\nwith open(sas_script_path, \"r\", encoding=\"latin1\") as sas_script:\n    script = sas_script.read().strip()\n    script = \"DATA \" + script.split(\"DATA \")[1] # Forkort ned scriptet til det vi trenger\nprint(script)\n\nHer henter jeg inn et innlesingsskript fra Datadok som jeg har lagret som en tekstfil i en mappe på linux-serveren i prodsonen. Deretter gjør jeg den om til et streng-objekt i minnet som kan sendes til saspy-koden som er vist over. Dermed er det bare å finne en logikk som gjør at du vet hvilket innlesingskript som skal brukes til hvilke arkivfiler (siste valide datadok-script før datafil oppstod feks), og du kan jobbe veldig effektivt med konvertering. Når alt er konvertert kan du f.eks. kjøre et script som validerer datatypene på tvers av alle årganger og filnavn."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lagre-dataframen-til-parquet",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lagre-dataframen-til-parquet",
    "title": "Fra arkiv til parquet",
    "section": "Lagre dataframen til parquet",
    "text": "Lagre dataframen til parquet\nNå er det veldig lett å skrive filen til Parquet-formatet.\n\n\npython\n\ndf_auto.to_parquet(\n    \"/ssb/bruker/felles/flatfileksempel_dapla_manual_blogg.parquet\"\n    )"
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#nudb",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#nudb",
    "title": "Fra arkiv til parquet",
    "section": "NUDB",
    "text": "NUDB\nI omleggingen av NUDB (Nasjonal utdanningsdatabase), måtte vi konvertere hele arkivet vårt på 750+ dat-filer.\nDet var ønskelig å slippe å lagre til sas7bdat i mellom, for å slippe mye dataduplikasjon og arbeidsprosesser. Målet vårt var pseudonymiserte parquetfiler i sky.\nI stor grad kunne dette arbeidet automatiseres (bortsett fra å lagre ut innlastingsscript fra gamle datadok). Funksjonene jeg utviklet for dette, ligger stort sett i denne filen:\ngithub.com/utd-nudb/prodsone/konverter_arkiv/archive.py"
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#footnotes",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#footnotes",
    "title": "Fra arkiv til parquet",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\n/n i strengen 112345      Ola Nordmann \\n345678KvinneKari Nordmann\\n betyr linjeskift.↩︎\nHvis du ønsker kan du bruker ssb-fagfunksjoner til å lagre passordet ditt i kryptert form. Da kan du lagre passordet i en fil på din egen maskin, og slipper å skrive det inn hver gang du skal koble til SAS. Funksjonen heter fagfunksjoner.prodsone.saspy_ssb.set_password().↩︎"
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html",
    "title": "Fra Fame til Python",
    "section": "",
    "text": "Mange i SSB har data lagret i Fame som de ønsker å bearbeide med Python og R. Dette er spesielt relevant når man skal flytte statistikkproduksjon til Dapla. fython er en Python-pakke som gjør dette på en enkel måte for deg. Den lar deg eksportere data fra Fame med en enkel funksjon, og kan returnere dataene som enten CSV eller Pandas DataFrame.\nPakken finner du på GitHub."
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#installasjon",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#installasjon",
    "title": "Fra Fame til Python",
    "section": "Installasjon",
    "text": "Installasjon\nPakken er avhengig av at Fame er installert miljøet der den benyttes. Siden den er installert på sl-fame-1.ssb.no1 så vil de færreste har behov for å installere den selv.\nSkulle du likevel ønske å installere pakken selv kan det gjøres med Poetry på følgende måte:\n\n\nterminal\n\npoetry add git+https://github.com/statisticsnorway/ssb-fame-to-python.git"
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#bruk-av-funksjonene",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#bruk-av-funksjonene",
    "title": "Fra Fame til Python",
    "section": "Bruk av funksjonene",
    "text": "Bruk av funksjonene\nfython har to funksjoner: fame_to_csv og fame_to_df. Begge disse funksjonene tar inn de samme argumentene og de er listet opp i Tabell 1.\n\n\n\nTabell 1: Forklaring av argumentene i funksjonene til fython\n\n\n\n\n\n\n\n\n\n\n\nArgument\nForklaring\nfame_to_csv()\nfame_to_pandas()\n\n\n\n\ndatabases\nList of Fame databases to access (with full path).\n✓\n✓\n\n\nfrequency\nFrequency of the data (‘a’, ‘q’, ‘m’).\n✓\n✓\n\n\ndate_from\nStart date for the data in Fame syntax (e.g., ‘2023:1’ for quarterly, ‘2023’ for annual).\n✓\n✓\n\n\ndate_to\nEnd date for the data in Fame syntax (e.g., ‘2023:1’ for quarterly, ‘2023’ for annual).\n✓\n✓\n\n\nsearch_string\nQuery string for fetching specific data. The search is not case sensitive, and “^” and “?” are wildcards (for exactly one and any number of characters, respectively)\n✓\n✓\n\n\ndecimals\nNumber of decimal places in the fetched data (default is 10).\n✓\n✓\n\n\npath\nPath to write the csv-file.\n✓\n\n\n\n\n\n\n\nLa se på noen eksempler.\n\nEksempler\nDersom vi ønsker å hente alt i database1.db og database2.db fra januar 2012 til desember 2022, og få det returnert i en DataFrame, kan vi skrive følgende kode:\n\n\npython\n\nfrom fython import fame_to_pandas\n\ndf = fame_to_pandas(\n  ['sti/til/database1.db', 'sti/til/database2.db', 'm', '2012:1', '2022:12', '?']\n  )\n\nDersom vi i stedet ønsker å hente alle serier som begynner på abc, slutter på d etterfulgt av ett vilkårlig tegn, kan vi skrive følgende kode:\n\n\npython\n\nfrom fython import fame_to_pandas\n\ndf = fame_to_pandas(\n  ['sti/til/database1.db', 'sti/til/database2.db', 'm', '2012:1', '2022:12', 'abc?d^']\n  )\n\n? og ^ er altså jokertegn/wildcards som representerer henholdvis et vilkårlig antall tegn og nøyaktig ett tegn.\nDersom vi i stedet vil lagre dataene til en csv-fil kan vi skrive\n\n\npython\n\nfrom fython import fame_to_csv\n\nfame_to_csv(\n  ['sti/til/database1.db', 'sti/til/database2.db', 'm', '2012:1', '2022:12', 'abc?d^', 'sti/til/csv-fil.csv']\n  )\n\n\n\n\n\n\n\nWarning\n\n\n\nDet er viktig å påpeke at enhver serie kun skrives én gang, og da fra den første databasen den finnes i (kronologisk iht. til listen med databaser)."
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#kjøringer-på-serveren",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#kjøringer-på-serveren",
    "title": "Fra Fame til Python",
    "section": "Kjøringer på serveren",
    "text": "Kjøringer på serveren\nNår du skal bruke fython så må du ta hensyn til hvilken server Fame er installert på, og hvilken server du har tenkt til å jobbe på. Fame er som sagt installert på sl-fame-1.ssb.no, mens Jupyterlab er installert på sl-jupyter-p.ssb.no. Dvs. at hvis du ønsker å bruke fython i en notebook i Jupyterlab, så må du bruke ssh til å koble deg til sl-fame-1.ssb.no, og så kjøre koden derfra. Koden din kan skrive en fil til ønsket stammeområdet, som du igjen kan lese inn direkte i Jupyterlab."
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#automatiserte-uttrekk",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#automatiserte-uttrekk",
    "title": "Fra Fame til Python",
    "section": "Automatiserte uttrekk",
    "text": "Automatiserte uttrekk\nHvis man ønsker at utrekk fra Fame skal skje automatisk på gitte tidspunkter eller intervaller, så kan man ta kontakt med Kundeservice. Fordelen med dette er at man ikke trenger å bruke ssh slik som beskrevet over. Man kan lese inn direkte fra stammeområdet."
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#overføre-data-til-dapla",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#overføre-data-til-dapla",
    "title": "Fra Fame til Python",
    "section": "Overføre data til Dapla",
    "text": "Overføre data til Dapla\nHvis man ønsker å overføre data fra Fame til Dapla, så kan dette settes opp som en MoveIt-operasjon. For å sette opp en MoveIt-jobb må ma kontakte Kundeservice. Overføring til Dapla forutsetter at man har et Dapla-team, og at man setter opp en synkroniseringjobb med Transfer Service."
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#footnotes",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#footnotes",
    "title": "Fra Fame til Python",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPakken er installert i Python-versjon 3.6 på serveren. Du kan åpne et Python-shell i terminalen på sl-fame-1.ssb.no ved å skrive: python3.6.↩︎"
  },
  {
    "objectID": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html",
    "href": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html",
    "title": "Ideen bak bloggen",
    "section": "",
    "text": "SSB-ere løser hele tiden problemer på nye måter som andre gjerne skulle nyttiggjort seg av. Spesielt når vi gjør en så stor overgang i arbeidsform som overgangen til en ny plattform (Dapla), og vi samtidig skifter mange verktøyene vi har i verktøykassen vår. Av den grunn har vi opprettet denne bloggen. Her vil vi skrive om hvordan vi løser problemer, hvilke verktøy vi bruker og hvordan vi bruker dem. Vi vil også skrive om hvordan vi jobber med å utvikle nye verktøy og hvordan vi jobber med å utvikle Dapla.\nMålsetningen med denne bloggen er at alle i SSB som ønsker å dele noe med andre kan skrive en artikkel og dele i bloggen. Mens Byrånettet er kanal for å dele informasjon med alle i SSB, og Viva Engage en kanal for å si det du tenker uten særlig noen formell struktur, er denne bloggen en kanal for å dele informasjon med andre som jobber med data og teknologi i SSB.\nFordelen med bloggen er at den er tilpasset hvordan statistikkere, forskere og IT-utviklere jobber til daglig. Artiklene kan skrives samme sted som man utvikler kode, og man inkludere output fra kodekjøringer i artikler."
  },
  {
    "objectID": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html#hvordan-skrive-en-artikkel",
    "href": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html#hvordan-skrive-en-artikkel",
    "title": "Ideen bak bloggen",
    "section": "Hvordan skrive en artikkel?",
    "text": "Hvordan skrive en artikkel?\nBloggen er generert med Quarto. Quarto er et rammeverk for å skrive artikler i markdown. Det er enkelt å komme i gang med Quarto, og det er enkelt å skrive artikler i Quarto.\nFor å skrive en artikkel gjør du følgende:\n\nSkriv artikkelen som en markdown-fil (.qmd-fil) eller en notebook (.ipynb-fil).\nKlon dapla-manual-internal repoet:\ngit clone https://github.com/statisticsnorway/dapla-manual-internal.git\nOpprett en mappe for artikkelen din i mappen ./dapla-manual-internal/blog/posts/. Gi mappen et navn som beskriver artikkelen din.\nInne mappen legger du din .qmd- eller .ipynb-fil. Eventuelle bilder i artikkelen legges også i samme mappe.\nOpprette en pull request på repoet og noen vil se over artikkelen din og publisere den."
  },
  {
    "objectID": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html#metadata-om-artikkelen",
    "href": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html#metadata-om-artikkelen",
    "title": "Ideen bak bloggen",
    "section": "Metadata om artikkelen",
    "text": "Metadata om artikkelen\nNår du skriver artikkelen så må du starte dokumentet med følgende metadata:\n\n\nindex.qmd\n\n---\ntitle: Ideen bak bloggen\nsubtitle: Hvorfor vi har opprettet denne bloggen? \ncategories:\n  - Quarto\nauthor:\n  - name: Øyvind Bruer-Skarsbø\n    affiliation: \n      - name: Seksjon for dataplattform (724)\n        email: obr@ssb.no\ndate: \"01/11/2024\"\ndate-modified: \"01/11/2024\"\nimage: ../../../images/dapla-long.png\nimage-alt: \"Bilde av Fame-logoen\"\ndraft: false\n---\n\nHusk å fylle ut alle feltene slik at det blir riktig informasjon for din artikkel. Skriver du en ipynb-fil så må metadataene ligge i en celle av typen raw.\nØnsker du å komme fort igang så kan se hvordan denne artikkelen ble skrevet."
  },
  {
    "objectID": "blog/posts/2024-12-16-data-validering-pandera/index.html",
    "href": "blog/posts/2024-12-16-data-validering-pandera/index.html",
    "title": "Intro til Pandera",
    "section": "",
    "text": "Pandera er en python pakke og rammeverk for testing av data - altså datavalidering.\nBegrepet testing kan føre til misforståelser mellom statistikkere og utviklere: en statistikker vil ofte tenke på testing av data, og utvikler på testing av kode. Sistnevnte omtales som enhetstesting.\nDet finnes flere rammeverk for testing av kode og datavalidering. Når det kommer til Python bruker vi i SSB som oftest Pytest pakken for testing av kode, og Pandera eller Pydantic pakkene for datavalidering. Alle disse pakkene står oppført på godkjentlista i SSB.\nPandera eller Pydantic? Hvem av dem som bør benyttes avhenger mest av strukturen på dataene din. Dersom dataene er semi-strukturert (ofte filformater som json og xml) så vil fort Pydantic være mest aktuell, mens er dataene strukturerte (som en dataframe, eller filformat som csv) så vil Pandera være ett mer naturlig valg. Her vil det gis en intro til Pandera. Vel og merke vil innholdet her dreie seg om grunnleggende bruk, samt forskjellige tips og triks i hvordan det kan brukes, og muligens en bonus til slutt. Mer avanserte temaer, som f.eks. hypotesetesting, er ikke med her.\nMen hvorfor Pandera? Og hvorfor validere data? Siste er enkelt å besvare og ligger godt integrert i SSBs samfunnsansvar: Vi skal ha god kvalitet i all statistikk, forskning og analyse.\nI den moderniseringsprosessen SSB er i, overgang til Dapla, er det naturlig at dette integreres i kodene våre. Det er en anbefaling fra KVAKK anbefaler også å kontrollere data for hvert trinn. Da er datavalideringspakker som Pandera høyst aktuell. I tillegg er det en annen anbefaling fra KVAKK, og en ADR vedtatt i SSB, om at kildekode skal være offentlig tilgjengelig. En eller annen gang skal altså produksjonskoden vår bli offentlig tilgjengelig. Dette er kanskje mine personlige meninger rundt det, men jeg vil tro at det vil foreligge en stor forventning der ute om at SSB validerer data i kode. Selv om Pandera er relativt nytt støtter den de aller mest brukte dataframe-rammeverkene som er i bruk i SSB, slik som Pandas, Polars, og PySpark."
  },
  {
    "objectID": "blog/posts/2024-12-16-data-validering-pandera/index.html#import-og-testdata",
    "href": "blog/posts/2024-12-16-data-validering-pandera/index.html#import-og-testdata",
    "title": "Intro til Pandera",
    "section": "Import og testdata",
    "text": "Import og testdata\nFørst importerer vi noen biblioteker som vi skal benytte. For å benytte Pandera pakken må det lastes inn til ett virtuelt miljø, som vi i SSB benytter ssb-project for;\n\n\nterminal\n\npoetry add pandera\n\nVersjonen av Pandera som benyttes i introduksjonen her er 0.20.4. Følgende pakker får jeg importert deretter;\n\nimport uuid\nfrom typing import Dict\nimport pandas as pd\nimport numpy as np\nimport pandera as pan\nfrom pandera.typing import DataFrame, Series\nfrom pandera.errors import SchemaErrors\n\nJeg lager også følgende lekedata vi skal ta for oss i eksemplene;\n\nsize = 6\n\nrandom_data = pd.DataFrame({\n    \"id_nr\": [str(uuid.uuid4()) for _ in range(size)],\n    \"lope_id_nr\": [\"L\" + str(1).zfill(4) for _ in range(size)],\n    \"aar\": np.random.choice(['2023', '2024'], size),\n    \"navn\": np.random.choice(['Ola', 'Kari', 'Per', 'Ida'], size),\n    \"produkt\": np.random.choice(['Eple', 'Gulrot', 'Brokkoli'], size),\n    \"salgsverdi\": np.random.randint(1000, 10000, size),\n    \"vekt\": np.random.randint(500, 5000, size)\n})\n\nrandom_data['kostverdi'] = (\n    random_data['salgsverdi'] * 0.75\n).astype(int)\n\nbad_data = pd.DataFrame({\n    \"id_nr\": [\"random-id1\", \"random-id1\", \"random-id2\",\n              \"random-id2\", \"random-id3\"],\n    \"lope_id_nr\": [\"L0001\", \"L0002\", \"L0001\", \"L0001\", \"0001\"],\n    \"aar\": ['2023', '2023', '2024', '2024', '2024Q1'],\n    \"navn\": ['Ola', 'Ola', 'Per', 'Kari', None],\n    \"produkt\": ['Banan', 'Eple', 'Eple', 'Agurk', 'Eple'],\n    \"salgsverdi\": [5000, 4000, 7000, 3000, 50],\n    \"vekt\": [700, 600, 700, 100, 5],\n    \"kostverdi\": [3500, 2500, 5000, 3100, 55],\n})\n\ndata = pd.concat([random_data, bad_data], ignore_index=True)\ndata\n\n\n\n\n\n\n\n\nid_nr\nlope_id_nr\naar\nnavn\nprodukt\nsalgsverdi\nvekt\nkostverdi\n\n\n\n\n0\n9c2a0f28-830f-4ae3-9400-8d992bcaa4b4\nL0001\n2023\nPer\nEple\n9921\n3698\n7440\n\n\n1\n97a1e3cf-c465-49a9-9d58-28810ff5acd6\nL0001\n2023\nOla\nGulrot\n8806\n994\n6604\n\n\n2\nce06b21d-c416-4a12-9180-71a36a891fb4\nL0001\n2024\nKari\nBrokkoli\n6387\n4145\n4790\n\n\n3\n43903841-4231-4471-b9a5-c8947ca4c985\nL0001\n2023\nKari\nEple\n2813\n4778\n2109\n\n\n4\n76cf98f9-9e6b-42a3-b1d7-d04816b7a1be\nL0001\n2023\nIda\nBrokkoli\n7053\n3606\n5289\n\n\n5\nf78f7396-2554-4f8a-a171-66682628b6db\nL0001\n2024\nKari\nEple\n3221\n2344\n2415\n\n\n6\nrandom-id1\nL0001\n2023\nOla\nBanan\n5000\n700\n3500\n\n\n7\nrandom-id1\nL0002\n2023\nOla\nEple\n4000\n600\n2500\n\n\n8\nrandom-id2\nL0001\n2024\nPer\nEple\n7000\n700\n5000\n\n\n9\nrandom-id2\nL0001\n2024\nKari\nAgurk\n3000\n100\n3100\n\n\n10\nrandom-id3\n0001\n2024Q1\nNone\nEple\n50\n5\n55\n\n\n\n\n\n\n\nDataframen består av følgende kolonner:\n\nid_nr: identifiseringsnummer\nlope_nr_id: et slags løpenummerid\naar: perioden i år for gjeldende rad\nnavn: navn på enheten (person eller kunde)\nprodukt: produktet det gjelder - la oss si i en frukt og grønt butikk\nsalgsverdi: sluttverdien varen ble solgt for\nvekt: sluttvekten som ble solgt\nkostverdi: kostnaden tilknyttet innkjøp av produktet eller varen.\n\nDet er elementer her som ikke nødvendigvis er fullt realistist med virkeligheten, men sammensetningen av disse kolonnene er mest bygd opp for å demonstrere mulighetene og fleksibiliteten ved bruk av pandera."
  },
  {
    "objectID": "blog/posts/2024-12-16-data-validering-pandera/index.html#grunnleggende-bruk---schema",
    "href": "blog/posts/2024-12-16-data-validering-pandera/index.html#grunnleggende-bruk---schema",
    "title": "Intro til Pandera",
    "section": "Grunnleggende bruk - schema",
    "text": "Grunnleggende bruk - schema\nFor å ta i bruk pandera må vi definere et schema. Schemaer definerer hvordan dataene forventes at skal se ut, spesielt når det kommer til datatyper.\nMed pandera kan du validere både datatyper og innhold. Det er flere måter å definere ett schema på, men jeg kommer til å vise den anbefalte måten å gjøre det på. Den er ikke nødvendigvis den enkleste, men den er enkel nok, og har likheter til Pydantic.\nEt schema i pandera defineres som følgende;\n\nclass SchemaValidation1(pan.DataFrameModel):\n    \n    id_nr: Series[str] = pan.Field(unique=True)\n    lope_id_nr: Series[str] = pan.Field(\n        str_startswith='L',\n        str_length={'min_value': 5,\n                    'max_value': 5}\n    )\n    aar: Series[str] = pan.Field(\n        str_length={'min_value': 4,\n                    'max_value': 4}\n    )\n    navn: Series[str] = pan.Field(\n        nullable=False # Default\n    )\n    produkt: Series[str] = pan.Field(\n        isin=['Eple', 'Banan', 'Gulrot', 'Brokkoli']\n    )\n    salgsverdi: Series[int] = pan.Field(ge=1000)\n    vekt: Series[int] = pan.Field(ge=500)\n    kostverdi: Series[int] = pan.Field(gt=700)\n\nSå hva er det vi har definert her?\nVi har nå definert et eget Objekt, en class, kalt SchemaValidation1, som arver egenskapene til Pandera sitt objekt DataFrameModel. Mer avansert fra objekt og class verden trenger du ikke å gjøre eller kunne her egentlig, så ikke bli skremt med det første. Deretter definerer vi kolonnene som vi forventer i dette schemaet. Pandera er bygget på typing systemet til Python vel og merke, som enkelt forklart vil si at jeg kan bruke typing-pakkens objekter i definisjonen som han vil bruke til å validere for, men det gir også muligheten til å benytte pythons standardobjekter som str og int i definisjonen. Vi har også definert regler tilknyttet hver av disse kolonnene som da vil bli validert sammen med datatypene.\n\nid_nr er en Serie (kolonner i pandas dataframe er av datatypen pandas serie) med forventet datatype string (str). Regler som er satt er at innholdet er unikt, altså ingen duplikater i de verdiene som ligger i kolonnen.\nlope_id_nr er også forventet datatype string. Den har 2 regler; at alle verdier starter med ‘L’, og at teksten er minimum og maksimum 5 karakterer lang.\naar er forventet å være string, med regel om at den er 4 tegn lang.\nnavn er forventet å være string, med regel om at det ikke skal være noen manglende verdier (missing values). Dette er standard for alle regler, så det er ikke nødvendig å notere, men for demonstrasjonens skyld så gjorde jeg det her.\nprodukt er forventet å være string, med regler om at innholdet er blant verdiene i en gitt liste. I dette tilfellet Eple, Banan, Gulrot, Brokkoli. Kanskje er dette varene butikken selger og har i sortimentet sitt.\nsalgsverdi er forventet å være en integer (int). Regel som er satt her er at verdiene er større eller lik 1000.\nvekt er forventet å være en integer. Regel som er satt her er at verdiene er større eller lik 500.\nkostverdi er forventet å være en integer. Regel som er satt her er at verdiene er større enn 700.\n\nOkei, da har vi definert schemaet. Vi skal bygge videre på dette snart. Det finnes mange flere innebygde valideringsregler enn de vi benytter her, og man må inn i dokumentasjonen til Pandera for å se om noe kan passe deg og ditt behov der, men her demonstrerer vi hvertfall noen som sikkert kommer til å bli brukt.\nFor å utføre valideringen gjør vi følgende;\n\ntry:\n    valresult = SchemaValidation1.validate(data, lazy=True)\nexcept SchemaErrors as error:\n    # Rapport av feil utslag i dataframe\n    valresult = error.failure_cases\n    # Dataframe som ble sendt inn\n    errdata = error.data\n    # Antall feil utslag\n    num_errors = error.error_counts\n    # Rapportmeilding av feil utslag i dict\n    error_message = error.message\n\nvalresult\n\n\n\n\n\n\n\n\nschema_context\ncolumn\ncheck\ncheck_number\nfailure_case\nindex\n\n\n\n\n0\nColumn\nid_nr\nfield_uniqueness\nNone\nrandom-id1\n6\n\n\n1\nColumn\nid_nr\nfield_uniqueness\nNone\nrandom-id1\n7\n\n\n2\nColumn\nid_nr\nfield_uniqueness\nNone\nrandom-id2\n8\n\n\n3\nColumn\nid_nr\nfield_uniqueness\nNone\nrandom-id2\n9\n\n\n4\nColumn\nlope_id_nr\nstr_length(5, 5)\n0\n0001\n10\n\n\n5\nColumn\nlope_id_nr\nstr_startswith('L')\n1\n0001\n10\n\n\n6\nColumn\naar\nstr_length(4, 4)\n0\n2024Q1\n10\n\n\n7\nColumn\nnavn\nnot_nullable\nNone\nNone\n10\n\n\n8\nColumn\nprodukt\nisin(['Eple', 'Banan', 'Gulrot', 'Brokkoli'])\n0\nAgurk\n9\n\n\n9\nColumn\nsalgsverdi\ngreater_than_or_equal_to(1000)\n0\n50\n10\n\n\n10\nColumn\nvekt\ngreater_than_or_equal_to(500)\n0\n100\n9\n\n\n11\nColumn\nvekt\ngreater_than_or_equal_to(500)\n0\n5\n10\n\n\n12\nColumn\nkostverdi\ngreater_than(700)\n0\n55\n10\n\n\n\n\n\n\n\nObjektet SchemaValidation1 har en metode validate som vi kan sende inn dataframen som skal valideres opp mot schemaet(som vi arvet fra pandera DataFrameModel objektet). Jeg har satt lazy til True her fordi jeg vil at han skal validere alt og ikke stoppe ved første feil han finner. Dersom valideringen feiler har pandera et error objekt SchemaErrors hvor flere nyttige rapporter blir lagd tilgjengelig for oss. Du kan selv legge med flere av dem, men her tar vi for oss dataframen med alle feilmeldingene som dukker opp. Dersom valideringen gikk bra vil du få dataframen du sendte inn i retur.\nRapporten vi har fått ut nå i dataframen valresult har vi flere utslag på. Kolonnen id_nr finnes det duplikater blant annet. Kolonnen lope_id_nr er det funnes en som har slått ut i begge definerte reglene som nevnt tidligere. osv, osv. Denne rapporten har vi kanskje et potensial for å utnytte videre? Men det får være opp til den enkelte."
  },
  {
    "objectID": "blog/posts/2024-12-16-data-validering-pandera/index.html#behov-for-flere-kontroller",
    "href": "blog/posts/2024-12-16-data-validering-pandera/index.html#behov-for-flere-kontroller",
    "title": "Intro til Pandera",
    "section": "Behov for flere kontroller",
    "text": "Behov for flere kontroller\nDersom de innebygde mulighetene for validering ikke strekker til kan man definere reglene selv ved å definere egne metoder med tilhørende decorator (alfakrøll over metoden). Under her definerer jeg SchemaValidation2 som sett bort ifra de nye metodene er nesten helt identisk med SchemaValidation1. Forskjellen er at nå har kolonnen id_nr kun regelen om at den skal ikke ha manglende verdier i stedet for at det skal unike verdier.\n\nclass SchemaValidation2(pan.DataFrameModel):\n    \n    id_nr: Series[str] = pan.Field(nullable=False) # Default\n    lope_id_nr: Series[str] = pan.Field(\n        str_startswith='L',\n        str_length={'min_value': 5,\n                    'max_value': 5}\n    )\n    aar: Series[str] = pan.Field(\n        str_length={'min_value': 4,\n                    'max_value': 4}\n    )\n    navn: Series[str] = pan.Field(nullable=False) # Default\n    produkt: Series[str] = pan.Field(\n        isin=['Eple', 'Banan', 'Gulrot', 'Brokkoli']\n    )\n    salgsverdi: Series[int] = pan.Field(ge=1000)\n    vekt: Series[int] = pan.Field(ge=500)\n    kostverdi: Series[int] = pan.Field(gt=700)\n\n    # Sjekke at kolonne aar er tekst med tall i seg\n    @pan.check(\"aar\",\n               # Valgfritt, men gir eget navn til regelen enn metodenavnet\n               name=\"str_isdigits\",\n               # Valgfritt, men her kan man styre feilmeldingen\n               error=\"str_not_digits\")\n    def check_isdigits(cls, s: Series[str]) -&gt; Series[bool]:\n        return s.str.isdigit()\n\n    # En metode kan sjekke flere kolonner,\n    # her sjekker vi både kostverdi og salgsverdi.\n    # Validerer at Bananer har både høyere\n    # salgsverdi og kostverdi enn Epler\n    @pan.check(\"kostverdi\", \"salgsverdi\",\n               groupby=\"produkt\",\n               name=\"check_epler_bananer\")\n    def check_groupby(cls, grouped_value: Dict[str, Series[int]]) -&gt; bool:\n        return grouped_value[\"Eple\"].sum() &lt; grouped_value[\"Banan\"].sum()\n\n    # Trenger du å sjekke mer enn bare en kolonne av gangen?\n    # f.eks. at forholde mellom flere kolonner\n    # har en bestemt regel å følge?\n    # Her sjekkes at kombinasjonen for kolonnene\n    # id_nr og lope_id_nr er unike\n    @pan.dataframe_check\n    def unique_combo_idnr_lopeidnr(cls, df: pd.DataFrame) -&gt; Series[bool]:\n        df2 = df.copy()\n        df3 = (\n            df2\n            .groupby(['id_nr', 'lope_id_nr'])\n            .agg({'aar': 'count'})\n            .rename(columns={'aar': 'duplikater'}) == 1\n        ).reset_index()\n        df2 = df2.merge(df3,\n                        on=['id_nr', 'lope_id_nr'],\n                        how='left')\n        return df2['duplikater']\n\nSchemaet SchemaValidation2 har som vi ser nå 3 metoder;\n\ncheck_isdigits som sjekker at teksten faktisk kun inneholder tall. Her sjekkes kun kolonnen aar.\ncheck_groupby som grupperer verdiene i kolonnen produkt. Det sjekkes her for kolonnene kostverdi og salgsverdi. Den sjekker at summen av bananer er høyere enn summen av epler (for å gjøre noe enkelt og irrelevant).\nde 2 første sjekkene kan kun jobbe med en kolonne av gangen, ev. en groupby på en annen kolonne med fokus på de gjeldende kolonnene en har tenkt å sjekke for. Den tredje siste sjekken er litt annerledes, for de andre sjekkene har benyttet decoratoren check, mens den siste har dataframe_check. Dette vil si at hele dataframen sendes inn, og her vil du ha full fleksibilitet til å sjekke det du måtte ønske på tvers av alle kolonner. Viktigste er at det returneres en serie(kolonne) av boolske verdier (True/False). I denne siste sjekken unique_combo_idnr_lopeidnr sjekkes det at kombinasjonen av kolonnene id_nr og lope_id_nr er unike i dataframen.\n\nIgjen kan dataene valideres;\n\ntry:\n    valresult = SchemaValidation2.validate(data, lazy=True)\nexcept SchemaErrors as error:\n    valresult = error.failure_cases\n\nvalresult\n\n\n\n\n\n\n\n\nschema_context\ncolumn\ncheck\ncheck_number\nfailure_case\nindex\n\n\n\n\n14\nDataFrameSchema\nlope_id_nr\nunique_combo_idnr_lopeidnr\n0\nL0001\n8\n\n\n15\nDataFrameSchema\nlope_id_nr\nunique_combo_idnr_lopeidnr\n0\nL0001\n9\n\n\n26\nDataFrameSchema\nkostverdi\nunique_combo_idnr_lopeidnr\n0\n5000\n8\n\n\n25\nDataFrameSchema\nvekt\nunique_combo_idnr_lopeidnr\n0\n100\n9\n\n\n24\nDataFrameSchema\nvekt\nunique_combo_idnr_lopeidnr\n0\n700\n8\n\n\n23\nDataFrameSchema\nsalgsverdi\nunique_combo_idnr_lopeidnr\n0\n3000\n9\n\n\n22\nDataFrameSchema\nsalgsverdi\nunique_combo_idnr_lopeidnr\n0\n7000\n8\n\n\n21\nDataFrameSchema\nprodukt\nunique_combo_idnr_lopeidnr\n0\nAgurk\n9\n\n\n20\nDataFrameSchema\nprodukt\nunique_combo_idnr_lopeidnr\n0\nEple\n8\n\n\n19\nDataFrameSchema\nnavn\nunique_combo_idnr_lopeidnr\n0\nKari\n9\n\n\n18\nDataFrameSchema\nnavn\nunique_combo_idnr_lopeidnr\n0\nPer\n8\n\n\n17\nDataFrameSchema\naar\nunique_combo_idnr_lopeidnr\n0\n2024\n9\n\n\n16\nDataFrameSchema\naar\nunique_combo_idnr_lopeidnr\n0\n2024\n8\n\n\n27\nDataFrameSchema\nkostverdi\nunique_combo_idnr_lopeidnr\n0\n3100\n9\n\n\n13\nDataFrameSchema\nid_nr\nunique_combo_idnr_lopeidnr\n0\nrandom-id2\n9\n\n\n12\nDataFrameSchema\nid_nr\nunique_combo_idnr_lopeidnr\n0\nrandom-id2\n8\n\n\n1\nColumn\nlope_id_nr\nstr_startswith('L')\n1\n0001\n10\n\n\n11\nColumn\nkostverdi\ncheck_epler_bananer\n1\nFalse\nNone\n\n\n10\nColumn\nkostverdi\ngreater_than(700)\n0\n55\n10\n\n\n9\nColumn\nvekt\ngreater_than_or_equal_to(500)\n0\n5\n10\n\n\n8\nColumn\nvekt\ngreater_than_or_equal_to(500)\n0\n100\n9\n\n\n7\nColumn\nsalgsverdi\ncheck_epler_bananer\n1\nFalse\nNone\n\n\n6\nColumn\nsalgsverdi\ngreater_than_or_equal_to(1000)\n0\n50\n10\n\n\n5\nColumn\nprodukt\nisin(['Eple', 'Banan', 'Gulrot', 'Brokkoli'])\n0\nAgurk\n9\n\n\n4\nColumn\nnavn\nnot_nullable\nNone\nNone\n10\n\n\n3\nColumn\naar\nstr_not_digits\n1\n2024Q1\n10\n\n\n2\nColumn\naar\nstr_length(4, 4)\n0\n2024Q1\n10\n\n\n0\nColumn\nlope_id_nr\nstr_length(5, 5)\n0\n0001\n10\n\n\n\n\n\n\n\nDesverre vil sjekker som gjelder hele dataframen registrere flere feil ettersom han sjekker alle kolonner for gjeldende rader. Derimot er fleksibiliteten ganske stor!"
  },
  {
    "objectID": "blog/posts/2024-12-16-data-validering-pandera/index.html#bruk-av-validering-i-funksjonene",
    "href": "blog/posts/2024-12-16-data-validering-pandera/index.html#bruk-av-validering-i-funksjonene",
    "title": "Intro til Pandera",
    "section": "Bruk av validering i funksjonene",
    "text": "Bruk av validering i funksjonene\nOver til et eksempel hvor pandera viser seg som veldig nyttig! La oss si at vi har klargjorte data klart, ihht. datatilstandene, og vi er da klare for å lage statistikkdata. Det er ikke gitt at løpet er helt rett fram mellom disse datatilstandene, men i dette eksempelet er jobben bare å få aggregert klargjorte data.\nNedenfor her lager jeg klargjorte data av de dataene som vi har jobbet med, og som er korrekte. Lager et tilhørende skjema, som bare arver fra det første schemaet vi lagde. Valideringen her vil selvsagt gå smertefritt igjennom.\n\nklargjort_df = data.head(6)\n\n\nclass KlargjortSchema(SchemaValidation1):\n    pass\n\n\ntry:\n    klargjort_df = KlargjortSchema.validate(klargjort_df, lazy=True)\nexcept SchemaErrors as error:\n    valresult = error.failure_cases\n    raise error\n\nklargjort_df\n\n\n\n\n\n\n\n\nid_nr\nlope_id_nr\naar\nnavn\nprodukt\nsalgsverdi\nvekt\nkostverdi\n\n\n\n\n0\n9c2a0f28-830f-4ae3-9400-8d992bcaa4b4\nL0001\n2023\nPer\nEple\n9921\n3698\n7440\n\n\n1\n97a1e3cf-c465-49a9-9d58-28810ff5acd6\nL0001\n2023\nOla\nGulrot\n8806\n994\n6604\n\n\n2\nce06b21d-c416-4a12-9180-71a36a891fb4\nL0001\n2024\nKari\nBrokkoli\n6387\n4145\n4790\n\n\n3\n43903841-4231-4471-b9a5-c8947ca4c985\nL0001\n2023\nKari\nEple\n2813\n4778\n2109\n\n\n4\n76cf98f9-9e6b-42a3-b1d7-d04816b7a1be\nL0001\n2023\nIda\nBrokkoli\n7053\n3606\n5289\n\n\n5\nf78f7396-2554-4f8a-a171-66682628b6db\nL0001\n2024\nKari\nEple\n3221\n2344\n2415\n\n\n\n\n\n\n\nDeretter definerer vi et eget schema for statistikkdata, med noen tilhørende regler og datatyper;\n\nclass StatistikkSchema(pan.DataFrameModel):\n    \n    aar: Series[pd.CategoricalDtype] = pan.Field(\n        coerce=True, # Vil konvertere datatypene for meg\n        str_length={'min_value': 4,\n                    'max_value': 4})\n    produkt: Series[pd.CategoricalDtype] = pan.Field(\n        coerce=True, # Vil konvertere datatypene for meg\n        isin=['Eple', 'Banan', 'Gulrot', 'Brokkoli'])\n    salgsverdi: Series[int] = pan.Field(ge=0)\n\nSå over til magien; Pandera schemaene kan innlemmes i hvilken som helst funksjon som har dataframes som input eller output, og det uten at du selv skriver at valideringen skal skje i funksjonen, det skjer automagisk! Og det gjøres som følgende;\n\n# Lazy for at valideringen skal utføres igjennom hele dataframene\n@pan.check_types(lazy=True)\ndef agg_statistikk(\n    df: DataFrame[KlargjortSchema]\n) -&gt; DataFrame[StatistikkSchema]:\n    dff = (\n        df\n        .copy()\n        .groupby(['aar', 'produkt'], as_index=False)\n        .agg({'salgsverdi': 'sum'})\n    )\n    return dff\n\nSå nå ved å bruke funksjonen, vil du ikke få lagd statistikkdata uten at både klargjorte data blir validert og godkjent, og at statistikkdata som er på vei ut av funksjonen er validert og godkjent. I vårt tilfelle skal det gå fint nå;\n\nstatistikk_df = agg_statistikk(klargjort_df)\nstatistikk_df\n\n\n\n\n\n\n\n\naar\nprodukt\nsalgsverdi\n\n\n\n\n0\n2023\nBrokkoli\n7053\n\n\n1\n2023\nEple\n12734\n\n\n2\n2023\nGulrot\n8806\n\n\n3\n2024\nBrokkoli\n6387\n\n\n4\n2024\nEple\n3221\n\n\n\n\n\n\n\nMan skal også kunne validere flere schemaer samtidig også hvis en ønsker det. Altså at input til funksjonen sjekkes opp mot flere schema samtidig, eller at output blir det. Det er ikke blitt demonstrert her.\nMed det samme kan vi sjekke datatypene, vi hadde satt at Pandera skulle endre datatypene for oss. Både før og etter;\n\n\nCode\nfrom IPython.display import HTML, display\n\nkdf = pd.DataFrame(klargjort_df.dtypes, columns=['Datatyper'])\nsdf = pd.DataFrame(statistikk_df.dtypes, columns=['Datatyper'])\n\n# Style dataframes\nstyled_df1 = kdf.style.set_caption(\"Klargjorte-data\")\nstyled_df2 = sdf.style.set_caption(\"Statistikk data\")\n\ndisplay(HTML(\nf\"\"\"\n&lt;div style=\"display: flex; justify-content: space-around;\"&gt;\n&lt;div&gt;{styled_df1.to_html()}&lt;/div&gt;\n&lt;div&gt;{styled_df2.to_html()}&lt;/div&gt;\n&lt;/div&gt;\n\"\"\"\n))\n\n\n\n\n\n\n\n\nTabell 1: Klargjorte-data\n\n\n\n\n\n \nDatatyper\n\n\n\n\nid_nr\nobject\n\n\nlope_id_nr\nobject\n\n\naar\nobject\n\n\nnavn\nobject\n\n\nprodukt\nobject\n\n\nsalgsverdi\nint64\n\n\nvekt\nint64\n\n\nkostverdi\nint64\n\n\n\n\n\n\n\n\n\n\n\n\n\nTabell 2: Statistikk data\n\n\n\n\n\n \nDatatyper\n\n\n\n\naar\ncategory\n\n\nprodukt\ncategory\n\n\nsalgsverdi\nint64"
  },
  {
    "objectID": "blog/posts/2024-12-16-data-validering-pandera/index.html#bonus-auto-transformasjon-av-kolonneverdier",
    "href": "blog/posts/2024-12-16-data-validering-pandera/index.html#bonus-auto-transformasjon-av-kolonneverdier",
    "title": "Intro til Pandera",
    "section": "BONUS: Auto-transformasjon av kolonneverdier",
    "text": "BONUS: Auto-transformasjon av kolonneverdier\nPandera har noe som kalles parsers, som gir oss muligheten til å utføre preprosesseringer på dataene før validering. Dette kan være flere typer transformasjoner som man bør sørge for er gjort før valideringen utføres, ev. om transformasjonen bare skal gjennomføres.\nLa oss ta et eksempel med en liten del av dataene vi har jobbet med til nå, da med data vi vet det ikke skal bli noe problemer med;\n\ndata['dekningsbidrag'] = data['salgsverdi'] - data['kostverdi']\n\ndf = data.head(3).copy()\ndf\n\n\n\n\n\n\n\n\nid_nr\nlope_id_nr\naar\nnavn\nprodukt\nsalgsverdi\nvekt\nkostverdi\ndekningsbidrag\n\n\n\n\n0\n9c2a0f28-830f-4ae3-9400-8d992bcaa4b4\nL0001\n2023\nPer\nEple\n9921\n3698\n7440\n2481\n\n\n1\n97a1e3cf-c465-49a9-9d58-28810ff5acd6\nL0001\n2023\nOla\nGulrot\n8806\n994\n6604\n2202\n\n\n2\nce06b21d-c416-4a12-9180-71a36a891fb4\nL0001\n2024\nKari\nBrokkoli\n6387\n4145\n4790\n1597\n\n\n\n\n\n\n\nJeg beregner her dekningsbidraget for hver observasjon, som da er differansen mellom salgsverdi og kostverdi. Det er mer eller mindre en funksjon som avhenger av disse to variablene, og må holdes oppdatert.\nOg la oss nå si at kostverdien på første observasjonen ikke skulle være på 75 % av salgsverdi slik vi startet med, men av en eller annen grunn heller skulle være på 85 %. Vi kan editere det inn;\n\ndf.loc[0, ['kostverdi']] = int(round(\n    df.iloc[0]['salgsverdi'] * 0.85, 0)\n                              )\n\ndf\n\n\n\n\n\n\n\n\nid_nr\nlope_id_nr\naar\nnavn\nprodukt\nsalgsverdi\nvekt\nkostverdi\ndekningsbidrag\n\n\n\n\n0\n9c2a0f28-830f-4ae3-9400-8d992bcaa4b4\nL0001\n2023\nPer\nEple\n9921\n3698\n8433\n2481\n\n\n1\n97a1e3cf-c465-49a9-9d58-28810ff5acd6\nL0001\n2023\nOla\nGulrot\n8806\n994\n6604\n2202\n\n\n2\nce06b21d-c416-4a12-9180-71a36a891fb4\nL0001\n2024\nKari\nBrokkoli\n6387\n4145\n4790\n1597\n\n\n\n\n\n\n\nSå nå har vi fått korrigert kostverdien på første observasjon, men dekningsbidraget er fortsatt den samme. Dette kan løses som en egen funksjon, men hvorfor ikke innlemme det i data valideringen vår, da pandera støtter slik transformering. Vi lager først et tilhørende schema;\n\nclass ParserSchema(SchemaValidation1):\n    dekningsbidrag: Series[int]\n\n    @pan.check(\"navn\")\n    def is_uppercase(cls, s: Series[str]) -&gt; Series[bool]:\n        return s.str.isupper()\n\n    # konverterer all tekst i kolonnen til å ha kun store bokstaver\n    @pan.parser(\"navn\")\n    def uppercase(cls, s: Series[str]) -&gt; Series[str]:\n        return s.str.upper()\n\n    # Sørger for at dekningsbidrag blir rekalkulert\n    @pan.dataframe_parser\n    def kalkuler_dekningsbidrag(cls, df: pd.DataFrame) -&gt; pd.DataFrame:\n        df['dekningsbidrag'] = df['salgsverdi'] - df['kostverdi']\n        return df\n\nSå her tar jeg i bruk det aller første schema som vi definerte, men legger på dekningsbidrag som ikke har noen andre valideringer enn datatype. Med dataene vi har nå skal det ikke dukke opp noen feil med dette. Jeg legger ved en valideringsregel for navn i dette tilfelle, hvor nå alt i kolonnen navn skal være store bokstaver. Vi vet allerede at det ikke er noen store bokstaver der, så vi legger inn en metode som har decorator parser som vil transformere dette. I tillegg legger vi til en egen metode med decorator dataframe_parser for å rekalkulere dekningsbidraget.\nSå sånn sett skulle man kanskje tro at valideringen av kolonnen navn vil kunne slå ut i valideringen, men som nevnt så kjøres transformasjonene først før valideringen. I tillegg, når valideringen går igjennom, vil du få dataframen du sendte inn i retur ved utførelsen av valideringen;\n\ntry:\n    valresult = ParserSchema.validate(df, lazy=True)\nexcept SchemaErrors as error:\n    valresult = error.failure_cases\n\nvalresult\n\n\n\n\n\n\n\n\nid_nr\nlope_id_nr\naar\nnavn\nprodukt\nsalgsverdi\nvekt\nkostverdi\ndekningsbidrag\n\n\n\n\n0\n9c2a0f28-830f-4ae3-9400-8d992bcaa4b4\nL0001\n2023\nPER\nEple\n9921\n3698\n8433\n1488\n\n\n1\n97a1e3cf-c465-49a9-9d58-28810ff5acd6\nL0001\n2023\nOLA\nGulrot\n8806\n994\n6604\n2202\n\n\n2\nce06b21d-c416-4a12-9180-71a36a891fb4\nL0001\n2024\nKARI\nBrokkoli\n6387\n4145\n4790\n1597\n\n\n\n\n\n\n\nSom vi nå ser har valideringen gått fint for seg. Vi ser at alle verdier i kolonnen navn har blitt tekst med kun store bokstaver, og vi ser at dekningsbidraget har blitt rekalkulert så det nå er korrekt!"
  },
  {
    "objectID": "blog/posts/2024-12-16-data-validering-pandera/index.html#oppsummering",
    "href": "blog/posts/2024-12-16-data-validering-pandera/index.html#oppsummering",
    "title": "Intro til Pandera",
    "section": "Oppsummering",
    "text": "Oppsummering\nFor at vi skal kunne produsere og levere statistikk av høy kvalitet er det viktig at vi validerer data løpende i produksjonsløpene våre. Store deler av dataene våre er strukturerte, ev. tidy om du vil, og da er python pakken Pandera en sterk kandidat å benytte inn i kodene våre. Hvertfall hvis du programmerer i Python. For R så er pakken Validate aktuell. Her har vi introdusert generell bruk av Pandera for validering av data; hvordan definere schema og valideringsregler, hvordan validere en dataframe med det, og hvordan det kan tas i bruk i blant annet funksjoner. Trenger du hjelp til å implementere data validering med Pandera inn i koden din, så er Støtteteamene mulig å spørre, ellers kommer man ikke unna dokumentasjonen til Pandera selv."
  },
  {
    "objectID": "blog/posts/2025-02-07-parquet-viewer/index.html",
    "href": "blog/posts/2025-02-07-parquet-viewer/index.html",
    "title": "Parquet-utforsker i VS Code",
    "section": "",
    "text": "I Dapla Lab tjenesten Vscode-python er nå extension’en vscode-parquet-visualizer installert. Den lar deg åpne en Parquet-fil uten bruk av Python- eller R-biblioteker. I tillegg lar den deg spørre mot datasettet med SQL, filtrere dataettet uten kode, sortere kolonner, gir deg metadata om kolonner og datasett, og gir en forhåndsvisning av komplekse celler.\nI videoen under ser man hvordan man åpner en Parquet-fil i en bøtte fra et ssb-project. Filen som åpnes har 4 kolonner, 5 millioner rader og er på 85 megabytes."
  },
  {
    "objectID": "blog/posts/2025-02-07-parquet-viewer/index.html#bruksområde",
    "href": "blog/posts/2025-02-07-parquet-viewer/index.html#bruksområde",
    "title": "Parquet-utforsker i VS Code",
    "section": "Bruksområde",
    "text": "Bruksområde\nBruksområdet for denne funksjonaliteten er å utforske Parquet-filer og ikke prosessere data i produksjon. Selv om man kan skrive ut et filtrert datasett med løsningen, så skal det ikke benyttes til prosessering som skal være reproduserbar siden det ikke dokumenteres med kode."
  },
  {
    "objectID": "blog/posts/2025-02-07-parquet-viewer/index.html#skrive-sql",
    "href": "blog/posts/2025-02-07-parquet-viewer/index.html#skrive-sql",
    "title": "Parquet-utforsker i VS Code",
    "section": "Skrive SQL",
    "text": "Skrive SQL\nSQL-en som skrives må være duckdbsql siden det er dette extension’en benytter for å hente informasjon fra Parquet-filen."
  },
  {
    "objectID": "blog/posts/2025-05-09-analyse-av-deltbøtter/index.html",
    "href": "blog/posts/2025-05-09-analyse-av-deltbøtter/index.html",
    "title": "Analyse av deltbøtter",
    "section": "",
    "text": "UTFORSK DASHBOARD HER.\nI Seksjon for IT-arkitektur (702) jobber vi kontinuerlig med å forstå og forbedre prosessene våre, spesielt når det gjelder deling av data. Ved å analysere og visualisere deltbøtter får vi verdifull innsikt i hvordan team deler data. Dette hjelper oss med å identifisere muligheter for bedre samarbeid og ressursutnyttelse."
  },
  {
    "objectID": "blog/posts/2025-05-09-analyse-av-deltbøtter/index.html#hvorfor-analysere-deltbøtter",
    "href": "blog/posts/2025-05-09-analyse-av-deltbøtter/index.html#hvorfor-analysere-deltbøtter",
    "title": "Analyse av deltbøtter",
    "section": "Hvorfor analysere deltbøtter?",
    "text": "Hvorfor analysere deltbøtter?\nAnalyse av deltbøtter gir innsikt i hvordan team deler og bruker data. Dette kan avdekke forbedringsmuligheter og bidra til å finne enklere måter å dele data på. Når en datakatalog er på plass, vil vi få enda bedre forståelse av effekten av datadeling. Denne innsikten gir oss mulighet til å justere prosessene våre og styrke samarbeidet mellom team."
  },
  {
    "objectID": "blog/posts/2025-05-09-analyse-av-deltbøtter/index.html#hvordan-fungerer-løsningen",
    "href": "blog/posts/2025-05-09-analyse-av-deltbøtter/index.html#hvordan-fungerer-løsningen",
    "title": "Analyse av deltbøtter",
    "section": "Hvordan fungerer løsningen?",
    "text": "Hvordan fungerer løsningen?\nLøsningen visualiserer hvordan data deles mellom team gjennom deltbøtter. Den benytter Dapla Team-API for å hente informasjon om delingsforhold og eierskap. Dataene behandles og presenteres i en interaktiv graf som lar brukerne utforske relasjonene mellom team og deltbøtter.\nVisualiseringen fokuserer på deltbøtter og gir et målrettet innblikk i delingsforholdene mellom team. Dette hjelper oss med å identifisere mønstre og muligheter for forbedring.\nI grafen representerer:\n\nGule noder: Deltbøtter, som fungerer som delingspunkter for data.\nBlå noder: Team, som enten eier eller bruker data fra deltbøtter.\nGrønne linjer: Eierforhold mellom team og deltbøtter.\nRøde linjer: Delingsforhold mellom team.\n\nBrukere kan filtrere visualiseringen basert på team, deltbøtter, avdelinger og seksjoner for å fokusere på spesifikke områder. Dette gjør det enklere å analysere delingsforhold og identifisere forbedringsområder."
  },
  {
    "objectID": "blog/posts/2025-05-09-analyse-av-deltbøtter/index.html#hvordan-data-deles-som-grafer",
    "href": "blog/posts/2025-05-09-analyse-av-deltbøtter/index.html#hvordan-data-deles-som-grafer",
    "title": "Analyse av deltbøtter",
    "section": "Hvordan data deles som grafer",
    "text": "Hvordan data deles som grafer\nData visualiseres som grafer for å gi en intuitiv forståelse av relasjonene mellom team og deltbøtter. Grafene består av:\n\nNoder: Representerer team eller deltbøtter.\nRelasjoner: Viser sammenhenger mellom noder, som eierskap eller deling av data.\n\nDenne tilnærmingen gjør det enklere å identifisere mønstre, som hvilke team som deler mest data, eller hvilke deltbøtter som er mest sentrale i delingsnettverket. Interaktive grafer gir brukerne bedre innsikt i delingsprosessene."
  },
  {
    "objectID": "blog/posts/2025-05-09-analyse-av-deltbøtter/index.html#tilgang-til-løsningen",
    "href": "blog/posts/2025-05-09-analyse-av-deltbøtter/index.html#tilgang-til-løsningen",
    "title": "Analyse av deltbøtter",
    "section": "Tilgang til løsningen",
    "text": "Tilgang til løsningen\nLøsningen er tilgjengelig her: Interaktiv visualisering av bøtter og team. Du må ha en GitHub-bruker for å se den."
  },
  {
    "objectID": "blog/posts/2025-05-09-analyse-av-deltbøtter/index.html#last-ned-rådata",
    "href": "blog/posts/2025-05-09-analyse-av-deltbøtter/index.html#last-ned-rådata",
    "title": "Analyse av deltbøtter",
    "section": "Last ned rådata",
    "text": "Last ned rådata\nFor egne analyser kan du laste ned rådataene som brukes i visualiseringen her: Last ned rådata."
  },
  {
    "objectID": "blog/posts/2025-05-09-analyse-av-deltbøtter/index.html#github-repository",
    "href": "blog/posts/2025-05-09-analyse-av-deltbøtter/index.html#github-repository",
    "title": "Analyse av deltbøtter",
    "section": "GitHub Repository",
    "text": "GitHub Repository\nFor mer informasjon: ssb-dapla-bucket-analysis."
  },
  {
    "objectID": "blog/posts/2025-05-09-analyse-av-deltbøtter/index.html#oppdateringer-og-spørsmål",
    "href": "blog/posts/2025-05-09-analyse-av-deltbøtter/index.html#oppdateringer-og-spørsmål",
    "title": "Analyse av deltbøtter",
    "section": "Oppdateringer og spørsmål",
    "text": "Oppdateringer og spørsmål\nSi gjerne ifra om det er noen ønsker eller spørsmål til løsningen. Den er laget ganske raskt og oppdateres med nye data ved (u)jevne mellomrom."
  },
  {
    "objectID": "statistikkere/jupyter.html",
    "href": "statistikkere/jupyter.html",
    "title": "Jupyter",
    "section": "",
    "text": "Jupyter er en tjeneste på Dapla Lab som lar brukerne kode i Jupyterlab. Tjenesten kommer med R og Python og noen vanlige Jupyterlab-extensions ferdig installert. Målgruppen for tjenesten er brukere som skal skrive produksjonskode i Jupyterlab.\nSiden tjenesten er ment for produksjonskode så er det veldig få R- og Python-pakker som er forhåndsinstallert. Antagelsen er at brukeren/teamet heller bør installere de pakkene de selv trenger, framfor at det ligger ferdiginstallerte pakker som skal dekke behovet til alle brukere/team i SSB. Det reduserer kompleksitet i tjenesten og dermed sannsynligheten for feilsituasjoner.\nFor uerfarne brukere finnes det en egen tjeneste som heter Jupyter-playground. Her er mange av de vanlige R- og Python-pakkene installert og det er opprettet en ferdig kernel som lar brukerne komme i gang fort med koding. Denne tjenesten er ikke tenkt for bruk i produksjonskode.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#forberedelser",
    "href": "statistikkere/jupyter.html#forberedelser",
    "title": "Jupyter",
    "section": "Forberedelser",
    "text": "Forberedelser\nFør man starter Jupyter-tjenesten bør man ha lest kapitlet om Dapla Lab og satt opp Git- og GitHub-konfigurasjonen under Min konto. Deretter gjør du følgende:\n\nLogg deg inn på Dapla Lab\nUnder Tjenestekatalog trykker du på Start-knappen for Jupyter\nGi tjenesten et navn\nÅpne Jupyter konfigurasjoner",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#konfigurasjon",
    "href": "statistikkere/jupyter.html#konfigurasjon",
    "title": "Jupyter",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nFør man åpner en tjeneste kan man konfigurere hvor mye ressurser man ønsker, hvilket team man skal representere, om et GitHub-repo skal klones ved oppstart, og mange andre ting. Valgene man gjør kan også lagres slik at man å slipper å gjøre samme jobb senere. Figur 1 viser Tjeneste-delen i konfigurasjonen for Jupyter hvor man kan velge hvilken versjon av Jupyter man vil bruke.\n\n\n\n\n\n\nFigur 1: Jupyter-versjon i Dapla Lab\n\n\n\n\nData\nUnder Data-menyen kan man velge hvilket team og tilgangsgruppe man skal representere, som igjen bestemmer hvilke data man får tilgang til. Man gjør dette ved å velge navnet på tilgangsgruppen, og denne er alltid på formen &lt;teamnavn&gt;-&lt;tilgangsgruppe&gt;. Figur 2 viser at brukeren har valgt tilgangsgruppen dapla-felles-developers, dvs. at de representerer tilgangsgruppen developers for teamet dapla-felles.\n\n\n\n\n\n\nFigur 2: Detaljert tjenestekonfigurasjon for bøttetilgang i Dapla Lab\n\n\n\nUnder Team og tilgangsgruppe kan brukeren også velge å representere tilgangsgruppen data-admins for et team. I de tilfellene er det et krav om brukeren oppgir en skriftlig begrunnelse for hvorfor tilgangen er nødvendig. I tillegg må kan de maksimalt aktivere tilgangen i 8 timer.\nFigur 3 viser en bruker som aktiverer sin data-admins tilgang for team dapla-felles. Hvis brukeren ikke oppgir en begrunnelse vil de få en feilmelding ved oppstart av tjenesten.\n\n\n\n\n\n\nFigur 3: Aktivere tilgang til kildedata for data-admins.\n\n\n\nNår man åpner tjeneste, og representerer et team, så tilgjengeliggjøres det teamets bøtter inne i tjenesten under filstien /buckets/. Men et team kan også ha tilgang til andre sine delt-bøtter og ønske å tilgjengliggjøre disse også. Figur 4 viser hvordan man spesifiserer hvilke delt-bøtter man ønsker å tilgjengeliggjøre inne i tjenesten. Man gjør det ved å spesifisere det tekniske teamnavnet til teamet som eier dataene, og spesifiserer kortnavnet for delt-bøtta1.\n\n\n\n\n\n\nFigur 4: Konfigurer hvilke delt-bøtter fra andre som skal tilgjengeliggjøres i tjenesten.\n\n\n\n\n\n\n\n\n\nCautionHvor finner jeg filene inne i tjenesten?\n\n\n\n\n\nBøtter som tilgjengeliggjøres inne tjenesten finner du alltid under filstien /buckets/ i tjenesten. Under er et eksempel som vil være vanlig for et statistikkteam:\n\n\nFilsystem\n\n/buckets/\n├── produkt/\n│   ├── inndata/\n│   └── klargjorte data/\n├── frasky/\n├── tilsky/\n├── delt-ledstill/\n├── delt-freg/\n└── shared/\n    ├── arbmark-register/\n    │   └── ameld/\n    └── vof/\n        └── rollebasen/\n\nBøttene som eies av teamet (produkt, frasky, tilsky, delt-ledstill og delt-freg) tilgjengeliggjøres rett under filstien /buckets/, mens andre team sine delt-bøtter tilgjengeliggjøres under /buckets/shared/&lt;teamnavn&gt;/&lt;kortnavn for bøtte&gt;. Teamets egne delt-bøtter får et delt-prefiks slik at en delt-bøtte med kortnavn ledstill blir tilgjengeliggjort som delt-ledstill.\n\n\n\nMan kan også velge å jobbe direkte mot bøttene, og da trenger man ikke å tilgjengeliggjøre bøttene i filsystemet. Under tjenestekonfigurasjonen Avansert kan man skru av tilgjengeliggjøringen av bøtter i filsystemet.\n\n\nGit/GitHub\nUnder menyen Git/GitHub kan man konfigurere Git og GitHub slik at det blir lettere å jobbe med inne i tjenesten. Som standard arves informasjonen som er lagret under Min konto-Git i Dapla Lab. Informasjonen under tjenestekonfigurasjonen blir tilgjengeliggjort som miljøvariabler i tjenesten. Informasjonen blir også lagt i $HOME/.netrc slik at man kan benytte ikke trenger å gjøre noe mer for å jobbe mot GitHub fra tjenesten.\n\n\n\n\n\n\nFigur 5: Konfigurasjon av Git og GitHub for Jupyter-tjenesten i Dapla Lab\n\n\n\nFigur 5 viser at brukeren som standard får aktivert Aktiver Git. Dette innebærer at Git-brukernavn, Git e-post og GitHub-token arves fra brukerkonfigurasjonen. I tillegg så opprettes SSBs standard Git-konfigurasjon i ~/.gitconfig.\nMan kan også velge at ssb-project build skal kjøres på repoet under oppstart av tjenesten. Det fører til litt lengre oppstartstid, men er alle pakker installert og en kernel opprettet når tjenesten er klar.\n\nKlone repo ved oppstart\nLegger man inn lenke til repo under Repo i tjenestekonfigurasjonen vil repoet klones ved oppstart. Hvis jeg for eksempel vil klone repoet for Dapla-manualen ved oppstart skriver jeg https://github.com/statisticsnorway/dapla-manual.git i feltet.\n\n\n\nPython/R\nUnder menyen Python/R kan man velge hvilke versjon av R og Python man ønsker å kjøre. Man kan velge mellom alle tidligere tilbudte kombinasjoner av R og Python.\nI Figur 6 ser vi av navnet r4.4.0-py311-v55-2024.10.31 at tjenesten som default vil startes versjon 4.4.0 av R og 3.11 for Python. Etterhvert som nye versjoner av R og Python kommer kan disse tilgjengeliggjøres i tjenesten, men brukeren kan velge å starte en eldre versjon av tjenesten.\n\n\n\n\n\n\nFigur 6: Konfigurasjon av Git og GitHub for Jupyter-tjenesten i Dapla Lab\n\n\n\n\n\nRessurser\nUnder menyen Resources kan man velge hvor mye CPU og RAM man ønsker i tjenesten, slik som vist i Figur 7. Velg så lite som trengs for å gjøre jobben du skal gjøre.\n\n\n\n\n\n\nFigur 7: Konfigurasjon av ressurser for Jupyter-tjenesten i Dapla Lab\n\n\n\n\n\nDiskplass\nSom default får alle som starter en instans av Jupyter-tjenesten en lokal disk på 10GB inne i tjenesten. Under Diskplass-menyen kan man velge å øke størrelsen på disken eller ikke noe disk i det hele tatt. Siden lokal disk i tjenesten hovedsakelig skal benyttes til å lagre en lokal kopi av koden som lagres på GitHub mens man gjør endringer bør ikke størrelsen på disken være stor. Figur 8 viser valgene som kan gjøres under Diskplass-fanen.\n\n\n\n\n\n\nFigur 8: Konfigurasjon av lokal disk for Jupyter-tjenesten i Dapla Lab\n\n\n\n\n\nAvansert\nUnder Data i Avansert kan man velge å ikke tilgjengeliggjøre bøtter som filsystem inne i tjenesten. Konsekvensen av dette er at man må lese og skrive filer ved å referere til bøttene direkte.\n\n\n\n\n\n\nFigur 9: Avansert konfigurasjon for Jupyter-tjenesten i Dapla Lab\n\n\n\nUnder Oppstartsskript kan man velge angi et bash-script som skal kjøres ved oppstart av tjenesten. Skriptet må ligge lagret på www.github.com/statisticsnorway, og under Bash-skript-delen oppgis kun navn på repo og sti i repoet, f.eks. stat-ledstill/utilities/ledstill-startupscript.sh. Hvis skriptet tar argumenter kan man oppgi disse under Argumenter.\nI oppstartsskriptet kan man gjøre alt som er mulig å gjøre i terminalen etter at tjenesten er startet. F.eks. kan man:\n\ndefinere miljøvariabler\nlegge til ønsket konfigurasjon i .bashrc\nkonfigurere farger og andre innstillinger i Jupyter, VS Code og RStudio\n\n\n\n\n\n\n\nCautionEksempel på et oppstartskript\n\n\n\n\n\n\n\ndemo-script.sh\n\n#!/bin/bash\n\n# Update .bashrc with environment variables and aliases\necho \"\" &gt;&gt; \"$HOME/.bashrc\"\necho \"# Opprettet av mitt personlige startupscript:\" &gt;&gt; \"$HOME/.bashrc\"\necho \"export TEST=true\" &gt;&gt; \"$HOME/.bashrc\"\necho \"alias gs='git status'\" &gt;&gt; \"$HOME/.bashrc\"\necho \"alias ll='ls -alF'\" &gt;&gt; \"$HOME/.bashrc\"\n\n# Set Jupyter-theme via settings file\nTHEME_NAME=\"JupyterLab Dark\"\nSETTINGS_DIR=\"$HOME/work/.jupyter/config/lab/user-settings/@jupyterlab/apputils-extension\"\nSETTINGS_FILE=\"$SETTINGS_DIR/themes.jupyterlab-settings\"\n\nmkdir -p \"$SETTINGS_DIR\"\n\ncat &gt; \"$SETTINGS_FILE\" &lt;&lt;EOF\n{\n    // Theme set from init-script\n    \"theme\": \"$THEME_NAME\"\n}\nEOF\n\necho \"[Init Script] JupyterLab theme set to '$THEME_NAME'\"\n\n\n\n\nHvis oppstartsskriptet feiler, eller man er interessert i hva som ble kjørt, så kan man undersøke logg-filen til oppstartskriptet inne i tjenesten: $HOME/personal_init_script.log.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#datatilgang",
    "href": "statistikkere/jupyter.html#datatilgang",
    "title": "Jupyter",
    "section": "Datatilgang",
    "text": "Datatilgang\nSlik kan man inspisere dataene fra en terminal inne i tjenesten:\n\nÅpne en instans av Jupyter med data fra bøtter\nÅpne en terminal inne i Jupyter\nGå til mappen med bøttene ved å kjøre dette fra terminalen cd /buckets\nKjør ls -ahl i teminalen for å se på hvilke bøtter som er montert.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#installere-pakker",
    "href": "statistikkere/jupyter.html#installere-pakker",
    "title": "Jupyter",
    "section": "Installere pakker",
    "text": "Installere pakker\nSiden det nesten ikke er installert noen pakker i tjenesten så kan brukeren opprette et ssb-project og installere pakker som vanlig.\nFor å bygge et eksisterende ssb-project så kan brukeren også bruke ssb-project.\nFor å installere R-pakker følger man beskrivelsen for renv.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#slette-tjenesten",
    "href": "statistikkere/jupyter.html#slette-tjenesten",
    "title": "Jupyter",
    "section": "Slette tjenesten",
    "text": "Slette tjenesten\nFor å slette tjenesten kan man trykke på Slette-knappen i Dapla Lab under Mine tjenester. Når man sletter en tjeneste så sletter man hele disken inne i tjenesten og frigjør alle ressurser som er reservert. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#pause-tjenesten",
    "href": "statistikkere/jupyter.html#pause-tjenesten",
    "title": "Jupyter",
    "section": "Pause tjenesten",
    "text": "Pause tjenesten\nMan kan pause tjenesten ved å trykke på Pause-knappen i Dapla Lab under Mine tjenester. Når man pauser så slettes alt påden lokale disken som ikke er lagret under $HOME/work. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#monitorering",
    "href": "statistikkere/jupyter.html#monitorering",
    "title": "Jupyter",
    "section": "Monitorering",
    "text": "Monitorering\n\n\nMan kan moniterere en instans av Jupyter ved å trykke på Jupyter-teksten under Mine tjenester i Dapla Lab, slik som vist i Figur 10.\nDenne funksjonaliteten er under arbeid og mer informasjon kommer snart.\n\n\n\n\n\n\n\n\n\nFigur 10: Monitorering av Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/jupyter.html#footnotes",
    "href": "statistikkere/jupyter.html#footnotes",
    "title": "Jupyter",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nKortnavnet til en delt-bøtte kan leses ut av Dapla Ctrl eller hentes fra selve bøttenavnet. F.eks. er det kortnavnet til delt-bøtta ssb-vof-data-delt-rollebase-fnr-prod bare rollebase-fnr. Det tekniske teamnavnet er vof.↩︎",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jupyter"
    ]
  },
  {
    "objectID": "statistikkere/poetry-ssb-project.html",
    "href": "statistikkere/poetry-ssb-project.html",
    "title": "Pakkehåndtering i Python",
    "section": "",
    "text": "I tillegg til å opprette GitHub repoer etter vår SSB-mal hjelper SSB-project deg med å lage kernels og holde styr på Python-pakkene dine ved bruk av Poetry",
    "crumbs": [
      "Manual",
      "Kode",
      "Pakkehåndtering i Python"
    ]
  },
  {
    "objectID": "statistikkere/poetry-ssb-project.html#installere-pakker",
    "href": "statistikkere/poetry-ssb-project.html#installere-pakker",
    "title": "Pakkehåndtering i Python",
    "section": "Installere pakker",
    "text": "Installere pakker\n\n\n\n\n\n\nNoteForsikre deg om at pakken er trygg!\n\n\n\n\n\nFør du installerer en pakke bør gjøre følgende for å sikre deg at du ikke installerer en pakke med skadelig kode:\n\nSøk opp pakken på PyPI.\nSjekk om pakken er et populært/velkjent prosjekt ved å besøke repoet der koden ligger. Antall Stars og Forks på gitHub er en grei indikasjon på dette.\nHvis du er i tvil om pakken er trygg å installere, så kan du spørre kollegaer om de har erfaring med den, eller spørre på en egnet Yammer-kanal i SSB.\nHvis du fortsatt ønsker å installere pakken så anbefaler vi å copy-paste navnet fra PyPi, ikke skrive det inn manuelt når du installerer.\n\n\n\n\nNår du har opprettet et ssb-project kan du installere de python-pakkene du trenger fra PyPI.\nSelve installeringen av pakken gjøres enkelt på følgende måte:\n\nÅpne en terminal i Jupyterlab.\nGå inn i prosjektmappen din ved å skrive:\n\n\n\nterminal\n\ncd &lt;sti til prosjektmappe&gt;\n\n\nLag en branch/utviklingsbranch som f.eks. heter install-pandas:\n\n\n\nterminal\n\ngit switch -c install-pandas\n\n\nInstaller, f.eks. Pandas, ved å skrive følgende:\n\n\n\nterminal\n\npoetry add pandas\n\n\n\n\n\n\n\nFigur 1: Installasjon av Pandas med ssb-project\n\n\n\nFigur 1 viser hvordan dette vil se ut i en Jupyterlab-terminal. Kommandoen for å installere noe er poetry add etterfulgt av pakkenavnet. Vi ser også at den automatisk legger til Pandas-versjonen i filen poetry.lock.\nDu kan også spesifisere en konkret versjon av pakken som skal installeres med følgende kommando:\n\n\nterminal\n\npoetry add pandas@1.2.3\n\n\nAvinstallere pakker\nOfte eksperimenterer man med nye pakker, og alle blir ikke med videre i produksjonskoden. Det er god praksis å fjerne pakker som ikke brukes, blant annet for å unngå at de blir en sikkerhetsrisiko. Det gjør du enkelt ved å skrive følgende i terminalen:\n\n\nterminal\n\npoetry remove pandas\n\n\n\nOppdatere pakker\nHvis det kommer en ny versjon av en pakke du bruker, så kan du oppdatere den med følgende kommando:\n\n\nterminal\n\npoetry update pandas\n\nhvis du kjører poetry update uten noe pakkenavn, så vil alle pakkene dine oppdateres til siste versjon, med mindre du har spesifisert versjonsbegrensninger i pyproject.toml-filen.\n\n\nUndersøk avhengigheter\nHvis du lurer på hvilke pakker som har hvilke avhengigheter, så kan du lett liste ut dette i terminalen med følgende kommando:\n\n\nterminal\n\npoetry show --tree\n\nDet vil gi en grafisk fremstilling av avhengighetene som vist i Figur 2.\n\n\n\n\n\n\nFigur 2: Visning av pakke-avhengigheter i ssb-project",
    "crumbs": [
      "Manual",
      "Kode",
      "Pakkehåndtering i Python"
    ]
  },
  {
    "objectID": "statistikkere/poetry-ssb-project.html#push-til-github",
    "href": "statistikkere/poetry-ssb-project.html#push-til-github",
    "title": "Pakkehåndtering i Python",
    "section": "Push til GitHub",
    "text": "Push til GitHub\nNår du nå har installert en pakke så har filen poetry.lock endret seg. For at dine samarbeidspartnere skal få tilgang til denne endringen i et SSB-project, så må du pushe en ny versjon av poetry.lock-filen opp Github, og kollegaene må pulle ned og bygge prosjektet på nytt. Du kan gjøre dette på følgende måte etter at du har installert en ny pakke:\n\nVi kan stage alle endringer med følgende kommando i terminalen når vi står i prosjektmappen:\n\n\n\nterminal\n\ngit add -A\n\n\nDeretter commite en endring, dvs. ta et stillbilde av koden i dette øyeblikket, ved å skrive følgende:\n\n\n\nterminal\n\ngit commit -m \"Installert pandas\"\n\n\nPush det opp til GitHub1. Anta at vi gjorde dette i branchen install-pandas som ble opprettet tidligere. Da kan vi skrive følgende :\n\n\n\nterminal\n\ngit push --set-upstream origin install-pandas\n\nDeretter kan kollegaene dine pulle ned endringene og bygge prosjektet på nytt. Vi forklarer hvordan man kan bygge prosjektet på nytt senere i kapitlet.\n\nHold prosjektet oppdatert\nHvis du sitter med en lokal kopi av prosjektet ditt, og flere andre jobber med den samme kodebasen, så er det viktig at du holder din lokale kopi oppdatert. Hvis du jobber i en branch på en lokal kopi, bør du holde denne oppdatert med main-branchen på GitHub. Det er vanlig Git-praksis. Når man også bruker ssb-project, så man huske å også bygge prosjektet på nytt hver gang det er endringer som er gjort av andre i poetry.lock.-filen.",
    "crumbs": [
      "Manual",
      "Kode",
      "Pakkehåndtering i Python"
    ]
  },
  {
    "objectID": "statistikkere/poetry-ssb-project.html#dependabot",
    "href": "statistikkere/poetry-ssb-project.html#dependabot",
    "title": "Pakkehåndtering i Python",
    "section": "Dependabot",
    "text": "Dependabot\nNår man installerer pakker så vil det etter hvert utvikle seg et sett av potensielt kompliserte avhengigheter mellom disse pakkene. Dette skyldes at en pakke kan benytte seg av funksjonalitet i andre pakker, som igjen benytter seg av funksjonalitet i andre pakker, osv.. Hvis noen finner en sikkerhetssårbarhet i en pakke så kan det fikses ved at en ny versjon av den pakken slippes, som igjen kan få konsekvenser for pakker som er avhengig av denne.\nI SSB er det tilrettelagt for at alle som versjonshåndterer koden sin på GitHub kan skanne pakkene sine for sårbarheter og nye versjoner av pakker med Dependabot. Dependabot hjelper oss med å finne og fikse sårbarheter og gamle pakkeversjoner. Dette er spesielt viktig når man installerer sine egne pakker.\nDependabot sjekker med jevne mellomrom om det finnes oppdateringer i pakkene som er listet i din pyproject.toml-fil, og den tilhørende poetry.lock. Hvis det finnes oppdateringer så vil den lage en pull request som du kan godkjenne. Når du godkjenner den så vil den oppdatere poetry.lock-filen og lage en ny commit som du kan pushe til GitHub. Dependabot gir også en sikkerhetsvarslinger hvis det finnes kjente sårbarheter i pakkene du bruker.\nDet er anbefalt at alle som installerer sine egne pakker i SSB skrur på Dependabot i sine GitHub-repoer.\n\nAktivere Dependabot\nDu kan aktivere Dependabot ved å gi inn i GitHub-repoet ditt og gjøre følgende:\n\nGå inn repoet\nTrykk på Settings for det repoet som vist på Figur 3.\n\n\n\n\n\n\n\nFigur 3: Åpne Settings for et GitHub-repo.\n\n\n\n\nI menyen til venstre velger du Code security and analysis\nUnder seksjonen Dependabot velger Enable på minst Dependabot alerts og Dependabot security updates, slik som vist i Figur 4.\n\n\n\n\n\n\n\nFigur 4: Skru på Dependabot i GitHub.\n\n\n\nNår du har gjort dette vil GitHub varsle deg hvis det finnes en kjent sårbarhet i pakkene som benyttes.\n\n\nOppdatere pakker\nHvis en av pakkene du bruker kommer med en oppdatering, så vil Dependabot lage en pull request (PR) i GitHub som du kan godkjenne. Dependabot sjekker også om oppdateringen er i konflikt med andre pakker du bruker. Hvis det er tilfellet så vil den lage en pull request som oppdaterer alle pakkene som er i konflikt.\nHvis en av pakkene du bruker har en kjent sikkerhetssårbarhet, så vil Dependabot varsle deg om dette under Security-fanen i GitHub-repoet ditt. Hvis du trykker på View Dependabot alerts så vil du få en oversikt over alle sårbarhetene som er funnet, og hvilken alvorlighetsgrad den har. Hvis du trykker på en av sårbarhetene så vil du få mer informasjon om den, og du kan trykke på Create pull request for å oppdatere pakken.\nSom nevnt over kan du enkelt oppdatere pakker fra GitHub ved hjelp av Dependabot. Men det finnes tilfeller der du vil teste om en oppdatering gjør at deler av koden din ikke fungerer lenger. Anta at du bruker en Python-pakken Pandas i koden din, og at du får en pull request fra Dependabot om å oppdatere den fra versjon 1.5 til 2.0. Hvis du ønsker å teste om koden din fortsatt fungerer med den nye versjonen av Pandas, så kan du gjøre dette i Jupyterlab ved å følge ved å lage en branch som f.eks. heter update-pandas. Deretter kan du installere den nye versjonen av Pandas med følgende kommando fra terminalen:\n\n\nterminal\n\npoetry update pandas@2.0\n\nHvis du nå kjører koden din kan du teste om den fortsatt fungerer som forventet. Gjør den ikke det kan du tilpasse koden din og pushe endringene til Github. Deretter kan du godkjenne pull requesten fra Dependabot og merge den. Etter dette kan du slette den PR-en som Dependabot lagde for deg.",
    "crumbs": [
      "Manual",
      "Kode",
      "Pakkehåndtering i Python"
    ]
  },
  {
    "objectID": "statistikkere/poetry-ssb-project.html#footnotes",
    "href": "statistikkere/poetry-ssb-project.html#footnotes",
    "title": "Pakkehåndtering i Python",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nÅ pushe til GitHub uten å sende ved Personal Access Token fordrer at du har lagret det lokalt så Git kan finne det. Her et eksempel på hvordan det kan gjøres.↩︎",
    "crumbs": [
      "Manual",
      "Kode",
      "Pakkehåndtering i Python"
    ]
  },
  {
    "objectID": "statistikkere/standarder.html",
    "href": "statistikkere/standarder.html",
    "title": "Standarder",
    "section": "",
    "text": "Når man driver statistikkproduksjon er det flere regler man må forholde seg til.\nStandardutvalget står for utarbeidingen av flere av disse reglene og standardene på Dapla, for eksempel navnestandarden som er beskrevet i artikkelen navnestandard.\n\n\n\n\n\n\nNoteKort om standardutvalget!\n\n\n\n\n\n\nAnsvarlig for det sentrale standardiseringsarbeidet knyttet til bruk og gjenbruk av data i SSB.\nBeslutningsmydnighet fra DM, men prinsipielle saker, og saker med store konsekvenser, sendes til DM for vedtak.\nUtvikler hovedsaklig obligatoriske standarder, men statistikkprodusenter oppfordres også til å gjøre seg kjent med anbefalingene.\nLenke: Standardutvalgets side på byrånettet\nLenke: Vedtak fra Standardutvalget\n\n\n\n\nDenne artikkelen er i stor grad basert på standardutvalgets mandat og standardutvalgets side på byrånettet.\n\nStandardutvalget\nStandardutvalget har ansvar for det sentrale standardiseringsarbeidet knyttet til bruk og gjenbruk av data i SSB. Alle avdelinger er representert med unntak av avdeling 100. Standardutvalgets side på byrånettet inneholder en oversikt over utvalgsmedlemmene.\nFormålet med utvalget, slik det er beskrevet i mandatet fra 2023, er å legge grunnlag for effektiv bruk og gjenbruk av data som SSB samler inn, bearbeider og forvalter.\nStandardutvalget har beslutningsmyndighet delegert fra DM og kan dermed vedta krav, regler, anbefalinger eller obligatoriske standarder.\n\nHvilke områder jobber standardutvalget med?\nStandardutvalget utvikler standarder innenfor følgende områder:\n\nKodeverk (klassifikasjoner og kodelister)\nVariabler og variabelnavn\nEnhetstyper\nNavngivning, versjonering, dokumentasjon og lagring av datasett og populasjoner i alle ledd i produksjonsprosessen\nKvalitetsindikatorer\nProsessdata\nDatatilstander\nInformasjonsmodeller\n\nStandardutvalget har ikke ansvar for standarder innenfor koding og statistiske metoder. Det håndteres av KVAKK (Kvalitet i kode og koding) og s811 - Seksjon for metoder.\n\n\n\nHvilke standarder finnes?\n\n1. DataDoc - dokumentasjon av datasett\n\nDataDoc - Krav til dokumentasjon av datasett på Dapla (Confluence)\nDatadoc-editor - Artikkel i Dapla-manualen\n\n\n\n2. Datatilstander i SSB\n\nDatatilstander - Artikkel i Dapla-manualen\nInterne dokumenter - Datatilstander, skrevet av Standardutvalget\n\n\n\n\n\n\n\nFigur 1: En grafisk fremstilling av forskjellene mellom datatilstandene i SSB (Standardutvalget 2023).\n\n\n\n\n\n3. VarDef - dokumentasjon av variabler\n\nVarDef - Confluence-side\n\n\n\n4. Standardformater for lagring av data\n\nStandardformater - Confluence-side\n\n\n\n5. Navnestandard for henholdsvis:\n\nEnhetstypeidentifikatorer - DM-vedtak (internet dokument)\nGitHub repoer - Internt dokument\nNøkkelvariabler (anbefaling) - Byrånettside\nDatasett - Dapla-manualen: Navnestandard (og versjonering)\n\n\n\n6. Kvalitetsindikatorer\n\nStandardutvalget har definert et sett med anbefalte kvalitetsindikatorer for statistikkproduksjon - særlig kvantitative indikatorer\nInternt dokument - Anbefalte kvalitetsindikatorer i statistikkproduksjonen\nInternt dokument - Mal for dokumentasjon av kvalitetsindikatorer\n\n\n\n7. Editering - prinsipper og retningslinjer\n\nOffentlig dokument - Prinsipper og retningslinjer for dataeditering\nDokumentet lenket i punktet over inneholder ni prinsipper for editering i SSB, blant annet:\n\nFormålet med dataediteringen skal være klart formulert\nKontrollene, kontrollustlagene og endringene skal være veldokumenterte\nAutomatiser editeringsprosessen så mye som mulig\n\n\n\n\n8. Kodelister (anbefaling)\n\nLes om de anbefalte standardiserinenge i byrånettsiden for standardutvalgets vedtak\n\n\n\n\n\n\n\nReferanser\n\nStandardutvalget. 2023. “Datatilstander i SSB.” Statistisk sentralbyrå. https://ssbno.sharepoint.com/sites/Internedokumenter/Delte%20dokumenter/Forms/AllItems.aspx?id=%2Fsites%2FInternedokumenter%2FDelte%20dokumenter%2FInterne%20dokumenter%202023%2F2023%2D14%20Datatilstander%20i%20SSB%2Epdf&parent=%2Fsites%2FInternedokumenter%2FDelte%20dokumenter%2FInterne%20dokumenter%202023.",
    "crumbs": [
      "Manual",
      "Standarder"
    ]
  },
  {
    "objectID": "statistikkere/git-grensesnitt.html",
    "href": "statistikkere/git-grensesnitt.html",
    "title": "Git arbeidsflyt i Jupyter",
    "section": "",
    "text": "I denne artikkelen viser vi hvordan man gjennomfører anbefalt git arbeidsflyt i Jupyter med Git-grensesnittet.\n\n1. git clone\n\n\n\n\n\n\nFigur 1\n\n\n\n\n\n\n\n\n\nFigur 2\n\n\n\n\n\n2. git switch main + git pull\n\n\n\n\n\n\nFigur 3\n\n\n\n\n\n3. git status\n\n\n\n\n\n\nFigur 4\n\n\n\n\n\n4. git add\n\n\n\n\n\n\nFigur 5\n\n\n\n\n\n5. git commit\n\n\n\n\n\n\nFigur 6\n\n\n\n\n\n6. git push\n\n\n\n\n\n\nFigur 7\n\n\n\n\n\n7. git fetch\n\n\n\n\n\n\nFigur 8\n\n\n\n\n\n8. git merge origin/main\n\n\n\n\n\n\nFigur 9\n\n\n\n\n\n\n\n\n\nFigur 10\n\n\n\n\n\n9. git push\n\n\n\n\n\n\nFigur 11"
  },
  {
    "objectID": "statistikkere/dapla-overgang.html",
    "href": "statistikkere/dapla-overgang.html",
    "title": "Hvordan flytte statistikkproduksjon til Dapla?",
    "section": "",
    "text": "I denne artikkelen beskriver vi hva man bør gjøre og hvordan man bør jobbe for å få en statistikk over på Dapla. Rekkefølgen punktene er i er ikke tilfeldig men med unntak av første punkt - planlegging - kan rekkefølgen på hvordan man gjør ting variere.",
    "crumbs": [
      "Manual",
      "Kom i gang",
      "Hvordan flytte statistikkproduksjon til Dapla?"
    ]
  },
  {
    "objectID": "statistikkere/dapla-overgang.html#footnotes",
    "href": "statistikkere/dapla-overgang.html#footnotes",
    "title": "Hvordan flytte statistikkproduksjon til Dapla?",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nDu kan velge prosjekt øverst på siden, til høyre for teksten Google Cloud. I bildet under ser du at hvordan det ser ut når prosjektet dapla-felles-p er valgt.↩︎",
    "crumbs": [
      "Manual",
      "Kom i gang",
      "Hvordan flytte statistikkproduksjon til Dapla?"
    ]
  },
  {
    "objectID": "statistikkere/backup-data.html",
    "href": "statistikkere/backup-data.html",
    "title": "Backup",
    "section": "",
    "text": "WarningUnder arbeid\n\n\n\nUtrulling av løsning for sikkerhetskopi av data på Dapla rulles ut til alle Dapla-team ila oktober 2025. Funksjonaliteten beskrevet i artikkelen vil gjelde fra løsningen er rullet ut for et team. Bevaringspolicy i bøttene kan benyttes nå.\nSSBs evne til å gjenopprette data ved katastrofer, samt ved tilsiktet eller utilsiktet tap eller korrupsjon av data, ivaretas gjennom 2 ulike løsninger på Dapla:\nDisse løsningene gjelder for de standardiserte bøttene som tilbys av plattformen gjennom features som dapla-buckets og shared-buckets. Det er disse ressursene som typisk benyttes av statistikkproduserende managed Dapla-team.\nDapla-team som oppretter sine egne ressurser, eller har spesielle behov knyttet til sikkerhetskopier, må selv sørge for at disse bli ivaretatt.",
    "crumbs": [
      "Manual",
      "Data",
      "Backup"
    ]
  },
  {
    "objectID": "statistikkere/backup-data.html#ekstern-sikkerhetskopi",
    "href": "statistikkere/backup-data.html#ekstern-sikkerhetskopi",
    "title": "Backup",
    "section": "Ekstern sikkerhetskopi",
    "text": "Ekstern sikkerhetskopi\nEt Dapla-team som har featuren dapla-buckets og shared-buckets aktivert vil automatisk ha sikkerhetskopi på følgende bøtter:\n\nKildebøtte\nssb-&lt;team&gt;-data-kilde-prod\nProduktbøtte\nssb-&lt;team&gt;-data-produkt-prod\nDelt-bøtter\nssb-&lt;teamnavn&gt;-data-delt-&lt;kortnavn&gt;-prod\nssb-&lt;teamnavn&gt;-data-delt-delomat-&lt;kortnavn&gt;-prod\n\n\n\n\n\n\n\nCautionUnntak for en mappe\n\n\n\nMapper som heter temp/ eller tmp/, og som ligger på toppnivå i mappestrukturen, vil ikke inkluderes i sikkerhetskopien. F.eks. hvis du lagrer data i ssb-&lt;team&gt;-data-produkt-prod/temp/ så vil dataene i denne mappen IKKE sikkerhetskopieres, mens data lagres i ssb-&lt;team&gt;-data-produkt-prod/inndata/temp/ vil sikkerhetskopieres. Bakgrunnen for unntaket er å gi brukere muligheten å jobbe med midlertidige filer som ikke krever sikkerhetskopi.\n\n\nSikkerhetskopier tas automatisk hver dag kl. 21, og gjenopprettingspunkter lagres i inntil 500 dager.\nDette betyr at du kan gjenopprette filer fra en hvilken som helst dag de siste 500 dagene.\n\nGjenoppretting av filer\nGjenoppretting av filer fra en sikkerhetskopi gjøres på følgende måte:\n\nGå til https://ssb.pureservice.com/\n\nVelg “Skjema for gjenoppretting av data”\n\nFyll ut følgende felter i skjemaet:\n\nPlattform: velg Dapla\n\nFilsti: angi full sti til filen eller mappen som skal gjenopprettes\n\nGjenopprettingsdato: datoen du ønsker å hente filen fra\n\nSkal filen overskrives?: velg Ja eller Nei avhengig av behov\n\n\nSend inn skjemaet\n\nKundeservice vil behandle forespørselen og gjenopprette filene i henhold til informasjonen du har gitt.\nDet er mulig å gjenopprette filer, mapper og bøtter.",
    "crumbs": [
      "Manual",
      "Data",
      "Backup"
    ]
  },
  {
    "objectID": "statistikkere/backup-data.html#bevaringspolicy-i-gcs",
    "href": "statistikkere/backup-data.html#bevaringspolicy-i-gcs",
    "title": "Backup",
    "section": "Bevaringspolicy i GCS",
    "text": "Bevaringspolicy i GCS\nAlle bøttene som opprettes i featuren dapla-buckets eller shared-buckets har en bevaringspolicy som gjør at slettede eller overskrevne objekter kan gjenskapes i en viss periode.\nFiler som overskrives eller slettes blir satt til noncurrent. En non-current versjon blir slettet hvis den er mer enn 180 dager gammel og det er minst 2+ eller 3+ nyere versjoner i bøtta, avhengig av type bøtte:\nKildebøtta\nI kildebøtta blir non-current versjoner slettet hvis det er mer enn 3 nyere versjoner.\nProduktbøtta\nI produktbøtta blir non-current versjoner slettet hvis det er mer enn 2 nyere versjoner.\nFor å gjennopprette en non-current eller soft-deleted versjon så benytte funksjonalitet fra dapla-toolbelt. For å gjenskape en versjon så må man først bruke funksjonen get_versions() liste ut ID-en til versjonen:\n\n\nNotebook\n\nfrom dapla import FileClient\n\n# Set bucket name and folder name(if any)\nbucket = \"ssb-dapla-felles-data-produkt-prod\" \nfolder = \"restore\"\n\nFileClient.get_versions(bucket, folder)\n\nOutput\n[&lt;Blob: ssb-dapla-felles-data-produkt-prod, restore/data1.parquet, 1717762669778835&gt;,\n &lt;Blob: ssb-dapla-felles-data-produkt-prod, restore/data1.parquet, 1718015249969499&gt;,\n &lt;Blob: ssb-dapla-felles-data-produkt-prod, restore/data2.parquet, 1717762673242818&gt;,\n &lt;Blob: ssb-dapla-felles-data-produkt-prod, restore/data3.parquet, 1717762677832930&gt;]\nFra output over ser vi filen data1.parquet finnes i 2 versjoner. Vi kan undersøke den nærmere med følgende kode:\n\n\nnotebook\n\nfiles = FileClient.get_versions(\"ssb-dapla-felles-data-produkt-prod\", \"restore/data1.parquet\")\n\nfor file in files:\n    print(\"Name          : \",file.name)\n    print(\"Generation Id : \", file.generation)\n    print(\"Updated on    : \", file.updated)\n    print(\"Deleted on    : \", file.time_deleted)\n    print(\"------------------------------------------\")\n\nOutput\nName          :  restore/data1.parquet\nGeneration Id :  1718436304922143\nUpdated on    :  2024-06-15 07:25:04.928000+00:00\nDeleted on    :  2024-06-24 11:10:10.807000+00:00\n------------------------------------------\nName          :  restore/data1.parquet\nGeneration Id :  1719227410801992\nUpdated on    :  2024-06-24 11:10:10.807000+00:00\nDeleted on    :  None\n------------------------------------------\nAv output over ser vi at det er en fil som som har en verdi i Deleted on feltet og som kan gjenskapes. Ønsker vi å gjenskape versjonen ved å bruke restore_version()og referere til Generation ID:\n\n\nnotebook\n\nFileClient.restore_version( source_bucket_name=\"ssb-dapla-kildomaten-data-delt-test\",\n                            source_file_name=\"restore/data3.parquet\",\n                            source_generation_id=1718436304922143,\n                            new_name=\"\" (optional)\n                           )\n\nOver har vi gjenopprettet en tidligere versjon av fil. Dette betyr at det nå er en ny Live-versjon av filen, og at den som tidligere var Live har blitt non-current. Ved gjenoppretting av non-current-versjoner så kan man også spesifisere nytt navn på filen med parameteret new_name= i restore_version()-funksjonen.\n\n\n\n\n\n\nNoteHvorfor kan jeg ikke bruke Google Cloud Console (GCC)?\n\n\n\nMan kan liste ut Live, non-current og soft-deleted versjoner fra GCC, som vist i Figur 1. Men man har ikke tilgang til Object details og kan derfor ikke hente ut ID-en til versjonen. Til dette må man benytte dapla-toolbelt fra Dapla Lab.\n\n\n\n\n\n\nFigur 1: Hvordan liste ut versjoner fra GCC.",
    "crumbs": [
      "Manual",
      "Data",
      "Backup"
    ]
  },
  {
    "objectID": "statistikkere/renv.html",
    "href": "statistikkere/renv.html",
    "title": "Pakkehåndtering i R",
    "section": "",
    "text": "R-pakken, renv, er et verktøy som lar oss opprette et miljø for R og installere pakker. Det er anbefalt å bruke renv for å sikre at alle som jobber med prosjektet har samme versjon av pakkene. I tillegg er det enkelt å dele prosjektet med andre.\n\nHvor skal jeg kjøre kommandoer for renv?\nNår du oppretter et renv-miljø, installerer eller oppdaterer pakker må du kjøre noen kommandoer. Hvor disse skal kjøres er avhengig av om du bruker RStudio, eller Jupyter.\n\nJupyterRStudio\n\n\nI jupyter anbefaler vi at du oppretter renv-miljøet og installerer pakker fra R i terminalen. For å starte R i terminalen:\n\nÅpne en terminal fra Launcher\nStå i mappen der du vil aktivere det virtuelle miljøet/installere pakker, dvs. prosjekt mappen.\nStarte R ved å skrive in R\n\nHer er en kort video (uten lyd) for å vise dette:\n\n\n\n\n\n\n\nI Rstudio kan du fint kjøre kommandoene direkte i console. Du finner console vanligvis nederste på venstre i RStudio.\n\n\n\n\n\nStarte renv i et eksisterende prosjekt\nFor å installere R-pakker må du opprette et renv-miljø med renv. Dette kan gjøres ved å kjøre:\nrenv::init()\n\nJupyterRStudio\n\n\nFor å opprette et renv-miljø i Jupyter, navigere først til hovedmappen for prosjektet i terminalen og starter R. Etterpå, kan du kjøre renv::init().\n\n\n\n\n\n\n\nInitialisere renv-miljø i RStudio med renv::init() i console.\n\n\n\n\n\n\nWarning\n\n\n\nFor at renv skal fungere i RStudio er det viktig å jobbe i et R-project. Dvs. at du har en prosjekt-fil (.Rproj) i GitHub repoen din.\nHvis du ikke har det, kan du opprette en ved å velge File &gt; New Project… &gt; Version Control &gt; Git &gt; og lim inn git adressen for å clone ned repoen på nytt. Da vil det også kommer en .Rproj fil. Denne kan fint ligger på repoen.\n\n\n\n\n\nKommandoen renv::init() setter opp et renv-miljø i mappen du står i. Hvis du jobber i Jupyter, sjekk at du står i root av repoen (hovedmappen til repository).\nRent praktisk vil det si at du fikk følgende filer/mapper i mappen din:\n\nrenv.lock\nEn fil som inneholder versjoner av alle pakker du benytter i koden din.\n.Rprofile En fil som inneholder informasjon om oppsetting av miljø.\nrenv\nMappe som inneholder alle pakkene du installerer.\nrenv/activate.R En fil som aktiverer renv miljøet for et prosjekt.\n\n\n\nTa i bruk renv-miljø\nFor å ta i bruk et renv-miljø må det aktiveres før koden kjøres. Dette gjøres automatisk i både Jupyter og RStudio ved hjelp av renv/activate.R filen.\nFor at renv-miljø kan brukes i Jupyter må også følgende pakke installeres.\n\nJupyter\n\n\nFor å åpne .R filer som notebooks i det nye renv-miljø må pakken IRkernel være installert hvis du jobber i Jupyter. Dette er ikke automatisk installert ved oppstart av et nytt renv-miljø. Du kan installere dette ved å kjøre\nrenv::install(\"IRkernel\")\nFor at denne pakken skal være med i .lock-filen må det brukes et sted med for eksempel library(IRkernel) og lagres til renv.lock med renv::snapshot()-funksjonen (se lengere ned for detaljer)\n\n\n\n\n\n\nTip\n\n\n\nTidligere må også alle R-filene starte med renv::autoload() i Jupyter på Dapla. Dette er ikke nødvendig lenge med bruk av .R-filer, istedenfor .ipynb. Det er fortsatt nødvendig i produksjonssone (bakke-miljø).\n\n\n\n\n\n\n\nInstallering av pakker\nVi installerer pakker med funksjonen renv::install(). For eksempel, for å installere pakken PxWebApiData:\nrenv::install(\"PxWebApiData\")\n\n\n\n\n\nFor å installere R-pakker som ligger på ‘statisticsnorway’ området på GitHub må det spesifiseres foran pakkenavnet:\nrenv::install(\"statisticsnorway/ssb-klassr\")\n\n\n\n\n\nFor å installere en spesifikk versjon av en pakke kan du spesifisere dette med installering med @ og versjonsnummer. For eksempel å installere PxWEbApiData versjon 0.4.0:\nrenv::install(\"PxWebApiData@0.4.0\")\nFor å lagre versjonsnummer av de nye pakkene som er installerte til renv.lock filen, kjør:\nrenv::snapshot()\n\n\nDele prosjektet og renv-miljøet med andre\nFør du deler prosjektet forsikre det om at renv.lock-filen er oppdatert. Dette kan gjøres ved å kjøre renv::snapshot().\nFor at en pakke skal lagres i renv.lock må pakken benyttes ved library() et sted i prosjektet (på en .R eller .ipynb fil).\nFor å dele renv-miljøet som en del av prosjektet skal følgende filene være på github: renv.lock, .Rprofile og renv/activate.R\n\n\nTa i bruk et prosjekt og renv-miljø fra andre\nHvis prosjektet er opprettet av noen andre, og har blitt delt med deg, kan alle pakkene i prosjektet installeres samtidig. Clone repository kjør følgende for å installere alle nødvendige pakker:\nrenv:restore()\n\n\n\n\n\n\n\nAvinstallering\nIndividuelle pakker kan fjernes fra renv-miljøet ved renv::remove()-funksjonen. For eksempel:\nrenv::remove(\"PxWebApiData\")\nFor å fjerne fra renv.lock-filen også må du ta en snapshot() etterpå.\nrenv::snapshot()\nEn annen nyttig funksjon er renv::clean(). Dette fjerner alle pakker fra library som ikke er i bruk\nrenv::clean()\nIgjen må du ta en snapshot() for at endringer skal lagres på renv.lock-filen\n\n\nOppgradere pakker\nFor å oppgradere en pakke kan du bruke renv::update(). For eksempel, for å oppgradere PxWebApiData skriv:\nrenv::update(\"PxWebApiData\")\nHusk å ta en snapshot() etterpå for å lagre endringer til renv.lock-filen. Det betyr at du og andre kan gjenskape miljøet på nytt.\nrenv::snapshot()\n\n\nOppgradering av R (kun produksjonssone på bakken)\nVersjonen av R i produksjonssone oppgraderes jevnlig, i både Jupyter og RStudio. Dette er fordi operativsystemet og programmer som R er avhengig skal holdes oppdatert. For at R skal fungere optimalt må det oppgraderes ofte. Dette skaper noen utfordringer for renv-miljøer som er avhengig av en spesifikk versjon av R.\nHvis du plutselig får en feilmelding ved oppstart at R-versjonen er forskjellig fra det den som er oppgitt renv.lock-filen, har det trolig vært en oppgradering av R siden sist noen jobbet med koden. Følg denne oppskriften for å løse opp i problemene:\nKjør følgende kommandoer i R i terminal (Jupyter) eller i console (RStudio)\n\nOppgrader versjon av renv ved:\n\n\n\nterminal\n\nrenv::upgrade()\n\n\nOppgrader alle pakkene ved:\n\n\n\nterminal\n\nrenv::hydrate(update = \"all\")\n\n\nLagre renv.lock filen:\n\n\n\nterminal\n\nrenv::snapshot()\n\n\n\n\n\n\nFor mer informasjon kan du lese denne artikkelen om oppdatering av renv her\n\n\nTa det lang tid til å starte et renv-miljø?\nVed å aktivere et renv-miljø ved oppstart, søker R i alle programfiler for å sjekke at alle pakker som benyttes er installerte. Hvis du har en veldig stor repository kan dette ta lang tid. En løsning er å opprette en .renvignore fil som spesifisere hvilke filer skal ignoreres. For eksempel om alle R kode ligger i .R filer kan .renvignore inneholder linjen *.ipynb for å ignorere all notebookene om de finnes.\n\n\n\n\n\n\n\nFlytting mellom Jupyter på bakken og Dapla\nR-pakker installeres fra ulike sted når vi jobber på Dapla vs produksjonssone på bakken. For å bruke et renv-miljø i en repo som var laget på bakken, på Dapla, må vi endre adressen hvor pakkene skal installeres fra i renv.lock. Addressen skal se slik ut:\n\n\nrenv.lock\n\n\"Repositories\": [\n      {\n        \"Name\": \"CRAN\",\n        \"URL\": \"https://packagemanager.posit.co/cran/latest\"\n      }\n    ]\n\n\n\nKjøring mellom RStudio og Jupyter (prodsone)\nHvis du bytter mellom Jupyter og RStudio i prodsone, må du også endre adressen hvor pakkene skal installeres fra i renv.lock-filen. Disse to miljø kan har ulike versjoner av R og/eller ulike adresser å hente pakker fra. Det er anbefalt å holde deg til Jupyter eller RStudio om mulig.\n\n\nHvordan å løse problemer med pakkeinstallering i produksjonssone på bakken\nProsessen med å installere R-pakker på bakken er det samme som på Dapla. Noen pakker (for eksempel arrow og ROracle) kan være vanskelig å installere på grunn av 3. parti avhengigheter.\narrow\nHvis du sliter med installering av arrow kan du prøve å spesifisere versjonen som skal installeres. Det er foreløpige ulike versjoner av avhengigheter i RStudio og Jupyter pga de ulike systemer og oppsett.\n\nJupyterRStudio\n\n\nrenv::install(\"arrow@18.1.0.1\")\nPakken er ganske stor, så det kan ta tid å installere den.\n\n\nrenv::install(\"arrow@17.0.0.1\")\nPakken er ganske stor, så det kan ta tid å installere den.\n\n\n\nROracle\nPakken ROracle har systemavhengigheter som betyr at det kan være vanskelig å installere i produksjonssone på bakken i et renv-miljø. Hvis du opplever problemer, prøv å installere pakken fra en lokal versjon med følgende kommando i R:\nROracle_pakke &lt;- \"/ssb/bruker/felles/R_pakker/ROracle_1.4-1_R_x86_64-unknown-linux-gnu.tar.gz\"\n\ninstall.packages(\n  ROracle_pakke,\n  repos = NULL,\n  type = \"source\")",
    "crumbs": [
      "Manual",
      "Kode",
      "Pakkehåndtering i R"
    ]
  },
  {
    "objectID": "statistikkere/ssb-altinn-python.html",
    "href": "statistikkere/ssb-altinn-python.html",
    "title": "ssb-altinn-python",
    "section": "",
    "text": "ssb-altinn-python er en Python-pakke med funksjonalitet for å transformere XML-filer fra Altinn3 til et format som kan benyttes i ISEE.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "ssb-altinn-python"
    ]
  },
  {
    "objectID": "statistikkere/ssb-altinn-python.html#installasjon",
    "href": "statistikkere/ssb-altinn-python.html#installasjon",
    "title": "ssb-altinn-python",
    "section": "Installasjon",
    "text": "Installasjon\nssb-altinn-python er en pakke for å transformere kildedata. Siden kildedata skal prosesseres i Kildomaten så er pakken er tilgjengeliggjort der. Alle pakker som er tilgjengelig i Kildomaten er ferdiginstallert.\nDet er også mulig å installere i et ssb-project på DaplaLab og bruke det direkte fra Jupyter. Det kan være nyttig for å teste ut at koden fungerer som forventet på testdata.\n\n\nTerminal\n\npoetry add ssb-altinn-python",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "ssb-altinn-python"
    ]
  },
  {
    "objectID": "statistikkere/ssb-altinn-python.html#funksjonalitet",
    "href": "statistikkere/ssb-altinn-python.html#funksjonalitet",
    "title": "ssb-altinn-python",
    "section": "Funksjonalitet",
    "text": "Funksjonalitet\n\nisee_transform()\nFunksjonen isee_transform() flater ut XML-data til et pandas-dataframe med formatet som er nødvendig for å kunne laste data inn i ISEE.\nI den enkleste formen tar funksjonen kun en parameter - adressen til XML-filen som skal flates ut. Det er også mulig å mappe Altinn-feltnavn mot ønsket variabelnavn som benyttes i ISEE. En dictionary med variabelnavnene i Altinn som keys, og ønskede ISEE-variabelnavn som values. Denne dictionary’en settes så som en parameter i funksjonskallet til isee_transform().\nfrom altinn import isee_transform\n\nfile = \"gs://ssb-prod-dapla-felles-data-delt/altinn3/RA-0595/2023/2/6/810409282_460784f978a2_ebc7af7e-4ebe-4883-b844-66ee6292a93a/form_460784f978a2.xml\"\n\nmapping = {'kontAmbulForeDispJaNei':'ISEE_VAR1',\n           'kontAmbulForeDispAnt':'ISEE_VAR2',\n           'kontAmbulForeDriftAnt':'ISEE_VAR3',}\n\nisee_transform(file, mapping)\nNoen har behov for å flate ut flere tema enn bare SkjemaData, dette er mulig ved å legge ved en liste med temaer man ønsker flatet ut i funksjonskallet (tag_list). Defaultverdi på denne parameteren er SkjemaData, så hvis man ikke har behov for andre temaer så trenger man ikke bruke denne parameteren. Eksemplet under flater ut [‘SkjemaData’, ‘Kontakt’] fra en XML-fil fra Altinn3:\nfrom altinn import isee_transform\n\nfile = \"gs://ssb-prod-dapla-felles-data-delt/altinn3/RA-0595/2023/2/6/810409282_460784f978a2_ebc7af7e-4ebe-4883-b844-66ee6292a93a/form_460784f978a2.xml\"\n\nmapping = {'kontAmbulForeDispJaNei':'ISEE_VAR1',\n           'kontAmbulForeDispAnt':'ISEE_VAR2',\n           'kontAmbulForeDriftAnt':'ISEE_VAR3',}\n\ntags = ['SkjemaData', 'Kontakt']\n\nisee_transform(file, mapping, tag_list=tags)\nDet er mulig å flate ut checkbox-felter ved å legge feltene man ønsker å flate ut i en liste, og så bruke denne i funksjonskallet til isee_transform (checkbox_vars). Det er også mulig å legge ved en boolsk variabel (unique_code) som spesifiserer om kodene i checkbox-feltene man ønsker utflatet er unike eller ikke (True, False). Settes denne til False blir det nye feltnavnet lik det opprinnelige feltnavnet + koden som flates ut. Settes den til True, så blir det nye feltnavnet kun koden som flates ut. False er defaultverdi på unique_code. Det blir dannet en ny rad for hver kode som står i checkboxfeltet man ønsker utflatet, og den opprinnelige raden slettes.\nfrom altinn import isee_transform\n\nfile = 'gs://ra0187-01-altinn-data-staging-c629-ssb-altinn/2024/4/11/7d5b52259b89_de4a24aa-4948-48d8-b2e4-a0f2160a0bd0/form_7d5b52259b89.xml'\n\nmapping = {\n    \"storOkningOmsAarsak31\":\"ISEE_storOkningOmsAarsak31\",\n    \"storOkningOmsAarsak33\":\"ISEE_storOkningOmsAarsak33\",\n    \"omsForrigePerPrefill\":\"ISEE_omsForrigePerPrefill\",\n    }\n\ncheckboxList = [\"storOkningOmsAarsak\"]\n\nisee_transform(file=file, mapping=mapping, checkbox_vars=checkboxList, unique_code=False)\n\n\ncreate_isee_filename()\nDersom data skal overføres til ISEE, må csv-filen med dataene ha en spesifikk notasjon. Dette er noe funksjonen create_isee_filname() håndterer.\nfrom altinn import create_isee_filename\n\nfile = \"gs://ssb-prod-dapla-felles-data-delt/altinn3/RA-0595/2023/2/6/810409282_460784f978a2_ebc7af7e-4ebe-4883-b844-66ee6292a93a/form_460784f978a2.xml\"\n\ncreate_isee_filename(file)\n\n\nxml_transform()\nFunksjonen flater ut hele XML-filen og legger innholdet i en Pandas DataFrame. Funksjonen kan være nyttig for de som skal jobbe med Altinn3-data på Dapla.\nfrom altinn import xml_transform\n\nfile = \"gs://ssb-prod-dapla-felles-data-delt/altinn3/RA-0595/2023/2/6/810409282_460784f978a2_ebc7af7e-4ebe-4883-b844-66ee6292a93a/form_460784f978a2.xml\"\n\nxml_transform(file)\nDataframet som returneres fra funksjonen inneholder følgende kolonner:\nFELTNAVN\nInneholder også prefikset til området verdien er hentet fra, som Tidsbruk, Brukeropplevelse, Kontakt. Disse er ikke inkludert i isee_transform\nFELTVERDI\nVerdien tilhørende aktuelt FELTNAVN\nLEVEL\nEn liste med verdier/tellere som beskriver kompleksiteten og nivået til feltet. Denne kolonnen kan være nyttig hvis skjemaet inneholder komplekse data, som Tabell-i-tabell og Tabell-i-tabell-i-tabell. Da kan man sammen med isee_transform()-funksjonen koble disse sammen og lage egentilpassede løpenumre på felter som skal inn i ISEE.\nSiden det finnes mange ulike måter å bygge et skjema på, og ønske om å holde funskjonene så generiske som mulig uten for mange parametre, må man i noen tilfeller tilpasse lastefilen til ISEE med ekstra steg etter isee_transfom(). xml_transform() kan være til hjelp for å se kompleksiteten i et skjema, og så benytte opplysningene i FELTNAVN/LEVEL-kolonnene for å lage spesialtipassede løpenr m.m.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "ssb-altinn-python"
    ]
  },
  {
    "objectID": "statistikkere/ssb-altinn-python.html#lasting-til-isee",
    "href": "statistikkere/ssb-altinn-python.html#lasting-til-isee",
    "title": "ssb-altinn-python",
    "section": "Lasting til ISEE",
    "text": "Lasting til ISEE\nDet er skrevet en utfyllende blogg-post i Dapla-manualen som beskriver prosessen med lasting av Altinn 3 skjemaer til ISEE.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "ssb-altinn-python"
    ]
  },
  {
    "objectID": "statistikkere/tilgangsstyring.html",
    "href": "statistikkere/tilgangsstyring.html",
    "title": "Tilgangsstyring",
    "section": "",
    "text": "Hvert Dapla-team har sine egne lagringsområder for data som ingen andre har tilgang til, med mindre teamet eksplisitt velger å dele data med andre team. I tillegg har teamet tilgang til egne ressurser for å behandle dataene.\nDet er tilgangsgruppen managers som bestemmer hvilke personer som skal ha hvilke roller i et team, og dermed hvilke data de ulike team-medlemmene får tilgang til. Den som jobber med data kan bli plassert i tilgangsgruppene data-admins eller developers. Sistnevnte får tilgang til alle datatilstander utenom kildedata, mens data-admins er forhåndsgodkjent til å også å aksessere kildedata ved behov. Dermed er data-admins en priveligert rolle på teamet som er forbeholdt noen få personer.\n\n\n\n\n\n\nFigur 1: Datatilstander som et team sitt medlemmer har ilgang til.\n\n\n\nFigur 1 viser hvem som har tilgang til hvilke datatilstander. Som nevnt er data-admins ansett som forhåndsgodkjent til å aksessere kildedata ved behov. Måten dette er implementert på er at data-admins må aktivere denne tilgangen selv, ved å bruke et JIT-grensesnitt (Just-In-Time Access). Tilgangen krever en begrunnelse og bruken kan løpende monitoreres av managers for teamet.",
    "crumbs": [
      "Manual",
      "Dapla-team",
      "Tilgangsstyring"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html",
    "href": "statistikkere/vscode-python.html",
    "title": "Vscode-python",
    "section": "",
    "text": "Vscode-python er en tjeneste på Dapla Lab for utvikling av kode i Python1 og R. Tjenesten er spisset mot Python - man kan derfor ikke forvente like god støtte for R-arbeidsflyter som i RStudio. Målgruppen for tjenesten er brukere som skal skrive produksjonskode.\nSiden tjenesten er ment for produksjonskode så er det veldig få forhåndsinstallerte Python-pakker som er installert. Antagelsen er at brukerene/teamet heller bør installere de pakkene de trenger, framfor at alle pakker som alle brukere/team er forhåndsinstallert og skal dekke behovet til alle. Det reduserer kompleksitet i tjenesten og dermed sannsynligheten for feilsituasjoner.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#forberedelser",
    "href": "statistikkere/vscode-python.html#forberedelser",
    "title": "Vscode-python",
    "section": "Forberedelser",
    "text": "Forberedelser\nFør man starter Vscode-python bør man ha lest kapitlet om Dapla Lab og satt opp Git- og GitHub-konfigurasjonen under Min konto. Deretter gjør du følgende:\n\nLogg deg inn på Dapla Lab\nUnder Tjenestekatalog trykker du på Start-knappen for Vscode-python\nGi tjenesten et navn\nÅpne Vscode-python konfigurasjoner",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#konfigurasjon",
    "href": "statistikkere/vscode-python.html#konfigurasjon",
    "title": "Vscode-python",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nKonfigurasjonen av VSCode er nær identisk Jupyter-tjenesten sin konfigurasjon. Se dokumentasjonen for konfigurasjon av Jupyter.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#datatilgang",
    "href": "statistikkere/vscode-python.html#datatilgang",
    "title": "Vscode-python",
    "section": "Datatilgang",
    "text": "Datatilgang\nSlik kan man inspisere dataene fra en terminal inne i tjenesten:\n\nÅpne en instans av Vscode-python med data fra bøtter\nÅpne en terminal inne i Vscode-python\nGå til mappen med bøttene ved å kjøre dette fra terminalen cd /buckets\nKjør ls -ahl i teminalen for å se på hvilke bøtter som er montert.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#installere-pakker",
    "href": "statistikkere/vscode-python.html#installere-pakker",
    "title": "Vscode-python",
    "section": "Installere pakker",
    "text": "Installere pakker\nSiden det nesten ikke er installert noen pakker i tjenesten så kan brukeren opprette et ssb-project og installere pakker som vanlig.\nFor å bygge et eksisterende ssb-project så kan brukeren også bruke ssb-project.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#slette-tjenesten",
    "href": "statistikkere/vscode-python.html#slette-tjenesten",
    "title": "Vscode-python",
    "section": "Slette tjenesten",
    "text": "Slette tjenesten\nFor å slette tjenesten kan man trykke på Slette-knappen i Dapla Lab under Mine tjenester. Når man sletter en tjeneste så sletter man hele disken inne i tjenesten og frigjør alle ressurser som er reservert. Siden pakkene som er installert også ligger lagret på disken, betyr dette at pakkene må installeres på nytt etter at en tjeneste er slettet. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#pause-tjenesten",
    "href": "statistikkere/vscode-python.html#pause-tjenesten",
    "title": "Vscode-python",
    "section": "Pause tjenesten",
    "text": "Pause tjenesten\nMan kan pause tjenesten ved å trykke på Pause-knappen i Dapla Lab under Mine tjenester. Når man pauser så slettes alt påden lokale disken som ikke er lagret under $HOME/work. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#monitorering",
    "href": "statistikkere/vscode-python.html#monitorering",
    "title": "Vscode-python",
    "section": "Monitorering",
    "text": "Monitorering\n\n\nMan kan moniterere en instans av Vscode-python ved å trykke på Vscode-python-teksten under Mine tjenester i Dapla Lab, slik som vist i Figur 1.\n\n\n\n\n\n\n\n\n\nFigur 1: Monitorering av Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#extensions",
    "href": "statistikkere/vscode-python.html#extensions",
    "title": "Vscode-python",
    "section": "Extensions",
    "text": "Extensions\nVSCode følger med et sett med extensions ferdig installert. Disse kan per nå ikke installeres av brukeren selv.\n\nJupytext\nJupytext-filer kan jobbes med som notebooks i Jupyter. For å gjøre dette, må man legge til Jupytext som en Python-avhengighet i ditt Python-prosjekt:\npoetry add --group dev \"jupytext &gt;=1\"\n..og deretter velge din pakkes Python-versjon som å være interpreter. Dette gjør man ved å trykke på den røde boksen på bildet, og velge interpreteren på filstien &lt;PAKKENAVN&gt;/.venv/bin/python.\n\n\n\n\n\n\nFigur 2: Monitorering av Jupyter-tjenesten i Dapla Lab\n\n\n\nDeretter kan man høyreklikke på filen og trykke “Open as Jupyter Notebook”.\n\n\n\n\n\n\nFigur 3: Konfigurasjon av Git for Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/vscode-python.html#footnotes",
    "href": "statistikkere/vscode-python.html#footnotes",
    "title": "Vscode-python",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nVscode-python er web-versjonen av VS Code og er ikke helt identisk med desktop-versjonen av VS Code mange er kjent med. Blant annet er det kun extensions fra Open VSX Registry som kan installeres.↩︎",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vscode-python"
    ]
  },
  {
    "objectID": "statistikkere/ssb-project.html",
    "href": "statistikkere/ssb-project.html",
    "title": "SSB-project",
    "section": "",
    "text": "Note\n\n\n\nDenne artikkelen fokuserer på SSB-project som GitHub-mal. Vi har skrevet en egen artikkel for SSB-project som verktøy for å håndtere Python-pakker: Pakkehåndtering i Python.\nStatistikkproduksjon på Dapla må være reproduserbart, delbart og gjenkjennelig. SSB-project er et verktøy som hjelper deg med dette ved å gjøre følgende:\nVi mener at ssb-project er et naturlig sted å starte når man skal bygge opp koden i Python eller R. Det gjelder både på bakken og på sky. I denne delen av kapitlet forklarer vi deg hvordan du kan ta i bruk ssb-project.\nKort fortalt kan du kjøre denne kommandoen i en terminal:\nDa vil få en mappe som heter stat-testprod med følgende innhold:\nI tillegg lar ssb-project deg opprette et GitHub-repo hvis du ønsker. Hvis du velger å la ssb-project opprette et GitHub-repo for deg, så vil det også sette opp SSBs anbefalte GitHub-oppsett. Det er viktig for at du skal kunne dele koden din med andre i SSB på en sikker måte.",
    "crumbs": [
      "Manual",
      "Kode",
      "SSB-project"
    ]
  },
  {
    "objectID": "statistikkere/ssb-project.html#forberedelser",
    "href": "statistikkere/ssb-project.html#forberedelser",
    "title": "SSB-project",
    "section": "Forberedelser",
    "text": "Forberedelser\nFør du kan ta i bruk ssb-project så er det et par ting som må være på plass:\n\nDu må ha konfigurert Git etter SSB sin standard (les mer om hvordan her).\nHvis du ønsker at ssb-project også skal opprette et GitHub-repo for deg må også følgende være på plass:\n\nDu må ha en GitHub-bruker (les hvordan her)\nSkru på 2-faktor autentifisering for GitHub-brukeren din (les hvordan her)\nVære koblet mot SSBs organisasjon statisticsnorway på GitHub (les hvordan her)\nOpprette Personal Access Token (PAT) og godkjenne det for bruk mot statisticsnorway (les hvordan her)\n\n\nDet er også å anbefale at du lagrer PAT lokalt/i Dapla Lab slik at du ikke trenger å forholde deg til det når jobber med Git og GitHub. Hvis du har alt dette på plass så kan du bare fortsette å følge de neste kapitlene.",
    "crumbs": [
      "Manual",
      "Kode",
      "SSB-project"
    ]
  },
  {
    "objectID": "statistikkere/ssb-project.html#opprett-ssb-project",
    "href": "statistikkere/ssb-project.html#opprett-ssb-project",
    "title": "SSB-project",
    "section": "Opprett ssb-project",
    "text": "Opprett ssb-project\n\nUten GitHub-repo\nFor å opprette et nytt ssb-project uten GitHub-repo gjør du følgende:\n\nÅpne en terminal. De fleste vil gjøre dette i Jupyterlab på bakke eller sky og da kan de bare trykke på det blå ➕-tegnet i Jupyterlab og velge Terminal.\nFør vi kjører programmet må vi være obs på at ssb-project vil opprette en ny mappe der vi står. Gå derfor til den mappen du ønsker å ha den nye prosjektmappen i. For å opprette et prosjekt som heter stat-testprod så skriver du følgende i terminalen:\n\n\n\nterminal\n\nssb-project create stat-testprod\n\n\n\nHvis du stod i hjemmemappen din på når du skrev inn kommandoen over i terminalen, så har du fått mappestrukturen som vises i Figur 1. 1. Den inneholder følgende :\n\n.git-mappe som blir opprettet for å versjonshåndtere med Git.\nsrc-mappe som skal inneholde all koden som utgjør produksjonsløpet. src er kort for source\ntests-mappe som inneholder tester du skriver for koden din.\nLICENCE-fil som skal benyttes for public-repos i SSB.\npoetry.lock-fil som inneholder alle versjoner av Python-pakker som blir brukt.\npyproject.toml-fil som inneholder informasjon om prosjektet og hvilke pakker som blir brukt.\nREADME.md-fil som brukes for tekstlig innhold på GitHub-siden for prosjektet.\n\n\n\n\n\n\n\n\n\nFigur 1: Mappen som ble opprettet av ssb-project.\n\n\n\n\n\n\n\nMed Github-repo\nOver så opprettet vi et ssb-project uten å opprette et GitHub-repo. Hvis du ønsker å opprette et GitHub-repo også må du endre kommandoen over til:\n\n\nterminal\n\nssb-project create stat-testprod --github --github-token='blablabla'\n\nKommandoen over oppretter en mappestruktur slik vi så tidligere, men også et ssb-project som heter stat-testprod med et GitHub-repo med samme navn. Som du ser så må vi da sende med opsjonen --github og PAT med opsjonen --github-token='blablabla'. Repoet i GitHub ser da ut som i Figur 2. Hvis du ønsker å slippe å måtte forholde deg til PAT hver gang interagerer med GitHub, kan du følge denne beskrivelsen for å lagre den lokalt. Da kan droppe --github-token='blablabla' fra kommandoen over.\n\n\n\n\n\n\nFigur 2: GitHub-repo som er opprettet av ssb-project\n\n\n\n\n\n\n\n\n\nNår du har opprettet et nytt ssb-project, eller bygget et eksisterende prosjekt, så kan det ta rundt 30 sekunder før kernelen viser seg i Jupterlab-launcher. Vær tålmodig!",
    "crumbs": [
      "Manual",
      "Kode",
      "SSB-project"
    ]
  },
  {
    "objectID": "statistikkere/ssb-project.html#bygg-eksisterende-ssb-project",
    "href": "statistikkere/ssb-project.html#bygg-eksisterende-ssb-project",
    "title": "SSB-project",
    "section": "Bygg eksisterende ssb-project",
    "text": "Bygg eksisterende ssb-project\nNår vi skal samarbeide med andre om kode så gjør vi dette via GitHub. Når du pusher koden din til GitHub, så kan samarbeidspartnere pulle ned koden og jobbe videre med den. Men når de henter ned koden så vil de bare hente ned selve koden, ikke pakker og Python-versjonen som ble brukt. De må installere alt som du hadde installert. I tillegg trenger de en kernel hvis de skal jobbe i Jupyterlab. ssb-project gjør det svært enkelt å bygge opp det du trenger, siden det virtuelle miljøet har all informasjon om hva som trengs.\nFor at samarbeidspartneren din skal kunne bygge miljøet på nytt, må de ha gjort en minimal konfigurering av Git. Les mer om hvordan du frem for å gjøre dette her.\nFor å bygge opp et eksisterende miljø gjør du følgende:\n\nFørst må du kopiere prosjektet ned lokalt, eller klone repoet med git-terminologi\n\n\n\nterminal\n\ngit clone https://github.com/statisticsnorway/&lt;prosjektnavn&gt;\n\n\nGå inn i mappen du klonet\n\n\n\nterminal\n\ncd &lt;prosjektnavn&gt;\n\n\nSkape et virtuelt miljø og installere en tilsvarende Jupyter kernel med\n\n\n\nterminal\n\nssb-project build",
    "crumbs": [
      "Manual",
      "Kode",
      "SSB-project"
    ]
  },
  {
    "objectID": "statistikkere/ssb-project.html#slette-ssb-project",
    "href": "statistikkere/ssb-project.html#slette-ssb-project",
    "title": "SSB-project",
    "section": "Slette ssb-project",
    "text": "Slette ssb-project\nDet vil være tilfeller hvor man ønsker å slette et ssb-project, enten fordi man ikke trenger koden lenger eller fordi man bare testet litt.\n\nLokalt\nHvis man jobber med flere prosjekter så kan det fort bli mange Jupyter kerneler hengende igjen. Derfor er det også mulighet å kjøre\n\n\nterminal\n\nssb-project clean stat-testprod\n\nsom sletter Jupyter-kernelen og de installerte pakkene i prosjektet. Hvis du også ønsker å slette selve mappen med kode må du gjøre det manuelt2:\n\n\nterminal\n\nrm -rf ~/stat-testprod/\n\nProsjektmappen over lå direkte i hjemmemappen min og hjemmemappen på Linux kan alltid referes til med et tilda-tegn ~.\n\n\nArkiver GitHub-repo\nGitHub-repoer som er opprettet under SSB sin organinasjons statisticsnorway på GitHub kan ikke slettes, bare arkiveres. Grunnen er at hvis man oppdager en sårbarhet senere så er det viktig å kunne se repoet for å forstå hva som har skjedd.\nHvis du ikke trenger et GitHub-repo lenger kan man arkivere repoet. Det gjør du på følgende måte:\n\nGi inn i repoet Settings slik som vist med rød pil i Figur 3.\n\n\n\n\n\n\n\nFigur 3: Settings for repoet.\n\n\n\n\nUnder General scroller du deg ned til delen som heter Danger Zone og velger Archive this repository, slik som vist på Figur 4.\n\n\n\n\n\n\n\nFigur 4: Arkivering av et repo.\n\n\n\n\nI dialogboksen som dukker opp fyller du inn reponavnet som beskrevet og trykker på I understand the consequences, archive this repository.\n\nNår det er gjort så er repoet lesbart, men man kan ikke jobbe med det. Men som vi ser av @#fig-github-repo-settings-archive-warning kan man omgjøre arkiveringen senere hvis det skulle være ønskelig.",
    "crumbs": [
      "Manual",
      "Kode",
      "SSB-project"
    ]
  },
  {
    "objectID": "statistikkere/ssb-project.html#spark-i-ssb-project",
    "href": "statistikkere/ssb-project.html#spark-i-ssb-project",
    "title": "SSB-project",
    "section": "Spark i ssb-project",
    "text": "Spark i ssb-project\nFor å kunne bruke Spark i et ssb-project må man først installere pyspark. Det gjør du ved å skrive følgende i en terminal:\n\n\nterminal\n\npoetry add pyspark==$(pip show pyspark | grep Version | egrep -o \"([0-9]{1,}\\.)+[0-9]{1,}\") --no-dev\n\nHer installerer du samme versjon av pyspark som på Jupyterlab.\nVidere kan vi konfigurere Spark til å enten kjøre på lokal maskin eller på flere maskiner (såkalte clusters). Under beskriver vi begge variantene.\n\nLokal maskin\nOppsettet for Pyspark på lokal maskin er det enkleste å sette opp siden Pyspark vil ha direkte tilgang til det lokale filsystemet. Man kan bruke miljøvariabelen PYSPARK_PYTHON til å peke på det virtuelle miljøet, og dermed vil Pyspark også ha tilgang til alle pakkene som er installert der. I en notebook vil dette kunne settes opp slik:\n\n\nnotebook\n\nimport os\nimport subprocess\n\n# Finner filstien til det virtuelle miljøet\npython_path = subprocess.run(['poetry', 'run', 'which', 'python'],\n                             capture_output=True, text=True).stdout.rstrip('\\n')\n\nos.environ[\"PYSPARK_PYTHON\"] = python_path\nos.environ[\"PYSPARK_SUBMIT_ARGS\"] = os.environ[\"PYSPARK_LOCAL_SUBMIT_ARGS\"]\n\nNår du oppretter en Notebook og bruker den kernelen du har laget så må du alltid ha denne på toppen av notebooken. Det initialiserer Spark slik at det kan brukes i notebooken.\n\n\nnotebook\n\n%run /usr/local/share/jupyter/kernels/pyspark_local/init.py\n\nDette scriptet vil sette et spark objekt som brukes for å kalle API’et til pyspark.\n\n\nCluster\nHvis man vil kjøre Pyspark i et cluster (dvs. på flere maskiner) så vil databehandlingen foregå på andre maskiner som ikke har tilgang til det lokale filsystemet. Man må dermed lage en “pakke” av det virtuelle miljøet på lokal maskin og tilgjengeliggjøre dette for alle maskinene i clusteret. For å lage en slik “pakke” kan man bruke et bibliotek som heter venv-pack. Dette kan kjøres fra et terminalvindu slik:\n\n\nterminal\n\nvenv-pack -p .venv -o pyspark_venv.tar.gz\n\nMerk at kommandoen over må kjøres fra rot-mappen i prosjektet ditt. Her er pyspark_venv.tar.gz et tilfeldig valgt filnavn, men dette filnavnet skal brukes videre i notebooken.\n\n\nnotebook\n\nimport os\nimport subprocess\n\n# Miljøvariabel som peker på en utpakket versjon av det virtuelle miljøet\nos.environ[\"PYSPARK_PYTHON\"] = \"./environment/bin/python\"\n\n# Legg til et flagg, --archives, som peker på \"pakken\" med det virtuelle miljøet\nconf = os.environ[\"PYSPARK_K8S_SUBMIT_ARGS\"].split(' ')\nlast_index = conf.index('pyspark-shell')\nconf[last_index:last_index] = ['--archives', 'pyspark_venv.tar.gz#environment']\nos.environ[\"PYSPARK_SUBMIT_ARGS\"] = ' '.join(conf)\n\nNår du oppretter en Notebook og bruker den kernelen du har laget så må du alltid ha denne på toppen av notebooken. Det initialiserer Spark slik at det kan brukes i notebooken.\n\n\nnotebook\n\n%run /usr/local/share/jupyter/kernels/pyspark_k8s/init.py\n\nDette scriptet vil sette et spark objekt som brukes for å kalle API’et til pyspark.",
    "crumbs": [
      "Manual",
      "Kode",
      "SSB-project"
    ]
  },
  {
    "objectID": "statistikkere/ssb-project.html#tips-og-triks",
    "href": "statistikkere/ssb-project.html#tips-og-triks",
    "title": "SSB-project",
    "section": "Tips og triks",
    "text": "Tips og triks\nI denne delen av kapitlet vil vi gi deg noen tips og triks som kan være nyttige når du jobber med ssb-project.\n\nPoetry\nssb-project bruker Poetry for å håndtere virtuelle miljøer. Poetry er et verktøy som gjør det enkelt å installere pakker og håndtere versjoner av disse. Det er også Poetry som håndterer Jupyter-kernelen for deg.\nHvis du etterlyser funksjonalitet i et ssb-project så kan det være nyttig å lese dokumentasjonen til Poetry for å se om det er mulig å få til det du ønsker. Les også vår egne artikkel Poetry og SSB-project - Pakkehåndtering i Python.\n\n\nRenv\nR-pakken, renv, er et verktøy som lar oss opprette et miljø for R og installere pakker. Det er anbefalt å bruke renv for å sikre at alle som jobber med prosjektet har samme versjon av pakkene. Dette er ikke integrert i ssb-project så må settes opp selv. For detaljer, ser Pakkehåndtering i R.\n\n\nFull disk på Dapla\nDet “lokale” filsystemet på Dapla har kun 10GB diskplass. Har du mange virtuelle miljøer på denne disken kan det fort bli fullt, siden alle pakker blir installert her. Vanligvis er det 2 grunner til at disken blir full:\n\nFor mange virtuelle miljøer (ssb-projects) lagret lokalt.\nDette vil ofte kunne løses ved å slette virtuelle miljøer som ikke lenger er i bruk. Hvis du har 5 virtuelle miljøer som hver bruker 1GB, og du kun jobber på en av de nå, så vil du frigjøre 40% av disken ved å slette 4 av dem. Husk at det permanente lagringsstedet for kode er på GitHub, og du kan alltid klone ned et prosjekt senere og bygge det hvis det trengs.\n/home/jovyan/.cache/ har blitt for stort.\nDette er en mappe som brukes av applikasjoner til å lagre midlertidig data slik at de kan kjøre raskere. Denne kan bli ganske stor etter hvert. Ofte kan man frigjøre flere GB ved å slette denne. Du sletter denne mappen ved å skrive følgende i en terminal:\n\n\n\nterminal\n\nrm -rf /home/jovyan/.cache/\n\nHvis du opplever at disken er full, så kan det anbefales å undersøke hvilke mapper som tar størst plass med følgende kommando i terminalen:\n\n\nterminal\n\ncd ~ && du -h --max-depth=5 | sort -rh | head -n 10 && cd -\n\nKommandoen over sjekker, fra hjemmemappen din, hvilke mapper og undermapper som tar mest plass. Den viser de 10 største mappene. Hvis du ønsker å se flere mapper så kan du endre tallet etter head -n. Hvis du ønsker å se alle mapper så kan du fjerne head -n. --max-depth=5 betyr at den kun sjekker mapper som er 5 mapper dype fra hjemmemappen din.\nNår du har gjort det kan selv vurdere hvilke som kan slettes for å frigjøre plass.",
    "crumbs": [
      "Manual",
      "Kode",
      "SSB-project"
    ]
  },
  {
    "objectID": "statistikkere/ssb-project.html#footnotes",
    "href": "statistikkere/ssb-project.html#footnotes",
    "title": "SSB-project",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nFiler og mapper som starter med punktum er skjulte med mindre man ber om å se dem. I Jupyterlab kan disse vises i filutforskeren ved å velge View fra menylinjen, og deretter velge Show hidden files. I en terminal skriver man ls -a for å se de.↩︎\nDette kan også gjøres ved å høyreklikke på mappen i Jupyterlab sin filutforsker og velge Delete.↩︎",
    "crumbs": [
      "Manual",
      "Kode",
      "SSB-project"
    ]
  },
  {
    "objectID": "statistikkere/appendix/jit.html",
    "href": "statistikkere/appendix/jit.html",
    "title": "Just-in-Time Access",
    "section": "",
    "text": "Just-in-Time Access (JIT) er en applikasjon der data-admins kan gi seg selv midlertidig tilgang knyttet til kildedata. Hovedsakelig benyttes dette for å aktivere tilgang til Transfer Service i kildeprosjektet, og for å aktivere tilgang til å liste ut filer i kildeprosjektet.\nPå Dapla benyttes for dette for å unngå at data-admins har permanent tilgang til sensitive data, siden all kildedata er definert som sensitiv informasjon. data-admins må bruke JIT-applikasjonen for å gi seg selv korte, begrunnede tilganger til kildedata ved behov. Den som er ansvarlig for teamet kan monitere i hvilken grad teamets data-admins benytter seg av denne muligheten.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Just-in-Time Access"
    ]
  },
  {
    "objectID": "statistikkere/appendix/jit.html#hvordan-bruke-jit",
    "href": "statistikkere/appendix/jit.html#hvordan-bruke-jit",
    "title": "Just-in-Time Access",
    "section": "Hvordan bruke JIT",
    "text": "Hvordan bruke JIT\nFor å bruke JIT må du være data-admin for teamet som eier kildedataene du ønsker tilgang til. Du aktiverer tilganger i JIT-appen ved gjøre følgende:\n\nGå inn på nettsiden https://jitaccess.dapla.ssb.no/1\nOppgi Prosjekt-id for prosjektet der kildedataene du ønsker tilgang til er lagret.\nHuk av hvilke tilgangsroller du ønsker aktivert. Se beskrivelse av roller i Tabell 1.\nVelg hvor lenge tilgangen du ønsker at tilgangen skal vare. Den kan maksimalt vare i 8 timer.\nOppgi en begrunnelse for at aktiverer tilgangen.\nTil slutt trykker du på Request access.\n\nTilgangen vil deretter være aktiv ila. noen minutter.\n\n\n\n\n\n\nNoteHvordan bør begrunnelsen skrives?\n\n\n\nPrøv å gjør begrunnelsen forståelig for andre enn deg selv på det tidspunktet. Den som er ansvarlig for teamet skal kunne forstå begrunnelsen når de ser på loggene. I tillegg vil sentralt i SSB monitere i hvilken grad Dapla-teamene benytter seg av tilgangene.\n\n\n\n\n\nTabell 1: De mest relevante tilgangene som kan aktiveres i JIT-applikasjonen\n\n\n\n\n\n\n\n\n\nRoller\nHva?\n\n\n\n\nssb.buckets.list\nListe ut bøttene i kildeprosjektet.\n\n\nstoragetransfer.admin\nSette opp jobb med Transfer Service i kildeprosjektet.\n\n\nssb.bucket.write\nLese og skrive til kildebøtta.\n\n\n\n\n\n\nSkal du sette opp en Transfer Service overføring med kildedata så må du aktivere ssb.bucket.write og storagetransfer.admin.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Just-in-Time Access"
    ]
  },
  {
    "objectID": "statistikkere/appendix/jit.html#footnotes",
    "href": "statistikkere/appendix/jit.html#footnotes",
    "title": "Just-in-Time Access",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nHvis du jobber fra hjemmekontor så må du være på VPN for å få tilgang til JIT-appen.↩︎",
    "crumbs": [
      "Manual",
      "Appendix",
      "Just-in-Time Access"
    ]
  },
  {
    "objectID": "statistikkere/appendix/filinnsamling-moveit.html",
    "href": "statistikkere/appendix/filinnsamling-moveit.html",
    "title": "Filinnsamling via MoveIT",
    "section": "",
    "text": "Her finner du fremgangsmåten beskrevet for hvordan ditt team kan få på plass automatisert overføring av filer samlet inn via Moveit på bakken til teamets kildedatabøtte på Dapla.\nFremgangsmåten beskrevet her innebærer en begrensning på at overflytting av filer til Dapla ikke kan utføres oftere enn en gang i timen. For de aller fleste er ikke dette en utfordring. Dersom dette er en utfordring for ditt team, ta kontakt på arkitektur@ssb.no.\nTransfer Service er en tjeneste som brukes til å flytte filer mellom bakke og sky. Når du skal ta i bruk tjenesten for å overføre data fra bakken til en kildedata-bøtte i Dapla-teamet ditt, så følger du denne beskrivelsen på hvordan man setter opp overføringsjobber. Før du kan kan ta i bruk Transfer Service, må filene samlet inn via Moveit først flyttes til filområdet på “kilde” som Transfer Service benytter (/ssb/cloud_sync/”daplateamnavn”/tilsky).\nDette må settes opp av s782, og henvendelsen må gå via Kundeservice. Henvendelsen må inneholde:\nNår flyttingen av filene fra Moveit til Cloud_sync-området er etablert, så må du sette opp tjenesten for å overføre filene til en kildedata-bøtte i Dapla-teamet ditt. Da følger du denne beskrivelsen på hvordan man setter opp slike overføringsjobber.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Filinnsamling via MoveIT"
    ]
  },
  {
    "objectID": "statistikkere/appendix/filinnsamling-moveit.html#sette-opp-en-helt-ny-innsamling-i-moveit",
    "href": "statistikkere/appendix/filinnsamling-moveit.html#sette-opp-en-helt-ny-innsamling-i-moveit",
    "title": "Filinnsamling via MoveIT",
    "section": "Sette opp en helt ny innsamling i Moveit",
    "text": "Sette opp en helt ny innsamling i Moveit\nDersom det er en ny innsamling i Moveit, så må det også opprettes en filsluse-konto for den eksterne leverandøren av datafilene. Dette må settes opp av s782, og henvendelsen må gå via Kundeservice (Kundeservice@ssb.no) med følgende informasjon:\n\nEn kort beskrivelse av formålet med innsamlingen og hva slags data vil det gjelde for.\n\n\nPersonlig informasjon på ekstern leverandør:\n\nFullt navn\nE-postadresse\nTelefonnummer\nFirmanavn\nInformasjon om datafilene\n\n\n\nInformasjon for automatisk overføring av datafilene via MoveIT Automation:\n\nDaplateamets navn (teamet som datafilene skal overføres til på Dapla)\nØnsket frekvens/evt tidspunkt for overføringen\nEventuelle spesifikke krav eller preferanser for overføringen\n\nNår både kontoen til den eksterne leverandøren er på plass, og flyttingen av filene fra Moveit til Cloud_sync-området er etablert, så må du sette opp tjenesten for å overføre filene til en kildedata-bøtte i Dapla-teamet ditt. Da følger du denne beskrivelsen på hvordan man setter opp slike overføringsjobber.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Filinnsamling via MoveIT"
    ]
  },
  {
    "objectID": "statistikkere/appendix/langtkjorende-jobber.html",
    "href": "statistikkere/appendix/langtkjorende-jobber.html",
    "title": "Kjøre tidkrevende jobber på Dapla",
    "section": "",
    "text": "For å kjøre tidkrevende jobber på Dapla Lab må man bruke et verktøy som heter tmux. Hvis ikke risikerer kjøringen å avbrytes etter noen timer. Feilen har også oppstått når brukeren har lukket nettleser-fanen eller maskinen til brukeren har gått i dvale, selv om dette ikke i teorien skulle påvirke kjøringen. Årsaken virker å være IDE-ene (Jupyter, VS Code og RStudio) pauser jobbene når de blir pauset.\nFor å unngå denne oppførselen er terminal-verktøyet tmux installert i Jupyter, VS Code og RStudio. tmux er terminal multiplexer som lar brukeren kjøre og organisere mange terminal-sesjoner i et vindu. En av fordelene med tmux i denne sammenhengen er at den lar brukeren kjøre prosesser selv om terminalen lukkes. Derfor anbefaler vi at brukere som skal kjøre kode over lang tid (&gt;12t) kjører koden i en tmux-session.\nDu kan kjøre ditt Python-script ved å kjøre følgende:\n{.bash filename=\"Terminal\" tmux new -s minsesjon I koden over startes en session med navn minsesjon. For å aktivere et shell for ditt ssb-project, slik at din Python-installasjon kan kalles på med python-kommandoen i en terminal, så kan du kjøre kommandoen:\n\n\nTerminal\n\npoetry shell\n\nNå kan du kjøre scriptet ditt på følgende måte:\n\n\nTerminal\n\npython mittskript.py\n\nDeretter kan du lukke PC-en, eller nettleser-fanen, og være trygg på at skriptet ditt vil kjøre til det er ferdig. Husk at alle tjenester pauses kl 17 hver dag, så hvis du ikke ønsker det må du også følge denne oppskriften.\ntmux operer med konsepter som session, window og panes. Over startet vi en session med navnet minsesjon. Dette er et komplett arbeidsmiljø hvor man kan ha flere windows og splitte de opp i panes.\nEn av styrkene til tmux er at man kan kjøre prosesser i bakgrunnen. Derfor er det også viktig å kjenne til at en session kan være attached, at brukeren kan se sesjonen i terminalen, og detached, som betyr at den kan kjøre i bakgrunnen mens brukeren jobber med noe annet. Du kan liste ut dine sesjoner ved å skrive følgende i en terminal:\n\n\nTerminal\n\ntmux ls\n\nI eksempelet over så kan vi koble til (attach) til vår sesjon ved å skrive:\n\n\nTerminal\n\ntmux attach -t minsesjon\n\nDu vet at din sesjon er attached ved at du ser en grønn statusbar nederst med navnet til sesjonen, slik som vist Figur 1.\n\n\n\n\n\n\nFigur 1: Statusbar for en tmux-sesjon\n\n\n\nDu kan koble fra en sesjon (detach) ved å skrive CTRL B og deretter d, eller skrive følgende i terminalen:\n\n\nTerminal\n\ntmux detach -s minsesjon\n\nProsessen vil deretter kjøre i sesjonen, samtidig som du kan jobbe med andre ting i tjenesten.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Kjøre tidkrevende jobber på Dapla"
    ]
  },
  {
    "objectID": "statistikkere/appendix/arkivering.html",
    "href": "statistikkere/appendix/arkivering.html",
    "title": "Arkivering",
    "section": "",
    "text": "Alle som flytter produksjon til Dapla må fortsatt arkivere dataene i bakkemiljøet. Grunnen til dette er at det enda ikke er bestemt hvordan arkivering skal foregå på Dapla. Inntill videre må derfor statistikkteam arkivere i de gamle systemene.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Arkivering"
    ]
  },
  {
    "objectID": "statistikkere/appendix/arkivering.html#skrive-fil",
    "href": "statistikkere/appendix/arkivering.html#skrive-fil",
    "title": "Arkivering",
    "section": "Skrive fil",
    "text": "Skrive fil\nFør man kan arkivere data må det skrives ut slik det er definert i Datadok. Mens man tidligere gjorde dette fra SAS, skal man på Dapla gjøre det fra R eller Python.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Arkivering"
    ]
  },
  {
    "objectID": "statistikkere/appendix/arkivering.html#overføre-fil",
    "href": "statistikkere/appendix/arkivering.html#overføre-fil",
    "title": "Arkivering",
    "section": "Overføre fil",
    "text": "Overføre fil\nEtter at filen er skrevet må den flyttes fra Dapla til bakkemiljøet, og til slutt inn i riktig arkiv-mappe. Overføring av filer mellom bakke og sky gjøres med Transfer Service. Når filen er flyttet til bakkemiljøet, må brukeren selv flytte filen til arkiv-mappen. Ønsker man å automatisere flyttingen, så kan man sende en forespørsel til Kundeservice.",
    "crumbs": [
      "Manual",
      "Appendix",
      "Arkivering"
    ]
  },
  {
    "objectID": "statistikkere/standards-toolbelt.html",
    "href": "statistikkere/standards-toolbelt.html",
    "title": "Standards",
    "section": "",
    "text": "Modulen standards i dapla-toolbelt-metadata tilbyr metoder for å sjekke om filer i bøtter er i tråd med SSBs definerte navnestandard. Metodene inkluderer:\nFor å effektivisere validering av bøtter med store mengder filer, benytter metoden asynkronitet.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata",
      "Standards"
    ]
  },
  {
    "objectID": "statistikkere/standards-toolbelt.html#funksjonalitet",
    "href": "statistikkere/standards-toolbelt.html#funksjonalitet",
    "title": "Standards",
    "section": "Funksjonalitet",
    "text": "Funksjonalitet\nstandards tilbyr to typer av funksjonalitet. Den første er å sjekke om en bøtte, mappe eller fil følger standarden. Den andre er å produsere en rapport som oppsummerer resultatet av valideringen.\n\nValidering\nFor å sjekke om en bøtte, mappe eller fil følger navnestandarden kan man benytte funksjonen check_naming_standard(). Den returnerer en liste med resultater for alle objektene du har bedt om å få validert.\n\n\nNotebook\n\nfrom dapla_metadata.standards.standard_validators import check_naming_standard\n\nresults = await check_naming_standard(\"&lt;bøttenavn/mappe/filsti&gt;\")\nresults\n\n\n\n\n\n\n\nCautionOutput fra validering av enkeltfil\n\n\n\n\n\nValidationResult(\n    success=False, \n    file_path=\"/buckets/produkt/stat/inndata/bil_v1.parquet\", \n    messages=[\n        \"Det er oppdaget brudd på SSB-navnestandard:\"\n    ], \n    violations=[\n        \"Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\"\n    ]\n)\n\n\n\nSiden metoden bruker asynkrone kall, må nøkkelordet await brukes foran metodenavnet.\nFor å få tilgang til spesifikke deler av resultatet, kan du bruke punktnotasjon .. Hvis du for eksempel ønsker å hente ut listen med regelbrudd fra det første valideringsresultatet, kan du gjøre følgende:\nresults[0].violations\nHvis du har validert et stort antall filer så kan du benytte følgende kode for å få ut resultatene på en mer lesbar form:\n\n\nNotebook\n\nviolations = [r for r in results if not r.success]\n\nif not violations:\n    print(\"Gratulerer, ingen feil å vise\")\nelse:\n    for v in violations:\n        print(v.file_path)\n        print(\"\\t\" + \"\\n\\t\".join(v.messages))\n        print(\"\\t\\t\" + \"\\n\\t\\t\".join(v.violations) + \"\\n\")\n\n\n\n\n\n\n\nCautionEksempel på output fra validering av mange filer\n\n\n\n\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/butikker_kongsvinger.parquet\n    Det er oppdaget brudd på SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/butikkbygg_kongsvinger.parquet\n    Det er oppdaget brudd på SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/butikkbygg_oslo.parquet\n    Det er oppdaget brudd på SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/noen_boliger_kongsvinger.parquet\n    Det er oppdaget brudd på SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/noen_boliger_oslo.parquet\n    Det er oppdaget brudd på SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/butikker_oslo.parquet\n    Det er oppdaget brudd på SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/noen_boligbygg_oslo.parquet\n    Det er oppdaget brudd på SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/noen_boligbygg_kongsvinger.parquet\n    Det er oppdaget brudd på SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/NVE_Trafostasjon_punkt_p2023.parquet\n    Det er oppdaget brudd på SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/enkle_kommuner.parquet\n    Det er oppdaget brudd på SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/bygg_kongsvinger.parquet\n    Det er oppdaget brudd på SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/veger_kongsvinger.parquet\n    Det er oppdaget brudd på SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/noen_tettsteder_2023.parquet\n    Det er oppdaget brudd på SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/bygg_oslo.parquet\n    Det er oppdaget brudd på SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/veger_oslo.parquet\n    Det er oppdaget brudd på SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n        Filnavn mangler gyldighetsperiode ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n        Filnavn mangler versjon ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#filnavn\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/ABAS_kommune_flate_p2024_v1.parquet\n    Det er oppdaget brudd på SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/SSB_tettsted_flate_p2022_v1.parquet\n    Det er oppdaget brudd på SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n\ngs://ssb-dapla-felles-data-produkt-prod/GIS/testdata/SSB_tettsted_flate_p2023_v1.parquet\n    Det er oppdaget brudd på SSB-navnestandard:\n        Mappe for datatilstand mangler ref: https://manual.dapla.ssb.no/statistikkere/navnestandard.html#obligatoriske-mapper\n\n\n\n\n\nRapport\nHvis du ønsker en kort oppsummering og vurdering av resultatet, kan du importere følgende metode:\n\n\nNotebook\n\nfrom dapla_metadata.standards.standard_validators import generate_validation_report\n\nreport = generate_validation_report(results)\n\nMetoden tar en liste med valideringsresultater som input:\nOg hvis alt ser bra ut:\n\nFor å få tilgang til spesifikke deler av rapporten, kan du bruke punktnotasjon .. Hvis du for eksempel ønsker å hente ut kun suksess raten i prosent, kan du gjøre følgende:\nreport.success_rate()\nEller hvis du ønsker direkte tilgang til tallene:\nreport.num_files_validated\nreport.num_success\nreport.num_failures\n\n\n\n\n\n\nTipBruk av validering i statistikkproduksjon\n\n\n\nHvis man ønsker å benytte valideringsfunksjonaliteten i koden som kjøres i en statistikkproduksjon, så kan pre-commit hooks feile på grunn av nøkkelordet await benyttes utenfor en asynkron funksjon. En enkel løsning er å legge til # noqa: F704 på samme linje som await, slik:\n\n\nNotebook\n\nresults = await check_naming_standard(\"\") # noqa: F704",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata",
      "Standards"
    ]
  },
  {
    "objectID": "statistikkere/standards-toolbelt.html#footnotes",
    "href": "statistikkere/standards-toolbelt.html#footnotes",
    "title": "Standards",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nNår man validerer en filsti som ikke eksisterer så får man beskjed om at Filen eksisterer ikke. Validerer uansett..↩︎",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-metadata",
      "Standards"
    ]
  },
  {
    "objectID": "statistikkere/dapla-auth-client.html",
    "href": "statistikkere/dapla-auth-client.html",
    "title": "dapla-auth-client",
    "section": "",
    "text": "dapla-auth-client er et Python-bibliotek for å autentisere brukere på Dapla Lab mot tjenester på NAIS. Bibliotekets returnerer et JWT-token (med varighet på en time) som blant annet inneholder hvilke tilgangsgrupper brukeren er medlem av.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-auth-client"
    ]
  },
  {
    "objectID": "statistikkere/dapla-auth-client.html#forberedelser",
    "href": "statistikkere/dapla-auth-client.html#forberedelser",
    "title": "dapla-auth-client",
    "section": "Forberedelser",
    "text": "Forberedelser\nBiblioteket kan benyttes i tjenester på Dapla Lab der Python er installert. Den kan installeres med følgende kommando i et ssb-project:\n\n\nTerminal\n\npoetry add dapla-auth-client",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-auth-client"
    ]
  },
  {
    "objectID": "statistikkere/dapla-auth-client.html#funksjonalitet",
    "href": "statistikkere/dapla-auth-client.html#funksjonalitet",
    "title": "dapla-auth-client",
    "section": "Funksjonalitet",
    "text": "Funksjonalitet\nBiblioteket er ment for å støtte autentisering mot NAIS fra Dapla Lab. I tillegg kan brukere hente ut informasjon om hvilke tilgangsgrupper brukeren er medlem av fra tokenet.\n\nHente ut token\nFor å hente ut et JWT-token med varighet på en time kan brukeren skrive følgende:\n\n\nNotebook\n\nfrom dapla_auth_client import AuthClient\n\nAuthClient.fetch_personal_token()\n\nKoden over returnerer et JWT-token med informasjon om hvilken tilgangsgruppe brukeren representerer i tjenesten.\n\n\nListe ut tilgangsgrupper\nMan kan også dekode tokenet og liste ut alle tilgangsgrupper brukeren er medlem av. I koden under brukes Python-biblioteket PyJWT til å dekode tokenet:\n\n\nNotebook\n\nfrom dapla_auth_client import AuthClient\n\ntoken = AuthClient.fetch_personal_token([\"all_groups\", \"current_group\"])\ndecoded = jwt.decode(token, options={\"verify_signature\": False})\nprint(decoded)\n\nOutput vil da være på denne formen:\n{\n    \"dapla.group\": \"team-a-developers\",\n    \"dapla.groups\": [\n        \"team-a-data-admins\",\n        \"team-a-developers\",\n        \"team-b-developers\"\n    ],\n    \"exp\": 1750687898,\n    \"iat\": 1750684298,\n    \"iss\": \"https://labid.lab.dapla-external.ssb.no\",\n    \"scope\": \"all_groups,current_group\",\n    \"sub\": \"ini\"\n}",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-auth-client"
    ]
  },
  {
    "objectID": "statistikkere/dapla-whodat.html",
    "href": "statistikkere/dapla-whodat.html",
    "title": "dapla-toolbelt-whodat",
    "section": "",
    "text": "dapla-toolbelt-whodat er en Python-pakke som gir statistikere mulighet for å søke etter fødselsnummer (FNR) basert på hjelpeopplysninger - for eksempel navn, adresse eller kommunenummer. Funksjonaliteten kan hjelpe statistikere å beholde rader som ellers hadde blitt forkastet, fordi man har manglende eller feil identifikator i datasettet. dapla-toolbelt-whodat bygger på Skatteetatens folkeregister-søk og er utviklet av team statistikktjenester.\nSiden tilgang til direkte identifiserende opplysninger er underlagt strenge regler krever bruken av dapla-toolbelt-whodat at man forholder seg til vedtatte standarder som datatilstander og systemer som Kildomaten. I tillegg er det en streng tilgangsstyring til hvor man kan kalle funksjonaliteten fra.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-whodat"
    ]
  },
  {
    "objectID": "statistikkere/dapla-whodat.html#forberedelser",
    "href": "statistikkere/dapla-whodat.html#forberedelser",
    "title": "dapla-toolbelt-whodat",
    "section": "Forberedelser",
    "text": "Forberedelser\nFør man tar i bruk funksjonaliteten er det viktig at man kjenner godt til tilgangstyring i Dapla-team og Kildomaten, og har diskutert med seksjonen hvordan man skal behandle direkte identifiserende opplysninger i de aktuelle dataene.\nFor at et Dapla-team skal kunne bruke dapla-toolbelt-whodat må Kildomaten være skrudd på for miljøet1 man ønsker å jobbe fra. Som standard får alle statistikkteam skrudd på Kildomaten i prod-miljøet og ikke i test-miljøet. Ønsker du å aktivere Kildomaten i test-miljøet kan dette gjøres selvbetjent som en feature.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-whodat"
    ]
  },
  {
    "objectID": "statistikkere/dapla-whodat.html#tilgangsstyring",
    "href": "statistikkere/dapla-whodat.html#tilgangsstyring",
    "title": "dapla-toolbelt-whodat",
    "section": "Tilgangsstyring",
    "text": "Tilgangsstyring\nTilgang til funksjonalitet i dapla-toolbelt-whodat er i seg selv sensitivt. Tjenesten tilgangsstyres derfor strengt. I prod-miljøet kan man kun ta i bruk funksjonaliteten ved å prosessere dataene i Kildomaten. Det er bare tilgangsgruppen data-admins som har tilgang til å godkjenne slike automatiske prosesseringer. I test-miljøet, derimot, kan alle på teamet benytte seg av all funksjonalitet siden det aldri skal forekomme ekte data.\n\n\n\nTabell 1: Tilgangsstyring til dapla-toolbelt-whodat\n\n\n\n\n\n(a) Test-miljø\n\n\n\n\n\nAktør\nFNR-leting\n\n\n\n\nKildomaten\n✅\n\n\ndata-admins (interaktivt)\n✅\n\n\ndevelopers (interaktivt)\n✅\n\n\n\n\n\n\n\n\n\n(b) Prod-miljø\n\n\n\n\n\nAktør\nFNR-leting\n\n\n\n\nKildomaten\n✅\n\n\ndata-admins (interaktivt)\n🚫\n\n\ndevelopers (interaktivt)\n🚫\n\n\n\n\n\n\n\n\n\nVi ser fra Tabell 1 (a) at man i test-miljøet har full tilgang til funksjonaliteten i dapla-toolbelt-whodat, både fra Kildomaten og når man jobber interaktivt2 i Jupyterlab. Tabell 1 (b) viser at det kun er tilgang til FNR-leting fra Kildomaten i prod-miljøet, og man kan ikke kan bruke biblioteket interaktivt, da det potensielt kan avsløre samtlige FNR. Av den grunn er det alltid anbefalt å teste ut koden sin i test-miljøet før den produksjonssettes i prod-miljøet med Kildomaten.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-whodat"
    ]
  },
  {
    "objectID": "statistikkere/dapla-whodat.html#installering",
    "href": "statistikkere/dapla-whodat.html#installering",
    "title": "dapla-toolbelt-whodat",
    "section": "Installering",
    "text": "Installering\ndapla-toolbelt-whodat er ferdig installert i Kildomaten. Ønsker du å bruke den i test-miljøet til teamet kan du installere det i et ssb-project fra PyPI med denne kommandoen:\n\n\nTerminal\n\npoetry add dapla-toolbelt-whodat",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-whodat"
    ]
  },
  {
    "objectID": "statistikkere/dapla-whodat.html#spesifikasjon",
    "href": "statistikkere/dapla-whodat.html#spesifikasjon",
    "title": "dapla-toolbelt-whodat",
    "section": "Spesifikasjon",
    "text": "Spesifikasjon\n\nDataformater\ndapla-toolbelt-pseudo støtter innlesing av følgende dataformater:\n\nPandas dataframes\nPolars dataframes\n\nEksemplene under viser hovedsaklig innlesing av dataframes fra minnet.\n\n\nKolonner og opsjoner\nI dapla-toolbelt-whodat konstruerer man en DataFrame med hjelpevariabler man skal bruke for søket (eksempler under). Da er det veldig viktig at kolonnenenavnene er riktige, og at formatet er helt likt som i tabellen under.\n\n\n\n\n\n\nWarning\n\n\n\nNB: Alle variabler må være strengverdier i den faktiske DataFramen. Tall-typer støttes ikke.\n\n\n\n\n\nTabell 2: Variabler\n\n\n\n\n\n\n\n\n\n\nKolonnenavn\nFormat\nKommentar\n\n\n\n\nnavn\nEt eller flere hele ord fra personnavnet, skilt med mellomrom.\nTegnsetting: “Åse J. Ås” og “Günther” er gyldig, men ikke “مُحَمَّد”\n\n\nkjoenn\n‘mann’ eller ‘kvinne’\n\n\n\nfoedselsdato\n“YYYYMMDD”\n\n\n\nfoedselsaarFraOgMed\n“YYYY”\nLaveste fødselsår\n\n\nfoedselsaarTilOgMed\n“YYYY”\nHøyeste fødselsår\n\n\nadressenavn\nMinst 3 tegn fra begynnelsen av gatenavn\n\n\n\nhusnummer\nHusnummer, med eller uten bokstav\n\n\n\npostnummer\nFire siffer\n\n\n\nkommunenummer\nFire siffer\n\n\n\nfylkesnummer\nTo siffer\n\n\n\n\n\n\n\n\nOpsjoner er verdier som endrer søket, og utføres for hver rad.\n\n\n\nTabell 3: Opsjoner\n\n\n\n\n\n\n\n\n\n\n\nOpsjon\nFormat\nDefault-verdi\nKommentar\n\n\n\n\ninkluder_oppholdsadresse\ntrue eller false\nfalse\nTreffer oppholdsasdresse i tillegg til bostedsadresse\n\n\nsoek_fonetisk\ntrue eller false\nfalse\nSøk på fonetisk lignende navn\n\n\ninkluder_doede\ntrue eller false\nfalse\nSøket treffer også døde personer\n\n\nopplysningsgrunnlag\n“gjeldende” eller “historisk”\n“gjeldende”.\nStyrer håndtering av historikk. Påvirker kun navn og adresse – for andre opplysninger søkes det alltid kun på gjeldende",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-whodat"
    ]
  },
  {
    "objectID": "statistikkere/dapla-whodat.html#arbeidsflyt",
    "href": "statistikkere/dapla-whodat.html#arbeidsflyt",
    "title": "dapla-toolbelt-whodat",
    "section": "Arbeidsflyt",
    "text": "Arbeidsflyt\nEn vanlig arbeidsflyt i Kildomaten for å bruke dette biblioteket vil være følgende:\n\nBruke Validator()-funksjonalitet for å finne ugyldige FNR (Dokumentasjon)\nHent ut alle rader fra den originale DataFramen med ugyldig FNR funnet i steg 1.\nTransformer alle hjelpevariablene til formatet beskrevet i Tabell 2, og endre navn på kolonnene.\nUtfør et FNR-søk basert på hjelpevariablene.\nErstatt de ugyldige FNRene med gyldig FNR\nPseudonymiser FNRene\n\n\n\n\n\n\n\nWarning\n\n\n\nFra steg 2-5 er det veldig viktig at man tar vare på indeksen fra den originale DataFramen! Fordi man gjør et utdrag av DataFramen i steg 2, må man også kunne finne tilbake til de originale radene når man skal erstatte ugyldige FNR i steg 5. Ikke bruk Pandas’ df.reset_index() eller lignende i mellomtiden.\nPolars har ikke en innebygd indeks, og man burde opprette en indeks-kolonne før man gjør utdrag av DataFramen i steg 2 med df.with_row_index()\n\n\nI repoet whodat-examples finner man eksempler. Repoet inneholder en notebook man kan kjøre i Dapla Lab Test, med rollen dapla-felles-developers. Repoet inneholder også en notebook med et Kildomaten-eksempel",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-whodat"
    ]
  },
  {
    "objectID": "statistikkere/dapla-whodat.html#eksempler",
    "href": "statistikkere/dapla-whodat.html#eksempler",
    "title": "dapla-toolbelt-whodat",
    "section": "Eksempler",
    "text": "Eksempler\n\nEnkle eksempler\nFNR-leting følger et såkalt builder-pattern der man spesifiserer hva og i hvilken rekkefølge operasjonene skal gjøres. Anta at det finnes en Pandas dataframe i minnet som heter df hvor man skal gjøre et enkelt søk på “navn”-variabelen. Da vil koden se slik ut:\n\n\nEnkelt søk\n\nfrom dapla_whodat import Whodat\nimport pandas as pd\n\n# Vi \ndf = pd.DataFrame(\n  {\n    \"navn\": [\"Donald Duck\", \"Dolly Duck\", \"Onkel Skrue\"]\n    \"adressenavn\": [\"Lundlia\", \"Smøyatunvegen\", \"Simmenesvegen\"]\n  }\n)\n\nresult = (\n    Whodat.from_pandas(df)\n    .search_fnr()\n    .with_search_strategy([\"navn\"])\n    .run()\n)\n\nEtter importering av pakker begynner vi med å lage en Pandas DataFrame med variablene navn og adresse.\nVi bruker følgende metoder fra pakken vår Whodat:\n\nfrom_pandas(df): angir at hjelpevariablene vi ønsker å bruke for søket ligger i en Pandas DataFrame (df).\nsearch_fnr(): angir hva vi skal gjøre\nwith_search_strategy(): forteller hvordan vi skal søke. I eksempelet søker vi på [\"navn\"].\n\nman kan gjøre fonetisk navnesøk på hver rad ved å angi soek_fonetisk = True slik: .with_search_strategy([\"navn\"], soek_fonetisk = True)\n\nrun(): metoden som faktisk utfører søket.\n\n\n\nAvanserte eksempler\n\n\nHente ut FNR\n\nfrom dapla_whodat import Whodat\n\nresult = (\n    Whodat.from_pandas(df)\n    .search_fnr()\n    .with_search_strategy([\"navn\"])\n    .run()\n)\n\nfound_personal_ids = result.to_list()\n\nto_list() returnerer en liste av FNRer som er funnet.\n\n\n\n\n\n\nWarning\n\n\n\nDet blir kun returnert et FNR når søket gir et unikt treff. Søk som returnerer flere FNR, eller ingen FNR, vil bli erstattet med None.\n\n\nLa oss se på et eksempel:\n\n Eksempel Vi har en DataFrame med 3 rader, og vi kjører koden over. Anta at:\n\nSøk på den første raden med navn gir ett enkelt FNR.\nSøk på den andre raden gir flere treff.\nSøk på den tredje raden gir ingen treff.\n\n\nResultatet hadde da sett slik ut:\n\n\nEksempel ouput\n\nfound_personal_ids = result.to_list()\n\nprint(found_personal_ids)\n&gt; [\"11111123456\", None, None]\n\nHer vil da radnummer være tilsvarende til plasseringen i listen, slik at første rad gir et gyldig FNR på det første elementet i listen mens andre og tredje rad returnerer None.\n\n\nFlere søk\nVi kan også legge til flere søk for hver enkelt rad i en DataFrame. Dette ser slik ut:\nfrom dapla_whodat import Whodat\n\nresult = (\n    Whodat.from_pandas(df)\n    .search_fnr()\n    .with_search_strategy([\"navn\"])\n    .with_search_strategy([\"navn\", \"adressenavn\"])\n    .run()\n)\nI koden over har vi lagt til et søk som også søker på adressenavn. Dette endrer søkemetoden slik:\nFor hver rad, hvis vi får et unikt treff på det øverste søket (kun navn), returner det FNRet. Hvis ikke, gå videre til neste søk (navn og adressenavn). Repeter denne prosessen for hvert kall med with_search_strategy() til man enten har funnet et unikt FNR, eller alle søk har blitt forsøkt. Hvis man da ikke har funnet et unikt FNR, returner None.\nVi fortsetter med utgangspunkt i scenariet fra Eksempel over, men nå med koden over. Vi har nå fått returnert dataene:\nfound_personal_ids = result.to_list()\n\nprint(found_personal_ids)\n&gt; [\"11111123456\", \"22222209876\", None]\nNår vi kjørte koden tidligere fikk vi None på det andre elementet i listen fordi vi fikk mer enn ett FNR. Siden vi nå har snevret inn søket ved å legge til en adressevariabel inner vi nå et unikt treff.\nLikedan fikk vi, med den tidligere koden, returnert None for det tredje elementet, fordi vi ikke fikk treff på noen FNR. La oss se om vi kan finne det:\nfrom dapla_whodat import Whodat\n\nresult = (\n    Whodat.from_pandas(df)\n    .search_fnr()\n    .with_search_strategy([\"navn\"])\n    .with_search_strategy([\"navn\", \"adressenavn\"])\n    .with_search_strategy([\"navn\", \"adressenavn\"], inkluder_doede=True)\n    .run()\n)\n\nfound_personal_ids = result.to_list()\n\nprint(found_personal_ids)\n&gt; [\"11111123456\", \"22222209876\", \"33333376543\"]\nVed å legge til et tredje søk, som inkluderer døde, har vi funnet personnummeret i det tredje elementet. Merk at ved å legge til døde, har vi utvidet søket.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-whodat"
    ]
  },
  {
    "objectID": "statistikkere/dapla-whodat.html#søkestrategi",
    "href": "statistikkere/dapla-whodat.html#søkestrategi",
    "title": "dapla-toolbelt-whodat",
    "section": "Søkestrategi",
    "text": "Søkestrategi\nDet finnes ingen universell beste søkestrategi da søkemetodikken vil variere basert på datagrunnlaget og behov. Derfor har vi lagt inn mye funksjonalitet i dette biblioteket.\nHer er noen aspekter man bør ta stilling til:\n\nFNR-søk kan gi falskt positivt funn\n\nEksempel: Mån søker kun på etternavn, adresse, og fødselsdato - kan gi FNRet til en tvilling\n\nFNR-søk kan gi falskt negativt funn\n\nEksempel: Man søker på navn og adresse, men vedkommende har flyttet - da får man ingen treff\n\nFlere variabler (Tabell 2) gir et snevrere søk. Opsjoner (Tabell 3) utvider søket.\n\n\nCase 1: Snevert søk\nNoen seksjoner vil kanskje vurdere at man ønsker så langt som mulig å unngå falskt positive funn. Med andre ord tar man heller kostnaden av å ekskludere potensielt gyldige FNR til fordel for å være helt sikre på at de FNR-ene man finner er gyldige. En strategi kunne da ha vært å kjøre ett søk med alle variablene man har tilgjengelig - et snevert søk. Dette kunne ha sett slik ut:\nfrom dapla_whodat import Whodat\n\nresult = (\n    Whodat.from_pandas(df)\n    .search_fnr()\n    .with_search_strategy([\"navn\", \"adressenavn\", \"husnummer\", \"postnummer\", \"foedselsdato\"])\n    .run()\n)\n\n\nCase 2: Bredt søk\nAndre seksjoner rydder muligens opp i dataene sine senere i produksjonsløpet ved å koble mot annen data, og ønsker et veldig bredt søk for å få med alle potensielle FNRer. Vi kan da se for oss et iterativt søk: vi starter snevert og utvider søket etter hvert. Dette kunne ha sett slik ut:\nfrom dapla_whodat import Whodat\n\nresult = (\n    Whodat.from_pandas(df)\n    .search_fnr()\n    .with_search_strategy([\"navn\", \"adressenavn\", \"husnummer\", \"postnummer\", \"foedselsdato\"])\n    .with_search_strategy([\"navn\", \"adressenavn\", \"husnummer\", \"postnummer\", \"foedselsaarFraOgMed\", \"foedselsaarTilOgMed\"])\n    .with_search_strategy([\"navn\", \"adressenavn\", \"husnummer\", \"postnummer\", \"foedselsaarFraOgMed\", \"foedselsaarTilOgMed\"],\n      inkluder_doede=True)\n    .with_search_strategy([\"navn\", \"adressenavn\", \"husnummer\", \"postnummer\", \"foedselsaarFraOgMed\", \"foedselsaarTilOgMed\"],\n      inkluder_doede=True, opplysningsgrunnlag=True, inkluder_oppholdsadresse=True)\n    .with_search_strategy([\"navn\", \"adressenavn\", \"husnummer\", \"postnummer\", \"foedselsaarFraOgMed\", \"foedselsaarTilOgMed\"],\n      inkluder_doede=True, opplysningsgrunnlag=True, inkluder_oppholdsadresse=True, soek_fonetisk=True)\n    .run()\n)\nVi anbefaler statistikere å teste ut hvilke variabler som gir best resultater med datagrunnlaget man har tilgjengelig.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-whodat"
    ]
  },
  {
    "objectID": "statistikkere/dapla-whodat.html#detaljer-om-fnr-søket",
    "href": "statistikkere/dapla-whodat.html#detaljer-om-fnr-søket",
    "title": "dapla-toolbelt-whodat",
    "section": "Detaljer om FNR-søket",
    "text": "Detaljer om FNR-søket\nMan kan få metadata fra søket man har utført. Dette gir detaljert informasjon om på hvilken søkestrategi som ga treff, for hver rad.\n\n\nDetaljer\n\nfrom dapla_whodat import Whodat\nimport pandas as pd\n\ndf = pd.DataFrame(\n  {\n    \"navn\": [\"Donald Duck\", \"Dolly Duck\", \"Onkel Skrue\"]\n    \"adressenavn\": [\"Lundlia\", \"Smøyatunvegen\", \"Simmenesvegen\"]\n  }\n)\n\nresult = (\n    Whodat.from_pandas(df)\n    .search_fnr()\n    .with_search_strategy([\"navn\"])\n    .with_search_strategy([\"navn\", \"adressenavn\"])\n    .run()\n)\n\nfound_personal_ids = result.to_list()\n\nprint(found_personal_ids)\n&gt; [\"11111123456\", \"22222209876\", None]\n\nprint(result.details)\n&gt; [\n    {\n        \"index_fnr_search_df\": 0,\n        \"index_original_df\": 0,\n        \"number_of_found_ids\": 1,\n        \"unique_response_step_number\": 1,\n    },\n    {\n        \"index_fnr_search_df\": 1,\n        \"index_original_df\": 1,\n        \"number_of_found_ids\": 1,\n        \"unique_response_step_number\": 2,\n    },\n    {\n        \"index_fnr_search_df\": 2,\n        \"index_original_df\": 2,\n        \"number_of_found_ids\": 0,\n        \"unique_response_step_number\": None,\n    },\n  ]\n\nHver rad i df tilsvarer ett element i listen i result.details. Hvert element har følgende data:\n\n\n\nTabell 4: Beskrivelse av metadata-felter\n\n\n\n\n\n\n\n\n\nFelt\nBeskrivelse\n\n\n\n\nindex_fnr_search_df\nRadnummeret i DataFramen - uavhengig av indeks\n\n\nindex_original_df\nRadnummeret i forhold til indeksen i DataFramen (følger df.index). Brukes når man erstatter ugyldige fødselsnummer\n\n\nnumber_of_found_ids\nHvor mange FNRer som ble funnet på siste søk\n\n\nunique_response_step_number\nHvilken with_search_strategy() som fant et unikt FNR. Hvis man ikke finner et unikt FNR, er feltet None\n\n\n\n\n\n\nI eksempelet over fant vi to FNRer. De ligger i listen found_personal_ids.\nFørste fødselsnummer: unique_response_step_number = 1 forteller oss at vi fant et unikt FNR med kun navn.\nAndre fødselsnummer: unique_response_step_number = 2 forteller oss at navn og adresse var nødvendig for å få et unikt FNR.\n\n\nLagre metadata\n\nfrom gcsfs import GCSFileSystem\nimport json\n\ndetails = result.details\n\nwith GCSFileSystem().open(\"gs://ssb-someteam-data-produkt-prod/somefolder/details_fnrsearch.json\", \"w\") as f:\n  json.dump(details, f)\n\nMetadatane er ikke sensitive og kan derfor skrives til produktbøtta for videre utforskning fra Kildomaten.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-whodat"
    ]
  },
  {
    "objectID": "statistikkere/dapla-whodat.html#erstatte-ugyldige-med-gyldige-fnr",
    "href": "statistikkere/dapla-whodat.html#erstatte-ugyldige-med-gyldige-fnr",
    "title": "dapla-toolbelt-whodat",
    "section": "Erstatte ugyldige med gyldige FNR",
    "text": "Erstatte ugyldige med gyldige FNR\nVi antar at vi har fulgt fremgangsmåten i Arbeidsflyt. Vi har da to DataFrames når vi utfører FNR-søket:\n\ndf_original: Den originale DataFramen vi ønsker å utføre FNR-søk på. DataFramen sin FNR-kolonne heter fnr\ndf_helper_variables: Et utsnitt av df_original, som kun inneholder radene med ugyldig FNR, og hjelpevariablene nødvendig for søket. Indeksen er lik som i df_original\n\nVi har to måter å få ut data fra FNR-søket:\n\n1) Bruk av to_list()\nfound_personal_ids = result.to_list()\n\nprint(found_personal_ids)\n&gt; [\"11111123456\", \"22222209876\", None]\nVi kan da erstatte rader i df_original med følgende kode:\ndf_valid_personal_ids = (\n                        pd.DataFrame({\"fnr\": found_personal_ids},\n                        index=df_helper_variables.index, copy=False)\n                        )\n\ndf_valid_personal_ids = (\n                        df_valid_personal_ids\n                        .reindex(df.index) \n                        # Pad dataset with NaN for rows with valid FNR\n                        )\n  \ndf_original[\"fnr\"] = (\n                    df_valid_personal_ids[\"fnr\"]\n                    .where(df_valid_personal_ids[\"fnr\"].notna(),\n                    df_original[\"fnr\"]) # Merge valid FNRs into 'df_original'\n                    )\n\n\n2) Bruk av to_dict_from_original_indices()\nVi kan også få ut data som et Python dictionary, hvor den originale indeksen er nøkkelen og FNR-et er verdien. Hvis det ikke ble funnet et unikt FNR, er ikke dataen i dictionaryen.\nfound_personal_ids = result.to_dict_from_original_indices()\n\nprint(found_personal_ids)\n&gt; {0: \"11111123456\", 1: \"22222209876\"}\nFNR-erstatning:\n```{.python} df_original[“fnr”] = ( pd.Series(found_personal_ids) .combine_first(df_original[“fnr”]) )",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-whodat"
    ]
  },
  {
    "objectID": "statistikkere/dapla-whodat.html#footnotes",
    "href": "statistikkere/dapla-whodat.html#footnotes",
    "title": "dapla-toolbelt-whodat",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nEt Dapla-team har både et test- og et prod-miljø. Kildomaten må være skrudd på i det miljøet du ønkser å benytte dapla-toolbelt-whodat fra.↩︎\nMed interaktiv jobbing menes at man skriver og kode og får tilbake output i samme verktøy. F.eks. er Jupyterlab et eksempel på et verktøy som lar deg jobbe interaktivt med data.↩︎",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-whodat"
    ]
  },
  {
    "objectID": "statistikkere/dapla-pseudo.html",
    "href": "statistikkere/dapla-pseudo.html",
    "title": "dapla-toolbelt-pseudo",
    "section": "",
    "text": "dapla-toolbelt-pseudo er en python-pakke som har som sitt hovedformål å gi Dapla-brukere muligheten til å pseudonymisere, de-pseudonymisere og re-pseudonymisere data. Det skal sikre at brukerne av Dapla har verktøyene de trenger for å jobbe med direkte identifiserende opplysninger i henhold til lovverk og SSBs tolkninger av disse.\nSiden tilgang til direkte identifiserende opplysninger er underlagt strenge regler, så krever bruken av dapla-pseudo-toolbelt at man forholder seg til vedtatte standarder som datatilstander og systemer som Kildomaten. I tillegg er det en streng tilgangsstyring til hvor man kan kalle funksjonaliteten fra. Tjenestene er satt opp på en slik måte at Dapla-team skal være selvbetjent i bruken av funksjonaliteten, samtidig som regler, prosesser og standarder etterleves på enklest mulig måte.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-pseudo"
    ]
  },
  {
    "objectID": "statistikkere/dapla-pseudo.html#forberedelser",
    "href": "statistikkere/dapla-pseudo.html#forberedelser",
    "title": "dapla-toolbelt-pseudo",
    "section": "Forberedelser",
    "text": "Forberedelser\nFør man tar i bruk funksjonaliteten er det viktig at man kjenner godt til tilgangstyring i Dapla-team og Kildomaten, og har diskutert med seksjonen hvordan man skal behandle direkte identifiserende opplysninger i de aktuelle dataene.\nFor at et Dapla-team skal kunne bruke dapla-toolbelt-pseudo må Kildomaten være skrudd på for miljøet1 man ønsker å jobbe fra. Som standard får alle statistikkteam skrudd på Kildomaten i prod-miljøet og ikke i test-miljøet. Ønsker du å aktivere Kildomaten i test-miljøet kan dette gjøres selvbetjent som en feature.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-pseudo"
    ]
  },
  {
    "objectID": "statistikkere/dapla-pseudo.html#tilgangsstyring",
    "href": "statistikkere/dapla-pseudo.html#tilgangsstyring",
    "title": "dapla-toolbelt-pseudo",
    "section": "Tilgangsstyring",
    "text": "Tilgangsstyring\nTilgang til å funksjonalitet i dapla-toolbelt-pseudo kan regnes som sensitivt i seg selv, og derfor er det en streng tilgangsstyring for bruk av tjenesten. I prod-miljøet kan man kun ta i bruk funksjonaliteten ved å prosessere dataene i Kildomaten, og det er bare tilgangsgruppen data-admins som har tilgang til å godkjenne slike automatiske prosesseringer. I test-miljøet derimot kan alle på teamet benytte seg av all funksjonalitet, siden det aldri skal forekomme ekte data her.\n\n\n\nTabell 1: Tilgangsstyring til dapla-pseudo-toolbelt\n\n\n\n\n\n(a) Test-miljø\n\n\n\n\n\n\n\n\n\n\n\n\nAktør\nValidator\nPseudonymize()\nDepseudonymize()\nRepseudonymize()\n\n\n\n\nKildomaten\n✅\n✅\n✅\n✅\n\n\ndata-admins (interaktivt)\n✅\n✅\n✅\n✅\n\n\ndevelopers (interaktivt)\n✅\n✅\n✅\n✅\n\n\n\n\n\n\n\n\n\n(b) Prod-miljø\n\n\n\n\n\n\n\n\n\n\n\n\nAktør\nValidator\nPseudonymize()\nDepseudonymize()\nRepseudonymize()\n\n\n\n\nKildomaten\n✅\n✅\n🚫\n🚫\n\n\ndata-admins (interaktivt)\n🚫\n🚫\n🚫\n🚫\n\n\ndevelopers (interaktivt)\n🚫\n🚫\n🚫\n🚫\n\n\n\n\n\n\n\n\n\nI Tabell 1 ser vi fra Tabell 1 (a) at man i test-miljøet har full tilgang til funksjonaliteten i dapla-toolbelt-pseudo, både fra Kildomaten og når man jobber interaktivt2 i Jupyterlab. Tabell 1 (b) viser at det kun er tilgang til pseudonymize() og validator() fra Kildomaten i prod-miljøet, og man kan aldri interaktivt kan kalle på funksjoner som potensielt avslører et pseudonym. Av den grunn er det alltid anbefalt å teste ut koden sin i test-miljøet før den produksjonssettes i i prod-miljøet med Kildomaten.\n\n\n\n\n\n\nCautionUlike pseudonymer i prod og test\n\n\n\nSelv om man har videre rettigheter til å bruke funksjonaliteten i dapla-toolbelt-pseudo fra test-miljøet sammenlignet med prod-miljøet, så betyr ikke det at samme input i de to miljøene vil samme output. Når funksjonaliteten kalles fra test-miljøet så benyttes det automatisk en annen krypteringsnøkkel enn den som benyttes i prod. På den måten vil et pseudonym produsert fra prod-miljøet aldri være likt det som produseres fra prod-miljøet selv om input skulle være den samme.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-pseudo"
    ]
  },
  {
    "objectID": "statistikkere/dapla-pseudo.html#funksjonalitet",
    "href": "statistikkere/dapla-pseudo.html#funksjonalitet",
    "title": "dapla-toolbelt-pseudo",
    "section": "Funksjonalitet",
    "text": "Funksjonalitet\nI denne delen viser vi hvilken funksjonalitet som tilbys gjennom dapla-toolbelt-pseudo. Eksempelkoden under viser hvordan man ville kjørt det fra en notebook i test-miljøet til teamet, og ikke hvordan koden må skrives når det skal kjøres i Kildomaten3.\n\nInstallering\ndapla-toolbelt-pseudo er ferdig installert i Kildomaten. Men ønsker du å bruke den i test-miljøet til teamet så kan du installere det i et ssb-project fra PyPI med denne kommandoen:\n\n\nTerminal\n\npoetry add dapla-toolbelt-pseudo\n\n\n\nDataformater\ndapla-toolbelt-pseudo støtter innlesing av følgende dataformater:\n\nCSV\nJSON\nPandas dataframes\nPolars dataframes\n\nEksemplene under viser hovedskelig innlesing av dataframes fra minnet4, men du kan lese mer om filbasert prosessering lenger ned i kapitlet.\n\n\nPseudonymisering\nPseudonymisering tilbys via Pseudonymize-metoden i dapla-toolbelt-pseudo. Den følger et builder-pattern der man spesifiserer hva og i hvilken rekkefølge operasjonene skal gjøres. Anta at det finnes en Polars dataframe i minnet som heter df hvor kolonnen fnr skal pseudonymiseres. Da vil koden se slik ut:\n\n\nNotebook\n\nfrom dapla_pseudo import Pseudonymize\n\nresult_df = (\n    Pseudonymize.from_polars(df) \n    .on_fields(\"fnr\")\n    .with_default_encryption()                     \n    .run()                                \n    .to_polars()                                   \n)\n\nI koden over så angir from_polars(df) at kolonnen vi ønsker å pseudonymisere ligger i en Polars dataframe i minnet og heter df. Deretter spesifiserer vi at kolonnen fnr er den som skal behandles med funksjonen on_fields(\"fnr\"). Videre angir with_default_encryption() at fnr skal pseudonymiseres med dapla-toolbelt-speudo sin standard-algoritme5. Til slutt ber vi om at det ovennevnte blir kjørt med funksjonen run(), og at dataene skal returneres som en Polars dataframe med funksjonen to_polars().\nSe flere eksempler i dokumentasjonen.\n\n\nDe-pseudonymisering\nDe-pseudonymisering tilbys via Depseudonymize-metoden i dapla-toolbelt-pseudo. Den følger et builder-pattern der man spesifiserer hva og i hvilken rekkefølge operasjonene skal gjøres. Anta at det finnes i en Polars dataframe i minnet som heter df hvor kolonnen fnr skal de-pseudonymiseres. Da vil koden se slik ut:\n\n\nNotebook\n\nresult_df = (\n    Depseudonymize.from_polars(df)            \n    .on_fields(\"fnr\")                          \n    .with_default_encryption()                     \n    .run()                                         \n    .to_polars()                                   \n)\n\nOppbygningen av koden med Depseudonymize() er helt lik som for Pseudonomize(). Les beskrivelsen der for å se hva de ulike funksjonskallene gjør. Se flere eksempler i dokumentasjonen.\nDe-pseudonymisering er også støttet for informasjon som først er transformert til stabil ID og deretter pseudonymisert med Papis-nøkkelen. I disse tilfellene kan det også være behov for å spesifisere hvilken versjon av snr-katalogen man ønsker å benytte for å erstatte snr med fødselsnummer:\n\n\nNotebook\n\nfrom dapla_pseudo import Depseudonymize\n\nresult_df = (\n    Depseudonymize.from_pandas(df)            \n    .on_fields(\"fnr\")                          \n    .with_stable_id(\n      sid_snapshot_date=\"2023-05-29\")                    \n    .run()                                         \n    .to_pandas()                                   \n)\n\nI eksempelet over spesifiserer vi datoen 2023-05-29 og da benyttes snr-katalogen som ligger nærmest i tid til denne datoen. Hvis sid_snapshot_date ikke oppgis benyttes siste tilgjengelige versjon av katalogen.\n\n\n\n\n\n\nImportantDe-pseudonymisering ikke tilgjengelig i prod-miljø\n\n\n\nForeløpig er det kun tilgang til å pseudonymisere i test-miljøet med test-data. Ta kontakt med Dapla-teamene dersom det dukker opp behov for å kunne de-pseudonymisere i prod-miljøet.\n\n\n\n\nRe-pseudonymisering\nUnder utvikling.\n\n\nStabil ID\nI statistikkproduksjon og forskning er det viktig å kunne følge de samme personene over tid. Derfor har fødselsnummer ofte blitt oversatt til en mer stabilt identifikator, ofte kalt SNR eller stabil ID6. Funksjonene with_stable_id() og validator() bruker stabilID-katalogen til å henholdsvis bytte ut fødselsnummer med stabil ID, og for å validere om fødselsnummer er gyldige(se under).\nDu kan selv spesifisere hvilken versjon av SNR-katalogen du ønsker å bruke. Det gjør du ved å oppgi en gyldighetsdato og så finner dapla-toolbelt-pseudo hvilken versjon av katalogen som ligger nærmest i tid.\n\n\nPapis-pseudonym\nDapla tilbyr samme pseudonym som Papis-prosjektet7. Denne kan benyttes på 2 måter:\n\nPseudonymisere hvilken som helst informasjon med samme nøkkel som Papis.\nTransformere fødselsnummer til snr-nummer og deretter pseudonymisere med samme nøkkel som Papis.\n\nPunkt 1 er nyttig for de som har pseudonymisert informasjon på bakken tidligere og vil ha samme pseudonym på Dapla8. Dette kan gjelde hvilken som helst informasjon, også direkte pseudonymisering av fødselsnummer, uten at det er gått via snr-nummer. Her er et eksempel på hvordan man pseudonymiserer på denne måten:\n\n\nNotebook\n\nresult = (\n    Pseudonymize.from_pandas(df)\n    .on_fields(\"fornavn\")                      \n    .with_papis_compatible_encryption()         \n    .run()                               \n    .to_pandas()                                  \n)\n\nPunkt 2 over er nok den som benyttes mest i SSB siden den sikrer at pseudonymisert fødselsnummer kan kobles med data som er pseudonymisert på bakken. Her er et eksempel på hvordan man pseudonymiserer snr med Papis-nøkkelen:\n\n\nNotebook\n\nresult = (\n    Pseudonymize.from_pandas(df)\n    .on_fields(\"fnr\")                      \n    .with_stable_id()         \n    .run()                               \n    .to_pandas()                                  \n)\n\n\n\n\n\n\n\nNoteHva betyr det å tilby samme pseudonym?\n\n\n\nAt Papis og Dapla tilbyr samme pseudonum betyr egentlig at vi bruker samme krypteringsalgoritme, og en felles krypteringsnøkkel. Krypteringsalgoritmen som benyttes er formatpreserverende (FPE) og biblioteket som brukes er Tink FPE Python. En begrensning med algorimen er at kun karakterer som finnes i et forhåndsdefinert karaktersett (tall, store og små bokstaver fra a-z) blir vurdert. Andre karakterer f. eks æøå blir ikke kryptert. Papis-nøkkelen (som brukes f. eks for fnr og snr) benytter en SKIP-strategi for karakterer som faller utenom, som betyr at algoritmen simpelthen “hopper over” disse. FPE-algoritmen er også avhengig av størrelsen på det predefinterte karaktersettet for å avgjøre minimumslengden på teksten som pseudonymiseres. For Papis-nøkkelen betyr det at teksten minst må være 4 karakterer lang. Kortere tekster blir ikke kryptert.\n\n\n\n\nValidere fødselsnummer\nValidator-metoden kan benyttes til å sjekke om fødselsnummer finnes i SNR-katalogen (se over), og returnerer de ugyldige fødselsnummerne tilbake. Her kan man også spesifisere hvilken versjon av SNR-katalogen man ønsker å bruke. Standard, hvis ingenting velges, er at siste tilgjengelige versjon velges. Under er et eksempel på hvordan man validerer fødselsnummer for en gitt gyldighetsdato:\n\n\nNotebook\n\nfrom dapla_pseudo import Validator\n\nresult = (\n    Validator.from_polars(df)\n    .on_field(\"fnr\")\n    .validate_map_to_stable_id(\n        sid_snapshot_date=\"2023-08-29\"\n    )\n)\n# Vis hvilken versjon av SNR-katalogen som er benyttet\nresult.metadata\n# Vis fødselsnummer som ikke fikk treff i SNR-katalogen som en Polars dataframe\nresult.to_polars()\n# Eventuelt som en Pandas DataFrame\nresult.to_pandas()\n\nSe flere eksempler i dokumentasjonen.\n\nWildcards\nKommer snart.\n\n\n\nDataminimering\nKommer snart.\n\n\nAlgoritmer\nKommer snart.\n\n\nMetadata\nDet genereres to typer av metadata når man pseudonymiserer:\n\nDatadoc\nMetadata\n\nDe to typene av metadata returneres til brukeren i to forskjellige objekter.\n\nDatadoc\n\n\nDatadoc-metadata er på et format som er planlagt integrert i Datadoc9 på et senere tidspunkt. I koden til høyre så printes metadataene fra et kall til Pseudonomize ved å skrive print(result.datadoc). Da printer man objektet interaktivt i f.eks. Jupyterlab, noe som kun er mulig i test-miljøet. Skal man kjøre dette i Kildomaten så er det lettere å skrive filen direkte til riktig json-format med to_file-funksjonen. Da får får filen endelsen __DOC på slutten av filnavnet, og man slipper å tenke på om filen skrives med riktig formattering, osv..\n\n\n\n\n\nNotebook\n\nresult = (\n    Pseudonymize.from_polars(df)    \n    .on_fields(\"fnr\")           \n    .with_stable_id()\n    .run()                      \n)\n\n1print(result.datadoc)\n2result.to_file(\"gs://bucket/test.parquet\")\n\n\n1\n\nPrinter metadata i en Notebook.\n\n2\n\nSkriver metadata direkte til bøtte.\n\n\n\n\nNår man kjører pseudonymisering fra Kildomaten er det viktig å tenke på at felter som er pseudonymisert ikke må endres uten at metadataene også endrer. Da kan man risikere at metadatene ikke lenger beskriver riktig informasjon.\nUnder ser man hvilken informasjon som genereres fra pseudonymiseringen.\n\n\nDatadoc\n\n{\n  \"document_version\": \"0.0.1\",\n  \"datadoc\": null,\n  \"pseudonymization\": {\n    \"document_version\": \"0.1.0\",\n    \"pseudo_dataset\": null,\n    \"pseudo_variables\": [\n      {\n        \"short_name\": \"fnr\",   \n        \"data_element_path\": \"fnr\",\n        \"data_element_pattern\": \"**\",\n        \"stable_identifier_type\": \"FREG_SNR\",\n        \"stable_identifier_version\": \"2023-08-31\",\n        \"encryption_algorithm\": \"TINK-FPE\",\n        \"encryption_key_reference\": \"papis-common-key-1\",\n        \"encryption_algorithm_parameters\": [\n          {\n            \"keyId\": \"papis-common-key-1\"\n          },\n          {\n            \"strategy\": \"SKIP\"\n          }\n        ],\n        \"source_variable\": null,\n        \"source_variable_datatype\": null\n      }\n    ]\n  }\n}\n\nAv metadatene kan vi se fra pseudo_variables at det bare var feltet fnr som ble pseudonymisert. Vi ser også av stable_identifier_type ser vi at fnr ble oversatt til snr, og at versjonen av SNR-katalogen var fra 2023-08-31. encryption_algorithm angir at det var det var den formatpreserverende algoritmen TINK-FPE som ble benyttet. keyID: \"papis-common-key-1\" angir hvilken nøkkel-id som ble benyttet. strategy: \"SKIP\" refererer til at den format-preserverende algoritmen skal “hoppe over” ugyldige karakterer og la de være som de er.\nDenne informasjonen vil være svært nyttig i SSB hvis man senere skal kunne de-pseudonymisere eller re-pseudonymisere data.\n\n\nMetadata\nDen andre typen av metadata kan hentes ut etter et kall til Pseudonymize med kommandoen result.metadata. Den returnerer en python dictionary. Den inneholder hovedsaklig logginformasjon og metrikker foreløpig. For de som pseudonymiserer med with_stable_id() kan output se slik ut:\n\n\nMetadata\n\n{\n    'fnr': {\n        'logs': [\n            'No SID-mapping found for fnr 999999*****',\n            'No SID-mapping found for fnr XX',\n            'No SID-mapping found for fnr X8b7k2*'\n        ],\n        'metrics': [\n            {'MAPPED_SID': 10},\n            {'MISSING_SID': 3}\n        ]\n    }\n}\n\nI Kildomaten kan det vært nyttig å printe denne informasjonen til loggene. Av eksempelet over ser vi at verdier som er over 6 karakterer lange blir maskert.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-pseudo"
    ]
  },
  {
    "objectID": "statistikkere/dapla-pseudo.html#brukerveiledning",
    "href": "statistikkere/dapla-pseudo.html#brukerveiledning",
    "title": "dapla-toolbelt-pseudo",
    "section": "Brukerveiledning",
    "text": "Brukerveiledning\nPå grunn av den strenge tilgangsstyringen til dapla-pseudo-toolbelt og kildedata er det anbefalt å utvikle kode for overgangen fra kildedata til inndata i test-miljøet til teamet. I denne delen viser vi hvordan denne arbeidsflyten kan se ut, fra testing til en automatisert produksjon med ekte data, med et helt konkret eksempel.\n\nInteraktiv utvikling\nFor å kunne kjøre pseudonymiseringen interaktivt i f.eks. en notebook i Jupyter, så må man jobbe i test-miljøet til teamet.\n\n\nKildomaten i test\nKommer snart.\n\n\nKildomaten i prod\nKommer snart.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-pseudo"
    ]
  },
  {
    "objectID": "statistikkere/dapla-pseudo.html#footnotes",
    "href": "statistikkere/dapla-pseudo.html#footnotes",
    "title": "dapla-toolbelt-pseudo",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nEt Dapla-team har både et test- og et prod-miljø. Kildomaten må være skrudd på i det miljøet du ønkser å benytte dapla-toolbelt-pseudo fra.↩︎\nMed interaktiv jobbing menes at man skriver og kode og får tilbake output i samme verktøy. F.eks. er Jupyterlab et eksempel på et verktøy som lar deg jobbe interaktivt med data.↩︎\nI Kildomaten må koden blant annet pakke inn i main()-funksjon.↩︎\nPandas og Polars dataframes er eksempler på dataformater som lever i minnet, og må konverteres før de skrives til et lagringsommråde. I praksis vil det ofte si at man jobber med dataframes når man jobber i verktøy som Jupyterlab, mens man skriver til lagringsområde når man er ferdig i Jupyterlab.↩︎\nStandardalgoritmen i dapla-toolbelt-pseudo er den deterministiske krypteringsalgoritmen Deterministic Authenticated Encryption with Associated Data, eller DAEAD-algoritmen.↩︎\nSNR-katalogen eies og tilbys av Team Register på Dapla.↩︎\nPapis var et prosjekt med fokus på bakkesystemene i SSB som skulle bringe SSBs behandling av personopplysninger, som brukes i statistikkproduksjon, i samsvar med GDPR gjennom en enhetlig pseudonymiseringsløsning.↩︎\nGenerelt sett er det ikke å anbefale å benytte denne nøkkelen på annen informasjon enn fødselsnummer. Grunnen er at den er svakere enn andre algoritmer, der blant annet tekst som er kortere enn 4 karakter lang ikke blir pseudonymisert.↩︎\nDatadoc er det nye systemet for dokumentasjon av datasett i SSB. Systemet er fortsatt under utvikling.↩︎",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "dapla-toolbelt-pseudo"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html",
    "href": "statistikkere/rstudio.html",
    "title": "Rstudio",
    "section": "",
    "text": "Rstudio er en tjeneste på Dapla Lab for utvikling av kode i R1. Målgruppen for tjenesten er brukere som skal skrive produksjonskode i R2.\nSiden tjenesten er ment for produksjonskode så er det veldig få forhåndsinstallerte R-pakker. Antagelsen er at brukerene/teamet heller bør installere de pakkene de trenger, framfor at alle pakker som alle brukere/team er forhåndsinstallert og skal dekke behovet til alle. Det reduserer kompleksitet i tjenesten og dermed sannsynligheten for feilsituasjoner.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html#forberedelser",
    "href": "statistikkere/rstudio.html#forberedelser",
    "title": "Rstudio",
    "section": "Forberedelser",
    "text": "Forberedelser\nFør man starter Rstudio bør man ha lest kapitlet om Dapla Lab og satt opp Git- og GitHub-konfigurasjonen under Min konto. Deretter gjør du følgende:\n\nLogg deg inn på Dapla Lab\nUnder Tjenestekatalog trykker du på Start-knappen for Rstudio\nGi tjenesten et navn\nÅpne Rstudio konfigurasjoner",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html#konfigurasjon",
    "href": "statistikkere/rstudio.html#konfigurasjon",
    "title": "Rstudio",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nKonfigurasjonen av RStudio er nær identisk Jupyter-tjenesten sin konfigurasjon. Se dokumentasjonen for konfigurasjon av Jupyter.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html#datatilgang",
    "href": "statistikkere/rstudio.html#datatilgang",
    "title": "Rstudio",
    "section": "Datatilgang",
    "text": "Datatilgang\nSlik kan man inspisere dataene fra en terminal inne i tjenesten:\n\nÅpne en instans av Rstudio med data fra bøtter\nÅpne en terminal inne i Rstudio\nGå til mappen med bøttene ved å kjøre dette fra terminalen cd /buckets\nKjør ls -ahl i teminalen for å se på hvilke bøtter som er montert.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html#installere-pakker",
    "href": "statistikkere/rstudio.html#installere-pakker",
    "title": "Rstudio",
    "section": "Installere pakker",
    "text": "Installere pakker\nSiden det nesten ikke er installert noen pakker i tjenesten så kan brukeren opprette et renv og installere pakker som ønsker.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html#slette-tjenesten",
    "href": "statistikkere/rstudio.html#slette-tjenesten",
    "title": "Rstudio",
    "section": "Slette tjenesten",
    "text": "Slette tjenesten\nFor å slette tjenesten kan man trykke på Slette-knappen i Dapla Lab under Mine tjenester. Når man sletter en tjeneste så sletter man hele disken inne i tjenesten og frigjør alle ressurser som er reservert. Siden pakkene som er installert også ligger lagret på disken, betyr dette at pakkene må installeres på nytt etter at en tjeneste er slettet. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html#pause-tjenesten",
    "href": "statistikkere/rstudio.html#pause-tjenesten",
    "title": "Rstudio",
    "section": "Pause tjenesten",
    "text": "Pause tjenesten\nMan kan pause tjenesten ved å trykke på Pause-knappen i Dapla Lab under Mine tjenester. Når man pauser så slettes alt påden lokale disken som ikke er lagret under $HOME/work. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html#monitorering",
    "href": "statistikkere/rstudio.html#monitorering",
    "title": "Rstudio",
    "section": "Monitorering",
    "text": "Monitorering\n\n\nMan kan moniterere en instans av Rstudio ved å trykke på Rstudio-teksten under Mine tjenester i Dapla Lab, slik som vist i Figur 1.\nDenne funksjonaliteten er under arbeid og mer informasjon kommer snart.\n\n\n\n\n\n\n\n\n\nFigur 1: Monitorering av Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/rstudio.html#footnotes",
    "href": "statistikkere/rstudio.html#footnotes",
    "title": "Rstudio",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nRstudio er web-versjonen av Rstudio og er ikke helt identisk med desktop-versjonen som mange er kjent med.↩︎\nPython er ikke installert i Rstudio-tjenesten.↩︎",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Rstudio"
    ]
  },
  {
    "objectID": "statistikkere/vardef-forvaltning.html",
    "href": "statistikkere/vardef-forvaltning.html",
    "title": "Vardef-forvaltning",
    "section": "",
    "text": "Vardef-forvaltning er en tjeneste på Dapla Lab som lar brukere forvalte informasjon i Vardef med ferdiglagde notebooks i en instans av Jupyterlab. Tjenesten kommer med Python, dapla-toolbelt-metadata og tilhørende avhengigheter ferdig installert. I tillegg er det opprettet en kernel med navn variable_definitions som lar brukerne jobbe med Vardef uten noen ytterligere oppsett.\nNotebooks’ene som ligger inne i tjenesten inneholder dokumentasjon for vanlige brukerflyter i Vardef. Les mer notebooks’ene under Funksjonalitet senere i artikkelen.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vardef-forvaltning"
    ]
  },
  {
    "objectID": "statistikkere/vardef-forvaltning.html#forberedelser",
    "href": "statistikkere/vardef-forvaltning.html#forberedelser",
    "title": "Vardef-forvaltning",
    "section": "Forberedelser",
    "text": "Forberedelser\nFør man starter Vardef-forvaltning bør man ha lest kapitlet om Dapla Lab og gjort seg godt kjent med dokumentasjonen til Vardef. Deretter gjør du følgende:\n\nLogg deg inn på Dapla Lab\nUnder Tjenestekatalog trykker du på Start-knappen for Vardef-forvaltning\nGi tjenesten et navn\nVelg hvilket team og tilgangsgruppe du ønsker å representere (se Konfigurasjon)\nTrykk Start når du er klar for å starte tjenesten",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vardef-forvaltning"
    ]
  },
  {
    "objectID": "statistikkere/vardef-forvaltning.html#konfigurasjon",
    "href": "statistikkere/vardef-forvaltning.html#konfigurasjon",
    "title": "Vardef-forvaltning",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nDen eneste konfigurasjonen som må gjøres i Vardef-forvaltning er å velge hvilket team og tilgangsgruppe man ønsker å representere når man jobber i tjenesten. Dette er måten Vardef håndterer tilgangsstyring og det er viktig å være klar over når man jobber med Vardef fra Dapla Lab. Siden variabeldefinisjoner i Vardef forvaltes av team, så må man velge riktig team og tilgangsgruppe under Data i tjenestekonfigurasjonen før man starter tjenesten. F.eks. hvis dapla-felles-developers forvalter variabeldefinisjonen fnr, så må jeg velge dapla-felles-developers under Data før jeg starter tjenesten. Og tilsvarende hvis jeg ønsker å opprette en ny variabeldefinisjon, så må jeg representere det teamet jeg ønsker at skal forvalte variabeldefinisjonen før jeg oppretter den.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vardef-forvaltning"
    ]
  },
  {
    "objectID": "statistikkere/vardef-forvaltning.html#funksjonalitet",
    "href": "statistikkere/vardef-forvaltning.html#funksjonalitet",
    "title": "Vardef-forvaltning",
    "section": "Funksjonalitet",
    "text": "Funksjonalitet\nVardef-forvaltning er en tilrettelagt variant av Jupyter. Når man åpner tjenesten så er følgende tilrettelagt for brukeren:\n\ndapla-toolbelt-metadata er installert\nen kernel med navnet variable_definitions er opprettet\nflere ferdigskrevne notebooks er installert (lenke i høyre marg)\n\nNotebooks’ene består av kode og beskrivelser for vanlige brukerflyter i Vardef. F.eks. migrere fra Vardok, opprette ny variabeldefinisjon, redigere utkast, osv.. I hver notebook benyttes dapla-toolbelt-vardef til å jobbe mot Vardef. På den måten slipper brukerne i stor grad å skrive kode selv, og de kan benytte koden som finnes til utvikling av ny kode.\n\nMigrere fra Vardok\nØnsker man å migrere en eller flere variabeldefinisjoner kan man kjøre notebooken migrer_variabeldefinisjoner.ipynb.\nI variabelen min_id_liste som du finner i cellen nedenfor overskriften “Hente variabeldefinisjoner fra Vardok”, må det settes inn en eller flere variabeldefinisjons-ID’er. Alle seksjonsledere har oversikt over hvilke ID’er fra Vardok som deres seksjon er ansvarlig for. Kjør deretter gjennom notebook’en. Det vil skrives ut en oppsummering over hva som blir migrert.\nNår notebook’en er kjørt gjennom så er variabeldefinisjonene migrert fra Vardok til Vardef, der de får status Utkast. I denne prosessen oppdaterer dapla-toolbelt-metadata all informasjon som kan utledes automatisk, og deretter må brukeren oppdatere gjenværende informasjon før de publiserer den internt eller eksternt.\n\nOppdatere utkast\nEt utkast kan oppdateres ved å kjøre notebooken rediger_utkast_variabeldefinisjon.ipynb. Den skriver variabeldefinisjonen ned til en fil som brukeren kan gjøre endringer i. Når endringene er gjort, så kan man oppdatere utkastet i slutten av notebooken.\n\n\nPublisere utkast\nNår brukeren er fornøyd med utkastet og ønsker å publisere, så kan man velge å publisere internt eller eksternt. Ved migrering av tidligere eksternt publiserte definisjoner fra Vardok, så er hovedregelen at også man publiserer variabeldefinisjonen eksternt i Vardef.\nMan kan publisere en variabeldefinisjon eksternt ved å kjøre notebook’en publiser_variabeldefinisjon_eksternt.ipynb.\nMan kan publisere en variabeldefinisjon internt ved å kjøre notebook’en publiser_variabeldefinisjon_internt.ipynb.\n\n\n\nNy variabeldefinisjon\nFor å opprette en ny variabeldefinisjon må man først opprette et utkast, deretter redigere utkastet og til slutt publisere. Da vil man bruke følgende notebooks:\n\nny_variabeldefinisjon.ipynb\nrediger_utkast_variabeldefinisjon.ipynb\npubliser_variabeldefinisjon_internt.ipynb eller publiser_variabeldefinisjon_eksternt.ipynb\n\n\n\nNy gyldighetsperiode\nNy gyldighetsperiode i Vardef betyr at man ønsker å gjøre endringer i en eksisterende variabeldefinisjon som gjør at den får en ny betydning. Dette betyr at definisjonsteksten endres, og denne endringen krever at den får en ny gyldighetsperiode. Dette kan kun gjøres på variabeldefinisjoner som allerede er publisert internt eller eksternt.\nFor opprette en ny gyldighetsperiode kan man kjøre gjennom notebooken ny_gyldighetsperiode_variabeldefinisjon.ipynb.\nHer blir man bedt om å oppgi kortnavnet til variabeldefinisjonen som skal ha ny gyldighetsperiode, deretter skrives den til en fil hvor endringer kan gjøres. Til slutt oppdateres variabeldefinisjonen i Vardef.\n\n\nMindre endringer (patch)\nEn patch i Vardef er en mindre endring som ikke endrer betydningen av variabeldefinisjonen. Man kan opprette en patch ved å kjøre notebook’en rediger_publisert_variabeldefinisjon.ipynb.\n\n\nLese ut informasjon\nFor å hente ut informasjon fra Vardef finnes det ulike notebooks avhengig av hva man ønsker å hente ut.\n\nEn variabeldefinisjon\nNotebooken les_variabeldefinisjon.ipynb lar deg hente ut informasjon om en konkret variabeldefinisjon. Man kan hente ut all informasjon om variabeldefinisjonen, eller hente ut noen få informasjonselementer.\n\n\nAlle variabeldefinisjoner\nNotebooken les_variabeldefinisjoner.ipynb lar deg hente ut informasjon om alle variabeldefinisjoner og filtrere disse basert på ulike kriterier.\n\n\nLedige kortnavn\nNotebooken sjekk_kortnavn_variabeldefinisjon.ipynb lar deg sjekke om et kortnavn er tatt eller ikke. Kortnavn i Vardef må være unikem, så det kan være lurt å sjekke om ønsket kortnavn er ledig før man prøver å publisere.\n\n\n\nArbeide med YAML-filer\n\n\n\n\n\n\nCaution\n\n\n\nFargene i YAML-eksemplene er kun veiledende og kan variere fra verktøy til verktøy, ettersom de settes av automatisk syntaksutheving.\n\n\n\nYAML-filstruktur\nYAML-filene følger samme grunnstruktur, med små variasjoner avhengig av bruk.\n\nMal\nDenne filen genereres når du vil opprette en ny variabeldefinisjon.\n# --- Variabeldefinisjon mal ---\n\n\n\n\n\n\nNoteFilnavn mal\n\n\n\nLagres som variable_definition_template_&lt;timestamp&gt;.yaml\n\n\n\n\n\n\n\n\nCautionNB! Innholder eksempelverdier som må endres\n\n\n\ncontact:\n    title:\n        nb: |-\n            generert tittel\n        nn:\n        en:\n\n\nI noen tilfeller settes verdien automatisk av systemet. For eksempel:\nvariable_status: DRAFT\n\n\nVariabeldefinisjon\nDette er en YAML-fil med alle lagrede verdier for en variabeldefinisjon.\n# --- Variabeldefinisjon ---\n\n\n\n\n\n\nNoteFilnavn variabeldefinisjon\n\n\n\nLagres som variable_definition_&lt;kortnavn&gt;_&lt;id&gt;_&lt;timestamp&gt;.yaml\n\n\n\n\n\nRedigere YAML-filen\n\nEnkle verdier\n\nVerdien skrives på samme linje som feltnavn (nøkkel)\nBruk doble fnutter \"\"\n\n\n\n\n\n\n\nNoteFelt med enkle verdier\n\n\n\n\n\n\nclassification_reference\nmeasurement_type\nexternal_reference_uri\nowner.team\n\n\n\n\n\n\n\n\n\n\nTipKorrekt plassering og format\n\n\n\nshort_name: \"landbak\"\n\nmeasurement_type: \"03\"\n\n\n\nVanlige feil enkle verdier\n\n\n\n\n\n\nWarningFeil plassering av verdi\n\n\n\nmeasurement_type: \n\"03\"\n\n\n\n\n\n\n\n\nWarningFeil innrykk mellom nøkkel og verdi\n\n\n\nclassification_reference:\"91\"\n\n\n\n\n\n\n\n\nWarningMangler kolon\n\n\n\nshort_name\"landbak\"\n\n\n\n\n\n\n\n\nWarningFeil verdi type\n\n\n\n# Hvis fnuttene fjernes vil verdien tolkes som et tall og validering vil feile.\nmeasurement_type: 03\n\n\n\n\n\nFlerspråklige felt\n\nEt blokk symbol |- følger feltnavn(nøkkel).\nTeksten må starte rett under blokk-symbolet\nRiktig innrykk er viktig\nFormattering (avsnitt/linjeskift) lagres.\n\n\n\n\n\n\n\nNoteFlerspråklige felt\n\n\n\n\n\n\nname\ndefinition\ncomment\ncontact.title\n\n\n\n\n\n\n\n\n\n\nTipKorrekt plassering av tekst under blokk-symbol\n\n\n\n\n\ncontact:\n    title:\n        nb: |-\n            Her starter teksten akkurat under blokk symbolet og vil lagres uten feil.\n        nn:\n        en:\n    email: generert@ssb.no\n\n\n\n\n\n\n\n\n\nTipKorrekt lagring av avsnitt\n\n\n\n\n\ncomment:\n    nb: |-\n        Her er det en god plan, først er det en beskrivelse som innledning.\n\n        Deretter bevisst en blank linje:\n          - Liste-element 1\n          - Liste-element 2\n    nn:\n    en:\n\n\n\n\n\n\n\n\n\nNoteBruk av spesialtegn\n\n\n\n\n\nname:\n    nb: |-\n        Her er kan vi lagre spesialtegn som : men det kan føre til feil syntaksutheving.\n    nn:\n    en:\n\n\n\n\nVanlige feil flerspråklige felt\n\n\n\n\n\n\nCautionUønsket tekstoppsett (linjeskift midt i ord)\n\n\n\n\n\nname:\n    nb: |-\n        Her deles teksten opp\n        plutselig. Og den vil lagres akku-\n        rat slik du ser den.\n    nn:\n    en:\n\n\n\n\n\n\n\n\n\nWarningFeil indentering (mangler korrekt innrykk)\n\n\n\n\n\nname:\n    nb: |-\nHer er teksten ikke indentert og vil føre til feil når du lagrer.\n    nn:\n    en:\n\n\n\n\n\n\n\n\n\nWarningMangler blokk-symbolet\n\n\n\n\n\ncomment:\n    nb: Her er blokk symbolet fjernet og tekst med ':' vil feile.\n    nn:\n    en: Also text with paragraphs\n    will not be saved correctly.\n\n\n\n\n\n\nLister\n\nVerdien skrives på samme linje som liste-symbolet -\nBruk doble fnutter \"\"\n\n\n\n\n\n\n\nNoteFelt med lister\n\n\n\n\n\n\nunit_types\nsubject_fields\nrelated_variable_definition_uris\nowner.groups\n\n\n\n\n\n\n\n\n\n\nTipKorrekt listeformat og indentering\n\n\n\nunit_types:\n    - \"20\"\n\n\n\nVanlige feil lister\n\n\n\n\n\n\nWarningFeil type i listeelementer\n\n\n\nunit_types:\n    - 01\n    - 02\n\n\n\n\n\n\n\n\nWarningFeil indentering på liste-symbol\n\n\n\nunit_types:\n- \"20\"\n\n\n\n\n\n\n\n\nWarningFeil indentering ved listeverdi\n\n\n\nsubject_fields:\n  -\"bf\"\n\n\n\n\n\nAndre typer\n\n\n\n\n\n\nTipDato\n\n\n\nvalid_from: 2003-01-01\n\n\n\n\n\n\n\n\nTipBoolean\n\n\n\ncontains_special_categories_of_personal_data: true\n\n\n\n\n\n\n\n\nTipVariabel status\n\n\n\n\nvariable_status: DRAFT\n\nvariable_status: PUBLISHED_INTERNAL\n\nvariable_status: PUBLISHED_EXTERNAL\n\n\n\n\n\nYAML-fil: Ny variabeldefinisjon\n# --- Variabeldefinisjon mal ---\nname:\n    nb: |-\n        Buss\n    nn:\n    en:\nshort_name: \"bus\"\ndefinition:\n    nb: |-\n        Buss er en motorvogn som er spesialkonstruert for å frakte mange passasjerer.\n    nn:\n    en:\nclassification_reference:\nunit_types:\n    - \"15\"\nsubject_fields:\n    - \"tr01\"\ncontains_special_categories_of_personal_data: false\nmeasurement_type:\nvalid_from: 2025-04-01\nvalid_until:\nexternal_reference_uri:\ncomment:\nrelated_variable_definition_uris:\ncontact:\n    title:\n        nb: |-\n            Seksjon for Nærings- og miljøstatistikk\n        nn:\n        en:\n    email: s400@ssb.no\n\n# --- Statusfelt. Verdi 'DRAFT' før publisering. Ikke rediger hvis du oppretter en ny variabeldefinisjon. ---\nvariable_status: DRAFT\n\n# --- Eierteam og grupper. Ikke rediger hvis du oppretter en ny variabeldefinisjon, verdien genereres ---\nowner:\n    team: \"aordning-register\"\n    groups:\n        - \"aordning-register-developers\"\n\n# --- Maskin-genererte felt. Ikke rediger. ---\nid: \"wypvb3wd\"\npatch_id: 1\ncreated_at: 2025-01-11 08:15:19.038000+00:00\ncreated_by: \"ano@ssb.no\"\nlast_updated_at: 2025-01-11 08:15:19.038000+00:00\nlast_updated_by: \"ano@ssb.no\"\n\n\nYAML-fil: Redigere utkast\nEndre:\n\nshort_name\ncontains_special_categories_of_personal_data\n\nLegge til:\n\nclassification_reference\nexternal_reference_uri\nsubject_fields\n\n# --- Variabeldefinisjon ---\nname:\n    nb: |-\n        Buss\n    nn:\n    en:\nshort_name: \"reg_bus\"\ndefinition:\n    nb: |-\n        Buss er en motorvogn som er spesialkonstruert for å frakte mange passasjerer.\n    nn:\n    en:\nclassification_reference: \"111\"\nunit_types:\n    - \"15\"\nsubject_fields:\n    - \"tr01\"\n    - \"tr04\"\ncontains_special_categories_of_personal_data: true\nmeasurement_type:\nvalid_from: 2025-01-01\nvalid_until:\nexternal_reference_uri: \"https://www.norges-motorvignforbund.no\"\ncomment:\nrelated_variable_definition_uris:\ncontact:\n    title:\n        nb: |-\n            Seksjon for Nærings- og miljøstatistikk\n        nn:\n        en:\n    email: s400@ssb.no\n\n# --- Statusfelt. Verdi 'DRAFT' før publisering. Ikke rediger hvis du oppretter en ny variabeldefinisjon. ---\nvariable_status: DRAFT\n\n# --- Eierteam og grupper. Ikke rediger hvis du oppretter en ny variabeldefinisjon, verdien genereres ---\nowner:\n    team: \"aordning-register\"\n    groups:\n        - \"aordning-register-developers\"\n\n# --- Maskin-genererte felt. Ikke rediger. ---\nid: \"wypvb3wd\"\npatch_id: 1\ncreated_at: 2025-01-11 08:15:19.038000+00:00\ncreated_by: \"ano@ssb.no\"\nlast_updated_at: 2025-01-11 08:15:19.038000+00:00\nlast_updated_by: \"ano@ssb.no\"\n\n\nYAML-fil: Redigere publisert\nEndre:\n\nRette skrivefeil i external_reference_uri\n\nLegge til:\n\nverdier på norsk nynorsk og engelsk:\n\nname\ndefinition\ncontact.title\n\nowner.groups\nrelated_variable_definition_uris\nmeasurement_type\n\n# --- Variabeldefinisjon ---\nname:\n    nb: |-\n        Buss \n    nn: |-\n        Buss\n    en: |-\n        Bus\nshort_name: \"reg_bus\"\ndefinition:\n    nb: |-\n        Buss er en motorvogn som er spesialkonstruert for å frakte mange passasjerer.\n    nn: |-\n        Buss er eit køyretøy som er spesialkonstruert for å frakte mange passasjerar.\n    en: |-\n        A bus is a motor vehicle that is specially designed to carry many passengers.\nclassification_reference: \"111\"\nunit_types:\n    - \"15\"\nsubject_fields:\n    - \"tr01\"\n    - \"tr04\"\ncontains_special_categories_of_personal_data: true\nmeasurement_type: \"02\"\nvalid_from: 2025-01-01\nvalid_until:\nexternal_reference_uri: \"https://www.norges-motorvognforbund.no\"\ncomment:\nrelated_variable_definition_uris:\n    - \"https://example.com/\"\ncontact:\n    title:\n        nb: |-\n            Seksjon for Nærings- og miljøstatistikk\n        nn: |-\n            Seksjon for Nærings- og miljøstatistikk\n        en: |-\n            Section for Industrial and Environmental Statistics\n    email: s400@ssb.no\n\n# --- Statusfelt. Verdi 'DRAFT' før publisering. Ikke rediger hvis du oppretter en ny variabeldefinisjon. ---\nvariable_status: PUBLISHED_INTERNAL\n\n# --- Eierteam og grupper. Ikke rediger hvis du oppretter en ny variabeldefinisjon, verdien genereres ---\nowner:\n    team: \"aordning-register\"\n    groups:\n        - \"aordning-register-developers\"\n        - \"arbmark-aku-developers\"\n\n# --- Maskin-genererte felt. Ikke rediger. ---\nid: \"wypvb3wd\"\npatch_id: 1\ncreated_at: 2025-01-11 08:15:19.038000+00:00\ncreated_by: \"ano@ssb.no\"\nlast_updated_at: 2025-03-25 10:15:19.038000+00:00\nlast_updated_by: \"ano@ssb.no\"\n\n\nYAML-fil: Ny gyldighetsperiode\nLegge til:\n\ncomment\n\nEndre:\n\nvalid_from\ndefinition\n\n# --- Variabeldefinisjon ---\nname:\n    nb: |-\n        Buss \n    nn: |-\n        Buss\n    en: |-\n        Bus\nshort_name: \"reg_bus\"\ndefinition:\n    nb: |-\n        Buss er en motorvogn som er spesialkonstruert for å frakte flere enn \n        16 passasjerer.\n    nn: |-\n        Buss er eit køyretøy som er spesialkonstruert for å frakte flere enn \n        16 passasjerar.\n    en: |-\n        A bus is a motor vehicle that is specially designed to carry more than \n        16 passengers.\nclassification_reference: \"111\"\nunit_types:\n    - \"15\"\nsubject_fields:\n    - \"tr01\"\n    - \"tr04\"\ncontains_special_categories_of_personal_data: true\nmeasurement_type: \"\"\nvalid_from: 2025-06-01\nvalid_until:\nexternal_reference_uri: \"https://www.norges-motorvognforbund.no\"\ncomment:\n    nb: |-\n        Krav til spesifikasjon av antall passasjerer i en buss for å skille \"buss\" fra \"minibuss\".\n    nn: |-\n        Krav til spesifikasjon av talet på passasjerar i ein buss for å skilje \n        mellom «buss» og «minibuss».\n    en: |-\n        Requirements for specification of the number of passengers in a bus to distinguish \n        \"bus\" from \"minibus\".\nrelated_variable_definition_uris:\n    - \"https://example.com/\"\ncontact:\n    title:\n        nb: |-\n            Seksjon for Nærings- og miljøstatistikk\n        nn: |-\n            Seksjon for Nærings- og miljøstatistikk\n        en: |-\n            Section for Industrial and Environmental Statistics\n    email: s400@ssb.no\n\n# --- Statusfelt. Verdi 'DRAFT' før publisering. Ikke rediger hvis du oppretter en ny variabeldefinisjon. ---\nvariable_status: PUBLISHED_INTERNAL\n\n# --- Eierteam og grupper. Ikke rediger hvis du oppretter en ny variabeldefinisjon, verdien genereres ---\nowner:\n    team: \"aordning-register\"\n    groups:\n        - \"aordning-register-developers\"\n        - \"arbmark-aku-developers\"\n\n# --- Maskin-genererte felt. Ikke rediger. ---\nid: \"wypvb3wd\"\npatch_id: 2\ncreated_at: 2025-01-11 08:15:19.038000+00:00\ncreated_by: \"ano@ssb.no\"\nlast_updated_at: 2025-04-05 11:15:19.038000+00:00\nlast_updated_by: \"ano@ssb.no\"",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vardef-forvaltning"
    ]
  },
  {
    "objectID": "statistikkere/vardef-forvaltning.html#slette-tjenesten",
    "href": "statistikkere/vardef-forvaltning.html#slette-tjenesten",
    "title": "Vardef-forvaltning",
    "section": "Slette tjenesten",
    "text": "Slette tjenesten\nFor å slette tjenesten kan man trykke på Slette-knappen i Dapla Lab under Mine tjenester. Når man sletter en tjeneste så sletter man hele disken inne i tjenesten og frigjør alle ressurser som er reservert. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vardef-forvaltning"
    ]
  },
  {
    "objectID": "statistikkere/vardef-forvaltning.html#pause-tjenesten",
    "href": "statistikkere/vardef-forvaltning.html#pause-tjenesten",
    "title": "Vardef-forvaltning",
    "section": "Pause tjenesten",
    "text": "Pause tjenesten\nMan kan pause tjenesten ved å trykke på Pause-knappen i Dapla Lab under Mine tjenester. Når man pauser så slettes alt påden lokale disken som ikke er lagret under $HOME/work. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vardef-forvaltning"
    ]
  },
  {
    "objectID": "statistikkere/vardef-forvaltning.html#monitorering",
    "href": "statistikkere/vardef-forvaltning.html#monitorering",
    "title": "Vardef-forvaltning",
    "section": "Monitorering",
    "text": "Monitorering\n\n\nMan kan moniterere en instans av Jupyter ved å trykke på Jupyter-teksten under Mine tjenester i Dapla Lab, slik som vist i Figur 1.\nDenne funksjonaliteten er under arbeid og mer informasjon kommer snart.\n\n\n\n\n\n\n\n\n\nFigur 1: Monitorering av Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Vardef-forvaltning"
    ]
  },
  {
    "objectID": "statistikkere/jdemetra.html",
    "href": "statistikkere/jdemetra.html",
    "title": "Jdemetra",
    "section": "",
    "text": "Jdemetra er en tjeneste som tilbyr et grafisk grensesnitt (GUI) for sesongjustering og tidsserie-analyse. Formålet med tjenesten er å tilby statistikere i SSB et velkjent verktøy for opprette nye Jdemetra-workspaces, visuelt inspisere mange tidsserier samtidig, og benytte funksjonalitet som finnes for årlige evalueringer av modellene som benyttes.\nJdemetra+ er navnet på en samling programvare for tidsserie-analyse og sesongjustering som er utviklet av Belgias nasjonalbank i samarbeid med Eurostat, Insee og Deutche Bundesbank. I tillegg til GUI-et som tilbys på Dapla Lab, finnes det også et CLI for batch-prosessering som heter jwsacruncher, og en R-pakke ved navn RJDemetra. Alle bygger på de samme grunnleggende komponentene. Jwsacruncher er installert i Jupyter og Rstudio på Dapla Lab, mens RJdemetra kan installeres av brukeren selv i de samme tjenestene.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jdemetra"
    ]
  },
  {
    "objectID": "statistikkere/jdemetra.html#forberedelser",
    "href": "statistikkere/jdemetra.html#forberedelser",
    "title": "Jdemetra",
    "section": "Forberedelser",
    "text": "Forberedelser\nFør man starter Jdemetra-tjenesten bør man ha lest kapitlet om Dapla Lab. Deretter gjør du følgende:\n\nLogg deg inn på Dapla Lab\nUnder Tjenestekatalog trykker du på Start-knappen for Jdemetra\nGi tjenesten et navn\nÅpne Jdemetra konfigurasjoner",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jdemetra"
    ]
  },
  {
    "objectID": "statistikkere/jdemetra.html#konfigurasjon",
    "href": "statistikkere/jdemetra.html#konfigurasjon",
    "title": "Jdemetra",
    "section": "Konfigurasjon",
    "text": "Konfigurasjon\nFør man åpner en tjeneste kan man konfigurere hvilket team og tilgangsgruppe man skal representere og dermed hvilke bøtter man får tilgang til i Jdemetra. Man kan også velge hvilken versjon av Jdemetra man ønsker å kjøre, der default er siste versjon.\n\n\n\n\n\n\nFigur 1: Detaljert tjenestekonfigurasjon i JDemetra-tjenesten: data\n\n\n\nFigur 1 viser hvilke valg man gjøre under menyen Data. I tillegg viser bildet neddtrekksmenyen for hvilken versjon av Jdemetra man vil bruke. Først kan man velge hvilket team og tilgangsgruppe man ønsker å representere. I tillegg kan man aktivere kildedatatilgang. Alle i SSB er medlem av developers-gruppa i teamet Dapla Felles, derfor kan man velge dette teamet hvis man ønsker teste ut tjenesten.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jdemetra"
    ]
  },
  {
    "objectID": "statistikkere/jdemetra.html#datatilgang-og-lagring",
    "href": "statistikkere/jdemetra.html#datatilgang-og-lagring",
    "title": "Jdemetra",
    "section": "Datatilgang og lagring",
    "text": "Datatilgang og lagring\nNår man åpner Jdemetra, og velger å representere team og tilgangsgruppe, så blir bøttene som den tilgangsgruppa har tilgang til, tilgjengeliggjort som et filsystem under /buckets/. Som bruker kan du da lese og skrive til bøttene ved benytte denne filstien. F.eks. vil et statistikkteam som åpner Jdemetra som developers-gruppa ha et filsystem som typisk ser slik ut:\n\n\n/buckets/\n\n/buckets/  \n└─ produkt/  \n   ├── inndata/\n   ├── klargjorte-data/\n   ├── statistikk/\n   └── utdata/\n└─ frasky/  \n└─ tilsky/                     \n\nI eksempelet over ser vi at bøttene produkt, frasky og tilsky ligger under /buckets/.\n\nÅpne eksisterende workspace\nHvis jeg velger å representere gruppen dapla-felles-developers, så kan jeg åpne et Jdemetra-workspace som ligger i produktbøtta til team Dapla Felles ved å gjøre følgende:\n\nVelg File/Open workspace i menyen.\nFinn roten av filsystemet og åpne mappen /buckets/\nVelg xml-filen som definerer workspacet trykk Open\n\n\n\nOpprette nytt workspace\nFor å opprette et nytt workspace så importerer du input-filene på vanlig måte under Providers, legger de til i en workspace, og velger /File/Save Workspace As i menyen. Velg en filsti under /buckets/produkt/ for å lagre workspacet permanent i en bøtte.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jdemetra"
    ]
  },
  {
    "objectID": "statistikkere/jdemetra.html#slette-tjenesten",
    "href": "statistikkere/jdemetra.html#slette-tjenesten",
    "title": "Jdemetra",
    "section": "Slette tjenesten",
    "text": "Slette tjenesten\nFor å slette tjenesten kan man trykke på Slette-knappen i Dapla Lab under Mine tjenester. Når man sletter en tjeneste så sletter man hele disken inne i tjenesten og frigjør alle ressurser som er reservert. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jdemetra"
    ]
  },
  {
    "objectID": "statistikkere/jdemetra.html#pause-tjenesten",
    "href": "statistikkere/jdemetra.html#pause-tjenesten",
    "title": "Jdemetra",
    "section": "Pause tjenesten",
    "text": "Pause tjenesten\nMan kan pause tjenesten ved å trykke på Pause-knappen i Dapla Lab under Mine tjenester. Når man pauser så slettes alt påden lokale disken som ikke er lagret under $HOME/work. Vi anbefaler at man avslutter heller pauser tjenester som ikke benyttes.",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jdemetra"
    ]
  },
  {
    "objectID": "statistikkere/jdemetra.html#monitorering",
    "href": "statistikkere/jdemetra.html#monitorering",
    "title": "Jdemetra",
    "section": "Monitorering",
    "text": "Monitorering\n\n\nMan kan moniterere en instans ved å trykke på navnet på tjenesten under Mine tjenester i Dapla Lab, slik som vist i Figur 2 med en jupyter-instans.\nDenne funksjonaliteten er under arbeid og mer informasjon kommer snart.\n\n\n\n\n\n\n\n\n\nFigur 2: Monitorering av Jupyter-tjenesten i Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab",
      "Jdemetra"
    ]
  },
  {
    "objectID": "statistikkere/altinn-skjema-administrasjon.html",
    "href": "statistikkere/altinn-skjema-administrasjon.html",
    "title": "Administrasjon av skjema",
    "section": "",
    "text": "SU-V tilbyr både et grafisk brukergrensesnitt (GUI) og en Python-pakke for administrasjon av skjemaer. Du kan få tilgang til GUI via følgende lenker:\nMer informasjon om Python-pakken finner du her.\nAlle operasjoner og visninger som er tilgjengelige i GUI, kan også utføres ved hjelp av kode. Administrasjon av skjema innebærer håndtering av metadata knyttet til skjemaer, perioder, puljer og utsendinger.",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Administrasjon av skjema"
    ]
  },
  {
    "objectID": "statistikkere/altinn-skjema-administrasjon.html#tilgang-til-skjema",
    "href": "statistikkere/altinn-skjema-administrasjon.html#tilgang-til-skjema",
    "title": "Administrasjon av skjema",
    "section": "Tilgang til skjema",
    "text": "Tilgang til skjema\nAlle brukere har lesetilgang til skjemaene, noe som betyr at de kan se og hente data uten å gjøre endringer.\nFor å kunne utføre administrasjonsoppgaver og gjøre endringer knyttet til skjemaet, må du være medlem av data-admins eller developers i Dapla-teamet som eier skjemaet. Se mer om administrasjon av team her.",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Administrasjon av skjema"
    ]
  },
  {
    "objectID": "statistikkere/altinn-skjema-administrasjon.html#hierarki-for-skjema-administrasjon",
    "href": "statistikkere/altinn-skjema-administrasjon.html#hierarki-for-skjema-administrasjon",
    "title": "Administrasjon av skjema",
    "section": "Hierarki for skjema administrasjon",
    "text": "Hierarki for skjema administrasjon\nEt skjema består av følgende strukturer:\n\nSkjema kan inneholde flere perioder.\nEn periode kan inneholde flere puljer.\nEn pulje kan inneholde flere utsendinger.\n\nIllustrasjonen nedenfor viser sammenhengen mellom de ulike nivåene:\n\n\n\n\n\n\nFigur 1: Avhengigheter skjema administrasjon",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Administrasjon av skjema"
    ]
  },
  {
    "objectID": "statistikkere/altinn-skjema-administrasjon.html#generelle-ui-operasjoner",
    "href": "statistikkere/altinn-skjema-administrasjon.html#generelle-ui-operasjoner",
    "title": "Administrasjon av skjema",
    "section": "Generelle UI operasjoner",
    "text": "Generelle UI operasjoner\nMange av skjermbildene i GUI tilbyr de samme typene operasjoner. Disse operasjonene er beskrevet i tabellen nedenfor.\nTabell 1 beskriver generelle UI operasjoner.\n\n\n\nTabell 1: UI operasjoner\n\n\n\n\n\n\n\n\n\nIkon\nForklaring\n\n\n\n\n\nLegg til en ny rad.\n\n\n\nRedigere en rad.\n\n\n\nKopiere en rad.\n\n\n\nSlett en rad.\n\n\n\nVise detaljer for en rad.\n\n\n\nGå tilbake til forrige skjermbilde.",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Administrasjon av skjema"
    ]
  },
  {
    "objectID": "statistikkere/altinn-skjema-administrasjon.html#skjema",
    "href": "statistikkere/altinn-skjema-administrasjon.html#skjema",
    "title": "Administrasjon av skjema",
    "section": "Skjema",
    "text": "Skjema\nFor å begynne å jobbe med et skjema, må du finne riktig skjemaversjon i skjemakatalogen.\nSlik søker du fram et skjema\n\nVelg Skjemadata i venstremenyen.\nBruk søkefeltet i det nye skjermbildet for å finne skjemaet ditt.\nKlikk på Vis detaljer-ikonet i søkeresultatet for å se mer informasjon om skjemaet.\n\nBildet nedenfor viser eksempel på søkebilde:\n\n\n\n\n\n\nFigur 2: Søke fram skjema\n\n\n\n\n\n\n\n\n\nNoteHva hvis skjema mangler?\n\n\n\nHvis du ikke finner riktig skjemaversjon, ta kontakt med planleggeren for skjemaet på seksjon 821.\n\n\n\nSkjema metadata\nHvert skjema har tilknyttet metadata som gir detaljert informasjon om det. Bildet nedenfor viser et eksempel på metadata for et skjema: \n\n\n\n\n\n\nTipBeskrivelse av skjema metadata\n\n\n\n\n\nTabell 2 beskriver metadatafeltene for et skjema\n\n\n\nTabell 2: Skjema metadata\n\n\n\n\n\n\n\n\n\nMeta\nForklaring\n\n\n\n\nRA-nummer\nRA-nummer for skjemaet.\n\n\nVersjon\nSkjemaversjon.\n\n\nUnd.nr\nUndersøkelsesnummer.\n\n\nDatamodell\nNavn på datamodellen.\n\n\nNavn\nNavn på undersøkelsen.\n\n\nNavn nynorsk\nNavn på undersøkelsen (nynorsk).\n\n\nNavn engelsk\nNavn på undersøkelsen (engelsk).\n\n\nEier\nDapla-team.\n\n\nGyldig fra\nFra hvilken dato undersøkelsen er gyldig.\n\n\nGyldig til\nTil hvilken dato undersøkelsen er gyldig.\n\n\nURL infoside\nInformasjonsside for undersøkelsen.\n\n\nBeskrivelse\nTilleggsinformasjon om undersøkelsen.\n\n\n\n\n\n\n\n\n\n\n\nKodeeksempel\nFor å hente ut et skjema med tilhørende metadata i Python, kan du bruke metoden get_skjema_by_ra_nummer i SuvClient. Sørg for at du oppgir riktig RA-nummer og versjon.\n\n\nnotebook\n\nclient = SuvClient()\n\noutput = client.get_skjema_by_ra_nummer(\n            ra_nummer = \"RA-0678A3\", \n            versjon = 2 \n        )\n        \nprint(json.dumps(output, indent=4))\n\n\n\nLegge til skjema som favoritt\nFor å gjøre det enklere å finne skjemaene du jobber mest med, kan du legge dem til som favoritter. Dette er spesielt nyttig ettersom skjemakatalogen kan inneholde mange ulike skjemaer og versjoner.\nSlik legger du til et skjema som favoritt:\n\nSøk opp skjemaet: Bruk søkefunksjonen for å finne skjemaet du ønsker.\nVis detaljer: Klikk på skjemaet i tabellen for å åpne detaljvisningen.\nLegg til som favoritt: Trykk på stjerneikonet ved siden av skjemaets navn. Når stjernen er markert, er skjemaet lagt til som favoritt.\n\nSkjemaet vil deretter være tilgjengelig under Favoritter i venstremenyen.\nBildet nedenfor viser hvordan du velger et skjema som favoritt:\n\n\n\n\n\n\nFigur 3: Legge til skjema som favoritt",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Administrasjon av skjema"
    ]
  },
  {
    "objectID": "statistikkere/altinn-skjema-administrasjon.html#periode",
    "href": "statistikkere/altinn-skjema-administrasjon.html#periode",
    "title": "Administrasjon av skjema",
    "section": "Periode",
    "text": "Periode\nEt skjema kan ha en eller flere perioder knyttet til seg. Hvilke perioder som er lagt inn på skjemaet kan du se i detaljvisningen når du har valgt et skjema.\nAdministrasjon av utvalg og enheter Administrasjon av utvalg og enheter skjer fortsatt fra SFU på bakke. Knytningen mellom skjemaet og bakkesystemene skjer ved at et delregister-nr registreres på perioden. Mer informasjon om håndtering av utvalg fra SFU finner du her.\nEksempel på skjema med periode Bildet nedenfor viser eksempel på et skjema med en tilknyttet periode:\n\n\n\n\n\n\nFigur 4: Skjema periode\n\n\n\n\n\n\n\n\n\nTipBeskrivelse av periode metadata\n\n\n\n\n\nTabell 3 beskriver metadatafeltene for en periode\n\n\n\nTabell 3: Periode metadata\n\n\n\n\n\n\n\n\n\nMeta\nForklaring\n\n\n\n\nType\nUndersøkelsestype (f.eks Kvartal, Måned, År).\n\n\nNr\nPeriode nummer.\n\n\nÅr\nPeriode år.\n\n\nPeriode-dato\n\n\n\nDelreg-nr\nDelregister-nummer i SFU som er tilknyttet perioden.\n\n\nEnhet-type\nEnhetstype (f.eks Bedrift, Foretak, Person).\n\n\nOppgavebyrde\nAktivere oppgavebyrde i skjemaet.\n\n\nBrukeropplevelse\nAktivere oppgavebyrde i skjemaet.\n\n\nSkjemadata\n\n\n\nJournalnummer\n\n\n\n\n\n\n\n\n\n\n\nKodeeksempel\nFor å lage en ny periode, kan du bruke metoden create_periode i SuvClient.\n\n\nnotebook\n\nclient = SuvClient()\n\noutput = client.create_periode(\n            skjema_id = 142,\n            periode_type = 'KVRT',\n            periode_nr = 3,\n            periode_aar = 2024,\n            delreg_nr = 21130324,\n            enhet_type = 'BEDR'\n        )\n\nprint(output)\n\nFor å hente ut perioder med tilhørende metadata i Python, kan du bruke metoden get_perioder_by_skjema_id i SuvClient. Sørg for at du oppgir riktig skjema_id.\n\n\nnotebook\n\nclient = SuvClient()\n\noutput = client.get_perioder_by_skjema_id(\n            skjema_id=142\n        )\n\nprint(json.dumps(output, indent=4))",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Administrasjon av skjema"
    ]
  },
  {
    "objectID": "statistikkere/altinn-skjema-administrasjon.html#pulje",
    "href": "statistikkere/altinn-skjema-administrasjon.html#pulje",
    "title": "Administrasjon av skjema",
    "section": "Pulje",
    "text": "Pulje\nEn periode kan ha en eller flere puljer knyttet til seg. Hvilke puljer som er lagt inn på skjemaet kan du se i detaljvisningen når du har valgt en periode.\nEksempel på periode med pulje Bildet nedenfor viser eksempel på en periode med en tilknyttet pulje:\n\n\n\n\n\n\nFigur 5: Periode pulje\n\n\n\n\n\n\n\n\n\nTipBeskrivelse av pulje metadata\n\n\n\n\n\nTabell 4 beskriver metadatafeltene for en pulje\n\n\n\nTabell 4: Pulje metadata\n\n\n\n\n\n\n\n\n\nMeta\nForklaring\n\n\n\n\nPulje\nPulje nummer.\n\n\nTilgjengelig fra\nAngir fra når skjemaet er tilgjengelig for oppdragsgiver ute hos Altinn.\n\n\nSvarfrist\nAngir svarfrist for undersøkelsen.\n\n\nTvmulkt svarfrist\nAngir svarfrist før utsendelse av tvangsmulkt.\n\n\nSend SI\n\n\n\n\n\n\n\n\n\n\n\nKodeeksempel\nFor å lage en ny pulje, kan du bruke metoden create_pulje i SuvClient.\n\n\nnotebook\n\nclient = SuvClient()\n\n_altinn_tilgjengelig = datetime(2024, 11, 20, 14, 30, 0)\n_altinn_svarfrist = datetime(2024, 11, 21)\n_tvangsmulkt_svarfrist_ = datetime(2024, 11, 22)\n_send_si = datetime(2024, 11, 23)\n\nclient.create_pulje(\n    periode_id = 24,\n    pulje_nr = 1,\n    altinn_tilgjengelig=_altinn_tilgjengelig,\n    altinn_svarfrist = _altinn_svarfrist,\n    tvangsmulkt_svarfris t= _tvangsmulkt_svarfrist_,\n    send_si = _send_si\n)\n\nprint(output)\n\nFor å hente ut pulje med tilhørende metadata i Python, kan du bruke metoden get_pulje_by_periode_id i SuvClient. Sørg for at du oppgir riktig periode_id.\n\n\nnotebook\n\nclient = SuvClient()\n\noutput = client.get_pulje_by_periode_id(\n            periode_id = 99\n        )\n\nprint(output)",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Administrasjon av skjema"
    ]
  },
  {
    "objectID": "statistikkere/altinn-skjema-administrasjon.html#utsending",
    "href": "statistikkere/altinn-skjema-administrasjon.html#utsending",
    "title": "Administrasjon av skjema",
    "section": "Utsending",
    "text": "Utsending\nEn pulje kan ha en eller flere utsendinger knyttet til seg. Hvilke utsendinger som er lagt inn på puljen kan du se i detaljvisningen når du har valgt en pulje.\nEksempel på pulje med utsending Bildet nedenfor viser eksempel på en pulje med en tilknyttet utsending:\n\n\n\n\n\n\nFigur 6: Pulje utsending\n\n\n\n\n\n\n\n\n\nTipBeskrivelse av utsending metadata\n\n\n\n\n\nTabell 5 beskriver metadatafeltene for en utsending\n\n\n\nTabell 5: Pulje metadata\n\n\n\n\n\n\n\n\n\nMeta\nForklaring\n\n\n\n\nUtsendingstype\nAngir type utsending (f.eks instansiering).\n\n\nTrigger\nAngir trigger for utsending (Manuell, Auto, Ekstern).\n\n\nTest\nAngir testutsending (sendes ikke til utvalg)\n\n\nSend ut\nAngir tidspunkt for utsendelse.\n\n\n\n\n\n\n\n\n\n\nSend nå\nØnsker du å gjøre en utsending umiddelbart må dette gjøres fra GUI.\nSlik gjør du en utsending umiddelbart:\n\nVelg Skjemadata i venstremenyen.\nBruk søkefeltet i det nye skjermbildet for å finne skjemaet ditt.\nKlikk på Vis detaljer-ikonet i søkeresultatet for å se mer informasjon om skjemaet.\nFinn ønsket periode og velg Vis detaljer-ikonet i søkeresultatet for å se mer informasjon om perioden.\nFinn ønsket utsending og velg Vis detaljer-ikonet i søkeresultatet for å se mer informasjon om utsendingen (eventuelt så kan du opprette en helt ny utsending).\nTrykk “Send nå”.\n\n\n\n\n\n\n\nFigur 7: Send nå\n\n\n\n\n\n\n\n\n\nTipUtsending til utvalgt(e) enhet(er)\n\n\n\nFor å sende til én eller flere spesifikke enheter, kan du oppgi organisasjonsnumrene som en kommaseparert liste. Eksempel: 123456789, 987654321. Dersom ingen enheter oppgis, vil utsendingen automatisk gjelde for hele puljen.\n\n\n\n\nKodeeksempel\nFor å lage en ny utsending, kan du bruke metoden create_utsending i SuvClient.\n\n\nnotebook\n\nclient = SuvClient()\n\n_altinn_uts_tidspunkt = datetime(2024, 11, 20, 14, 30, 0)\n\noutput = client.create_utsending(\n            pulje_id = 75,\n            utsendingstype_navn = 'instansiering',\n            altinn_uts_tidspunkt = _altinn_uts_tidspunkt\n        )\n        \nprint(output)\n\nFor å hente ut utsending med tilhørende metadata i Python, kan du bruke metoden get_utsending_by_id i SuvClient. Sørg for at du oppgir riktig utsending_id.\n\n\nnotebook\n\nclient = SuvClient()\n\noutput = client.client.get_utsending_by_id(\n            utsending_id = 133   \n        )\n\nprint(output)",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Administrasjon av skjema"
    ]
  },
  {
    "objectID": "statistikkere/dapla-lab.html",
    "href": "statistikkere/dapla-lab.html",
    "title": "Dapla Lab",
    "section": "",
    "text": "Dapla Lab er SSBs arbeidsbenk for statistikkproduksjon og forskning. Løsningen er bygget på INSEE sin plattform Onyxia. Formålet med Dapla Lab er å kunne tilby moderne skybaserte dataverktøy til SSB-ere på en effektiv og enhetlig måte. Dapla Lab gir brukeren en enkel oversikt over hvilke verktøy som tilbys, både internt utviklet programvare og velkjente open-source verktøy. Alle tjenestene kan konfigureres etter brukerens ønsker.",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/dapla-lab.html#innlogging",
    "href": "statistikkere/dapla-lab.html#innlogging",
    "title": "Dapla Lab",
    "section": "Innlogging",
    "text": "Innlogging\nAlle som er på SSBs nettverk kan logge seg inn i Dapla ved å gå inn på nettadressen https://lab.dapla.ssb.no/ og velge Logg inn øverst i høyre hjørne. Figur 1 viser landingssiden som møter brukeren.\n\n\n\n\n\n\nFigur 1: Landingsside for Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/dapla-lab.html#funksjonalitet",
    "href": "statistikkere/dapla-lab.html#funksjonalitet",
    "title": "Dapla Lab",
    "section": "Funksjonalitet",
    "text": "Funksjonalitet\n\n\nSelv om Dapla Lab er en løsning for å tilby tjenester så er det også en del nyttig funksjonalitet Dapla Lab. Figur 2 viser menyen i Dapla Lab som gir en oversikt over funksjonaliteten som finnes. Under beskriver vi nærmere hvordan man blant annet kan:\n\ndefinere brukernavn og e-post for Git\nlagre GitHub-token\nlagre tjenestekonfigurasjon\nfå oversikt over hvilke tjenester man har kjørende\npause en tjeneste\n\nMenyen i Figur 2 inkluderer også lenker til andre nettsteder som er nyttig når man jobber med data på Dapla. Meny-innslagene Dapla Manualen, Dapla Ctrl, Google Cloud Console og FAQ er alle lenker til eksterne ressurser.\n\n\n\n\n\n\n\n\n\nFigur 2: Menyen i Dapla Lab\n\n\n\n\n\n\nHjem\nHjem tar deg til landingssiden i Dapla Lab, slik som vist i Figur 1. Her finner du nyttige lenker til læringsressurser for Dapla, felleskap på Viva Engage og opprettelse av Dapla-team.\n\n\nMin konto\nNår man logger seg inn i Dapla Lab så skjer det SSB-kontoen til brukeren. Under Min konto kan man se informasjon om sin konto og konfigurere noen nyttige verdier knyttet til brukeren din.\n\nKontoinformasjon\nUnder denne fanen kan lese ut hvilken bruker-id som er benyttet for innloggingen i Dapla Lab, ditt fulle navn og e-postadresse i Dapla Lab. Informasjonen blir definert ved innlogging og kan ikke endres i Dapla Lab.\n\n\n\n\n\n\nFigur 3: Kontoinformasjon under Min konto i Dapla Lab\n\n\n\n\n\nGit\nUnder fanen Git kan man definere brukernavn og e-post for Git, og et personlig tilgangstoken for GitHub. Dette vil deretter kunne brukes i tjenester som brukeren starter i Dapla Lab.\n\n\n\n\n\n\nFigur 4: Git-konfigurasjon under Min konto i Dapla Lab\n\n\n\n\n\nGrensesnittpreferanser\nUnder fanen Grensesnittpreferanser kan man tilpasse Dapla Lab til sine preferanser ved å velge om man blant annet ønsker Dark mode eller ikke. I tillegg kan man definere hvilket språk man ønsker i Dapla Lab. Det finnes også avanserte valg for avanserte brukere. F.eks. man ønsker å se hvilke Helm-kommandoer som kjøres i bakgrunnen når man starter en tjeneste.\n\n\n\n\n\n\nFigur 5: Grensesnittpreferanser under Min konto i Dapla Lab\n\n\n\n\n\n\nTjenestekatalog\nUnder Tjenestekatalogen ligger alle tjenestene som brukeren kan velge å starte.\n\n\n\n\n\n\nFigur 6: Tjenestekatalogen i Dapla Lab\n\n\n\nFigur 6 viser hvilke tjenester som nå er tilgjengelig i Dapla Lab, inkludert en kort beskrivelse av bruksområdet for hver tjeneste. Figur 7 viser hva som møter når de starter Jupyter-tjenesten.\n\n\nTjenestekonfigurasjon\nAlle tjenester på Dapla Lab kan konfigureres før de startes opp. Trykker man på Start på en av tjenestene i tjenestekatalogen kommer man inn tjenestekonfigurasjon for akkurat den tjenesten. Felles for alle tjenester er at man kan navngi hver tjeneste og velge versjon1, slik som vist under Vennlig navn og Versjon i Figur 7.\n\n\n\n\n\n\nFigur 7: Tjenestekonfigurasjon i Dapla Lab\n\n\n\nEkspanderer man Jupyter konfigurasjoner vist i Figur 7, så får man opp konfigurasjon som er spesifikk for akkurat den tjenesten. Hver tjenestetilbyder vurderer hvilken konfigurasjon som gir mening for den tjenesten de tilbyr.\nFor programmeringsmiljøer som Jupyter og VS Code kan brukeren velge hvilket team og tilgangsgruppe de skal representere, hvor mye ram og gpu de ønsker, hvor stor diskplass de ønsker, Git/GitHub-oppsett, etc..\nI Datadoc-tjenesten har tilbyderen kun valgt å la brukeren velge hvilket team de representerer og versjon av tjenesten. Les mer om tjenestekonfigurasjonen til en tjeneste i dokumentasjonen til tjenesten.\n\nLagre tjenestekonfigurasjon\nVanligvis vil brukeren ønske å starte en tjeneste med samme konfigurasjon som sist. Dapla Lab tilbyr derfor at du kan lagre en tjenestekonfigurasjon med egenvalgt navn. Etter at du har valgt verdiene du ønsker i tjenestekonfigurasjonen så trykker du på Lagre-ikonet vist i Figur 7. Deretter kan du se dine lagrede konfigurasjoner under Mine tjenester, slik som vist i Figur 8.\n\n\n\n\n\n\nFigur 8: Tjenestekonfigurasjon i Dapla Lab\n\n\n\n\n\nDele tjenestekonfigurasjon\nMan kan også dele sin tjenestekonfigurasjon med andre i SSB. Det forutsetter at de man deler med har de samme datatilgangene som den som deler. Man kan dele lagrede tjenestekonfigurasjoner ved å gå til Mine tjenester, trykke på de tre prikkene til høyre i tjenesten i ikonet, og deretter Kopier URL-lenke, slik som vist i Figur 9. Deretter er det bare å sende lenken til en kollega, og de kan åpne en likt konfigurert tjeneste med sine tilganger.\n\n\n\n\n\n\nFigur 9: Dele lagret tjenestekonfigurasjon i Dapla Lab\n\n\n\n\n\n\n\n\n\nWarningNoe konfigurasjon kan ikke deles\n\n\n\nKonfigurasjon som er knyttet brukerkonfigurasjon fra Dapla Lab, f.eks. GitHub-token, må settes manuelt av den man deler konfigurasjon med. Dette vil forhåpentligvis forbedres etter hvert.\n\n\n\n\n\nMine tjenester\nUnder Mine tjenester får man oversikt over hvilke tjenester som er startet av brukeren. Figur 10 viser en bruker som har 3 tjenester kjørende. For hver tjeneste vises informasjon om hvilken tjeneste som er startet, hvor lenge den har kjørt, og muligheten til å pause eller avslutte tjenesten, og hvilken tilgangsgruppen den ble startet med.\n\n\n\n\n\n\nFigur 10: Oversikt over brukerens kjørende tjenester\n\n\n\nHvis man trykker på søppelkasse-ikonet så avsluttes tjenesten og alt som er lagret inne i tjenesten blir slettet. Hvis man trykker på pause-knappen så bevares alt som brukeren har lagret under $HOME/work, mens alt annet blir slettet.\n\n\n\n\n\n\nImportantViktigheten av å avslutte ubrukte tjenester\n\n\n\nEn tjeneste som står som aktiv vil reservere ressursene (CPU, GPU, RAM, etc.) som brukeren valgte ved oppstart. Hvis tjenesten ikke benyttes bør derfor brukeren enten avslutte eller pause tjenesten, slik at SSB ikke må betale for ubrukte ressurser.\n\n\n\n\nMonitorering\n\n\n\n\n\n\nWarningAll monitorering er ikke på plass enda\n\n\n\nInnholdet på Overvåkningssiden til tjenestene er fullstendig enda. Når du kommer inn på siden så skal loggene fra tjenesten viser, men dette er ikke på plass enda. Dette jobbes det med å få på plass.\nDerimot fungerer Ekstern overvåkning-lenken (se beskrivelse under) og den tar deg til et Grafana-dashboard som viser vanlige metrikker for tjenesten. I tillegg kan man trykke på lenken Helm-verdier som teknisk informasjon om hvilke verdier som ble satt når tjenesten ble startet.\n\n\nUnder Mine tjenester får du oversikt over hvilke kjørende tjenester. Hvis du ønsker å monitorere hvor mye ram, cpu diskplass eller gpu tjenester bruker, så kan du inspisere et ferdig oppsatt Grafana-dashboard. For å åpne dashboardet trykker du først på navnet på tjenesten du ønsker å monitorere, slik som vist i Figur 11 (a). Det åpner en side for Overvåkning av tjenesten. På denne siden er det en lenke til et Grafana-dashboard, slik som vist i Figur 11 (b).\n\n\n\n\n\n\n\n\n\n\n\n(a) Åpne Overvåkningssiden\n\n\n\n\n\n\n\n\n\n\n\n(b) Åpne Grafana-dashboard\n\n\n\n\n\n\n\nFigur 11: Åpne Grafana-dashboard for kjørende tjenester\n\n\n\nFigur 12 viser hvordan et Grafana-dashboard ser ut.\n\n\n\n\n\n\nFigur 12: Grafana dashbaordet for en spesifikk tjeneste på Dapla Lab",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/dapla-lab.html#datatilgang",
    "href": "statistikkere/dapla-lab.html#datatilgang",
    "title": "Dapla Lab",
    "section": "Datatilgang",
    "text": "Datatilgang\nBrukere som skal ha tilgang til data fra en tjeneste må først oppgi hvilket team og tilgangsgruppe2 de skal representere. Siden brukere ofte er medlem av flere team, så er dette et viktig sikkerhetstiltak for å sikre at data ikke kobles på tvers av team uten at dette er godkjent av data-ansvarlige.\nFor å få tilgang til lagringsbøttene i prod-prosjektet til et team, så må man logge seg inn i prod-miljøet til Dapla Lab (https://lab.dapla.ssb.no/). Skal man ha tilgang til lagringsbøttene i test-prosjektet til et team må man logge seg inn i test-miljøet til Dapla Lab (https://lab.dapla-test.ssb.no/). For team som har er et dev-miljø så gjelder følgende dev-miljøet til Dapla Lab (https://lab.dapla-test.ssb.no/).\n\nBøtter som filsystem\nTjenestene i Dapla Lab gjør teamets bøtter tilgjengelig som mapper i filsystemet i tjenesten. Det vil si at man kan referere til data som man er vant til på vanlige filsystem, og man kan bruke biblioteker uten å autentisere seg mot bøtter.\nAlle tjenester som tilgjengeliggjør data fra bøtter monterer filsystemet på stien /buckets/. Videre representeres bøttene ved sitt kortnavn. F.eks. vil bøttestien gs://ssb-dapla-felles-data-produkt-prod/ representeres som /buckets/produkt/ i tjenesten.\n\n\nJobbe med data\nSiden Dapla Lab tilbyr å tilgjengliggjøre lagringsbøtter som filsystem inne i tjenesten, så finnes det nå to måter å aksessere data på:\n\nBruke vanlige pakker som Pandas, Polars, Pyarrow, etc. mot filsystemet under /buckets/.\nDen “gamle” måten med dapla-toolbelt som er et overbygg over Pandas og Pyarrow3.\n\nDet er anbefalt at alle benytter seg av alternativ 1 siden det er enklere for de fleste og gjør at alle medlemmer av et team kan se hverandres endringer når man jobber mot samme bøtte (se boks under).\n\n\n\n\n\n\nCautionEksterne endringer i bøtter\n\n\n\nHvis to brukere åpner en tjeneste med den samme team og tilgangsgruppe, så vil man kun se hverandres endringer i filsystemet4 hvis begge jobber direkte mot buckets-filstien. Hvis en av de skriver filer med dapla-toolbelt, mens den andre bruker dapla-toolbelt eller et annet verktøy, så vil man ikke se dette i filsystemet inne i tjenesten. Brukeren kan da kjøre refresh-buckets fra terminalen i tjenesten for å se hva som har dukket opp. Vi anbefaler derfor alle å bruke buckets-tilnærmingen.\nHvis brukeren refererer til en fil som finnes i bøtta, men som ikke synes i filsystemet, så vil det fortsatt kunne leses inn. Dette gjelder også for filer produsert av Kildomaten. Fremover kommer vi til å tilpasse K",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/dapla-lab.html#miljøer",
    "href": "statistikkere/dapla-lab.html#miljøer",
    "title": "Dapla Lab",
    "section": "Miljøer",
    "text": "Miljøer\nDet finnes 2 adskilte miljøer for Dapla Lab: prod og test. Tabell 1 viser hvilke url-er som gjelder for de ulike miljøene.\n\n\n\nTabell 1: Oversikt over miljøer og tilhørende url-er for Dapla Lab.\n\n\n\n\n\nMiljø\nUrl\n\n\n\n\nProd\nhttps://lab.dapla.ssb.no/\n\n\nTest\nhttps://lab.dapla-test.ssb.no/\n\n\n\n\n\n\nMiljøene er knyttet til datatilgang for prosjektene til Dapla-team. Hvert Dapla-team kan ha ressurser i prod- eller test-miljøet. For å få tilgang til ressursene i et av miljøene må de logge seg inn på tilsvarende miljø i Dapla Lab. Det er f.eks. ikke mulig å aksessere prod-data fra test-miljøet i Dapla Lab og omvendt.",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/dapla-lab.html#automatisk-pausing",
    "href": "statistikkere/dapla-lab.html#automatisk-pausing",
    "title": "Dapla Lab",
    "section": "Automatisk pausing",
    "text": "Automatisk pausing\nHver dag pauses alle tjenester automatisk hver hele time mellom kl. 17:00 - 05.00. Det gjøres for redusere ressursbruken og dermed kostnader. Man kan unnta en tjeneste fra å bli automatisk pauset, men da har brukeren selv ansvar for å pause eller slette tjenesten når den ikke er i bruk.\nMan kan når som helst, også etter at tjenesten er startet, unnta en tjeneste fra den automatiske pausingen ved å endre visningsnavnet til tjenesten. Legger man til [nosuspend] i visningsnavnet, slik som vist i Figur 13, så vil tjenesten aldri bli pauset.\n\n\n\n\n\n\nFigur 13: Eksempel på en tjeneste som ikke blir pauset hver dag kl. 17.\n\n\n\nMan endrer visningsnavnet til en tjeneste ved å trykke på 🖊️-ikonet og skrive inn et nytt navn.",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/dapla-lab.html#e-postvarsling",
    "href": "statistikkere/dapla-lab.html#e-postvarsling",
    "title": "Dapla Lab",
    "section": "E-postvarsling",
    "text": "E-postvarsling\nHver mandag morgen blir brukere av Dapla Lab varslet på e-post om hvilke de tjenester de har som ble startet for mer enn 7 dager siden. Det sendes en e-post per bruker per miljø. Hvis man ikke har noen tjenester som tilfredstiller kriteriene, så mottar man ingen e-post.\nFormålet med varslingen er å informere brukeren om at det ikke er anbefalt å la tjenestene leve for lenge siden det betyr at man trolig ikke kjører på siste versjon av tjenesten. Det betyr igjen at brukeren kan gå glipp av viktige oppdateringer eller forbedringer som er blitt gjort.",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/dapla-lab.html#footnotes",
    "href": "statistikkere/dapla-lab.html#footnotes",
    "title": "Dapla Lab",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nBrukere har sjelden behov for å endre versjon her.↩︎\nHvis en bruker er medlem i både data-admins- og developers-gruppa til et team, så må de velge hvilken av de to gruppene de skal representere i tjenesten som startes.↩︎\ndapla-toolbelt er en pakke som ble bygget som et overbygg over Pandas og Pyarrow slik at det ble lettere å lese/skrive mot bøtter. Med bøtter som filsystem inne i tjenestene er ikke dette lenger nødvendig.↩︎\nMed endringer menes her at man oppretter en ny mappe↩︎",
    "crumbs": [
      "Manual",
      "Dapla Lab"
    ]
  },
  {
    "objectID": "statistikkere/altinn-bygge-prefill.html",
    "href": "statistikkere/altinn-bygge-prefill.html",
    "title": "Bygge skjemaprefill",
    "section": "",
    "text": "Denne siden forklarer hvordan du går fram for å bygge din egen skjemaprefill. Med skjemaprefill menes prefill som går ut over det som er standard prefill i skjemaet.",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Prefill"
    ]
  },
  {
    "objectID": "statistikkere/altinn-bygge-prefill.html#skjema-prefill-meta",
    "href": "statistikkere/altinn-bygge-prefill.html#skjema-prefill-meta",
    "title": "Bygge skjemaprefill",
    "section": "Skjema prefill meta",
    "text": "Skjema prefill meta\nFor hver skjemaversjon eksisterer det en skjema prefill meta tabell. Tabellen gir informasjon om hvilke skjemaspesifikke prefill-felter som skjemaet kan inneholde.\n\n\n\n\n\n\nFigur 1: Eksempel på skjema prefill meta tabell\n\n\n\n\n\n\n\n\n\nNoteHva hvis skjemaprefill meta mangler?\n\n\n\nTa kontakt med planleggeren for skjemaet på seksjon 821 dersom skjemaet ditt skal inneholde skjemaprefill og meta-tabellen ikke inneholder data.\n\n\n\nMetadata beskrivelse\nTabell 1 beskriver feltene i skjemaprefill meta-tabellen.\n\n\n\nTabell 1: Prefill Meta\n\n\n\n\n\n\n\n\n\nMeta\nForklaring\n\n\n\n\nKomplett sti\nRefererer til hele stien som brukes for å navigere fra SkjemaData til en spesifikk verdi. Den beskriver nøyaktig plasseringen av dataene i hierarkiet og kan bestå av flere nivåer.\n\n\nType\nBeskriver hvilken datatype feltet kan inneholde.\n\n\nMin\nBegrensing (minimum) på datatypen.\n\n\nMaks\nBegrensing (makimum) på datatypen.\n\n\nObligatorisk\nIndikerer om feltet er påkrevd for prefill.\n\n\nDublett-sjekk\nAngir om feltet skal inneholde unike verdier.\n\n\nStatistikk navn\nKan brukes for mapping til statistikkteamenes interne systemer.\n\n\nBeskrivelse\nEventuell beskrivelse av feltet.\n\n\nKommentar\nEventuelle kommentarer om feltet\n\n\n\n\n\n\n\n\nHente prefill meta med kode\nFor å hente ut skjema prefill meta i Python, kan du bruke metoden get_prefill_meta_by_skjema_def i SuvClient. Sørg for at du oppgir riktig RA-nummer, versjon og undersøkelsesnummer.\n\n\nnotebook\n\nclient = SuvClient()\n\noutput = client.get_prefill_meta_by_skjema_def(\n            ra_nummer = 'RA-0666A3',\n            versjon = 1,\n            undersokelse_nr = '1060'\n        )\n\nprint(json.dumps(output, indent=4))",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Prefill"
    ]
  },
  {
    "objectID": "statistikkere/altinn-bygge-prefill.html#struktur-på-skjemaprefill",
    "href": "statistikkere/altinn-bygge-prefill.html#struktur-på-skjemaprefill",
    "title": "Bygge skjemaprefill",
    "section": "Struktur på skjemaprefill",
    "text": "Struktur på skjemaprefill\nSkjemaprefill kan representeres som en hierarkisk struktur basert på sti_navn. Dette gir en oversikt over hvordan dataene er organisert. Følgende kode viser hvordan du kan vise strukturen med valgfri inkludering av metadata.\n\n\nnotebook\n\nclient = SuvClient()\n\nresultat = client.get_prefill_meta_by_skjema_def(\n                ra_nummer = 'RA-0666A3',\n                versjon = 1,\n                undersokelse_nr = '1060'            \n)\n\ndef build_structure(data, include_metadata=False):\n    structure = {}\n\n    for item in data:\n        path = item['sti_navn'].split('.') if item['sti_navn'] else [item['navn']]\n        current_level = structure\n\n        for part in path:\n            if part not in current_level:\n                current_level[part] = {\"metadata\": {}} if include_metadata else {}\n            current_level = current_level[part]\n\n        if include_metadata:\n            current_level[\"metadata\"][\"type\"] = item.get(\"type\")\n            current_level[\"metadata\"][\"kontroll\"] = item.get(\"kontroll\")\n            current_level[\"metadata\"][\"stat_navn\"] = item.get(\"stat_navn\")\n            current_level[\"metadata\"][\"kommentar\"] = item.get(\"kommentar\")\n\n    return structure          \n\nresult_without_metadata = build_structure(resultat, include_metadata=False)\nprint(json.dumps(result_without_metadata, indent=4))\n\nEksempel på output for hierarkisk struktur uten metadata:\n{\n    \"innkjoptElektriskKraft\": {},\n    \"innkjoptElektriskKost\": {},\n    \"GassOgPetroleumKost\": {\n        \"produktTypeBruktId\": {},\n        \"produktTypeBrukt\": {},\n        \"produktEnhet\": {}\n    },\n    \"EgneprodEnergiProd\": {\n        \"egenprodEnergiProdTypeID\": {},\n        \"egenprodEnergiProdType\": {},\n        \"egenprodEnergiProdEnhet\": {}\n    }\n}",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Prefill"
    ]
  },
  {
    "objectID": "statistikkere/altinn-bygge-prefill.html#lagre-prefill-data",
    "href": "statistikkere/altinn-bygge-prefill.html#lagre-prefill-data",
    "title": "Bygge skjemaprefill",
    "section": "Lagre prefill data",
    "text": "Lagre prefill data\nFor å lagre prefill for en enhet bruker du metoden save_prefill_for_enhet i SuvClient. Metoden lagrer skjemaprefill for en spesifikk enhet i utvalget. Beskrivelse av hvordan du henter utvalg finner du på siden Utvalg fra SFU\n\n\nnotebook\n\nclient = SuvClient()\n\noutput = client.save_prefill_for_enhet(\n     ra_nummer = 'RA-0666A3',\n     versjon = 1,\n     periode_aar = 2024,\n     periode_type = 'KVRT',\n     periode_nr = 2,\n     enhetsident = 'A3TF0018',\n     enhetstype = 'FRTK',\n     prefill = {\n        \"innkjoptElektriskKost\": 10, \n        \"innkjoptElektriskKraft\": 20, \n        \"GassOgPetroleumKost\": [\n            {\"produktTypeBruktId\": \"1\"},\n            {\"produktTypeBruktId\": \"2\"}\n        ]\n    }                  \n )\nprint(output)    \n\nTabell 2 viser en beskrivelse av de ulike parameterne i save_prefill_for_enhet\n\n\n\nTabell 2: Beskrivelse av parametere\n\n\n\n\n\n\n\n\n\nParameter\nForklaring\n\n\n\n\nRA-nummer\nRA-nummer for skjemaet.\n\n\nversjon\nSkjemaversjon.\n\n\nperiode_aar\nÅrstall for perioden.\n\n\nperiode_type\nPeriode type. Gyldige verdier er AAR, MND, KVRT, UKE\n\n\nperiode_nr\nPeriode nummer\n\n\nenhetsident\nIdentifikator for enheten\n\n\nenhetstype\nType enhet. Gyldige verdier er FRTK, BEDR, PERS\n\n\nprefill\nPrefill-data i gyldig JSON-format.",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Prefill"
    ]
  },
  {
    "objectID": "statistikkere/altinn-bygge-prefill.html#validering-av-prefill-data",
    "href": "statistikkere/altinn-bygge-prefill.html#validering-av-prefill-data",
    "title": "Bygge skjemaprefill",
    "section": "Validering av prefill data",
    "text": "Validering av prefill data\nValidering av skjemaprefill sikrer at dataene oppfyller kravene til struktur og innhold. Dette gjøres automatisk i save_prefill_for_enhet. Feil i valideringen vil føre til at dataene ikke lagres, og det gis en feilmelding. Dersom du ønsker å sjekke om prefill er gyldig før lagring er dette mulig ved hjelp av metoden validate_skjemadata.\n\n\nnotebook\n\nclient = SuvClient()\n\nclient.validate_skjemadata(      \n        ra_nummer = 'RA-0678A3',\n        versjon = 2,\n        skjemadata = {\"antallAnsattePrefill\": \"10\"}\n)   \n\ndapla-suv-tools pakken inneholder også metoder for å hente ut lagret prefill og slette prefill. Det er mulig å slette prefill enten på enhetsnivå eller skjemanivå.\nValideringen er basert på JSON Schema.\n\n\n\n\n\n\nWarning\n\n\n\nDersom skjemaet blir instansiert med ugyldige prefill data vil skjemaet feile ved instansiering hos Altinn. Skjemaet vil i dette tilfellet bli slettet fra innboksen til oppdragsgiver.",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Prefill"
    ]
  },
  {
    "objectID": "statistikkere/altinn-bygge-prefill.html#eksempelkode",
    "href": "statistikkere/altinn-bygge-prefill.html#eksempelkode",
    "title": "Bygge skjemaprefill",
    "section": "Eksempelkode",
    "text": "Eksempelkode\nNoe demokode ligger i repoet, og kan være ett godt utgangspunkt å kopiere og endre fra.\n\nRA-0678 (Ledige stillinger)\nDette eksemplet viser hvordan du kan bygge skjemaprefill for RA-0678 (Ledige stillinger). Nedenfor ser du hvordan innholdet i SkjemaData blokka er strukturert.\n{\n    \"SkjemaData\": {\n        \"antallAnsattePrefill\": {\n            \"type\": \"string\"\n        },\n        \"datoPrefill\": {\n            \"type\": \"string\"\n        }\n    }\n}\nSkjemaet inneholder to felter som skal forhåndsutfylles. Det er antallAnsattePrefill og datoPrefill.\nTabell 3 beskriver feltene som kan forhåndsutfylles.\n\n\n\nTabell 3: Prefill felter\n\n\n\n\n\n\n\n\n\nFelt\nSti\n\n\n\n\nantallAnsattePrefill\n-\n\n\ndatoPrefill\n-\n\n\n\n\n\n\nEksempelet leser prefill-data fra en tekstfil og bygger den nødvendige strukturen for å forhåndsutfylle skjemaet for utvalgte enheter.\n\n\nRA-0692 (Utenrikshandel med tjenester)\nDette eksemplet viser hvordan du kan bygge skjemaprefill for RA-0692 (Utenrikshandel med tjenester). Nedenfor ser du hvordan innholdet i SkjemaData blokka er strukturert.\n{\n   \"SkjemaData\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"leveransetype\": {\n                \"type\": \"string\",\n                \"enum\": [\n                    \"0\",\n                    \"1\",\n                    \"2\"\n                  ]\n            },\n            \"Eksport\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"$ref\": \"#/$defs/Eksport\"                \n              }\n            },\n            \"Import\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"$ref\": \"#/$defs/Import\"\n            }\n        }\n    },   \n    \"Eksport\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"cpaLevel1Eksport\": {\n                \"type\": \"string\"        \n            },\n            \"cpaEksport\": {\n                \"type\": \"string\"\n            },\n            \"PostEksport\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"$ref\": \"#/$defs/PostEksport\"\n            }\n        }\n    },\n    \"Import\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"cpaLevel1Import\": {\n                \"type\": \"string\",          \n            },\n            \"cpaImport\": {\n                \"type\": \"string\"\n            },\n            \"PostImport\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"$ref\": \"#/$defs/PostImport\"\n            }\n        } \n    },\n    \"PostEksport\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"landkodeEksport\": {\n                \"type\": \"string\"\n            },\n            \"forrigeKvartalKrEksport\": {\n                \"type\": \"integer\"\n            },\n            \"forrigeKonsernIntKrEksport\": {\n                \"type\": \"integer\"\n            },\n        }\n    },\n    \"PostImport\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"landkodeImport\": {\n                \"type\": \"string\",\n            },\n            \"forrigeKvartalKrImport\": {\n                \"type\": \"integer\"\n            },\n            \"forrigeKonsernIntKrImport\": {\n                \"type\": \"integer\"\n            },\n        }\n    }\n}\nMerk at både Eksport/Import og PostEksport/PostImport er repeterende. Den ytterste gruppa er CPA-verdien, mens den innerste gruppa inneholder landskodene og kronebeløpene.\nTabell 4 beskriver feltene som kan forhåndsutfylles.\n\n\n\nTabell 4: Prefill felter\n\n\n\n\n\n\n\n\n\nFelt\nSti\n\n\n\n\nleveransetype\n-\n\n\ncpaLevel1Eksport\nEksport\n\n\ncpaEksport\nEksport\n\n\nlandkodeEksport\nEksport.PostEksport\n\n\nforrigeKvartalKrEksport\nEksport.PostEksport\n\n\nforrigeKonsernIntKrEksport\nEksport.PostEksport\n\n\ncpaLevel1Import\nImport\n\n\ncpaImport\nImport\n\n\nlandkodeImport\nImport.PostImport\n\n\nforrigeKvartalKrImport\nImport.PostImport\n\n\nforrigeKonsernIntKrImport\nImport.PostImport\n\n\n\n\n\n\nEksempelet leser prefill-data fra flere tekstfiler og bygger den nødvendige strukturen for å forhåndsutfylle skjemaet for utvalgte enheter.",
    "crumbs": [
      "Manual",
      "Altinn 3",
      "Prefill"
    ]
  },
  {
    "objectID": "statistikkere/administrasjon-av-team.html",
    "href": "statistikkere/administrasjon-av-team.html",
    "title": "Administrasjon av team",
    "section": "",
    "text": "I dette kapitlet viser vi hvordan du kan opprette et nytt team eller gjøre endringer i et eksisterende team. Typiske endringer er å:",
    "crumbs": [
      "Manual",
      "Dapla-team",
      "Administrasjon av team"
    ]
  },
  {
    "objectID": "statistikkere/administrasjon-av-team.html#opprette-dapla-team",
    "href": "statistikkere/administrasjon-av-team.html#opprette-dapla-team",
    "title": "Administrasjon av team",
    "section": "Opprette Dapla-team",
    "text": "Opprette Dapla-team\nFor å opprette et Dapla-team så må en seksjonsleder gå inn i Teamoversikten i Dapla Ctrl og trykke på ikonet Opprett team. Her blir man bedt om å fylle inn relevant informasjon.\n\n\n\n\n\n\nNote\n\n\n\nLes mer om Dapla Ctrl her.",
    "crumbs": [
      "Manual",
      "Dapla-team",
      "Administrasjon av team"
    ]
  },
  {
    "objectID": "statistikkere/administrasjon-av-team.html#legge-til-eller-fjerne-medlemmer-i-et-team",
    "href": "statistikkere/administrasjon-av-team.html#legge-til-eller-fjerne-medlemmer-i-et-team",
    "title": "Administrasjon av team",
    "section": "Legge til eller fjerne medlemmer i et team",
    "text": "Legge til eller fjerne medlemmer i et team\nFor å legge til, fjerne eller endre medlemmer i et team må kan gjøres av medlemmer i managers-gruppen i teamet. Dette gjøres i Dapla Ctrl. Les mer om hvordan dette gjøres her.\n\n\n\n\n\n\nWarningManagers i semi- eller self-managed team\n\n\n\nManagers i semi- eller self-managed teams kan ikke legge til, fjerne eller endre medlemmer fra Dapla Ctrl enda. Disse må foreløpig kontakte Kundeservice for å gjøre endringer.",
    "crumbs": [
      "Manual",
      "Dapla-team",
      "Administrasjon av team"
    ]
  },
  {
    "objectID": "statistikkere/administrasjon-av-team.html#se-medlemmer-og-roller-i-et-team",
    "href": "statistikkere/administrasjon-av-team.html#se-medlemmer-og-roller-i-et-team",
    "title": "Administrasjon av team",
    "section": "Se medlemmer og roller i et team",
    "text": "Se medlemmer og roller i et team\nDapla Ctrl lar alle i SSB se hvilke team som finnes, hvem som er medlemmer og hvilke tilgangsgrupper de ligger i. Man kan også få oversikt over hvilke data alle team deler. Les mer her.",
    "crumbs": [
      "Manual",
      "Dapla-team",
      "Administrasjon av team"
    ]
  },
  {
    "objectID": "statistikkere/altinn-oversikt.html",
    "href": "statistikkere/altinn-oversikt.html",
    "title": "Oversikt",
    "section": "",
    "text": "Frem mot sommeren 2026 skal alle skjema-undersøkelser i SSB som gjennomføres på Altinn 2 flyttes over til Altinn 3. Skjemaer som flyttes til Altinn 3 vil motta sine data på Dapla, og ikke på bakken som tidligere. Datafangsten håndteres av Team SU-V, mens statistikkseksjonene henter sine data fra Team SU-V sitt lagringsområde på Dapla. I dette kapitlet beskriver vi nærmere hvordan statistikkseksjonene kan jobbe med Altinn3-data på Dapla. Kort oppsummert består det av disse stegene:\nI resten av kapitlet gis en oversikt over hvordan statistikkteam kan jobbe med data fra Altinn 3.",
    "crumbs": [
      "Manual",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn-oversikt.html#ansvarsfordeling",
    "href": "statistikkere/altinn-oversikt.html#ansvarsfordeling",
    "title": "Oversikt",
    "section": "Ansvarsfordeling",
    "text": "Ansvarsfordeling\nTeam SU-V har ansvaret for datafangst fra Altinn3 til SSB. Deretter tilgjengeliggjør de dette for statistikkteamet som skal jobbe videre med dataene for å produsere statistikk.\nDet er statistikteamet som lagrer dataene som sin kildedata som er ansvarlig for dataene og at disse håndteres på riktig måte. Av den grunn er det statistikteamet som setter opp jobben for å synkronisere data fra bøtta til Team SU-V til sin kildebøtte, slik at de kan organisere dataene som de ønsker. I tillegg vil backup av data bli håndtert i kildebøtta til statistikkteamet.",
    "crumbs": [
      "Manual",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn-oversikt.html#tilgangsstyring",
    "href": "statistikkere/altinn-oversikt.html#tilgangsstyring",
    "title": "Oversikt",
    "section": "Tilgangsstyring",
    "text": "Tilgangsstyring\nSkjemaer fra Altinn 3 hentes inn til SSB av team SU-V og lagres i delt-bøtter som SU-V administrerer. SU-V kan deretter gi tilgang til statistikkteam slik at de hente data fra bøttene og lagre det i sine kildebøtter. Siden tilgangsstyring mellom prod- og test-miljøet fungerer ulikt så forklarer vi hver av de under.\n\nProd\nTilgang til bøtter med skjemadata i prod-miljøet gis kun til Transfer Service for kildeprosjektet til statistikkteamet, og aldri direkte til brukere. Dvs. at team SU-V gir tilgang til Transfer Service til statistikkteamet, og deretter kan statistikkteamet sette opp en automatisk jobb med Transfer Service som synkroniserer data fra SU-V sin delt-bøtte og over til statistikkteamets kildebøtte.\nSiden tilgangsstyringen til data i prod er ganske restriktiv, så anbefaler vi at statistikere gjør seg kjent med sine skjemadata i test-miljøet (se under) før de starter arbeidet i prod.\n\n\nTest\nTilgang til bøtter med skjemadata i test-miljøet er mindre restriktive enn for prod-miljøet, siden disse dataene er fiktive. Derfor anbefales det at statistikere først jobber i test-miljøet når de skal gjøre seg kjent med dataene fra Altinn 3. I test-miljøet kan både brukere og Transfer Service få tilgang til SU-V sine delt-data.",
    "crumbs": [
      "Manual",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn-oversikt.html#data",
    "href": "statistikkere/altinn-oversikt.html#data",
    "title": "Oversikt",
    "section": "Data",
    "text": "Data\nDataene som hentes inn av Team SU-V lagres som xml-filer i en bøtte. Statistikkteamet som skal hente inn dataene synkroniseres deretter dataene til sin delt-bøtte med Transfer Service. Det finnes både data og metadata om hvert skjema som sendes inn og under forklares innholdet i hver av de.\n\nSkjemadata\nHvert skjema som leveres inn av en oppgavegiver blir lagret som en separat xml-fil med et unikt filnavn2. Under ser du et eksempel på hvordan et skjema kan se ut.\n\n\n\n\n\n\nCautionEksempel på xml-fil\n\n\n\n\n\n&lt;?xml version=\"1.0\" ?&gt;\n&lt;melding xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" dataFormatProvider=\"SERES\" dataFormatId=\"7079\" dataFormatVersion=\"47315\"&gt;\n  &lt;InternInfo&gt;\n    &lt;raNummer&gt;RA-XXXX&lt;/raNummer&gt;\n    &lt;skjemaVersjon&gt;1.0&lt;/skjemaVersjon&gt;\n    &lt;undersoekelsesNr&gt;XXXXX&lt;/undersoekelsesNr&gt;\n    &lt;visOppgaveByrde&gt;0&lt;/visOppgaveByrde&gt;\n    &lt;visBrukeropplevelse&gt;0&lt;/visBrukeropplevelse&gt;\n    &lt;delregNr&gt;2XXXXX&lt;/delregNr&gt;\n    &lt;periodeFritekst&gt;3. kvartal 2022&lt;/periodeFritekst&gt;\n    &lt;periodeFomDato&gt;2022-07-01&lt;/periodeFomDato&gt;\n    &lt;periodeTomDato&gt;2022-09-30&lt;/periodeTomDato&gt;\n    &lt;periodeType&gt;KVRT&lt;/periodeType&gt;\n    &lt;periodeNummer&gt;3&lt;/periodeNummer&gt;\n    &lt;periodeAAr&gt;2022&lt;/periodeAAr&gt;\n    &lt;periodeDato&gt; &lt;/periodeDato&gt;\n    &lt;enhetsIdent&gt;XXXXXX&lt;/enhetsIdent&gt;\n    &lt;enhetsType&gt;BEDR&lt;/enhetsType&gt;\n    &lt;enhetsOrgNr&gt;XXXXXXXXX&lt;/enhetsOrgNr&gt;\n    &lt;enhetsNavn&gt;STATISTISK SENTRALBYRÅ &lt;/enhetsNavn&gt;\n    &lt;enhetsGateadresse&gt;Akersveien 26&lt;/enhetsGateadresse&gt;\n    &lt;enhetsPostnr&gt;0177&lt;/enhetsPostnr&gt;\n    &lt;enhetsPoststed&gt;OSLO&lt;/enhetsPoststed&gt;\n    &lt;enhetsAvdeling&gt;AVD XXXX&lt;/enhetsAvdeling&gt;\n    &lt;reporteeOrgNr&gt;XXXXXXXXX&lt;/reporteeOrgNr&gt;\n  &lt;/InternInfo&gt;\n  &lt;Kontakt&gt;\n    &lt;kontaktPersonNavn&gt;OLA NORDMANN&lt;/kontaktPersonNavn&gt;\n    &lt;kontaktPersonEpost&gt;ola@tull.no&lt;/kontaktPersonEpost&gt;\n    &lt;kontaktPersonTelefon&gt; &lt;/kontaktPersonTelefon&gt;\n    &lt;kontaktInfoBekreftet&gt;1&lt;/kontaktInfoBekreftet&gt;\n    &lt;kontaktInfoKommentar&gt;Dette er en et eksempel for Dapla-manualen&lt;/kontaktInfoKommentar&gt;\n  &lt;/Kontakt&gt;\n  &lt;Skjemadata&gt;\n    &lt;ledigeStillinger&gt;75&lt;/ledigeStillinger&gt;\n    &lt;datoPrefill&gt; &lt;/datoPrefill&gt;\n  &lt;/Skjemadata&gt;\n&lt;/melding&gt;\n\n\n\nFilstien til filene i team SUV-V sine delt-bøtter følger en streng navnestandard. Figur 1 viser et eksempel på hvordan filstien til et fiktivt skjema kan se ut i bøtta til team SU-V. Filstien har egenskapen at den er globalt unik og inneholder informasjon om tidspunktet skjemaet ble innkvittert i SSB sine systemer.\n\n\n\n\n\n\nFigur 1: Typisk filsti for et Altinn3-skjema.\n\n\n\nSkjemanavnet du ser i Figur 1 er unike. Dvs. at hvis en oppgavegiver sender inn flere skjemaer på samme dag, så er fortsatt skjemanavnet unikt. Det er viktig å være klar over når man bruker automatiseringstjenesten for kildedata senere. Siden tjenesten trigges per fil, og man er avhengig av unike navn for å ikke skrive over filer, så er det nyttig å vite at man kan videreføre skjemanavnet i overgangen fra kildedata til inndata.\n\n\nMetadata\nFor hver ny innsending fra Altinn3 i test- og prod-miljøet, så tilrettelegger team SU-V en json-fil med metadata om innsedningen. Filen inneholder foreløpig to variabler:\n\nReferansenummeret som oppgavegiver får ved innsending.\nTidspunkt for når skjema er levert i Altinn3.\n\nBrukerbehovet er i hovedsak dublettkontroll og svartjeneste. For dette trenger man et eksakt tidspunkt for når skjema faktisk ble sendt inn (trykket på knappen i Altinn). Merk at tidspunkt i fila er UTC.\nFilen ligger i bøttene sammen med xml/pdf (og eventuelle vedlegg). Team T-Rex vil se videre på å integrere dette inn i sin Python-pakke.\n{\n    \"altinnReferanse\": \"f23415ca6b2f\", \n    \"altinnTidspunktLevert\": \"2024-04-29T07:16:10.5080448Z\"\n}\n\n\n\n\n\n\nFigur 2: Metadata fra Altinn",
    "crumbs": [
      "Manual",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn-oversikt.html#synkronisering-av-data",
    "href": "statistikkere/altinn-oversikt.html#synkronisering-av-data",
    "title": "Oversikt",
    "section": "Synkronisering av data",
    "text": "Synkronisering av data\nNår et statistikkteam har fått beskjed av team SU-V at data er tilgjengelig deres delt-bøtte og at statistikkteamets Transfer Service har fått tilgang, så kan man sette opp synkronisering av data fra team SU-V til statistikkteamets kildebøtte.\nNår man synkroniserer data fra team SU-V til egen kildebøtte er det anbefalt å gjøre følgende:\nLag en mappe per skjema\nFør man setter opp synkroniseringen bør man opprette en mappe per datakilde som teamet har. Dette inkluderer både Altinn-kilder og andre kilder. Hvis teamet har flere skjemaer fra Altinn så kan man f.eks. bruke skjemanummer som øverste mappenivå i kildebøtta, og synkronisere hvert skjema til sin egen undermappe.\n\n\n\n\n\n\nCautionEksempel på mappestruktur\n\n\n\n\n\nssb-dapla-felles-data-kilde-prod\n├── altinn\n│   ├── ra0678\n│   └── ra0778\n└── andrekilder\n\n\n\nBehold mappestrukturen til team SU-V\nNår vi bruker Transfer Service til å synkronisere innholdet i Team SU-V sitt lagringsområde til Dapla-teamet sin kildebøtte, så er det anbefalt å fortsette å bruke mappe-strukturen som Team SU-V har for å sikre at ingen filer blir overskrevet på grunn av at de har identiske navn.\n\n\n\n\n\n\nCautionEksempel på mappestruktur for ra0678\n\n\n\n\n\nssb-dapla-felles-data-kilde-prod\n├── altinn\n│   ├── ra0678\n│   │   └── 2026\n│   │       └── 3\n│   │           └── 28\n│   │               └── b66abe1880cc_a35bceb7-950d-4e9b-a4a0-caea736ab270\n│   │                   └── form_b66abe1880cc.xml\n│   └── ra0778\n└── andrekilder\n\n\n\n\nTransfer Service\nNår vi skal overføre filer fra Team SU-V sin bøtte til vår kildebøtte, så kan vi gjøre det manuelt fra Jupyter som forklart her.. Men det er en bedre løsning å bruke en tjeneste som gjør dette for deg. Transfer Service er en tjeneste som kan brukes til å synkronisere innholdet mellom bøtter på Dapla, samt mellom bakke og sky. Når du skal ta i bruk tjenesten for å overføre data mellom en bøtte fra Team SU-V sitt prosjekt suv-altinn-data-p, til en kildedata-bøtte i Dapla-teamet ditt, så gjør du følgende:\n\nFølg denne beskrivelsen hvordan man setter opp overføringsjobber.\nEtter at du har trykket på Create Transfer Job velger du Google Cloud Storage på begge alternativene under Get Started. Deretter går du videre ved å klikke på Next Step.\nUnder Choose a source så skal du velge hvor du skal kopiere data fra. Trykk på Browse. I vinduet som dukker opp trykker du på 🔻-ikonet ved siden av Project ID. I søkevinduet som dukker opp søker du opp suv-altinn-data-p og trykker på navnet. Da får du listet opp alle bøttene i suv-altinn-data-p prosjektet. Til slutt trykker du på bøtta som Team SU-V har opprettet for undersøkelsen3 og klikker Select til nederst på siden. Trykk deretter Next step for å gå videre.\nUnder Choose a destination så skal du velge hvor du skal kopiere data til. Dette steget er ganske likt som forrige, men du skal nå velge ditt eget projekt og kildebøtta der. Trykk på Browse. I vinduet som dukker opp trykker du på 🔻-ikonet ved siden av Project ID. I søkevinduet som dukker opp søker du opp &lt;ditt teamnavn&gt;-kilde-&lt;miljø&gt; og trykker på navnet. Da får du listet opp alle bøttene i ditt team sitt prosjekt. Velg kildebøtta som har navnet ssb-&lt;teamnavn&gt;-kilde-&lt;miljø&gt;. Hvis du ønsker å kopiere data til en undermappe i bøtta, så trykker du på &gt;-ikonet ved bøttenavnet og velger ønsket undermappe4. Til slutt trykker du på Select til nederst på siden. Trykk deretter Next step for å gå videre.\nUnder Choose how and when to run job velger du Run with custom frequency og Starting now i Batch-modus. Hvis du ønsker å overføre så ofte som mulig, så velger du Custom frequency og Repeat every 1 Hours. Til slutt trykker du Next Step nederst på siden.\nUnder Choose Settings så legger du til en kort beskrivelse av jobben du har opprettet. Under de andre valgene kan du gjøre følgende:\n\nUnder Advanced transfer Options trenger du ikke gjøre noen endringer.\nUnder When to overwrite kan du velge If different.\nUnder When to delete kan du velge Never.\nUnder Manifest trenger du ikke huke av.\nUnder Logging options velger du samme som vist i Figur 3.\n\n\n\n\n\n\n\n\nFigur 3: Valg av opsjoner for logging i Transfer Service\n\n\n\nTil slutt trykker du Create for å aktivere tjenesten. Den vil da sjekke Team SU-V sin bøtte hver time og kopiere over alle filer som ikke allerede eksisterer statistikkteamets kildebøtte.",
    "crumbs": [
      "Manual",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn-oversikt.html#kildomaten",
    "href": "statistikkere/altinn-oversikt.html#kildomaten",
    "title": "Oversikt",
    "section": "Kildomaten",
    "text": "Kildomaten\nNår du har satt opp Transfer Service til å kopiere over filer fra Team SU-V sin bøtte til statistikkteamets kildebøtte, så vil det potensielt komme inn nye skjemaer hver time. Siden ingen på statistikkteamet har tilgang til kildebøtta som standard5, så er neste steg å prosessere dataene med Kildomaten og lagre dataene i produktbøtta hvot alle i teamet har tilgang.\nLes mer om hvordan du kan bruker tjenesten her.",
    "crumbs": [
      "Manual",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn-oversikt.html#altinn3-til-isee",
    "href": "statistikkere/altinn-oversikt.html#altinn3-til-isee",
    "title": "Oversikt",
    "section": "Altinn3 til ISEE",
    "text": "Altinn3 til ISEE\nEnkelte Dapla-team må flytte sine skjemaer over til Altinn 3, men ønsker å bruke ISEE videre i produksjonsprosessen. Det er fullt mulig og dokumentert her.\nDet er også utviklet en Altinn3-pakke i Python som flater ut XML-filer fra Altinn 3 og lager en csv-fil som er på ISEE-format av innholdet. ssb-altinn-python er tilgjengelig i Kildomaten, og kan benyttes for å automatisk flate ut Altinn3-skjema mellom kilde- og produktbøtte.\nDet er også skrevet et blogginnlegg som beskriver i mer detaljer hvordan man inkludere Altinn 3, Dapla og ISEE i et produksjonsløp.",
    "crumbs": [
      "Manual",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn-oversikt.html#ende-til-ende-testing-i-altinn",
    "href": "statistikkere/altinn-oversikt.html#ende-til-ende-testing-i-altinn",
    "title": "Oversikt",
    "section": "Ende-til-ende-testing i Altinn",
    "text": "Ende-til-ende-testing i Altinn\nDet ble satt sammen en tverrfaglig arbeidsgruppe for å se på muligheten for å samle dokumentasjon knyttet til testing av skjematjenester i Altinn. Mandatet for gruppen har vært å få oversikt over prosessen «Ende-til-ende-test». Dokumentere og vise til relevant tilgjengelig informasjon.",
    "crumbs": [
      "Manual",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn-oversikt.html#footnotes",
    "href": "statistikkere/altinn-oversikt.html#footnotes",
    "title": "Oversikt",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nEn typisk prosessering som de fleste vil ønske å gjøre er å konvertere fra xml-formatet det kom på, og over til parquet-formatet.↩︎\nTeknisk sett er hele filstien det samme som et filnavn i en bøtte. Men det omtales heretter som filnavn og filsti for å kunne skille mellom hele stien, og det som tradisjonelt er oppfattet som filnavn.↩︎\nBøttenavnet starter alltid med RA-nummeret til undersøkelsen.↩︎\nAlternativt oppretter du en mappe direkte vinduet ved å trykke på mappe-ikonet med en +-tegn i seg.↩︎\nKun data-admins i teamet kan aktivere tilgang ved behov↩︎",
    "crumbs": [
      "Manual",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/github-oppsett.html",
    "href": "statistikkere/github-oppsett.html",
    "title": "GitHub: Bruker og oppsett",
    "section": "",
    "text": "I denne artikkelen dekker vi hvordan man lager og konfigurerer bruker på GitHub. Konfigurering innebærer å koble seg opp til SSB sin organisasjon på GitHub og å lage og lagre Personal Access Token (PAT).\n\nOpprett GitHub-bruker\nFor å lage repoer og bidra til eksisterende repoer må man ha en GitHub-bruker.\n\n\n\n\n\n\nSSB har valgt å ikke sette opp SSB-brukerne til de ansatte som GitHub-brukere. En viktig årsak er at en GitHub-konto ofte regnes som en del av den ansattes CV. For de som aldri har brukt GitHub før kan det virke fremmed, men det er nok en fordel på sikt når alle blir godt kjent med denne arbeidsformen.\n\n\n\nSlik gjør du det:\n\nGå til https://GitHub.com/\nTrykk Sign up øverst i høyre hjørne\nI dialogboksen som åpnes, se Figur 1, skriver du inn din e-postkonto og lager et passord. Dette trenger ikke være din SSB-bruker og e-post. Hvis du bruker en en personlig e-postkonto er det viktig at du tydeliggjør hvem du er så kollegaer vet at du jobber i SSB når de ser aktivitet fra deg.\n\n\n\n\n\n\n\nFigur 1: Dialogboks for opprettelse av GitHub-bruker.\n\n\n\nDu har nå laget en egen GitHub-bruker. I neste steg skal vi knytte denne kontoen til din SSB-bruker.\n\n\nTo-faktor autentisering\nHvis du har fullført forrige steg har du nå en GitHub-konto. Hvis du står på din profil-side ser den ut som i Figur 2.\n\n\n\n\n\n\nFigur 2: Et eksempel på hjemmeområdet til en GitHub-bruker\n\n\n\nDet neste vi må gjøre er å aktivere 2-faktor autentisering, siden det er dette som benyttes i SSB. Hvis du står på siden i bildet over, så gjør du følgende for å aktivere 2-faktor autentisering mot GitHub:\n\n\n\nTrykk på den lille pilen øverst til høyre og velg Settings(se Figur 3).\nDeretter velger du Password and authentification i menyen til venstre.\nUnder Two-factor authentication trykker du på Enable.\n\n\n\n\n\n\n\n\n\nFigur 3: Åpne settings for din GitHub-bruker.\n\n\n\n\n\n\nFigur 4 viser dialogboksen som vises. Velg Enable two-factor authentification.\n\n\n\n\n\n\n\nFigur 4: Dialogboks som åpnes når 2FA skrus på første gang.\n\n\n\n\nFigur 5 viser dialogboksen som vises for å velge hvordan man skal autentisere seg. Her anbefales det å velge Set up using an app, slik at du kan bruke Microsoft Authenticator-appen på din mobil.\n\n\n\n\n\n\n\nFigur 5: Dialogboks for å velge hvordan man skal autentisere seg med 2FA.\n\n\n\nFigur 6 viser QR-koden som vises. Denne skal vi bruke i neste steg.\n\n\n\n\n\n\nFigur 6: QR-kode som skannes av Microsoft Authenticator.\n\n\n\n\n\n\nStrekkoden over skal skannes i din Microsoft Authenticator-app på mobilen, som vist i Figur 7. Åpne appen, trykk på Bekreftede ID-er, og til slutt trykk på Skann QR-kode. Deretter skanner du QR-koden fra punkt 5.\nNår koden er skannet har du fått opp følgende bilde på appens hovedside (se bilde til høyre). Skriv inn den 6-siffer koden på GitHub-siden med QR-koden.\nTil slutt lagrer du Recovery-codes et trygt sted som bare du har tilgang til.\n\n\n\n\n\n\n\n\n\nFigur 7: Mobilappen Microsoft authenticator\n\n\n\n\n\nNå har vi aktivert 2-faktor autentisering for GitHub og er klare til å knytte vår personlige konto til vår SSB-bruker på SSBs “GitHub organisation” statisticsnorway.\n\n\nKoble deg til SSB\nI forrige steg aktiverte vi 2-faktor autentisering for GitHub. Det neste vi må gjøre er å koble oss til Single Sign On (SSO) for SSB sin organisasjon på GitHub:\n\nTrykk på lenken https://GitHub.com/orgs/statisticsnorway/sso\nI dialogboksen som dukker opp trykker du på Continue, slik som vist i Figur 8.\n\n\n\n\n\n\n\nFigur 8: Single Sign on (SSO) for SSB sin organisasjon på GitHub\n\n\n\nNår du har gjennomført dette så har du tilgang til statisticsnorway på GitHub. Går du inn på denne lenken så skal du nå kunne lese både Public, Private og Internal repoer, slik som vist i Figur 9.\n\n\n\n\n\n\nFigur 9: Medlemsvisning for SSB sin GitHub-organisasjon.\n\n\n\n\n\nPersonal Access Token (PAT)\nNår vi skal jobbe med SSB-kode som ligger lagret hos statistcsnorway på GitHub, så må vi autentisere oss. Måten vi gjøre det på er ved å generere et Personal Access Token (ofte forkortet PAT) som vi oppgir når vi vil hente eller oppdatere kode på GitHub. Da sender vi med PAT for å autentisere oss for GitHub.\n\nOpprette PAT\nFor å lage en PAT som er godkjent mot statisticsnorway gjør man følgende:\n\nGå til din profilside på GitHub og åpne Settings slik som ble vist Seksjon 0.2.\nVelg Developer Settings i menyen til venstre.\nI menyen til venstre velger du Personal Access Token, og deretter Tokens (classic).\nVelg Generate new token og deretter Generate new token (classic).\nUnder Note kan du gi PAT’en et navn. Velg et navn som er intuitivt for deg. Hvis du skal bruke PAT til å jobbe mot Dapla, så ville jeg ganske enkelt kalt den dapla. Hvis du skal bruke den mot bakkemiljøet ville jeg kalt den prodsone eller noe annet som gjør det lett for det skjønne innholdet i ettertid.\nUnder Expiration velger du hvor lang tid som skal gå før PAT blir ugyldig. Dette er en avvening mellom sikkerhet og hva som er praktisk. Det anbefales at du velger 365 dager. Når PAT utløper må du gjenta stegene i dette kapittelet.\nUnder Select scopes velger du repo og workflow slik som vist i Figur 10.\n\n\n\n\n\n\n\nFigur 10: Gi token et kort og beskrivende navn\n\n\n\n\nTrykk på Generate token nederst på siden og du får noe lignende det du ser i Figur 11.\n\n\n\n\n\n\n\nFigur 11: Token som ble generert.\n\n\n\n\nKopier deretter PAT til en midlertidig fil. Grunnen er at du aldri vil se det igjen her etter at vi har gjennomført neste steg.\nDeretter trykker du på Configure SSO og velger Authorize ved siden statisticsnorway, slik som vist i Figur 12. Svar deretter på spørsmålene som dukker opp.\n\n\n\n\n\n\n\nFigur 12: Autorisering av Token mot SSBs GiHub-organisasjon.\n\n\n\nVi har nå opprettet en PAT som er godkjent for bruk mot SSB sin kode på GitHub. Det betyr at hvis vi vil jobbe med Git på SSB sine maskiner i sky eller på bakken, så må vi sendte med dette tokenet for å få lov til å jobbe med koden som ligger på statisticsnorway på GitHub.\n\n\nLagre PAT i Dapla Lab\nAnta at min personlige GitHub-bruker er SSB-Chad og at min Personal Access Token er a13bc12. Da kan jeg gjøre følgende for å lagre det i Dapla Lab:\n\nLogg inn på https://lab.dapla.ssb.no\nTrykk på ‘My account’\nNaviger til Git-fanen\nSkriv inn brukernavn (f.eks SSB-Chad) under Username for Git\nSkriv inn e-posten som er koblet opp mot Git\nLim inn token (f.eks a13bc12) der det står ‘Git Forge Personal Access Token’ vist i Figur 13\n\n\n\n\n\n\n\nFigur 13: DaplaLab My account: lagre PAT",
    "crumbs": [
      "Manual",
      "Kode",
      "GitHub: Bruker og oppsett"
    ]
  },
  {
    "objectID": "statistikkere/data-collector.html",
    "href": "statistikkere/data-collector.html",
    "title": "Data-Collector",
    "section": "",
    "text": "WarningData Collector skal avvikles\n\n\n\nDet er bestemt at Data Collector skal avvikles og derfor er det ikke ønskelig å tilby nye team å bruke tjenesten. Ta kontakt med team Statistikktjenester dersom du har et behov for å bruke Data Collector.\nData Collector (DC) er et rammeverk for bruk av REST APIer som samler inn data fra eksterne ressurser og skriver det til kildebøtter. DC kjører en deklarativ spesifikasjon ved kjøretid som beskriver hvordan data skal samles inn. Spesifikasjonen er bygget med en veldefinert DSL.\nDC-jobb startes fra Jupyter ved å bruke en funksjon fra Dapla Toolbelt. Innsamlingsjobber beskrives med en specification (json-fil).\nLes mer om arkitektur og funksjonalitet",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Data-Collector"
    ]
  },
  {
    "objectID": "statistikkere/data-collector.html#bruke-collector-fra-dapla-lab",
    "href": "statistikkere/data-collector.html#bruke-collector-fra-dapla-lab",
    "title": "Data-Collector",
    "section": "Bruke Collector fra Dapla-lab",
    "text": "Bruke Collector fra Dapla-lab\nFør brukeren kan kjøre DC fra Dapla Lab, må en team Statistikktjenester ha satt opp en instans for teamet.\n\nSett opp collector\n\n\nnotebook\n\nimport json\n\nfrom dapla import CollectorClient\n\ncollector_url = \"https://data-collector-&lt;team_navn&gt;.intern.ssb.no/tasks\"\ncollector = CollectorClient(collector_url)\nspecification = None\n\n# Load specification from file\nwith open(\"&lt;specification_file&gt;.json\") as specification_file:\n    specification = json.load(specification_file)\n\ntopic = specification['configure'][0]['globalState']['global.topic']\nprint (topic)\n\n\n\nStart data-innsamlingsjobb\n\n\nnotebook\n\nresponse = collector.start(specification)\ntask_id = response.json()['workerId']\nprint(f\"Startet collector jobb, data skal bli skrevet til gs://&lt;kilde-bøtte&gt;/{topic}/\")\n\n\n\nListe kjøreneder tasks\n\n\nnotebook\n\nrunning_tasks = collector.running_tasks().json()\nprint(running_tasks)\n\n\n\nStoppe kjøreneder tasks\n\n\nnotebook\n\n\nstop_response = collector.stop(task_id)\nprint(stop_response)",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Data-Collector"
    ]
  },
  {
    "objectID": "statistikkere/data-collector.html#konfigurasjoner",
    "href": "statistikkere/data-collector.html#konfigurasjoner",
    "title": "Data-Collector",
    "section": "Konfigurasjoner",
    "text": "Konfigurasjoner\nDet er 3 Dapla-team med kjørende DC-instanser i prod- og test-miljøet:\ncollector-url\n\nskatt-person\n\nTEST:\n\nskattemelding: https://data-collector-skatt-person-skattemelding.intern.test.ssb.no/tasks\nskatteoppgjor: https://data-collector-skatt-person-skatteoppgjor.intern.test.ssb.no/tasks\n\nPROD:\n\nskattemelding: https://data-collector-skatt-person-skattemelding.intern.ssb.no/tasks\nskatteoppgjor: https://data-collector-skatt-person-skatteoppgjor.intern.ssb.no/tasks\n\n\nskatt-naering\n\nTEST: https://data-collector-skatt-naering.intern.test.ssb.no/tasks\nPROD: https://data-collector-skatt-naering.intern.ssb.no/tasks\n\nstrukt-mva\n\nTEST: https://data-collector-strukt-mva.intern.test.ssb.no/tasks\nPROD: https://data-collector-strukt-mva.intern.ssb.no/tasks",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Data-Collector"
    ]
  },
  {
    "objectID": "statistikkere/data-collector.html#logger",
    "href": "statistikkere/data-collector.html#logger",
    "title": "Data-Collector",
    "section": "Logger",
    "text": "Logger\nDet er mulig å sjekke logger fra google-console for test og prod.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Data-Collector"
    ]
  },
  {
    "objectID": "statistikkere/features.html",
    "href": "statistikkere/features.html",
    "title": "Features",
    "section": "",
    "text": "WarningUnder arbeid\n\n\n\nFeatures må foreløpig skrus på av plattformteamene. Ta kontakt med Kundeservice hvis du ønsker å få en feature skrudd på.\nEn feature er en GCP-tjeneste som som er satt opp og konfigurert slik at Dapla-team kan ta det i bruk på en enkel og selvbetjent måte. Når man tar i bruk en feature kan man være sikker på at sikkerhet og beste-praksis i SSB er ivaretatt. Et viktig poeng med features er at teamene selv skal kunne skru av og på features etter behov.\nForeløpig er det tilgjengeliggjort følgende features på Dapla:",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Features"
    ]
  },
  {
    "objectID": "statistikkere/features.html#aktivere-feature",
    "href": "statistikkere/features.html#aktivere-feature",
    "title": "Features",
    "section": "Aktivere feature",
    "text": "Aktivere feature\n\n\n\n\n\n\nWarningSkru på en feature om gangen.\n\n\n\nHvis du ønsker å skru på flere features samtidig, så må du gjøre det i flere PR-er. Atlantis vil ikke klare å håndtere flere features i samme PR. Følg oppskriften under for hver feature du ønsker å skru på.\n\n\nAktivering av en feature for Dapla-teamet ditt er veldig enkelt. Endringen er så liten at vi anbefaler å gjøre endringen direkte i GitHubs grensesnitt, uten å klone repoet først. Slik går du frem:\n\nSøk opp ditt teams sitt IaC-repoet under https://github.com/statisticsnorway/.\n\n\n\n\nI repoet åpner du fila ./infra/projects.yaml. Se eksempel for teamet play-obr til høyre.\nFinn ut om du ønsker å skru på en feature i test eller prod. Hvis du ønsker å gjøre det i prod, så skal du legge til en linje under features der env: prod. Hvis du ønsker å gjøre det i test, så skal du legge til en linje under features der env: test.\nDu kan legge til linjen ved å trykke på -ikonet øverst til høyre i fila, endre teksten, og trykke på Commit changes. Velg deretter hvilket navn du ønsker på branchen, og trykk Create pull request.\nI PR-en venter du og ser at atlantis/plan1 har blitt kjørt og får en  til venstre for hver kjøring, slik som vist i Figur 1.\n\n\n\n\n./infra/projects.yaml\n\nteam_uniform_name: play-obr\n\nprojects:\n  - project_name: play-obr\n    env: test\n    features:\n      - dapla-buckets\n      - kildomaten\n\n  - project_name: play-obr\n    env: prod\n    features:\n      - dapla-buckets\n\n\n\n\n\n\n\n\n\nFigur 1: Atlantis plan planlegger endringen og sjekker om den er klar til gjennomføring.\n\n\n\n\nHvis alt er i orden så ber du en kollega om å se over endringen og godkjenne hvis alt ser riktig ut. Når den er godkjent vil du se et bilde som ligner det du ser i Figur 2.\n\n\n\n\n\n\n\nFigur 2: Sjekker av PR-er i teamets IaC-repo.\n\n\n\n\nNår PR er godkjent av en kollega, og atlantis plan ikke viser noen feilmeldinger, så kan du effektuere endringene ved å atlantis apply i kommentarfeltet til PR-en. Kommentareren vil trigge en kjøring som effektuerer alle endringer på plattformen.\nEtter at atlantis apply er kjørt, så kan du merge inn branchen til main.\nTil slutt kan du slette branchen.\n\nNår dette er gjort så endringen effektuert på Dapla og klar til bruk. For eksempel, hvis vi hadde aktivert transfer-service i prod for projects.yaml over, så ville den etter endringen sett slik ut:\n\n\n./infra/projects.yaml\n\nteam_uniform_name: play-obr\n\nprojects:\n  - project_name: play-obr\n    env: test\n    features:\n      - dapla-buckets\n      - kildomaten\n\n  - project_name: play-obr\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Features"
    ]
  },
  {
    "objectID": "statistikkere/features.html#deaktivere-en-feature",
    "href": "statistikkere/features.html#deaktivere-en-feature",
    "title": "Features",
    "section": "Deaktivere en feature",
    "text": "Deaktivere en feature\nFor å deaktivere en feature som ikke lenger i bruk, så følger du bare beskrivelsen for Aktivere tjeneste over, med det unntak at du nå fjerner en linje istedenfor å legge til.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Features"
    ]
  },
  {
    "objectID": "statistikkere/features.html#footnotes",
    "href": "statistikkere/features.html#footnotes",
    "title": "Features",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nDet som skjer når atlantis plan kjøres er at det genereres en detaljert beskrivelse av hvilke endringer som må skje på plattformen for at teamets feature skal aktiveres. Derfor må eventuelle feilmeldinger fra atlantis plan fikses før man faktiske kan effektuere endringene med atlantis apply. ↩︎",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Features"
    ]
  },
  {
    "objectID": "statistikkere/transfer-service.html",
    "href": "statistikkere/transfer-service.html",
    "title": "Transfer Service",
    "section": "",
    "text": "Storage Transfer Service1 er en Google-tjeneste for å flytte data mellom lagringsområder. I SSB bruker vi hovedsakelig tjenesten til å:\nTjenesten støtter både automatiserte og ad-hoc overføringer, og den inkluderer et brukergrensesnitt for å sette opp og administrere overføringene i Google Cloud Console (GCC).",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Transfer Service"
    ]
  },
  {
    "objectID": "statistikkere/transfer-service.html#tilgangsstyring",
    "href": "statistikkere/transfer-service.html#tilgangsstyring",
    "title": "Transfer Service",
    "section": "Tilgangsstyring",
    "text": "Tilgangsstyring\nTilgangsstyringen til data gjelder også for overføringer av data med Transfer Service. Det betyr at du må ha tilgang til dataene du skal sette opp overføringsjobber for. Ved bruk av Transfer Service for overføring av data mellom bakke og sky så er det satt opp en dedikerte mapper for dette i prodsonen. Også her følges tilgangsstyringen til dataene, med unntak av at data-admins har permanent tilgang til kildedata som er synkronisert ned til bakken, mens man på Dapla må de gi seg selv korte, begrunnede tilganger ved behov.\n\n\nPå Dapla så er det opprettet dedikerte bøtter for overføring av data mellom bakke og sky. Disse heter tilsky og frasky. Tanken med disse “mellomstasjonene” for overføring av data er at de skal beskytte Dapla-team fra å overskrive data ved en feil. Ved å ha egne bøtter som data blir synkronisert gjennom, så legges det opp til at man deretter manuelt3 flytter dataene til riktig bøtte.\nMen det er ikke lagt noen sperrer for synkronisere direkte til en annen bøtte man har tilgang til. Systembrukeren (se forklaringsboks) som kjører Transfer Service har tilgang til alle bøttene i prosjektet. Det betyr at en data-admin kan velge å synkronisere data direkte inn i kildebøtta hvis man mener at det er hensiktsmessig. Det samme gjelder for developers som setter opp dataoverføringer i standardprosjektet. Men da er det som sagt viktig å være bevisst på hvordan man setter opp reglene for overskriving av data hvis filene har like navn. Disse opsjonene forklares nærmere senere i kapitlet.\n\n\n\n\n\n\n\n\n\nNotePersonlig bruker vs systembruker\n\n\n\nNår du setter opp en overføringsjobb med Transfer Service så setter du opp en jobb som kjøres av en systembruker4 og ikke din egen personlige bruker. Dette er spesielt viktig å være klar over når man setter opp automatiserte overføringsjobber. En konsekves av dette er at automatiske overføringsjobber vil fortsette å kjøre selv om din tilgang til dataene er midlertidig, siden det er en systembruker som faktisk kjører jobben.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Transfer Service"
    ]
  },
  {
    "objectID": "statistikkere/transfer-service.html#forberedelser",
    "href": "statistikkere/transfer-service.html#forberedelser",
    "title": "Transfer Service",
    "section": "Forberedelser",
    "text": "Forberedelser\nFørste gang du bruker Transfer Service må du sjekke at tjenesten er aktivert for teamet. Transfer Service er en såkalt feature som teamet kan skru av og på selv. For å sjekke om den er skrudd på går du inn i teamets IaC-repo5 og sjekker filen ./infra/projects.yaml.\n\n\ndapla-example-iac/infra/projects.yaml\n\nteam_uniform_name: dapla-example\n\nprojects:\n  - project_name: dapla-example\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service\n\nI filen over ser du at teamet har skrudd på tjenesten i prod-miljøet, siden den transfer-service er listet under features. Hvis tjenesten ikke er skrudd på kan du lese om hvordan du skrur den på i feature-dokumentasjonen.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Transfer Service"
    ]
  },
  {
    "objectID": "statistikkere/transfer-service.html#overføring-av-data",
    "href": "statistikkere/transfer-service.html#overføring-av-data",
    "title": "Transfer Service",
    "section": "Overføring av data",
    "text": "Overføring av data\n\n\n\n\n\n\nNoteOverføring av kildedata\n\n\n\nOverføring av kildedata må gjøres av en data-admin i teamet som har aktivert sin forhåndsgodkjente tilgang til kildedata. Tilgangen aktiveres ved å gå inn i JIT-applikasjonen og velge prosjekt-id. Deretter velger du rollene ssb.bucket.write, ssb.buckets.list og storagetransfer.admin, og hvor lenge du ønsker tilgangen. Til slutt oppgir du en begrunnelse for hvorfor du trenger tilgangentilgangen og trykker Request access. Når du har gjort dette vil du få en bekreftelse på at tilgangen er aktivert, og det tar ca 1 minutt før den aktiverte tilgangen er synlig i GCC.\n\n\nGrensesnittet for å sette opp overføringsjobber i Transfer Service er tilgjengelig i Google Cloud Console (GCC).\n\n\n\nGå inn på Google Cloud Console i en nettleser.\nSjekk, øverst i høyre hjørne, at du er logget inn med din SSB-konto (xxx@ssb.no).\nVelg prosjektet6 som overføringen skal settes opp under.\nEtter at du har valgt prosjekt kan du søke etter Storage Transfer i søkefeltet øverst på siden, og gå inn på siden.\n\n\n\n\n\n\n\n\n\n\nNoteHva er mitt prosjektnavn?\n\n\n\nNår det opprettes et Dapla-team, så opprettes det flere Google-prosjekter for teamet. Når du skal velge hvilket prosjekt du skal jobbe på i GCC, så følger de en fast navnestruktur. For eksempel så vil et team med navnet dapla-example få et standardprosjekt som heter dapla-example-p. Det blir også opprettet et kildeprosjekt som heter dapla-example-kilde-p.\n\n\n\n\nFørste gang du bruker Storage Transfer må man gjøre en engangsjobb for å bruke tjenesten. Dette gjøres kun første gang din bruker setter opp en jobb, og deretter trenger du ikke å gjøre det flere ganger.\nNår du kommer inn på siden til Storage Transfer så trykker du på Set Up Connection. Når du trykker på denne vil det dukke opp et nytt felt hvor du får valget Create Pub-Sub Resources. Trykk på den blå Create-knappen, og deretter trykk på Close lenger nede. Da er engangsjobben gjort, og du kan begynne å sette opp overføringsjobber.\n\nI navigasjonsmenyen til venstre trykk Transfer jobs, og deretter trykk på + Create transfer job øverst på siden for å opprette en ny overføringsjobb. Da får du opp bildet som vist i Figur 1.\n\n\n\n\n\n\n\nFigur 1: Opprett overføringsjobb i Google Cloud Console.\n\n\n\nVidere vil det variere om man skal overføre data mellom bøtter eller mellom Dapla og prodsonen. Under forklarer vi begge fremgangsmåtene.\n\nProdsonen og Dapla\nOverføring mellom bakke og sky er en overføring av data mellom en bøtte på Dapla og en mappe i prodsonen. Siden tilgangsstyring til kildedata er strengere enn tilgangsstyring til annen data, så er det det to litt fremgangsmåter for å sette opp overføringsjobber for disse.\nSiden stegene er litt forskjellig avhengig av om man skal flytte kildedata eller annen data, så deler vi denne delen i to. Figur 2 viser hvordan dette er satt opp. Kildeprosjektet på Dapla har en tilsky-bøtte for å flytting av data fra prodsonen til Dapla, og den har en frasky-bøtte for å flytte data fra Dapla til prodsonen. Standardprosjektet på Dapla har også en tilsky-bøtte for å flytte data fra prodsonen til Dapla, og den har en frasky-bøtte for å flytte data fra Dapla til prodsonen.\n\n\n\n\n\n\nFigur 2: Overføring av data mellom prodsonen og Dapla.\n\n\n\nVidere viser vi hvordan man overfører fra Dapla til prodsonen. Overføring motsatt vei innebærer bare at man bytter om på Source type og Destination type.\n\nI fanen Get started velger du:\n\nSource type: Google Cloud Storage\nDestination type: POSIX filesystem\n\nI fanen Choose a source trykker du på Browse, velger hvilken bøtte eller “undermappe” i en bøtte du skal overføre fra, og trykker Select7.\nI fanen Choose a destination velger du transfer_service_default under Agent pool. Under Destination directory path velger du hvilken undermappe av som filen skal overføres til. Tjenesten vet allerede om du er i kilde- eller standardprosjektet, så du trenger kun å skrive inn frasky/ eller tilsky/ her, og evt. undermappenavn hvis det er aktuelt (f.eks. frasky/data/8). Trykk Next step.\nI fanen Choose when to run job velger du hvor ofte og hvordan jobber skal kjøre. Tabell 1 viser hvilke valg du kan ta. Trykk Next step.\n\n\n\n\nTabell 1: Valg under Choose when to run job\n\n\n\n\n\n\n\n\n\nValg\nFrekvens\n\n\n\n\nRun once\nEngangoverføringer\n\n\nRun every day\nSynkroniser hver dag\n\n\nRun every week\nSynkroniser hver uke\n\n\nRun with custom frequency\nSynkroniser inntill hver time\n\n\nRun on demand\nSynkroniserer når du manuelt trigger jobben\n\n\n\n\n\n\n\nI fanen Choose settings kan du velge hvordan detaljer knyttet til overføringen skal håndteres. Tabell 2 viser hvilke valg du kan ta.\n\n\n\n\nTabell 2: Valg under Choose settings\n\n\n\n\n\n\n\n\n\n\nValg\nUndervalg\nHandling\n\n\n\n\nIdentify your job\n\nBeskriv jobben kort.\n\n\nManifest file\n\nIkke relevant. Bruk default valg.\n\n\nChoose how to handle your data\nMetadata options\nIkke relevant. Bruk default valg.\n\n\n\nWhen to overwrite\nTenk nøye gjennom hva du velger her.\n\n\n\nWhen to delete\nTenk nøye gjennom hva du velger her.\n\n\nChoose how to keep track of transfer progress\nLogging options\nSkru på logging.\n\n\n\n\n\n\nValgene When to overwrite og When to delete er det viktig at tenkes nøye gjennom, spesielt ved automatiske synkroniseringer. When to overwrite er spesielt siden det kan føre til data blir overskrevet eller tapt.\n\nTrykk på den blå Create-knappen for å opprette overføringsjobben. Du vil kunne se kjørende jobber under menyen Transfer jobs.\n\n\nMappestrukturen i prodsonen\nMappestrukturen for overføringer med Transfer Service mellom bakke og sky har en fast struktur som er likt for alle team. Hvis du logger deg inn i terminalen på en av Linux-serverne i prodsonen, åpner du mappen ved å skrive cd /ssb/cloud_sync. Under denne mappen finner du en mappe for hvert team som har aktivert Transfer Service. Hvis et team for eksempel heter dapla-example så vil det være en mappe som heter dapla-example. Her kan teamet hente og levere data som skal synkroniseres mellom bakke og sky. Videre er det undermapper for kilde- og standardprosjektet til teamet. Det er kun data-admins som har tilgang til kildeprosjektet, og det er kun developers som har tilgang til standardprosjektet. Under finner du en oversikt over hvordan mappene ser ut for et team som heter dapla-example.\n\n\n/ssb/cloud_sync/dapla-example/\n\ndapla-example\n│\n├── kilde\n│   │\n│   │── tilsky\n│   │\n│   └── frasky\n│\n└── standard\n    │\n    │── tilsky\n    │\n    └── frasky\n\n\n\n\nBøtte til bøtte\nOverføring mellom bøtter er en overføring av data mellom to bøtter på Dapla. Fremgangsmåten er helt likt som beskrevet tidligere, men at du nå velger Google Cloud Storage som både kilde og destinasjon. Igjen er vi avhengig av at systembrukeren som utfører jobben har tilgang til begge bøttene som er involvert i overføringen. Default er at et team kan overføre mellom bøtter i kildeprosjektet, og at de kan overføre mellom bøtter i standardprosjektet, men aldri mellom de to. Hvis du ønsker å overføre mellom bøtter i ditt prosjekt og et annet teams prosjekt, så må du be det andre teamet om å gi din systembruker tilgang til dette.",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Transfer Service"
    ]
  },
  {
    "objectID": "statistikkere/transfer-service.html#footnotes",
    "href": "statistikkere/transfer-service.html#footnotes",
    "title": "Transfer Service",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI SSB kaller vi tjenesten for Transfer Service, men du kan oppleve at Google kaller den litt forskjellige ting. Den blir omtalt som Storage Transfer Service noen steder, mens i Google Cloud Console blir den omtalt som Data Transfer eller Storage Transfer↩︎\nFlytting av data mellom bøtter krever at prosjektets Transfer Service har tilgang til begge bøttene.↩︎\nMed manuelt menes her at man går inn og flytter filer fra en bøtte til en annen. Men det kan også bety at man flytter data til riktig bøtte som en del produksjonskoden sin, som igjen kan kjøres automatisk.↩︎\nSystembrukere heter Service Accounts på engelsk og blir ofte referert til som SA-er i dagligtale.↩︎\nDu finner teamets IaC-repo ved å gå inn på https://github.com/orgs/statisticsnorway/repositories og søke etter ditt teamnavn og åpne den som har navnestrukturen teamnavn-iac. For eksempel vil et team som heter dapla-example har et IaC-repo som heter dapla-example-iac.↩︎\nDu kan velge prosjekt øverst på siden, til høyre for teksten Google Cloud. I bildet under ser du at hvordan det ser ut når prosjektet dapla-felles-p er valgt.↩︎\nNår du skal velge en undermappe i en bøtte så er grensesnittet litt lite intuitivt. Du kan ikke trykke på navnet, men du på trykke på -tegnet for å se undermappene.↩︎\nNår du skal synkronisere fra Dapla til en undermappe i prodsonen, så må mappen i prodsonen allerede eksisterere. Hvis den ikke gjør det vil jobben feile. Ved synkronsiering fra prodsonen til Dapla trenger ikke undermappen eksistere, siden bøtter egentlig ikke har undermapper og filstien fra prodsonen bare blir til filnavnet i bøtta.↩︎",
    "crumbs": [
      "Manual",
      "Datatjenester",
      "Transfer Service"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html",
    "href": "statistikkere/datatilstander.html",
    "title": "Datatilstander",
    "section": "",
    "text": "En datatilstand er et resultat av at et datasett har gått gjennom gitte operasjoner og prosesser (Standardutvalget 2023, 5). Denne siden er ment som en kort innføring i de forskjellige datatilstandene. Siden er basert på det interne dokumentet Datatilstander SSB - 2. utgave. Definisjonene er direkte utdrag fra dette dokumentet. Se interndokumentet for en mer grundig gjennomgang av datatilstander i SSB.\nI SSB skiller vi mellom fem datatilstander:\nAlle datatilstander er obligatoriske bortsett fra inndata. Figur 1 viser hvordan de forskjellige datatilstandene henger sammen.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Datatilstander"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#kildedata",
    "href": "statistikkere/datatilstander.html#kildedata",
    "title": "Datatilstander",
    "section": "Kildedata",
    "text": "Kildedata\nKildedata er data lagret slik de ble levert til SSB fra dataeier. Eksempler på kildedata er: grunndata, transaksjonsdata, administrative data, statistiske data og aggregerte data og rapporter (Standardutvalget 2023, 7). Kildedata lagres i bøtten ssb-&lt;teamnavn&gt;-data-kilde-prod. Les mer om bøtter her og lagringsstandarder her.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Datatilstander"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#inndata",
    "href": "statistikkere/datatilstander.html#inndata",
    "title": "Datatilstander",
    "section": "Inndata",
    "text": "Inndata\nInndata er kildedata som er transformert til SSBs standard lagringsformat (Standardutvalget 2023, 8). Denne transformeringer inkluderer blant annet at dataene skal benytte UTF-8 tegnsett. Les mer om SSBs standard lagringsformat her. Inndata kan også være andre statistikkers klargjorte data og/eller statistikkdata (Standardutvalget 2023, 8). Inndata er ikke en obligatorisk datatilstand. Inndata lagres i bøtten ssb-&lt;teamnavn&gt;-data-produkt-prod.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Datatilstander"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#klargjorte-data",
    "href": "statistikkere/datatilstander.html#klargjorte-data",
    "title": "Datatilstander",
    "section": "Klargjorte data",
    "text": "Klargjorte data\nKlargjorte data er inndata hvor:\n\nvariablene er beregnet gjennom utregninger og koblinger mellom datasett\nnøyaktigheten er forbedret\n\nfor eksempel som resultat av editering eller imputering\n\nmetadata med variabeldefinisjoner er lagt til.\n\nEnhver endring som er gjort skal være sporbare og dokumentert slik at statistikkene skal være etterprøvbare. Klargjorte date er som regel ikke aggregerte - med mindre dataen vi mottar er aggregert. Med andre ord inneholder klargjorte data oftest enkeltobservasjoner - i likhet med kildedata og inndata (Standardutvalget 2023, 9). Klargjorte data lagres i bøtten ssb-&lt;teamnavn&gt;-data-produkt-prod.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Datatilstander"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#statistikk",
    "href": "statistikkere/datatilstander.html#statistikk",
    "title": "Datatilstander",
    "section": "Statistikk",
    "text": "Statistikk\nStatistikk er “Tallfestede opplysninger om en gruppe eller et fenomen, og som kommer frem ved en sammenstilling og bearbeidelse av opplysninger om de enkelte enhetene i gruppen eller et utvalg av disse enhetene, eller ved systematisk observasjon av fenomenet” ifølge statistikkloven § 3a (Standardutvalget 2023, 10). Statistikk lagres i bøtten ssb-&lt;teamnavn&gt;-data-produkt-prod.\nStatistikk er ofte aggregerte data eller estimerte størrelser. Vi skiller mellom ujustert statistikk og justert statistikk. Indekser og sesongjusterte tall er eksempler på justert statistikk (Standardutvalget 2023, 10).\nStatistikk kan være inndata til andre statistikker, og kan dermed inneholde konfidensielle og detaljerte data som ikke publiseres.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Datatilstander"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#utdata",
    "href": "statistikkere/datatilstander.html#utdata",
    "title": "Datatilstander",
    "section": "Utdata",
    "text": "Utdata\nUtdata er statistikk der kravene til konfidensialtet er ivaretatt. Dette er datatilstanden som publiseres. Eksempler inkluderer: statistikkbanktabeller, tabelloppdrag og internasjonal rapportering (Standardutvalget 2023, 11). Utdata lagres i bøtten ssb-&lt;teamnavn&gt;-data-produkt-produkt.",
    "crumbs": [
      "Manual",
      "Standarder",
      "Datatilstander"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#metadata-for-datatilstandene",
    "href": "statistikkere/datatilstander.html#metadata-for-datatilstandene",
    "title": "Datatilstander",
    "section": "Metadata for datatilstandene",
    "text": "Metadata for datatilstandene\nDet er forskjellige forventinger til metadata for de ulike datatilstandene. Forskjellene er skildret underdisse punktene:\n\nKildedata\n\nInformasjon på datasettnivå som dataeier, området dataene omhandler og tidsinformasjon\nMetadata om enkeltvariabler er begrenset til informasjonen dataeier selv avleverer.\n\n\n\nInndata\n\nI utgangspunktet samme som kildedata\n\n\n\nKlargjorte data\n\nVariabeldefinisjoner - beskrivelse av hver enkelt variabel og hvordan den er beregnet\nNøyaktighetsforbedrende tiltak som er utført\n\n\n\nStatistikk\n\nVariabeldefinisjoner\nHvilke metoder og programmer/kode som er benyttet for å produsere statistikken\n\n\n\nUtdata\n\nI utgangspunktet samme som for statistikk",
    "crumbs": [
      "Manual",
      "Standarder",
      "Datatilstander"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html",
    "href": "statistikkere/hva-er-dapla-team.html",
    "title": "Hva er Dapla-team?",
    "section": "",
    "text": "For å kunne jobbe på Dapla må man være en del av et Dapla-team. Et Dapla-team er en gruppe personer som har tilgang til spesifikke ressurser på Dapla. Ressursene kan være data, kode eller tjenester. Følgelig er teamet helt sentral for tilgangsstyringen på Dapla. Derfor er det viktig at alle som jobber på Dapla gjør seg godt kjent med innholdet i denne delen.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#opprette-dapla-team",
    "href": "statistikkere/hva-er-dapla-team.html#opprette-dapla-team",
    "title": "Hva er Dapla-team?",
    "section": "Opprette Dapla-team",
    "text": "Opprette Dapla-team\nAlle Dapla-team tilhører en seksjon og opprettes av seksjonslederen i den seksjonen. Dapla-team opprettes i applikasjonen Dapla-Ctrl.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#autonomitetsnivå",
    "href": "statistikkere/hva-er-dapla-team.html#autonomitetsnivå",
    "title": "Hva er Dapla-team?",
    "section": "Autonomitetsnivå",
    "text": "Autonomitetsnivå\nEt team på Dapla er i en av følgende autonomitetsnivåer:\n\nManaged\nSemi-Managed\nSelf-Managed\n\nNivået er definert i metadataene til teamet og vises i Teamvisningen i Dapla Ctrl. Det er kun plattformteamene som kan endre nivået til et team.\nFormålet med å definere autonomiten til et team er å tydeliggjøre hvem som har hvilket ansvar for de produktene teamet benytter på Dapla. Nivået settes når teamet opprettes, men kan også endres senere ved behov.\nEt Managed team benytter seg kun av tjenestene/features som tilbys av plattformen, og kan være sikker på at disse er satt opp i iht de krav som gjelder i SSB. Typisk vil de fleste statistikkteam være managed, og eksempler på tjenester er standard lagringsbøtter, Transfer Service, Kildomaten, osv.. Et Managed team kan kun benytte seg av tilgangsgruppene managers, data-admins og developers.\nEt Semi-Managed team benytter seg av tjenestene som tilbys på plattformen, akkurat som et Managed team, men de ønsker også ta i bruk noe funksjonalitet som ikke tilbys enda. F.eks. kan det være et statistikkproduserende team som ønsker å ta i bruk en Google-tjeneste som ikke tilbys på Dapla. I disse tilfellene kan teamet velge å ta et større ansvar for denne tjenesten og få noe bredere tilganger på plattformen. Ansvaret fordrer at teamet har kompetansen til å ta dette ansvaret, og de spesifikke detaljene vil avhenge hvilken tjeneste det er snakk om, og om de ønsker å benytte tjenesten i prod- eller test-miljøet til teamet.\nEt Self-Managed team vil typisk være team som består IT-utviklere med god kompetanse på skyutvikling i Google Cloud Platform. Teamet har kompetanse til å ta i bruk de tjenestene de mener er best for å løse sine oppgaver.\nI resten av dette kapitlet beskrives hovedsakelig Managed teams.\n\n\n\n\n\n\nNoteAutonomitetsnivå og tilganger i IaC-repo\n\n\n\n\n\nSiden hvert team får definert sine ressurser i et eget IaC-repo, så er det nær sammenheng mellom Autonomitetsnivå og hvilke tilganger teamet har til å gjøre endringer i IaC-repoet. Tabell 1 viser hvilke tilganger de ulike nivåene gir.\n\n\n\nTabell 1: Autonomitet og tilganger i IaC-repo\n\n\n\n\n\n\n\n\n\n\n\n\nManaged\nSemi-Managed\nSelf-Managed\n\n\n\n\nKan lage PR på IaC-kode\nJa\nJa\nJa\n\n\nTilgang på IaC-kode\nKun yaml-“støttefiler” (team-info, iam, buckets-shared, projects) og filer de endrer (dapla-filer)\nJa\nJa\n\n\nKan ta i bruk funksjonalitet utover dapla-features\nNei\nJa\nJa\n\n\nIaC-filstruktur (under infra-mappen)\nPredefinert\nPredefinert + egne filer\nFritt\n\n\n\n\n\n\nLes mer her.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#roller-i-teamet",
    "href": "statistikkere/hva-er-dapla-team.html#roller-i-teamet",
    "title": "Hva er Dapla-team?",
    "section": "Roller i teamet",
    "text": "Roller i teamet\nMedlemskap i et Dapla-team gir tilgang på spesifikke ressurser på Dapla. Men siden kildedataene til alle team er klassifisert som sensitive, så kan ikke alle på teamet ha lik tilgang til alle ressurser. Av den grunn er det definert 3 ulike roller på et team. To av disse, data-admins og developers, er forbeholdt de som jobber med data på teamet. Mens den tredje, managers, skal innehas av de som er ansvarlige for teamet. I de fleste tilfeller vil managers være seksjonslederen som er ansvarlige for statistikkproduktene teamet leverer. Under forklarer vi nærmere hva de ulike rollene innebærer.\n\nManagers\nRollen managers skal bestå av en eller flere data-ansvarlige (ofte omtalt som data-eiere eller seksjonsledere). managers har ansvar for følgende i teamet:\n\nhvem i teamet som får tilgang til hvilke data og tjenester.\nat teamet følger SSBs retningslinjer for tilgangsstyring.\nat teamet følger SSBs retningslinjer for klassifisering av data.\nvedlikehold og monitorering av tilganger.\nat teamet følger og forstår hvordan sensitive data skal behandles i SSB.\n\nManager-rollen krever ingen tilgang til data eller databehandlende tjenester på Dapla.\n\n\nDevelopers\nRollen developers er den mest vanlige rollen på et Dapla-team. Denne rollen skal tildeles alle som jobber med data i teamet. developers har tilgang til følgende ressurser:\n\nalt av teamets data, med unntak av kildedata.\nalle ressurser som behandler datatilstandene fra inndata til utdata.\n\n\n\nData-admins\nRollen data-admins er en priveligert rolle blant de som jobber med data i teamet. Rollen skal kun tildeles 2-3 personer på et team og disse er da medlem av både developers- og data-admins-gruppen. De som er medlem av data-admins-gruppen kan gjøre følgende:\n\nde er forhåndsgodkjent til å gi seg selv tidsbegrenset tilgang til kildedata ved behov. Tilgang til kildedata skal kun aktiveres i særskilte tilfeller, der eneste løsning er å se på kildedata i klartekst. Tilgang skal kun aktiveres i en begrenset periode, og krever en skriftlig begrunnelse. managers skal lett kunne monitere hvem som aktiverer denne tilgangen og hvor ofte.\nde kan godkjenne endringer i automatiske jobber som prosesserer kildedata til inndata.\nde kan overføre kildedata mellom bakke og sky.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#ressurser",
    "href": "statistikkere/hva-er-dapla-team.html#ressurser",
    "title": "Hva er Dapla-team?",
    "section": "Ressurser",
    "text": "Ressurser\nNår du oppretter et dapla-team så får man en grunnpakke med ressurser som de fleste i SSB vil trenge for å kunne jobbe med data på Dapla. I tillegg kan teamet selvbetjent skru på andre tjenester hvis man ønsker det. I det følgende forklarer vi hva som er inkludert i grunnpakken, og hva som er tilgjengelig for å skru på ved behov.\n\nGrunnpakken\nFigur 1 viser et overordnet bilde av hvilke ressurser som er inkludert i “grunnpakken”. Et Dapla-team får et testmiljø og prodmiljø. Det er i prodmiljøet at man jobber med skarpe data, mens testmiljøet er forbeholdt arbeid med testdata. I hvert miljø får teamet to Google-prosjekter. Ett for kildedata og et for datatilstandene inndata, klargjorte data, statistikkdata og utdata. Sistnevnte prosjekt kaller vi for standardprosjektet, siden det er her mesteparten av databehandlingen skjer.\n\n\n\n\n\n\nFigur 1: Diagram over hvilke miljøer, Google-prosjekter og bøtter et Dapla-team som et får ved opprettelse.\n\n\n\nAv Figur 1 ser vi at prosjektene i prodmiljøet får noen flere bøtter enn prosjektene i testmiljøet. Disse ekstrabøttene er forbeholdt synkronisering av data mellom bakke og sky, noe vi ikke legger til rette for i testmiljøet1. Les mer om overføring av data mellom bakke og sky her.\nRessursene som opprettes for et Dapla-team reflekterer i stor grad at kildedata er klassifisert som sensitive. Dette er grunnen til at det opprettes et eget prosjekt for kildedata, og at det kun er data-admins som potensielt kan få tilgang til dataene her. Opprettelsen av et eget testmiljø skyldes at Dapla-team i større grad enn før forventes å jobbe med testdata istedenfor skarpe data.\nAlle ressursene som opprettes for teamet er definert i tekstfiler i et GitHub-repo. Dette repoet kaller vi for et IaC-repo (Infrastructure as Code). IaC-repoet er en del av grunnpakken, og er tilgjengelig for alle på teamet. Statistikkere trenger ikke å forholde seg til dette repoet i stor grad, med unntak av når de skal aktivere/deaktivere features og når de skal sette opp Kildomaten.\n\n\nFeatures\nI tillegg til grunnpakken med ressurser, så kan teamet selvbetjent skru på følgende features eller tjenester ved behov:\n\nTransfer Service kan skrus på hvis teamet trenger å synkronisere data mellom ulike lagringssystemer. For eksempel mellom bakke og sky, eller mellom to ulike skytjenester.\nKildomaten kan skrus på hvis teamet trenger å automatisere overgangen fra kildedata til inndata.\nShared-buckets kan skrus på hvis teamet trenger å opprette delt-bøtter.\n\nForeløpig er det kun disse tre features som er tilgjengelig. Det vil komme flere etterhvert som behovene melder seg.\nLes mer om features her.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#github-team",
    "href": "statistikkere/hva-er-dapla-team.html#github-team",
    "title": "Hva er Dapla-team?",
    "section": "GitHub-team",
    "text": "GitHub-team\nVed opprettelsen av et Dapla-team så blir det også opprettet et tilsvarende GitHub-team med samme navn som Dapla-teamet. Grunnen til at det blir opprettet et GitHub-team er at GitHub er en sentral del av Dapla. Alle ressurser som skal opprettes på plattformen defineres av GitHub-repoer, og vi ønsker at tilganger her også skal reflektere tilgangene på Dapla.\nFor eksempel vil et team med navnet dapla-example få et GitHub-team med navnet dapla-example. Alle som er medlem av Dapla-teamet vil automatisk bli medlem av GitHub-teamet. I tillegg vil gruppetilhørighet og tilgangsroller på GitHub-teamet reflektere tilgangsroller på Dapla-teamet. For eksempel så kan dapla-example-data-admins gis tilgang til repo, og da vil alle som er medlem av Dapla-teamet med rollen data-admins få tilgang til repoet. Dette benyttes blant annet for å gi teamet tilgang til automation-mappen i sitt IaC-repo. I tillegg kan teamet bruke GitHub-teamet til å gi tilgang til andre GitHub-repoer som er relevante for teamet, for eksempel kodenbasen til en statistikkproduksjon eller lignende. Fordelen er at tilganger er gitt på teamnivå og ikke på personnivå. For eksempel hvis manager for teamet fjerner en ansatt fra developers-gruppa, så mister de all tilgang til data, tjenester og kode på GitHub som er tilgjengelig for developers.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#navnestruktur",
    "href": "statistikkere/hva-er-dapla-team.html#navnestruktur",
    "title": "Hva er Dapla-team?",
    "section": "Navnestruktur",
    "text": "Navnestruktur\nNår du oppretter et Dapla-team så må du velge et navn på teamet. Teamet velger selv et navn som reflekterer domene og subdomene. For eksempel kan et team som jobber med statistikkproduksjonen skattestatistikk for næringslivet velge å kalle teamet Skatt næring. Hvis vi bruker dette teamet som et eksempel, så vil det få opprettet et teknisk navn som følger denne strukturen: skatt-naering. Dette navnet er det som brukes i tekniske sammenhenger, for eksempel som navn på GitHub-teamet, IaC-repoet, Google-prosjektene og bøttene. Tabell 2 viser en tabell over hvordan ressursene for dette teamet vil se ut:\n\n\n\nTabell 2: Navnestruktur for teamet Skatt næring sine ressurser\n\n\n\n\n\n\n\n\n\nNavn\nBeskrivelse\n\n\n\n\nskatt-naering\nTeknisk teamnavn\n\n\nskatt-naering-managers\nAD-gruppe for managers\n\n\nskatt-naering-data-admins\nAD-gruppe for data-admins og et GitHub-team\n\n\nskatt-naering-developers\nAD-gruppe for developers og et GitHub-team\n\n\nskatt-naering-kilde-p\nNavn på kildeprosjekt i prod\n\n\nskatt-naering-p\nNavn på standardprosjekt i prod\n\n\nskatt-naering-kilde-t\nNavn på kildeprosjekt i test\n\n\nskatt-naering-t\nNavn på standardprosjekt i test\n\n\n\n\n\n\nI Tabell 2 ser vi at teamet får opprettet 3 AD-grupper og 4 Google-prosjekter. AD-gruppene brukes til å gi tilgang til ressursene på Dapla, mens Google-prosjektene brukes til å organisere ressursene. I tillegg er det en fast navnestruktur for bøttene i hvert prosjekt, slikt som vist i Tabell 3.\n\n\n\nTabell 3: Navnestruktur for teamet Skatt næring sine bøtter\n\n\n\n\n\nProsjektnavn\nBøttenavn\n\n\n\n\nskatt-naering-kilde-p\nssb-skatt-naering-data-kilde-prod\n\n\n\nssb-skatt-naering-data-kilde-frasky-prod\n\n\n\nssb-skatt-naering-data-kilde-tilsky-prod\n\n\nskatt-naering-p\nssb-skatt-naering-data-produkt-prod\n\n\n\nssb-skatt-naering-data-frasky-prod\n\n\n\nssb-skatt-naering-data-tilsky-prod\n\n\nskatt-naering-kilde-t\nssb-skatt-naering-data-kilde-test\n\n\nskatt-naering-t\nssb-skatt-naering-data-produkt-test",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#dapla-felles",
    "href": "statistikkere/hva-er-dapla-team.html#dapla-felles",
    "title": "Hva er Dapla-team?",
    "section": "Dapla Felles",
    "text": "Dapla Felles\nAlle i SSB er med i developers-gruppa til team Dapla Felles. Formålet med teamet er gjøre det lett som mulig for alle i SSB å komme-i-gang med Dapla, samtidig som det er et egnet sted for å dele åpne data eller kursmateriell. Teamet har autonomitetsnivå managed, og har de samme bøttene som et vanlig statistikkproduserende team.\nAlle i SSB har lese- og skrivetilgang til produkt-bøtta til Dapla Felles, og derfor skal det aldri deles data som ikke alle i SSB kan benytte. I tillegg må alle forvente at data her kan slettes og overskrives med jevne mellomrom.",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#footnotes",
    "href": "statistikkere/hva-er-dapla-team.html#footnotes",
    "title": "Hva er Dapla-team?",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nTa kontakt med produkteier for Dapla hvis du trenger å synkronisere testdata mellom bakke og sky↩︎",
    "crumbs": [
      "Manual",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/git-og-github.html",
    "href": "statistikkere/git-og-github.html",
    "title": "Hva er Git og GitHub?",
    "section": "",
    "text": "I SSB bruker vi Git til versjonskontroll av koden vår og deler den med andre via GitHub. For å mestre disse verktøyene er det viktig å forstå forskjellen mellom Git og GitHub.\nGit og GitHub er viktige verktøy for å sikre at produksjonssystemene våre er trygge og reproduserbare. De gjør det enkelt å spore endringer og gjennomgå eller godkjenne hverandres bidrag.\nI denne artikkelen ser vi nærmere på Git og GitHub og hvordan de er implementert i SSB. Selv om ssb-project gjør det lettere å forholde seg til Git og GitHub vil vi dette kapittelet forklare nærmere hvordan det funker uten dette hjelpemiddelet.\nEr du her for å opprette Personal Access Token? Les siste del av artikkelen! Eller lurer du på hvordan du bruker Git-grensesnittet i Jupyter? Les denne artikkelen!",
    "crumbs": [
      "Manual",
      "Kode"
    ]
  },
  {
    "objectID": "statistikkere/git-og-github.html#git",
    "href": "statistikkere/git-og-github.html#git",
    "title": "Hva er Git og GitHub?",
    "section": "Git ",
    "text": "Git \n\nHva er Git?\nGit er en programvare for distribuert versjonshåndtering av filer:\n\nGit tar vare på historien til koden din\nAlle som jobber med koden har en kopi av koden hos seg selv (distribuert)\n\nNår man ønsker å dele kode med andre laster man det opp til et felles kodelager på GitHub kalt repository (repo). Vanligvis versjonshåndteres rene tekstfiler, men git kan også versjonshåndtere bilder og PDFer.\nGit er installert på maskinen du jobber på og brukes fra terminalen. Det finnes pek-og-klikk versjoner av Git, blant annet i Jupyterlab og RStudio, men noen situasjoner vil bare kunne løses i terminalen.\nI SSB anbefaler vi at du starter et nytt Git-prosjekt ved å benytte ssb-project. Da vil du ikke bare aktivere Git, men du kan også få implementert en del andre gode praksiser for å holde koden din ryddig, oversiktlig og sikker.\nLes mer om Git på https://git-scm.com/.\n\n\n\n\n\n\nNoteGit i praksis: Kort fortalt\n\n\n\n\n\nMan aktiverer Git på en mappe i filsystemet sitt med kommandoen git init når man står i mappen som skal versjonshånderes. Da vil Git versjonshåndtere alle filer som er i den mappen og i eventuelle undermapper. Når du så gjør endringer på en fil i mappen, vil Git registrere endringer. Ønsker du at endringen skal bli et punkt i historikken til prosjektet, så må du først legge til filen i Git med kommandoen git add filnavn. Når du har gjort dette, så kan du lagre endringen med kommandoen git commit -m \"Din melding her\". Når du har gjort dette, så vil endringen være lagret i Git. Når du har gjort mange endringer, så kan du sende endringene til GitHub med kommandoen git push. Når du har gjort dette, så vil endringene være synlige for alle som har tilgang til GitHub-prosjektet.\n\n\n\n\n\nOppsett av Git/GitHub på Dapla Lab\nMan kan lagre GitHub-brukernavn og PAT i Dapla Lab sine instillinger. Da vil de hentes automatisk hver gang man Git/Dapla Lab trenger det. Vi har skrevet en egen del om det i artikkelen vår om Dapla Lab.\n\n\nGit, notebooks og filformat: Jupytext\nNår man versjonshåndterer bør man ta hensyn til hvilket filformat man lagrer kode i. Versjonshåndtering av notebooks (.ipynb) er ikke anbefalt siden filene inneholder mer enn bare tekst. De inneholder metadata og kode er delt inn i celler. Det gjør de vanskelig for mennesker å lese og dermed kontrollere/versjonshåndtere. I tillegg risikerer man å dytte opp data hvis man versjonshåndterer notebooks da notebooks kan lagre output. Tekstfiler (.pyog .R) er derimot enklere og tryggere å versjonshåndtere.\nDerfor bruker vi et verktøy som heter Jupytext. Jupytext synkroniserer notebooks med en tilsvarende tekstfil. Det lar oss skrive kode i notebooks mens programmene våre lagres og versjonshåndteres som tekstfiler. Det gir oss brukervennligheten til notebooks uten krøllet som oppstår når man skal versjonshåndtere de. Det er derfor vanlig å ha linjen *.ipynb i .gitignore-filen: da vil alle notebooks ignoreres av Git, og den vil bare forholde seg til de synkroniserte tekstfilene (altså tilsvarende .py eller .R-fil). Les om hvordan man implementerer og bruker Jupytext på IT-avdelingen sin confluence-side.\nStandardformatet for kobling med Jupytext er prosent-formatet.\n\n\n\n\n\n\nNoteBruk av Jupytext et vedtak fra IT\n\n\n\n\n\nBruk av jupytext er en Architecture Decision Record (ADR) - altså et vedtak fra IT. Les mer om ADR0020 på Confluence.\n\n\n\n\n\nVanlige Git-operasjoner\nI git er det noen operasjoner som er så vanlige at alle som jobber med kode i SSB bør kjenne dem:\n\ngit status for å se hvilke endringer Git har oppdaget\ngit add &lt;filnavn&gt; for å fortelle Git at endringene skal lagres\ngit commit -m \"Din melding her\" for å gjøre endringene om til et punkt i historien til koden din (flere filer kan samles under en commit)\ngit push for å sende endringene opp til GitHub\n\nNår man utvikler kode gjør man det fra såkalte branches1. Hvis vi tenker oss at din eksisterende kodebase er stammen på et tre (ofte kalt master eller main) legger Git opp til at man gjør endringer på denne koden via grener av treet. Med andre ord holder vi stammen (main) urørt helt til vi vet at endringen fungerer som den skal. Til slutt merger vi arbeidsgrenen inn i main. Vi kan opprette en ny branch og gå inn i den ved å skrive git switch -c &lt;branch navn&gt;. Da står du i en branch og kan bruke kommandoer som git add og git commit som vist tidligere.\nNår man er fornøyd med endringene i en branch, så vil man pushe den opp til GitHub, slik at en kollega kan vurdere om den skal merges inn i main. Denne vurderingen gjøres i en pull request. Dermed gjøres selve mergen i GitHub-grensenittet. Vi skal se nærmere på GitHub i neste kapittel.",
    "crumbs": [
      "Manual",
      "Kode"
    ]
  },
  {
    "objectID": "statistikkere/git-og-github.html#github",
    "href": "statistikkere/git-og-github.html#github",
    "title": "Hva er Git og GitHub?",
    "section": "GitHub ",
    "text": "GitHub \nGitHub er et nettsted som bl.a. fungerer som vårt felles mappesystem for deling av kode. SSB har sin egen organisasjonskonto med navn statisticsnorway. Selv om SSB har en organisasjonskonto på GitHub må alle ansatte opprette sin egen brukerprofil og knytte den mot SSB sin organisasjonskonto.",
    "crumbs": [
      "Manual",
      "Kode"
    ]
  },
  {
    "objectID": "statistikkere/git-og-github.html#footnotes",
    "href": "statistikkere/git-og-github.html#footnotes",
    "title": "Hva er Git og GitHub?",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nBranches kan oversettes til grener på norsk. Men i denne boken velger vi å bruke det engelske ordet branches. Grunnen er at det erfaringsmessig er lettere forholde seg til det engelske ordet når man skal søke etter informasjon i annen dokumentasjon↩︎",
    "crumbs": [
      "Manual",
      "Kode"
    ]
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html",
    "href": "notebooks/spark/deltalake-intro.html",
    "title": "Introduksjon til Delta Lake",
    "section": "",
    "text": "Delta Lake er et databaselag som kan legges over parquet-filer i bøtter. Det kan gi oss mye av den funksjonaliteten vi har vært vant til i relasjonelle databaser og datavarehus. I denne notebooken vises hvordan man kan ta det i bruk på Dapla."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#oppsett",
    "href": "notebooks/spark/deltalake-intro.html#oppsett",
    "title": "Introduksjon til Delta Lake",
    "section": "Oppsett",
    "text": "Oppsett\nHvis du logger deg inn på https://jupyter.dapla.ssb.no/ kan du ta i bruk Delta Lake via Spark. Men for å gjøre det må du installere delta-spark. For å installere pakken må du jobbe i et ssb-project. I tillegg må du installere delta-spark-versjon som er kompatibel med Spark-versjonen som er installert på Dapla. Gjør derfor følgende:\n\nOpprett et ssb-project med kommandoen:\nssb-project create test-delta-lake\nI terminalen skriver du følgende for å sjekke Spark-versjonen som er installert:\nspark-shell --version\nSjekk hvilken delta-spark-versjon som er kompatibel med din Spark-versjon og installer den med kommandoen1:\npoetry add delta-spark@2.3\nÅpne en ny notebook og velg kernel test-delta-lake.\n\nNå har du satt opp et virtuelt miljø med en PySpark-kernel som kjører en maskin (såkalt Pyspark local kernel), der du har installert delta-spark. Vi kan nå importere de bibliotekene vi trenger og sette igang en Spark-session.\n\n# Importerer biblioteker\nimport pyspark\nfrom delta import *\n\nDeretter initialiserer jeg en Spark-session. %%capture_output er med for å forhindre at det ikke blir printet ut sensitiv informasjon i en notebook som skal inn i Dapla-manualen.\n\n%%capture captured_output\n%run /usr/local/share/jupyter/kernels/pyspark_local/init.py"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#genererer-noe-data",
    "href": "notebooks/spark/deltalake-intro.html#genererer-noe-data",
    "title": "Introduksjon til Delta Lake",
    "section": "Genererer noe data",
    "text": "Genererer noe data\n\n# Genererer noe data med Spark\ndata = spark.range(10, 15)\ndata.show()\n\n+---+\n| id|\n+---+\n| 10|\n| 11|\n| 12|\n| 13|\n| 14|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#skriver-ut-en-delta-lake-table",
    "href": "notebooks/spark/deltalake-intro.html#skriver-ut-en-delta-lake-table",
    "title": "Introduksjon til Delta Lake",
    "section": "Skriver ut en Delta Lake Table",
    "text": "Skriver ut en Delta Lake Table\nLa oss skrive ut Spark DataFrame som en Delta Lake Table og bli kjent med strukturen i objektet:\n\n%%time\ndata.write.format(\"delta\").mode(\"overwrite\").save(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp4\"\n)\n\nCPU times: user 3.38 ms, sys: 1.7 ms, total: 5.08 ms\nWall time: 5.59 s\n\n\nVi kan deretter printe ut hva som ble opprettet når vi skrev ut en Delta Lake Table:\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/temp4/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp4/_delta_log',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000000.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-9b3b81a9-2771-4fb4-9f0f-659fd160d643-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-0f2f8ba5-3161-41e8-b5d1-2084128a5bed-c000.snappy.parquet']\n\n\n\nDelta Lake Tabellstruktur\nMappenstrukturen du ser over er typisk for en Delta Lake-tabell. La oss bryte ned komponentene:\n\n_delta_log/:\n\nDette er transaksjonsloggmappen for Delta Lake-tabellen. Den inneholder en serie med JSON-filer som registrerer alle transaksjoner gjort på tabellen.\nTransaksjonsloggen er avgjørende for Delta Lakes ACID-transaksjonsegenskaper, som muliggjør funksjoner som atomiske forpliktelser, tilbakerullinger og tid-reise-forespørsler.\n\n_delta_log/00000000000000000000.json:\n\nDette er en JSON-fil innenfor transaksjonsloggkatalogen. Hver JSON-fil i denne mappen tilsvarer en transaksjon (eller en batch av transaksjoner) gjort på tabellen.\nFilnavnet er null-fylt og representerer transaksjonsversjonen. I dette tilfellet representerer 00000000000000000000.json den aller første versjonen (versjon 0) av tabellen. Etter hvert som flere transaksjoner blir gjort, vil nye filer bli lagt til med økende versjonsnumre.\n\npart-00000-450715bd-6b0c-4827-bb8a-b0265505ca72-c000.snappy.parquet og part-00001-977ed55f-5ce5-469f-8a1d-9eafb143c215-c000.snappy.parquet:\n\nDette er de faktiske datafilene til Delta Lake-tabellen, lagret i Parquet-format.\n.snappy.parquet-utvidelsen indikerer at dataene er komprimert ved hjelp av Snappy-komprimeringsalgoritmen.\nFilnavnene er typiske for Sparks navngivningskonvensjon for datadeler. Prefiksene part-00000 og part-00001 indikerer partisjonsnumre. De lange UUID-lignende strengene er unike identifikatorer for datafilene. Suffikset c000 indikerer Spark-oppgaven som skrev filen.\n\n\nMappen representerer en Delta Lake-tabell med data lagret i Parquet-format og en transaksjonslogg som sporer endringer i tabellen. Tilstedeværelsen av _delta_log-mappen og innholdet er det som skiller en Delta Lake-tabell fra et vanlig Parquet-datasett."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#lese-inn-tabell",
    "href": "notebooks/spark/deltalake-intro.html#lese-inn-tabell",
    "title": "Introduksjon til Delta Lake",
    "section": "Lese inn tabell",
    "text": "Lese inn tabell\n\ndeltaTable = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 13|\n| 14|\n| 10|\n| 11|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#modifisere-datasettet",
    "href": "notebooks/spark/deltalake-intro.html#modifisere-datasettet",
    "title": "Introduksjon til Delta Lake",
    "section": "Modifisere datasettet",
    "text": "Modifisere datasettet\nLa oss modifisere datasettet ved å bytte ut verdien 13 med 15 i id-kolonnen:\n\nfrom pyspark.sql.functions import col, lit\n\n# Update the cell with value 13 to 15\ndeltaTable.update(condition=(col(\"id\") == 13), set={\"id\": lit(15)})\n\nEt viktig poeng å få med seg her er at vi nå oppdaterte Delta Lake Table objektet både i minnet og på disk. La oss bevise det ved å lese inn fra disk:\n\ndeltaTable2 = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\ndeltaTable2.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n+---+\n\n\n\nOg deretter ved å printe ut det opprinnelige objektet vi leste inn:\n\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#append-data",
    "href": "notebooks/spark/deltalake-intro.html#append-data",
    "title": "Introduksjon til Delta Lake",
    "section": "Append data",
    "text": "Append data\nLa oss legge til verdiene 20 og 21 til datasettet. Først lager vi en Spark dataframe:\n\nnew_data = [(20,), (21,)]\nnew_df = spark.createDataFrame(new_data, [\"id\"])\nnew_df.show()\n\n+---+\n| id|\n+---+\n| 20|\n| 21|\n+---+\n\n\n\nDeretter kan vi appendere det til vår opprinnelige dataframe:\n\nnew_df.write.format(\"delta\").mode(\"append\").save(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp4\"\n)\n\n\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n| 21|\n| 20|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#historien-og-metadata-til-filen",
    "href": "notebooks/spark/deltalake-intro.html#historien-og-metadata-til-filen",
    "title": "Introduksjon til Delta Lake",
    "section": "Historien og metadata til filen",
    "text": "Historien og metadata til filen\nNå som vi har gjort noen endringer kan vi se på historien til filen:\n\n# Lister ut filene i bøtta\nfs = FileClient.get_gcs_file_system()\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/temp4/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp4/_delta_log',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000000.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000001.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000002.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-73e5052f-1b82-48da-ab37-2cbc01bb46c1-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-9b3b81a9-2771-4fb4-9f0f-659fd160d643-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-d04d0ca2-8e8b-42e9-a8a3-0fed9a0e4e41-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-0f2f8ba5-3161-41e8-b5d1-2084128a5bed-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-30d707e4-dd9a-4bfd-a4c7-7fbb1933e9ae-c000.snappy.parquet']\n\n\nVi ser av transaksjonsloggen i _delta_log-mappen at det nå har vært 3 transaksjoner på datasettet. vi ser også av navnene på parquet-filene, part-00000 og part-00001, at det finnes to versjoner av filen. Hvis vi ønsker å bli bedre kjent med hva slags informasjon som blir lagret fra transaksjonene, så kan vi printe ut den siste transaksjonen som heter 00000000000000000002.json:\n\nimport json\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\n# Filsti\npath = \"gs://ssb-dapla-felles-data-produkt-prod/temp4/_delta_log/00000000000000000002.json\"\n\nwith fs.open(path, \"r\") as f:\n    for line in f:\n        data = json.loads(line)\n        pretty_content = json.dumps(data, indent=4)\n        print(pretty_content)\n        print(\"-\" * 50)  # Print separator for clarity\n\n{\n    \"commitInfo\": {\n        \"timestamp\": 1696942544879,\n        \"operation\": \"WRITE\",\n        \"operationParameters\": {\n            \"mode\": \"Append\",\n            \"partitionBy\": \"[]\"\n        },\n        \"readVersion\": 1,\n        \"isolationLevel\": \"Serializable\",\n        \"isBlindAppend\": true,\n        \"operationMetrics\": {\n            \"numFiles\": \"2\",\n            \"numOutputRows\": \"2\",\n            \"numOutputBytes\": \"956\"\n        },\n        \"engineInfo\": \"Apache-Spark/3.3.1 Delta-Lake/2.3.0\",\n        \"txnId\": \"a3dcd582-8362-4fc2-a8ce-57613d2eb2b8\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00000-73e5052f-1b82-48da-ab37-2cbc01bb46c1-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 478,\n        \"modificationTime\": 1696942544755,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":20},\\\"maxValues\\\":{\\\"id\\\":20},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00001-30d707e4-dd9a-4bfd-a4c7-7fbb1933e9ae-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 478,\n        \"modificationTime\": 1696942544833,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":21},\\\"maxValues\\\":{\\\"id\\\":21},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n\n\nHer ser vi at vi får masse informasjon om endringen, både metadata om transaksjonen i commitInfo, og informasjon om nye data-filer i add-delen. Vi ser at det er en veldig rik beskrivelse av endringer, men det kan være vanskeig å lese innholdet direkte. La oss heller bruke history()-funksjonen som kommer med delta:\n\nhistory = deltaTable.history()\nhistory.show()\n\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n|      2|2023-10-10 12:55:...|  null|    null|    WRITE|{mode -&gt; Append, ...|null|    null|     null|          1|  Serializable|         true|{numFiles -&gt; 2, n...|        null|Apache-Spark/3.3....|\n|      1|2023-10-10 12:55:...|  null|    null|   UPDATE|{predicate -&gt; (id...|null|    null|     null|          0|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n|      0|2023-10-10 12:55:...|  null|    null|    WRITE|{mode -&gt; Overwrit...|null|    null|     null|       null|  Serializable|        false|{numFiles -&gt; 2, n...|        null|Apache-Spark/3.3....|\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n\n\n\nSiden det blit trangt i tabellen over så kan vi velge hvilke variabler vi ønsker å se på:\n\n# Oversikt over alle kolonner som finnes i historien\nhistory.columns\n\n['version',\n 'timestamp',\n 'userId',\n 'userName',\n 'operation',\n 'operationParameters',\n 'job',\n 'notebook',\n 'clusterId',\n 'readVersion',\n 'isolationLevel',\n 'isBlindAppend',\n 'operationMetrics',\n 'userMetadata',\n 'engineInfo']\n\n\n\n# Velger de kolonnene jeg er interessert i\nselected_history = deltaTable.history().select(\n    \"version\", \"timestamp\", \"operation\", \"operationParameters\"\n)\n\n\n# Display the selected columns\nselected_history.show(truncate=50)\n\n+-------+-----------------------+---------+--------------------------------------+\n|version|              timestamp|operation|                   operationParameters|\n+-------+-----------------------+---------+--------------------------------------+\n|      2|2023-10-10 12:55:45.014|    WRITE|   {mode -&gt; Append, partitionBy -&gt; []}|\n|      1|2023-10-10 12:55:37.054|   UPDATE|        {predicate -&gt; (id#4452L = 13)}|\n|      0|2023-10-10 12:55:29.048|    WRITE|{mode -&gt; Overwrite, partitionBy -&gt; []}|\n+-------+-----------------------+---------+--------------------------------------+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#egendefinert-metadata",
    "href": "notebooks/spark/deltalake-intro.html#egendefinert-metadata",
    "title": "Introduksjon til Delta Lake",
    "section": "Egendefinert metadata",
    "text": "Egendefinert metadata\nDelta Lake støtter også egendefinert metadata. Det kan for eksempel være nyttig hvis man ønsker å bruke Delta Lake som en backend for et GUI som lar brukeren oppdatere en tabell fra GUI-et. Da ønsker man typisk å lagre hvem som gjorde endringer og når det ble gjort. La oss legge på noe slik metadata:\n\n# Leser inn filen\ndf = spark.read.format(\"delta\").load(\"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\n\n\n\nCode\n# Lagrer egendefinert metadata i en json-fil\nimport json\n\nmetadata = {\n    \"comment\": \"Kontaktet oppgavegiver og kranglet!\",\n    \"manueltEditert\": \"True\",\n    \"maskineltEditert\": \"False\",\n}\nmetadata\n\n\n{'comment': 'Kontaktet oppgavegiver og kranglet!',\n 'manueltEditert': 'True',\n 'maskineltEditert': 'False'}\n\n\nVi kan deretter appende dette til den siste versjonen av fila.\n\n(\n    df.write.format(\"delta\")\n    .mode(\"append\")\n    .option(\"userMetadata\", json.dumps(metadata))  # Serialize metadata to a string\n    .save(\"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\n)\n\n\n# Laster inn tabellen\ndeltaTable = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\n\n# Henter ut historien\nhistory_df = deltaTable.history()\n\n\n# Show the operation details, including metadata\nhistory_df.select(\n    \"version\", \"timestamp\", \"operation\", \"operationParameters\", \"userMetadata\"\n).show(truncate=10)\n\n+-------+----------+---------+-------------------+------------+\n|version| timestamp|operation|operationParameters|userMetadata|\n+-------+----------+---------+-------------------+------------+\n|      3|2023-10...|    WRITE|         {mode -...|  {\"comme...|\n|      2|2023-10...|    WRITE|         {mode -...|        null|\n|      1|2023-10...|   UPDATE|         {predic...|        null|\n|      0|2023-10...|    WRITE|         {mode -...|        null|\n+-------+----------+---------+-------------------+------------+\n\n\n\n\nhistory_df.select(\"version\", \"userMetadata\").show(truncate=50)\n\n+-------+--------------------------------------------------+\n|version|                                      userMetadata|\n+-------+--------------------------------------------------+\n|      3|{\"comment\": \"Kontaktet oppgavegiver og kranglet...|\n|      2|                                              null|\n|      1|                                              null|\n|      0|                                              null|\n+-------+--------------------------------------------------+\n\n\n\nVi ser at vi la til vår egen metadata i versjon 3 av fila. Vi kan printe ut den rå transaksjonsloggen som tidligere, men nå er vi på transaksjon 3 00000000000000000003.json:\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\n# Filsti\npath = \"gs://ssb-dapla-felles-data-produkt-prod/temp4/_delta_log/00000000000000000003.json\"\n\nwith fs.open(path, \"r\") as f:\n    for line in f:\n        data = json.loads(line)\n        pretty_content = json.dumps(data, indent=4)\n        print(pretty_content)\n        print(\"-\" * 50)  # Print separator for clarity\n\n{\n    \"commitInfo\": {\n        \"timestamp\": 1696942553907,\n        \"operation\": \"WRITE\",\n        \"operationParameters\": {\n            \"mode\": \"Append\",\n            \"partitionBy\": \"[]\"\n        },\n        \"readVersion\": 2,\n        \"isolationLevel\": \"Serializable\",\n        \"isBlindAppend\": false,\n        \"operationMetrics\": {\n            \"numFiles\": \"2\",\n            \"numOutputRows\": \"7\",\n            \"numOutputBytes\": \"989\"\n        },\n        \"userMetadata\": \"{\\\"comment\\\": \\\"Kontaktet oppgavegiver og kranglet!\\\", \\\"manueltEditert\\\": \\\"True\\\", \\\"maskineltEditert\\\": \\\"False\\\"}\",\n        \"engineInfo\": \"Apache-Spark/3.3.1 Delta-Lake/2.3.0\",\n        \"txnId\": \"e7de92bf-b0f9-4341-8bbb-9b382f2f3eb6\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00000-96369f3d-fe4a-4365-a0df-00c813027399-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 503,\n        \"modificationTime\": 1696942553856,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":5,\\\"minValues\\\":{\\\"id\\\":10},\\\"maxValues\\\":{\\\"id\\\":15},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00001-0f1bc8e6-093b-49a9-ad0b-78d5a148cfb6-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 486,\n        \"modificationTime\": 1696942553853,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":2,\\\"minValues\\\":{\\\"id\\\":20},\\\"maxValues\\\":{\\\"id\\\":21},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n\n\nVi er da at metadataene ligger som forventet i metadata-delen."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#footnotes",
    "href": "notebooks/spark/deltalake-intro.html#footnotes",
    "title": "Introduksjon til Delta Lake",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI eksempelet er det Spark V3.3.1 som er installert og jeg installerer derfor delta-spark v2.3↩︎"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html",
    "href": "notebooks/spark/deltalake-intro.out.html",
    "title": "Introduksjon til Delta Lake",
    "section": "",
    "text": "Delta Lake er et databaselag som kan legges over parquet-filer i bøtter. Det kan gi oss mye av den funksjonaliteten vi har vært vant til i relasjonelle databaser og datavarehus. I denne notebooken vises hvordan man kan ta det i bruk på Dapla."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#oppsett",
    "href": "notebooks/spark/deltalake-intro.out.html#oppsett",
    "title": "Introduksjon til Delta Lake",
    "section": "Oppsett",
    "text": "Oppsett\nHvis du logger deg inn på https://jupyter.dapla.ssb.no/ kan du ta i bruk Delta Lake via Spark. Men for å gjøre det må du installere delta-spark. For å installere pakken må du jobbe i et ssb-project. I tillegg må du installere delta-spark-versjon som er kompatibel med Spark-versjonen som er installert på Dapla. Gjør derfor følgende:\n\nOpprett et ssb-project med kommandoen:\nssb-project create test-delta-lake\nI terminalen skriver du følgende for å sjekke Spark-versjonen som er installert:\nspark-shell --version\nSjekk hvilken delta-spark-versjon som er kompatibel med din Spark-versjon og installer den med kommandoen[1]:\npoetry add delta-spark@2.3\nÅpne en ny notebook og velg kernel test-delta-lake.\n\nNå har du satt opp et virtuelt miljø med en PySpark-kernel som kjører en maskin (såkalt Pyspark local kernel), der du har installert delta-spark. Vi kan nå importere de bibliotekene vi trenger og sette igang en Spark-session.\n[1] I eksempelet er det Spark V3.3.1 som er installert og jeg installerer derfor delta-spark v2.3\n\n# Importerer biblioteker\nimport pyspark\nfrom delta import *\n\nDeretter initialiserer jeg en Spark-session. %%capture_output er med for å forhindre at det ikke blir printet ut sensitiv informasjon i en notebook som skal inn i Dapla-manualen.\n\n%%capture captured_output\n%run /usr/local/share/jupyter/kernels/pyspark_local/init.py"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#genererer-noe-data",
    "href": "notebooks/spark/deltalake-intro.out.html#genererer-noe-data",
    "title": "Introduksjon til Delta Lake",
    "section": "Genererer noe data",
    "text": "Genererer noe data\n\n# Genererer noe data med Spark\ndata = spark.range(10, 15)\ndata.show()\n\n+---+\n| id|\n+---+\n| 10|\n| 11|\n| 12|\n| 13|\n| 14|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#skriver-ut-en-delta-lake-table",
    "href": "notebooks/spark/deltalake-intro.out.html#skriver-ut-en-delta-lake-table",
    "title": "Introduksjon til Delta Lake",
    "section": "Skriver ut en Delta Lake Table",
    "text": "Skriver ut en Delta Lake Table\nLa oss skrive ut Spark DataFrame som en Delta Lake Table og bli kjent med strukturen i objektet:\n\n%%time\ndata.write.format(\"delta\").mode(\"overwrite\").save(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp4\"\n)\n\nCPU times: user 3.38 ms, sys: 1.7 ms, total: 5.08 ms\nWall time: 5.59 s\n\n\nVi kan deretter printe ut hva som ble opprettet når vi skrev ut en Delta Lake Table:\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/temp4/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp4/_delta_log',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000000.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-9b3b81a9-2771-4fb4-9f0f-659fd160d643-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-0f2f8ba5-3161-41e8-b5d1-2084128a5bed-c000.snappy.parquet']\n\n\n\nDelta Lake Tabellstruktur\nMappenstrukturen du ser over er typisk for en Delta Lake-tabell. La oss bryte ned komponentene:\n\n_delta_log/:\n\nDette er transaksjonsloggmappen for Delta Lake-tabellen. Den inneholder en serie med JSON-filer som registrerer alle transaksjoner gjort på tabellen.\nTransaksjonsloggen er avgjørende for Delta Lakes ACID-transaksjonsegenskaper, som muliggjør funksjoner som atomiske forpliktelser, tilbakerullinger og tid-reise-forespørsler.\n\n_delta_log/00000000000000000000.json:\n\nDette er en JSON-fil innenfor transaksjonsloggkatalogen. Hver JSON-fil i denne mappen tilsvarer en transaksjon (eller en batch av transaksjoner) gjort på tabellen.\nFilnavnet er null-fylt og representerer transaksjonsversjonen. I dette tilfellet representerer 00000000000000000000.json den aller første versjonen (versjon 0) av tabellen. Etter hvert som flere transaksjoner blir gjort, vil nye filer bli lagt til med økende versjonsnumre.\n\npart-00000-450715bd-6b0c-4827-bb8a-b0265505ca72-c000.snappy.parquet og part-00001-977ed55f-5ce5-469f-8a1d-9eafb143c215-c000.snappy.parquet:\n\nDette er de faktiske datafilene til Delta Lake-tabellen, lagret i Parquet-format.\n.snappy.parquet-utvidelsen indikerer at dataene er komprimert ved hjelp av Snappy-komprimeringsalgoritmen.\nFilnavnene er typiske for Sparks navngivningskonvensjon for datadeler. Prefiksene part-00000 og part-00001 indikerer partisjonsnumre. De lange UUID-lignende strengene er unike identifikatorer for datafilene. Suffikset c000 indikerer Spark-oppgaven som skrev filen.\n\n\nMappen representerer en Delta Lake-tabell med data lagret i Parquet-format og en transaksjonslogg som sporer endringer i tabellen. Tilstedeværelsen av _delta_log-mappen og innholdet er det som skiller en Delta Lake-tabell fra et vanlig Parquet-datasett."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#lese-inn-tabell",
    "href": "notebooks/spark/deltalake-intro.out.html#lese-inn-tabell",
    "title": "Introduksjon til Delta Lake",
    "section": "Lese inn tabell",
    "text": "Lese inn tabell\n\ndeltaTable = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 13|\n| 14|\n| 10|\n| 11|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#modifisere-datasettet",
    "href": "notebooks/spark/deltalake-intro.out.html#modifisere-datasettet",
    "title": "Introduksjon til Delta Lake",
    "section": "Modifisere datasettet",
    "text": "Modifisere datasettet\nLa oss modifisere datasettet ved å bytte ut verdien 13 med 15 i id-kolonnen:\n\nfrom pyspark.sql.functions import col, lit\n\n# Update the cell with value 13 to 15\ndeltaTable.update(condition=(col(\"id\") == 13), set={\"id\": lit(15)})\n\nEt viktig poeng å få med seg her er at vi nå oppdaterte Delta Lake Table objektet både i minnet og på disk. La oss bevise det ved å lese inn fra disk:\n\ndeltaTable2 = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\ndeltaTable2.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n+---+\n\n\nOg deretter ved å printe ut det opprinnelige objektet vi leste inn:\n\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#append-data",
    "href": "notebooks/spark/deltalake-intro.out.html#append-data",
    "title": "Introduksjon til Delta Lake",
    "section": "Append data",
    "text": "Append data\nLa oss legge til verdiene 20 og 21 til datasettet. Først lager vi en Spark dataframe:\n\nnew_data = [(20,), (21,)]\nnew_df = spark.createDataFrame(new_data, [\"id\"])\nnew_df.show()\n\n+---+\n| id|\n+---+\n| 20|\n| 21|\n+---+\n\n\nDeretter kan vi appendere det til vår opprinnelige dataframe:\n\nnew_df.write.format(\"delta\").mode(\"append\").save(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp4\"\n)\n\n\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n| 21|\n| 20|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#historien-og-metadata-til-filen",
    "href": "notebooks/spark/deltalake-intro.out.html#historien-og-metadata-til-filen",
    "title": "Introduksjon til Delta Lake",
    "section": "Historien og metadata til filen",
    "text": "Historien og metadata til filen\nNå som vi har gjort noen endringer kan vi se på historien til filen:\n\n# Lister ut filene i bøtta\nfs = FileClient.get_gcs_file_system()\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/temp4/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp4/_delta_log',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000000.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000001.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000002.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-73e5052f-1b82-48da-ab37-2cbc01bb46c1-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-9b3b81a9-2771-4fb4-9f0f-659fd160d643-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-d04d0ca2-8e8b-42e9-a8a3-0fed9a0e4e41-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-0f2f8ba5-3161-41e8-b5d1-2084128a5bed-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-30d707e4-dd9a-4bfd-a4c7-7fbb1933e9ae-c000.snappy.parquet']\n\n\nVi ser av transaksjonsloggen i _delta_log-mappen at det nå har vært 3 transaksjoner på datasettet. vi ser også av navnene på parquet-filene, part-00000 og part-00001, at det finnes to versjoner av filen. Hvis vi ønsker å bli bedre kjent med hva slags informasjon som blir lagret fra transaksjonene, så kan vi printe ut den siste transaksjonen som heter 00000000000000000002.json:\n\nimport json\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\n# Filsti\npath = \"gs://ssb-dapla-felles-data-produkt-prod/temp4/_delta_log/00000000000000000002.json\"\n\nwith fs.open(path, \"r\") as f:\n    for line in f:\n        data = json.loads(line)\n        pretty_content = json.dumps(data, indent=4)\n        print(pretty_content)\n        print(\"-\" * 50)  # Print separator for clarity\n\n{\n    \"commitInfo\": {\n        \"timestamp\": 1696942544879,\n        \"operation\": \"WRITE\",\n        \"operationParameters\": {\n            \"mode\": \"Append\",\n            \"partitionBy\": \"[]\"\n        },\n        \"readVersion\": 1,\n        \"isolationLevel\": \"Serializable\",\n        \"isBlindAppend\": true,\n        \"operationMetrics\": {\n            \"numFiles\": \"2\",\n            \"numOutputRows\": \"2\",\n            \"numOutputBytes\": \"956\"\n        },\n        \"engineInfo\": \"Apache-Spark/3.3.1 Delta-Lake/2.3.0\",\n        \"txnId\": \"a3dcd582-8362-4fc2-a8ce-57613d2eb2b8\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00000-73e5052f-1b82-48da-ab37-2cbc01bb46c1-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 478,\n        \"modificationTime\": 1696942544755,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":20},\\\"maxValues\\\":{\\\"id\\\":20},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00001-30d707e4-dd9a-4bfd-a4c7-7fbb1933e9ae-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 478,\n        \"modificationTime\": 1696942544833,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":21},\\\"maxValues\\\":{\\\"id\\\":21},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n\n\nHer ser vi at vi får masse informasjon om endringen, både metadata om transaksjonen i commitInfo, og informasjon om nye data-filer i add-delen. Vi ser at det er en veldig rik beskrivelse av endringer, men det kan være vanskeig å lese innholdet direkte. La oss heller bruke history()-funksjonen som kommer med delta:\n\nhistory = deltaTable.history()\nhistory.show()\n\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n|      2|2023-10-10 12:55:...|  null|    null|    WRITE|{mode -&gt; Append, ...|null|    null|     null|          1|  Serializable|         true|{numFiles -&gt; 2, n...|        null|Apache-Spark/3.3....|\n|      1|2023-10-10 12:55:...|  null|    null|   UPDATE|{predicate -&gt; (id...|null|    null|     null|          0|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n|      0|2023-10-10 12:55:...|  null|    null|    WRITE|{mode -&gt; Overwrit...|null|    null|     null|       null|  Serializable|        false|{numFiles -&gt; 2, n...|        null|Apache-Spark/3.3....|\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n\n\nSiden det blit trangt i tabellen over så kan vi velge hvilke variabler vi ønsker å se på:\n\n# Oversikt over alle kolonner som finnes i historien\nhistory.columns\n\n['version',\n 'timestamp',\n 'userId',\n 'userName',\n 'operation',\n 'operationParameters',\n 'job',\n 'notebook',\n 'clusterId',\n 'readVersion',\n 'isolationLevel',\n 'isBlindAppend',\n 'operationMetrics',\n 'userMetadata',\n 'engineInfo']\n\n\n\n# Velger de kolonnene jeg er interessert i\nselected_history = deltaTable.history().select(\n    \"version\", \"timestamp\", \"operation\", \"operationParameters\"\n)\n\n\n# Display the selected columns\nselected_history.show(truncate=50)\n\n+-------+-----------------------+---------+--------------------------------------+\n|version|              timestamp|operation|                   operationParameters|\n+-------+-----------------------+---------+--------------------------------------+\n|      2|2023-10-10 12:55:45.014|    WRITE|   {mode -&gt; Append, partitionBy -&gt; []}|\n|      1|2023-10-10 12:55:37.054|   UPDATE|        {predicate -&gt; (id#4452L = 13)}|\n|      0|2023-10-10 12:55:29.048|    WRITE|{mode -&gt; Overwrite, partitionBy -&gt; []}|\n+-------+-----------------------+---------+--------------------------------------+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.out.html#egendefinert-metadata",
    "href": "notebooks/spark/deltalake-intro.out.html#egendefinert-metadata",
    "title": "Introduksjon til Delta Lake",
    "section": "Egendefinert metadata",
    "text": "Egendefinert metadata\nDelta Lake støtter også egendefinert metadata. Det kan for eksempel være nyttig hvis man ønsker å bruke Delta Lake som en backend for et GUI som lar brukeren oppdatere en tabell fra GUI-et. Da ønsker man typisk å lagre hvem som gjorde endringer og når det ble gjort. La oss legge på noe slik metadata:\n\n# Leser inn filen\ndf = spark.read.format(\"delta\").load(\"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\n\n\n# Lagrer egendefinert metadata i en json-fil\nimport json\n\nmetadata = {\n    \"comment\": \"Kontaktet oppgavegiver og kranglet!\",\n    \"manueltEditert\": \"True\",\n    \"maskineltEditert\": \"False\",\n}\nmetadata\n\n{'comment': 'Kontaktet oppgavegiver og kranglet!',\n 'manueltEditert': 'True',\n 'maskineltEditert': 'False'}\n\n\nVi kan deretter appende dette til den siste versjonen av fila.\n\n(\n    df.write.format(\"delta\")\n    .mode(\"append\")\n    .option(\"userMetadata\", json.dumps(metadata))  # Serialize metadata to a string\n    .save(\"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\n)\n\n\n# Laster inn tabellen\ndeltaTable = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-produkt-prod/temp4\")\n\n# Henter ut historien\nhistory_df = deltaTable.history()\n\n\n# Show the operation details, including metadata\nhistory_df.select(\n    \"version\", \"timestamp\", \"operation\", \"operationParameters\", \"userMetadata\"\n).show(truncate=10)\n\n+-------+----------+---------+-------------------+------------+\n|version| timestamp|operation|operationParameters|userMetadata|\n+-------+----------+---------+-------------------+------------+\n|      3|2023-10...|    WRITE|         {mode -...|  {\"comme...|\n|      2|2023-10...|    WRITE|         {mode -...|        null|\n|      1|2023-10...|   UPDATE|         {predic...|        null|\n|      0|2023-10...|    WRITE|         {mode -...|        null|\n+-------+----------+---------+-------------------+------------+\n\n\n\nhistory_df.select(\"version\", \"userMetadata\").show(truncate=50)\n\n+-------+--------------------------------------------------+\n|version|                                      userMetadata|\n+-------+--------------------------------------------------+\n|      3|{\"comment\": \"Kontaktet oppgavegiver og kranglet...|\n|      2|                                              null|\n|      1|                                              null|\n|      0|                                              null|\n+-------+--------------------------------------------------+\n\n\nVi ser at vi la til vår egen metadata i versjon 3 av fila. Vi kan printe ut den rå transaksjonsloggen som tidligere, men nå er vi på transaksjon 3 00000000000000000003.json:\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\n# Filsti\npath = \"gs://ssb-dapla-felles-data-produkt-prod/temp4/_delta_log/00000000000000000003.json\"\n\nwith fs.open(path, \"r\") as f:\n    for line in f:\n        data = json.loads(line)\n        pretty_content = json.dumps(data, indent=4)\n        print(pretty_content)\n        print(\"-\" * 50)  # Print separator for clarity\n\n{\n    \"commitInfo\": {\n        \"timestamp\": 1696942553907,\n        \"operation\": \"WRITE\",\n        \"operationParameters\": {\n            \"mode\": \"Append\",\n            \"partitionBy\": \"[]\"\n        },\n        \"readVersion\": 2,\n        \"isolationLevel\": \"Serializable\",\n        \"isBlindAppend\": false,\n        \"operationMetrics\": {\n            \"numFiles\": \"2\",\n            \"numOutputRows\": \"7\",\n            \"numOutputBytes\": \"989\"\n        },\n        \"userMetadata\": \"{\\\"comment\\\": \\\"Kontaktet oppgavegiver og kranglet!\\\", \\\"manueltEditert\\\": \\\"True\\\", \\\"maskineltEditert\\\": \\\"False\\\"}\",\n        \"engineInfo\": \"Apache-Spark/3.3.1 Delta-Lake/2.3.0\",\n        \"txnId\": \"e7de92bf-b0f9-4341-8bbb-9b382f2f3eb6\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00000-96369f3d-fe4a-4365-a0df-00c813027399-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 503,\n        \"modificationTime\": 1696942553856,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":5,\\\"minValues\\\":{\\\"id\\\":10},\\\"maxValues\\\":{\\\"id\\\":15},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00001-0f1bc8e6-093b-49a9-ad0b-78d5a148cfb6-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 486,\n        \"modificationTime\": 1696942553853,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":2,\\\"minValues\\\":{\\\"id\\\":20},\\\"maxValues\\\":{\\\"id\\\":21},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n\n\nVi er da at metadataene ligger som forventet i metadata-delen."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html",
    "href": "notebooks/spark/pyspark-intro.out.html",
    "title": "Introduksjon til PySpark",
    "section": "",
    "text": "Apache Spark er et sterkt verktøy som utvider mulighetsrommet for databehandling med R og Python. Kjernefunksjonaliteten ligger i at den lar deg kjøre en jobb på flere maskiner samtidig, noe som ikke er mulig med klassiske rammeverk som Pandas og Tidyverse. Følgelig er det et rammeverk som blant annet er veldig egnet for å prosessere store datamengder eller gjøre store beregninger. Les om mer om Apache Spark i Dapla-manualen\nI denne notebooken vises noen enkle eksempler på hvordan du kan jobbe med data med PySpark, et Python-grensesnitt mot Spark."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html#oppsett",
    "href": "notebooks/spark/pyspark-intro.out.html#oppsett",
    "title": "Introduksjon til PySpark",
    "section": "Oppsett",
    "text": "Oppsett\nNår du logger deg inn på Dapla kan du velge mellom 2 ferdigoppsatte kernels for å jobbe med PySpark:\n\nPyspark (local)\nPyspark (k8s cluster)\n\nDen første lar deg bruke Spark på en enkeltmaskin, mens den andre lar deg distribuere kjøringen på mange maskiner avhengig av hvor store jobbene er. I eksemplene under brukes Pyspark (local).\n\n# Importer biblioteker\nimport numpy as np\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import date_format, explode, expr, sequence, sum, avg\nfrom pyspark.sql.types import DateType, DoubleType, StructField, StructType\n\n# Initialierer en SparkSession\nspark = (\n    SparkSession.builder.master(\"local[2]\")\n    .appName(\"Dapla-manual-example\")\n    .getOrCreate()\n)\n\nI koden over importerer vi de bibliotekene vi skal bruke under. Grunnen til at vi importerer pyspark.sql er at dette er at Spark SQL er Apache Spark sin modul for å jobbe med strukturerte data. Og som navnet tilsier vil det si at vi kan blande Python og SQL i koden vår. Vi skal nærmere på hvordan man bruke SQL fra PySpark-notebook senere.\nSpark tilbyr også et eget grensesnitt, Spark UI, for å monitorere hva som skjer under en SparkSession. Vi kan bruke følgende kommando for å få opp en lenke til Spark UI i notebooken vår:\n\nspark.sparkContext\n\n\n        \n            SparkContext\n\n            Spark UI\n\n            \n              Version\n                v3.3.1\n              Master\n                local[*]\n              AppName\n                pyspark-shell\n            \n        \n        \n\n\nKlikker du på Spark UI-lenken så tar den deg til dashboard som lar deg monitorere, debugge, optimalisere og forstå kjøringene dine. Det kan være et svært nyttig verktøy i mange tilfeller."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html#generere-data",
    "href": "notebooks/spark/pyspark-intro.out.html#generere-data",
    "title": "Introduksjon til PySpark",
    "section": "Generere data",
    "text": "Generere data\nVi kan begynne med å generere en Spark DataFrame med en kolonne som inneholder månedlige datoer for perioden 2000M1-2023M8.\n\n# Genererer månedlige data\ndates_df = spark.range(1).select(\n    explode(\n        sequence(\n            start=expr(\"date '2000-01-01'\"),\n            stop=expr(\"date '2023-08-01'\"),\n            step=expr(\"interval 1 month\"),\n        )\n    ).alias(\"Date\")\n)\ndates_df.show(5)\n\n+----------+\n|      Date|\n+----------+\n|2000-01-01|\n|2000-02-01|\n|2000-03-01|\n|2000-04-01|\n|2000-05-01|\n+----------+\nonly showing top 5 rows\n\n\nEn Spark DataFrame er en distribuert samling av data som er organisert inn i kolonner. Siden Spark lar deg distribuere kjøringer på flere maskiner, er DataFrames optimalisert for å kunne splittes opp slik at de kan brukes på flere maskiner. Med andre er dette ikke det samme som en Pandas dataframe mange kjenner fra før.\nOver genererte vi en datokolonne. For å få litt mer data kan vi også generere 100 kolonner med tidsseriedata og så printer vi de 2 første av disse:\n\n# Genererer random walk data\nschema = StructType(\n    [StructField(f\"serie{i:02d}\", DoubleType(), True) for i in range(100)]\n)\n\ndata = [\n    tuple((10 + np.random.normal(0, 1, 100)).cumsum().tolist())\n    for _ in range(284)  # 284 months from 2000-01 to 2023-08\n]\n\ndata_df = spark.createDataFrame(data, schema=schema)\n\ndata_df.select(\"serie00\", \"serie01\").show(5)\n\n+------------------+------------------+\n|           serie00|           serie01|\n+------------------+------------------+\n|10.410703377184355| 21.06318801110079|\n|10.509249410154466|  19.5674295298024|\n| 9.618310122060274|17.635805093465642|\n| 9.691112692298294|18.593842915949082|\n| 9.903675228685067|20.012215769058564|\n+------------------+------------------+\nonly showing top 5 rows\n\n\nTil slutt kan vi joine de to datasettene sammen og lage noen kolonner som viser år, kvartal og måned. Deretter printer vi ut noen av kolonnene med kommandoen show().\n\n# Legger til row index til DataFrame før join med dates_df\ndata_df = data_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n\n# Joiner de to datasettene\ndf = (\n    dates_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n    .join(data_df, \"row_index\")\n    .drop(\"row_index\")\n)\n\n# Legger til år, kvartal og mnd\ndf = df.withColumn(\"Year\", date_format(df.Date, \"yyyy\"))\ndf = df.withColumn(\"Quarter\", expr(\"quarter(Date)\"))\ndf = df.withColumn(\"Month\", date_format(df.Date, \"MM\"))\n\ndf.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows\n\n\nOg med det har vi noe data vi kan jobbe med i resten av notebooken."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html#skrive-til-parquet",
    "href": "notebooks/spark/pyspark-intro.out.html#skrive-til-parquet",
    "title": "Introduksjon til PySpark",
    "section": "Skrive til Parquet",
    "text": "Skrive til Parquet\nPySpark tilbyr mange opsjoner ved skriving til parquet-filer som vi kanskje ikke er vant til å forholde oss til med enklere rammeverk som Pandas. Den enkleste måten å skrive ut en fil er som følger:\ndf.write.parquet(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp/timeseries.parquet\"\n)\nDette vil fungere hvis filen ikke finnes fra før. Hvis den finnes fra før så vil den feile. Grunnen er at vi ikke har spesifisert hva vi ønsker at den skal gjøre. Vi kan velge mellom overwrite, append, ignore eller errorifexists. Sistnevnte er også default-oppførsel hvis du ikke ber den gjøre noe annet.\nUnder bruker vi opsjonen overwrite, det vil si at den skriver over en evt eksisterende fil med samme navn.\n\ndf.write.mode(\"overwrite\").parquet(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp/timeseries.parquet\"\n)\n\nVi kan inspisere hva som ble skrevet ved å liste ut innholder i bøtta.\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-produkt-prod/temp/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp/',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/_SUCCESS',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/part-00000-b32e7299-0590-4b31-bcc2-dc3d58725529-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/part-00001-b32e7299-0590-4b31-bcc2-dc3d58725529-c000.snappy.parquet']\n\n\nHvis denne parquet-filen hadde vært partisjonert etter en kolonne, så ville det vært egne undermapper med navnestruktur column_name=value som indikerte hva filen er partisjonert på. Siden vi her bruker en maskin og har et lite datasett, valgte Spark å ikke partisjonere."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html#lese-inn-parquet",
    "href": "notebooks/spark/pyspark-intro.out.html#lese-inn-parquet",
    "title": "Introduksjon til PySpark",
    "section": "Lese inn Parquet",
    "text": "Lese inn Parquet\nApache Spark kan lese inn flere parquet-filer samtidig. Syntaxen er like enkel som den for å skrive ut.\n\ndf_ts = spark.read.parquet(\n    \"gs://ssb-dapla-felles-data-produkt-prod/temp/timeseries.parquet\"\n)\ndf_ts.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie66\", \"serie55\").show(5)\n\n+----------+----+-------+-----+-----------------+-----------------+\n|      Date|Year|Quarter|Month|          serie66|          serie55|\n+----------+----+-------+-----+-----------------+-----------------+\n|2000-01-01|2000|      1|   01|670.2679830025959|562.4312808525777|\n|2000-02-01|2000|      1|   02|675.4233411662802|562.5168447360121|\n|2000-03-01|2000|      1|   03|687.3412458214908|568.6203957584232|\n|2000-04-01|2000|      2|   04|673.1128047244955|557.4633871253379|\n|2000-05-01|2000|      2|   05| 667.513406101114|561.7766450346327|\n+----------+----+-------+-----+-----------------+-----------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html#pyspark-og-sql",
    "href": "notebooks/spark/pyspark-intro.out.html#pyspark-og-sql",
    "title": "Introduksjon til PySpark",
    "section": "PySpark og SQL",
    "text": "PySpark og SQL\nDu kan også skrive SQL med Spark. For å skrive SQL må vi først lage et temporary view. Under kaller vi viewt for tidsserie.\n\ndf.createOrReplaceTempView(\"tidsserie\")\n\nVi kan deretter skrive en SQL-statement som vi ønsker å kjøre på viewet:\n\nquery = \"SELECT * FROM tidsserie WHERE Year = 2010\"\n\nDeretter kan vi bruke det til å filtrere datasettet:\n\nresult_df = spark.sql(query)\nresult_df.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2010-01-01|2010|      1|   01| 11.26910423907778|21.730128215168268|\n|2010-02-01|2010|      1|   02| 8.722783282690738| 17.46851086792347|\n|2010-03-01|2010|      1|   03|10.376873608348605|20.109386343182802|\n|2010-04-01|2010|      2|   04|11.459959305590926|21.995141825523866|\n|2010-05-01|2010|      2|   05|10.441456792180572| 21.25096473981906|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.out.html#aggregering",
    "href": "notebooks/spark/pyspark-intro.out.html#aggregering",
    "title": "Introduksjon til PySpark",
    "section": "Aggregering",
    "text": "Aggregering\nvi kan aggregere opp enten med PySpark-syntax eller SQL-syntax. Under viser vi begge:\n\nfrom pyspark.sql import functions as F\n\n# Assuming df_ts is your DataFrame\naggregated_df = df_ts.groupBy(\"Quarter\").agg(F.sum(\"serie00\").alias(\"Sum\"),\n                                             F.avg(\"serie00\").alias(\"Average\"),\n                                             F.max(\"serie00\").alias(\"Maximum\"))\n\n# Show the result\naggregated_df.show()\n\n+-------+------------------+------------------+------------------+\n|Quarter|               Sum|           Average|           Maximum|\n+-------+------------------+------------------+------------------+\n|      1|363.95869885234185|10.109963857009497|11.829453550532005|\n|      3|365.68324879453405| 10.15786802207039|12.233378837422391|\n|      4| 342.2334082209804|10.065688477087658|12.210138970053695|\n|      2|  361.991445506568|10.055317930738001|12.276030776082463|\n+-------+------------------+------------------+------------------+\n\n\nLa oss gjøre det samme med SQL, men grupperer etter to variabler og sorterer output etterpå.\n\n# Assuming df_ts is your DataFrame\ndf_ts.createOrReplaceTempView(\"temp_table\")\n\n# Now you can run a SQL query\nquery = \"\"\"\n    SELECT\n        Year,\n        Quarter,\n        SUM(serie00) AS Sum,\n        AVG(serie00) AS Average,\n        MAX(serie00) AS Maximum\n    FROM \n        temp_table\n    GROUP BY \n        Year, Quarter\n    ORDER BY\n        YEAR, QUARTER\n\"\"\"\n\naggregated_df_sql = spark.sql(query)\n\n# Show the result\naggregated_df_sql.show(10)\n\n+----+-------+------------------+------------------+------------------+\n|Year|Quarter|               Sum|           Average|           Maximum|\n+----+-------+------------------+------------------+------------------+\n|2000|      1|31.323050432219453|10.441016810739818|11.118293927071951|\n|2000|      2|28.911192473027377| 9.637064157675793| 9.900977410685329|\n|2000|      3|33.670797229797415|11.223599076599138|12.233378837422391|\n|2000|      4|28.094793356286914| 9.364931118762305| 10.32000478359274|\n|2001|      1|31.636678535169537|10.545559511723178|11.367822302191831|\n|2001|      2|29.629770128521507| 9.876590042840503|11.135215930381191|\n|2001|      3| 30.75408440118315| 10.25136146706105|10.723803326978505|\n|2001|      4|30.361048932627902|  10.1203496442093|10.368365984482093|\n|2002|      1|31.184163218551227|10.394721072850409|10.550579652234951|\n|2002|      2|29.128978392451202|   9.7096594641504|10.186365745367246|\n+----+-------+------------------+------------------+------------------+\nonly showing top 10 rows"
  },
  {
    "objectID": "notebooks/editeringsrammeverk/editering_demo.html",
    "href": "notebooks/editeringsrammeverk/editering_demo.html",
    "title": "Editering av enkel dataframe",
    "section": "",
    "text": "Imports\n\nfrom ssb_dash_framework import main_layout\nfrom ssb_dash_framework import app_setup\nfrom ssb_dash_framework import set_variables\nfrom ssb_dash_framework import EditingTableTab # Tab modulen\nfrom ssb_dash_framework import EditingTableWindow # Vindu modulen\nfrom ssb_dash_framework import apply_edits\n\nimport pandas as pd\nimport numpy as np\n\nimport os\nimport uuid\nfrom datetime import datetime, timezone\n\nfrom fagfunksjoner import next_version_path\n\n\n\nLes inn data\n\nfilsti_data = '/buckets/produkt/editering/editering_test_v1.parquet'\n\n\ndf = pd.read_parquet(\n    filsti_data\n)\n\n\nfilsti_logg = '/buckets/produkt/editering/logg/prosessdata/prosess-editering_test_v1.jsonl'\ntry: os.makedirs('/buckets/produkt/editering/logg/prosessdata') # lag mappe for logg \nexcept: pass\n\n\n\nKlargjøre data\nEditeringen avhenger av at man har en uuid-kolonne i dataframen. Dette er for å forsikre at riktig rad blir endret.\n\nif 'uuid' not in df.columns: df['uuid'] = [uuid.uuid4() for _ in range(len(df))]\n\n\n\nKjør app\n\nApp setup\n\nport = 8070\nservice_prefix = os.getenv(\"JUPYTERHUB_SERVICE_PREFIX\", \"/\")\ndomain = os.getenv(\"JUPYTERHUB_HTTP_REFERER\", None)\ntheme = \"cosmo\" # sjekk ut flere temaer: https://www.dash-bootstrap-components.com/docs/themes/explorer/ \napp = app_setup(port, service_prefix, theme)\napp_timestamp = datetime.now()\n\n\n\nVelg variabler\n\nset_variables([\"orgnr\",\"aar\"]) # Dette gjør at orgnr og aar er tilgjengelig i applikasjonen din.\nstart_verdier = { # Valgfritt å ha med, men kan være praktisk for brukervennlighet. Puttes inn i main_layout() funksjonen.\n    \"orgnr\": \"971526920\", \n    \"aar\": \"2020\"\n}\n\n# Definer get_data funkjson\ndef get_data(orgnr,aar):\n    return df[(df['orgnr'] == orgnr) & (df['aar']==aar)]\n    \nenhetstabell = EditingTableTab(\n    label=\"Enhetstabell\",\n    inputs=[\"orgnr\", \"aar\"], # evt. start_verdier.keys()\n    states=[],\n    get_data_func=get_data,\n    update_table_func=lambda x, *_: x ,\n    output=\"aar\",\n    log_filepath = filsti_logg,\n    justify_edit=False\n    \n)\n\ntab_list = [\n    enhetstabell,\n]\nwindow_list = []\n\napp.layout = main_layout(window_list, tab_list, default_values = start_verdier, )\n\nif __name__ == \"__main__\":\n    app.run(\n        port=port,\n        jupyter_server_url=domain,\n        jupyter_mode=\"tab\",\n    )\n\n\n\n\n\n\nPåføre endringer fra logg\n\nlogg = pd.read_json(filsti_logg, lines = True)\n\nprint(logg)\n\n\ndf_editert = apply_edits(df, filsti_logg, app_timestamp)\n\n\n\nSjekk hva som har blitt endret\n\npd.concat([df,df_editert]).drop_duplicates(keep=False)\n\n\n\nLagre det editerte datasettet\n\nfilsti_data_neste_versjon = next_version_path(filsti_data)\nprint(filsti_data_neste_versjon)\n\n\ndf.to_parquet(filsti_data_neste_versjon)"
  },
  {
    "objectID": "utviklere/lenker.html",
    "href": "utviklere/lenker.html",
    "title": "Lenker",
    "section": "",
    "text": "Dokumentasjon for utviklere i SSB er spredt over mange ulike sider. Denne siden er en oversikt over dokumentasjonen som finnes.\n\n\n\nSlack\nGitHub\nConfluence\nByrånettet\n\n\n\n\n\n\nhttps://backstage.intern.ssb.no\n\nProgramvarekatalog\nTeamkatalog\nTeknologiradar\n\n\n\n\n\nOffisielle docs fra nav: https://docs.ssb.cloud.nais.io/\nIntern system docs: https://statisticsnorway.github.io/nais-system (krever GitHub innlogging)\nNais console: https://console.ssb.cloud.nais.io/ (krever naisdevice)\n\n\n\n\n\nDapla ctrl (teamkatalog): https://dapla-ctrl.intern.ssb.no/\n\n\n\n\n\nhttps://lab.dapla.ssb.no/\nIntern system docs: https://statisticsnorway.github.io/dapla-lab-system (krever GitHub innlogging)\nBrukerdokumentasjon: https://manual.dapla.ssb.no/statistikkere/dapla-lab.html\n\n\n\n\n\nhttps://docs.bip.ssb.no (krever GitHub innlogging)\nhttps://docs.dapla.ssb.no/ (krever GitHub innlogging)\n\n\n\n\n\n\nArkitekturprinsipper\nArchitecture Decision Records (krever GitHub innlogging)\nTeknologiradar (krever naisdevice)\n\n\n\n\nGenerellt er utviklere i SSB veldig fri til å velge sin lokal dev-oppsett utifra preferanser og teknologivalg. Følgende lenker går på SSB spesifikk oppsett, særlig relatert til nettverkstilgang og sikkerhet ellers.\n\nKundeservice FAQ\n\nWiFi\nMicrosoft Authenticator\nVPN\nEpost\nPassordbytte\n\nGit/GitHub\nIntelliJ lisenser\nInstaller naisdevice\n\n\n\n\n\n\n\nDependabot\nSnyk\nDetectify\nSonarcloud\nMabl\n\n\n\n\n\n\nOperasjonell modell\nIT Avdelingens leveranse demo (finner sted en gang i måneden på fredag)",
    "crumbs": [
      "Utviklere",
      "Lenker"
    ]
  },
  {
    "objectID": "utviklere/lenker.html#samhandling",
    "href": "utviklere/lenker.html#samhandling",
    "title": "Lenker",
    "section": "",
    "text": "Slack\nGitHub\nConfluence\nByrånettet",
    "crumbs": [
      "Utviklere",
      "Lenker"
    ]
  },
  {
    "objectID": "utviklere/lenker.html#plattform",
    "href": "utviklere/lenker.html#plattform",
    "title": "Lenker",
    "section": "",
    "text": "https://backstage.intern.ssb.no\n\nProgramvarekatalog\nTeamkatalog\nTeknologiradar\n\n\n\n\n\nOffisielle docs fra nav: https://docs.ssb.cloud.nais.io/\nIntern system docs: https://statisticsnorway.github.io/nais-system (krever GitHub innlogging)\nNais console: https://console.ssb.cloud.nais.io/ (krever naisdevice)\n\n\n\n\n\nDapla ctrl (teamkatalog): https://dapla-ctrl.intern.ssb.no/\n\n\n\n\n\nhttps://lab.dapla.ssb.no/\nIntern system docs: https://statisticsnorway.github.io/dapla-lab-system (krever GitHub innlogging)\nBrukerdokumentasjon: https://manual.dapla.ssb.no/statistikkere/dapla-lab.html\n\n\n\n\n\nhttps://docs.bip.ssb.no (krever GitHub innlogging)\nhttps://docs.dapla.ssb.no/ (krever GitHub innlogging)",
    "crumbs": [
      "Utviklere",
      "Lenker"
    ]
  },
  {
    "objectID": "utviklere/lenker.html#arkitektur",
    "href": "utviklere/lenker.html#arkitektur",
    "title": "Lenker",
    "section": "",
    "text": "Arkitekturprinsipper\nArchitecture Decision Records (krever GitHub innlogging)\nTeknologiradar (krever naisdevice)",
    "crumbs": [
      "Utviklere",
      "Lenker"
    ]
  },
  {
    "objectID": "utviklere/lenker.html#oppsett",
    "href": "utviklere/lenker.html#oppsett",
    "title": "Lenker",
    "section": "",
    "text": "Generellt er utviklere i SSB veldig fri til å velge sin lokal dev-oppsett utifra preferanser og teknologivalg. Følgende lenker går på SSB spesifikk oppsett, særlig relatert til nettverkstilgang og sikkerhet ellers.\n\nKundeservice FAQ\n\nWiFi\nMicrosoft Authenticator\nVPN\nEpost\nPassordbytte\n\nGit/GitHub\nIntelliJ lisenser\nInstaller naisdevice",
    "crumbs": [
      "Utviklere",
      "Lenker"
    ]
  },
  {
    "objectID": "utviklere/lenker.html#test-og-kvalitet",
    "href": "utviklere/lenker.html#test-og-kvalitet",
    "title": "Lenker",
    "section": "",
    "text": "Dependabot\nSnyk\nDetectify\nSonarcloud\nMabl",
    "crumbs": [
      "Utviklere",
      "Lenker"
    ]
  },
  {
    "objectID": "utviklere/lenker.html#prosess",
    "href": "utviklere/lenker.html#prosess",
    "title": "Lenker",
    "section": "",
    "text": "Operasjonell modell\nIT Avdelingens leveranse demo (finner sted en gang i måneden på fredag)",
    "crumbs": [
      "Utviklere",
      "Lenker"
    ]
  }
]