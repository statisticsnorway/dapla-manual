[
  {
    "objectID": "om-dapla.html",
    "href": "om-dapla.html",
    "title": "Om Dapla",
    "section": "",
    "text": "Om Dapla\nDapla stÃ¥r for dataplattform, og er en skybasert lÃ¸sning for statistikkproduksjon og forskning.\n\n\nHvorfor Dapla?\nSom dataplattform skal Dapla stimulerere til Ã¸kt kvalitet pÃ¥ statistikk og forskning, samtidig som den gjÃ¸r organisasjonen mer tilpasningsdyktig i mÃ¸te med fremtiden.\n\n\nDen nye skybaserte dataplattformen (Dapla) skal bli viktig for Ã¥ effektivisere arbeids-og produksjonsprosesser, den skal sikre effektiv lagring og gjenfinning av data og metadata, og stÃ¸tte opp under deling av data pÃ¥ tvers av statistikkomrÃ¥der.\n\nKilde: Langtidsplan for SSB (2022-2024)\n\n\n\nMÃ¥let med Dapla er Ã¥ tilby tjenester og verktÃ¸y som lar statistikkprodusenter og forskere produsere resultater pÃ¥ en sikker og effektiv mÃ¥te."
  },
  {
    "objectID": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html",
    "href": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html",
    "title": "Ideen bak bloggen",
    "section": "",
    "text": "SSB-ere lÃ¸ser hele tiden problemer pÃ¥ nye mÃ¥ter som andre gjerne skulle nyttiggjort seg av. Spesielt nÃ¥r vi gjÃ¸r en sÃ¥ stor overgang i arbeidsform som overgangen til en ny plattform (Dapla), og vi samtidig skifter mange verktÃ¸yene vi har i verktÃ¸ykassen vÃ¥r. Av den grunn har vi opprettet denne bloggen. Her vil vi skrive om hvordan vi lÃ¸ser problemer, hvilke verktÃ¸y vi bruker og hvordan vi bruker dem. Vi vil ogsÃ¥ skrive om hvordan vi jobber med Ã¥ utvikle nye verktÃ¸y og hvordan vi jobber med Ã¥ utvikle Dapla.\nMÃ¥lsetningen med denne bloggen er at alle i SSB som Ã¸nsker Ã¥ dele noe med andre kan skrive en artikkel og dele i bloggen. Mens ByrÃ¥nettet er kanal for Ã¥ dele informasjon med alle i SSB, og Viva Engage en kanal for Ã¥ si det du tenker uten sÃ¦rlig noen formell struktur, er denne bloggen en kanal for Ã¥ dele informasjon med andre som jobber med data og teknologi i SSB.\nFordelen med bloggen er at den er tilpasset hvordan statistikkere, forskere og IT-utviklere jobber til daglig. Artiklene kan skrives samme sted som man utvikler kode, og man inkludere output fra kodekjÃ¸ringer i artikler."
  },
  {
    "objectID": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html#hvordan-skrive-en-artikkel",
    "href": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html#hvordan-skrive-en-artikkel",
    "title": "Ideen bak bloggen",
    "section": "Hvordan skrive en artikkel?",
    "text": "Hvordan skrive en artikkel?\nBloggen er generert med Quarto. Quarto er et rammeverk for Ã¥ skrive artikler i markdown. Det er enkelt Ã¥ komme i gang med Quarto, og det er enkelt Ã¥ skrive artikler i Quarto.\nFor Ã¥ skrive en artikkel gjÃ¸r du fÃ¸lgende:\n\nSkriv artikkelen som en markdown-fil (.qmd-fil) eller en notebook (.ipynb-fil).\nKlon dapla-manual-internal repoet:\ngit clone https://github.com/statisticsnorway/dapla-manual-internal.git\nOpprett en mappe for artikkelen din i mappen ./dapla-manual-internal/blog/posts/. Gi mappen et navn som beskriver artikkelen din.\nInne mappen legger du din .qmd- eller .ipynb-fil. Eventuelle bilder i artikkelen legges ogsÃ¥ i samme mappe.\nOpprette en pull request pÃ¥ repoet og noen vil se over artikkelen din og publisere den."
  },
  {
    "objectID": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html#metadata-om-artikkelen",
    "href": "blog/posts/2023-01-11-hvorfor-en-blogg/index.html#metadata-om-artikkelen",
    "title": "Ideen bak bloggen",
    "section": "Metadata om artikkelen",
    "text": "Metadata om artikkelen\nNÃ¥r du skriver artikkelen sÃ¥ mÃ¥ du starte dokumentet med fÃ¸lgende metadata:\n\n\nindex.qmd\n\n---\ntitle: Ideen bak bloggen\nsubtitle: Hvorfor vi har opprettet denne bloggen? \ncategories:\n  - Quarto\nauthor:\n  - name: Ã˜yvind Bruer-SkarsbÃ¸\n    affiliation: \n      - name: Seksjon for dataplattform (724)\n        email: obr@ssb.no\ndate: \"01/11/2024\"\ndate-modified: \"01/11/2024\"\nimage: ../../../images/dapla-long.png\nimage-alt: \"Bilde av Fame-logoen\"\ndraft: false\n---\n\nHusk Ã¥ fylle ut alle feltene slik at det blir riktig informasjon for din artikkel. Skriver du en ipynb-fil sÃ¥ mÃ¥ metadataene ligge i en celle av typen raw.\nÃ˜nsker du Ã¥ komme fort igang sÃ¥ kan se hvordan denne artikkelen ble skrevet."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html",
    "title": "Fra arkiv til parquet",
    "section": "",
    "text": "I arkivet til SSB ligger data lagret som posisjonerte flatfiler, ogsÃ¥ kalt fastbredde-fil eller fixed width file pÃ¥ engelsk. I Datadok ligger det spesifisert hvordan du leser inn disse filene fra dat eller txt i arkivet til sas7bdat-formatet, men ikke hvordan man konverterer til Parquet-formatet. I denne artikkelen deler jeg hvordan jeg gikk frem for Ã¥ konvertere arkivfiler til Parquet."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#hva-er-en-fastbredde-fil",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#hva-er-en-fastbredde-fil",
    "title": "Fra arkiv til parquet",
    "section": "Hva er en fastbredde-fil?",
    "text": "Hva er en fastbredde-fil?\nEn fastbredde-fil er en fil der hver rad har en fast lengde, og hver kolonne har en fast posisjon. Det er ingen komma eller andre tegn som skiller kolonnene, slik som i en CSV-fil. En fastbredde-fil er en ren tekstfil, dvs. at du kan Ã¥pne den opp i teksteditor og kikke pÃ¥ innholdet direkte.\nUnder er et eksempel hvor samme data er lagret bÃ¥de pÃ¥ CSV-formatet og som fastbredde-fil:\n\n\n\n\ncsv\n\n012345;;Ola Nordmann;\n345678;Kvinne;Kari Nordmann;\n\n\n\n\n\n\nfastbredde-fil\n\n012345      Ola Nordmann \n345678KvinneKari Nordmann\n\n\n\nI csv-filen over til venstre ser vi at hver kolonne er separert med et semikolon, og at hver rad er separert med et linjeskift. I fastbredde-filen til hÃ¸yre ser vi at hver kolonne har en fast lengde, den tomme kjÃ¸nnsvariabelen pÃ¥ rad 1 fylles med spaces, hver rad har dermed den samme lengden i antall tegn. I tillegg er det et ekstra mellomrom etter Ola Nordmann ift. Kari Nordmann. Dette er fordi Ola Nordmann er 12 tegn lang, mens Kari Nordmann er 13 tegn lang."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lese-med-pandas",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lese-med-pandas",
    "title": "Fra arkiv til parquet",
    "section": "Lese med Pandas",
    "text": "Lese med Pandas\nVi kan bruke pandas-funksjonen read_fwf() for Ã¥ lese inn en fastbredde-fil. Denne funksjonen tar inn en filsti, og en liste med bredder for hver kolonne. I tillegg kan vi spesifisere navn pÃ¥ kolonnene, og hvilken datatype kolonnene skal ha og hvordan missing-verdier skal representeres.\nVi er helt avhengig av Ã¥ vite bredden pÃ¥ hver kolonne for Ã¥ kunne lese inn en fastbredde-fil. Dette kan vi finne ut ved Ã¥ Ã¥pne filen i en teksteditor og telle/gjette antall tegn i hver kolonne. Alternativt kan vi bruke innlesingsskriptet for SAS som finnes i Datadok, siden breddene er spesifisert der. Under er et ekspempel pÃ¥ hvordan vi kan lese inn en fastbredde-fil fra forrige avsnitt1:\n\nimport pandas as pd\nfrom io import StringIO  # NÃ¸dvendig siden vi sender en streng, ikke en filsti til .read_fwf\ninstring = \"112345      Ola Nordmann \\n345678KvinneKari Nordmann\\n\"\ndf = pd.read_fwf(StringIO(instring),\n                 names=['pers_id', 'kjonn', 'navn'],  # Navngi kolonner\n                 dtype='object',  # Alle kolonnene settes til \"object\"\n                 na_values=['.', ' .'],  # Hvilke karakterer bruker SAS for tom verdi?\n                 widths=[6, 6, 13])  # Tell/regn ut dissa sjÃ¸l\ndf\n\n\n\n\n\n\n\n\n\npers_id\nkjonn\nnavn\n\n\n\n\n0\n112345\nNaN\nOla Nordmann\n\n\n1\n345678\nKvinne\nKari Nordmann\n\n\n\n\n\n\n\n\nKoden over returnerer en Pandas Dataframe i minnet. Den kan vi lett lagre til Parquet-formatet. Men innlesingen mÃ¥tte vi spesifisere en masse detaljer manuelt. Hvis vi skal lese inn mange filer med ulik struktur, sÃ¥ er ikke denne fremgangsmÃ¥ten skalerbar. Dette er en fremgangsmÃ¥te for Ã¥ lese inn noen fÃ¥ filer."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#datadok",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#datadok",
    "title": "Fra arkiv til parquet",
    "section": "Datadok",
    "text": "Datadok\nSom nevnt over sÃ¥ finnes det et innlesingsskript for SAS i Datadok. Dette skriptet kan vi bruke til Ã¥ lese inn en fastbredde-fil i Python. Vi kan ogsÃ¥ bruke det til Ã¥ finne breddene pÃ¥ hver kolonne. Et slik skript har denne formen:\n\n\ninnlesingsskript.sas\n\nDATA sas_data;\n   INFILE '/ssb/bruker/felles/flatfileksempel_dapla_manual_blogg.dat' MISSOVER PAD LRECL=36;\n   INPUT\n      @1 pers_id 6.0\n      @7 kjonn $CHAR6.0\n      @13 navn $CHAR13.0\n   ;\nRUN;\n\nVi kunne lest av informasjonen her og omsatt innholdet til argumentene read_fwf() trenger. Men fortsatt innebÃ¦rer dette potensielt en del manuelt arbeid."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lese-med-saspy",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lese-med-saspy",
    "title": "Fra arkiv til parquet",
    "section": "Lese med saspy",
    "text": "Lese med saspy\nEn annen tilnÃ¦rming enn Ã¥ bruke .read_fwf fra Pandas er Ã¥ bruke biblioteket saspy. Dette biblioteket lar oss kjÃ¸re SAS-kode fra Python, pÃ¥ SAS-serverene i prodsonen, og fÃ¥ Dataframes tilbake. Vi kan bruke det til Ã¥ kjÃ¸re sas-skript hentet fra Datadok, konvertere til en pandas dataframe, og til slutt skrive til Parquet. I det fÃ¸lgende antar vi at du jobber i Jupyterlab i prodsonen (sl-jupyter-p), og at du har lagret innlesingsskriptet i en variabel, slik som vist under:\n\n\npython\n\nscript = \"\"\"\nDATA sas_data;\n   INFILE '/ssb/bruker/felles/flatfileksempel_dapla_manual_blogg.dat' MISSOVER PAD LRECL=36;\n   INPUT\n      @1 pers_id 6.0\n      @7 kjonn $CHAR6.0\n      @13 navn $CHAR13.0\n   ;\nRUN;\n\"\"\"\n\nLa oss deretter kjÃ¸re fÃ¸lgende kode fra Jupyterlab:\n\n\npython\n\nfrom fagfunksjoner import saspy_session\n\n# Kobler til sas-serverne\nsas = saspy_session()\n\n# Vi bruker tilkoblingen til Ã¥ sende inn Datadok-skriptet\nresult = sas.submit(script)\n\n# Lagre sas-loggen i en variabel\nlog = result[\"LOG\"]\n\n# Ber om Ã¥ fÃ¥ dataframe tilbake\ndf_frasas = sas.sd2df(\"sas_data\", \"work\")\n\n# Lukker koblingen til sas-serverne\nsas._endsas()\n\n# Printer ut datasettet\ndf_frasas\n\nI koden over har vi brukt en pakke som heter ssb-fagfunksjoner for Ã¥ opprette koblingen til sas-serveren. Pakken inneholder et overbygg over saspy, og koden over forutsetter at du har lagret passordet ditt pÃ¥ en spesiell mÃ¥te2.\n\nDatatyper\nVi har nÃ¥ en pandas dataframe med datatyper pÃ¥fÃ¸rt, men disse er basert pÃ¥ den lave mengden datatyper i SAS. Ofte bÃ¸r det ryddes i datatyper fÃ¸r man skriver til Parquet. Spesielt bÃ¸r du tenke pÃ¥ fÃ¸lgende:\n\nCharacter mappes gjerne til object i pandas, ikke den strengere varianten string eller den mer spesifikke string[pyarrow].\nNumeric mappes stort sett til float64 i pandas, vi fÃ¥r som regel ikke heltall direkte Int64 uten videre behandling.\n\nDu kan la Pandas gjÃ¸re ett nytt forsÃ¸k pÃ¥ Ã¥ gjette datatyper ved Ã¥ kjÃ¸re fÃ¸lgende kode:\n\n\npython\n\ndf_pd_dtypes = df_frasas.convert_dtypes()\ndf_pd_dtypes.dtypes\n\nOm du vil teste min selvskrevne funksjon for Ã¥ gjette pÃ¥ datatyper sÃ¥ ligger den i fellesfunksjons-pakken:\n\n\npython\n\nfrom fagfunksjoner import auto_dtype\ndf_auto = auto_dtype(df_frasas)\ndf_auto.dtypes\n\nSjekk gjerne ut parameteret cardinality_threshold pÃ¥ auto_dtype, om du er interessert i Ã¥ automatisk sette categorical dtypes.\n\n\nSkalering\nHvis du har mange arkivfiler, med mange forskjellige innlesingsskript, sÃ¥ kan du lagre alle skriptene i en mappe, og sÃ¥ hente innholdet programmatisk. Her er koden for Ã©n slik â€œhentingâ€.\n\n\npython\n\nsas_script_path = \"/ssb/bruker/felles/flatfileksempel_dapla_manual_blogg.sas\"\nwith open(sas_script_path, \"r\", encoding=\"latin1\") as sas_script:\n    script = sas_script.read().strip()\n    script = \"DATA \" + script.split(\"DATA \")[1] # Forkort ned scriptet til det vi trenger\nprint(script)\n\nHer henter jeg inn et innlesingsskript fra Datadok som jeg har lagret som en tekstfil i en mappe pÃ¥ linux-serveren i prodsonen. Deretter gjÃ¸r jeg den om til et streng-objekt i minnet som kan sendes til saspy-koden som er vist over. Dermed er det bare Ã¥ finne en logikk som gjÃ¸r at du vet hvilket innlesingskript som skal brukes til hvilke arkivfiler (siste valide datadok-script fÃ¸r datafil oppstod feks), og du kan jobbe veldig effektivt med konvertering. NÃ¥r alt er konvertert kan du f.eks. kjÃ¸re et script som validerer datatypene pÃ¥ tvers av alle Ã¥rganger og filnavn."
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lagre-dataframen-til-parquet",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#lagre-dataframen-til-parquet",
    "title": "Fra arkiv til parquet",
    "section": "Lagre dataframen til parquet",
    "text": "Lagre dataframen til parquet\nNÃ¥ er det veldig lett Ã¥ skrive filen til Parquet-formatet.\n\n\npython\n\ndf_auto.to_parquet(\n    \"/ssb/bruker/felles/flatfileksempel_dapla_manual_blogg.parquet\"\n    )"
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#nudb",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#nudb",
    "title": "Fra arkiv til parquet",
    "section": "NUDB",
    "text": "NUDB\nI omleggingen av NUDB (Nasjonal utdanningsdatabase), mÃ¥tte vi konvertere hele arkivet vÃ¥rt pÃ¥ 750+ dat-filer.\nDet var Ã¸nskelig Ã¥ slippe Ã¥ lagre til sas7bdat i mellom, for Ã¥ slippe mye dataduplikasjon og arbeidsprosesser. MÃ¥let vÃ¥rt var pseudonymiserte parquetfiler i sky.\nI stor grad kunne dette arbeidet automatiseres (bortsett fra Ã¥ lagre ut innlastingsscript fra gamle datadok). Funksjonene jeg utviklet for dette, ligger stort sett i denne filen:\ngithub.com/utd-nudb/prodsone/konverter_arkiv/archive.py"
  },
  {
    "objectID": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#footnotes",
    "href": "blog/posts/2023-01-19-fra-arkiv-til-parquet/index.html#footnotes",
    "title": "Fra arkiv til parquet",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\n/n i strengen 112345      Ola Nordmann \\n345678KvinneKari Nordmann\\n betyr linjeskift.â†©ï¸\nHvis du Ã¸nsker kan du bruker ssb-fagfunksjoner til Ã¥ lagre passordet ditt i kryptert form. Da kan du lagre passordet i en fil pÃ¥ din egen maskin, og slipper Ã¥ skrive det inn hver gang du skal koble til SAS. Funksjonen heter fagfunksjoner.prodsone.saspy_ssb.set_password().â†©ï¸"
  },
  {
    "objectID": "utviklere/dapla-team.html",
    "href": "utviklere/dapla-team.html",
    "title": "Dapla-team",
    "section": "",
    "text": "Kommer snart.",
    "crumbs": [
      "Utviklere",
      "Hurtigstart"
    ]
  },
  {
    "objectID": "utviklere/dapla-team.html#fÃ¥-opprettet-et-team-pÃ¥-dapla",
    "href": "utviklere/dapla-team.html#fÃ¥-opprettet-et-team-pÃ¥-dapla",
    "title": "Dapla-team",
    "section": "FÃ¥ opprettet et team pÃ¥ Dapla",
    "text": "FÃ¥ opprettet et team pÃ¥ Dapla\n\n\n\n\n\n\nNote\n\n\n\nstart.dapla.ssb.no er ikke klar for opprettelse av team i ny struktur.\n\n\nOpprettelse av team gjÃ¸res via start.dapla.ssb.no",
    "crumbs": [
      "Utviklere",
      "Hurtigstart"
    ]
  },
  {
    "objectID": "utviklere/appendix.html",
    "href": "utviklere/appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "Appendix\ndksfjkldsjf"
  },
  {
    "objectID": "utviklere/iac.html",
    "href": "utviklere/iac.html",
    "title": "Infrastructure-as-Code (IaC)",
    "section": "",
    "text": "IaC-repo\n\ndapla-example-iac\nâ”‚\nâ”œâ”€â”€ automation\nâ”‚   â”‚\nâ”‚   â””â”€â”€ source_data\nâ”‚       â”‚\nâ”‚       â”œâ”€â”€ dapla-example-test\nâ”‚       â”‚\nâ”‚       â””â”€â”€ dapla-example-prod\nâ”‚\nâ”œâ”€â”€ data\nâ”‚\nâ”œâ”€â”€ docs\nâ”‚\nâ”œâ”€â”€ infra\nâ”‚   â”‚\nâ”‚   â”œâ”€â”€ stack\nâ”‚   â”‚\nâ”‚   â””â”€â”€ projects\nâ”‚       â”‚\nâ”‚       â”œâ”€â”€ dapla-example-test\nâ”‚       â”‚   â”‚\nâ”‚       â”‚   â”œâ”€â”€ project.yaml\nâ”‚       â”‚   â”‚\nâ”‚       â”‚   â”œâ”€â”€ ....tf\nâ”‚       â”‚\nâ”‚       â””â”€â”€ dapla-example-prod\nâ”‚           â”‚\nâ”‚           â”œâ”€â”€ project.yaml\nâ”‚           â”‚\nâ”‚           â”œâ”€â”€ ....tf\nâ”‚\nâ””â”€â”€ atlantis.yaml\n\n\n\n\nMappe for skript som blir brukt av automatiseringstjenesten.\n\n\n\nMappe for Ã¥ ha metadata som teamet selv Ã¸nsker.\n\n\n\n\n\n\nViktig\n\n\n\nData mappen er ikke ment for Ã¥ lagre data til bruk i statistikkproduksjon.\n\n\n\n\n\nMappe for Ã¥ samle dokumentasjon.\n\n\n\nMappe for teamets infrastruktur.\n\n\nDette er en fil som teamet selv kan spesifisere prosjekter og features som er Ã¸nsket at et prosjekt skal ha. NÃ¥r man lager en PR etter Ã¥ lagt inn f.eks et nytt prosjekt, sÃ¥ trigger det en Github Action som genererer Terraform koden som lager prosjektet. project_name feltet skal ikke inneholde miljÃ¸typen eller andre prefiks eller suffikser, disse blir automatisk lagt pÃ¥.\nEksemplet under vil lage 3 prosjekter i Google Cloud som vil fÃ¥ fÃ¸lgende prosjekt navn dapla-example-p, dapla-example-t, og dapla-example-d, prosjekt IDâ€™ene vil vÃ¦re det samme bare med 2 tilfeldige karakterer pÃ¥ slutten.\n\n\nteam_uniform_name: dapla-example\n\nprojects:\n  - project_name: dapla-example\n    env: test\n    features:\n      - dapla-buckets\n\n  - project_name: dapla-example\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service\n  \n  - project_name: dapla-example\n    env: dev\n\n\n\n\n\n\n\n\n\n\n\nName\nDescription\nInteracts with\n\n\n\n\ndapla-buckets\nOppretter â€œstandardâ€ Dapla-bÃ¸ttene (kilde, produkt, delt) samt et kildeprosjekt.\ntransfer-service,disable-default-bucket-iam,disable-default-project-iam\n\n\ntransfer-service\nOppretter bindinger for Transfer Service service kontoer\ndapla-buckets\n\n\nkildomaten\nIkke helt klar ennÃ¥ :)\nnone\n\n\ndisable-default-project-iam\nDeaktiverer standard IAM-bindinger for standard Dapla-prosjekter\ndapla-buckets\n\n\ndisable-default-bucket-iam\nDeaktiverer standard IAM-bindinger for standard Dapla-bÃ¸ttene\ndapla-buckets\n\n\n\n\n\n\n\nHer er det en mappe for hvert prosjekt teamet har i Google Cloud. Hvis man bruker infra/projects.yaml for Ã¥ opprette prosjekter sÃ¥ blir det automatisk opprettet en egen mappe under her som inneholder all Terraform kode for det prosjektet.\n\n\n\nHvis teamet har noen gjenbrukbare moduler som skal brukes i alle prosjekter sÃ¥ kan de opprettes her.\n\n\n\n\nKonfigurasjon av prosjektene Atlantis automatisk kjÃ¸rer atlantis plan for. Hvis et prosjekt er opprettet via infra/projects.yaml sÃ¥ blir det automatisk lagt til i denne filen.\n\n\nversion: 3\nparallel_plan: true\nparallel_apply: true\nprojects:\n- dir: ./infra/projects/dapla-example-prod\n  autoplan:\n    when_modified: [\"*.tf\", \"terraform.tfvars\", \"*.yaml\"]\n- dir: ./infra/projects/dapla-example-test\n  autoplan:\n    when_modified: [\"*.tf\", \"terraform.tfvars\", \"*.yaml\"]\n- dir: ./infra/projects/dapla-example-dev\n  autoplan:\n    when_modified: [\"*.tf\", \"terraform.tfvars\", \"*.yaml\"]",
    "crumbs": [
      "Utviklere",
      "Appendix"
    ]
  },
  {
    "objectID": "utviklere/iac.html#struktur",
    "href": "utviklere/iac.html#struktur",
    "title": "Infrastructure-as-Code (IaC)",
    "section": "",
    "text": "IaC-repo\n\ndapla-example-iac\nâ”‚\nâ”œâ”€â”€ automation\nâ”‚   â”‚\nâ”‚   â””â”€â”€ source_data\nâ”‚       â”‚\nâ”‚       â”œâ”€â”€ dapla-example-test\nâ”‚       â”‚\nâ”‚       â””â”€â”€ dapla-example-prod\nâ”‚\nâ”œâ”€â”€ data\nâ”‚\nâ”œâ”€â”€ docs\nâ”‚\nâ”œâ”€â”€ infra\nâ”‚   â”‚\nâ”‚   â”œâ”€â”€ stack\nâ”‚   â”‚\nâ”‚   â””â”€â”€ projects\nâ”‚       â”‚\nâ”‚       â”œâ”€â”€ dapla-example-test\nâ”‚       â”‚   â”‚\nâ”‚       â”‚   â”œâ”€â”€ project.yaml\nâ”‚       â”‚   â”‚\nâ”‚       â”‚   â”œâ”€â”€ ....tf\nâ”‚       â”‚\nâ”‚       â””â”€â”€ dapla-example-prod\nâ”‚           â”‚\nâ”‚           â”œâ”€â”€ project.yaml\nâ”‚           â”‚\nâ”‚           â”œâ”€â”€ ....tf\nâ”‚\nâ””â”€â”€ atlantis.yaml\n\n\n\n\nMappe for skript som blir brukt av automatiseringstjenesten.\n\n\n\nMappe for Ã¥ ha metadata som teamet selv Ã¸nsker.\n\n\n\n\n\n\nViktig\n\n\n\nData mappen er ikke ment for Ã¥ lagre data til bruk i statistikkproduksjon.\n\n\n\n\n\nMappe for Ã¥ samle dokumentasjon.\n\n\n\nMappe for teamets infrastruktur.\n\n\nDette er en fil som teamet selv kan spesifisere prosjekter og features som er Ã¸nsket at et prosjekt skal ha. NÃ¥r man lager en PR etter Ã¥ lagt inn f.eks et nytt prosjekt, sÃ¥ trigger det en Github Action som genererer Terraform koden som lager prosjektet. project_name feltet skal ikke inneholde miljÃ¸typen eller andre prefiks eller suffikser, disse blir automatisk lagt pÃ¥.\nEksemplet under vil lage 3 prosjekter i Google Cloud som vil fÃ¥ fÃ¸lgende prosjekt navn dapla-example-p, dapla-example-t, og dapla-example-d, prosjekt IDâ€™ene vil vÃ¦re det samme bare med 2 tilfeldige karakterer pÃ¥ slutten.\n\n\nteam_uniform_name: dapla-example\n\nprojects:\n  - project_name: dapla-example\n    env: test\n    features:\n      - dapla-buckets\n\n  - project_name: dapla-example\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service\n  \n  - project_name: dapla-example\n    env: dev\n\n\n\n\n\n\n\n\n\n\n\nName\nDescription\nInteracts with\n\n\n\n\ndapla-buckets\nOppretter â€œstandardâ€ Dapla-bÃ¸ttene (kilde, produkt, delt) samt et kildeprosjekt.\ntransfer-service,disable-default-bucket-iam,disable-default-project-iam\n\n\ntransfer-service\nOppretter bindinger for Transfer Service service kontoer\ndapla-buckets\n\n\nkildomaten\nIkke helt klar ennÃ¥ :)\nnone\n\n\ndisable-default-project-iam\nDeaktiverer standard IAM-bindinger for standard Dapla-prosjekter\ndapla-buckets\n\n\ndisable-default-bucket-iam\nDeaktiverer standard IAM-bindinger for standard Dapla-bÃ¸ttene\ndapla-buckets\n\n\n\n\n\n\n\nHer er det en mappe for hvert prosjekt teamet har i Google Cloud. Hvis man bruker infra/projects.yaml for Ã¥ opprette prosjekter sÃ¥ blir det automatisk opprettet en egen mappe under her som inneholder all Terraform kode for det prosjektet.\n\n\n\nHvis teamet har noen gjenbrukbare moduler som skal brukes i alle prosjekter sÃ¥ kan de opprettes her.\n\n\n\n\nKonfigurasjon av prosjektene Atlantis automatisk kjÃ¸rer atlantis plan for. Hvis et prosjekt er opprettet via infra/projects.yaml sÃ¥ blir det automatisk lagt til i denne filen.\n\n\nversion: 3\nparallel_plan: true\nparallel_apply: true\nprojects:\n- dir: ./infra/projects/dapla-example-prod\n  autoplan:\n    when_modified: [\"*.tf\", \"terraform.tfvars\", \"*.yaml\"]\n- dir: ./infra/projects/dapla-example-test\n  autoplan:\n    when_modified: [\"*.tf\", \"terraform.tfvars\", \"*.yaml\"]\n- dir: ./infra/projects/dapla-example-dev\n  autoplan:\n    when_modified: [\"*.tf\", \"terraform.tfvars\", \"*.yaml\"]",
    "crumbs": [
      "Utviklere",
      "Appendix"
    ]
  },
  {
    "objectID": "utviklere/tldr/overvaakning.html#mÃ¥-eksponere-metrikkadminservice-endepunkter-pÃ¥-egne-porter.",
    "href": "utviklere/tldr/overvaakning.html#mÃ¥-eksponere-metrikkadminservice-endepunkter-pÃ¥-egne-porter.",
    "title": "OvervÃ¥kning",
    "section": "MÃ… eksponere metrikk/admin/service-endepunkter pÃ¥ egne porter.",
    "text": "MÃ… eksponere metrikk/admin/service-endepunkter pÃ¥ egne porter.",
    "crumbs": [
      "Utviklere",
      "TL;DR",
      "OvervÃ¥kning"
    ]
  },
  {
    "objectID": "utviklere/tldr/overvaakning.html#bÃ¸r-strukturere-logger-i-json-format.",
    "href": "utviklere/tldr/overvaakning.html#bÃ¸r-strukturere-logger-i-json-format.",
    "title": "OvervÃ¥kning",
    "section": "BÃ˜R strukturere logger i JSON-format.",
    "text": "BÃ˜R strukturere logger i JSON-format.",
    "crumbs": [
      "Utviklere",
      "TL;DR",
      "OvervÃ¥kning"
    ]
  },
  {
    "objectID": "utviklere/tldr/overvaakning.html#bÃ¸r-skrive-logger-til-stdout.",
    "href": "utviklere/tldr/overvaakning.html#bÃ¸r-skrive-logger-til-stdout.",
    "title": "OvervÃ¥kning",
    "section": "BÃ˜R skrive logger til stdout.",
    "text": "BÃ˜R skrive logger til stdout.",
    "crumbs": [
      "Utviklere",
      "TL;DR",
      "OvervÃ¥kning"
    ]
  },
  {
    "objectID": "utviklere/tldr/versjonskontroll.html#mÃ¥-fÃ¸lge-ssbs-retningslinjer-nÃ¥r-nye-repoer-opprettes.",
    "href": "utviklere/tldr/versjonskontroll.html#mÃ¥-fÃ¸lge-ssbs-retningslinjer-nÃ¥r-nye-repoer-opprettes.",
    "title": "Versjonskontroll",
    "section": "MÃ… fÃ¸lge SSBs retningslinjer nÃ¥r nye repoer opprettes.",
    "text": "MÃ… fÃ¸lge SSBs retningslinjer nÃ¥r nye repoer opprettes.",
    "crumbs": [
      "Utviklere",
      "TL;DR",
      "Versjonskontroll"
    ]
  },
  {
    "objectID": "utviklere/tldr/versjonskontroll.html#mÃ¥-fÃ¸lge-ssbs-retningslinjer-for-Ã¥penkildekode.",
    "href": "utviklere/tldr/versjonskontroll.html#mÃ¥-fÃ¸lge-ssbs-retningslinjer-for-Ã¥penkildekode.",
    "title": "Versjonskontroll",
    "section": "MÃ… fÃ¸lge SSBs retningslinjer for Ã¥penkildekode.",
    "text": "MÃ… fÃ¸lge SSBs retningslinjer for Ã¥penkildekode.",
    "crumbs": [
      "Utviklere",
      "TL;DR",
      "Versjonskontroll"
    ]
  },
  {
    "objectID": "utviklere/tldr/versjonskontroll.html#mÃ¥-anvende-feature-branch-mÃ¸nster.",
    "href": "utviklere/tldr/versjonskontroll.html#mÃ¥-anvende-feature-branch-mÃ¸nster.",
    "title": "Versjonskontroll",
    "section": "MÃ… anvende Feature Branch mÃ¸nster.",
    "text": "MÃ… anvende Feature Branch mÃ¸nster.",
    "crumbs": [
      "Utviklere",
      "TL;DR",
      "Versjonskontroll"
    ]
  },
  {
    "objectID": "utviklere/tldr/versjonskontroll.html#mÃ¥-unngÃ¥-Ã¥-legge-hemmeligheter-under-versjonskontroll.",
    "href": "utviklere/tldr/versjonskontroll.html#mÃ¥-unngÃ¥-Ã¥-legge-hemmeligheter-under-versjonskontroll.",
    "title": "Versjonskontroll",
    "section": "MÃ… unngÃ¥ Ã¥ legge hemmeligheter under versjonskontroll.",
    "text": "MÃ… unngÃ¥ Ã¥ legge hemmeligheter under versjonskontroll.",
    "crumbs": [
      "Utviklere",
      "TL;DR",
      "Versjonskontroll"
    ]
  },
  {
    "objectID": "statistikkere/dapla-team.html",
    "href": "statistikkere/dapla-team.html",
    "title": "Dapla Team",
    "section": "",
    "text": "Dapla Team\nI forrige del beskrev vi noen grunnleggende ting rundt Dapla. I denne delen tar vi for oss hvordan du kan begynne Ã¥ jobbe med skarpe data pÃ¥ plattformen.\nKapittelet som beskriver hvordan man logger seg inn pÃ¥ Dapla vil fungere uten at du mÃ¥ gjÃ¸re noen forberedelser. Er man koblet pÃ¥ SSB sitt nettverk sÃ¥ vil alle SSB-ansatte kunne gÃ¥ inn pÃ¥ plattformen og kode i Python og R. Men du fÃ¥r ikke tilgang til SSBs omrÃ¥de for datalagring pÃ¥ plattformen. I praksis vil det si at man kan generere data med kode, men man kan ikke jobbe med skarpe data.\nFor Ã¥ fÃ¥ muligheten til Ã¥ jobbe med skarpe data MÃ… du fÃ¸rst opprette et dapla-team. Dette er det fÃ¸rste naturlige steget Ã¥ ta nÃ¥r man skal begynne Ã¥ jobbe med statistikkproduksjon pÃ¥ Dapla. I dette kapittelet vil vi forklare det du trenger Ã¥ vite om det Ã¥ opprette og jobbe innenfor et team."
  },
  {
    "objectID": "statistikkere/hurtigstart.html",
    "href": "statistikkere/hurtigstart.html",
    "title": "Hurtigstart",
    "section": "",
    "text": "Hurtigstart",
    "crumbs": [
      "Statistikere",
      "Hurtigstart"
    ]
  },
  {
    "objectID": "statistikkere/ordforklaringer.html",
    "href": "statistikkere/ordforklaringer.html",
    "title": "Ordforklaringer",
    "section": "",
    "text": "Ordforklaringer\n\nbip\nbip er det tidligere navnet pÃ¥ den underliggende plattformen som SSB bygger i GCP, hovedsakelig ment for utviklere som bygger tjenester pÃ¥ Dapla. Plattformen skulle vÃ¦re selvbetjent for utviklere og basert pÃ¥ DevOps-prinsipper. bip eksisterer fortsatt, men er nÃ¥ blitt en del av det stÃ¸rre begrepet dapla.\n\n\nbucket\nbucket (eller bÃ¸tte pÃ¥ norsk) er en lagringsenhet pÃ¥ Dapla. Det ligner litt pÃ¥ en klassisk diskstasjon, for eksempel X-disken eller C-disken pÃ¥ en lokal maskin. I en bÃ¸tte kan det ligge undermapper slik som i et klassisk filsystem.\n\n\nconsumer\nconsumer er en AD-gruppe som gir tilgang til et Dapla-team sin delt-bÃ¸tte. En SSB-ansatt som skal bruke data fra et Dapla-team mÃ¥ vÃ¦re medlem av consumer-gruppen til det aktuelle Dapla-teamet.\n\n\ndapla\nDapla er et akronym for den nye dataplattformen til SSB, der Da stÃ¥r for Data og pla stÃ¥r for Plattform. Dapla er en plattform for lagring, prosessering og deling av SSB sine data. Den bestÃ¥r bÃ¥de av Jupyter-miljÃ¸et, som er et verktÃ¸y for Ã¥ utfÃ¸re beregninger og analysere data, og et eget omrÃ¥de for lagre data. I tillegg inkluderer begrepet Dapla ogsÃ¥ en rekke andre verktÃ¸y som er nÃ¸dvendige for Ã¥ kunne bruke plattformen.\n\n\ndapla-team\nKommer snart.\n\n\ndapla-toolbelt\nKommer snart.\n\n\ndata-admin\ndata-admin er en AD-gruppe som gir de videste tilgangene i et dapla-team. En SSB-ansatt som har data-admin-rollen i et Dapla-team har tilgang til alle bÃ¸tter for det teamet, inkludert kilde-bÃ¸tta som kan inneha sensitive data.\nKommer snart.\n\n\ndapla-start\n*dapla-start** er et brukergrensesnitt der SSB-ansatte kan sÃ¸ke om Ã¥ fÃ¥ opprettet et nytt dapla-team.\n\n\ndelt-bÃ¸tte\nKommer snart.\n\n\ndeveloper\nKommer snart.\n\n\nPersonidentifiserende Informasjon (PII)\nPII er variabler som kan identifisere en person i et datasett.\nMer informasjon finnes hos Datatilsynet.\n\n\ngoogle cloud platform (gcp)\nAllmenn skyplattform utviklet og levert av Google. Konkurrent med Amazon Web Services (AWS) og Microsoft Azure. Dapla primÃ¦rt benytter seg av tjenester pÃ¥ GCP.\nVideo som forklarer hva GCP er.\n\n\ngcp\nForkortelse for Google Cloud Platform. Se forklaring under google cloud platform (GCP).\n\n\nInfrastructure as Code (IaC)\nInfrastuktur som kode pÃ¥ norsk. Kode som defineres ressurser, typisk pÃ¥ en allmenn skyplatform som GCP. Eksempler av ressurser er bÃ¸tter, databaser, virtuelle maskiner, nettverk og sikkerhetsregler.\n\n\nkilde-bÃ¸tte\nKommer snart.\n\n\nprodukt-bÃ¸tte\nKommer snart.\n\n\nPull Request (PR)\nEn PR er en Github konsept, som gir et forum for kodegjennomgang, diskusjon og ikke minst dokumentasjon av kodeendringer.\nDette er anbefalt av KVAKK som mÃ¥ten Ã¥ endre kode pÃ¥ i SSB.\n\n\nssb-project\nKommer snart.\n\n\ntransfer service\nKommer snart.\n\n\nPyflakes\nPyflakes er et enkelt kodeanalyseverktÃ¸y som finner feil i Python kode. Les mer om Pyflakes pÃ¥ deres PyPi side"
  },
  {
    "objectID": "statistikkere/index.html",
    "href": "statistikkere/index.html",
    "title": "Velkommen",
    "section": "",
    "text": "Denne delen tar sikte pÃ¥ Ã¥ gi SSB-ansatte mulighet til Ã¥ ta i bruk grunnleggende funksjonalitet pÃ¥ DAPLA uten hjelp fra eksperter. Boken er bygget opp som den reisen vi mener en statistikker skal gjennom nÃ¥r de flytter sin produksjon fra bakke til sky1. FÃ¸rste del inneholder en del grunnleggende kunnskap som vi mener er viktig Ã¥ ha fÃ¸r man skal starte Ã¥ jobbe i skyen. Andre del forklarer hvordan man sÃ¸ker om Ã¥ opprette et Dapla-team, en forutsetning for Ã¥ drive databehandling pÃ¥ plattformen. Det vil ofte vÃ¦re fÃ¸rste steget i ta i bruk plattformen, siden det er slik man fÃ¥r et sted Ã¥ lagre data. Her forklarer vi hvilke tjenester som inkluderes i et statistikkteam og hvordan man bruker og administerer dem. Den tredje delen tar utgangspunkt i at man skal starte Ã¥ kode opp sin statistikkproduksjon eller kjÃ¸re eksisterende kode. ssb-project er et verktÃ¸y som er utviklet i SSB for Ã¥ gjÃ¸re denne prosessen sÃ¥ enkel som mulig. Da kan brukerne implementere det som anses som god praksis i SSB med noen fÃ¥ tastetrykk, samtidig som vi ogsÃ¥ forklarer mer detaljert hva som skjer under panseret.\nDet er tilrettelagt for en treningsarena i bakkemiljÃ¸et. Dette miljÃ¸et er nesten identisk med det som mÃ¸ter deg pÃ¥ Dapla, med unntak av at du her har tilgang til mange av de gamle systemene og mye mindre hestekrefter i maskinene. Ideen er at SSB-ere ofte vil Ã¸nske Ã¥ lÃ¦re seg de nye verktÃ¸yene2 i kjente og kjÃ¦re omgivelser fÃ¸rst, og deretter flytte et ferdig skrevet produksjonslÃ¸p til Dapla. Del 4 av denne boken beskriver mer utfyllende hvordan dette miljÃ¸et skiller seg fra Dapla, og hvordan man gjÃ¸r en del vanlige operasjoner mot de gamle bakkesystemene.\nSiste delen av boken kaller vi Avansert og tar for seg ulike emner som mer avanserte brukere typisk trenger informasjon om. Her finner man blant annet informasjon om hvilke databaser man kan bruke og hvilke formÃ¥l de er egnet for. Her beskrives ogsÃ¥ hvordan man kan bruke andre IDE-er enn Jupyterlab hvis man Ã¸nsker det. Tjenester for schedulerte kjÃ¸ringer av Notebooks blir ogsÃ¥ diskutert.\nForhÃ¥pentligvis senker denne boken terskelen for Ã¥ ta i bruk Dapla. Kommentarer og Ã¸nsker vedrÃ¸rende boken tas imot med Ã¥pne armer.\nGod fornÃ¸yelseğŸ˜",
    "crumbs": [
      "Statistikere",
      "Velkommen"
    ]
  },
  {
    "objectID": "statistikkere/index.html#footnotes",
    "href": "statistikkere/index.html#footnotes",
    "title": "Velkommen",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI denne boken omtaler vi den gamle produksjonssonen, ofte kalt prodsonen, som bakke, og det nye skymiljÃ¸et Google Cloud som sky. Det er ikke helt presist men duger for formÃ¥lene i denne boken.â†©ï¸\nDet som omtales som nye verktÃ¸y er vil som regel bety R, Python, Git, GitHub og Jupyterlab.â†©ï¸",
    "crumbs": [
      "Statistikere",
      "Velkommen"
    ]
  },
  {
    "objectID": "statistikkere/github-app-integrasjon.html",
    "href": "statistikkere/github-app-integrasjon.html",
    "title": "Koble prosjektet til Github",
    "section": "",
    "text": "For at automatiseringslÃ¸sningen pÃ¥ Dapla skal kunne settes opp automatisk mÃ¥ denne ha tilgang til Ã¥ lese fra prosjektets IAC-repo1. Dette avsnittet vil beskrive denne prosessen. Merk at dette er en engangsjobb som mÃ¥ gjÃ¸res av prosjektets kildedataansvarlige.\n\n\n\n\n\n\nViktig: Prosjektets kildedataansvarlige ogsÃ¥ mÃ¥ ha administrator-rettigheter til IAC-repoet i Github.\n\n\n\n\nLogg inn pÃ¥ Google Cloud Console og velg det prosjektet som skal konfigureres Ã¸verst venstre hjÃ¸rte. SÃ¸k opp Cloud Build i sÃ¸kefeltet og trykk pÃ¥ det valget som kommer opp.\nDet skal nÃ¥ vÃ¦re en venstremeny tilgjengelig med tittel Cloud Build. Trykk pÃ¥ menyvalget som heter Triggers (FigurÂ 1)\n\n\n\n\n\n\n\nFigurÂ 1: Bilde av venstremeny\n\n\n\n\nI nedtrekkslisten Region sÃ¸rg for at europe-north1 er valgt (FigurÂ 2)\n\n\n\n\n\n\n\nFigurÂ 2: Velg korrekt region\n\n\n\n\nTrykk deretter pÃ¥ en link som heter CONNECT REPOSITORY ca. midt pÃ¥ siden.\n\n\n\n\n\n\n\nFigurÂ 3: Oversikt over triggers\n\n\n\n\nNÃ¥ vil det dukke opp et vindu pÃ¥ hÃ¸yre side med overskrift Connect repository (FigurÂ 4). Velg GitHub (Cloud Build GitHub App) og trykk pÃ¥ CONTINUE\n\n\n\n\n\n\n\nFigurÂ 4: Vindu for Ã¥ velge Cloud Build Github App\n\n\n\n\nEt pop-up vindu tilsvarende FigurÂ 5 vil komme opp. Trykk pÃ¥ Authorize. Vinduet vil etter hvert lukke seg og man kommer videre til et steg som heter Select repository (FigurÂ 6)\n\n\n\n\n\n\n\nFigurÂ 5: Pop-up vindu for Github\n\n\n\n\n\n\n\n\n\nFigurÂ 6: Valg av Github repository\n\n\n\n\nTrykk pÃ¥ nedtrekkslisten Repository og skriv inn teamets navn. Huk av boksen ved teamets IAC-repo og trykk OK.\n\n\n\n\n\n\n\nFigurÂ 7: Gi Google Build tilgang til Github repository\n\n\n\n\nKryss sÃ¥ av i sjekkboksen som i (FigurÂ 8) og trykk CONNECT.\n\n\n\n\n\n\n\nFigurÂ 8: Bekreft nytt Github repository\n\n\n\n\nTil slutt vil skjermbildet se ut som vist i FigurÂ 9. Det siste steget Create a trigger kan du hoppe over. Dette vil bli satt opp av automatiseringslÃ¸sningen senere. Trykk pÃ¥ knappen DONE\n\n\n\n\n\n\n\nFigurÂ 9: Siste steg - Create a trigger"
  },
  {
    "objectID": "statistikkere/github-app-integrasjon.html#footnotes",
    "href": "statistikkere/github-app-integrasjon.html#footnotes",
    "title": "Koble prosjektet til Github",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nIAC-repo er et en kodebase i Github pÃ¥ formen https://github.com/statisticsnorway/team-navn-iac.â†©ï¸"
  },
  {
    "objectID": "statistikkere/datatilstander.html",
    "href": "statistikkere/datatilstander.html",
    "title": "Datatilstander",
    "section": "",
    "text": "En datatilstand er et resultat av at et datasett har gÃ¥tt gjennom gitte operasjoner og prosesser (Standardutvalget 2023, 5). Denne siden er ment som en kort innfÃ¸ring i de forskjellige datatilstandene. Siden er basert pÃ¥ det interne dokumentet Datatilstander SSB - 2. utgave. Definisjonene er direkte utdrag fra dette dokumentet. Se interndokumentet for en mer grundig gjennomgang av datatilstander i SSB.\nI SSB skiller vi mellom fem datatilstander:\nAlle datatilstander er obligatoriske bortsett fra inndata. FigurÂ 1 viser hvordan de forskjellige datatilstandene henger sammen.",
    "crumbs": [
      "Statistikere",
      "Standarder"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#kildedata",
    "href": "statistikkere/datatilstander.html#kildedata",
    "title": "Datatilstander",
    "section": "Kildedata",
    "text": "Kildedata\nKildedata er data lagret slik de ble levert til SSB fra dataeier. Eksempler pÃ¥ kildedata er: grunndata, transaksjonsdata, administrative data, statistiske data og aggregerte data og rapporter (Standardutvalget 2023, 7). Kildedata lagres i bÃ¸tten ssb-&lt;teamnavn&gt;-data-kilde-prod. Les mer om bÃ¸tter her og lagringsstandarder her.",
    "crumbs": [
      "Statistikere",
      "Standarder"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#inndata",
    "href": "statistikkere/datatilstander.html#inndata",
    "title": "Datatilstander",
    "section": "Inndata",
    "text": "Inndata\nInndata er kildedata som er transformert til SSBs standard lagringsformat (Standardutvalget 2023, 8). Denne transformeringer inkluderer blant annet at dataene skal benytte UTF-8 tegnsett. Les mer om SSBs standard lagringsformat her. Inndata kan ogsÃ¥ vÃ¦re andre statistikkers glargjorte data og/eller statistikkdata (Standardutvalget 2023, 8). Inndata er ikke en obligatorisk datatilstand. Inndata lagres i bÃ¸tten ssb-&lt;teamnavn&gt;-data-produkt-prod.",
    "crumbs": [
      "Statistikere",
      "Standarder"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#klargjorte-data",
    "href": "statistikkere/datatilstander.html#klargjorte-data",
    "title": "Datatilstander",
    "section": "Klargjorte data",
    "text": "Klargjorte data\nKlargjorte data er inndata hvor:\n\nvariablene er beregnet gjennom utregninger og koblinger mellom datasett\nnÃ¸yaktigheten er forbedret\n\nfor eksempel som resultat av editering eller imputering\n\nmetadata med variabeldefinisjoner er lagt til.\n\nEnhver endring som er gjort skal vÃ¦re sporbare og dokumentert slik at statistikkene skal vÃ¦re etterprÃ¸vbare. Klargjorte date er som regel ikke aggregerte - med mindre dataen vi mottar er aggregert. Med andre ord inneholder klargjorte data oftest enkeltobservasjoner - i likhet med kildedata og inndata (Standardutvalget 2023, 9). Klargjorte data lagres i bÃ¸tten ssb-&lt;teamnavn&gt;-data-produkt-prod.",
    "crumbs": [
      "Statistikere",
      "Standarder"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#statistikk",
    "href": "statistikkere/datatilstander.html#statistikk",
    "title": "Datatilstander",
    "section": "Statistikk",
    "text": "Statistikk\nStatistikk er â€œTallfestede opplysninger om en gruppe eller et fenomen, og som kommer frem ved en sammenstilling og bearbeidelse av opplysninger om de enkelte enhetene i gruppen eller et utvalg av disse enhetene, eller ved systematisk observasjon av fenomenetâ€ ifÃ¸lge statistikkloven Â§ 3a (Standardutvalget 2023, 10). Statistikk lagres i bÃ¸tten ssb-&lt;teamnavn&gt;-data-produkt-prod.\nStatistikk er ofte aggregerte data eller estimerte stÃ¸rrelser. Vi skiller mellom ujustert statistikk og justert statistikk. Indekser og sesongjusterte tall er eksempler pÃ¥ justert statistikk (Standardutvalget 2023, 10).\nStatistikk kan vÃ¦re inndata til andre statistikker, og kan dermed inneholde konfidensielle og detaljerte data som ikke publiseres.",
    "crumbs": [
      "Statistikere",
      "Standarder"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#utdata",
    "href": "statistikkere/datatilstander.html#utdata",
    "title": "Datatilstander",
    "section": "Utdata",
    "text": "Utdata\nUtdata er statistikk der kravene til konfidensialtet er ivaretatt. Dette er datatilstanden som publiseres. Eksempler inkluderer: statistikkbanktabeller, tabelloppdrag og internasjonal rapportering (Standardutvalget 2023, 11). Utdata lagres i bÃ¸tten ssb-&lt;teamnavn&gt;-data-produkt-produkt.",
    "crumbs": [
      "Statistikere",
      "Standarder"
    ]
  },
  {
    "objectID": "statistikkere/datatilstander.html#metadata-for-datatilstandene",
    "href": "statistikkere/datatilstander.html#metadata-for-datatilstandene",
    "title": "Datatilstander",
    "section": "Metadata for datatilstandene",
    "text": "Metadata for datatilstandene\nDet er forskjellige forventinger til metadata for de ulike datatilstandene. Forskjellene er skildret underdisse punktene:\n\nKildedata\n\nInformasjon pÃ¥ datasettnivÃ¥ som dataeier, omrÃ¥det dataene omhandler og tidsinformasjon\nMetadata om enkeltvariabler er begrenset til informasjonen dataeier selv avleverer.\n\n\n\nInndata\n\nI utgangspunktet samme som kildedata\n\n\n\nKlargjorte data\n\nVariabeldefinisjoner - beskrivelse av hver enkelt variabel og hvordan den er beregnet\nNÃ¸yaktighetsforbedrende tiltak som er utfÃ¸rt\n\n\n\nStatistikk\n\nVariabeldefinisjoner\nHvilke metoder og programmer/kode som er benyttet for Ã¥ produsere statistikken\n\n\n\nUtdata\n\nI utgangspunktet samme som for statistikk",
    "crumbs": [
      "Statistikere",
      "Standarder"
    ]
  },
  {
    "objectID": "statistikkere/features.html",
    "href": "statistikkere/features.html",
    "title": "Features",
    "section": "",
    "text": "Under arbeid\n\n\n\nFeatures mÃ¥ forelÃ¸pig skrus pÃ¥ av plattformteamene. Ta kontakt med Kundeservice hvis du Ã¸nsker Ã¥ fÃ¥ en feature skrudd pÃ¥.\n\n\nEn feature er en GCP-tjeneste som som er satt opp og konfigurert slik at Dapla-team kan ta det i bruk pÃ¥ en enkel og selvbetjent mÃ¥te. NÃ¥r man tar i bruk en feature kan man vÃ¦re sikker pÃ¥ at sikkerhet og beste-praksis i SSB er ivaretatt. Et viktig poeng med features er at teamene selv skal kunne skru av og pÃ¥ features etter behov.\nForelÃ¸pig er det tilgjengeliggjort fÃ¸lgende features pÃ¥ Dapla:\n\ndapla-buckets\ndapla-buckets er en feature som gir deg Google Cloud Storage bÃ¸ttene som statistikkteam skal bruke for Ã¥ lagre data i Dapla. Dvs. en bÃ¸tte for kildedata, en bÃ¸tte for produkt-data, og en bÃ¸tte for delt data.\nkildomaten kildomaten er en feature som gir deg tilgang til Kildomaten. Den lar deg automatisere prosessering av data fra kildedata til inndata ved hjelp av Cloud Run.\ntransfer-service\ntransfer-service er en feature som gir deg tilgang til Ã¥ overfÃ¸re data mellom lagringstjenester i Dapla. Den lar deg overfÃ¸re data mellom bÃ¸tter, og mellom bakke- og skyplattformen i SSB. Den er bygget pÃ¥ GCP-tjenesten Google Transfer Service.\n\n\n\n\n\n\n\n\n\nSkru pÃ¥ en feature om gangen.\n\n\n\nHvis du Ã¸nsker Ã¥ skru pÃ¥ flere features samtidig, sÃ¥ mÃ¥ du gjÃ¸re det i flere PR-er. Atlantis vil ikke klare Ã¥ hÃ¥ndtere flere features i samme PR. FÃ¸lg oppskriften under for hver feature du Ã¸nsker Ã¥ skru pÃ¥.\n\n\nAktivering av en feature for Dapla-teamet ditt er veldig enkelt. Endringen er sÃ¥ liten at vi anbefaler Ã¥ gjÃ¸re endringen direkte i GitHubs grensesnitt, uten Ã¥ klone repoet fÃ¸rst. Slik gÃ¥r du frem:\n\nSÃ¸k opp ditt teams sitt IaC-repoet under https://github.com/statisticsnorway/.\n\n\n\n\nI repoet Ã¥pner du fila ./infra/projects.yaml. Se eksempel for teamet play-obr til hÃ¸yre.\nFinn ut om du Ã¸nsker Ã¥ skru pÃ¥ en feature i test eller prod. Hvis du Ã¸nsker Ã¥ gjÃ¸re det i prod, sÃ¥ skal du legge til en linje under features der env: prod. Hvis du Ã¸nsker Ã¥ gjÃ¸re det i test, sÃ¥ skal du legge til en linje under features der env: test.\nDu kan legge til linjen ved Ã¥ trykke pÃ¥ -ikonet Ã¸verst til hÃ¸yre i fila, endre teksten, og trykke pÃ¥ Commit changes. Velg deretter hvilket navn du Ã¸nsker pÃ¥ branchen, og trykk Create pull request.\nI PR-en venter du og ser at atlantis/plan1 har blitt kjÃ¸rt og fÃ¥r en  til venstre for hver kjÃ¸ring, slik som vist i FigurÂ 1.\n\n\n\n\n./infra/projects.yaml\n\nteam_uniform_name: play-obr\n\nprojects:\n  - project_name: play-obr\n    env: test\n    features:\n      - dapla-buckets\n      - kildomaten\n\n  - project_name: play-obr\n    env: prod\n    features:\n      - dapla-buckets\n\n\n\n\n\n\n\n\n\nFigurÂ 1: Atlantis plan planlegger endringen og sjekker om den er klar til gjennomfÃ¸ring.\n\n\n\n\nHvis alt er i orden sÃ¥ ber du en kollega om Ã¥ se over endringen og godkjenne hvis alt ser riktig ut. NÃ¥r den er godkjent vil du se et bilde som ligner det du ser i FigurÂ 2.\n\n\n\n\n\n\n\nFigurÂ 2: Sjekker av PR-er i teamets IaC-repo.\n\n\n\n\nNÃ¥r PR er godkjent av en kollega, og atlantis plan ikke viser noen feilmeldinger, sÃ¥ kan du effektuere endringene ved Ã¥ atlantis apply i kommentarfeltet til PR-en. Kommentareren vil trigge en kjÃ¸ring som effektuerer alle endringer pÃ¥ plattformen.\nEtter at atlantis apply er kjÃ¸rt, sÃ¥ kan du merge inn branchen til main.\nTil slutt kan du slette branchen.\n\nNÃ¥r dette er gjort sÃ¥ endringen effektuert pÃ¥ Dapla og klar til bruk. For eksempel, hvis vi hadde aktivert transfer-service i prod for projects.yaml over, sÃ¥ ville den etter endringen sett slik ut:\n\n\n./infra/projects.yaml\n\nteam_uniform_name: play-obr\n\nprojects:\n  - project_name: play-obr\n    env: test\n    features:\n      - dapla-buckets\n      - kildomaten\n\n  - project_name: play-obr\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service\n\n\n\n\nFor Ã¥ deaktivere en feature som ikke lenger i bruk, sÃ¥ fÃ¸lger du bare beskrivelsen for Aktivere tjeneste over, med det unntak at du nÃ¥ fjerner en linje istedenfor Ã¥ legge til.",
    "crumbs": [
      "Statistikere",
      "Dapla-team",
      "Features"
    ]
  },
  {
    "objectID": "statistikkere/features.html#aktivere-feature",
    "href": "statistikkere/features.html#aktivere-feature",
    "title": "Features",
    "section": "",
    "text": "Skru pÃ¥ en feature om gangen.\n\n\n\nHvis du Ã¸nsker Ã¥ skru pÃ¥ flere features samtidig, sÃ¥ mÃ¥ du gjÃ¸re det i flere PR-er. Atlantis vil ikke klare Ã¥ hÃ¥ndtere flere features i samme PR. FÃ¸lg oppskriften under for hver feature du Ã¸nsker Ã¥ skru pÃ¥.\n\n\nAktivering av en feature for Dapla-teamet ditt er veldig enkelt. Endringen er sÃ¥ liten at vi anbefaler Ã¥ gjÃ¸re endringen direkte i GitHubs grensesnitt, uten Ã¥ klone repoet fÃ¸rst. Slik gÃ¥r du frem:\n\nSÃ¸k opp ditt teams sitt IaC-repoet under https://github.com/statisticsnorway/.\n\n\n\n\nI repoet Ã¥pner du fila ./infra/projects.yaml. Se eksempel for teamet play-obr til hÃ¸yre.\nFinn ut om du Ã¸nsker Ã¥ skru pÃ¥ en feature i test eller prod. Hvis du Ã¸nsker Ã¥ gjÃ¸re det i prod, sÃ¥ skal du legge til en linje under features der env: prod. Hvis du Ã¸nsker Ã¥ gjÃ¸re det i test, sÃ¥ skal du legge til en linje under features der env: test.\nDu kan legge til linjen ved Ã¥ trykke pÃ¥ -ikonet Ã¸verst til hÃ¸yre i fila, endre teksten, og trykke pÃ¥ Commit changes. Velg deretter hvilket navn du Ã¸nsker pÃ¥ branchen, og trykk Create pull request.\nI PR-en venter du og ser at atlantis/plan1 har blitt kjÃ¸rt og fÃ¥r en  til venstre for hver kjÃ¸ring, slik som vist i FigurÂ 1.\n\n\n\n\n./infra/projects.yaml\n\nteam_uniform_name: play-obr\n\nprojects:\n  - project_name: play-obr\n    env: test\n    features:\n      - dapla-buckets\n      - kildomaten\n\n  - project_name: play-obr\n    env: prod\n    features:\n      - dapla-buckets\n\n\n\n\n\n\n\n\n\nFigurÂ 1: Atlantis plan planlegger endringen og sjekker om den er klar til gjennomfÃ¸ring.\n\n\n\n\nHvis alt er i orden sÃ¥ ber du en kollega om Ã¥ se over endringen og godkjenne hvis alt ser riktig ut. NÃ¥r den er godkjent vil du se et bilde som ligner det du ser i FigurÂ 2.\n\n\n\n\n\n\n\nFigurÂ 2: Sjekker av PR-er i teamets IaC-repo.\n\n\n\n\nNÃ¥r PR er godkjent av en kollega, og atlantis plan ikke viser noen feilmeldinger, sÃ¥ kan du effektuere endringene ved Ã¥ atlantis apply i kommentarfeltet til PR-en. Kommentareren vil trigge en kjÃ¸ring som effektuerer alle endringer pÃ¥ plattformen.\nEtter at atlantis apply er kjÃ¸rt, sÃ¥ kan du merge inn branchen til main.\nTil slutt kan du slette branchen.\n\nNÃ¥r dette er gjort sÃ¥ endringen effektuert pÃ¥ Dapla og klar til bruk. For eksempel, hvis vi hadde aktivert transfer-service i prod for projects.yaml over, sÃ¥ ville den etter endringen sett slik ut:\n\n\n./infra/projects.yaml\n\nteam_uniform_name: play-obr\n\nprojects:\n  - project_name: play-obr\n    env: test\n    features:\n      - dapla-buckets\n      - kildomaten\n\n  - project_name: play-obr\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service",
    "crumbs": [
      "Statistikere",
      "Dapla-team",
      "Features"
    ]
  },
  {
    "objectID": "statistikkere/features.html#deaktivere-en-feature",
    "href": "statistikkere/features.html#deaktivere-en-feature",
    "title": "Features",
    "section": "",
    "text": "For Ã¥ deaktivere en feature som ikke lenger i bruk, sÃ¥ fÃ¸lger du bare beskrivelsen for Aktivere tjeneste over, med det unntak at du nÃ¥ fjerner en linje istedenfor Ã¥ legge til.",
    "crumbs": [
      "Statistikere",
      "Dapla-team",
      "Features"
    ]
  },
  {
    "objectID": "statistikkere/features.html#footnotes",
    "href": "statistikkere/features.html#footnotes",
    "title": "Features",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nDet som skjer nÃ¥r atlantis plan kjÃ¸res er at det genereres en detaljert beskrivelse av hvilke endringer som mÃ¥ skje pÃ¥ plattformen for at teamets feature skal aktiveres. Derfor mÃ¥ eventuelle feilmeldinger fra atlantis plan fikses fÃ¸r man faktiske kan effektuere endringene med atlantis apply. â†©ï¸",
    "crumbs": [
      "Statistikere",
      "Dapla-team",
      "Features"
    ]
  },
  {
    "objectID": "statistikkere/kildedata.html",
    "href": "statistikkere/kildedata.html",
    "title": "Kildedata",
    "section": "",
    "text": "Kildedata er data lagret slik de ble levert til SSB fra dataeier, det vil si pÃ¥ dataeiers dataformat og med informasjon om tidspunkt og rekkefÃ¸lge for avlevering. Kildedata er en del av statistikkenes dokumentasjon, og kan vÃ¦re en nÃ¸dvendig kilde for forskning og nye statistikker. Uten kildedataene vil det ikke vÃ¦re mulig Ã¥ etterprÃ¸ve SSB sine statistikker. De originale kildedataene vil ofte komprimeres og krypteres etter at relevante deler er transformert til inndata.\n\n(Standardutvalget 2021, 7)\n\nStatistikkloven Â§ 9 Informasjonssikkerhet stiller krav om at direkte identifiserende opplysninger skal behandles og lagres adskilt fra Ã¸vrige opplysninger, med mindre det vil vÃ¦re uforenlig med formÃ¥let med behandlingen eller Ã¥penbart unÃ¸dvendig. I henhold til policy om Datatilstander er kildedata i utgangspunktet den eneste datatilstanden som kan inneholde denne type data. I Ã¸vrige tilstander skal direkteidentifiserende opplysninger som hovedregel vÃ¦re pseudonymisert. Avvik skal dokumenteres og godkjennes av seksjonsleder som er ansvarlig for avviket.\n\n(DirektÃ¸rmÃ¸tet 2022, 2)\nFordi Kildedata kan inneholde PII1 implementerer Dapla fÃ¸lgende tiltak:\n\nKildedata er lagret adskilt fra andre datatilstander.\nTilgang til dataene begrenses sÃ¥ langt som mulig, kun en begrenset gruppe personer2 har tilgang til kildedata.\nProsessering av kildedata utfÃ¸res automatisk for minske behov for tilgang til dataene."
  },
  {
    "objectID": "statistikkere/kildedata.html#footnotes",
    "href": "statistikkere/kildedata.html#footnotes",
    "title": "Kildedata",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPersonidentifiserende Informasjonâ†©ï¸\nData adminsâ†©ï¸"
  },
  {
    "objectID": "statistikkere/spark.html",
    "href": "statistikkere/spark.html",
    "title": "Apache Spark",
    "section": "",
    "text": "Koding i R og Python har historisk sett foregÃ¥tt pÃ¥ en enkelt maskin og vÃ¦rt begrenset av minnet (RAM) og prosessorkraften pÃ¥ maskinen. For bearbeiding av smÃ¥ og mellomstore datasett er det sjelden et problem pÃ¥ kjÃ¸re pÃ¥ en enkelt maskin. PopulÃ¦re pakker som dplyr for R, og pandas for Python, blir ofte brukt i denne typen databehandling. I senere Ã¥r har det ogsÃ¥ kommet pakker som er optimalisert for Ã¥ kjÃ¸re kode parallelt pÃ¥ flere kjerner pÃ¥ en enkelt maskin, skrevet i minne-effektive sprÃ¥k som Rust og C++.\nMen selv om man kommer langt med Ã¥ kjÃ¸re kode pÃ¥ en enkelt maskin, vil enkelte oppgaver kreve mer minne og prosessorkraft enn dette. For stÃ¸rre datasett, eller store beregninger, kan det vÃ¦re nyttig Ã¥ bruke et rammeverk som kan kjÃ¸re kode parallelt pÃ¥ flere maskiner. Et slikt rammeverk er Apache Spark.\nApache Spark er et rammeverk for Ã¥ kjÃ¸re kode parallelt pÃ¥ flere maskiner. Det er bygget for Ã¥ hÃ¥ndtere store datasett og store beregninger. Det er derfor et nyttig verktÃ¸y for Ã¥ lÃ¸se problemer som er for store for Ã¥ kjÃ¸re pÃ¥ en enkelt maskin. Men det finnes ogsÃ¥ andre bruksomrÃ¥der som er nyttige for Apache Spark. Her er noen eksempler:\nDette er noen av bruksomrÃ¥dene der Spark kan lÃ¸se problemer som er for store for Ã¥ kjÃ¸re pÃ¥ en enkelt maskin med for eksempel Pandas eller dplyr.",
    "crumbs": [
      "Statistikere",
      "Appendix",
      "Apache Spark"
    ]
  },
  {
    "objectID": "statistikkere/spark.html#spark-pÃ¥-dapla",
    "href": "statistikkere/spark.html#spark-pÃ¥-dapla",
    "title": "Apache Spark",
    "section": "Spark pÃ¥ Dapla",
    "text": "Spark pÃ¥ Dapla\nDapla kjÃ¸rer pÃ¥ et Kubernetes-kluster og er derfor er et svÃ¦rt egnet sted for Ã¥ kjÃ¸re kode parallelt pÃ¥ flere maskiner. Jupyter pÃ¥ Dapla har ogsÃ¥ en flere klargjorte kernels for Ã¥ kjÃ¸re kode i Apache Spark. Denne koden vil kjÃ¸re pÃ¥ et eget kluster av maskiner som er dedikert til Apache Spark, slik som vist i FigurÂ 1.\n\n\n\n\n\n\n\n\n\n\n\n(a) PySpark pÃ¥ kubernetes\n\n\n\n\n\n\n\n\n\n\n\n(b) PySpark pÃ¥ 1 maskin\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) SparkR pÃ¥ kubernetes\n\n\n\n\n\n\n\n\n\n\n\n(d) SparkR pÃ¥ 1 maskin\n\n\n\n\n\n\n\nFigurÂ 1: Ferdigkonfigurerte kernels for Spark pÃ¥ Dapla.\n\n\n\nFigurÂ 1 (a) og FigurÂ 1 (c) kan velges hvis du Ã¸nsker Ã¥ bruke Spark for Ã¥ kjÃ¸re store jobber pÃ¥ flere maskiner, for henholdsvis Python- og R-grensesnittene for Spark.\nFigurÂ 1 (b) og FigurÂ 1 (d) bÃ¸r du velge hvis du Ã¸nsker Ã¥ bruke Spark av andre grunner enn Ã¥ kjÃ¸re store jobber pÃ¥ flere maskiner. For eksempel hvis du Ã¸nsker Ã¥ bruke en av de mange pakker som er bygget pÃ¥ Spark, eller hvis du Ã¸nsker Ã¥ bruke Spark til Ã¥ lese og skrive data fra Dapla.\nHvis du Ã¸nsker Ã¥ sette opp et eget virtuelt miljÃ¸ for Ã¥ kjÃ¸re Spark, sÃ¥ kan du bruke ssb-project. Se ssb-project for mer informasjon.",
    "crumbs": [
      "Statistikere",
      "Appendix",
      "Apache Spark"
    ]
  },
  {
    "objectID": "statistikkere/spark.html#spark-i-og-python",
    "href": "statistikkere/spark.html#spark-i-og-python",
    "title": "Apache Spark",
    "section": "Spark i  og Python",
    "text": "Spark i  og Python\nSpark er implementert i programmeringssprÃ¥ket Scala. Men det tilbys ogsÃ¥ mange grensesnitt for Ã¥ bruke Spark fra andre sprÃ¥k. De mest populÃ¦re grensesnittene er PySpark for Python og SparkR for R. Disse grensesnittene er bygget pÃ¥ Spark, og gir tilgang til Spark-funksjonalitet fra Python og R.\n\nPySpark\nPySpark er et Python-grensesnitt for Apache Spark. Det er en Python-pakke som gir tilgang til Spark-funksjonalitet fra Python. Det er enkelt Ã¥ bruke, og har mange av de samme funksjonene som Pandas.\nUnder ser du datasettet som benyttes for i vedlagt notebook pyspark-intro.ipynb. Den viser hvordan man kan gjÃ¸re vanlige databehandling med PySpark. I eksempelet brukes kernel som vist i FigurÂ 1 (b).\n\n\n\nCode\n# Legger til row index til DataFrame fÃ¸r join med dates_df\ndata_df = data_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n\n# Joiner de to datasettene\ndf = (\n    dates_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n    .join(data_df, \"row_index\")\n    .drop(\"row_index\")\n)\n\n# Legger til Ã¥r, kvartal og mnd\ndf = df.withColumn(\"Year\", date_format(df.Date, \"yyyy\"))\ndf = df.withColumn(\"Quarter\", expr(\"quarter(Date)\"))\ndf = df.withColumn(\"Month\", date_format(df.Date, \"MM\"))\n\ndf.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows\n\n\n\nSource: Introduksjon til PySpark\nDet finnes ogsÃ¥ et Pandas API/grensesnitt mot Spark. MÃ¥let med en er Ã¥ gjÃ¸re overgangen fra Pandas til Spark lettere for nybegynneren. Men hvis man skal gjÃ¸re litt mer avansert databehandling anbefales det at man bruker PySpark direkte og ikke Pandas API-et.\n\n\nSparkR\nSparkR er et R-grensesnitt for Apache Spark. Det er en R-pakke som gir tilgang til Spark-funksjonalitet fra R. Det er enkelt Ã¥ bruke, og har mange av de samme funksjonene som dplyr. Se eksempel i notebook under:\n\n\nselectedColumns &lt;- select(file, \"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\")\nshowDF(selectedColumns, numRows = 5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows\n\n\nSource: Introduksjon til SparkR",
    "crumbs": [
      "Statistikere",
      "Appendix",
      "Apache Spark"
    ]
  },
  {
    "objectID": "statistikkere/spark.html#lakehouse-arkitektur",
    "href": "statistikkere/spark.html#lakehouse-arkitektur",
    "title": "Apache Spark",
    "section": "Lakehouse-arkitektur",
    "text": "Lakehouse-arkitektur\n\n\n\n\n\n\nWarning\n\n\n\nI denne delen viser vi hvordan funksjonalitet som kan bli relevant for SSB Ã¥ benytte seg av i fremtiden. Men det er fortsatt under testing og ta det i betraktning fÃ¸r man eventuelt implementerer dette i produksjon.\n\n\nEn av utvidelsene som er laget rundt Apache Spark er den sÃ¥kalte Lakehouse-arkitekturen. Kort fortalt kan den dekke behovene som et klassisk datavarehus har tatt seg av tidligere. I kontekst av SSB sine teknologier kan det ogsÃ¥ benyttes som et databaselag over Parquet-filer i bÃ¸tter. Det finnes flere open source lÃ¸sninger for dette, men mest aktuelle er:\n\nDelta Lake\nApache Hudi\nApache Iceberg\n\nI det fÃ¸lgende omtaler vi hovedsakelig egenskapene til Delta Lake, men alle rammeverkene har mange av de samme egenskapene. Delta Lake kan ogsÃ¥ benyttes pÃ¥ Dapla nÃ¥.\nSentrale egenskaper ved Delta Lake er:\n\nACID-transactions som sikrer data-integritet og stabilitet, ogsÃ¥ nÃ¥r det skjer feil.\nMetadata som bli hÃ¥ndtert akkurat som all annen data og er veldig skalebar. Den stÃ¸tter ogsÃ¥ egendefinert metadata.\nSchema Enforcement and Evolution sikrer at skjemaet til dataene blir hÃ¥ndhevet, og den tillater ogsÃ¥ den kan endres over tid.\nTime travel sikrer at alle versjoner av dataene er blitt lagret, og at du kan gÃ¥ tilbake til tidligere versjoner av en fil.\nAudit history sikrer at du kan fÃ¥ full oversikt over hvilke operasjoner som utfÃ¸rt pÃ¥ dataene.\nInserts, updates and deletes betyr at du lettere kan manipulere data enn hva som er tilfellet med vanlige Parquet-filer.\nIndexing er stÃ¸ttes for forbedre spÃ¸rringer mot store datamengder.\n\nI vedlagt notebook deltalake-intro.ipynb finner du blant annet eksempler pÃ¥ hvordan du legger til fÃ¸lgende metadata i spesifikk versjon av en fil:\n\n\n\nCode\n# Lagrer egendefinert metadata i en json-fil\nimport json\n\nmetadata = {\n    \"comment\": \"Kontaktet oppgavegiver og kranglet!\",\n    \"manueltEditert\": \"True\",\n    \"maskineltEditert\": \"False\",\n}\nmetadata\n\n\n{'comment': 'Kontaktet oppgavegiver og kranglet!',\n 'manueltEditert': 'True',\n 'maskineltEditert': 'False'}\n\n\nSource: Introduksjon til Delta Lake",
    "crumbs": [
      "Statistikere",
      "Appendix",
      "Apache Spark"
    ]
  },
  {
    "objectID": "statistikkere/test.html",
    "href": "statistikkere/test.html",
    "title": "Notebooks",
    "section": "",
    "text": "Notebooks\nsdlÃ¸kfjlksdjfkl"
  },
  {
    "objectID": "statistikkere/introduksjon.html",
    "href": "statistikkere/introduksjon.html",
    "title": "Introduksjon",
    "section": "",
    "text": "Introduksjon\nMÃ¥let med dette kapittelet er Ã¥ gi en grunnleggende innfÃ¸ring i hva som legges i ordet Dapla. I tillegg gis en forklaring pÃ¥ hvorfor disse valgene er tatt."
  },
  {
    "objectID": "statistikkere/nytt-ssbproject.html",
    "href": "statistikkere/nytt-ssbproject.html",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "I dette kapittelet forklarer vi hvordan du oppretter et ssb-project og hva det innebÃ¦rer. ssb-project er et CLI1 for Ã¥ raskt komme i gang med koding pÃ¥ Dapla, hvor en del SSB-spesifikke beste-prakiser er ivaretatt. Kode som naturlig hÃ¸rer sammen, f.eks. koden til et produksjonslÃ¸p for en statistikk, er mÃ¥lgruppen for dette programmet. Kort fortalt kan du kjÃ¸re denne kommandoen i en terminal\n\n\nterminal\n\nssb-project create stat-testprod\n\nog du vil fÃ¥ en mappe som heter stat-testprod med fÃ¸lgende innhold:\n\nStandard mappestruktur En standard mappestruktur gjÃ¸r det lettere Ã¥ dele og samarbeide om kode, som igjen reduserer sÃ¥rbarheten knyttet til at fÃ¥ personer kjenner koden.\nVirtuelt miljÃ¸ Virtuelle miljÃ¸er isolerer og lagrer informasjon knyttet til kode. For eksempel hvilken versjon av Python du bruker og tilhÃ¸rende pakkeversjoner. Det er viktig for at publiserte tall skal vÃ¦re reproduserbare. VerktÃ¸yet for Ã¥ lage virtuelt miljÃ¸ er Poetry.\nVersjonshÃ¥ndtering med Git Initierer versjonshÃ¥ndtering med Git og legger til SSBs anbefalte .gitignore og .gitattributes. Det sikrer at du ikke versjonhÃ¥ndterer filer/informasjon som ikke skal versjonshÃ¥ndteres.\n\nI tillegg lar ssb-project deg opprette et GitHub-repo hvis du Ã¸nsker. Les mer om hvordan du kan ta i bruk dette verktÃ¸yet under.\n\n\n\n\n\n\nNote\n\n\n\nDokumentasjonen for ssb-project finnes her: https://statisticsnorway.github.io/ssb-project-cli/. Det oppdateres hver gang en ny versjon av ssb-project slippes.\n\n\n\n\nFÃ¸r du kan ta i bruk ssb-project sÃ¥ er det et par ting som mÃ¥ vÃ¦re pÃ¥ plass:\n\nDu mÃ¥ ha opprettet en git-bruker og git-epost lokalt der du skal kalle pÃ¥ programmet (les mer om hvordan her).\nHvis du Ã¸nsker at ssb-project ogsÃ¥ skal opprette et GitHub-repo for deg mÃ¥ du ogsÃ¥ fÃ¸lgende vÃ¦re pÃ¥ plass:\n\nDu mÃ¥ ha en GitHub-bruker (les hvordan her)\nSkru pÃ¥ 2-faktor autentisering for GitHub-brukeren din (les hvordan her)\nVÃ¦re koblet mot SSBs organisasjon statisticsnorway pÃ¥ GitHub (les hvordan her)\nOpprette Personal Access Token (PAT) og godkjenne det for bruk mot statisticsnorway (les hvordan her)\n\n\nDet er ogsÃ¥ Ã¥ anbefale at du lagrer PAT lokalt slik at du ikke trenger Ã¥ forholde deg til det nÃ¥r jobber med Git og GitHub. Hvis du har alt dette pÃ¥ plass sÃ¥ kan du bare fortsette Ã¥ fÃ¸lge de neste kapitlene.\n\n\n\n\n\n\n\n\n\nHar du Github bruker? Noe funksjonalitet i ssb-project krever det. Finn ut hvordan ved Ã¥ lese forrige kapittel.\n\n\n\nssb-project lar deg opprette en prosjekt-mappe med og uten GitHub-repo. La oss ta for oss hver av alternativene.\n\n\nFor Ã¥ opprette et nytt ssb-project uten GitHub-repo gjÃ¸r du fÃ¸lgende:\n\nÃ…pne en terminal. De fleste vil gjÃ¸re dette i Jupyterlab pÃ¥ bakke eller sky og da kan de bare trykke pÃ¥ det blÃ¥ â•-tegnet i Jupyterlab og velge Terminal.\nFÃ¸r vi kjÃ¸rer programmet mÃ¥ vi vÃ¦re obs pÃ¥ at ssb-project vil opprette en ny mappe der vi stÃ¥r. GÃ¥ derfor til den mappen du Ã¸nsker Ã¥ ha den nye prosjektmappen. For Ã¥ opprette et prosjekt som heter stat-testprod sÃ¥ skriver du fÃ¸lgende i terminalen:\n\n\n\nterminal\n\nssb-project create stat-testprod\n\n\n\nHvis du stod i hjemmemappen din pÃ¥ nÃ¥r du skrev inn kommandoen over i terminalen, sÃ¥ har du fÃ¥tt mappestrukturen som vises i FigurÂ 1. 2. Den inneholder fÃ¸lgende :\n\n.git-mappe som blir opprettet for Ã¥ versjonshÃ¥ndtere med Git.\nsrc-mappe som skal inneholde all koden som utgjÃ¸r produksjonslÃ¸pet.\ntests-mappe som inneholder tester du skriver for koden din.\nLICENCE-fil som skal benyttes for public-repos i SSB.\npoetry.lock-fil som inneholder alle versjoner av Python-pakker som blir brukt.\nREADME.md-fil som brukes for tekstlig innhold pÃ¥ GitHub-siden for prosjektet.\n\n\n\n\n\n\n\n\n\nFigurÂ 1: Mappen som ble opprettet av ssb-project.\n\n\n\n\n\n\n\n\nOver sÃ¥ opprettet vi et ssb-project uten Ã¥ opprette et GitHub-repo. Hvis du Ã¸nsker Ã¥ opprette et GitHub-repo ogsÃ¥ mÃ¥ du endre kommandoen over til:\n\n\nterminal\n\nssb-project create stat-testprod --github --github-token='blablabla'\n\nKommandoen over oppretter en mappestruktur slik vi sÃ¥ tidligere, men ogsÃ¥ et ssb-project som heter stat-testprod med et GitHub-repo med samme navn. Som du ser sÃ¥ mÃ¥ vi da sende med opsjonen --github og PAT med opsjonen --github-token='blablabla'. Repoet i GitHub ser da ut som i FigurÂ 2.\n\n\n\n\n\n\nFigurÂ 2: GitHub-repo som er opprettet av ssb-project\n\n\n\n\n\n\n\n\n\nNÃ¥r du har opprettet et nytt ssb-project, eller bygget et eksisterende prosjekt, sÃ¥ kan det ta rundt 30 sekunder fÃ¸r kernelen viser seg i Jupterlab-launcher. VÃ¦r tÃ¥lmodig!\n\n\n\n\n\n\n\nNÃ¥r du har opprettet et ssb-project sÃ¥ kan du installere de python-pakkene du trenger fra PyPI. Hvis du for eksempel Ã¸nsker Ã¥ installere Pandas, et populÃ¦rt data wrangling bibliotek, sÃ¥ kan du gjÃ¸re fÃ¸lgende:\n\nÃ…pne en terminal i Jupyterlab.\nGÃ¥ inn i prosjektmappen din ved Ã¥ skrive\n\n\n\nterminal\n\ncd &lt;sti til prosjektmappe&gt;\n\n\nLag en branch/utviklingsg som f.eks. heter install-pandas:\n\n\n\nterminal\n\ngit checkout -b install-pandas\n\n\nInstaller Pandas ved Ã¥ skrive fÃ¸lgende\n\n\n\nterminal\n\npoetry add pandas\n\n\n\n\n\n\n\nFigurÂ 3: Installasjon av Pandas med ssb-project\n\n\n\nFigurÂ 3 viser hvordan dette vil se ut i en Jupyterlab-terminal. Kommandoen for Ã¥ installere noe er poetry add etterfulgt av pakkenavnet. Vi ser ogsÃ¥ at den automatisk legger til Pandas-versjonen i filen poetry.lock. Les mer om hvordan man installerer pakker her.\n\n\n\nNÃ¥r du nÃ¥ har installert en pakke sÃ¥ har filen poetry.lock endret seg. La oss for eksempelets skyld anta at du Ã¸nsker Ã¥ bruke Git til Ã¥ dokumentere denne hendelsen, og dele det med en kollega via GitHub. Hvis vi har opprettet et ssb-project med et GitHub-repo sÃ¥ kan vi gjÃ¸re akkurat dette:\n\nVi kan stage alle endringer med fÃ¸lgende kommando i terminalen nÃ¥r vi stÃ¥r i prosjektmappen:\n\n\n\nterminal\n\ngit add -A\n\n\nVidere kan commit en endring, dvs. ta et stillbilde av koden i dette Ã¸yeblikket, ved Ã¥ skrive fÃ¸lgende:\n\n\n\nterminal\n\ngit commit -m \"Installert pandas\"\n\n\nPush det opp til GitHub3. Anta at vi gjorde dette i branchen install-pandas som ble opprettet tidligere. Da kan vi skrive fÃ¸lgende:\n\n\n\nterminal\n\ngit push --set-upstream origin install-pandas\n\nMer kommer her.\n\n\n\nNÃ¥r vi skal samarbeide med andre om kode sÃ¥ gjÃ¸r vi dette via GitHub. NÃ¥r du pusher koden din til GitHub, sÃ¥ kan samarbeidspartnere pulle ned koden og jobbe videre med den. Men nÃ¥r de henter ned koden sÃ¥ vil de bare hente ned selve koden, ikke pakker og Python-versjonen som ble brukt. De mÃ¥ installere alt som du hadde installert. I tillegg trenger de en kernel hvis de skal jobbe i Jupyterlab. ssb-project gjÃ¸r det svÃ¦rt enkelt Ã¥ bygge opp det du trenger, siden det virtuelle miljÃ¸et har all informasjon om hva som trengs.\nFor at samarbeidspartneren din skal kunne bygge miljÃ¸et pÃ¥ nytt, mÃ¥ de ha gjort en minimal konfigurering av Git. Les mer om hvordan du frem for Ã¥ gjÃ¸re dette her.\nFor Ã¥ bygge opp et eksisterende miljÃ¸ gjÃ¸r du fÃ¸lgende:\n\nFÃ¸rst mÃ¥ du kopiere prosjektet ned lokalt, eller klone repoet med git-terminologi\n\n\n\nterminal\n\ngit clone https://github.com/statisticsnorway/&lt;prosjektnavn&gt;\n\n\nGÃ¥ inn i mappen du klonet\n\n\n\nterminal\n\ncd &lt;prosjektnavn&gt;\n\n\nSkape et virtuelt miljÃ¸ og installere en tilsvarende Jupyter kernel med\n\n\n\nterminal\n\nssb-project build\n\n\n\n\nDet vil vÃ¦re tilfeller hvor man Ã¸nsker Ã¥ slette et ssb-project, enten fordi man ikke trenger koden lenger eller fordi man bare testet litt.\n\n\nHvis man jobber med flere prosjekter sÃ¥ kan det fort bli mange Jupyter kerneler hengende igjen. Derfor er det ogsÃ¥ mulighet Ã¥ kjÃ¸re\n\n\nterminal\n\nssb-project clean stat-testprod\n\nsom sletter Jupyter-kernelen og de installerte pakkene i prosjektet. Hvis du ogsÃ¥ Ã¸nsker Ã¥ slette selve mappen med kode mÃ¥ du gjÃ¸re det manuelt4:\n\n\nterminal\n\nrm -rf ~/stat-testprod/\n\nProsjektmappen over lÃ¥ direkte i hjemmemappen min og hjemmemappen pÃ¥ Linux kan alltid referes til med et tilda-tegn ~.\n\n\n\nGitHub-repoer som er opprettet under SSB sin organinasjons statisticsnorway pÃ¥ GitHub kan ikke slettes, bare arkiveres. Grunnen er at hvis man oppdager en sÃ¥rbarhet senere sÃ¥ er det viktig Ã¥ kunne se repoet for Ã¥ forstÃ¥ hva som har skjedd.\nHvis du ikke trenger et GitHub-repo lenger kan man arkivere repoet. Det gjÃ¸r du pÃ¥ fÃ¸lgende mÃ¥te:\n\nGi inn i repoet Settings slik som vist med rÃ¸d pil i FigurÂ 4.\n\n\n\n\n\n\n\nFigurÂ 4: Settings for repoet.\n\n\n\n\nUnder General scroller du deg ned til delen som heter Danger Zone og velger Archive this repository, slik som vist pÃ¥ FigurÂ 5.\n\n\n\n\n\n\n\nFigurÂ 5: Arkivering av et repo.\n\n\n\n\nI dialogboksen som dukker opp fyller du inn reponavnet som beskrevet og trykker pÃ¥ I understand the consequences, archive this repository, som vist i FigurÂ 6.\n\n\n\n\n\n\n\nFigurÂ 6: Bekreftelse av arkiveringen.\n\n\n\nNÃ¥r det er gjort sÃ¥ er repoet lesbart, men man kan ikke jobbe med det. Men som vi ser av @#fig-github-repo-settings-archive-warning kan man omgjÃ¸re arkiveringen senere hvis det skulle vÃ¦re Ã¸nskelig.\n\n\n\n\nVi har forelÃ¸pig ikke integret R i ssb-project. Grunnen er at det mest populÃ¦re virtuelle miljÃ¸-verktÃ¸et for R, renv, kun tilbyr Ã¥ passe pÃ¥ versjoner av R-pakker og ikke selve R-installasjonen. Det er en svakhet som trolig gjÃ¸r det vanskeligere enn nÃ¸dvendig Ã¥ gjenskape tidligere publiserte resultater med ssb-project. I tillegg klarer den ikke Ã¥ gjenkjenne pakker som blir brukt i ipynb-filer.\nPlanen er Ã¥ finne et annet verktÃ¸y enn renv som kan ogsÃ¥ reprodusere R-versjonen. Team Statistikktjenester ser nÃ¦rmere pÃ¥ hvilke alternativer som finnes og vil tilby noe i fremtiden.\nI mellomtiden kan man bruke renv slik det er beskrevet her for skymiljÃ¸et, og med denne modifiseringen for bakkemiljÃ¸et."
  },
  {
    "objectID": "statistikkere/nytt-ssbproject.html#forberedelser",
    "href": "statistikkere/nytt-ssbproject.html#forberedelser",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "FÃ¸r du kan ta i bruk ssb-project sÃ¥ er det et par ting som mÃ¥ vÃ¦re pÃ¥ plass:\n\nDu mÃ¥ ha opprettet en git-bruker og git-epost lokalt der du skal kalle pÃ¥ programmet (les mer om hvordan her).\nHvis du Ã¸nsker at ssb-project ogsÃ¥ skal opprette et GitHub-repo for deg mÃ¥ du ogsÃ¥ fÃ¸lgende vÃ¦re pÃ¥ plass:\n\nDu mÃ¥ ha en GitHub-bruker (les hvordan her)\nSkru pÃ¥ 2-faktor autentisering for GitHub-brukeren din (les hvordan her)\nVÃ¦re koblet mot SSBs organisasjon statisticsnorway pÃ¥ GitHub (les hvordan her)\nOpprette Personal Access Token (PAT) og godkjenne det for bruk mot statisticsnorway (les hvordan her)\n\n\nDet er ogsÃ¥ Ã¥ anbefale at du lagrer PAT lokalt slik at du ikke trenger Ã¥ forholde deg til det nÃ¥r jobber med Git og GitHub. Hvis du har alt dette pÃ¥ plass sÃ¥ kan du bare fortsette Ã¥ fÃ¸lge de neste kapitlene."
  },
  {
    "objectID": "statistikkere/nytt-ssbproject.html#opprett-ssb-project",
    "href": "statistikkere/nytt-ssbproject.html#opprett-ssb-project",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "Har du Github bruker? Noe funksjonalitet i ssb-project krever det. Finn ut hvordan ved Ã¥ lese forrige kapittel.\n\n\n\nssb-project lar deg opprette en prosjekt-mappe med og uten GitHub-repo. La oss ta for oss hver av alternativene.\n\n\nFor Ã¥ opprette et nytt ssb-project uten GitHub-repo gjÃ¸r du fÃ¸lgende:\n\nÃ…pne en terminal. De fleste vil gjÃ¸re dette i Jupyterlab pÃ¥ bakke eller sky og da kan de bare trykke pÃ¥ det blÃ¥ â•-tegnet i Jupyterlab og velge Terminal.\nFÃ¸r vi kjÃ¸rer programmet mÃ¥ vi vÃ¦re obs pÃ¥ at ssb-project vil opprette en ny mappe der vi stÃ¥r. GÃ¥ derfor til den mappen du Ã¸nsker Ã¥ ha den nye prosjektmappen. For Ã¥ opprette et prosjekt som heter stat-testprod sÃ¥ skriver du fÃ¸lgende i terminalen:\n\n\n\nterminal\n\nssb-project create stat-testprod\n\n\n\nHvis du stod i hjemmemappen din pÃ¥ nÃ¥r du skrev inn kommandoen over i terminalen, sÃ¥ har du fÃ¥tt mappestrukturen som vises i FigurÂ 1. 2. Den inneholder fÃ¸lgende :\n\n.git-mappe som blir opprettet for Ã¥ versjonshÃ¥ndtere med Git.\nsrc-mappe som skal inneholde all koden som utgjÃ¸r produksjonslÃ¸pet.\ntests-mappe som inneholder tester du skriver for koden din.\nLICENCE-fil som skal benyttes for public-repos i SSB.\npoetry.lock-fil som inneholder alle versjoner av Python-pakker som blir brukt.\nREADME.md-fil som brukes for tekstlig innhold pÃ¥ GitHub-siden for prosjektet.\n\n\n\n\n\n\n\n\n\nFigurÂ 1: Mappen som ble opprettet av ssb-project.\n\n\n\n\n\n\n\n\nOver sÃ¥ opprettet vi et ssb-project uten Ã¥ opprette et GitHub-repo. Hvis du Ã¸nsker Ã¥ opprette et GitHub-repo ogsÃ¥ mÃ¥ du endre kommandoen over til:\n\n\nterminal\n\nssb-project create stat-testprod --github --github-token='blablabla'\n\nKommandoen over oppretter en mappestruktur slik vi sÃ¥ tidligere, men ogsÃ¥ et ssb-project som heter stat-testprod med et GitHub-repo med samme navn. Som du ser sÃ¥ mÃ¥ vi da sende med opsjonen --github og PAT med opsjonen --github-token='blablabla'. Repoet i GitHub ser da ut som i FigurÂ 2.\n\n\n\n\n\n\nFigurÂ 2: GitHub-repo som er opprettet av ssb-project\n\n\n\n\n\n\n\n\n\nNÃ¥r du har opprettet et nytt ssb-project, eller bygget et eksisterende prosjekt, sÃ¥ kan det ta rundt 30 sekunder fÃ¸r kernelen viser seg i Jupterlab-launcher. VÃ¦r tÃ¥lmodig!"
  },
  {
    "objectID": "statistikkere/nytt-ssbproject.html#installere-pakker",
    "href": "statistikkere/nytt-ssbproject.html#installere-pakker",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "NÃ¥r du har opprettet et ssb-project sÃ¥ kan du installere de python-pakkene du trenger fra PyPI. Hvis du for eksempel Ã¸nsker Ã¥ installere Pandas, et populÃ¦rt data wrangling bibliotek, sÃ¥ kan du gjÃ¸re fÃ¸lgende:\n\nÃ…pne en terminal i Jupyterlab.\nGÃ¥ inn i prosjektmappen din ved Ã¥ skrive\n\n\n\nterminal\n\ncd &lt;sti til prosjektmappe&gt;\n\n\nLag en branch/utviklingsg som f.eks. heter install-pandas:\n\n\n\nterminal\n\ngit checkout -b install-pandas\n\n\nInstaller Pandas ved Ã¥ skrive fÃ¸lgende\n\n\n\nterminal\n\npoetry add pandas\n\n\n\n\n\n\n\nFigurÂ 3: Installasjon av Pandas med ssb-project\n\n\n\nFigurÂ 3 viser hvordan dette vil se ut i en Jupyterlab-terminal. Kommandoen for Ã¥ installere noe er poetry add etterfulgt av pakkenavnet. Vi ser ogsÃ¥ at den automatisk legger til Pandas-versjonen i filen poetry.lock. Les mer om hvordan man installerer pakker her."
  },
  {
    "objectID": "statistikkere/nytt-ssbproject.html#push-til-github",
    "href": "statistikkere/nytt-ssbproject.html#push-til-github",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "NÃ¥r du nÃ¥ har installert en pakke sÃ¥ har filen poetry.lock endret seg. La oss for eksempelets skyld anta at du Ã¸nsker Ã¥ bruke Git til Ã¥ dokumentere denne hendelsen, og dele det med en kollega via GitHub. Hvis vi har opprettet et ssb-project med et GitHub-repo sÃ¥ kan vi gjÃ¸re akkurat dette:\n\nVi kan stage alle endringer med fÃ¸lgende kommando i terminalen nÃ¥r vi stÃ¥r i prosjektmappen:\n\n\n\nterminal\n\ngit add -A\n\n\nVidere kan commit en endring, dvs. ta et stillbilde av koden i dette Ã¸yeblikket, ved Ã¥ skrive fÃ¸lgende:\n\n\n\nterminal\n\ngit commit -m \"Installert pandas\"\n\n\nPush det opp til GitHub3. Anta at vi gjorde dette i branchen install-pandas som ble opprettet tidligere. Da kan vi skrive fÃ¸lgende:\n\n\n\nterminal\n\ngit push --set-upstream origin install-pandas\n\nMer kommer her."
  },
  {
    "objectID": "statistikkere/nytt-ssbproject.html#bygg-eksisterende-ssb-project",
    "href": "statistikkere/nytt-ssbproject.html#bygg-eksisterende-ssb-project",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "NÃ¥r vi skal samarbeide med andre om kode sÃ¥ gjÃ¸r vi dette via GitHub. NÃ¥r du pusher koden din til GitHub, sÃ¥ kan samarbeidspartnere pulle ned koden og jobbe videre med den. Men nÃ¥r de henter ned koden sÃ¥ vil de bare hente ned selve koden, ikke pakker og Python-versjonen som ble brukt. De mÃ¥ installere alt som du hadde installert. I tillegg trenger de en kernel hvis de skal jobbe i Jupyterlab. ssb-project gjÃ¸r det svÃ¦rt enkelt Ã¥ bygge opp det du trenger, siden det virtuelle miljÃ¸et har all informasjon om hva som trengs.\nFor at samarbeidspartneren din skal kunne bygge miljÃ¸et pÃ¥ nytt, mÃ¥ de ha gjort en minimal konfigurering av Git. Les mer om hvordan du frem for Ã¥ gjÃ¸re dette her.\nFor Ã¥ bygge opp et eksisterende miljÃ¸ gjÃ¸r du fÃ¸lgende:\n\nFÃ¸rst mÃ¥ du kopiere prosjektet ned lokalt, eller klone repoet med git-terminologi\n\n\n\nterminal\n\ngit clone https://github.com/statisticsnorway/&lt;prosjektnavn&gt;\n\n\nGÃ¥ inn i mappen du klonet\n\n\n\nterminal\n\ncd &lt;prosjektnavn&gt;\n\n\nSkape et virtuelt miljÃ¸ og installere en tilsvarende Jupyter kernel med\n\n\n\nterminal\n\nssb-project build"
  },
  {
    "objectID": "statistikkere/nytt-ssbproject.html#rydd-opp-etter-deg",
    "href": "statistikkere/nytt-ssbproject.html#rydd-opp-etter-deg",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "Det vil vÃ¦re tilfeller hvor man Ã¸nsker Ã¥ slette et ssb-project, enten fordi man ikke trenger koden lenger eller fordi man bare testet litt.\n\n\nHvis man jobber med flere prosjekter sÃ¥ kan det fort bli mange Jupyter kerneler hengende igjen. Derfor er det ogsÃ¥ mulighet Ã¥ kjÃ¸re\n\n\nterminal\n\nssb-project clean stat-testprod\n\nsom sletter Jupyter-kernelen og de installerte pakkene i prosjektet. Hvis du ogsÃ¥ Ã¸nsker Ã¥ slette selve mappen med kode mÃ¥ du gjÃ¸re det manuelt4:\n\n\nterminal\n\nrm -rf ~/stat-testprod/\n\nProsjektmappen over lÃ¥ direkte i hjemmemappen min og hjemmemappen pÃ¥ Linux kan alltid referes til med et tilda-tegn ~.\n\n\n\nGitHub-repoer som er opprettet under SSB sin organinasjons statisticsnorway pÃ¥ GitHub kan ikke slettes, bare arkiveres. Grunnen er at hvis man oppdager en sÃ¥rbarhet senere sÃ¥ er det viktig Ã¥ kunne se repoet for Ã¥ forstÃ¥ hva som har skjedd.\nHvis du ikke trenger et GitHub-repo lenger kan man arkivere repoet. Det gjÃ¸r du pÃ¥ fÃ¸lgende mÃ¥te:\n\nGi inn i repoet Settings slik som vist med rÃ¸d pil i FigurÂ 4.\n\n\n\n\n\n\n\nFigurÂ 4: Settings for repoet.\n\n\n\n\nUnder General scroller du deg ned til delen som heter Danger Zone og velger Archive this repository, slik som vist pÃ¥ FigurÂ 5.\n\n\n\n\n\n\n\nFigurÂ 5: Arkivering av et repo.\n\n\n\n\nI dialogboksen som dukker opp fyller du inn reponavnet som beskrevet og trykker pÃ¥ I understand the consequences, archive this repository, som vist i FigurÂ 6.\n\n\n\n\n\n\n\nFigurÂ 6: Bekreftelse av arkiveringen.\n\n\n\nNÃ¥r det er gjort sÃ¥ er repoet lesbart, men man kan ikke jobbe med det. Men som vi ser av @#fig-github-repo-settings-archive-warning kan man omgjÃ¸re arkiveringen senere hvis det skulle vÃ¦re Ã¸nskelig."
  },
  {
    "objectID": "statistikkere/nytt-ssbproject.html#hva-med-r",
    "href": "statistikkere/nytt-ssbproject.html#hva-med-r",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "Vi har forelÃ¸pig ikke integret R i ssb-project. Grunnen er at det mest populÃ¦re virtuelle miljÃ¸-verktÃ¸et for R, renv, kun tilbyr Ã¥ passe pÃ¥ versjoner av R-pakker og ikke selve R-installasjonen. Det er en svakhet som trolig gjÃ¸r det vanskeligere enn nÃ¸dvendig Ã¥ gjenskape tidligere publiserte resultater med ssb-project. I tillegg klarer den ikke Ã¥ gjenkjenne pakker som blir brukt i ipynb-filer.\nPlanen er Ã¥ finne et annet verktÃ¸y enn renv som kan ogsÃ¥ reprodusere R-versjonen. Team Statistikktjenester ser nÃ¦rmere pÃ¥ hvilke alternativer som finnes og vil tilby noe i fremtiden.\nI mellomtiden kan man bruke renv slik det er beskrevet her for skymiljÃ¸et, og med denne modifiseringen for bakkemiljÃ¸et."
  },
  {
    "objectID": "statistikkere/nytt-ssbproject.html#footnotes",
    "href": "statistikkere/nytt-ssbproject.html#footnotes",
    "title": "Nytt ssb-project",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nCLI = Command-Line-Interface. Dvs. et program som er skrevet for Ã¥ brukes terminalen ved hjelp av enkle kommandoer.â†©ï¸\nFiler og mapper som starter med punktum er skjulte med mindre man ber om Ã¥ se dem. I Jupyterlab kan disse vises i filutforskeren ved Ã¥ velge View fra menylinjen, og deretter velge Show hidden files. I en terminal skriver man ls -a for Ã¥ se de.â†©ï¸\nÃ… pushe til GitHub uten Ã¥ sende ved Personal Access Token fordrer at du har lagret det lokalt sÃ¥ Git kan finne det. Her et eksempel pÃ¥ hvordan det kan gjÃ¸res.â†©ï¸\nDette kan ogsÃ¥ gjÃ¸res ved Ã¥ hÃ¸yreklikke pÃ¥ mappen i Jupyterlab sin filutforsker og velge Delete.â†©ï¸"
  },
  {
    "objectID": "statistikkere/lage-nettsider.html",
    "href": "statistikkere/lage-nettsider.html",
    "title": "Lage nettsider",
    "section": "",
    "text": "Lage nettsider"
  },
  {
    "objectID": "statistikkere/jit.html",
    "href": "statistikkere/jit.html",
    "title": "Just-in-Time Access",
    "section": "",
    "text": "Just-in-Time Access (JIT) er en applikasjon der data-admins kan gi seg selv midlertidig tilgang til kildedata. PÃ¥ Dapla benyttes for dette for Ã¥ unngÃ¥ at data-admins har permanent tilgang til sensitive data, siden all kildedata er definert som sensitiv informasjon. data-admins mÃ¥ bruke JIT-applikasjonen for Ã¥ gi seg selv korte, begrunnede tilganger til kildedata ved behov. Den som er ansvarlig for teamet kan monitere i hvilken grad teamets data-admins benytter seg av denne muligheten.",
    "crumbs": [
      "Statistikere",
      "Appendix",
      "Just-in-Time Access"
    ]
  },
  {
    "objectID": "statistikkere/jit.html#hvordan-bruke-jit",
    "href": "statistikkere/jit.html#hvordan-bruke-jit",
    "title": "Just-in-Time Access",
    "section": "Hvordan bruke JIT",
    "text": "Hvordan bruke JIT\nFor Ã¥ bruke JIT mÃ¥ du vÃ¦re data-admin for teamet som eier kildedataene du Ã¸nsker tilgang til. Du aktiverer tilganger i JIT-appen ved gjÃ¸re fÃ¸lgende:\n\nGÃ¥ inn pÃ¥ nettsiden https://jitaccess.dapla.ssb.no/1\nOppgi Prosjekt-id for prosjektet der kildedataene du Ã¸nsker tilgang til er lagret.\nHuk av hvilke tilgangsroller du Ã¸nsker aktivert. Se beskrivelse av roller i TabellÂ 1.\nVelg hvor lenge tilgangen du Ã¸nsker at tilgangen skal vare. Den kan maksimalt vare i 8 timer.\nOppgi en begrunnelse for at aktiverer tilgangen.\nTil slutt trykker du pÃ¥ Request access.\n\nTilgangen vil deretter vÃ¦re aktiv ila. noen minutter.\n\n\n\n\n\n\nHvordan bÃ¸r begrunnelsen skrives?\n\n\n\nPrÃ¸v Ã¥ gjÃ¸r begrunnelsen forstÃ¥elig for andre enn deg selv pÃ¥ det tidspunktet. Den som er ansvarlig for teamet skal kunne forstÃ¥ begrunnelsen nÃ¥r de ser pÃ¥ loggene. I tillegg vil sentralt i SSB monitere i hvilken grad Dapla-teamene benytter seg av tilgangene.\n\n\n\n\n\nTabellÂ 1: De mest relevante tilgangene som kan aktiveres i JIT-applikasjonen\n\n\n\n\n\n\n\n\n\nRoller\nHva?\n\n\n\n\nssb.buckets.list\nListe ut bÃ¸ttene i kildeprosjektet.\n\n\nstoragetransfer.admin\nSette opp jobb med Transfer Service i kildeprosjektet.\n\n\nssb.bucket.write\nLese og skrive til kildebÃ¸tta.\n\n\n\n\n\n\nSkal du sette opp en Transfer Service overfÃ¸ring med kildedata sÃ¥ mÃ¥ du aktivere ssb.bucket.write og storagetransfer.admin. SKal du skrive og lese fra et miljÃ¸ som Jupyter, sÃ¥ mÃ¥ du aktivere rollene ssb.bucket.write og ssb.buckets.list.",
    "crumbs": [
      "Statistikere",
      "Appendix",
      "Just-in-Time Access"
    ]
  },
  {
    "objectID": "statistikkere/jit.html#footnotes",
    "href": "statistikkere/jit.html#footnotes",
    "title": "Just-in-Time Access",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nHvis du jobber fra hjemmekontor sÃ¥ mÃ¥ du vÃ¦re pÃ¥ VPN for Ã¥ fÃ¥ tilgang til JIT-appen.â†©ï¸",
    "crumbs": [
      "Statistikere",
      "Appendix",
      "Just-in-Time Access"
    ]
  },
  {
    "objectID": "statistikkere/opprette-dapla-team.html",
    "href": "statistikkere/opprette-dapla-team.html",
    "title": "Opprette Dapla-team",
    "section": "",
    "text": "Opprette Dapla-team\nFor Ã¥ komme i gang med Ã¥ opprette et Dapla-team trengs det en oversikt over teamets medlemmer og hvilke tilgangsgrupper medlemmene skal vÃ¦re med i. Det trengs ogsÃ¥ informasjon om hvilke Dapla-tjenester som er aktuelle for teamet Ã¥ ta i bruk. Derfor har det blitt opprettet en egen veileder for dette kalt Dapla Start.\n\n\n\n\n\n\nGÃ¥ til Dapla Start for starte bestilling av et nytt Dapla-team.\n\n\n\nNÃ¥r teamet er opprettet fÃ¥r alle medlemmene tilgang til sitt eget prosjekt i Google Cloud Platform (GCP), som er SSBs leverandÃ¸r av skytjenester. Videre fÃ¥r hvert prosjekt et sett med tjenester og tilganger som knyttes til teamet. Det opprettes ogsÃ¥ datalagringsomrÃ¥der (kalt bÃ¸tter) som bare kan aksesseres av brukere som er med i teamets tilgangsgrupper.\nDapla-teamet vil ogsÃ¥ fÃ¥ sin egen gruppe i SSBs Active Directory slik at medlemskapet i gruppen kan administreres av Kundeservice."
  },
  {
    "objectID": "statistikkere/gcc.html",
    "href": "statistikkere/gcc.html",
    "title": "Google Cloud Console (GCC)",
    "section": "",
    "text": "Google Cloud Console (GCC) er et web-basert grensesnitt for Ã¥ administrere ressurser og tjenester pÃ¥ Google Cloud Platform (GCP). Alle i SSB kan logge seg inn i GCC med sin SSB-bruker. Dapla-team har sjelden mulighet til Ã¥ opprette nye ressurser fra dette grensesnittet, siden vi Ã¸nsker at det skal gjÃ¸res med kode. Men det er likevel et nyttig verktÃ¸y for Ã¥ se pÃ¥ ressurser og gjÃ¸re endringer pÃ¥ eksisterende ressurser. I SSB bruker bruker vi GCC hovedsakelig til fÃ¸lgende:",
    "crumbs": [
      "Statistikere",
      "Datatjenester",
      "Google Cloud Console (GCC)"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html#innlogging",
    "href": "statistikkere/gcc.html#innlogging",
    "title": "Google Cloud Console (GCC)",
    "section": "Innlogging",
    "text": "Innlogging\nFor Ã¥ logge inn i GCC sÃ¥ gjÃ¸r du fÃ¸lgende:\n\nÃ…pne Google Cloud Console i en nettleser.\nLogg in med din SSB-bruker.\n\nHvis du ogsÃ¥ har en privat Google-konto som benyttes i samme nettleser, mÃ¥ du noen ganger passe pÃ¥ at du er logget inn med riktig konto. Dette kan du sjekke ved Ã¥ trykke pÃ¥ profilbildet ditt Ã¸verst til hÃ¸yre i GCC. Hvis du ikke er logget inn med riktig konto, sÃ¥ trykker du pÃ¥ Logg ut og logger inn pÃ¥ nytt med riktig konto.",
    "crumbs": [
      "Statistikere",
      "Datatjenester",
      "Google Cloud Console (GCC)"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html#prosjektvelger",
    "href": "statistikkere/gcc.html#prosjektvelger",
    "title": "Google Cloud Console (GCC)",
    "section": "Prosjektvelger",
    "text": "Prosjektvelger\nEtter at du har logget deg pÃ¥ med din SSB-bruker, sÃ¥ mÃ¥ du velge hvilket av ditt teams prosjekter du Ã¸nsker Ã¥ jobbe med. Dette gjÃ¸r du ved Ã¥ trykke pÃ¥ prosjektvelgeren Ã¸verst til venstre pÃ¥ siden. Vidoen under viser hvordan du velger et prosjekt og lister ut hvilke bÃ¸tter som finnes i prosjektet.",
    "crumbs": [
      "Statistikere",
      "Datatjenester",
      "Google Cloud Console (GCC)"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html#filutforsker",
    "href": "statistikkere/gcc.html#filutforsker",
    "title": "Google Cloud Console (GCC)",
    "section": "Filutforsker",
    "text": "Filutforsker\nFor Ã¥ utforske bÃ¸tter og filer i et Dapla-team sitt Google-prosjekt sÃ¥ kan man bruke Cloud Storage-grensesnittet i GCC. For Ã¥ bruke denne funksjonaliteten gjÃ¸r du fÃ¸lgende:\n\nBruk prosjektvelgeren til Ã¥ velge Ã¸nsket prosjekt.\nDeretter sÃ¸ker du opp Google Storage i sÃ¸kefeltet Ã¸verst pÃ¥ siden.\n\nDa fÃ¥r du en oversikt over alle bÃ¸ttene i prosjektet. Velg Ã¸nsker bÃ¸tte for Ã¥ utforske innholdet i bÃ¸tta.",
    "crumbs": [
      "Statistikere",
      "Datatjenester",
      "Google Cloud Console (GCC)"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html#logger",
    "href": "statistikkere/gcc.html#logger",
    "title": "Google Cloud Console (GCC)",
    "section": "Logger",
    "text": "Logger\nKommer snart.",
    "crumbs": [
      "Statistikere",
      "Datatjenester",
      "Google Cloud Console (GCC)"
    ]
  },
  {
    "objectID": "statistikkere/gcc.html#transfer-service",
    "href": "statistikkere/gcc.html#transfer-service",
    "title": "Google Cloud Console (GCC)",
    "section": "Transfer Service",
    "text": "Transfer Service\nLes mer om hvordan man bruker Transfer Service her.",
    "crumbs": [
      "Statistikere",
      "Datatjenester",
      "Google Cloud Console (GCC)"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html",
    "href": "statistikkere/hva-er-dapla-team.html",
    "title": "Hva er Dapla-team?",
    "section": "",
    "text": "For Ã¥ kunne jobbe pÃ¥ Dapla mÃ¥ man vÃ¦re en del av et Dapla-team. Et Dapla-team er en gruppe personer som har tilgang til spesifikke ressurser pÃ¥ Dapla. Ressursene kan vÃ¦re data, kode eller tjenester. FÃ¸lgelig er teamet helt sentral for tilgangsstyringen pÃ¥ Dapla. Derfor er det viktig at alle som jobber pÃ¥ Dapla gjÃ¸r seg godt kjent med innholdet i dette delen.",
    "crumbs": [
      "Statistikere",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#opprette-dapla-team",
    "href": "statistikkere/hva-er-dapla-team.html#opprette-dapla-team",
    "title": "Hva er Dapla-team?",
    "section": "Opprette Dapla-team",
    "text": "Opprette Dapla-team\nDapla-team opprettes av dataeier for teamets kildedata. I de fleste tilfeller vil dette vÃ¦re en seksjonsleder i SSB. Selve opprettelsen av teamet gjÃ¸res i Manager-portalen.\n\n\n\n\n\n\nManager-portalen er under arbeid\n\n\n\nManager-portalen er under arbeid og vil vÃ¦re klar i 1. kvartal 2024. I mellomtiden kan man bruke den gamle portalen for Ã¥ opprette Dapla-team.",
    "crumbs": [
      "Statistikere",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#roller-i-teamet",
    "href": "statistikkere/hva-er-dapla-team.html#roller-i-teamet",
    "title": "Hva er Dapla-team?",
    "section": "Roller i teamet",
    "text": "Roller i teamet\nMedlemskap i et Dapla-team gir tilgang pÃ¥ spesifikke ressurser pÃ¥ Dapla. Men siden kildedataene til alle team er klassifisert som sensitive, sÃ¥ kan ikke alle pÃ¥ teamet ha lik tilgang til alle ressurser. Av den grunn er det definert 3 ulike roller pÃ¥ et team. To av disse, data-admins og developers, er forbeholdt de som jobber med data pÃ¥ teamet. Mens den tredje, managers, skal innehas av de som er ansvarlige for teamet. I de fleste tilfeller vil managers vÃ¦re seksjonslederen som er ansvarlige for statistikkproduktene teamet leverer. Under forklarer vi nÃ¦rmere hva de ulike rollene innebÃ¦rer.\n\nManagers\nRollen managers skal bestÃ¥ av en eller flere data-ansvarlige (ofte omtalt som data-eiere eller seksjonsledere). managers har ansvar for fÃ¸lgende i teamet:\n\nopprette teamet\nhvem i teamet som fÃ¥r tilgang til hvilke data og tjenester.\nat teamet fÃ¸lger SSBs retningslinjer for tilgangsstyring.\nat teamet fÃ¸lger SSBs retningslinjer for klassifisering av data.\nvedlikehold og monitorering av tilganger.\nat teamet fÃ¸lger og forstÃ¥r hvordan sensitive data skal behandles i SSB.\n\nManager-rollen krever ingen tilgang til data eller databehandlende tjenester pÃ¥ Dapla.\n\n\nData-admins\nRollen data-admins er en priveligert rolle blant de som jobber med data i teamet. Rollen skal kun tildeles 2-3 personer pÃ¥ et team. data-admins har tilgang til samme data og ressurser som developers, med fÃ¸lgende unntak:\n\nde er forhÃ¥ndsgodkjent til Ã¥ gi seg selv tidsbegrenset tilgang til kildedata ved behov. Tilgang til kildedata skal kun aktiveres i sÃ¦rskilte tilfeller, der eneste lÃ¸sning er Ã¥ se pÃ¥ data i klartekst. Tilgang til kildedata skal kun gis i en begrenset periode, og krever en skriftlig begrunnelse. managers skal lett kunne monitere hvem som aktiverer denne tilgangen og hvor ofte.\nde kan godkjenne endringer i automatiske jobber som prosesserer kildedata til inndata.\nde kan overfÃ¸re kildedata mellom bakke og sky.\n\n\n\nDevelopers\nRollen developers er den mest vanlige rollen pÃ¥ et Dapla-team. Denne rollen skal tildeles alle som jobber med data i teamet som ikke har data-admins-rollen. developers har tilgang til fÃ¸lgende ressurser:\n\nalt av teamets data, med unntak av kildedata.\nalle ressurser som behandler datatilstandene fra inndata til utdata.",
    "crumbs": [
      "Statistikere",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#ressurser",
    "href": "statistikkere/hva-er-dapla-team.html#ressurser",
    "title": "Hva er Dapla-team?",
    "section": "Ressurser",
    "text": "Ressurser\nNÃ¥r du oppretter et dapla-team sÃ¥ fÃ¥r man en grunnpakke med ressurser som de fleste i SSB vil trenge for Ã¥ kunne jobbe med data pÃ¥ Dapla. I tillegg kan teamet selvbetjent skru pÃ¥ andre tjenester hvis man Ã¸nsker det. I det fÃ¸lgende forklarer vi hva som er inkludert i grunnpakken, og hva som er tilgjengelig for Ã¥ skru pÃ¥ ved behov.\n\nGrunnpakken\nFigurÂ 1 viser et overordnet bilde av hvilke ressurser som er inkludert i â€œgrunnpakkenâ€. Et Dapla-team fÃ¥r et testmiljÃ¸ og prodmiljÃ¸. Det er i prodmiljÃ¸et at man jobber med skarpe data, mens testmiljÃ¸et er forbeholdt arbeid med testdata. I hvert miljÃ¸ fÃ¥r teamet to Google-prosjekter. Ett for kildedata og et for datatilstandene inndata, klargjorte data, statistikkdata og utdata. Sistnevnte prosjekt kaller vi for standardprosjektet, siden det er her mesteparten av databehandlingen skjer.\n\n\n\n\n\n\nFigurÂ 1: Diagram over hvilke miljÃ¸er, Google-prosjekter og bÃ¸tter et Dapla-team som et fÃ¥r ved opprettelse.\n\n\n\nAv FigurÂ 1 ser vi at prosjektene i prodmiljÃ¸et fÃ¥r noen flere bÃ¸tter enn prosjektene i testmiljÃ¸et. Disse ekstrabÃ¸ttene er forbeholdt synkronisering av data mellom bakke og sky, noe vi ikke legger til rette for i testmiljÃ¸et1. Les mer om overfÃ¸ring av data mellom bakke og sky her.\nRessursene som opprettes for et Dapla-team reflekterer i stor grad at kildedata er klassifisert som sensitive. Dette er grunnen til at det opprettes et eget prosjekt for kildedata, og at det kun er data-admins som potensielt kan fÃ¥ tilgang til dataene her. Opprettelsen av et eget testmiljÃ¸ skyldes at Dapla-team i stÃ¸rre grad enn fÃ¸r forventes Ã¥ jobbe med testdata istedenfor skarpe data.\nAlle ressursene som opprettes for teamet er definert i tekstfiler i et GitHub-repo. Dette repoet kaller vi for et IaC-repo (Infrastructure as Code). IaC-repoet er en del av grunnpakken, og er tilgjengelig for alle pÃ¥ teamet. Statistikkere trenger ikke Ã¥ forholde seg til dette repoet i stor grad, med unntak av nÃ¥r de skal aktivere/deaktivere features og nÃ¥r de skal sette opp Kildomaten.\n\n\nFeatures\nI tillegg til grunnpakken med ressurser, sÃ¥ kan teamet selvbetjent skru pÃ¥ fÃ¸lgende features eller tjenester ved behov:\n\nTransfer Service kan skrus pÃ¥ hvis teamet trenger Ã¥ synkronisere data mellom ulike lagringssystemer. For eksempel mellom bakke og sky, eller mellom to ulike skytjenester.\nKildomaten kan skrus pÃ¥ hvis teamet trenger Ã¥ automatisere overgangen fra kildedata til inndata.\n\nForelÃ¸pig er det kun disse to features som er tilgjengelig. Det vil komme flere etterhvert som behovene melder seg.\nLes mer om features her.",
    "crumbs": [
      "Statistikere",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#github-team",
    "href": "statistikkere/hva-er-dapla-team.html#github-team",
    "title": "Hva er Dapla-team?",
    "section": "GitHub-team",
    "text": "GitHub-team\nVed opprettelsen av et Dapla-team sÃ¥ blir det ogsÃ¥ opprettet et tilsvarende GitHub-team med samme navn som Dapla-teamet. Grunnen til at det blir opprettet et GitHub-team er at GitHub er en sentral del av Dapla. Alle ressurser som skal opprettes pÃ¥ plattformen defineres av GitHub-repoer, og vi Ã¸nsker at tilganger her ogsÃ¥ skal reflektere tilgangene pÃ¥ Dapla.\nFor eksempel vil et team med navnet dapla-example fÃ¥ et GitHub-team med navnet dapla-example. Alle som er medlem av Dapla-teamet vil automatisk bli medlem av GitHub-teamet. I tillegg vil gruppetilhÃ¸righet og tilgangsroller pÃ¥ GitHub-teamet reflektere tilgangsroller pÃ¥ Dapla-teamet. For eksempel sÃ¥ kan dapla-example-data-admins gis tilgang til repo, og da vil alle som er medlem av Dapla-teamet med rollen data-admins fÃ¥ tilgang til repoet. Dette benyttes blant annet for Ã¥ gi teamet tilgang til automation-mappen i sitt IaC-repo. I tillegg kan teamet bruke GitHub-teamet til Ã¥ gi tilgang til andre GitHub-repoer som er relevante for teamet, for eksempel kodenbasen til en statistikkproduksjon eller lignende. Fordelen er at tilganger er gitt pÃ¥ teamnivÃ¥ og ikke pÃ¥ personnivÃ¥. For eksempel hvis manager for teamet fjerner en ansatt fra developers-gruppa, sÃ¥ mister de all tilgang til data, tjenester og kode pÃ¥ GitHub som er tilgjengelig for developers.",
    "crumbs": [
      "Statistikere",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#navnestruktur",
    "href": "statistikkere/hva-er-dapla-team.html#navnestruktur",
    "title": "Hva er Dapla-team?",
    "section": "Navnestruktur",
    "text": "Navnestruktur\nNÃ¥r du oppretter et Dapla-team sÃ¥ mÃ¥ du velge et navn pÃ¥ teamet. Teamet velger selv et navn som reflekterer domene og subdomene. For eksempel kan et team som jobber med statistikkproduksjonen skattestatistikk for nÃ¦ringslivet velge Ã¥ kalle teamet Skatt nÃ¦ring. Hvis vi bruker dette teamet som et eksempel, sÃ¥ vil det fÃ¥ opprettet et teknisk navn som fÃ¸lger denne strukturen: skatt-naering. Dette navnet er det som brukes i tekniske sammenhenger, for eksempel som navn pÃ¥ GitHub-teamet, IaC-repoet, Google-prosjektene og bÃ¸ttene. TabellÂ 1 viser en tabell over hvordan ressursene for dette teamet vil se ut:\n\n\n\nTabellÂ 1: Navnestruktur for teamet Skatt nÃ¦ring sine ressurser\n\n\n\n\n\n\n\n\n\nNavn\nBeskrivelse\n\n\n\n\nskatt-naering\nTeknisk teamnavn\n\n\nskatt-naering-managers\nAD-gruppe for managers\n\n\nskatt-naering-data-admins\nAD-gruppe for data-admins og et GitHub-team\n\n\nskatt-naering-developers\nAD-gruppe for developers og et GitHub-team\n\n\nskatt-naering-kilde-p\nNavn pÃ¥ kildeprosjekt i prod\n\n\nskatt-naering-p\nNavn pÃ¥ standardprosjekt i prod\n\n\nskatt-naering-kilde-t\nNavn pÃ¥ kildeprosjekt i test\n\n\nskatt-naering-t\nNavn pÃ¥ standardprosjekt i test\n\n\n\n\n\n\nI TabellÂ 1 ser vi at teamet fÃ¥r opprettet 3 AD-grupper og 4 Google-prosjekter. AD-gruppene brukes til Ã¥ gi tilgang til ressursene pÃ¥ Dapla, mens Google-prosjektene brukes til Ã¥ organisere ressursene. I tillegg er det en fast navnestruktur for bÃ¸ttene i hvert prosjekt, slikt som vist i TabellÂ 2.\n\n\n\nTabellÂ 2: Navnestruktur for teamet Skatt nÃ¦ring sine bÃ¸tter\n\n\n\n\n\nProsjektnavn\nBÃ¸ttenavn\n\n\n\n\nskatt-naering-kilde-p\nssb-skatt-naering-data-kilde-prod\n\n\n\nssb-skatt-naering-data-kilde-frasky-prod\n\n\n\nssb-skatt-naering-data-kilde-tilsky-prod\n\n\nskatt-naering-p\nssb-skatt-naering-data-produkt-prod\n\n\n\nssb-skatt-naering-data-frasky-prod\n\n\n\nssb-skatt-naering-data-tilsky-prod\n\n\nskatt-naering-kilde-t\nssb-skatt-naering-data-kilde-test\n\n\nskatt-naering-t\nssb-skatt-naering-data-produkt-test",
    "crumbs": [
      "Statistikere",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-dapla-team.html#footnotes",
    "href": "statistikkere/hva-er-dapla-team.html#footnotes",
    "title": "Hva er Dapla-team?",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nTa kontakt med produkteier for Dapla hvis du trenger Ã¥ synkronisere testdata mellom bakke og skyâ†©ï¸",
    "crumbs": [
      "Statistikere",
      "Dapla-team"
    ]
  },
  {
    "objectID": "statistikkere/orkestrering.html",
    "href": "statistikkere/orkestrering.html",
    "title": "Orkestrering",
    "section": "",
    "text": "Orkestrering"
  },
  {
    "objectID": "statistikkere/innlogging.html",
    "href": "statistikkere/innlogging.html",
    "title": "Innlogging",
    "section": "",
    "text": "Innlogging pÃ¥ Dapla er veldig enkelt. Dapla er en nettadresse som alle SSB-ere kan gÃ¥ inn pÃ¥ hvis de er logget pÃ¥ SSB sitt nettverk. Ã… vÃ¦re logget pÃ¥ SSB sitt nettverk betyr i denne sammenhengen at man er logget pÃ¥ med VPN, enten man er pÃ¥ kontoret eller pÃ¥ hjemmekontor. For Ã¥ gjÃ¸re det enda enklere har vi laget en fast snarvei til denne nettadressen pÃ¥ vÃ¥rt intranett/ByrÃ¥nettet(se FigurÂ 1).\n\n\n\n\n\n\nFigurÂ 1: Snarvei til Dapla fra intranett\n\n\n\nMen samtidig som det er lett Ã¥ logge seg pÃ¥, sÃ¥ er det noen kompliserende ting som fortjener en forklaring. Noe skyldes at vi mangler et klart sprÃ¥k for Ã¥ definere bakkemiljÃ¸et og skymiljÃ¸et slik at alle skjÃ¸nner hva man snakker om. I denne boken definerer bakkemiljÃ¸et som stedet der man har drevet med statistikkproduksjon de siste tiÃ¥rene. SkymiljÃ¸et er den nye dataplattformen Dapla pÃ¥ Google Cloud.\nDet som gjÃ¸r ting litt komplisert er at vi har 2 Jupyter-miljÃ¸er pÃ¥ bÃ¥de bakke og sky. Ã…rsaken er at vi har ett test- og ett prod-omrÃ¥de for hver, og det blir i alt 4 Jupyter-miljÃ¸er. FigurÂ 2 viser dette.\n\n\n\n\n\n\nFigurÂ 2: De 4 Jupyter-miljÃ¸ene i SSB. Et test-miljÃ¸ og et prod-miljÃ¸ pÃ¥ bakke og sky/Dapla\n\n\n\nHver av disse miljÃ¸ene har sin egen nettadresse og sitt eget bruksomrÃ¥de.\n\n\nI de fleste tilfeller vil en statistikker eller forsker Ã¸nske Ã¥ logge seg inn i prod-miljÃ¸et. Det er her man skal kjÃ¸re koden sin i et produksjonslÃ¸p som skal publiseres eller utvikles. I noen tilfeller hvor man ber om Ã¥ fÃ¥ tilgjengliggjort en ny tjeneste sÃ¥ vil denne fÃ¸rst rulles ut i testomrÃ¥det som vi kaller staging-omrÃ¥det. Ã…rsaken er at vi Ã¸nsker Ã¥ beskytte prod-miljÃ¸et fra software som potensielt Ã¸delegger for eksisterende funksjonalitet. Derfor ruller vi ut nye ting i staging fÃ¸rst. Av den grunn vil de fleste oppleve Ã¥ bli bedt om Ã¥ logge seg inn der for testing en eller annen gang. Under forklarer vi hvordan man gÃ¥r frem for Ã¥ logge seg pÃ¥ de to ulike miljÃ¸ene pÃ¥ Dapla.\n\n\nFor Ã¥ logge seg inn inn i prod-miljÃ¸et pÃ¥ Dapla kan man gjÃ¸re fÃ¸lgende:\n\nGÃ¥ inn pÃ¥ lenken https://jupyter.dapla.ssb.no/ i en Chrome-nettleser eller klikk pÃ¥ lenken pÃ¥ ByrÃ¥nettet som vist i FigurÂ 1.\nAlle i SSB har en Google Cloud-konto som mÃ¥ brukes nÃ¥r man logger seg pÃ¥ Dapla. Brukernavnet i Google er det samme som din korte epost-adresse (f.eks. cth@ssb.no). Hvis du ikke allerede er logget inn i Google vil du fÃ¥ spÃ¸rsmÃ¥l om Ã¥ velge hvilken Google-konto som skal brukes (FigurÂ 3). Logg inn med din Google-konto (ssb.no) og ditt AD-passord.\n\n\n\n\n\n\n\nFigurÂ 3: Velg en Google-konto\n\n\n\n\nDeretter blir man spurt om man godtar at ssb.no (altsÃ¥ Dapla) kan bruke din Google Cloud-konto (FigurÂ 4). Trykk Allow.\n\n\n\n\n\n\n\nFigurÂ 4: Tillat at ssb.no fÃ¥r bruke din Google Cloud-konto\n\n\n\n\nDeretter lander man pÃ¥ en side som lar deg avgjÃ¸re hvor mye maskinkraft som skal holdes av til deg (FigurÂ 5). Det Ã¸verste alternativet er valgt som standard, og er tilstrekkelig for de fleste.\n\n\n\n\n\n\n\nFigurÂ 5: Velg hvor mye maskinkraft du trenger\n\n\n\n\nVent til maskinen din starter opp (FigurÂ 6). Oppstartstiden kan variere.\n\n\n\n\n\n\n\nFigurÂ 6: Starter opp Jupyter\n\n\n\nEtter dette er man logget inn i et Jupyter-miljÃ¸ som kjÃ¸rer pÃ¥ en minimal Ubuntu-maskin. Hvis man er del av et Dapla-team fÃ¥r man ogsÃ¥ tilgang til alt teamet har tilgang til.\n\n\n\nInnlogging til staging-miljÃ¸et er identisk med innloggingen til prod-miljÃ¸et, med ett viktig unntak: nettadressen er nÃ¥ https://jupyter.dapla-staging.ssb.no/.\nLitt mer om hva som er tilgjenglig her kommer.\n\n\n\n\n\nNoen ganger kan man komme i en situasjon hvor lÃ¸sningen for Single Sign-On (pÃ¥logging pÃ¥ tvers av flere systemer) gir en feilmelding a la FigurÂ 7:\n\n\n\n\n\n\nFigurÂ 7: Feil som kan oppstÃ¥ ved pÃ¥logging\n\n\n\nI denne situasjonen mÃ¥ man trykke pÃ¥ knappen â€œAdd to existing accountâ€. Da vil skjermbildet FigurÂ 8 dukke opp:\n\n\n\n\n\n\nFigurÂ 8: Klikk pÃ¥ Google-knappen for Ã¥ logge pÃ¥ igjen\n\n\n\nHer mÃ¥ man tykke pÃ¥ Google-knappen (se pil), og deretter logge inn som vist i FigurÂ 3 tidligere i dette avsnittet.\n\n\n\n\n\nJupyter-miljÃ¸et pÃ¥ bakken bruker samme base-image1 for Ã¥ installere Jupyterlab, og er derfor identisk pÃ¥ mange mÃ¥ter. Men innloggingen er ganske forskjellig.\n\n\n\n\n\n\nNote\n\n\n\nFom. 5. desember 2022 har vi byttet ut Jupyter-miljÃ¸et pÃ¥ bakken. Beskrivelsene under gjelder derfor det nye miljÃ¸et. Fram til 15. januar vil du kunne bruke det gamle miljÃ¸et ved Ã¥ gÃ¥ inn pÃ¥ lenken https://jupyter-prod.ssb.no/ manuelt i Google Chrome. Etter 15. januar blir det gamle Jupyter-miljÃ¸et avviklet.\n\n\n\n\nDu logger deg inn pÃ¥ prod i bakkemiljÃ¸et pÃ¥ fÃ¸lgende mÃ¥te:\n\nLogg deg inn pÃ¥ Citrix-Windows i bakkemiljÃ¸et. Det kan gjÃ¸res ved Ã¥ bruke lenken Citrix pÃ¥ ByrÃ¥nettet, som ogsÃ¥ vises i FigurÂ 1.\nTrykk pÃ¥ Jupyterlab-ikonet, som vist pÃ¥ FigurÂ 9, og logg deg inn med vanlig brukernavn og passord.\n\n\n\n\n\n\n\nFigurÂ 9: Jupyterlab-ikon pÃ¥ Skrivebordet i Citrix-Windows.\n\n\n\nNÃ¥r du trykker pÃ¥ ikonet blir du tatt til nettadressen https://sl-jupyter-p.ssb.no/. Du kunne ogsÃ¥ Ã¥pnet Jupyterlab ved Ã¥pne Chrome-nettleseren og skrive inn adressen manuelt.\n\n\n\nInnlogging til staging-miljÃ¸et har ingen snarvei pÃ¥ Skrivebordet, og du mÃ¥ gjÃ¸re fÃ¸lgende for Ã¥ Ã¥pne miljÃ¸et:\n\nÃ…pne Chrome-nettleseren i Citrix-Windows.\nSkriv inn url-en https://sl-jupyter-t.ssb.no/",
    "crumbs": [
      "Statistikere",
      "Appendix",
      "Innlogging"
    ]
  },
  {
    "objectID": "statistikkere/innlogging.html#dapla",
    "href": "statistikkere/innlogging.html#dapla",
    "title": "Innlogging",
    "section": "",
    "text": "I de fleste tilfeller vil en statistikker eller forsker Ã¸nske Ã¥ logge seg inn i prod-miljÃ¸et. Det er her man skal kjÃ¸re koden sin i et produksjonslÃ¸p som skal publiseres eller utvikles. I noen tilfeller hvor man ber om Ã¥ fÃ¥ tilgjengliggjort en ny tjeneste sÃ¥ vil denne fÃ¸rst rulles ut i testomrÃ¥det som vi kaller staging-omrÃ¥det. Ã…rsaken er at vi Ã¸nsker Ã¥ beskytte prod-miljÃ¸et fra software som potensielt Ã¸delegger for eksisterende funksjonalitet. Derfor ruller vi ut nye ting i staging fÃ¸rst. Av den grunn vil de fleste oppleve Ã¥ bli bedt om Ã¥ logge seg inn der for testing en eller annen gang. Under forklarer vi hvordan man gÃ¥r frem for Ã¥ logge seg pÃ¥ de to ulike miljÃ¸ene pÃ¥ Dapla.\n\n\nFor Ã¥ logge seg inn inn i prod-miljÃ¸et pÃ¥ Dapla kan man gjÃ¸re fÃ¸lgende:\n\nGÃ¥ inn pÃ¥ lenken https://jupyter.dapla.ssb.no/ i en Chrome-nettleser eller klikk pÃ¥ lenken pÃ¥ ByrÃ¥nettet som vist i FigurÂ 1.\nAlle i SSB har en Google Cloud-konto som mÃ¥ brukes nÃ¥r man logger seg pÃ¥ Dapla. Brukernavnet i Google er det samme som din korte epost-adresse (f.eks. cth@ssb.no). Hvis du ikke allerede er logget inn i Google vil du fÃ¥ spÃ¸rsmÃ¥l om Ã¥ velge hvilken Google-konto som skal brukes (FigurÂ 3). Logg inn med din Google-konto (ssb.no) og ditt AD-passord.\n\n\n\n\n\n\n\nFigurÂ 3: Velg en Google-konto\n\n\n\n\nDeretter blir man spurt om man godtar at ssb.no (altsÃ¥ Dapla) kan bruke din Google Cloud-konto (FigurÂ 4). Trykk Allow.\n\n\n\n\n\n\n\nFigurÂ 4: Tillat at ssb.no fÃ¥r bruke din Google Cloud-konto\n\n\n\n\nDeretter lander man pÃ¥ en side som lar deg avgjÃ¸re hvor mye maskinkraft som skal holdes av til deg (FigurÂ 5). Det Ã¸verste alternativet er valgt som standard, og er tilstrekkelig for de fleste.\n\n\n\n\n\n\n\nFigurÂ 5: Velg hvor mye maskinkraft du trenger\n\n\n\n\nVent til maskinen din starter opp (FigurÂ 6). Oppstartstiden kan variere.\n\n\n\n\n\n\n\nFigurÂ 6: Starter opp Jupyter\n\n\n\nEtter dette er man logget inn i et Jupyter-miljÃ¸ som kjÃ¸rer pÃ¥ en minimal Ubuntu-maskin. Hvis man er del av et Dapla-team fÃ¥r man ogsÃ¥ tilgang til alt teamet har tilgang til.\n\n\n\nInnlogging til staging-miljÃ¸et er identisk med innloggingen til prod-miljÃ¸et, med ett viktig unntak: nettadressen er nÃ¥ https://jupyter.dapla-staging.ssb.no/.\nLitt mer om hva som er tilgjenglig her kommer.\n\n\n\n\n\nNoen ganger kan man komme i en situasjon hvor lÃ¸sningen for Single Sign-On (pÃ¥logging pÃ¥ tvers av flere systemer) gir en feilmelding a la FigurÂ 7:\n\n\n\n\n\n\nFigurÂ 7: Feil som kan oppstÃ¥ ved pÃ¥logging\n\n\n\nI denne situasjonen mÃ¥ man trykke pÃ¥ knappen â€œAdd to existing accountâ€. Da vil skjermbildet FigurÂ 8 dukke opp:\n\n\n\n\n\n\nFigurÂ 8: Klikk pÃ¥ Google-knappen for Ã¥ logge pÃ¥ igjen\n\n\n\nHer mÃ¥ man tykke pÃ¥ Google-knappen (se pil), og deretter logge inn som vist i FigurÂ 3 tidligere i dette avsnittet.",
    "crumbs": [
      "Statistikere",
      "Appendix",
      "Innlogging"
    ]
  },
  {
    "objectID": "statistikkere/innlogging.html#bakkemiljÃ¸et",
    "href": "statistikkere/innlogging.html#bakkemiljÃ¸et",
    "title": "Innlogging",
    "section": "",
    "text": "Jupyter-miljÃ¸et pÃ¥ bakken bruker samme base-image1 for Ã¥ installere Jupyterlab, og er derfor identisk pÃ¥ mange mÃ¥ter. Men innloggingen er ganske forskjellig.\n\n\n\n\n\n\nNote\n\n\n\nFom. 5. desember 2022 har vi byttet ut Jupyter-miljÃ¸et pÃ¥ bakken. Beskrivelsene under gjelder derfor det nye miljÃ¸et. Fram til 15. januar vil du kunne bruke det gamle miljÃ¸et ved Ã¥ gÃ¥ inn pÃ¥ lenken https://jupyter-prod.ssb.no/ manuelt i Google Chrome. Etter 15. januar blir det gamle Jupyter-miljÃ¸et avviklet.\n\n\n\n\nDu logger deg inn pÃ¥ prod i bakkemiljÃ¸et pÃ¥ fÃ¸lgende mÃ¥te:\n\nLogg deg inn pÃ¥ Citrix-Windows i bakkemiljÃ¸et. Det kan gjÃ¸res ved Ã¥ bruke lenken Citrix pÃ¥ ByrÃ¥nettet, som ogsÃ¥ vises i FigurÂ 1.\nTrykk pÃ¥ Jupyterlab-ikonet, som vist pÃ¥ FigurÂ 9, og logg deg inn med vanlig brukernavn og passord.\n\n\n\n\n\n\n\nFigurÂ 9: Jupyterlab-ikon pÃ¥ Skrivebordet i Citrix-Windows.\n\n\n\nNÃ¥r du trykker pÃ¥ ikonet blir du tatt til nettadressen https://sl-jupyter-p.ssb.no/. Du kunne ogsÃ¥ Ã¥pnet Jupyterlab ved Ã¥pne Chrome-nettleseren og skrive inn adressen manuelt.\n\n\n\nInnlogging til staging-miljÃ¸et har ingen snarvei pÃ¥ Skrivebordet, og du mÃ¥ gjÃ¸re fÃ¸lgende for Ã¥ Ã¥pne miljÃ¸et:\n\nÃ…pne Chrome-nettleseren i Citrix-Windows.\nSkriv inn url-en https://sl-jupyter-t.ssb.no/",
    "crumbs": [
      "Statistikere",
      "Appendix",
      "Innlogging"
    ]
  },
  {
    "objectID": "statistikkere/innlogging.html#footnotes",
    "href": "statistikkere/innlogging.html#footnotes",
    "title": "Innlogging",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nHva er base-image?â†©ï¸",
    "crumbs": [
      "Statistikere",
      "Appendix",
      "Innlogging"
    ]
  },
  {
    "objectID": "statistikkere/altinn3.html",
    "href": "statistikkere/altinn3.html",
    "title": "Altinn 3",
    "section": "",
    "text": "Frem mot sommeren 2025 skal alle skjema-undersÃ¸kelser i SSB som gjennomfÃ¸res pÃ¥ Altinn 2 flyttes over til Altinn 3. Skjemaer som flyttes til Altinn 3 vil motta sine data pÃ¥ Dapla, og ikke pÃ¥ bakken som tidligere. Datafangsten hÃ¥ndteres av Team SUV, mens statistikkseksjonene henter sine data fra Team SUV sitt lagringsomrÃ¥de pÃ¥ Dapla. I dette kapitlet beskriver vi nÃ¦rmere hvordan statistikkseksjonene kan jobbe med Altinn3-data pÃ¥ Dapla. Kort oppsummert bestÃ¥r det av disse stegene:\n\nStatistikkprodusenten avtaler overfÃ¸ring av skjema fra Altinn 2 til Altinn 3 med planleggere pÃ¥ S821, som koordinerer denne jobben.\nNÃ¥r statistikkprodusentene fÃ¥r beskjed om at Altinn3-skjemaet skal sendes ut til oppgavegiverne, sÃ¥ mÃ¥ de opprette et Dapla-team.\nNÃ¥r Dapla-teamet er opprettet, og fÃ¸rste skjema er sendt inn, sÃ¥ ber de Team SUV om Ã¥ gi statistikkteamet tilgang til dataene som har kommet inn fra Altinn 3. I tillegg ber de om at Team SUV gir tilgang til teamets Transfer Service instans. 1 Merk at det mÃ¥ gis separate tilganger for data i staging- og produksjonsmiljÃ¸.\nStatistikkprodusenten setter opp en automatisk overfÃ¸ring av skjemadata med Transfer Service, fra Team SUV sitt lagringsomrÃ¥de over til Dapla-teamet sin kildebÃ¸tte.\nStatistikkprodusentene kan begynne Ã¥ jobbe med dataene i Dapla. Blant annet tilbyr Dapla en automatiseringstjeneste man kan bruke for Ã¥ prosessere dataene fra kildedata til inndata2.\n\nUnder forklarer vi mer med mer detaljer hvordan man gÃ¥r frem for gjennomfÃ¸re steg 4-5 over.\n\n\n\n\n\n\nAnsvar for kildedata\n\n\n\nSelv om Team SUV tar ansvaret for datafangst fra Altinn3, sÃ¥ er det statistikkteamet som har ansvaret for langtidslagring av dataene i sin kildebÃ¸tte. Det vil si at at statistikkteamet mÃ¥ sÃ¸rge for at data overfÃ¸res til sin kildebÃ¸tte, og at de kan ikke regne med at Team SUV tar vare pÃ¥ en backup av dataene.\n\n\n\n\nNÃ¥r skjema-dataene kommer inn fra Altinn 3 til Team SUV sitt lagringsomrÃ¥de, sÃ¥ er det en del ting som er verdt Ã¥ tenke pÃ¥:\n\nHvordan er filene organisert hos Team SUV?\nFilstrukturen der Team SUV lagrer dataene som kommer inn fra Altinn 3, har en klar struktur. Du kan selv gÃ¥ inn Ã¥ kikke pÃ¥ dataene, enten fra Jupyter eller Google Cloud Console, og bli bedre kjent med strukturen3. FigurÂ 1 viser en hvordan en typisk filsti ser ut pÃ¥ lagringsomrÃ¥det til Team SUV. Det starter med navnet til bÃ¸tta som Team SUV har opprettet for skjemaet. Deretter viser den innvitteringsdato, deretter et teknisk navn, og til slutt selve skjemanavnet.\n\n\n\n\n\n\n\nFigurÂ 1: Typisk filsti for et Altinn3-skjema.\n\n\n\n\nHvordan organisere dataene i din kildebÃ¸tte?\nNÃ¥r vi bruker Transfer Service til Ã¥ synkronisere innholdet i Team SUV sitt lagringsomrÃ¥de til Dapla-teamet sitt lagringsomrÃ¥de, sÃ¥ er det mest hensiktmessig Ã¥ fortsette Ã¥ bruke mapppe-strukturen som Team SUV har. Grunnen er at vi ikke kan legge pÃ¥ noe logikk som lager en ny struktur. Tjenesten bare kopierer over data. Men du kan kan lage et nytt toppnivÃ¥-mappe som du Ã¸nsker at dataene skal synkroniseres til. F.eks. at alle Altinn-dataene legger seg inn i en mappe som heter altinn. Det er spesielt nyttig hvis du har flere datakilder som skal ligge i samme kildebÃ¸tte.\nUnike skjemanavn\nSkjemanavnet du ser i FigurÂ 1 er unike. Dvs. at hvis en oppgavegiver sender inn flere skjemaer pÃ¥ samme dag, sÃ¥ er fortsatt skjemanavnet unikt. Det er viktig Ã¥ vÃ¦re klar over nÃ¥r man bruker automatiseringstjenesten for kildedata senere. Siden tjenesten trigges per fil, og man er avhengig av unike navn for Ã¥ ikke skrive over filer, sÃ¥ er det nyttig Ã¥ vite at man kan viderefÃ¸re skjemanavnet i overgangen fra kildedata til inndata.\n\n\n\n\nNÃ¥r vi skal overfÃ¸re filer fra Team SUV sin bÃ¸tte til vÃ¥r kildebÃ¸tte, sÃ¥ kan vi gjÃ¸re det manuelt fra Jupyter som forklart her.. Men det er en bedre lÃ¸sning Ã¥ bruke en tjeneste som gjÃ¸r dette for deg. Transfer Service er en tjeneste som kan brukes til Ã¥ synkronisere innholdet mellom bÃ¸tter pÃ¥ Dapla, samt mellom bakke og sky. NÃ¥r du skal ta i bruk tjenesten for Ã¥ overfÃ¸re data mellom en bÃ¸tte fra Team SUV sitt prosjekt altinn-data-prod, til en kildedata-bÃ¸tte i Dapla-teamet ditt, sÃ¥ gjÃ¸r du fÃ¸lgende:\n\nFÃ¸lg denne beskrivelsen hvordan man setter opp overfÃ¸ringsjobber.\nEtter at du har trykket pÃ¥ Create Transfer Job velger du Google Cloud Storage pÃ¥ begge alternativene under Get Started. Deretter gÃ¥r du videre ved Ã¥ klikke pÃ¥ Next Step.\nUnder Choose a source sÃ¥ skal du velge hvor du skal kopiere data fra. Trykk pÃ¥ Browse. I vinduet som dukker opp trykker du pÃ¥ ğŸ”»-ikonet ved siden av Project ID. I sÃ¸kevinduet som dukker opp sÃ¸ker du opp altinn-data-prod og trykker pÃ¥ navnet. Da fÃ¥r du listet opp alle bÃ¸ttene i altinn-data-prod prosjektet. Til slutt trykker du pÃ¥ bÃ¸tta som Team SUV har opprettet for undersÃ¸kelsen4 og klikker Select til nederst pÃ¥ siden. Trykk deretter Next step for Ã¥ gÃ¥ videre.\nUnder Choose a destination sÃ¥ skal du velge hvor du skal kopiere data til. Dette steget er ganske likt som forrige, men du skal nÃ¥ velge ditt eget projekt og kildebÃ¸tta der. Trykk pÃ¥ Browse. I vinduet som dukker opp trykker du pÃ¥ ğŸ”»-ikonet ved siden av Project ID. I sÃ¸kevinduet som dukker opp sÃ¸ker du opp prod-&lt;ditt teamnavn&gt; og trykker pÃ¥ navnet. Da fÃ¥r du listet opp alle bÃ¸ttene i ditt team sitt prosjekt. Velg kildebÃ¸tta som har navnet ssb-prod-&lt;teamnavn&gt;-data-kilde. Hvis du Ã¸nsker Ã¥ kopiere data til en undermappe i bÃ¸tta, sÃ¥ trykker du pÃ¥ &gt;-ikonet ved bÃ¸ttenavnet og velger Ã¸nsket undermappe5. Til slutt trykker du pÃ¥ Select til nederst pÃ¥ siden. Trykk deretter Next step for Ã¥ gÃ¥ videre.\nUnder Choose how and when to run job velger du Run with custom frequency og Starting now i Batch-modus. Hvis du Ã¸nsker Ã¥ overfÃ¸re sÃ¥ ofte som mulig, sÃ¥ velger du Custom frequency og Repeat every 1 Hours. Til slutt trykker du Next Step nederst pÃ¥ siden.\nUnder Choose Settings sÃ¥ legger du til en kort beskrivelse av jobben du har opprettet. Under de andre valgene kan du gjÃ¸re fÃ¸lgende:\n\nUnder Advanced transfer Options trenger du ikke gjÃ¸re noen endringer.\nUnder When to overwrite kan du velge If different.\nUnder When to delete kan du velge Never.\nUnder Manifest trenger du ikke huke av.\nUnder Logging options velger du samme som vist i FigurÂ 2.\n\n\n\n\n\n\n\n\nFigurÂ 2: Valg av opsjoner for logging i Transfer Service\n\n\n\nTil slutt trykker du Create for Ã¥ aktivere tjenesten. Den vil da sjekke Team SUV sin bÃ¸tte hver time og kopiere over alle filer som ikke allerede eksisterer statistikkteamets kildebÃ¸tte.\n\n\n\nNÃ¥r du har satt opp Transfer Service til Ã¥ kopiere over filer fra Team SUV sin bÃ¸tte til statistikkteamets kildebÃ¸tte, sÃ¥ vil det potensielt komme inn nye skjemaer hver time. Disse kan du lese inn i Jupyter og jobbe med hvis du er kildedata-ansvarlig i teamet. Hvis du ikke er det sÃ¥ mÃ¥ du vente til dataene er tilgjengeliggjort i produkt-bÃ¸tta til teamet.\nSiden fÃ¥ personer innehar rollen som kildedata-ansvarlig sÃ¥ er det laget en automatiseringstjeneste som kan bearbeide alle filer som kommer inn i kildebÃ¸tta. Den lar deg kjÃ¸re et python-script pÃ¥ alle filer som kommer inn i kildebÃ¸tta.\nLes mer om hvordan du kan bruker tjenesten her.\n\n\n\nI denne delen deles noen tips og triks for Ã¥ jobbe med Altinn3-dataene pÃ¥ Dapla. Fokuset vil vÃ¦re pÃ¥ hvordan du kan lese inn og transformere xml-filer fra skjema-dataene.\n\n\nFor Ã¥ se innholdet i en mappe gir det mest mening Ã¥ bruke Google Cloud Console. Her kan du se bÃ¥de filer og mapper i et pek-og-klikk grensesnitt. Men du kan ikke se pÃ¥ innholdet i filene der. Til det mÃ¥ du bruke Jupyter.\nAnta at vi Ã¸nsker Ã¥ liste ut alle som leverte skjema den 10. mars 2023. Da kan vi bruke gcsfs til Ã¥ gjÃ¸re det6:\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/**.xml\")\n\n# Print the list of parquet files\nxml_files\nHer bruker vi fs.glob()-funksjonen fra gcsfs til Ã¥ loope gjennom alle undermapper av gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/, og legge alle filer som slutter med .xml til listen xml_files. Dermed har vi et Python-objekt som kun brukes til Ã¥ hente inn de filene vi Ã¸nsker. Vi bruker da det vi vet om mappestrukturen i Team SUV sin bÃ¸tte som vi sÃ¥ tidligere i FigurÂ 1.\n\n\n\nNoen ganger kan det vÃ¦re nyttig Ã¥ se en xml-fil direkte. Da kan vi lese den inn i Jupyter og printe den ut. Her er et eksempel pÃ¥ hvordan vi kan gjÃ¸re det, samtidig som vi formatterer den med xml.dom.minidom:\nfrom xml.dom.minidom import parseString\n\nfrom dapla import FileClient\n\n# Kobler oss pÃ¥ bÃ¸ttene\nfs = FileClient.get_gcs_file_system()\n\n# Sett inn filstien din her\nfile = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/form_dc551844cd74.xml\"\n\ndom = parseString(fs.cat_file(file))\npretty_xml = dom.toprettyxml(indent=\"  \")\nprint(pretty_xml)\nVi brukte da fs.cat_file() til Ã¥ hente inn innholdet i mappen, gjorde det om til xml-object, og formatterer outputen med xml.dom.minidom. Resultatet vil da se ut noe sÃ¥nt som dette:\n&lt;?xml version=\"1.0\" ?&gt;\n&lt;melding xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" dataFormatProvider=\"SERES\" dataFormatId=\"7079\" dataFormatVersion=\"47315\"&gt;\n  &lt;InternInfo&gt;\n    &lt;raNummer&gt;RA-XXXX&lt;/raNummer&gt;\n    &lt;skjemaVersjon&gt;1.0&lt;/skjemaVersjon&gt;\n    &lt;undersoekelsesNr&gt;XXXXX&lt;/undersoekelsesNr&gt;\n    &lt;visOppgaveByrde&gt;0&lt;/visOppgaveByrde&gt;\n    &lt;visBrukeropplevelse&gt;0&lt;/visBrukeropplevelse&gt;\n    &lt;delregNr&gt;2XXXXX&lt;/delregNr&gt;\n    &lt;periodeFritekst&gt;3. kvartal 2022&lt;/periodeFritekst&gt;\n    &lt;periodeFomDato&gt;2022-07-01&lt;/periodeFomDato&gt;\n    &lt;periodeTomDato&gt;2022-09-30&lt;/periodeTomDato&gt;\n    &lt;periodeType&gt;KVRT&lt;/periodeType&gt;\n    &lt;periodeNummer&gt;3&lt;/periodeNummer&gt;\n    &lt;periodeAAr&gt;2022&lt;/periodeAAr&gt;\n    &lt;periodeDato&gt; &lt;/periodeDato&gt;\n    &lt;enhetsIdent&gt;XXXXXX&lt;/enhetsIdent&gt;\n    &lt;enhetsType&gt;BEDR&lt;/enhetsType&gt;\n    &lt;enhetsOrgNr&gt;XXXXXXXXX&lt;/enhetsOrgNr&gt;\n    &lt;enhetsNavn&gt;STATISTISK SENTRALBYRÃ… &lt;/enhetsNavn&gt;\n    &lt;enhetsGateadresse&gt;Akersveien 26&lt;/enhetsGateadresse&gt;\n    &lt;enhetsPostnr&gt;0177&lt;/enhetsPostnr&gt;\n    &lt;enhetsPoststed&gt;OSLO&lt;/enhetsPoststed&gt;\n    &lt;enhetsAvdeling&gt;AVD XXXX&lt;/enhetsAvdeling&gt;\n    &lt;reporteeOrgNr&gt;XXXXXXXXX&lt;/reporteeOrgNr&gt;\n  &lt;/InternInfo&gt;\n  &lt;Kontakt&gt;\n    &lt;kontaktPersonNavn&gt;OLA NORDMANN&lt;/kontaktPersonNavn&gt;\n    &lt;kontaktPersonEpost&gt;ola@tull.no&lt;/kontaktPersonEpost&gt;\n    &lt;kontaktPersonTelefon&gt; &lt;/kontaktPersonTelefon&gt;\n    &lt;kontaktInfoBekreftet&gt;1&lt;/kontaktInfoBekreftet&gt;\n    &lt;kontaktInfoKommentar&gt;Dette er en et eksempel for Dapla-manualen&lt;/kontaktInfoKommentar&gt;\n  &lt;/Kontakt&gt;\n  &lt;Skjemadata&gt;\n    &lt;ledigeStillinger&gt;75&lt;/ledigeStillinger&gt;\n    &lt;datoPrefill&gt; &lt;/datoPrefill&gt;\n  &lt;/Skjemadata&gt;\n&lt;/melding&gt;\n\n\n\nDe fÃ¦rreste Ã¸nsker Ã¥ jobbe direkte med XML-filer. Derfor er det nyttig Ã¥ kunne transformere XML-filene til et mer brukervennlig format, f.eks. en Pandas Dataframe. Her er et eksempel pÃ¥ hvordan vi kan gjÃ¸re det med Pandas:\n\nimport xml.etree.ElementTree as ET\nimport pandas as pd\nfrom dapla import FileClient\n\ndef single_xml_to_dataframe(file_path: str) -&gt; pd.DataFrame:\n    fs = FileClient.get_gcs_file_system()\n\n    with fs.open(file_path, mode=\"r\") as f:\n        single_xml = f.read()\n\n    root = ET.fromstring(single_xml)\n    intern_info = root.find(\"InternInfo\")\n    kontakt = root.find(\"Kontakt\")\n    skjemadata = root.find(\"Skjemadata\")\n\n    data = []\n    all_tags = set()\n\n    for element in intern_info:\n        all_tags.add(element.tag)\n\n    for element in kontakt:\n        all_tags.add(element.tag)\n\n    for element in skjemadata:\n        all_tags.add(element.tag)\n\n    for tag in all_tags:\n        element = intern_info.find(tag)\n        if element is None:\n            element = kontakt.find(tag)\n        if element is None:\n            element = skjemadata.find(tag)\n        if element is not None:\n            value = element.text\n            data.append(value)\n        else:\n            data.append(None)\n\n    # Include the full path to the XML file in the data list\n    data.append(file_path)\n\n    # Create the DataFrame inside the function\n    df = pd.DataFrame([data], columns=list(all_tags) + [\"pathToXmlFile\"])\n    path_to_xml_file_split = df[\"pathToXmlFile\"].str.split(\"/\")\n    return df\n\n# Run the function\nfile_path = \"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/24/28c34dce4232_e7bed8a4-def5-42dc-b1e2-b9fc66beb769/form_28c34dce4232.xml\"\ndf = single_xml_to_dataframe(file_path=file_path)\nI funksjonen sÃ¥ sÃ¸ker vi etter alle elementer under taggene intern_info, kontakt og skjemadata. Dvs. at vi lager en dataframe med en rad, der vi tar med all data fra xml-filen. Til slutt legger vi til en kolonne med filstien til filen. Det kan vÃ¦re nyttig senere hvis man gÃ¥ tilbake til xml-filen for Ã¥ sjekke om konverteringen til en dataframe er riktig.\nFunksjonen single_xml_to_dataframe() kan brukes til Ã¥ loope over en liste med filstier (vi viste hvordan man lager en slik liste tidligere i kapitlet). Men et typisk problem som vil oppstÃ¥ da er at xml-filene har forskjellig antall elementer i seg, som gir dataframes med ulikt antall kolonner, og derfor vil ikke Pandas vite hvordan man konkatinerer disse. For Ã¥ fikse dette mÃ¥ du modifisere funksjonen til Ã¥ ta hÃ¸yde for dette.\n\n\n\nHvis vi Ã¸nsker Ã¥ kopiere filer manuelt (dvs. ikke med Transfer Service) fra Team SUV sine bÃ¸tter til egen kildebÃ¸tte, kan vi gjÃ¸re det fra Jupyter. Vi har tidligere sett hvordan vi kan kopiere enkeltfiler mellom to bÃ¸tter. Under viser vi hvordan man kan kopiere alle filer under en viss filsti med kommandoen fs.copy():\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfrom_path = \"gs://ssb-prod-arbmark-skjema-data-kilde/ledstill/altinn/2022/11/21/\"\nto_path = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/\"\nfs.copy(from_path, to_path, recursive=True)\nI koden over sÃ¥ kopierer vi alle filer under from_path til to_path. Vi bruker recursive=True for Ã¥ sÃ¸rge for at vi kopierer alle filer under from_path.\nI eksempelet over sÃ¥ kopierer vi over alt som er from_path og dets undermapper. I tilfellet med Altinn-data sÃ¥ ligger det ogsÃ¥ pdf-filer av skjemaet som kanskje ikke Ã¸nsker Ã¥ kopiere. I de tilfellene kan vi fÃ¸rst sÃ¸ke etter de filene vi Ã¸nsker Ã¥ kopiere og legge de i en liste. Deretter kan vi kopiere over en og en fil fra listen. Denne tilnÃ¦rmingen er veldig nyttig hvis vi Ã¸nsker Ã¥ filtrere ut filer som ikke er XML-filer, eller vi Ã¸nsker en annen mappestruktur en den som ligger i from_path. Her er en mÃ¥te vi kan gjÃ¸re det pÃ¥:\nfrom dapla import FileClient\n\n# Lager en filsystem-instans av GCS\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/**.xml\")\n\n# Stien du Ã¸nsker Ã¥ kopiere til.\n# Koden under foutsetter at du har med gs:// fÃ¸rst\nto_folder = \"gs://ssb-dapla-felles-data-delt-prod/\"\n\n# Kopierer over filene\nfor file in xml_files:\n    from_bucket = file.split(\"/\")[0]\n    to_bucket = to_folder.split(\"/\")[2]\n    to_path = file.replace(\n        from_bucket, to_bucket\n    )\n    fs.cp(file, to_path)\nI koden over sÃ¥ bruker vi fs.glob() og ** til Ã¥ sÃ¸ke rekursivt etter alle xml-filer under filstien gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10. Deretter kopierer vi over filene til egen kildebÃ¸tte med fs.cp(). NÃ¥r vi skal kopiere over til en ny bÃ¸tte mÃ¥ vi bestemme oss for hvor filene skal ligge og hvor mye av den gamle filstien vi skal beholde. Anta at vi skal beholde hele mappestrukturen i Team SUV sin bÃ¸tte. Da kan vi egentlig bare erstatte ra0678-01-altinn-data-prod-e17d-ssb-altinn/ med den nye bÃ¸tte-navnet, og vi vil fÃ¥ den samme strukturen som i Team SUV sin bÃ¸tte.",
    "crumbs": [
      "Statistikere",
      "Appendix",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn3.html#forberedelse",
    "href": "statistikkere/altinn3.html#forberedelse",
    "title": "Altinn 3",
    "section": "",
    "text": "NÃ¥r skjema-dataene kommer inn fra Altinn 3 til Team SUV sitt lagringsomrÃ¥de, sÃ¥ er det en del ting som er verdt Ã¥ tenke pÃ¥:\n\nHvordan er filene organisert hos Team SUV?\nFilstrukturen der Team SUV lagrer dataene som kommer inn fra Altinn 3, har en klar struktur. Du kan selv gÃ¥ inn Ã¥ kikke pÃ¥ dataene, enten fra Jupyter eller Google Cloud Console, og bli bedre kjent med strukturen3. FigurÂ 1 viser en hvordan en typisk filsti ser ut pÃ¥ lagringsomrÃ¥det til Team SUV. Det starter med navnet til bÃ¸tta som Team SUV har opprettet for skjemaet. Deretter viser den innvitteringsdato, deretter et teknisk navn, og til slutt selve skjemanavnet.\n\n\n\n\n\n\n\nFigurÂ 1: Typisk filsti for et Altinn3-skjema.\n\n\n\n\nHvordan organisere dataene i din kildebÃ¸tte?\nNÃ¥r vi bruker Transfer Service til Ã¥ synkronisere innholdet i Team SUV sitt lagringsomrÃ¥de til Dapla-teamet sitt lagringsomrÃ¥de, sÃ¥ er det mest hensiktmessig Ã¥ fortsette Ã¥ bruke mapppe-strukturen som Team SUV har. Grunnen er at vi ikke kan legge pÃ¥ noe logikk som lager en ny struktur. Tjenesten bare kopierer over data. Men du kan kan lage et nytt toppnivÃ¥-mappe som du Ã¸nsker at dataene skal synkroniseres til. F.eks. at alle Altinn-dataene legger seg inn i en mappe som heter altinn. Det er spesielt nyttig hvis du har flere datakilder som skal ligge i samme kildebÃ¸tte.\nUnike skjemanavn\nSkjemanavnet du ser i FigurÂ 1 er unike. Dvs. at hvis en oppgavegiver sender inn flere skjemaer pÃ¥ samme dag, sÃ¥ er fortsatt skjemanavnet unikt. Det er viktig Ã¥ vÃ¦re klar over nÃ¥r man bruker automatiseringstjenesten for kildedata senere. Siden tjenesten trigges per fil, og man er avhengig av unike navn for Ã¥ ikke skrive over filer, sÃ¥ er det nyttig Ã¥ vite at man kan viderefÃ¸re skjemanavnet i overgangen fra kildedata til inndata.",
    "crumbs": [
      "Statistikere",
      "Appendix",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn3.html#transfer-service",
    "href": "statistikkere/altinn3.html#transfer-service",
    "title": "Altinn 3",
    "section": "",
    "text": "NÃ¥r vi skal overfÃ¸re filer fra Team SUV sin bÃ¸tte til vÃ¥r kildebÃ¸tte, sÃ¥ kan vi gjÃ¸re det manuelt fra Jupyter som forklart her.. Men det er en bedre lÃ¸sning Ã¥ bruke en tjeneste som gjÃ¸r dette for deg. Transfer Service er en tjeneste som kan brukes til Ã¥ synkronisere innholdet mellom bÃ¸tter pÃ¥ Dapla, samt mellom bakke og sky. NÃ¥r du skal ta i bruk tjenesten for Ã¥ overfÃ¸re data mellom en bÃ¸tte fra Team SUV sitt prosjekt altinn-data-prod, til en kildedata-bÃ¸tte i Dapla-teamet ditt, sÃ¥ gjÃ¸r du fÃ¸lgende:\n\nFÃ¸lg denne beskrivelsen hvordan man setter opp overfÃ¸ringsjobber.\nEtter at du har trykket pÃ¥ Create Transfer Job velger du Google Cloud Storage pÃ¥ begge alternativene under Get Started. Deretter gÃ¥r du videre ved Ã¥ klikke pÃ¥ Next Step.\nUnder Choose a source sÃ¥ skal du velge hvor du skal kopiere data fra. Trykk pÃ¥ Browse. I vinduet som dukker opp trykker du pÃ¥ ğŸ”»-ikonet ved siden av Project ID. I sÃ¸kevinduet som dukker opp sÃ¸ker du opp altinn-data-prod og trykker pÃ¥ navnet. Da fÃ¥r du listet opp alle bÃ¸ttene i altinn-data-prod prosjektet. Til slutt trykker du pÃ¥ bÃ¸tta som Team SUV har opprettet for undersÃ¸kelsen4 og klikker Select til nederst pÃ¥ siden. Trykk deretter Next step for Ã¥ gÃ¥ videre.\nUnder Choose a destination sÃ¥ skal du velge hvor du skal kopiere data til. Dette steget er ganske likt som forrige, men du skal nÃ¥ velge ditt eget projekt og kildebÃ¸tta der. Trykk pÃ¥ Browse. I vinduet som dukker opp trykker du pÃ¥ ğŸ”»-ikonet ved siden av Project ID. I sÃ¸kevinduet som dukker opp sÃ¸ker du opp prod-&lt;ditt teamnavn&gt; og trykker pÃ¥ navnet. Da fÃ¥r du listet opp alle bÃ¸ttene i ditt team sitt prosjekt. Velg kildebÃ¸tta som har navnet ssb-prod-&lt;teamnavn&gt;-data-kilde. Hvis du Ã¸nsker Ã¥ kopiere data til en undermappe i bÃ¸tta, sÃ¥ trykker du pÃ¥ &gt;-ikonet ved bÃ¸ttenavnet og velger Ã¸nsket undermappe5. Til slutt trykker du pÃ¥ Select til nederst pÃ¥ siden. Trykk deretter Next step for Ã¥ gÃ¥ videre.\nUnder Choose how and when to run job velger du Run with custom frequency og Starting now i Batch-modus. Hvis du Ã¸nsker Ã¥ overfÃ¸re sÃ¥ ofte som mulig, sÃ¥ velger du Custom frequency og Repeat every 1 Hours. Til slutt trykker du Next Step nederst pÃ¥ siden.\nUnder Choose Settings sÃ¥ legger du til en kort beskrivelse av jobben du har opprettet. Under de andre valgene kan du gjÃ¸re fÃ¸lgende:\n\nUnder Advanced transfer Options trenger du ikke gjÃ¸re noen endringer.\nUnder When to overwrite kan du velge If different.\nUnder When to delete kan du velge Never.\nUnder Manifest trenger du ikke huke av.\nUnder Logging options velger du samme som vist i FigurÂ 2.\n\n\n\n\n\n\n\n\nFigurÂ 2: Valg av opsjoner for logging i Transfer Service\n\n\n\nTil slutt trykker du Create for Ã¥ aktivere tjenesten. Den vil da sjekke Team SUV sin bÃ¸tte hver time og kopiere over alle filer som ikke allerede eksisterer statistikkteamets kildebÃ¸tte.",
    "crumbs": [
      "Statistikere",
      "Appendix",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn3.html#automatiseringstjeneste-for-kildedata",
    "href": "statistikkere/altinn3.html#automatiseringstjeneste-for-kildedata",
    "title": "Altinn 3",
    "section": "",
    "text": "NÃ¥r du har satt opp Transfer Service til Ã¥ kopiere over filer fra Team SUV sin bÃ¸tte til statistikkteamets kildebÃ¸tte, sÃ¥ vil det potensielt komme inn nye skjemaer hver time. Disse kan du lese inn i Jupyter og jobbe med hvis du er kildedata-ansvarlig i teamet. Hvis du ikke er det sÃ¥ mÃ¥ du vente til dataene er tilgjengeliggjort i produkt-bÃ¸tta til teamet.\nSiden fÃ¥ personer innehar rollen som kildedata-ansvarlig sÃ¥ er det laget en automatiseringstjeneste som kan bearbeide alle filer som kommer inn i kildebÃ¸tta. Den lar deg kjÃ¸re et python-script pÃ¥ alle filer som kommer inn i kildebÃ¸tta.\nLes mer om hvordan du kan bruker tjenesten her.",
    "crumbs": [
      "Statistikere",
      "Appendix",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn3.html#tips-og-triks",
    "href": "statistikkere/altinn3.html#tips-og-triks",
    "title": "Altinn 3",
    "section": "",
    "text": "I denne delen deles noen tips og triks for Ã¥ jobbe med Altinn3-dataene pÃ¥ Dapla. Fokuset vil vÃ¦re pÃ¥ hvordan du kan lese inn og transformere xml-filer fra skjema-dataene.\n\n\nFor Ã¥ se innholdet i en mappe gir det mest mening Ã¥ bruke Google Cloud Console. Her kan du se bÃ¥de filer og mapper i et pek-og-klikk grensesnitt. Men du kan ikke se pÃ¥ innholdet i filene der. Til det mÃ¥ du bruke Jupyter.\nAnta at vi Ã¸nsker Ã¥ liste ut alle som leverte skjema den 10. mars 2023. Da kan vi bruke gcsfs til Ã¥ gjÃ¸re det6:\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/**.xml\")\n\n# Print the list of parquet files\nxml_files\nHer bruker vi fs.glob()-funksjonen fra gcsfs til Ã¥ loope gjennom alle undermapper av gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/, og legge alle filer som slutter med .xml til listen xml_files. Dermed har vi et Python-objekt som kun brukes til Ã¥ hente inn de filene vi Ã¸nsker. Vi bruker da det vi vet om mappestrukturen i Team SUV sin bÃ¸tte som vi sÃ¥ tidligere i FigurÂ 1.\n\n\n\nNoen ganger kan det vÃ¦re nyttig Ã¥ se en xml-fil direkte. Da kan vi lese den inn i Jupyter og printe den ut. Her er et eksempel pÃ¥ hvordan vi kan gjÃ¸re det, samtidig som vi formatterer den med xml.dom.minidom:\nfrom xml.dom.minidom import parseString\n\nfrom dapla import FileClient\n\n# Kobler oss pÃ¥ bÃ¸ttene\nfs = FileClient.get_gcs_file_system()\n\n# Sett inn filstien din her\nfile = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/form_dc551844cd74.xml\"\n\ndom = parseString(fs.cat_file(file))\npretty_xml = dom.toprettyxml(indent=\"  \")\nprint(pretty_xml)\nVi brukte da fs.cat_file() til Ã¥ hente inn innholdet i mappen, gjorde det om til xml-object, og formatterer outputen med xml.dom.minidom. Resultatet vil da se ut noe sÃ¥nt som dette:\n&lt;?xml version=\"1.0\" ?&gt;\n&lt;melding xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" dataFormatProvider=\"SERES\" dataFormatId=\"7079\" dataFormatVersion=\"47315\"&gt;\n  &lt;InternInfo&gt;\n    &lt;raNummer&gt;RA-XXXX&lt;/raNummer&gt;\n    &lt;skjemaVersjon&gt;1.0&lt;/skjemaVersjon&gt;\n    &lt;undersoekelsesNr&gt;XXXXX&lt;/undersoekelsesNr&gt;\n    &lt;visOppgaveByrde&gt;0&lt;/visOppgaveByrde&gt;\n    &lt;visBrukeropplevelse&gt;0&lt;/visBrukeropplevelse&gt;\n    &lt;delregNr&gt;2XXXXX&lt;/delregNr&gt;\n    &lt;periodeFritekst&gt;3. kvartal 2022&lt;/periodeFritekst&gt;\n    &lt;periodeFomDato&gt;2022-07-01&lt;/periodeFomDato&gt;\n    &lt;periodeTomDato&gt;2022-09-30&lt;/periodeTomDato&gt;\n    &lt;periodeType&gt;KVRT&lt;/periodeType&gt;\n    &lt;periodeNummer&gt;3&lt;/periodeNummer&gt;\n    &lt;periodeAAr&gt;2022&lt;/periodeAAr&gt;\n    &lt;periodeDato&gt; &lt;/periodeDato&gt;\n    &lt;enhetsIdent&gt;XXXXXX&lt;/enhetsIdent&gt;\n    &lt;enhetsType&gt;BEDR&lt;/enhetsType&gt;\n    &lt;enhetsOrgNr&gt;XXXXXXXXX&lt;/enhetsOrgNr&gt;\n    &lt;enhetsNavn&gt;STATISTISK SENTRALBYRÃ… &lt;/enhetsNavn&gt;\n    &lt;enhetsGateadresse&gt;Akersveien 26&lt;/enhetsGateadresse&gt;\n    &lt;enhetsPostnr&gt;0177&lt;/enhetsPostnr&gt;\n    &lt;enhetsPoststed&gt;OSLO&lt;/enhetsPoststed&gt;\n    &lt;enhetsAvdeling&gt;AVD XXXX&lt;/enhetsAvdeling&gt;\n    &lt;reporteeOrgNr&gt;XXXXXXXXX&lt;/reporteeOrgNr&gt;\n  &lt;/InternInfo&gt;\n  &lt;Kontakt&gt;\n    &lt;kontaktPersonNavn&gt;OLA NORDMANN&lt;/kontaktPersonNavn&gt;\n    &lt;kontaktPersonEpost&gt;ola@tull.no&lt;/kontaktPersonEpost&gt;\n    &lt;kontaktPersonTelefon&gt; &lt;/kontaktPersonTelefon&gt;\n    &lt;kontaktInfoBekreftet&gt;1&lt;/kontaktInfoBekreftet&gt;\n    &lt;kontaktInfoKommentar&gt;Dette er en et eksempel for Dapla-manualen&lt;/kontaktInfoKommentar&gt;\n  &lt;/Kontakt&gt;\n  &lt;Skjemadata&gt;\n    &lt;ledigeStillinger&gt;75&lt;/ledigeStillinger&gt;\n    &lt;datoPrefill&gt; &lt;/datoPrefill&gt;\n  &lt;/Skjemadata&gt;\n&lt;/melding&gt;\n\n\n\nDe fÃ¦rreste Ã¸nsker Ã¥ jobbe direkte med XML-filer. Derfor er det nyttig Ã¥ kunne transformere XML-filene til et mer brukervennlig format, f.eks. en Pandas Dataframe. Her er et eksempel pÃ¥ hvordan vi kan gjÃ¸re det med Pandas:\n\nimport xml.etree.ElementTree as ET\nimport pandas as pd\nfrom dapla import FileClient\n\ndef single_xml_to_dataframe(file_path: str) -&gt; pd.DataFrame:\n    fs = FileClient.get_gcs_file_system()\n\n    with fs.open(file_path, mode=\"r\") as f:\n        single_xml = f.read()\n\n    root = ET.fromstring(single_xml)\n    intern_info = root.find(\"InternInfo\")\n    kontakt = root.find(\"Kontakt\")\n    skjemadata = root.find(\"Skjemadata\")\n\n    data = []\n    all_tags = set()\n\n    for element in intern_info:\n        all_tags.add(element.tag)\n\n    for element in kontakt:\n        all_tags.add(element.tag)\n\n    for element in skjemadata:\n        all_tags.add(element.tag)\n\n    for tag in all_tags:\n        element = intern_info.find(tag)\n        if element is None:\n            element = kontakt.find(tag)\n        if element is None:\n            element = skjemadata.find(tag)\n        if element is not None:\n            value = element.text\n            data.append(value)\n        else:\n            data.append(None)\n\n    # Include the full path to the XML file in the data list\n    data.append(file_path)\n\n    # Create the DataFrame inside the function\n    df = pd.DataFrame([data], columns=list(all_tags) + [\"pathToXmlFile\"])\n    path_to_xml_file_split = df[\"pathToXmlFile\"].str.split(\"/\")\n    return df\n\n# Run the function\nfile_path = \"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/24/28c34dce4232_e7bed8a4-def5-42dc-b1e2-b9fc66beb769/form_28c34dce4232.xml\"\ndf = single_xml_to_dataframe(file_path=file_path)\nI funksjonen sÃ¥ sÃ¸ker vi etter alle elementer under taggene intern_info, kontakt og skjemadata. Dvs. at vi lager en dataframe med en rad, der vi tar med all data fra xml-filen. Til slutt legger vi til en kolonne med filstien til filen. Det kan vÃ¦re nyttig senere hvis man gÃ¥ tilbake til xml-filen for Ã¥ sjekke om konverteringen til en dataframe er riktig.\nFunksjonen single_xml_to_dataframe() kan brukes til Ã¥ loope over en liste med filstier (vi viste hvordan man lager en slik liste tidligere i kapitlet). Men et typisk problem som vil oppstÃ¥ da er at xml-filene har forskjellig antall elementer i seg, som gir dataframes med ulikt antall kolonner, og derfor vil ikke Pandas vite hvordan man konkatinerer disse. For Ã¥ fikse dette mÃ¥ du modifisere funksjonen til Ã¥ ta hÃ¸yde for dette.\n\n\n\nHvis vi Ã¸nsker Ã¥ kopiere filer manuelt (dvs. ikke med Transfer Service) fra Team SUV sine bÃ¸tter til egen kildebÃ¸tte, kan vi gjÃ¸re det fra Jupyter. Vi har tidligere sett hvordan vi kan kopiere enkeltfiler mellom to bÃ¸tter. Under viser vi hvordan man kan kopiere alle filer under en viss filsti med kommandoen fs.copy():\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfrom_path = \"gs://ssb-prod-arbmark-skjema-data-kilde/ledstill/altinn/2022/11/21/\"\nto_path = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/\"\nfs.copy(from_path, to_path, recursive=True)\nI koden over sÃ¥ kopierer vi alle filer under from_path til to_path. Vi bruker recursive=True for Ã¥ sÃ¸rge for at vi kopierer alle filer under from_path.\nI eksempelet over sÃ¥ kopierer vi over alt som er from_path og dets undermapper. I tilfellet med Altinn-data sÃ¥ ligger det ogsÃ¥ pdf-filer av skjemaet som kanskje ikke Ã¸nsker Ã¥ kopiere. I de tilfellene kan vi fÃ¸rst sÃ¸ke etter de filene vi Ã¸nsker Ã¥ kopiere og legge de i en liste. Deretter kan vi kopiere over en og en fil fra listen. Denne tilnÃ¦rmingen er veldig nyttig hvis vi Ã¸nsker Ã¥ filtrere ut filer som ikke er XML-filer, eller vi Ã¸nsker en annen mappestruktur en den som ligger i from_path. Her er en mÃ¥te vi kan gjÃ¸re det pÃ¥:\nfrom dapla import FileClient\n\n# Lager en filsystem-instans av GCS\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/**.xml\")\n\n# Stien du Ã¸nsker Ã¥ kopiere til.\n# Koden under foutsetter at du har med gs:// fÃ¸rst\nto_folder = \"gs://ssb-dapla-felles-data-delt-prod/\"\n\n# Kopierer over filene\nfor file in xml_files:\n    from_bucket = file.split(\"/\")[0]\n    to_bucket = to_folder.split(\"/\")[2]\n    to_path = file.replace(\n        from_bucket, to_bucket\n    )\n    fs.cp(file, to_path)\nI koden over sÃ¥ bruker vi fs.glob() og ** til Ã¥ sÃ¸ke rekursivt etter alle xml-filer under filstien gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10. Deretter kopierer vi over filene til egen kildebÃ¸tte med fs.cp(). NÃ¥r vi skal kopiere over til en ny bÃ¸tte mÃ¥ vi bestemme oss for hvor filene skal ligge og hvor mye av den gamle filstien vi skal beholde. Anta at vi skal beholde hele mappestrukturen i Team SUV sin bÃ¸tte. Da kan vi egentlig bare erstatte ra0678-01-altinn-data-prod-e17d-ssb-altinn/ med den nye bÃ¸tte-navnet, og vi vil fÃ¥ den samme strukturen som i Team SUV sin bÃ¸tte.",
    "crumbs": [
      "Statistikere",
      "Appendix",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/altinn3.html#footnotes",
    "href": "statistikkere/altinn3.html#footnotes",
    "title": "Altinn 3",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nForslag til e-post til Team SUV etter at teamet er opprettet:\nVi har opprettet et Dapla-tema som heter &lt;ditt teamnavn&gt; for Ã¥ jobbe med skjema &lt;RA-XXXX&gt;. Kan dere gi oss tilgang til riktig lagringsomrÃ¥de og ogsÃ¥ gi vÃ¥r Transfer Service lesetilgang.â†©ï¸\nEn typisk prosessering som de fleste vil Ã¸nske Ã¥ gjÃ¸re er Ã¥ konvertere fra xml-formatet det kom pÃ¥, og over til parquet-formatet.â†©ï¸\nDu kan gÃ¥ inn i Google Cloud Console og sÃ¸ke opp prosjektet til Team SUV som de bruker for Ã¥ dele data. Det heter altinn-data-prod, og du finner bÃ¸ttene ved Ã¥ klikke deg inn pÃ¥ Cloud Storageâ†©ï¸\nBÃ¸ttenavnet starter alltid med RA-nummeret til undersÃ¸kelsen.â†©ï¸\nAlternativt oppretter du en mappe direkte vinduet ved Ã¥ trykke pÃ¥ mappe-ikonet med en +-tegn i seg.â†©ï¸\nFor Ã¥ jobbe mot datat i GCS som i et â€œvanligâ€ filsysten kan vi bruke FileClient.get_gcs_file_system() fra dapla-toolbelt.â†©ï¸",
    "crumbs": [
      "Statistikere",
      "Appendix",
      "Altinn 3"
    ]
  },
  {
    "objectID": "statistikkere/daplalab-overview.html",
    "href": "statistikkere/daplalab-overview.html",
    "title": "DaplaLab",
    "section": "",
    "text": "DaplaLab",
    "crumbs": [
      "Statistikere",
      "DaplaLab"
    ]
  },
  {
    "objectID": "statistikkere/gcp-overview.html",
    "href": "statistikkere/gcp-overview.html",
    "title": "Google Cloud Platform (GCP)",
    "section": "",
    "text": "Google Cloud Platform (GCP)"
  },
  {
    "objectID": "statistikkere/jupyterlab.html",
    "href": "statistikkere/jupyterlab.html",
    "title": "Jupyterlab",
    "section": "",
    "text": "Mer kommer.\n\n\n\nMÃ¥ nevne operativsystemet og at noe programvare ligger installert her (git, jwsacruncher, quarto, ++)\n\n\n\nNoe er i base-image, noe bÃ¸r gjÃ¸res i virtuelle milÃ¸er. Hvordan liste ut pakker som er pre-installert?\n\n\n\nJupyterlab er en samling extension. Kan bare installeres av admin. Sikkerhet. Hvilke extension har vi tilgjengeliggjort?\n\n\n\nSane defaults for Jupyterlab.",
    "crumbs": [
      "Statistikere",
      "DaplaLab",
      "Jupyterlab"
    ]
  },
  {
    "objectID": "statistikkere/jupyterlab.html#hva-er-jupyterlab",
    "href": "statistikkere/jupyterlab.html#hva-er-jupyterlab",
    "title": "Jupyterlab",
    "section": "",
    "text": "Mer kommer.",
    "crumbs": [
      "Statistikere",
      "DaplaLab",
      "Jupyterlab"
    ]
  },
  {
    "objectID": "statistikkere/jupyterlab.html#terminalen",
    "href": "statistikkere/jupyterlab.html#terminalen",
    "title": "Jupyterlab",
    "section": "",
    "text": "MÃ¥ nevne operativsystemet og at noe programvare ligger installert her (git, jwsacruncher, quarto, ++)",
    "crumbs": [
      "Statistikere",
      "DaplaLab",
      "Jupyterlab"
    ]
  },
  {
    "objectID": "statistikkere/jupyterlab.html#pakkeinstallasjoner",
    "href": "statistikkere/jupyterlab.html#pakkeinstallasjoner",
    "title": "Jupyterlab",
    "section": "",
    "text": "Noe er i base-image, noe bÃ¸r gjÃ¸res i virtuelle milÃ¸er. Hvordan liste ut pakker som er pre-installert?",
    "crumbs": [
      "Statistikere",
      "DaplaLab",
      "Jupyterlab"
    ]
  },
  {
    "objectID": "statistikkere/jupyterlab.html#extensions",
    "href": "statistikkere/jupyterlab.html#extensions",
    "title": "Jupyterlab",
    "section": "",
    "text": "Jupyterlab er en samling extension. Kan bare installeres av admin. Sikkerhet. Hvilke extension har vi tilgjengeliggjort?",
    "crumbs": [
      "Statistikere",
      "DaplaLab",
      "Jupyterlab"
    ]
  },
  {
    "objectID": "statistikkere/jupyterlab.html#tips-triks",
    "href": "statistikkere/jupyterlab.html#tips-triks",
    "title": "Jupyterlab",
    "section": "",
    "text": "Sane defaults for Jupyterlab.",
    "crumbs": [
      "Statistikere",
      "DaplaLab",
      "Jupyterlab"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-data.html",
    "href": "statistikkere/jobbe-med-data.html",
    "title": "Jobbe med data",
    "section": "",
    "text": "NÃ¥r vi oppretter et dapla-team sÃ¥ fÃ¥r vi tildelt et eget omrÃ¥det for lagring av data. For Ã¥ kunne lese og skrive data fra Jupyter til disse omrÃ¥dene mÃ¥ vi autentisere oss, siden Jupyter og lagringsomrÃ¥det er to separate sikkerhetsoner.\nFigurÂ 1 viser dette klarer skillet mellom hvor vi koder og hvor dataene ligger pÃ¥ Dapla1. I dette kapitlet beskriver vi nÃ¦rmere hvordan du kan jobbe med dataene dine pÃ¥ Dapla.\n\n\n\n\n\n\nFigurÂ 1: Tydelig skille mellom kodemiljÃ¸ og datalagring pÃ¥ Dapla.\n\n\n\n\n\nFor Ã¥ gjÃ¸re det enklere Ã¥ jobbe data pÃ¥ tvers av Jupyter og lagringsomrÃ¥det er det laget noen egne SSB-utviklede biblioteker for Ã¥ gjÃ¸re vanlige operasjoner mot lagringsomrÃ¥det. Siden bÃ¥de R og Python skal brukes pÃ¥ Dapla, sÃ¥ er det laget to biblioteker, en for hver av disse sprÃ¥kene. fellesr er biblioteket for R, og dapla-toolbelt er biblioteket for Python.\n\n\ndapla-toolbelt er en en pakke som lar deg enkelt lese og skrive til lagringsomrÃ¥det uten Ã¥ mÃ¥tte autentifisere deg manuelt. Den har en Pandas-aktig syntaks som forhÃ¥pentlig er gjenkjennbar for de fleste. Pakken er installert i alle Python-kernels pÃ¥ Dapla, sÃ¥ du trenger ikke Ã¥ installere den selv hvis du Ã¥pner en notebook med Python3 for eksempel. For Ã¥ importere hele biblioteket i en notebook skriver du bare\n\n\nnotebook\n\nimport dapla as dp\n\ndapla-toolbelt bruker en pakke som heter gcsfs for Ã¥ kommunisere med lagringsomrÃ¥det. gcsfs er en pakke som lar deg bruke Google Cloud Storage (GCS) som om det var en filsystem. Det betyr at du kan bruke samme syntaks som du bruker for Ã¥ lese og skrive til filer pÃ¥ din egen maskin. Du kan lese mulighetene i gcsfs her. Et eksempel pÃ¥ hvordan de to pakkene kan brukes sammen ser du her:\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Example of how you can use gcsfs and dapla-toolbelt together\nfs.touch(\"gs://my-bucket/my-folder/\")\n\nI koden over brukte jeg kommandoen touch fra gcsfs og FileClient fra dapla-toolbelt for Ã¥ opprette en mappe i lagringsomrÃ¥det.\nI kapitlene under finner du konkrete eksempler pÃ¥ hvordan du kan bruke dapla-toolbelt til Ã¥ jobbe med data i SSBs lagringsomrÃ¥det.\n\n\n\nR-pakken fellesr er under utvikling og gir mye av den samme funksjonaliteten som dapla-toolbelt gir for Python. I tillegg til Ã¥ kunne lese og skrive til lagringsomrÃ¥det pÃ¥ Dapla, sÃ¥ har fellesr ogsÃ¥ funksjoner for Ã¥ jobbe med metadata pÃ¥ Dapla.\nfellesr er installert pÃ¥ Dapla og funksjoner kan benyttes ved:\n\n\nnotebook\n\nlibrary(fellesr)\n\nHvis du benytte en renv miljÃ¸, mÃ¥ pakken installeres en gang. Dette kan gjÃ¸res ved:\n\n\nnotebook\n\nrenv::install(\"statisticsnorway/fellesr\")\n\n\n\n\n\nI denne delen viser vi hvordan man gjÃ¸r veldig vanlige operasjoner nÃ¥r man koder et produksonslÃ¸p for en statistikk. Flere eksempler pÃ¥ nyttige systemkommandoer finner du her.\n\n\n\n\n\n\n\n\nEksempeldata\n\n\n\nDet finnes et omrÃ¥de som alle SSB-ansatte har lese- og skrivetilgang til. Det er\ngs://ssb-dapla-felles-data-delt-prod/ i prod-miljÃ¸et pÃ¥ Dapla, og\ngs://ssb-dapla-felles-data-delt-test/ i staging-miljÃ¸et. Eksemplene under bruker fÃ¸rstnevnte i koden, slik at alle kan kjÃ¸re koden selv.\nKode-eksemplene finnes for bÃ¥de R og Python, og du kan velge hvilken du skal se ved Ã¥ trykke pÃ¥ den arkfanen du er interessert i.\n\n\nÃ… liste ut innhold i et gitt mappe pÃ¥ Dapla er ganske enkelt. Under ser du hvordan du kan liste ut innholdet i fÃ¸lgende mappe:\ngs://ssb-dapla-felles-data-delt-prod/felles/veiledning/python/eksempler/purchases\n\nPython \n\n\nVi bruker modulen FileClient fra dapla-toolbelt for Ã¥ liste ut innholdet i en mappe.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Set path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\nFileClient.ls(f\"{bucket}/{folder}\")\n\nMed kommandoen over fÃ¥r du listet ut alle filnavn i mappen. Hvis du vil ha mer informasjon om filene sÃ¥ kan du bruke ls-kommandoen med detail = True, som under:\n\n\nnotebook\n\nFileClient.ls(f\"{bucket}/{folder}\", detail = True)\n\nSyntaksen med ls er veldig lik det man kjenner fra en Linux-terminal. Men nÃ¥r vi bruker detail = True blir metadata om filene returnert som en Python-liste med dictionaries. Det kan vÃ¦re svÃ¦rt nyttig nÃ¥r du f.eks. trenger Ã¥ vite dato og tidspunkt for nÃ¥r en fil ble opprettet, eller nÃ¥r den sist ble oppdatert.\n\n\n\n\nnotebook\n\n# Loading functions into notebook\nlibrary(fellesr)\n\n# Path to folder\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod/\"\nfolder &lt;- \"felles/veiledning/python/eksempler/purchases\"\n\n# List files in folder \nlist.files(paste0(bucket, folder))\n\nMerknad: NÃ¥r du spesifisere bÃ¸tter i R, trenger du ikke â€œgs://â€ foran.\n\n\n\n\n\n\nÃ… skrive filer til et lagringsomrÃ¥de pÃ¥ Dapla er ogsÃ¥ ganske enkelt. Det ligner mye pÃ¥ den syntaksen vi er kjent med fra vanlige R- og Python-pakker, med noen smÃ¥ unntak.\n\n\nUnder lager vi en dataframe i en notebook og skriver den ut til en parquet-fil.\n\nPython \n\n\nNÃ¥r vi leser en Parquet-fil med dapla-toolbelt sÃ¥ bruker den pyarrow i bakgrunnen. Dette er en av de raskeste mÃ¥tene Ã¥ lese og skrive Parquet-filer pÃ¥.\n\n\nnotebook\n\nimport dapla as dp\nimport pandas as pd\nimport numpy as np\n\n# Set path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Create pandas dataframe\npurchases = pd.DataFrame(np.random.randn(10, 5),\n                        columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\n# Write pandas dataframe as parquet to GCS bucket\ndp.write_pandas(df = purchases,\n                gcs_path = f\"{bucket}/{folder}/data.parquet\",\n                file_format = \"parquet\",)\n\nNÃ¥r vi kalte write_pandas over sÃ¥ spesifiserte vi at filformatet skulle vÃ¦re parquet. Dette er default, sÃ¥ vi kunne ogsÃ¥ ha skrevet det slik:\n\n\nnotebook\n\ndp.write_pandas(df = purchases,\n                gcs_path = f\"{bucket}/{folder}/data.parquet\")\n\nMen for de andre filformatene mÃ¥ vi altsÃ¥ spesifisere dette.\n\n\nNÃ¥r vi jobber med Parquet-fil i R, bruker vi pakken arrow. Dette er en del av fellesr pakken sÃ¥ du trenger kun Ã¥ kalle inn dette. Pakken inneholder funksjonen write_SSB som kan brukes til Ã¥ skrive data til bÃ¸tte pÃ¥ Dapla.\n\n\nnotebook\n\nlibrary(fellesr)\n\n# Set stien til hvor data skal lagres\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"felles/veiledning/r/eksempler/purchases\"\n\n# Lage en eksempel dataframe\npurchases = data.frame(A = runif(10), B= runif(10), C=runif(10))\n\n# Skrive data til bÃ¸ttet som en parquet\nwrite_SSB(purchases, file.path(bucket, folder, \"purchases.parquet\"))\n\nMerknad: NÃ¥r du spesifisere bÃ¸tter i R, trenger du ikke â€œgs://â€ foran.\n\n\n\n\n\n\nNoen ganger Ã¸nsker vi Ã¥ lagre data i andre formatter slik som CSV, JSON og XML.\n\nPython \n\n\ndapla-toolbelt kan skrive ut json, csv og posisjonsfiler (fixed-width-files/fwf) til lagringsomrÃ¥det. MÃ¥ten den gjÃ¸r det pÃ¥ er Ã¥ bruke Pandas sine funksjoner read_json, read_csv, read_fwf under panseret. Dette kan vÃ¦re nyttig Ã¥ vite for skjÃ¸nne hvordan dapla-toolbelt hÃ¥ndterer ulike strukturer i (spesielt hierarkiske) tekstfiler. Under ser du hvordan du kan skrive ut en dataframe til en json-fil.\n\n\nnotebook\n\nimport numpy as np\nimport pandas as pd\nfrom dapla import FileClient\n\n# Set path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Create a dataframe with Pandas\ndf = pd.DataFrame(np.random.randn(10, 5), columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\n# Save dataframe as json with dapla-toolbelt\ndp.write_pandas(df = df,\n                gcs_path = f\"{bucket}/{folder}/test.json\",\n                file_format = \"json\")\n\nSom vi ser at syntaksen over sÃ¥ kunne vi skrevet ut til noe annet enn json ved Ã¥ endre verdien i argumentet file_format.\n\n\nPakken fellesr kan ogsÃ¥ brukes til Ã¥ skrive andre type filer, for eksempel csv, til bÃ¸tter. Dette gjÃ¸res med funksjonen write_SSB og spesifisere Ã¸nsket filtype i filnavn.\nFÃ¸rst kaller vi biblioteket og lage noe test data ved:\n\n\nnotebook\n\nlibrary(fellesr)\n\n# Set stien til hvor data skal lagres\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"felles/veiledning/r/eksempler/purchases\"\n\n# Lage en eksempel dataframe\npurchases = data.frame(A = runif(10), B= runif(10), C=runif(10))\n\n# Skrive til csv\nwrite_SSB(purchases, file.path(bucket, folder, \"purchases.csv\")\n\n\n\n\n\n\n\nDet er ikke anbefalt Ã¥ bruke xlsx-formatet, men her ser du hvordan det kan skrives ut. Mer kommer.\n\nPython \n\n\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import AuthClient\n\n# Henter token for Ã¥ kunne lese fra Dapla\ntoken = AuthClient.fetch_google_credentials()\n\n# Filsti\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\ndf.to_excel(f\"{bucket}/{folder}/test.xlsx\",\n           storage_options={\"token\": token})\n\n\n\nKommer snart\n\n\n\n\n\n\n\nUnder finner du eksempler pÃ¥ hvordan du kan lese inn data til en Jupyter Notebooks pÃ¥ Dapla.\n\n\n\nPython \n\n\n\n\nnotebook\n\nimport dapla as dp\n\n# Set path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Read path into pandas dataframe \ndp.read_pandas(gcs_path= f\"{bucket}/{folder}/data.parquet\",\n               file_format = \"parquet\",\n               columns = None,)\n\nSom vi sÃ¥ med write_pandas sÃ¥ er file_format default satt til parquet, og default for columns = None, sÃ¥ vi kunne ogsÃ¥ ha skrevet det slik:\ndp.read_pandas(gcs_path= f\"{bucket}/{folder}/data.parquet\")\ncolumns-argumentet er en liste med kolonnenavn som vi Ã¸nsker Ã¥ lese inn. Hvis vi ikke spesifiserer noen kolonner sÃ¥ vil alle kolonnene leses inn.\n\n\nPakken fellesr kan brukes til Ã¥ lese inn data. Funksjonen read_SSB() kan lese inn filer i flere format inkluderende parquet.\nHer er et eksempel av Ã¥ lese inn parquet fil â€œ1987â€.\n\n\nnotebook\n\nlibrary(fellesr)\n\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"R_smoke_test\"\n\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.parquet\"))\n\nVi kan ogsÃ¥ filtrere hvilke variabel vi Ã¸nsker Ã¥ lese inn ved Ã¥ spesifisere parameter col_select. For eksempel:\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.parquet\"),\n                    col_select = c(\"Year\", \"Month\"))\nInnlesning av parquet som er kartdata finner du her: Lese kartdata\n\n\n\n\n\n\nKommer mer snart. Python-koden under bygger pÃ¥ eksempelet over.\n\nPython \n\n\n\n\nnotebook\n\nimport dapla as dp\n\n# Path to write to\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Read in json-file from dapla-storage\ndf = dp.read_pandas(gcs_path = f\"{bucket}/{folder}/test3.json\",\n               file_format = \"json\")\n\n\n\nFunksjonen read_SSB() kan lese inn flere type av fil-format, slik som csv og json. Du trenger ikke Ã¥ endre koden, kun spesifisere hele filnavn.\nFÃ¸rst kaller vi inn biblioteket fellesr og spesifisere bÃ¸tte/mappen:\n\n\nnotebook\n\nlibrary(fellesr)\n\n# Filsti\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"R_smoke_test\"\n\n# Lese inn CSV-fil\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.csv\"))\n\nFor Ã¥ lese inn en json-fil kan skrive fÃ¸lgende:\n\n\nnotebook\n\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.json\"))\n\n\n\n\n\n\n\n\nPython \n\n\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import AuthClient\n\n# Hent token\ntoken = AuthClient.fetch_google_credentials()\n\n# Les inn fil\ndf = pd.read_excel(\"gs://ssb-prod-arbmark-skjema-data-produkt/test_gcp.xlsx\",\n    storage_options={\"token\": token})\n\n\n\nKommer snart\n\n\n\n\n\n\nHer er et eksempel pÃ¥ hvordan man leser inn en sas7bdat-fil pÃ¥ Dapla som har blitt generert i prodsonen.\n\nPython \n\n\n\n\nnotebook\n\nimport dapla as dp\n\nsti = \"gs://ssb-dapla-felles-data-delt-prod/felles/veiledning/sas/statbank_ledstill.sas7bdat\"\n\ndp.read_pandas(sti, file_format=\"sas7bdat\", encoding=\"latin1\")\n\n\n\nI produksjon sone (pÃ¥ bakken) kan R-pakken haven benyttes for Ã¥ lese inn .sas7bdat filer. Dette er ikke implementert i fellesr enda for lesing fra Dapla bÃ¸tte.\nMer om dette kommer.\n\n\n\n\n\n\n\nÃ… slette filer fra lagringsomrÃ¥det kan gjÃ¸res pÃ¥ flere mÃ¥ter. I kapitlet om sletting av data viste vi hvordan man gjÃ¸r det med pek-og-klikk i Google Cloud Console. Under ser du hvordan du kan slette filer med dapla-toolbelt og gcsfs eller fellesr.\n\nPython \n\n\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\nfs.rm(f\"{bucket}/{from_folder}/df.json\")\n\n\n\nFunksjonen gc_delete_object kan brukes til Ã¥ slette data pÃ¥ lagringsomrÃ¥det.\n\n\nnotebook\n\nlibrary(fellesr)\n\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"felles/veiledning/r/eksempler/purchases\"\n\ngcs_delete_object(file.path(bucket, folder, \"purchases.parquet\"))\n\n\n\n\n\n\n\nÃ… kopiere filer mellom mapper pÃ¥ et Linux-filsystem innebÃ¦rer som regel bruke cp-kommandoen. PÃ¥ Dapla er det ikke sÃ¥ mye forskjell. Vi bruker en ligende tilnÃ¦rming nÃ¥ vi skal kopiere mellom bÃ¸tter eller mapper pÃ¥ lagringsomrÃ¥det til SSB. Under ser du hvordan du kan kopiere en fil fra en mappe til en annen.\n\nPython \n\n\nLa oss begynne med et eksempel der vi kopierer en fil fra en mappe til en annen i samme bÃ¸tte.\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Path to folders\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfrom_folder = \"felles/veiledning/python/eksempler/purchases\"\nto_folder = \"felles/veiledning/python/eksempler\"\n\n# Copy file\nfs.cp(f\"{bucket}/{from_folder}/data.parquet\",\n      f\"{bucket}/{to_folder}/data_copy.parquet\")\n\nDet ogsÃ¥ fungere for Ã¥ kopiere filer mellom bÃ¸tter.\nEt annet scenario vi ofte vil stÃ¸te pÃ¥ er at vi Ã¸nsker Ã¥ kopiere en fil fra vÃ¥rt Jupyter-filsystem til en mappe pÃ¥ lagringsomrÃ¥det. Her kan vi bruke fs.put-metoden.\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Create a new file in your home directory called test.txt\nwith open('/home/jovyan/test.txt', 'w') as f:\n    f.write('Create a new text file!')\n\n#Path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler\"\n\n# Copy file from local to remote file system\nfs.put(lpath=f\"/home/jovyan/test.txt\", rpath=f\"{bucket}/{folder}/test.txt\")\n\nÃ˜nsker vi Ã¥ kopiere en hel mappe fra lagringsomrÃ¥det til Jupyter-filsystemet, kan vi bruke fs.get-metoden, med opsjonen recursive=True.\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Copy file\nfs.get(&lt;from_bucket&gt;,\n      \"/home/jovyan/sesongjustering/\",\n      recursive=True)\n\n\n\nKommer snart\n\n\n\n\n\n\n\nPython \n\n\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfrom_folder = \"felles/veiledning/python/eksempler/purchases\"\nto_folder = \"felles/veiledning/python/eksempler\"\n\nfs.mv(f\"{bucket}/{from_folder}/data.parquet\", f\"{bucket}/{to_folder}/data.parquet\")\n\n\n\nKommer snart\n\n\n\n\n\n\nSelv om bÃ¸tter ikke har mapper med en hierarkisk struktur slik man er kjent med fra klassike filsystemer, sÃ¥ kan man opprette det som ser ut som mapper i objektnavnet. I realiteten blir bare / oppfattet som en del av navnet pÃ¥ objektet. Skulle du likevel Ã¸nske Ã¥ opprette dette sÃ¥ kan du gjÃ¸re det fÃ¸lgende mÃ¥te:\n\nPython \n\n\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n#Path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler\"\n\n# Create folder\nfs.touch(f\"{bucket}/{folder}/testmappe/\")\n\n\n\nKommer snart",
    "crumbs": [
      "Statistikere",
      "Data"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-data.html#ssb-biblioteker",
    "href": "statistikkere/jobbe-med-data.html#ssb-biblioteker",
    "title": "Jobbe med data",
    "section": "",
    "text": "For Ã¥ gjÃ¸re det enklere Ã¥ jobbe data pÃ¥ tvers av Jupyter og lagringsomrÃ¥det er det laget noen egne SSB-utviklede biblioteker for Ã¥ gjÃ¸re vanlige operasjoner mot lagringsomrÃ¥det. Siden bÃ¥de R og Python skal brukes pÃ¥ Dapla, sÃ¥ er det laget to biblioteker, en for hver av disse sprÃ¥kene. fellesr er biblioteket for R, og dapla-toolbelt er biblioteket for Python.\n\n\ndapla-toolbelt er en en pakke som lar deg enkelt lese og skrive til lagringsomrÃ¥det uten Ã¥ mÃ¥tte autentifisere deg manuelt. Den har en Pandas-aktig syntaks som forhÃ¥pentlig er gjenkjennbar for de fleste. Pakken er installert i alle Python-kernels pÃ¥ Dapla, sÃ¥ du trenger ikke Ã¥ installere den selv hvis du Ã¥pner en notebook med Python3 for eksempel. For Ã¥ importere hele biblioteket i en notebook skriver du bare\n\n\nnotebook\n\nimport dapla as dp\n\ndapla-toolbelt bruker en pakke som heter gcsfs for Ã¥ kommunisere med lagringsomrÃ¥det. gcsfs er en pakke som lar deg bruke Google Cloud Storage (GCS) som om det var en filsystem. Det betyr at du kan bruke samme syntaks som du bruker for Ã¥ lese og skrive til filer pÃ¥ din egen maskin. Du kan lese mulighetene i gcsfs her. Et eksempel pÃ¥ hvordan de to pakkene kan brukes sammen ser du her:\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Example of how you can use gcsfs and dapla-toolbelt together\nfs.touch(\"gs://my-bucket/my-folder/\")\n\nI koden over brukte jeg kommandoen touch fra gcsfs og FileClient fra dapla-toolbelt for Ã¥ opprette en mappe i lagringsomrÃ¥det.\nI kapitlene under finner du konkrete eksempler pÃ¥ hvordan du kan bruke dapla-toolbelt til Ã¥ jobbe med data i SSBs lagringsomrÃ¥det.\n\n\n\nR-pakken fellesr er under utvikling og gir mye av den samme funksjonaliteten som dapla-toolbelt gir for Python. I tillegg til Ã¥ kunne lese og skrive til lagringsomrÃ¥det pÃ¥ Dapla, sÃ¥ har fellesr ogsÃ¥ funksjoner for Ã¥ jobbe med metadata pÃ¥ Dapla.\nfellesr er installert pÃ¥ Dapla og funksjoner kan benyttes ved:\n\n\nnotebook\n\nlibrary(fellesr)\n\nHvis du benytte en renv miljÃ¸, mÃ¥ pakken installeres en gang. Dette kan gjÃ¸res ved:\n\n\nnotebook\n\nrenv::install(\"statisticsnorway/fellesr\")",
    "crumbs": [
      "Statistikere",
      "Data"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-data.html#vanlige-operasjoner",
    "href": "statistikkere/jobbe-med-data.html#vanlige-operasjoner",
    "title": "Jobbe med data",
    "section": "",
    "text": "I denne delen viser vi hvordan man gjÃ¸r veldig vanlige operasjoner nÃ¥r man koder et produksonslÃ¸p for en statistikk. Flere eksempler pÃ¥ nyttige systemkommandoer finner du her.\n\n\n\n\n\n\n\n\nEksempeldata\n\n\n\nDet finnes et omrÃ¥de som alle SSB-ansatte har lese- og skrivetilgang til. Det er\ngs://ssb-dapla-felles-data-delt-prod/ i prod-miljÃ¸et pÃ¥ Dapla, og\ngs://ssb-dapla-felles-data-delt-test/ i staging-miljÃ¸et. Eksemplene under bruker fÃ¸rstnevnte i koden, slik at alle kan kjÃ¸re koden selv.\nKode-eksemplene finnes for bÃ¥de R og Python, og du kan velge hvilken du skal se ved Ã¥ trykke pÃ¥ den arkfanen du er interessert i.\n\n\nÃ… liste ut innhold i et gitt mappe pÃ¥ Dapla er ganske enkelt. Under ser du hvordan du kan liste ut innholdet i fÃ¸lgende mappe:\ngs://ssb-dapla-felles-data-delt-prod/felles/veiledning/python/eksempler/purchases\n\nPython \n\n\nVi bruker modulen FileClient fra dapla-toolbelt for Ã¥ liste ut innholdet i en mappe.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Set path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\nFileClient.ls(f\"{bucket}/{folder}\")\n\nMed kommandoen over fÃ¥r du listet ut alle filnavn i mappen. Hvis du vil ha mer informasjon om filene sÃ¥ kan du bruke ls-kommandoen med detail = True, som under:\n\n\nnotebook\n\nFileClient.ls(f\"{bucket}/{folder}\", detail = True)\n\nSyntaksen med ls er veldig lik det man kjenner fra en Linux-terminal. Men nÃ¥r vi bruker detail = True blir metadata om filene returnert som en Python-liste med dictionaries. Det kan vÃ¦re svÃ¦rt nyttig nÃ¥r du f.eks. trenger Ã¥ vite dato og tidspunkt for nÃ¥r en fil ble opprettet, eller nÃ¥r den sist ble oppdatert.\n\n\n\n\nnotebook\n\n# Loading functions into notebook\nlibrary(fellesr)\n\n# Path to folder\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod/\"\nfolder &lt;- \"felles/veiledning/python/eksempler/purchases\"\n\n# List files in folder \nlist.files(paste0(bucket, folder))\n\nMerknad: NÃ¥r du spesifisere bÃ¸tter i R, trenger du ikke â€œgs://â€ foran.\n\n\n\n\n\n\nÃ… skrive filer til et lagringsomrÃ¥de pÃ¥ Dapla er ogsÃ¥ ganske enkelt. Det ligner mye pÃ¥ den syntaksen vi er kjent med fra vanlige R- og Python-pakker, med noen smÃ¥ unntak.\n\n\nUnder lager vi en dataframe i en notebook og skriver den ut til en parquet-fil.\n\nPython \n\n\nNÃ¥r vi leser en Parquet-fil med dapla-toolbelt sÃ¥ bruker den pyarrow i bakgrunnen. Dette er en av de raskeste mÃ¥tene Ã¥ lese og skrive Parquet-filer pÃ¥.\n\n\nnotebook\n\nimport dapla as dp\nimport pandas as pd\nimport numpy as np\n\n# Set path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Create pandas dataframe\npurchases = pd.DataFrame(np.random.randn(10, 5),\n                        columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\n# Write pandas dataframe as parquet to GCS bucket\ndp.write_pandas(df = purchases,\n                gcs_path = f\"{bucket}/{folder}/data.parquet\",\n                file_format = \"parquet\",)\n\nNÃ¥r vi kalte write_pandas over sÃ¥ spesifiserte vi at filformatet skulle vÃ¦re parquet. Dette er default, sÃ¥ vi kunne ogsÃ¥ ha skrevet det slik:\n\n\nnotebook\n\ndp.write_pandas(df = purchases,\n                gcs_path = f\"{bucket}/{folder}/data.parquet\")\n\nMen for de andre filformatene mÃ¥ vi altsÃ¥ spesifisere dette.\n\n\nNÃ¥r vi jobber med Parquet-fil i R, bruker vi pakken arrow. Dette er en del av fellesr pakken sÃ¥ du trenger kun Ã¥ kalle inn dette. Pakken inneholder funksjonen write_SSB som kan brukes til Ã¥ skrive data til bÃ¸tte pÃ¥ Dapla.\n\n\nnotebook\n\nlibrary(fellesr)\n\n# Set stien til hvor data skal lagres\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"felles/veiledning/r/eksempler/purchases\"\n\n# Lage en eksempel dataframe\npurchases = data.frame(A = runif(10), B= runif(10), C=runif(10))\n\n# Skrive data til bÃ¸ttet som en parquet\nwrite_SSB(purchases, file.path(bucket, folder, \"purchases.parquet\"))\n\nMerknad: NÃ¥r du spesifisere bÃ¸tter i R, trenger du ikke â€œgs://â€ foran.\n\n\n\n\n\n\nNoen ganger Ã¸nsker vi Ã¥ lagre data i andre formatter slik som CSV, JSON og XML.\n\nPython \n\n\ndapla-toolbelt kan skrive ut json, csv og posisjonsfiler (fixed-width-files/fwf) til lagringsomrÃ¥det. MÃ¥ten den gjÃ¸r det pÃ¥ er Ã¥ bruke Pandas sine funksjoner read_json, read_csv, read_fwf under panseret. Dette kan vÃ¦re nyttig Ã¥ vite for skjÃ¸nne hvordan dapla-toolbelt hÃ¥ndterer ulike strukturer i (spesielt hierarkiske) tekstfiler. Under ser du hvordan du kan skrive ut en dataframe til en json-fil.\n\n\nnotebook\n\nimport numpy as np\nimport pandas as pd\nfrom dapla import FileClient\n\n# Set path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Create a dataframe with Pandas\ndf = pd.DataFrame(np.random.randn(10, 5), columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\n# Save dataframe as json with dapla-toolbelt\ndp.write_pandas(df = df,\n                gcs_path = f\"{bucket}/{folder}/test.json\",\n                file_format = \"json\")\n\nSom vi ser at syntaksen over sÃ¥ kunne vi skrevet ut til noe annet enn json ved Ã¥ endre verdien i argumentet file_format.\n\n\nPakken fellesr kan ogsÃ¥ brukes til Ã¥ skrive andre type filer, for eksempel csv, til bÃ¸tter. Dette gjÃ¸res med funksjonen write_SSB og spesifisere Ã¸nsket filtype i filnavn.\nFÃ¸rst kaller vi biblioteket og lage noe test data ved:\n\n\nnotebook\n\nlibrary(fellesr)\n\n# Set stien til hvor data skal lagres\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"felles/veiledning/r/eksempler/purchases\"\n\n# Lage en eksempel dataframe\npurchases = data.frame(A = runif(10), B= runif(10), C=runif(10))\n\n# Skrive til csv\nwrite_SSB(purchases, file.path(bucket, folder, \"purchases.csv\")\n\n\n\n\n\n\n\nDet er ikke anbefalt Ã¥ bruke xlsx-formatet, men her ser du hvordan det kan skrives ut. Mer kommer.\n\nPython \n\n\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import AuthClient\n\n# Henter token for Ã¥ kunne lese fra Dapla\ntoken = AuthClient.fetch_google_credentials()\n\n# Filsti\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\ndf.to_excel(f\"{bucket}/{folder}/test.xlsx\",\n           storage_options={\"token\": token})\n\n\n\nKommer snart\n\n\n\n\n\n\n\nUnder finner du eksempler pÃ¥ hvordan du kan lese inn data til en Jupyter Notebooks pÃ¥ Dapla.\n\n\n\nPython \n\n\n\n\nnotebook\n\nimport dapla as dp\n\n# Set path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Read path into pandas dataframe \ndp.read_pandas(gcs_path= f\"{bucket}/{folder}/data.parquet\",\n               file_format = \"parquet\",\n               columns = None,)\n\nSom vi sÃ¥ med write_pandas sÃ¥ er file_format default satt til parquet, og default for columns = None, sÃ¥ vi kunne ogsÃ¥ ha skrevet det slik:\ndp.read_pandas(gcs_path= f\"{bucket}/{folder}/data.parquet\")\ncolumns-argumentet er en liste med kolonnenavn som vi Ã¸nsker Ã¥ lese inn. Hvis vi ikke spesifiserer noen kolonner sÃ¥ vil alle kolonnene leses inn.\n\n\nPakken fellesr kan brukes til Ã¥ lese inn data. Funksjonen read_SSB() kan lese inn filer i flere format inkluderende parquet.\nHer er et eksempel av Ã¥ lese inn parquet fil â€œ1987â€.\n\n\nnotebook\n\nlibrary(fellesr)\n\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"R_smoke_test\"\n\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.parquet\"))\n\nVi kan ogsÃ¥ filtrere hvilke variabel vi Ã¸nsker Ã¥ lese inn ved Ã¥ spesifisere parameter col_select. For eksempel:\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.parquet\"),\n                    col_select = c(\"Year\", \"Month\"))\nInnlesning av parquet som er kartdata finner du her: Lese kartdata\n\n\n\n\n\n\nKommer mer snart. Python-koden under bygger pÃ¥ eksempelet over.\n\nPython \n\n\n\n\nnotebook\n\nimport dapla as dp\n\n# Path to write to\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Read in json-file from dapla-storage\ndf = dp.read_pandas(gcs_path = f\"{bucket}/{folder}/test3.json\",\n               file_format = \"json\")\n\n\n\nFunksjonen read_SSB() kan lese inn flere type av fil-format, slik som csv og json. Du trenger ikke Ã¥ endre koden, kun spesifisere hele filnavn.\nFÃ¸rst kaller vi inn biblioteket fellesr og spesifisere bÃ¸tte/mappen:\n\n\nnotebook\n\nlibrary(fellesr)\n\n# Filsti\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"R_smoke_test\"\n\n# Lese inn CSV-fil\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.csv\"))\n\nFor Ã¥ lese inn en json-fil kan skrive fÃ¸lgende:\n\n\nnotebook\n\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.json\"))\n\n\n\n\n\n\n\n\nPython \n\n\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import AuthClient\n\n# Hent token\ntoken = AuthClient.fetch_google_credentials()\n\n# Les inn fil\ndf = pd.read_excel(\"gs://ssb-prod-arbmark-skjema-data-produkt/test_gcp.xlsx\",\n    storage_options={\"token\": token})\n\n\n\nKommer snart\n\n\n\n\n\n\nHer er et eksempel pÃ¥ hvordan man leser inn en sas7bdat-fil pÃ¥ Dapla som har blitt generert i prodsonen.\n\nPython \n\n\n\n\nnotebook\n\nimport dapla as dp\n\nsti = \"gs://ssb-dapla-felles-data-delt-prod/felles/veiledning/sas/statbank_ledstill.sas7bdat\"\n\ndp.read_pandas(sti, file_format=\"sas7bdat\", encoding=\"latin1\")\n\n\n\nI produksjon sone (pÃ¥ bakken) kan R-pakken haven benyttes for Ã¥ lese inn .sas7bdat filer. Dette er ikke implementert i fellesr enda for lesing fra Dapla bÃ¸tte.\nMer om dette kommer.\n\n\n\n\n\n\n\nÃ… slette filer fra lagringsomrÃ¥det kan gjÃ¸res pÃ¥ flere mÃ¥ter. I kapitlet om sletting av data viste vi hvordan man gjÃ¸r det med pek-og-klikk i Google Cloud Console. Under ser du hvordan du kan slette filer med dapla-toolbelt og gcsfs eller fellesr.\n\nPython \n\n\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\nfs.rm(f\"{bucket}/{from_folder}/df.json\")\n\n\n\nFunksjonen gc_delete_object kan brukes til Ã¥ slette data pÃ¥ lagringsomrÃ¥det.\n\n\nnotebook\n\nlibrary(fellesr)\n\nbucket &lt;- \"ssb-dapla-felles-data-delt-prod\"\nfolder &lt;- \"felles/veiledning/r/eksempler/purchases\"\n\ngcs_delete_object(file.path(bucket, folder, \"purchases.parquet\"))\n\n\n\n\n\n\n\nÃ… kopiere filer mellom mapper pÃ¥ et Linux-filsystem innebÃ¦rer som regel bruke cp-kommandoen. PÃ¥ Dapla er det ikke sÃ¥ mye forskjell. Vi bruker en ligende tilnÃ¦rming nÃ¥ vi skal kopiere mellom bÃ¸tter eller mapper pÃ¥ lagringsomrÃ¥det til SSB. Under ser du hvordan du kan kopiere en fil fra en mappe til en annen.\n\nPython \n\n\nLa oss begynne med et eksempel der vi kopierer en fil fra en mappe til en annen i samme bÃ¸tte.\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Path to folders\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfrom_folder = \"felles/veiledning/python/eksempler/purchases\"\nto_folder = \"felles/veiledning/python/eksempler\"\n\n# Copy file\nfs.cp(f\"{bucket}/{from_folder}/data.parquet\",\n      f\"{bucket}/{to_folder}/data_copy.parquet\")\n\nDet ogsÃ¥ fungere for Ã¥ kopiere filer mellom bÃ¸tter.\nEt annet scenario vi ofte vil stÃ¸te pÃ¥ er at vi Ã¸nsker Ã¥ kopiere en fil fra vÃ¥rt Jupyter-filsystem til en mappe pÃ¥ lagringsomrÃ¥det. Her kan vi bruke fs.put-metoden.\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Create a new file in your home directory called test.txt\nwith open('/home/jovyan/test.txt', 'w') as f:\n    f.write('Create a new text file!')\n\n#Path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler\"\n\n# Copy file from local to remote file system\nfs.put(lpath=f\"/home/jovyan/test.txt\", rpath=f\"{bucket}/{folder}/test.txt\")\n\nÃ˜nsker vi Ã¥ kopiere en hel mappe fra lagringsomrÃ¥det til Jupyter-filsystemet, kan vi bruke fs.get-metoden, med opsjonen recursive=True.\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Copy file\nfs.get(&lt;from_bucket&gt;,\n      \"/home/jovyan/sesongjustering/\",\n      recursive=True)\n\n\n\nKommer snart\n\n\n\n\n\n\n\nPython \n\n\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfrom_folder = \"felles/veiledning/python/eksempler/purchases\"\nto_folder = \"felles/veiledning/python/eksempler\"\n\nfs.mv(f\"{bucket}/{from_folder}/data.parquet\", f\"{bucket}/{to_folder}/data.parquet\")\n\n\n\nKommer snart\n\n\n\n\n\n\nSelv om bÃ¸tter ikke har mapper med en hierarkisk struktur slik man er kjent med fra klassike filsystemer, sÃ¥ kan man opprette det som ser ut som mapper i objektnavnet. I realiteten blir bare / oppfattet som en del av navnet pÃ¥ objektet. Skulle du likevel Ã¸nske Ã¥ opprette dette sÃ¥ kan du gjÃ¸re det fÃ¸lgende mÃ¥te:\n\nPython \n\n\n\n\nnotebook\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n#Path to folder\nbucket = \"gs://ssb-dapla-felles-data-delt-prod\"\nfolder = \"felles/veiledning/python/eksempler\"\n\n# Create folder\nfs.touch(f\"{bucket}/{folder}/testmappe/\")\n\n\n\nKommer snart",
    "crumbs": [
      "Statistikere",
      "Data"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-data.html#footnotes",
    "href": "statistikkere/jobbe-med-data.html#footnotes",
    "title": "Jobbe med data",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI de tidligere systemene pÃ¥ bakken sÃ¥ var det ikke nÃ¸dvendig med autentisering mellom kodemiljÃ¸ og datalagringenâ†©ï¸",
    "crumbs": [
      "Statistikere",
      "Data"
    ]
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Eksempler",
    "section": "",
    "text": "Eksempler\nlsdkjflkdsjfklj",
    "crumbs": [
      "Eksempler"
    ]
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.html",
    "href": "notebooks/spark/sparkr-intro.html",
    "title": "Introduksjon til SparkR",
    "section": "",
    "text": "Akkurat som PySpark sÃ¥ gir SparkR oss et grensesnitt mot Apache Spark fra R. I denne notebooken viser vi noen eksempler hvordan du gjÃ¸re vanlige operasjoner med SparkR."
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.html#oppsett",
    "href": "notebooks/spark/sparkr-intro.html#oppsett",
    "title": "Introduksjon til SparkR",
    "section": "Oppsett",
    "text": "Oppsett\nEksemplene i notebooken bruker SparkR (k8s cluster) pÃ¥ https://jupyter.dapla.ssb.no/. Det vil si at den kan distribuere kjÃ¸ringene pÃ¥ flere maskiner i Kubernetes.\n\nspark\n\nJava ref type org.apache.spark.sql.SparkSession id 1"
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.html#lese-inn-fil",
    "href": "notebooks/spark/sparkr-intro.html#lese-inn-fil",
    "title": "Introduksjon til SparkR",
    "section": "Lese inn fil",
    "text": "Lese inn fil\n\nfile = read.parquet(\"gs://ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet\")\n\n\nselectedColumns &lt;- select(file, \"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\")\nshowDF(selectedColumns, numRows = 5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html",
    "href": "notebooks/spark/deltalake-intro.html",
    "title": "Introduksjon til Delta Lake",
    "section": "",
    "text": "Delta Lake er et databaselag som kan legges over parquet-filer i bÃ¸tter. Det kan gi oss mye av den funksjonaliteten vi har vÃ¦rt vant til i relasjonelle databaser og datavarehus. I denne notebooken vises hvordan man kan ta det i bruk pÃ¥ Dapla."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#oppsett",
    "href": "notebooks/spark/deltalake-intro.html#oppsett",
    "title": "Introduksjon til Delta Lake",
    "section": "Oppsett",
    "text": "Oppsett\nHvis du logger deg inn pÃ¥ https://jupyter.dapla.ssb.no/ kan du ta i bruk Delta Lake via Spark. Men for Ã¥ gjÃ¸re det mÃ¥ du installere delta-spark. For Ã¥ installere pakken mÃ¥ du jobbe i et ssb-project. I tillegg mÃ¥ du installere delta-spark-versjon som er kompatibel med Spark-versjonen som er installert pÃ¥ Dapla. GjÃ¸r derfor fÃ¸lgende:\n\nOpprett et ssb-project med kommandoen:\nssb-project create test-delta-lake\nI terminalen skriver du fÃ¸lgende for Ã¥ sjekke Spark-versjonen som er installert:\nspark-shell --version\nSjekk hvilken delta-spark-versjon som er kompatibel med din Spark-versjon og installer den med kommandoen1:\npoetry add delta-spark@2.3\nÃ…pne en ny notebook og velg kernel test-delta-lake.\n\nNÃ¥ har du satt opp et virtuelt miljÃ¸ med en PySpark-kernel som kjÃ¸rer en maskin (sÃ¥kalt Pyspark local kernel), der du har installert delta-spark. Vi kan nÃ¥ importere de bibliotekene vi trenger og sette igang en Spark-session.\n\n# Importerer biblioteker\nimport pyspark\nfrom delta import *\n\nDeretter initialiserer jeg en Spark-session. %%capture_output er med for Ã¥ forhindre at det ikke blir printet ut sensitiv informasjon i en notebook som skal inn i Dapla-manualen.\n\n%%capture captured_output\n%run /usr/local/share/jupyter/kernels/pyspark_local/init.py"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#genererer-noe-data",
    "href": "notebooks/spark/deltalake-intro.html#genererer-noe-data",
    "title": "Introduksjon til Delta Lake",
    "section": "Genererer noe data",
    "text": "Genererer noe data\n\n# Genererer noe data med Spark\ndata = spark.range(10, 15)\ndata.show()\n\n+---+\n| id|\n+---+\n| 10|\n| 11|\n| 12|\n| 13|\n| 14|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#skriver-ut-en-delta-lake-table",
    "href": "notebooks/spark/deltalake-intro.html#skriver-ut-en-delta-lake-table",
    "title": "Introduksjon til Delta Lake",
    "section": "Skriver ut en Delta Lake Table",
    "text": "Skriver ut en Delta Lake Table\nLa oss skrive ut Spark DataFrame som en Delta Lake Table og bli kjent med strukturen i objektet:\n\n%%time\ndata.write.format(\"delta\").mode(\"overwrite\").save(\n    \"gs://ssb-dapla-felles-data-delt-prod/temp4\"\n)\n\nCPU times: user 3.38 ms, sys: 1.7 ms, total: 5.08 ms\nWall time: 5.59 s\n\n\nVi kan deretter printe ut hva som ble opprettet nÃ¥r vi skrev ut en Delta Lake Table:\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/temp4/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp4/_delta_log',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000000.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-9b3b81a9-2771-4fb4-9f0f-659fd160d643-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-0f2f8ba5-3161-41e8-b5d1-2084128a5bed-c000.snappy.parquet']\n\n\n\nDelta Lake Tabellstruktur\nMappenstrukturen du ser over er typisk for en Delta Lake-tabell. La oss bryte ned komponentene:\n\n_delta_log/:\n\nDette er transaksjonsloggmappen for Delta Lake-tabellen. Den inneholder en serie med JSON-filer som registrerer alle transaksjoner gjort pÃ¥ tabellen.\nTransaksjonsloggen er avgjÃ¸rende for Delta Lakes ACID-transaksjonsegenskaper, som muliggjÃ¸r funksjoner som atomiske forpliktelser, tilbakerullinger og tid-reise-forespÃ¸rsler.\n\n_delta_log/00000000000000000000.json:\n\nDette er en JSON-fil innenfor transaksjonsloggkatalogen. Hver JSON-fil i denne mappen tilsvarer en transaksjon (eller en batch av transaksjoner) gjort pÃ¥ tabellen.\nFilnavnet er null-fylt og representerer transaksjonsversjonen. I dette tilfellet representerer 00000000000000000000.json den aller fÃ¸rste versjonen (versjon 0) av tabellen. Etter hvert som flere transaksjoner blir gjort, vil nye filer bli lagt til med Ã¸kende versjonsnumre.\n\npart-00000-450715bd-6b0c-4827-bb8a-b0265505ca72-c000.snappy.parquet og part-00001-977ed55f-5ce5-469f-8a1d-9eafb143c215-c000.snappy.parquet:\n\nDette er de faktiske datafilene til Delta Lake-tabellen, lagret i Parquet-format.\n.snappy.parquet-utvidelsen indikerer at dataene er komprimert ved hjelp av Snappy-komprimeringsalgoritmen.\nFilnavnene er typiske for Sparks navngivningskonvensjon for datadeler. Prefiksene part-00000 og part-00001 indikerer partisjonsnumre. De lange UUID-lignende strengene er unike identifikatorer for datafilene. Suffikset c000 indikerer Spark-oppgaven som skrev filen.\n\n\nMappen representerer en Delta Lake-tabell med data lagret i Parquet-format og en transaksjonslogg som sporer endringer i tabellen. TilstedevÃ¦relsen av _delta_log-mappen og innholdet er det som skiller en Delta Lake-tabell fra et vanlig Parquet-datasett."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#lese-inn-tabell",
    "href": "notebooks/spark/deltalake-intro.html#lese-inn-tabell",
    "title": "Introduksjon til Delta Lake",
    "section": "Lese inn tabell",
    "text": "Lese inn tabell\n\ndeltaTable = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-delt-prod/temp4\")\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 13|\n| 14|\n| 10|\n| 11|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#modifisere-datasettet",
    "href": "notebooks/spark/deltalake-intro.html#modifisere-datasettet",
    "title": "Introduksjon til Delta Lake",
    "section": "Modifisere datasettet",
    "text": "Modifisere datasettet\nLa oss modifisere datasettet ved Ã¥ bytte ut verdien 13 med 15 i id-kolonnen:\n\nfrom pyspark.sql.functions import col, lit\n\n# Update the cell with value 13 to 15\ndeltaTable.update(condition=(col(\"id\") == 13), set={\"id\": lit(15)})\n\nEt viktig poeng Ã¥ fÃ¥ med seg her er at vi nÃ¥ oppdaterte Delta Lake Table objektet bÃ¥de i minnet og pÃ¥ disk. La oss bevise det ved Ã¥ lese inn fra disk:\n\ndeltaTable2 = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-delt-prod/temp4\")\ndeltaTable2.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n+---+\n\n\n\nOg deretter ved Ã¥ printe ut det opprinnelige objektet vi leste inn:\n\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#append-data",
    "href": "notebooks/spark/deltalake-intro.html#append-data",
    "title": "Introduksjon til Delta Lake",
    "section": "Append data",
    "text": "Append data\nLa oss legge til verdiene 20 og 21 til datasettet. FÃ¸rst lager vi en Spark dataframe:\n\nnew_data = [(20,), (21,)]\nnew_df = spark.createDataFrame(new_data, [\"id\"])\nnew_df.show()\n\n+---+\n| id|\n+---+\n| 20|\n| 21|\n+---+\n\n\n\nDeretter kan vi appendere det til vÃ¥r opprinnelige dataframe:\n\nnew_df.write.format(\"delta\").mode(\"append\").save(\n    \"gs://ssb-dapla-felles-data-delt-prod/temp4\"\n)\n\n\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n| 12|\n| 15|\n| 14|\n| 10|\n| 11|\n| 21|\n| 20|\n+---+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#historien-og-metadata-til-filen",
    "href": "notebooks/spark/deltalake-intro.html#historien-og-metadata-til-filen",
    "title": "Introduksjon til Delta Lake",
    "section": "Historien og metadata til filen",
    "text": "Historien og metadata til filen\nNÃ¥ som vi har gjort noen endringer kan vi se pÃ¥ historien til filen:\n\n# Lister ut filene i bÃ¸tta\nfs = FileClient.get_gcs_file_system()\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/temp4/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp4/_delta_log',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000000.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000001.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000002.json',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-73e5052f-1b82-48da-ab37-2cbc01bb46c1-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-9b3b81a9-2771-4fb4-9f0f-659fd160d643-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00000-d04d0ca2-8e8b-42e9-a8a3-0fed9a0e4e41-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-0f2f8ba5-3161-41e8-b5d1-2084128a5bed-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp4/part-00001-30d707e4-dd9a-4bfd-a4c7-7fbb1933e9ae-c000.snappy.parquet']\n\n\nVi ser av transaksjonsloggen i _delta_log-mappen at det nÃ¥ har vÃ¦rt 3 transaksjoner pÃ¥ datasettet. vi ser ogsÃ¥ av navnene pÃ¥ parquet-filene, part-00000 og part-00001, at det finnes to versjoner av filen. Hvis vi Ã¸nsker Ã¥ bli bedre kjent med hva slags informasjon som blir lagret fra transaksjonene, sÃ¥ kan vi printe ut den siste transaksjonen som heter 00000000000000000002.json:\n\nimport json\n\nfrom dapla import FileClient\n\n# Kobler oss pÃ¥ bÃ¸ttene\nfs = FileClient.get_gcs_file_system()\n\n# Filsti\npath = \"gs://ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000002.json\"\n\nwith fs.open(path, \"r\") as f:\n    for line in f:\n        data = json.loads(line)\n        pretty_content = json.dumps(data, indent=4)\n        print(pretty_content)\n        print(\"-\" * 50)  # Print separator for clarity\n\n{\n    \"commitInfo\": {\n        \"timestamp\": 1696942544879,\n        \"operation\": \"WRITE\",\n        \"operationParameters\": {\n            \"mode\": \"Append\",\n            \"partitionBy\": \"[]\"\n        },\n        \"readVersion\": 1,\n        \"isolationLevel\": \"Serializable\",\n        \"isBlindAppend\": true,\n        \"operationMetrics\": {\n            \"numFiles\": \"2\",\n            \"numOutputRows\": \"2\",\n            \"numOutputBytes\": \"956\"\n        },\n        \"engineInfo\": \"Apache-Spark/3.3.1 Delta-Lake/2.3.0\",\n        \"txnId\": \"a3dcd582-8362-4fc2-a8ce-57613d2eb2b8\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00000-73e5052f-1b82-48da-ab37-2cbc01bb46c1-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 478,\n        \"modificationTime\": 1696942544755,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":20},\\\"maxValues\\\":{\\\"id\\\":20},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00001-30d707e4-dd9a-4bfd-a4c7-7fbb1933e9ae-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 478,\n        \"modificationTime\": 1696942544833,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":21},\\\"maxValues\\\":{\\\"id\\\":21},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n\n\nHer ser vi at vi fÃ¥r masse informasjon om endringen, bÃ¥de metadata om transaksjonen i commitInfo, og informasjon om nye data-filer i add-delen. Vi ser at det er en veldig rik beskrivelse av endringer, men det kan vÃ¦re vanskeig Ã¥ lese innholdet direkte. La oss heller bruke history()-funksjonen som kommer med delta:\n\nhistory = deltaTable.history()\nhistory.show()\n\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n|      2|2023-10-10 12:55:...|  null|    null|    WRITE|{mode -&gt; Append, ...|null|    null|     null|          1|  Serializable|         true|{numFiles -&gt; 2, n...|        null|Apache-Spark/3.3....|\n|      1|2023-10-10 12:55:...|  null|    null|   UPDATE|{predicate -&gt; (id...|null|    null|     null|          0|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n|      0|2023-10-10 12:55:...|  null|    null|    WRITE|{mode -&gt; Overwrit...|null|    null|     null|       null|  Serializable|        false|{numFiles -&gt; 2, n...|        null|Apache-Spark/3.3....|\n+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n\n\n\nSiden det blit trangt i tabellen over sÃ¥ kan vi velge hvilke variabler vi Ã¸nsker Ã¥ se pÃ¥:\n\n# Oversikt over alle kolonner som finnes i historien\nhistory.columns\n\n['version',\n 'timestamp',\n 'userId',\n 'userName',\n 'operation',\n 'operationParameters',\n 'job',\n 'notebook',\n 'clusterId',\n 'readVersion',\n 'isolationLevel',\n 'isBlindAppend',\n 'operationMetrics',\n 'userMetadata',\n 'engineInfo']\n\n\n\n# Velger de kolonnene jeg er interessert i\nselected_history = deltaTable.history().select(\n    \"version\", \"timestamp\", \"operation\", \"operationParameters\"\n)\n\n\n# Display the selected columns\nselected_history.show(truncate=50)\n\n+-------+-----------------------+---------+--------------------------------------+\n|version|              timestamp|operation|                   operationParameters|\n+-------+-----------------------+---------+--------------------------------------+\n|      2|2023-10-10 12:55:45.014|    WRITE|   {mode -&gt; Append, partitionBy -&gt; []}|\n|      1|2023-10-10 12:55:37.054|   UPDATE|        {predicate -&gt; (id#4452L = 13)}|\n|      0|2023-10-10 12:55:29.048|    WRITE|{mode -&gt; Overwrite, partitionBy -&gt; []}|\n+-------+-----------------------+---------+--------------------------------------+"
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#egendefinert-metadata",
    "href": "notebooks/spark/deltalake-intro.html#egendefinert-metadata",
    "title": "Introduksjon til Delta Lake",
    "section": "Egendefinert metadata",
    "text": "Egendefinert metadata\nDelta Lake stÃ¸tter ogsÃ¥ egendefinert metadata. Det kan for eksempel vÃ¦re nyttig hvis man Ã¸nsker Ã¥ bruke Delta Lake som en backend for et GUI som lar brukeren oppdatere en tabell fra GUI-et. Da Ã¸nsker man typisk Ã¥ lagre hvem som gjorde endringer og nÃ¥r det ble gjort. La oss legge pÃ¥ noe slik metadata:\n\n# Leser inn filen\ndf = spark.read.format(\"delta\").load(\"gs://ssb-dapla-felles-data-delt-prod/temp4\")\n\n\n\nCode\n# Lagrer egendefinert metadata i en json-fil\nimport json\n\nmetadata = {\n    \"comment\": \"Kontaktet oppgavegiver og kranglet!\",\n    \"manueltEditert\": \"True\",\n    \"maskineltEditert\": \"False\",\n}\nmetadata\n\n\n{'comment': 'Kontaktet oppgavegiver og kranglet!',\n 'manueltEditert': 'True',\n 'maskineltEditert': 'False'}\n\n\nVi kan deretter appende dette til den siste versjonen av fila.\n\n(\n    df.write.format(\"delta\")\n    .mode(\"append\")\n    .option(\"userMetadata\", json.dumps(metadata))  # Serialize metadata to a string\n    .save(\"gs://ssb-dapla-felles-data-delt-prod/temp4\")\n)\n\n\n# Laster inn tabellen\ndeltaTable = DeltaTable.forPath(spark, \"gs://ssb-dapla-felles-data-delt-prod/temp4\")\n\n# Henter ut historien\nhistory_df = deltaTable.history()\n\n\n# Show the operation details, including metadata\nhistory_df.select(\n    \"version\", \"timestamp\", \"operation\", \"operationParameters\", \"userMetadata\"\n).show(truncate=10)\n\n+-------+----------+---------+-------------------+------------+\n|version| timestamp|operation|operationParameters|userMetadata|\n+-------+----------+---------+-------------------+------------+\n|      3|2023-10...|    WRITE|         {mode -...|  {\"comme...|\n|      2|2023-10...|    WRITE|         {mode -...|        null|\n|      1|2023-10...|   UPDATE|         {predic...|        null|\n|      0|2023-10...|    WRITE|         {mode -...|        null|\n+-------+----------+---------+-------------------+------------+\n\n\n\n\nhistory_df.select(\"version\", \"userMetadata\").show(truncate=50)\n\n+-------+--------------------------------------------------+\n|version|                                      userMetadata|\n+-------+--------------------------------------------------+\n|      3|{\"comment\": \"Kontaktet oppgavegiver og kranglet...|\n|      2|                                              null|\n|      1|                                              null|\n|      0|                                              null|\n+-------+--------------------------------------------------+\n\n\n\nVi ser at vi la til vÃ¥r egen metadata i versjon 3 av fila. Vi kan printe ut den rÃ¥ transaksjonsloggen som tidligere, men nÃ¥ er vi pÃ¥ transaksjon 3 00000000000000000003.json:\n\nfrom dapla import FileClient\n\n# Kobler oss pÃ¥ bÃ¸ttene\nfs = FileClient.get_gcs_file_system()\n\n# Filsti\npath = \"gs://ssb-dapla-felles-data-delt-prod/temp4/_delta_log/00000000000000000003.json\"\n\nwith fs.open(path, \"r\") as f:\n    for line in f:\n        data = json.loads(line)\n        pretty_content = json.dumps(data, indent=4)\n        print(pretty_content)\n        print(\"-\" * 50)  # Print separator for clarity\n\n{\n    \"commitInfo\": {\n        \"timestamp\": 1696942553907,\n        \"operation\": \"WRITE\",\n        \"operationParameters\": {\n            \"mode\": \"Append\",\n            \"partitionBy\": \"[]\"\n        },\n        \"readVersion\": 2,\n        \"isolationLevel\": \"Serializable\",\n        \"isBlindAppend\": false,\n        \"operationMetrics\": {\n            \"numFiles\": \"2\",\n            \"numOutputRows\": \"7\",\n            \"numOutputBytes\": \"989\"\n        },\n        \"userMetadata\": \"{\\\"comment\\\": \\\"Kontaktet oppgavegiver og kranglet!\\\", \\\"manueltEditert\\\": \\\"True\\\", \\\"maskineltEditert\\\": \\\"False\\\"}\",\n        \"engineInfo\": \"Apache-Spark/3.3.1 Delta-Lake/2.3.0\",\n        \"txnId\": \"e7de92bf-b0f9-4341-8bbb-9b382f2f3eb6\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00000-96369f3d-fe4a-4365-a0df-00c813027399-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 503,\n        \"modificationTime\": 1696942553856,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":5,\\\"minValues\\\":{\\\"id\\\":10},\\\"maxValues\\\":{\\\"id\\\":15},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n{\n    \"add\": {\n        \"path\": \"part-00001-0f1bc8e6-093b-49a9-ad0b-78d5a148cfb6-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 486,\n        \"modificationTime\": 1696942553853,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":2,\\\"minValues\\\":{\\\"id\\\":20},\\\"maxValues\\\":{\\\"id\\\":21},\\\"nullCount\\\":{\\\"id\\\":0}}\"\n    }\n}\n--------------------------------------------------\n\n\nVi er da at metadataene ligger som forventet i metadata-delen."
  },
  {
    "objectID": "notebooks/spark/deltalake-intro.html#footnotes",
    "href": "notebooks/spark/deltalake-intro.html#footnotes",
    "title": "Introduksjon til Delta Lake",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI eksempelet er det Spark V3.3.1 som er installert og jeg installerer derfor delta-spark v2.3â†©ï¸"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html",
    "href": "notebooks/spark/pyspark-intro.html",
    "title": "Introduksjon til PySpark",
    "section": "",
    "text": "Apache Spark er et sterkt verktÃ¸y som utvider mulighetsrommet for databehandling med R og Python. Kjernefunksjonaliteten ligger i at den lar deg kjÃ¸re en jobb pÃ¥ flere maskiner samtidig, noe som ikke er mulig med klassiske rammeverk som Pandas og Tidyverse. FÃ¸lgelig er det et rammeverk som blant annet er veldig egnet for Ã¥ prosessere store datamengder eller gjÃ¸re store beregninger. Les om mer om Apache Spark i Dapla-manualen\nI denne notebooken vises noen enkle eksempler pÃ¥ hvordan du kan jobbe med data med PySpark, et Python-grensesnitt mot Spark."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#oppsett",
    "href": "notebooks/spark/pyspark-intro.html#oppsett",
    "title": "Introduksjon til PySpark",
    "section": "Oppsett",
    "text": "Oppsett\nNÃ¥r du logger deg inn pÃ¥ Dapla kan du velge mellom 2 ferdigoppsatte kernels for Ã¥ jobbe med PySpark:\n\nPyspark (local)\nPyspark (k8s cluster)\n\nDen fÃ¸rste lar deg bruke Spark pÃ¥ en enkeltmaskin, mens den andre lar deg distribuere kjÃ¸ringen pÃ¥ mange maskiner avhengig av hvor store jobbene er. I eksemplene under brukes Pyspark (local).\n\n# Importer biblioteker\nimport numpy as np\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import date_format, explode, expr, sequence, sum, avg\nfrom pyspark.sql.types import DateType, DoubleType, StructField, StructType\n\n# Initialierer en SparkSession\nspark = (\n    SparkSession.builder.master(\"local[2]\")\n    .appName(\"Dapla-manual-example\")\n    .getOrCreate()\n)\n\nI koden over importerer vi de bibliotekene vi skal bruke under. Grunnen til at vi importerer pyspark.sql er at dette er at Spark SQL er Apache Spark sin modul for Ã¥ jobbe med strukturerte data. Og som navnet tilsier vil det si at vi kan blande Python og SQL i koden vÃ¥r. Vi skal nÃ¦rmere pÃ¥ hvordan man bruke SQL fra PySpark-notebook senere.\nSpark tilbyr ogsÃ¥ et eget grensesnitt, Spark UI, for Ã¥ monitorere hva som skjer under en SparkSession. Vi kan bruke fÃ¸lgende kommando for Ã¥ fÃ¥ opp en lenke til Spark UI i notebooken vÃ¥r:\n\nspark.sparkContext\n\n\n        \n            SparkContext\n\n            Spark UI\n\n            \n              Version\n                v3.3.1\n              Master\n                local[*]\n              AppName\n                pyspark-shell\n            \n        \n        \n\n\nKlikker du pÃ¥ Spark UI-lenken sÃ¥ tar den deg til dashboard som lar deg monitorere, debugge, optimalisere og forstÃ¥ kjÃ¸ringene dine. Det kan vÃ¦re et svÃ¦rt nyttig verktÃ¸y i mange tilfeller."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#generere-data",
    "href": "notebooks/spark/pyspark-intro.html#generere-data",
    "title": "Introduksjon til PySpark",
    "section": "Generere data",
    "text": "Generere data\nVi kan begynne med Ã¥ generere en Spark DataFrame med en kolonne som inneholder mÃ¥nedlige datoer for perioden 2000M1-2023M8.\n\n# Genererer mÃ¥nedlige data\ndates_df = spark.range(1).select(\n    explode(\n        sequence(\n            start=expr(\"date '2000-01-01'\"),\n            stop=expr(\"date '2023-08-01'\"),\n            step=expr(\"interval 1 month\"),\n        )\n    ).alias(\"Date\")\n)\ndates_df.show(5)\n\n+----------+\n|      Date|\n+----------+\n|2000-01-01|\n|2000-02-01|\n|2000-03-01|\n|2000-04-01|\n|2000-05-01|\n+----------+\nonly showing top 5 rows\n\n\n\nEn Spark DataFrame er en distribuert samling av data som er organisert inn i kolonner. Siden Spark lar deg distribuere kjÃ¸ringer pÃ¥ flere maskiner, er DataFrames optimalisert for Ã¥ kunne splittes opp slik at de kan brukes pÃ¥ flere maskiner. Med andre er dette ikke det samme som en Pandas dataframe mange kjenner fra fÃ¸r.\nOver genererte vi en datokolonne. For Ã¥ fÃ¥ litt mer data kan vi ogsÃ¥ generere 100 kolonner med tidsseriedata og sÃ¥ printer vi de 2 fÃ¸rste av disse:\n\n# Genererer random walk data\nschema = StructType(\n    [StructField(f\"serie{i:02d}\", DoubleType(), True) for i in range(100)]\n)\n\ndata = [\n    tuple((10 + np.random.normal(0, 1, 100)).cumsum().tolist())\n    for _ in range(284)  # 284 months from 2000-01 to 2023-08\n]\n\ndata_df = spark.createDataFrame(data, schema=schema)\n\ndata_df.select(\"serie00\", \"serie01\").show(5)\n\n+------------------+------------------+\n|           serie00|           serie01|\n+------------------+------------------+\n|10.410703377184355| 21.06318801110079|\n|10.509249410154466|  19.5674295298024|\n| 9.618310122060274|17.635805093465642|\n| 9.691112692298294|18.593842915949082|\n| 9.903675228685067|20.012215769058564|\n+------------------+------------------+\nonly showing top 5 rows\n\n\n\nTil slutt kan vi joine de to datasettene sammen og lage noen kolonner som viser Ã¥r, kvartal og mÃ¥ned. Deretter printer vi ut noen av kolonnene med kommandoen show().\n\n\nCode\n# Legger til row index til DataFrame fÃ¸r join med dates_df\ndata_df = data_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n\n# Joiner de to datasettene\ndf = (\n    dates_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n    .join(data_df, \"row_index\")\n    .drop(\"row_index\")\n)\n\n# Legger til Ã¥r, kvartal og mnd\ndf = df.withColumn(\"Year\", date_format(df.Date, \"yyyy\"))\ndf = df.withColumn(\"Quarter\", expr(\"quarter(Date)\"))\ndf = df.withColumn(\"Month\", date_format(df.Date, \"MM\"))\n\ndf.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows\n\n\n\nOg med det har vi noe data vi kan jobbe med i resten av notebooken."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#skrive-til-parquet",
    "href": "notebooks/spark/pyspark-intro.html#skrive-til-parquet",
    "title": "Introduksjon til PySpark",
    "section": "Skrive til Parquet",
    "text": "Skrive til Parquet\nPySpark tilbyr mange opsjoner ved skriving til parquet-filer som vi kanskje ikke er vant til Ã¥ forholde oss til med enklere rammeverk som Pandas. Den enkleste mÃ¥ten Ã¥ skrive ut en fil er som fÃ¸lger:\ndf.write.parquet(\n    \"gs://ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet\"\n)\nDette vil fungere hvis filen ikke finnes fra fÃ¸r. Hvis den finnes fra fÃ¸r sÃ¥ vil den feile. Grunnen er at vi ikke har spesifisert hva vi Ã¸nsker at den skal gjÃ¸re. Vi kan velge mellom overwrite, append, ignore eller errorifexists. Sistnevnte er ogsÃ¥ default-oppfÃ¸rsel hvis du ikke ber den gjÃ¸re noe annet.\nUnder bruker vi opsjonen overwrite, det vil si at den skriver over en evt eksisterende fil med samme navn.\n\ndf.write.mode(\"overwrite\").parquet(\n    \"gs://ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet\"\n)\n\nVi kan inspisere hva som ble skrevet ved Ã¥ liste ut innholder i bÃ¸tta.\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/temp/**\")\n\n['ssb-dapla-felles-data-delt-prod/temp/',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/_SUCCESS',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/part-00000-b32e7299-0590-4b31-bcc2-dc3d58725529-c000.snappy.parquet',\n 'ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet/part-00001-b32e7299-0590-4b31-bcc2-dc3d58725529-c000.snappy.parquet']\n\n\nHvis denne parquet-filen hadde vÃ¦rt partisjonert etter en kolonne, sÃ¥ ville det vÃ¦rt egne undermapper med navnestruktur column_name=value som indikerte hva filen er partisjonert pÃ¥. Siden vi her bruker en maskin og har et lite datasett, valgte Spark Ã¥ ikke partisjonere."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#lese-inn-parquet",
    "href": "notebooks/spark/pyspark-intro.html#lese-inn-parquet",
    "title": "Introduksjon til PySpark",
    "section": "Lese inn Parquet",
    "text": "Lese inn Parquet\nApache Spark kan lese inn flere parquet-filer samtidig. Syntaxen er like enkel som den for Ã¥ skrive ut.\n\ndf_ts = spark.read.parquet(\n    \"gs://ssb-dapla-felles-data-delt-prod/temp/timeseries.parquet\"\n)\ndf_ts.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie66\", \"serie55\").show(5)\n\n+----------+----+-------+-----+-----------------+-----------------+\n|      Date|Year|Quarter|Month|          serie66|          serie55|\n+----------+----+-------+-----+-----------------+-----------------+\n|2000-01-01|2000|      1|   01|670.2679830025959|562.4312808525777|\n|2000-02-01|2000|      1|   02|675.4233411662802|562.5168447360121|\n|2000-03-01|2000|      1|   03|687.3412458214908|568.6203957584232|\n|2000-04-01|2000|      2|   04|673.1128047244955|557.4633871253379|\n|2000-05-01|2000|      2|   05| 667.513406101114|561.7766450346327|\n+----------+----+-------+-----+-----------------+-----------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#pyspark-og-sql",
    "href": "notebooks/spark/pyspark-intro.html#pyspark-og-sql",
    "title": "Introduksjon til PySpark",
    "section": "PySpark og SQL",
    "text": "PySpark og SQL\nDu kan ogsÃ¥ skrive SQL med Spark. For Ã¥ skrive SQL mÃ¥ vi fÃ¸rst lage et temporary view. Under kaller vi viewt for tidsserie.\n\ndf.createOrReplaceTempView(\"tidsserie\")\n\nVi kan deretter skrive en SQL-statement som vi Ã¸nsker Ã¥ kjÃ¸re pÃ¥ viewet:\n\nquery = \"SELECT * FROM tidsserie WHERE Year = 2010\"\n\nDeretter kan vi bruke det til Ã¥ filtrere datasettet:\n\nresult_df = spark.sql(query)\nresult_df.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2010-01-01|2010|      1|   01| 11.26910423907778|21.730128215168268|\n|2010-02-01|2010|      1|   02| 8.722783282690738| 17.46851086792347|\n|2010-03-01|2010|      1|   03|10.376873608348605|20.109386343182802|\n|2010-04-01|2010|      2|   04|11.459959305590926|21.995141825523866|\n|2010-05-01|2010|      2|   05|10.441456792180572| 21.25096473981906|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#aggregering",
    "href": "notebooks/spark/pyspark-intro.html#aggregering",
    "title": "Introduksjon til PySpark",
    "section": "Aggregering",
    "text": "Aggregering\nvi kan aggregere opp enten med PySpark-syntax eller SQL-syntax. Under viser vi begge:\n\nfrom pyspark.sql import functions as F\n\n# Assuming df_ts is your DataFrame\naggregated_df = df_ts.groupBy(\"Quarter\").agg(F.sum(\"serie00\").alias(\"Sum\"),\n                                             F.avg(\"serie00\").alias(\"Average\"),\n                                             F.max(\"serie00\").alias(\"Maximum\"))\n\n# Show the result\naggregated_df.show()\n\n+-------+------------------+------------------+------------------+\n|Quarter|               Sum|           Average|           Maximum|\n+-------+------------------+------------------+------------------+\n|      1|363.95869885234185|10.109963857009497|11.829453550532005|\n|      3|365.68324879453405| 10.15786802207039|12.233378837422391|\n|      4| 342.2334082209804|10.065688477087658|12.210138970053695|\n|      2|  361.991445506568|10.055317930738001|12.276030776082463|\n+-------+------------------+------------------+------------------+\n\n\n\nLa oss gjÃ¸re det samme med SQL, men grupperer etter to variabler og sorterer output etterpÃ¥.\n\n# Assuming df_ts is your DataFrame\ndf_ts.createOrReplaceTempView(\"temp_table\")\n\n# Now you can run a SQL query\nquery = \"\"\"\n    SELECT\n        Year,\n        Quarter,\n        SUM(serie00) AS Sum,\n        AVG(serie00) AS Average,\n        MAX(serie00) AS Maximum\n    FROM \n        temp_table\n    GROUP BY \n        Year, Quarter\n    ORDER BY\n        YEAR, QUARTER\n\"\"\"\n\naggregated_df_sql = spark.sql(query)\n\n# Show the result\naggregated_df_sql.show(10)\n\n+----+-------+------------------+------------------+------------------+\n|Year|Quarter|               Sum|           Average|           Maximum|\n+----+-------+------------------+------------------+------------------+\n|2000|      1|31.323050432219453|10.441016810739818|11.118293927071951|\n|2000|      2|28.911192473027377| 9.637064157675793| 9.900977410685329|\n|2000|      3|33.670797229797415|11.223599076599138|12.233378837422391|\n|2000|      4|28.094793356286914| 9.364931118762305| 10.32000478359274|\n|2001|      1|31.636678535169537|10.545559511723178|11.367822302191831|\n|2001|      2|29.629770128521507| 9.876590042840503|11.135215930381191|\n|2001|      3| 30.75408440118315| 10.25136146706105|10.723803326978505|\n|2001|      4|30.361048932627902|  10.1203496442093|10.368365984482093|\n|2002|      1|31.184163218551227|10.394721072850409|10.550579652234951|\n|2002|      2|29.128978392451202|   9.7096594641504|10.186365745367246|\n+----+-------+------------------+------------------+------------------+\nonly showing top 10 rows"
  },
  {
    "objectID": "statistikkere/hva-er-botter.html",
    "href": "statistikkere/hva-er-botter.html",
    "title": "Hva er bÃ¸tter?",
    "section": "",
    "text": "PÃ¥ Dapla er det Google Cloud Storage (GCS) som benyttes til Ã¥ lagre data og filer. FÃ¸lgelig er det GCS som erstatter det vi kjente som Linux-stammene i prodsonen tidligere. I SSB har vi vÃ¦rt vant til Ã¥ jobbe med data lagret pÃ¥ filsystemer i et Linux-miljÃ¸1. GCS-bÃ¸ttene skiller seg fra klassiske filsystemer pÃ¥ flere mÃ¥ter, og det er viktig Ã¥ vÃ¦re klar over disse forskjellene. I denne kapitlet vil vi gÃ¥ gjennom noen av de viktigste forskjellene og hvordan man gjÃ¸r vanlige operasjoner mot bÃ¸tter i GCS.\n\n\nI et Linux- eller Windows-filsystem, som vi har vÃ¦rt vant til tidligere, sÃ¥ er filer og mapper organisert i en hierarkisk struktur pÃ¥ et operativsystem (OS). I SSB har OS-ene vÃ¦rt installert pÃ¥ fysiske maskiner som vi vedlikeholder selv.\nEn bÃ¸tte i GCS er derimot en kjÃ¸pt tjeneste som lar brukeren lagre alle typer objekter i en container. Man trenger altsÃ¥ ikke Ã¥ tenke pÃ¥ om filene ligger i et hierarki, hvilket operativsystem det kjÃ¸rer pÃ¥, eller hvor mye diskplass som er tilgjengelig.\nAt data er lagret som objekter i en bÃ¸tte har noen praktiske implikasjoner for hvordan vi jobber med data i SSB:\n\nMange er vant til Ã¥ jobbe direkte med filer i en Linux-terminal eller via systemkall fra sprÃ¥k som SAS, Pyton eller R. For Ã¥ gjÃ¸re det samme i Jupyter mot en bÃ¸tte, sÃ¥ kan vi bruke Python-pakken gcsfs. Se eksempler under.\nNÃ¥r vi bruker Python- eller R-pakker for lese eller skrive data fra bÃ¸tter, sÃ¥ er vi avhengig av at pakkene tilbyr integrasjon mot bÃ¸tter. Mange pakker gjÃ¸r det, men ikke alle. For de som ikke gjÃ¸r det kan vi bruke ofte bruke gcsfs til Ã¥ gi oss et python-objekt, som igjen kan brukes av de fleste pakker.\nDet finnes egentlig ikke noen mapper i bÃ¸tter. I motsetning til et vanlig filsystem sÃ¥ er det ikke en hierarkisk mappestruktur i en bÃ¸tte. Det vil si at alt ligger et sted, men de kan ha lange navn med /-tegn slik at det ligner pÃ¥ et klassisk filsystem. Bruker du / i objekt-navnet sÃ¥ vil ogsÃ¥ Google Cloud Console vise det som mapper, men det er bare for Ã¥ gjÃ¸re det enklere Ã¥ forholde seg til. En praktisk konsekvens av dette er f.eks. at man ikke trenger Ã¥ opprette en mappe fÃ¸r man legger et fil/objekt i den. Det er bare en tekststreng som er en del av objekt-navnet.\n\nUnder kommer det en del eksempler pÃ¥ hvordan man kan jobbe med objekter i bÃ¸tter pÃ¥ samme mÃ¥te som filer i et filsystem.\n\n\n\nPÃ¥ Dapla skal data lagres i bÃ¸tter. Men nÃ¥r du Ã¥pner Jupyterlab sÃ¥ fÃ¥r du ogsÃ¥ et â€œlokaltâ€ eller klassisk filsystem, slik vi definerte det i forrige kapittel. Det er dette filsystemet du ser i filutforskeren i Jupyterlab, slik som vist til venstre i FigurÂ 1. Det er ogsÃ¥ dette filsystemet du ser nÃ¥r du f.eks. bruker ls-kommandoen i en terminal i Jupyterlab.\n\n\n\n\n\n\nFigurÂ 1: Til venstre i bildet ser du filutforskeren i Jupyterlab\n\n\n\nDette filsystemet er ment for Ã¥ lagre kode midlertidig mens du jobber med dem. Det er ikke ment for Ã¥ lagre data. Det er heller ikke ment som et permanent lagringssted for kode. Permanent lagring av kode skal gjÃ¸res pÃ¥ GitHub. Selv om filene du lagrer der fortsetter Ã¥ eksistere for hver gang du logger deg inn i Jupyterlab, sÃ¥ bÃ¸r kode du Ã¸nsker Ã¥ bevare pushes til GitHub fÃ¸r du avslutter en sesjon i Jupyterlab. Se en mer teknisk beskrivelse av hvordan dette fungerer i boksen under.\n\n\n\n\n\n\nHvem er egentlig denne Jovyan?\n\n\n\nNÃ¥r du logger deg inn i Jupyterlab pÃ¥ Dapla, sÃ¥ ser du at brukeren din pÃ¥ det lokale filsystemet heter jovyan. Grunnen er at det er en Linux-container som kjÃ¸rer et Jupyterlab Docker-image, og alle bruker denne containeren. Det er derfor vi kaller det et â€œlokaltâ€ filsystem, fordi det er lokalt i containeren.\nI tillegg er det satt opp persistent volum (PV) og en persistent volume claim (PVC) som er knyttet til denne containeren. Det er dette som gjÃ¸r at filene du lagrer i Jupyterlab blir lagret mellom hver gang du logger deg inn. I tillegg tas det en backup av filsystemene hver dag, slik at vi kan gjenopprette filene dine hvis noe skulle skje med PV-en.\nLagringsplassen pÃ¥ PV-en er begrenset til 10 GB per bruker. Grunnen til at vi ikke tilbyr mer lagringsplass er fordi det kun er kode skal lagres her, og at denne lagringsplassen er relativt dyrt sammenlignet med GCS-bÃ¸tter. Hvis du jobber i virtuelle miljÃ¸er og lagrer mange miljÃ¸er lokalt, kan du oppleve at disken blir full. Les mer her om hvordan man bÃ¸r hÃ¥ndere dette.\n\n\n\n\n\nTidligere har vi diskutert forskjellene mellom bÃ¸tter og filsystemer. Mange kjenner hvordan man gjÃ¸r systemkommandoer2 i klassiske filsystemer fra en terminal eller fra sprÃ¥k som Python, R og SAS. I dette kapitlet viser vi hvordan dette kan gjÃ¸res mot bÃ¸tter fra Jupyterlab.\ngcsfs er en Python-pakke som lar deg jobbe med objekter i bÃ¸tter pÃ¥ nesten samme mÃ¥te som filer i et filsystem. For Ã¥ kunne gjÃ¸re det mÃ¥ vi fÃ¸rst sette opp en filsystem-instans som lar oss bruke en bÃ¸tte som et filsystem. Pakken dapla-toolbelt lar oss gjÃ¸re det ganske enkelt:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs er nÃ¥ et filsystem-versjon av bÃ¸ttene vi har tilgang til pÃ¥ GCS. Vi kan nÃ¥ bruk fs til Ã¥ gjÃ¸re typiske operasjoner vi har vÃ¦rt vant til Ã¥ gjÃ¸re i en Linux-terminal. Se en fullstendig liste over kommandoer her.\nUnder vises noen flere eksempler pÃ¥ nyttige funksjoner enn det som ble vist i kapitlet tidligere i boken\n\n\n\n\n\n\nWarning\n\n\n\nSelv om ordet mapper blir brukt i eksemplene under, sÃ¥ er det viktig Ã¥ huske at det ikke finnes noen mapper i bÃ¸tter. Det er bare en tekststreng som er en del av objekt-navnet. Men siden vi her jobber med et filsystem-representasjon av bÃ¸tten, sÃ¥ tillater vi oss Ã¥ gjÃ¸re det for Ã¥ gjÃ¸re det enklere Ã¥ lese.\n\n\n\n\nfs.glob() lar oss sÃ¸ke etter filer i bÃ¸tten. Vi kan bruke *, **, ? og [..] som wildcard for Ã¥ finne det vi trenger. Det som returneres er enten en liste eller dictionary avhengig av hva vi velger Ã¥ gjÃ¸re.\nHent en liste over alle filer i en undermappe R_smoke_test i bÃ¸tta gs://ssb-dapla-felles-data-delt-prod:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/*\")\n\nNÃ¥r vi legger til * pÃ¥ slutten av filstien sÃ¥ returnerer den alle filer i den eksakte undermappen. Men hvis vi Ã¸nsker Ã¥ Ã¥ fÃ¥ alle filer i alle undermapper, sÃ¥ kan vi bruke ** pÃ¥ denne mÃ¥ten:\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/**\").\nVi kan ogsÃ¥ sÃ¸ke mer avansert ved ved Ã¥ bruke ?. ?-tegnet sier at en enkeltkarakter kan vÃ¦re hva som helst. F.eks. kunne vi skrevet\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/**/R_smoke_test_??.csv\")\nfor Ã¥ rekursivt liste ut alle filer som starter med R_smoke_test_, etterfulgt av to karakterer av hvilken som helst type, og slutter med .csv.\nHvis vi viste at de to karakterene ikke skulle vÃ¦re av hvilken som helst karakterer, men det var en liten bokstav, etterfulgt av et tall mellom 2-6, sÃ¥ kunne vi brukt [a-z] og [2-6] for Ã¥ spesifisere dette. F.eks. kunne vi skrevet:\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/**/R_smoke_test_[a-z][2-6].csv\")\nSom vi ser er fs.glob() et verktÃ¸y som git oss mye den funksjonaliteten vi er vant til fra Linux-terminalen. Men i tillegg kan vi bruke den til Ã¥ hente inn metadataene til de filene/objektene vi fÃ¥r treff pÃ¥, ved Ã¥ bruke argumentet detail=True. Her er et eksempel:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\n    \"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/**/R_smoke_test_/[a-z][2-6].csv\",\n    detail=True,\n)\n\nMetadataene gir deg da informasjon om filstÃ¸rrelse, tidspunkt for opprettelse, tidspunkt for siste endring, og hvem som har opprettet og endret filen.\n\n\n\nI noen tilfeller kan det vÃ¦re nyttig Ã¥ sjekke om en fil eksisterer i bÃ¸tten. Det kan vi gjÃ¸re med fs.exists():\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.exists(\"gs://ssb-dapla-felles-data-delt-prod/altinn3/form_dc551844cd74.xml\")\n\nKoden returnerer enten True eller False avhengig av om filen eksisterer eller ikke.\n\n\n\nHvis du lurer pÃ¥ hvor mange GB data du har i en bÃ¸tte, sÃ¥ kan du bruke fs.du():\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss pÃ¥ bÃ¸ttene\nfs = FileClient.get_gcs_file_system()\n\ntotal_bytes = fs.du(\n    \"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/\",\n)\n\ntotal_size_gb = total_bytes / (1024**3)\nprint(f\"Total size: {total_size_gb:.3f} GB\")\n\n\n\n\nHvert objekt i bÃ¸tta har metadata knyttet til seg som kan hentes inn i Jupyter og benyttes.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss pÃ¥ bÃ¸ttene\nfs = FileClient.get_gcs_file_system()\n\nfile = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/form_dc551844cd74.xml\"\n\nfs.info(file)\n\nDette gir deg blant annet informasjon om filstÃ¸rrelse, tidspunkt for opprettelse, og tidspunkt for siste endring. Hvis du Ã¸nsker dette for flere filer sÃ¥ kan man ogsÃ¥ bruke fs.glob(&lt;pattern&gt;, details=True) som vi sÃ¥ pÃ¥ tidligere.\n\n\n\nfs.ls() brukes for Ã¥ gi en liste av filer i et omrÃ¥de. Det kan brukes for bÃ¥de bÃ¸tter og mapper.\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.ls(\"gs://ssb-dapla-felles-data-delt-prod/altinn3\")\n\nKoden returnerer en liste.\n\n\n\nfs.open() lar oss Ã¥pne en fil i bÃ¸tta for lesing og skriving med vanlige Python-funksjoner. Funksjonen returnerer et fil-aktig objekt som kan brukes med vanlige som Python-biblioteker som Pandas og NumPy.\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import FileClient\n\n# Kobler oss pÃ¥ bÃ¸ttene\nfs = FileClient.get_gcs_file_system()\n\nfile_path = \"gs://ssb-dapla-felles-data-delt-prod/dapla-metrics/number-of-teams.csv\"\n\n# Bruker fs.open() til Ã¥ Ã¥pne fil, og deretter leses den med Pandas\nwith fs.open(file_path, \"r\") as file:\n    df = pd.read_csv(file)\ndf\n\nDu kan ogsÃ¥ bruke fs.open() til Ã¥ skrive til en fil i bÃ¸tta. Her er et eksempel pÃ¥ hvordan man skriver en parquet-fil med Pandas og PyArrow:\n\n\nnotebook\n\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom dapla import FileClient\n\nfile_path = \"gs://ssb-dapla-felles-data-delt-prod/dapla-metrics/test.parquet\"\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\n# Lager eksempeldata\ndata = {\"A\": [1, 2, 3], \"B\": [4, 5, 6]}\ndf = pd.DataFrame(data)\n\n# Skriver parquet-fil\nwith fs.open(file_path, \"wb\") as file:\n    pq.write_table(pa.Table.from_pandas(df), file)\n\nOver brukte vi wb for Ã¥ Ã¥pne den binÃ¦re filen for skriving. Hvis du Ã¸nsker Ã¥ lese fra en binÃ¦r fil sÃ¥ bruker du rb. Skulle du jobbet en ren tekstfil, sÃ¥ hadde man brukt w til Ã¥ skrive og r til Ã¥ lese.\n\n\n\nfs.touch() lar deg opprette en tom fil i bÃ¸tta, eller oppdatere metadataene til objektet for nÃ¥r den sist ble modifisert.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nfs.touch(\"gs://ssb-dapla-felles-data-delt-prod/dapla-metrics/test.parquet\")\n\n\n\n\nfs.put() lar deg laste opp en fil fra Jupyter-filsystemet3 til bÃ¸tta. Husk at filstien til ditt hjemmeomrÃ¥de pÃ¥ Jupyter er /home/jovyan/. Her er et eksempel pÃ¥ hvordan man kan bruke det pÃ¥ enkelt-filer:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"/home/jovyan/custom.scss\"\ndestination = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/test2.csv\"\n\nfs.put(source, destination)\n\nDu kan ogsÃ¥ kopiere hele mapper mellom jovyan og bÃ¸ttene:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"/home/jovyan/sesongjustering/\"\ndestination = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/sesongjustering/\"\n\nfs.put(source, destination, recursive=True)\n\nEt brukstilfellet for Ã¥ kopiere mellom bÃ¸tter og jovyan er ved sesongjustering med Jdemetra+ og JWSACruncher. Siden Jdemetra+ ikke kan lese/skrive fra bÃ¸tter, sÃ¥ mÃ¥ vi midlertidig kopiere dataene til jovyan med fs.put() fÃ¸r vi kan kjÃ¸re sesongjusteringen. NÃ¥r vi er ferdige med kjÃ¸ringen kopierer vi dataene tilbake til bÃ¸tta med fs.get().\n\n\n\nfs.get() gjÃ¸r det samme som fs.put(), bare motsatt vei. Den kopierer fra en bÃ¸tte til jovyan.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/sesongjustering/\"\ndestination = \"/home/jovyan/sesongjustering/\"\n\nplaceholder = fs.get(source, destination, recursive=True)\n\nfs.get() returnerer potensielt mye output. Hvis du ikke vil ha det, sÃ¥ kan du bare definere det som et objekt. F.eks. placeholder som i eksempelet over.\n\n\n\nfs.mv() lar deg flytte filer og mapper mellom bÃ¸tter, samt Ã¥ gi objekter nye navn.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss pÃ¥ bÃ¸ttene\nfs = FileClient.get_gcs_file_system()\n\nsource = \"gs://ssb-dapla-felles-data-delt-prod/dapla-metrics/number-of-teams.csv\"\ndestination = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/number-of-teams.csv\"\nfs.mv(source, destination)\n\n\n\n\nfs.copy() lar deg kopiere filer og mapper mellom bÃ¸tter. I eksempelet under kopierer vi rekursivt:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfrom_path = \"gs://ssb-prod-arbmark-skjema-data-kilde/ledstill/altinn/2022/11/21/\"\nto_path = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/\"\nfs.copy(from_path, to_path, recursive=True)\n\n\n\n\nfs.rm() lar deg slette filer og mapper i bÃ¸tta. Her sletter vi en enkeltfil:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfs.rm(\"gs://ssb-dapla-felles-data-delt-prod/altinn3/number-of-teams.csv\")\n\nOgsÃ¥ denne funksjonen tar et recursive-argument hvis du Ã¸nsker Ã¥ slette en hel mappe.",
    "crumbs": [
      "Statistikere",
      "Data",
      "Hva er bÃ¸tter?"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-botter.html#bÃ¸tter-vs-filsystemer",
    "href": "statistikkere/hva-er-botter.html#bÃ¸tter-vs-filsystemer",
    "title": "Hva er bÃ¸tter?",
    "section": "",
    "text": "I et Linux- eller Windows-filsystem, som vi har vÃ¦rt vant til tidligere, sÃ¥ er filer og mapper organisert i en hierarkisk struktur pÃ¥ et operativsystem (OS). I SSB har OS-ene vÃ¦rt installert pÃ¥ fysiske maskiner som vi vedlikeholder selv.\nEn bÃ¸tte i GCS er derimot en kjÃ¸pt tjeneste som lar brukeren lagre alle typer objekter i en container. Man trenger altsÃ¥ ikke Ã¥ tenke pÃ¥ om filene ligger i et hierarki, hvilket operativsystem det kjÃ¸rer pÃ¥, eller hvor mye diskplass som er tilgjengelig.\nAt data er lagret som objekter i en bÃ¸tte har noen praktiske implikasjoner for hvordan vi jobber med data i SSB:\n\nMange er vant til Ã¥ jobbe direkte med filer i en Linux-terminal eller via systemkall fra sprÃ¥k som SAS, Pyton eller R. For Ã¥ gjÃ¸re det samme i Jupyter mot en bÃ¸tte, sÃ¥ kan vi bruke Python-pakken gcsfs. Se eksempler under.\nNÃ¥r vi bruker Python- eller R-pakker for lese eller skrive data fra bÃ¸tter, sÃ¥ er vi avhengig av at pakkene tilbyr integrasjon mot bÃ¸tter. Mange pakker gjÃ¸r det, men ikke alle. For de som ikke gjÃ¸r det kan vi bruke ofte bruke gcsfs til Ã¥ gi oss et python-objekt, som igjen kan brukes av de fleste pakker.\nDet finnes egentlig ikke noen mapper i bÃ¸tter. I motsetning til et vanlig filsystem sÃ¥ er det ikke en hierarkisk mappestruktur i en bÃ¸tte. Det vil si at alt ligger et sted, men de kan ha lange navn med /-tegn slik at det ligner pÃ¥ et klassisk filsystem. Bruker du / i objekt-navnet sÃ¥ vil ogsÃ¥ Google Cloud Console vise det som mapper, men det er bare for Ã¥ gjÃ¸re det enklere Ã¥ forholde seg til. En praktisk konsekvens av dette er f.eks. at man ikke trenger Ã¥ opprette en mappe fÃ¸r man legger et fil/objekt i den. Det er bare en tekststreng som er en del av objekt-navnet.\n\nUnder kommer det en del eksempler pÃ¥ hvordan man kan jobbe med objekter i bÃ¸tter pÃ¥ samme mÃ¥te som filer i et filsystem.",
    "crumbs": [
      "Statistikere",
      "Data",
      "Hva er bÃ¸tter?"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-botter.html#lokalt-filsystem-pÃ¥-dapla",
    "href": "statistikkere/hva-er-botter.html#lokalt-filsystem-pÃ¥-dapla",
    "title": "Hva er bÃ¸tter?",
    "section": "",
    "text": "PÃ¥ Dapla skal data lagres i bÃ¸tter. Men nÃ¥r du Ã¥pner Jupyterlab sÃ¥ fÃ¥r du ogsÃ¥ et â€œlokaltâ€ eller klassisk filsystem, slik vi definerte det i forrige kapittel. Det er dette filsystemet du ser i filutforskeren i Jupyterlab, slik som vist til venstre i FigurÂ 1. Det er ogsÃ¥ dette filsystemet du ser nÃ¥r du f.eks. bruker ls-kommandoen i en terminal i Jupyterlab.\n\n\n\n\n\n\nFigurÂ 1: Til venstre i bildet ser du filutforskeren i Jupyterlab\n\n\n\nDette filsystemet er ment for Ã¥ lagre kode midlertidig mens du jobber med dem. Det er ikke ment for Ã¥ lagre data. Det er heller ikke ment som et permanent lagringssted for kode. Permanent lagring av kode skal gjÃ¸res pÃ¥ GitHub. Selv om filene du lagrer der fortsetter Ã¥ eksistere for hver gang du logger deg inn i Jupyterlab, sÃ¥ bÃ¸r kode du Ã¸nsker Ã¥ bevare pushes til GitHub fÃ¸r du avslutter en sesjon i Jupyterlab. Se en mer teknisk beskrivelse av hvordan dette fungerer i boksen under.\n\n\n\n\n\n\nHvem er egentlig denne Jovyan?\n\n\n\nNÃ¥r du logger deg inn i Jupyterlab pÃ¥ Dapla, sÃ¥ ser du at brukeren din pÃ¥ det lokale filsystemet heter jovyan. Grunnen er at det er en Linux-container som kjÃ¸rer et Jupyterlab Docker-image, og alle bruker denne containeren. Det er derfor vi kaller det et â€œlokaltâ€ filsystem, fordi det er lokalt i containeren.\nI tillegg er det satt opp persistent volum (PV) og en persistent volume claim (PVC) som er knyttet til denne containeren. Det er dette som gjÃ¸r at filene du lagrer i Jupyterlab blir lagret mellom hver gang du logger deg inn. I tillegg tas det en backup av filsystemene hver dag, slik at vi kan gjenopprette filene dine hvis noe skulle skje med PV-en.\nLagringsplassen pÃ¥ PV-en er begrenset til 10 GB per bruker. Grunnen til at vi ikke tilbyr mer lagringsplass er fordi det kun er kode skal lagres her, og at denne lagringsplassen er relativt dyrt sammenlignet med GCS-bÃ¸tter. Hvis du jobber i virtuelle miljÃ¸er og lagrer mange miljÃ¸er lokalt, kan du oppleve at disken blir full. Les mer her om hvordan man bÃ¸r hÃ¥ndere dette.",
    "crumbs": [
      "Statistikere",
      "Data",
      "Hva er bÃ¸tter?"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-botter.html#systemkommandoer-mot-bÃ¸ttter",
    "href": "statistikkere/hva-er-botter.html#systemkommandoer-mot-bÃ¸ttter",
    "title": "Hva er bÃ¸tter?",
    "section": "",
    "text": "Tidligere har vi diskutert forskjellene mellom bÃ¸tter og filsystemer. Mange kjenner hvordan man gjÃ¸r systemkommandoer2 i klassiske filsystemer fra en terminal eller fra sprÃ¥k som Python, R og SAS. I dette kapitlet viser vi hvordan dette kan gjÃ¸res mot bÃ¸tter fra Jupyterlab.\ngcsfs er en Python-pakke som lar deg jobbe med objekter i bÃ¸tter pÃ¥ nesten samme mÃ¥te som filer i et filsystem. For Ã¥ kunne gjÃ¸re det mÃ¥ vi fÃ¸rst sette opp en filsystem-instans som lar oss bruke en bÃ¸tte som et filsystem. Pakken dapla-toolbelt lar oss gjÃ¸re det ganske enkelt:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs er nÃ¥ et filsystem-versjon av bÃ¸ttene vi har tilgang til pÃ¥ GCS. Vi kan nÃ¥ bruk fs til Ã¥ gjÃ¸re typiske operasjoner vi har vÃ¦rt vant til Ã¥ gjÃ¸re i en Linux-terminal. Se en fullstendig liste over kommandoer her.\nUnder vises noen flere eksempler pÃ¥ nyttige funksjoner enn det som ble vist i kapitlet tidligere i boken\n\n\n\n\n\n\nWarning\n\n\n\nSelv om ordet mapper blir brukt i eksemplene under, sÃ¥ er det viktig Ã¥ huske at det ikke finnes noen mapper i bÃ¸tter. Det er bare en tekststreng som er en del av objekt-navnet. Men siden vi her jobber med et filsystem-representasjon av bÃ¸tten, sÃ¥ tillater vi oss Ã¥ gjÃ¸re det for Ã¥ gjÃ¸re det enklere Ã¥ lese.\n\n\n\n\nfs.glob() lar oss sÃ¸ke etter filer i bÃ¸tten. Vi kan bruke *, **, ? og [..] som wildcard for Ã¥ finne det vi trenger. Det som returneres er enten en liste eller dictionary avhengig av hva vi velger Ã¥ gjÃ¸re.\nHent en liste over alle filer i en undermappe R_smoke_test i bÃ¸tta gs://ssb-dapla-felles-data-delt-prod:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/*\")\n\nNÃ¥r vi legger til * pÃ¥ slutten av filstien sÃ¥ returnerer den alle filer i den eksakte undermappen. Men hvis vi Ã¸nsker Ã¥ Ã¥ fÃ¥ alle filer i alle undermapper, sÃ¥ kan vi bruke ** pÃ¥ denne mÃ¥ten:\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/**\").\nVi kan ogsÃ¥ sÃ¸ke mer avansert ved ved Ã¥ bruke ?. ?-tegnet sier at en enkeltkarakter kan vÃ¦re hva som helst. F.eks. kunne vi skrevet\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/**/R_smoke_test_??.csv\")\nfor Ã¥ rekursivt liste ut alle filer som starter med R_smoke_test_, etterfulgt av to karakterer av hvilken som helst type, og slutter med .csv.\nHvis vi viste at de to karakterene ikke skulle vÃ¦re av hvilken som helst karakterer, men det var en liten bokstav, etterfulgt av et tall mellom 2-6, sÃ¥ kunne vi brukt [a-z] og [2-6] for Ã¥ spesifisere dette. F.eks. kunne vi skrevet:\nfs.glob(\"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/**/R_smoke_test_[a-z][2-6].csv\")\nSom vi ser er fs.glob() et verktÃ¸y som git oss mye den funksjonaliteten vi er vant til fra Linux-terminalen. Men i tillegg kan vi bruke den til Ã¥ hente inn metadataene til de filene/objektene vi fÃ¥r treff pÃ¥, ved Ã¥ bruke argumentet detail=True. Her er et eksempel:\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\n    \"gs://ssb-dapla-felles-data-delt-prod/R_smoke_test/**/R_smoke_test_/[a-z][2-6].csv\",\n    detail=True,\n)\n\nMetadataene gir deg da informasjon om filstÃ¸rrelse, tidspunkt for opprettelse, tidspunkt for siste endring, og hvem som har opprettet og endret filen.\n\n\n\nI noen tilfeller kan det vÃ¦re nyttig Ã¥ sjekke om en fil eksisterer i bÃ¸tten. Det kan vi gjÃ¸re med fs.exists():\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.exists(\"gs://ssb-dapla-felles-data-delt-prod/altinn3/form_dc551844cd74.xml\")\n\nKoden returnerer enten True eller False avhengig av om filen eksisterer eller ikke.\n\n\n\nHvis du lurer pÃ¥ hvor mange GB data du har i en bÃ¸tte, sÃ¥ kan du bruke fs.du():\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss pÃ¥ bÃ¸ttene\nfs = FileClient.get_gcs_file_system()\n\ntotal_bytes = fs.du(\n    \"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/\",\n)\n\ntotal_size_gb = total_bytes / (1024**3)\nprint(f\"Total size: {total_size_gb:.3f} GB\")\n\n\n\n\nHvert objekt i bÃ¸tta har metadata knyttet til seg som kan hentes inn i Jupyter og benyttes.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss pÃ¥ bÃ¸ttene\nfs = FileClient.get_gcs_file_system()\n\nfile = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/form_dc551844cd74.xml\"\n\nfs.info(file)\n\nDette gir deg blant annet informasjon om filstÃ¸rrelse, tidspunkt for opprettelse, og tidspunkt for siste endring. Hvis du Ã¸nsker dette for flere filer sÃ¥ kan man ogsÃ¥ bruke fs.glob(&lt;pattern&gt;, details=True) som vi sÃ¥ pÃ¥ tidligere.\n\n\n\nfs.ls() brukes for Ã¥ gi en liste av filer i et omrÃ¥de. Det kan brukes for bÃ¥de bÃ¸tter og mapper.\n\n\nnotebook\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.ls(\"gs://ssb-dapla-felles-data-delt-prod/altinn3\")\n\nKoden returnerer en liste.\n\n\n\nfs.open() lar oss Ã¥pne en fil i bÃ¸tta for lesing og skriving med vanlige Python-funksjoner. Funksjonen returnerer et fil-aktig objekt som kan brukes med vanlige som Python-biblioteker som Pandas og NumPy.\n\n\nnotebook\n\nimport pandas as pd\nfrom dapla import FileClient\n\n# Kobler oss pÃ¥ bÃ¸ttene\nfs = FileClient.get_gcs_file_system()\n\nfile_path = \"gs://ssb-dapla-felles-data-delt-prod/dapla-metrics/number-of-teams.csv\"\n\n# Bruker fs.open() til Ã¥ Ã¥pne fil, og deretter leses den med Pandas\nwith fs.open(file_path, \"r\") as file:\n    df = pd.read_csv(file)\ndf\n\nDu kan ogsÃ¥ bruke fs.open() til Ã¥ skrive til en fil i bÃ¸tta. Her er et eksempel pÃ¥ hvordan man skriver en parquet-fil med Pandas og PyArrow:\n\n\nnotebook\n\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom dapla import FileClient\n\nfile_path = \"gs://ssb-dapla-felles-data-delt-prod/dapla-metrics/test.parquet\"\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\n# Lager eksempeldata\ndata = {\"A\": [1, 2, 3], \"B\": [4, 5, 6]}\ndf = pd.DataFrame(data)\n\n# Skriver parquet-fil\nwith fs.open(file_path, \"wb\") as file:\n    pq.write_table(pa.Table.from_pandas(df), file)\n\nOver brukte vi wb for Ã¥ Ã¥pne den binÃ¦re filen for skriving. Hvis du Ã¸nsker Ã¥ lese fra en binÃ¦r fil sÃ¥ bruker du rb. Skulle du jobbet en ren tekstfil, sÃ¥ hadde man brukt w til Ã¥ skrive og r til Ã¥ lese.\n\n\n\nfs.touch() lar deg opprette en tom fil i bÃ¸tta, eller oppdatere metadataene til objektet for nÃ¥r den sist ble modifisert.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nfs.touch(\"gs://ssb-dapla-felles-data-delt-prod/dapla-metrics/test.parquet\")\n\n\n\n\nfs.put() lar deg laste opp en fil fra Jupyter-filsystemet3 til bÃ¸tta. Husk at filstien til ditt hjemmeomrÃ¥de pÃ¥ Jupyter er /home/jovyan/. Her er et eksempel pÃ¥ hvordan man kan bruke det pÃ¥ enkelt-filer:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"/home/jovyan/custom.scss\"\ndestination = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/test2.csv\"\n\nfs.put(source, destination)\n\nDu kan ogsÃ¥ kopiere hele mapper mellom jovyan og bÃ¸ttene:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"/home/jovyan/sesongjustering/\"\ndestination = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/sesongjustering/\"\n\nfs.put(source, destination, recursive=True)\n\nEt brukstilfellet for Ã¥ kopiere mellom bÃ¸tter og jovyan er ved sesongjustering med Jdemetra+ og JWSACruncher. Siden Jdemetra+ ikke kan lese/skrive fra bÃ¸tter, sÃ¥ mÃ¥ vi midlertidig kopiere dataene til jovyan med fs.put() fÃ¸r vi kan kjÃ¸re sesongjusteringen. NÃ¥r vi er ferdige med kjÃ¸ringen kopierer vi dataene tilbake til bÃ¸tta med fs.get().\n\n\n\nfs.get() gjÃ¸r det samme som fs.put(), bare motsatt vei. Den kopierer fra en bÃ¸tte til jovyan.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/sesongjustering/\"\ndestination = \"/home/jovyan/sesongjustering/\"\n\nplaceholder = fs.get(source, destination, recursive=True)\n\nfs.get() returnerer potensielt mye output. Hvis du ikke vil ha det, sÃ¥ kan du bare definere det som et objekt. F.eks. placeholder som i eksempelet over.\n\n\n\nfs.mv() lar deg flytte filer og mapper mellom bÃ¸tter, samt Ã¥ gi objekter nye navn.\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Kobler oss pÃ¥ bÃ¸ttene\nfs = FileClient.get_gcs_file_system()\n\nsource = \"gs://ssb-dapla-felles-data-delt-prod/dapla-metrics/number-of-teams.csv\"\ndestination = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/number-of-teams.csv\"\nfs.mv(source, destination)\n\n\n\n\nfs.copy() lar deg kopiere filer og mapper mellom bÃ¸tter. I eksempelet under kopierer vi rekursivt:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfrom_path = \"gs://ssb-prod-arbmark-skjema-data-kilde/ledstill/altinn/2022/11/21/\"\nto_path = \"gs://ssb-dapla-felles-data-delt-prod/altinn3/\"\nfs.copy(from_path, to_path, recursive=True)\n\n\n\n\nfs.rm() lar deg slette filer og mapper i bÃ¸tta. Her sletter vi en enkeltfil:\n\n\nnotebook\n\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfs.rm(\"gs://ssb-dapla-felles-data-delt-prod/altinn3/number-of-teams.csv\")\n\nOgsÃ¥ denne funksjonen tar et recursive-argument hvis du Ã¸nsker Ã¥ slette en hel mappe.",
    "crumbs": [
      "Statistikere",
      "Data",
      "Hva er bÃ¸tter?"
    ]
  },
  {
    "objectID": "statistikkere/hva-er-botter.html#footnotes",
    "href": "statistikkere/hva-er-botter.html#footnotes",
    "title": "Hva er bÃ¸tter?",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nEgentlig har vi jobbet med data-filer pÃ¥ bÃ¥de Linux- og Windows-filsystemer. Men Linux-stammene har vÃ¦rt det anbefalte stedet Ã¥ lagre datafiler.â†©ï¸\nMed systemkommandoer sÃ¥ mener vi bash-kommandoer som ls og mv, eller implementasjoner av disse kommandoene i Python, R eller SAS.â†©ï¸\nJupyter-miljÃ¸et har sitt eget filsystem, ofte kalt jovyan. Det er som et vanlig Linux-filsystem, og vil vÃ¦re det vi omtaler som â€œlokaltâ€ pÃ¥ maskinen din i Jupyter.â†©ï¸",
    "crumbs": [
      "Statistikere",
      "Data",
      "Hva er bÃ¸tter?"
    ]
  },
  {
    "objectID": "statistikkere/overforing-av-data.html",
    "href": "statistikkere/overforing-av-data.html",
    "title": "OverfÃ¸ring av data",
    "section": "",
    "text": "For Ã¥ overfÃ¸re data mellom bakke og sky brukes Data Transfer, som er en tjeneste i Google Cloud Console. Denne tjenesten kan brukes til Ã¥ flytte data bÃ¥de til og fra Linuxstammen og Dapla, og er tilgjengelig for teamets kildedataansvarlige.\nFor Ã¥ fÃ¥ tilgang til Ã¥ overfÃ¸re filer mÃ¥ man be om dette ved opprettelsen av teamet. Ber man om det skjer fÃ¸lgende:\n\nEn mappe blir opprettet pÃ¥ Linux i prodsonen under /ssb/cloud_sync/\nEt Google Project blir opprettet med navn &lt;team navn&gt;-ts.\n\nDette Google-prosjektet er ikke det samme som der du lagrer annen data. Det har navnet &lt;team navn&gt;-ts, og filstiene pÃ¥ bakken og sky vises i FigurÂ 1.\n\n\n\n\n\n\nFigurÂ 1: Hvordan Transfer Service kan flytte filer mellom bakke og sky.\n\n\n\nTeamets kildedataansvarlige vil vÃ¦re spesifisert som en del av Ã¥ opprette et Dapla-team.\n\n\nEnten man skal overfÃ¸re filer opp til sky eller ned til bakken sÃ¥ bruker man den samme Data Transfer tjenesten. For Ã¥ fÃ¥ tilgang til denne mÃ¥ man fÃ¸rst logge seg inn i Google Cloud Console. Sjekk at du er logget inn med din SSB-konto (xxx@ssb.no).\nÃ˜verst pÃ¥ siden, til hÃ¸yre for teksten Google Cloud finnes det en prosjektvelger, og her er det viktig Ã¥ velge korrekt Google prosjekt. Hvis du trykker pÃ¥ prosjektvelgeren vil det Ã¥pnes opp et nytt vindu. Sjekk at det stÃ¥r SSB.NO Ã¸verst i dette vinduet. Trykk deretter pÃ¥ fanen ALL for Ã¥ fÃ¥ opp alle tilgjengelige Google-prosjekter under organisasjonen ssb.no (FigurÂ 2)\n\n\n\n\n\n\nFigurÂ 2: Prosjektvelgeren i Google Cloud Console\n\n\n\nUnder ssb.no vil det ligge flere mapper. Ã…pne mappen som heter production og let frem en undermappe som har navnet pÃ¥ ditt Dapla-team. Strukturen skal se slik ut:\n    ssb.no\n    â”œâ”€â”€ production\n        â””â”€â”€ &lt;teamnavn&gt;\n            â”œâ”€â”€ prod-&lt;teamnavn&gt;\n            â””â”€â”€ &lt;teamnavn&gt;-ts\nDet underste nivÃ¥et (prod-&lt;teamnavn&gt; og &lt;teamnavn&gt;-ts) viser prosjektene, nivÃ¥et i mellom er mapper, og toppnivÃ¥et er organisasjonen (ssb.no). Prosjektet &lt;teamnavn&gt;-ts er et separat prosjekt som bare teamets kildedataansvarlige har tilgang til, og det er her tjenesten Data Transfer skal settes opp.\n\nVelg derfor prosjektet &lt;teamnavn&gt;-ts.\nI sÃ¸kefeltet til Google Cloud Console, skriv Data transfer og trykk pÃ¥ det valget som kommer opp.\nFÃ¸rste gang man kommer inn pÃ¥ siden til Transfer Services vil man bli vist en blÃ¥ knapp med teksten Set Up Connection. NÃ¥r du trykker pÃ¥ denne vil det dukke opp et nytt felt hvor du fÃ¥r valget Create Pub-Sub Resources. Dette er noe som bare trengs Ã¥ gjÃ¸re Ã©n gang. Trykk pÃ¥ den blÃ¥ CREATE knappen, og deretter trykk pÃ¥ Close lenger nede.\nI navigasjonsmenyen til venstre trykk Transfer jobs, og deretter trykk pÃ¥ + Create transfer job Ã¸verst pÃ¥ siden for Ã¥ opprette en ny overfÃ¸ringsjobb.\n\n\n\nFÃ¸lgende oppskrift tar utgangspunkt i siden Create a transfer job (FigurÂ 3):\n\n\n\n\n\n\nFigurÂ 3: Opprett overfÃ¸ringsjobb i Google Cloud Console\n\n\n\n\nVelg POSIX filesystem under â€œSource typeâ€ og Google cloud storage under â€œDestination typeâ€ (eller motsatt hvis overfÃ¸ringsjobben skal gÃ¥ fra Dapla til Linuxstammen). Trykk Next step\nNedtrekkslisten â€œAgent poolâ€ skal normalt bare ha ett valg: transfer_service_default. Velg denne.\nI feltet â€œSource directory pathâ€ skal man kun skrive data/tilsky siden overfÃ¸ringsagenten kun har tilgang til mapper som ligger relativt plassert under /ssb/cloud_sync/&lt;teamnavn&gt;/. Trykk Next step\nVelg en destinasjon for overfÃ¸ringsjobben. Trykk pÃ¥ Browse og velg bÃ¸tten med navn som passer til ssb-prod-&lt;teamnavn&gt;-data-synk-opp. Vi anbefaler at du ogsÃ¥ oppretter en mappe inne i denne bÃ¸tten. Det gjÃ¸res ved Ã¥ trykke pÃ¥ mappeikonet med et +-tegn foran. Skriv inn et passende mappenavn og trykk Select i bunnen av siden. Trykk deretter Next step\nNeste steg â€œChoose how and when to run this jobâ€ er opp til brukeren Ã¥ bestemme. Hvis man f.eks. velger at Data Transfer skal overfÃ¸re data en gang i uken, vil den kun starte en overfÃ¸ring hvis det finnes nye data. Trykk Next step\nBeskriv overfÃ¸ringsjobben, f.eks: â€œFlytter data for  til sky.â€. Resten av feltene er opp til brukeren Ã¥ bestemme. Standardverdiene er OK.\n\nTrykk til slutt pÃ¥ den blÃ¥ Create-knappen. Du vil kunne se kjÃ¸rende jobber under menyen Transfer jobs.\nFor Ã¥ sjekke om data har blitt overfÃ¸rt, skriv inn cloud storage i sÃ¸kefeltet Ã¸verst pÃ¥ siden og trykk pÃ¥ det fÃ¸rste valget som kommer opp. Her vil du finne en oversikt over alle teamets bÃ¸tter, deriblant en med navn ssb-prod-&lt;team-name&gt;-data-synk-opp. NÃ¥r overfÃ¸ringsjobben er ferdig vil du kunne finne igjen dataene i den mappen som ble definert i stegene overnfor.\n\n\n\nOverfÃ¸ringsjobben settes opp nesten identisk med OverfÃ¸ring fra Linuxstammen til Dapla med unntak av fÃ¸lgende:\n\nSteg 1: Velg Google cloud storage under â€œSource typeâ€ og POSIX filesystem under â€œDestination typeâ€\nSteg 2: Velg bÃ¸tten ssb-prod-&lt;team-name&gt;-data-synk-ned\nStep 3: Velg transfer_service_default som â€œAgent poolâ€ og skriv data/frasky inn i feltet for â€œDestination directory pathâ€.\n\nFor Ã¥ se om data har blitt overfÃ¸rt til Linuxstammen mÃ¥ du nÃ¥ gÃ¥ til mappen /ssb/cloud_sync/&lt;team-name&gt;/data/frasky fra FileZilla.\nHusk: Du kan alltids gÃ¥ tilbake og se pÃ¥ tidligere fullfÃ¸rte jobber, og starte en overfÃ¸ringsjobb manuelt fra menyen Transfer jobs.\n\n\n\n\nNÃ¥r du har satt opp en, enten for Ã¥ overfÃ¸re fra sky eller til sky, kan du skrive ut data til mappen eller bÃ¸tten som du har bedt Transfer Service om Ã¥ overfÃ¸re data fra.\nHvis du skal overfÃ¸re data fra bakken/prodsonen til sky, sÃ¥ mÃ¥ teamets kildedataansvarlige skrive ut data til Linux-mappen /ssb/cloud_sync/&lt;team navn&gt;/data/tilsky, og det vil ende opp i Dapla-bÃ¸tta gs://ssb-prod-&lt;team navn&gt;-data-synk-opp Dette kan du gjÃ¸re med alle programmeringsverktÃ¸y som har en kobling til Linux-stammene der dataene ligger. For eksempel:\n\nSAS EG\nSAS-installasjon pÃ¥ Linux\nJupyterlab i prodsonen\nRstudio pÃ¥ sl-stata-03\n\nSkal du flytte data fra Dapla til bakken/prodsonen, sÃ¥ mÃ¥ teamets kildedataansvarlige skrive ut data til gs://ssb-prod-&lt;team navn&gt;-data-synk-opp-bÃ¸tta pÃ¥ Dapla. Det er noe man typisk gjÃ¸r fra Jupyterlab pÃ¥ Dapla."
  },
  {
    "objectID": "statistikkere/overforing-av-data.html#sette-opp-overfÃ¸ringsjobber",
    "href": "statistikkere/overforing-av-data.html#sette-opp-overfÃ¸ringsjobber",
    "title": "OverfÃ¸ring av data",
    "section": "",
    "text": "Enten man skal overfÃ¸re filer opp til sky eller ned til bakken sÃ¥ bruker man den samme Data Transfer tjenesten. For Ã¥ fÃ¥ tilgang til denne mÃ¥ man fÃ¸rst logge seg inn i Google Cloud Console. Sjekk at du er logget inn med din SSB-konto (xxx@ssb.no).\nÃ˜verst pÃ¥ siden, til hÃ¸yre for teksten Google Cloud finnes det en prosjektvelger, og her er det viktig Ã¥ velge korrekt Google prosjekt. Hvis du trykker pÃ¥ prosjektvelgeren vil det Ã¥pnes opp et nytt vindu. Sjekk at det stÃ¥r SSB.NO Ã¸verst i dette vinduet. Trykk deretter pÃ¥ fanen ALL for Ã¥ fÃ¥ opp alle tilgjengelige Google-prosjekter under organisasjonen ssb.no (FigurÂ 2)\n\n\n\n\n\n\nFigurÂ 2: Prosjektvelgeren i Google Cloud Console\n\n\n\nUnder ssb.no vil det ligge flere mapper. Ã…pne mappen som heter production og let frem en undermappe som har navnet pÃ¥ ditt Dapla-team. Strukturen skal se slik ut:\n    ssb.no\n    â”œâ”€â”€ production\n        â””â”€â”€ &lt;teamnavn&gt;\n            â”œâ”€â”€ prod-&lt;teamnavn&gt;\n            â””â”€â”€ &lt;teamnavn&gt;-ts\nDet underste nivÃ¥et (prod-&lt;teamnavn&gt; og &lt;teamnavn&gt;-ts) viser prosjektene, nivÃ¥et i mellom er mapper, og toppnivÃ¥et er organisasjonen (ssb.no). Prosjektet &lt;teamnavn&gt;-ts er et separat prosjekt som bare teamets kildedataansvarlige har tilgang til, og det er her tjenesten Data Transfer skal settes opp.\n\nVelg derfor prosjektet &lt;teamnavn&gt;-ts.\nI sÃ¸kefeltet til Google Cloud Console, skriv Data transfer og trykk pÃ¥ det valget som kommer opp.\nFÃ¸rste gang man kommer inn pÃ¥ siden til Transfer Services vil man bli vist en blÃ¥ knapp med teksten Set Up Connection. NÃ¥r du trykker pÃ¥ denne vil det dukke opp et nytt felt hvor du fÃ¥r valget Create Pub-Sub Resources. Dette er noe som bare trengs Ã¥ gjÃ¸re Ã©n gang. Trykk pÃ¥ den blÃ¥ CREATE knappen, og deretter trykk pÃ¥ Close lenger nede.\nI navigasjonsmenyen til venstre trykk Transfer jobs, og deretter trykk pÃ¥ + Create transfer job Ã¸verst pÃ¥ siden for Ã¥ opprette en ny overfÃ¸ringsjobb.\n\n\n\nFÃ¸lgende oppskrift tar utgangspunkt i siden Create a transfer job (FigurÂ 3):\n\n\n\n\n\n\nFigurÂ 3: Opprett overfÃ¸ringsjobb i Google Cloud Console\n\n\n\n\nVelg POSIX filesystem under â€œSource typeâ€ og Google cloud storage under â€œDestination typeâ€ (eller motsatt hvis overfÃ¸ringsjobben skal gÃ¥ fra Dapla til Linuxstammen). Trykk Next step\nNedtrekkslisten â€œAgent poolâ€ skal normalt bare ha ett valg: transfer_service_default. Velg denne.\nI feltet â€œSource directory pathâ€ skal man kun skrive data/tilsky siden overfÃ¸ringsagenten kun har tilgang til mapper som ligger relativt plassert under /ssb/cloud_sync/&lt;teamnavn&gt;/. Trykk Next step\nVelg en destinasjon for overfÃ¸ringsjobben. Trykk pÃ¥ Browse og velg bÃ¸tten med navn som passer til ssb-prod-&lt;teamnavn&gt;-data-synk-opp. Vi anbefaler at du ogsÃ¥ oppretter en mappe inne i denne bÃ¸tten. Det gjÃ¸res ved Ã¥ trykke pÃ¥ mappeikonet med et +-tegn foran. Skriv inn et passende mappenavn og trykk Select i bunnen av siden. Trykk deretter Next step\nNeste steg â€œChoose how and when to run this jobâ€ er opp til brukeren Ã¥ bestemme. Hvis man f.eks. velger at Data Transfer skal overfÃ¸re data en gang i uken, vil den kun starte en overfÃ¸ring hvis det finnes nye data. Trykk Next step\nBeskriv overfÃ¸ringsjobben, f.eks: â€œFlytter data for  til sky.â€. Resten av feltene er opp til brukeren Ã¥ bestemme. Standardverdiene er OK.\n\nTrykk til slutt pÃ¥ den blÃ¥ Create-knappen. Du vil kunne se kjÃ¸rende jobber under menyen Transfer jobs.\nFor Ã¥ sjekke om data har blitt overfÃ¸rt, skriv inn cloud storage i sÃ¸kefeltet Ã¸verst pÃ¥ siden og trykk pÃ¥ det fÃ¸rste valget som kommer opp. Her vil du finne en oversikt over alle teamets bÃ¸tter, deriblant en med navn ssb-prod-&lt;team-name&gt;-data-synk-opp. NÃ¥r overfÃ¸ringsjobben er ferdig vil du kunne finne igjen dataene i den mappen som ble definert i stegene overnfor.\n\n\n\nOverfÃ¸ringsjobben settes opp nesten identisk med OverfÃ¸ring fra Linuxstammen til Dapla med unntak av fÃ¸lgende:\n\nSteg 1: Velg Google cloud storage under â€œSource typeâ€ og POSIX filesystem under â€œDestination typeâ€\nSteg 2: Velg bÃ¸tten ssb-prod-&lt;team-name&gt;-data-synk-ned\nStep 3: Velg transfer_service_default som â€œAgent poolâ€ og skriv data/frasky inn i feltet for â€œDestination directory pathâ€.\n\nFor Ã¥ se om data har blitt overfÃ¸rt til Linuxstammen mÃ¥ du nÃ¥ gÃ¥ til mappen /ssb/cloud_sync/&lt;team-name&gt;/data/frasky fra FileZilla.\nHusk: Du kan alltids gÃ¥ tilbake og se pÃ¥ tidligere fullfÃ¸rte jobber, og starte en overfÃ¸ringsjobb manuelt fra menyen Transfer jobs."
  },
  {
    "objectID": "statistikkere/overforing-av-data.html#skrive-ut-data",
    "href": "statistikkere/overforing-av-data.html#skrive-ut-data",
    "title": "OverfÃ¸ring av data",
    "section": "",
    "text": "NÃ¥r du har satt opp en, enten for Ã¥ overfÃ¸re fra sky eller til sky, kan du skrive ut data til mappen eller bÃ¸tten som du har bedt Transfer Service om Ã¥ overfÃ¸re data fra.\nHvis du skal overfÃ¸re data fra bakken/prodsonen til sky, sÃ¥ mÃ¥ teamets kildedataansvarlige skrive ut data til Linux-mappen /ssb/cloud_sync/&lt;team navn&gt;/data/tilsky, og det vil ende opp i Dapla-bÃ¸tta gs://ssb-prod-&lt;team navn&gt;-data-synk-opp Dette kan du gjÃ¸re med alle programmeringsverktÃ¸y som har en kobling til Linux-stammene der dataene ligger. For eksempel:\n\nSAS EG\nSAS-installasjon pÃ¥ Linux\nJupyterlab i prodsonen\nRstudio pÃ¥ sl-stata-03\n\nSkal du flytte data fra Dapla til bakken/prodsonen, sÃ¥ mÃ¥ teamets kildedataansvarlige skrive ut data til gs://ssb-prod-&lt;team navn&gt;-data-synk-opp-bÃ¸tta pÃ¥ Dapla. Det er noe man typisk gjÃ¸r fra Jupyterlab pÃ¥ Dapla."
  },
  {
    "objectID": "statistikkere/kartdata.html",
    "href": "statistikkere/kartdata.html",
    "title": "Kartdata",
    "section": "",
    "text": "Det er tilrettelagt kartdata i Dapla til analyse, statistikkproduksjon og visualisering. Det er de samme dataene som er i GEODB i vanlig produksjonssone. Kartdataene er lagret pÃ¥ ssb-prod-kart-data-delt. De er lagret som parquetfiler i standardprojeksjonen vi bruker i SSB (UTM sone 33N) hvis ikke annet er angitt. Det er ogsÃ¥ SSBs standard-rutenett i ulike stÃ¸rrelser samt Eurostats rutenett over Norge.\nMan sÃ¸ker om tilgang til dataene til kundeservice (tilgangsrollen kart-consumers), men bruk gjerne standard LDA-prosedyre som for Ã¸vrige data.\nI tillegg ligger det noe testdata i fellesbÃ¸tta her: ssb-dapla-felles-data-delt-prod/GIS/testdata\n\n\n\n\nGeopandas er et Python-bibliotek som integrerer Pandas med GIS-analysering og kartlegging. Geopandas er bygd opp rundt GeoDataFrames, som er en pandas DataFrame med geografisk informasjon. Disse kan lastes inn direkte hvis man har geografiske filer, eller man kan konvertere fra DataFrames med koordinat-kolonner (se eksempel under).\nGeopandas kan blant annet brukes til Ã¥ kartlegge dataene, beregne avstander og labe variabler for nÃ¦rmiljÃ¸ ved Ã¥ koble datasett sammen basert pÃ¥ geografisk overlapp.\nFor lesing og skriving av geodata i Dapla, kan man bruke SSB-pakken sgis. Med denne kan man blant annet ogsÃ¥ beregne reisetider langs veg og lage publiseringsverdige kart.\nPakkene kan installeres i Dapla sÃ¥nn her:\n\n\nterminal\n\npoetry add geopandas\npoetry add ssb-sgis\n\nOg sÃ¥ importeres det i Python pÃ¥ vanlig vis.\n\n\nnotebook\n\nimport geopandas as gpd\nimport sgis as sg\n\nEksempel pÃ¥ lesing/skriving av kartdata\nMan kan lese/skrive geodata i Dapla med sgis-funksjonene read_geopandas og write_geopandas. StÃ¸ttede formater er blant annet (geo)parquet, geojson og gml. En rekke kartdata for hele landet til analyse- og visualiseringsformÃ¥l ligger i bÃ¸tta â€œkartâ€. For eksempel kommuneflater til analyse eller statistikk:\n\n\nnotebook\n\nfilsti = \"ssb-prod-kart-data-delt/kartdata_analyse/klargjorte-data/2023/ABAS_kommune_flate_p2023_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\n\nEksempel pÃ¥ lesing av kommuneflater for visualisering. Disse dataene er generalisert slik at de egner seg for Ã¥ lage kart, men blir unÃ¸yaktige til statistikk:\n\n\nnotebook\n\nfilsti = \"ssb-prod-kart-data-delt/kartdata_visualisering/klargjorte-data/2023/parquet/N5000_kommune_flate_p2023_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\n\nTilsvarende for skriving til parquet eller annet geodataformat:\n\n\nnotebook\n\nsg.write_geopandas(kommuner, filsti_ny) \n\n\n\n\nI tillegg til kartdataene, ligger det testdata i fellesbÃ¸tta som man kan leke seg med. Testdataene finnes for Oslo og Kongsvinger, og kan leses sÃ¥nn her:\n\n\nnotebook\n\ntestdatasti = \"ssb-dapla-felles-data-delt-prod/GIS/testdata\"\nveger = sg.read_geopandas(f\"{testdatasti}/veger_oslo.parquet\")\n\nUnder fÃ¸lger noen eksempler pÃ¥ GIS-prosessering med testdataene.\nEksempel pÃ¥ avstandsberegning\nAvstand i luftlinje finner man enkelt med sjoin_nearest. SÃ¥nn her finner man avstand fra boligbygg (1000 tilfeldige) i Oslo til nÃ¦rmeste butikkbygg.\n\n\nnotebook\n\nboligbygg = sg.read_geopandas(f\"{testdatasti}/noen_boligbygg_oslo.parquet\")\nbutikkbygg = sg.read_geopandas(f\"{testdatasti}/butikkbygg_oslo.parquet\")\n\nboligbygg_med_butikkavstand = boligbygg.sjoin_nearest(butikkbygg, distance_col=\"meter_til_butikk\")\n\nFor Ã¥ finne avstand eller reisetid langs veier, kan man gjÃ¸re nettverksanalyse med sgis. Man mÃ¥ fÃ¸rst klargjÃ¸re vegnettet og bestemme regler for beregningen(e):\n\n\nnotebook\n\nveger_klargjort = (\n    sg.get_connected_components(veger)\n    .query(\"connected == 1\")\n    .pipe(sg.make_directed_network_norway)\n)\n\nregler = sg.NetworkAnalysisRules(directed=True, weight=\"minutes\")\n\nnwa = sg.NetworkAnalysis(network=veger_klargjort, rules=regler)\n\nSÃ¥ kan man beregne reisetider fra boligbygg til butikkbygg:\n\n\nnotebook\n\navstander = nwa.od_cost_matrix(boligbygg, butikkbygg)\n\nKorteste reisetid per bolig kan kobles pÃ¥ som kolonne i boligdataene sÃ¥nn her:\n\n\nnotebook\n\nboligbygg[\"minutter_til_butikk\"] = avstander.groupby(\"origin\")[\"minutes\"].min()\n\nUndersÃ¸k resultatene i interaktivt kart:\n\n\nnotebook\n\nsg.explore(boligbygg, butikkbygg, \"minutter_til_butikk\")\n\nSe mer informasjon og eksempler i dokumentasjonen.\nOg flere nettverksanalyse-eksempler her.\nEksempel pÃ¥ geografisk kobling\nDatasett kan kobles basert pÃ¥ geografisk overlapp med geopandas.sjoin. Denne fungerer akkurat som pandas.merge, bare at det kobles basert pÃ¥ geometrien.\nKodesnutten under returnerer Ã©n kommuneflate for hvert bygg som overlapper, med kolonner fra begge datasettene. Inner-join er default, men med left-join beholder man ogsÃ¥ geometriene som ikke overlapper.\n\n\nnotebook\n\nkommuner = sg.read_geopandas(f\"{testdatasti}/enkle_kommuner.parquet\")\nkommuner_med_boliginfo = kommuner.sjoin(boligbygg, how=\"left\")\n\nMed motsatt rekkefÃ¸lge, fÃ¥r man ett boligbygg per kommune den overlapper med. Siden byggene er punkter, vil hvert bygg havne i maksimalt Ã©n kommune.\n\n\nnotebook\n\nboligbygg_med_kommuneinfo = boligbygg.sjoin(kommuner, how=\"left\")\n\nEksempel pÃ¥ Ã¥ lage kart\nMan kan lage tematiske kart med sgis-klassen ThematicMap. Her er et enkelt eksempel pÃ¥ et kart over arealet i kommuner.\n\n\nnotebook\n\nkommuner[\"Kvadratmeter\"] = kommuner.area\n\nm = sg.ThematicMap(kommuner, column=\"Kvadratmeter\", size=15)\nm.title = \"Areal i kommunene\"\nm.plot()\n\nSe flere kart-eksempler her.\nKonvertere DataFrame til GeoDataFrame\nEn pandas.DataFrame med koordinat-kolonner, kan konverteres til GeoDataFrame sÃ¥nn her:\n\n\nnotebook\n\nimport pandas as pd\ndf = pd.DataFrame({\"x\": [10.8, 10.7, 10.9], \"y\": [59.9, 60, 59.85]})\n\ngdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.x, df.y), crs=4326)\n\nFor Ã¥ beregne avtand i meter og kunne koble med annen geodata i Dapla, mÃ¥ man ha UTM-koordinater (hvis man ikke hadde det fra fÃ¸r):\n\n\nnotebook\n\ngdf = gdf.to_crs(25833)\n\nSe ogsÃ¥ geopandas dokumentasjon for mer utfyllende informasjon.\n\n\n\n\nDen viktigste GIS-pakken i R er sf. Pakken er bygd opp rundt sf-objekter, som er data.frames med geografisk informasjon. Man kan gjÃ¸re standard tidyverse-opersjoner pÃ¥ sf-objektene, pluss GIS-operasjoner.\nSindre Mikael Haugen har lagd funksjoner for Ã¥ lese og skrive blant annet geodata i Dapla. For Ã¥ fÃ¥ geodata, setter man parametret â€˜sfâ€™ til TRUE:\n\n\nnotebook\n\nlibrary(fellesr)\n\nveger = read_SSB(\"ssb-dapla-felles-data-delt-prod/GIS/testdata/veger_oslo.parquet\", sf = TRUE)\n\nHaugen har ogsÃ¥ lagd en pakke for Ã¥ gjÃ¸re nettverksanalyse, som ogsÃ¥ lar deg geokode adresser, altsÃ¥ Ã¥ finne adressenes koordinater.\n\n\nnotebook\n\nlibrary(GISSB)\n\nLite eksempel pÃ¥ kobling basert geografisk sammenfall (enkle kommuner er egentlig til visualisering). Her fÃ¥r man ett bygg per kommune som overlapper (som maksimalt er Ã©n kommune siden dette er bygningspunkter):\n\n\nnotebook\n\nlibrary(sf)\n\nboligbygg = read_SSB(\"ssb-dapla-felles-data-delt-prod/GIS/testdata/noen_boligbygg_oslo.parquet\", sf = TRUE)\nenkle_kommuner = read_SSB(\"ssb-dapla-felles-data-delt-prod/GIS/testdata/enkle_kommuner.parquet\", sf = TRUE)\n\nboligbygg_med_kommuneinfo = st_join(boligbygg, enkle_kommuner)\n\nMed motsatt rekkefÃ¸lge, fÃ¥r man Ã©n kommuneflate per bolig som overlapper:\n\n\nnotebook\n\nkommuner_med_boliginfo = st_join(enkle_kommuner, boligbygg)",
    "crumbs": [
      "Statistikere",
      "Appendix",
      "Kartdata"
    ]
  },
  {
    "objectID": "statistikkere/kartdata.html#python",
    "href": "statistikkere/kartdata.html#python",
    "title": "Kartdata",
    "section": "",
    "text": "Geopandas er et Python-bibliotek som integrerer Pandas med GIS-analysering og kartlegging. Geopandas er bygd opp rundt GeoDataFrames, som er en pandas DataFrame med geografisk informasjon. Disse kan lastes inn direkte hvis man har geografiske filer, eller man kan konvertere fra DataFrames med koordinat-kolonner (se eksempel under).\nGeopandas kan blant annet brukes til Ã¥ kartlegge dataene, beregne avstander og labe variabler for nÃ¦rmiljÃ¸ ved Ã¥ koble datasett sammen basert pÃ¥ geografisk overlapp.\nFor lesing og skriving av geodata i Dapla, kan man bruke SSB-pakken sgis. Med denne kan man blant annet ogsÃ¥ beregne reisetider langs veg og lage publiseringsverdige kart.\nPakkene kan installeres i Dapla sÃ¥nn her:\n\n\nterminal\n\npoetry add geopandas\npoetry add ssb-sgis\n\nOg sÃ¥ importeres det i Python pÃ¥ vanlig vis.\n\n\nnotebook\n\nimport geopandas as gpd\nimport sgis as sg\n\nEksempel pÃ¥ lesing/skriving av kartdata\nMan kan lese/skrive geodata i Dapla med sgis-funksjonene read_geopandas og write_geopandas. StÃ¸ttede formater er blant annet (geo)parquet, geojson og gml. En rekke kartdata for hele landet til analyse- og visualiseringsformÃ¥l ligger i bÃ¸tta â€œkartâ€. For eksempel kommuneflater til analyse eller statistikk:\n\n\nnotebook\n\nfilsti = \"ssb-prod-kart-data-delt/kartdata_analyse/klargjorte-data/2023/ABAS_kommune_flate_p2023_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\n\nEksempel pÃ¥ lesing av kommuneflater for visualisering. Disse dataene er generalisert slik at de egner seg for Ã¥ lage kart, men blir unÃ¸yaktige til statistikk:\n\n\nnotebook\n\nfilsti = \"ssb-prod-kart-data-delt/kartdata_visualisering/klargjorte-data/2023/parquet/N5000_kommune_flate_p2023_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\n\nTilsvarende for skriving til parquet eller annet geodataformat:\n\n\nnotebook\n\nsg.write_geopandas(kommuner, filsti_ny) \n\n\n\n\nI tillegg til kartdataene, ligger det testdata i fellesbÃ¸tta som man kan leke seg med. Testdataene finnes for Oslo og Kongsvinger, og kan leses sÃ¥nn her:\n\n\nnotebook\n\ntestdatasti = \"ssb-dapla-felles-data-delt-prod/GIS/testdata\"\nveger = sg.read_geopandas(f\"{testdatasti}/veger_oslo.parquet\")\n\nUnder fÃ¸lger noen eksempler pÃ¥ GIS-prosessering med testdataene.\nEksempel pÃ¥ avstandsberegning\nAvstand i luftlinje finner man enkelt med sjoin_nearest. SÃ¥nn her finner man avstand fra boligbygg (1000 tilfeldige) i Oslo til nÃ¦rmeste butikkbygg.\n\n\nnotebook\n\nboligbygg = sg.read_geopandas(f\"{testdatasti}/noen_boligbygg_oslo.parquet\")\nbutikkbygg = sg.read_geopandas(f\"{testdatasti}/butikkbygg_oslo.parquet\")\n\nboligbygg_med_butikkavstand = boligbygg.sjoin_nearest(butikkbygg, distance_col=\"meter_til_butikk\")\n\nFor Ã¥ finne avstand eller reisetid langs veier, kan man gjÃ¸re nettverksanalyse med sgis. Man mÃ¥ fÃ¸rst klargjÃ¸re vegnettet og bestemme regler for beregningen(e):\n\n\nnotebook\n\nveger_klargjort = (\n    sg.get_connected_components(veger)\n    .query(\"connected == 1\")\n    .pipe(sg.make_directed_network_norway)\n)\n\nregler = sg.NetworkAnalysisRules(directed=True, weight=\"minutes\")\n\nnwa = sg.NetworkAnalysis(network=veger_klargjort, rules=regler)\n\nSÃ¥ kan man beregne reisetider fra boligbygg til butikkbygg:\n\n\nnotebook\n\navstander = nwa.od_cost_matrix(boligbygg, butikkbygg)\n\nKorteste reisetid per bolig kan kobles pÃ¥ som kolonne i boligdataene sÃ¥nn her:\n\n\nnotebook\n\nboligbygg[\"minutter_til_butikk\"] = avstander.groupby(\"origin\")[\"minutes\"].min()\n\nUndersÃ¸k resultatene i interaktivt kart:\n\n\nnotebook\n\nsg.explore(boligbygg, butikkbygg, \"minutter_til_butikk\")\n\nSe mer informasjon og eksempler i dokumentasjonen.\nOg flere nettverksanalyse-eksempler her.\nEksempel pÃ¥ geografisk kobling\nDatasett kan kobles basert pÃ¥ geografisk overlapp med geopandas.sjoin. Denne fungerer akkurat som pandas.merge, bare at det kobles basert pÃ¥ geometrien.\nKodesnutten under returnerer Ã©n kommuneflate for hvert bygg som overlapper, med kolonner fra begge datasettene. Inner-join er default, men med left-join beholder man ogsÃ¥ geometriene som ikke overlapper.\n\n\nnotebook\n\nkommuner = sg.read_geopandas(f\"{testdatasti}/enkle_kommuner.parquet\")\nkommuner_med_boliginfo = kommuner.sjoin(boligbygg, how=\"left\")\n\nMed motsatt rekkefÃ¸lge, fÃ¥r man ett boligbygg per kommune den overlapper med. Siden byggene er punkter, vil hvert bygg havne i maksimalt Ã©n kommune.\n\n\nnotebook\n\nboligbygg_med_kommuneinfo = boligbygg.sjoin(kommuner, how=\"left\")\n\nEksempel pÃ¥ Ã¥ lage kart\nMan kan lage tematiske kart med sgis-klassen ThematicMap. Her er et enkelt eksempel pÃ¥ et kart over arealet i kommuner.\n\n\nnotebook\n\nkommuner[\"Kvadratmeter\"] = kommuner.area\n\nm = sg.ThematicMap(kommuner, column=\"Kvadratmeter\", size=15)\nm.title = \"Areal i kommunene\"\nm.plot()\n\nSe flere kart-eksempler her.\nKonvertere DataFrame til GeoDataFrame\nEn pandas.DataFrame med koordinat-kolonner, kan konverteres til GeoDataFrame sÃ¥nn her:\n\n\nnotebook\n\nimport pandas as pd\ndf = pd.DataFrame({\"x\": [10.8, 10.7, 10.9], \"y\": [59.9, 60, 59.85]})\n\ngdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.x, df.y), crs=4326)\n\nFor Ã¥ beregne avtand i meter og kunne koble med annen geodata i Dapla, mÃ¥ man ha UTM-koordinater (hvis man ikke hadde det fra fÃ¸r):\n\n\nnotebook\n\ngdf = gdf.to_crs(25833)\n\nSe ogsÃ¥ geopandas dokumentasjon for mer utfyllende informasjon.",
    "crumbs": [
      "Statistikere",
      "Appendix",
      "Kartdata"
    ]
  },
  {
    "objectID": "statistikkere/kartdata.html#r",
    "href": "statistikkere/kartdata.html#r",
    "title": "Kartdata",
    "section": "",
    "text": "Den viktigste GIS-pakken i R er sf. Pakken er bygd opp rundt sf-objekter, som er data.frames med geografisk informasjon. Man kan gjÃ¸re standard tidyverse-opersjoner pÃ¥ sf-objektene, pluss GIS-operasjoner.\nSindre Mikael Haugen har lagd funksjoner for Ã¥ lese og skrive blant annet geodata i Dapla. For Ã¥ fÃ¥ geodata, setter man parametret â€˜sfâ€™ til TRUE:\n\n\nnotebook\n\nlibrary(fellesr)\n\nveger = read_SSB(\"ssb-dapla-felles-data-delt-prod/GIS/testdata/veger_oslo.parquet\", sf = TRUE)\n\nHaugen har ogsÃ¥ lagd en pakke for Ã¥ gjÃ¸re nettverksanalyse, som ogsÃ¥ lar deg geokode adresser, altsÃ¥ Ã¥ finne adressenes koordinater.\n\n\nnotebook\n\nlibrary(GISSB)\n\nLite eksempel pÃ¥ kobling basert geografisk sammenfall (enkle kommuner er egentlig til visualisering). Her fÃ¥r man ett bygg per kommune som overlapper (som maksimalt er Ã©n kommune siden dette er bygningspunkter):\n\n\nnotebook\n\nlibrary(sf)\n\nboligbygg = read_SSB(\"ssb-dapla-felles-data-delt-prod/GIS/testdata/noen_boligbygg_oslo.parquet\", sf = TRUE)\nenkle_kommuner = read_SSB(\"ssb-dapla-felles-data-delt-prod/GIS/testdata/enkle_kommuner.parquet\", sf = TRUE)\n\nboligbygg_med_kommuneinfo = st_join(boligbygg, enkle_kommuner)\n\nMed motsatt rekkefÃ¸lge, fÃ¥r man Ã©n kommuneflate per bolig som overlapper:\n\n\nnotebook\n\nkommuner_med_boliginfo = st_join(enkle_kommuner, boligbygg)",
    "crumbs": [
      "Statistikere",
      "Appendix",
      "Kartdata"
    ]
  },
  {
    "objectID": "statistikkere/statistikkbanken.html",
    "href": "statistikkere/statistikkbanken.html",
    "title": "dapla-statbank-client",
    "section": "",
    "text": "Pakken â€œdapla-statbank-clientâ€ kan brukes til Ã¥ overfÃ¸re tabeller til Statistikkbanken fra Jupyterlab i prodsonen og pÃ¥ Dapla. Den henter ogsÃ¥ â€œfilbeskrivelsenâ€ som beskriver formen dataene skal ha nÃ¥r de sendes inn til Statistikkbanken. Og den kan ogsÃ¥ hente publiserte data fra Statistikkbanken. Pakken er en python-pakke som baserer seg pÃ¥ at dataene (deltabellene) lastes inn i en eller flere pandas DataFrames fÃ¸r overfÃ¸ring. Ved Ã¥ hente ned â€œfilbeskrivelsenâ€ kan man validere dataene sine (dataframene) mot denne lokalt, uten Ã¥ sende dataene til Statistikkbanken. Dette kan vÃ¦re til hjelp under setting av formen pÃ¥ dataene. Ã… hente publiserte data fra Statistikkbanken kan gjÃ¸res gjennom lÃ¸se funksjoner, eller via â€œklientenâ€.\nLenker: - Pakken ligger her pÃ¥ Pypi. Og kan installeres via poetry med: poetry add dapla-statbank-client - Kodebasen for pakken ligger her, readme-en gir en teknisk innfÃ¸ring som du kan fÃ¸lge og kopiere kode fra, og om du finner noe du vil rapportere om bruken av pakken sÃ¥ gjÃ¸r det gjerne under â€œissuesâ€ pÃ¥ github-sidene. - Noe demokode ligger i repoet, og kan vÃ¦re ett godt utgangspunkt Ã¥ kopiere og endre fra.\n\n\nStatistikkbanken opererer med ulike databaser for testing og produksjonssetting. Om du vil at tallene faktisk skal oppdateres pÃ¥ nettsidene sÃ¥ mÃ¥ du sende til Statistikkbankens â€œPRODâ€-database. Om du kun vil teste innsending skal du sende til databasen â€œTESTâ€. Disse databasene har de samme lastebrukerne, men passordene er ulike. Om du vil teste innsending mÃ¥ du derfor skaffe deg â€œtest-passordetâ€ til den lastebrukeren som du har tilgjengelig. For Ã¥ gjÃ¸re tester via pakken mÃ¥ du vÃ¦re i staging pÃ¥ dapla: https://jupyter.dapla-staging.ssb.no/ eller staging i prodosonen: https://sl-jupyter-t.ssb.no/ Om du faktisk vil sende inn data til publisering i Statistikkbanken mÃ¥ du vÃ¦re i dapla-prod (den vanlige): https://jupyter.dapla.ssb.no/ eller i prodsonen pÃ¥: https://sl-jupyter-p.ssb.no/ For Ã¥ teste er det fint Ã¥ skaffe seg noe data fra fjorÃ¥rets publisering pÃ¥ et produksjonslÃ¸p man kjenner fra fÃ¸r. Evt. kan man hente data fra Statistikkbanken og sende disse tilbake snudd rett, og med riktig antall prikke-kolonner lagt til.\n\n\n\nSe mer detaljer i readme-en pÃ¥ prosjektets kodebase.\n\n\nFor Ã¥ kunne bruke pakken mÃ¥ du importere klienten:\n\n\nnotebook\n\nfrom statbank import StatbankClient\n\nSÃ¥ initialiserer du klienten med de innstillingene som oftest er faste pÃ¥ tvers av alle innsendingene fra ett produksjonslÃ¸p:\n\n\nnotebook\n\nstatcli = StatbankClient(loaduser=\"LAST360\", date=\"2050-01-01\", overwrite=True, approve=2)\n\nHer vil du bli bedt om Ã¥ skrive inn passordet til lastebrukeren.\n\n\n\nOm du har dataene klare er det bare Ã¥ overfÃ¸re, men du mÃ¥ vite navnet pÃ¥ deltabell-dat-filene. (Statistikkbanken lagrer disse ned igjen som dat-filer.)\n\n\nnotebook\n\ndata_07495 = {\"kargrs01fylker1.dat\" : df_07495_fylker,\n              \"kargrs01landet1.dat\" : df_07495_landet,}\nstatcli.transfer(data_07495, tableid=\"07495\")\n\nEtter innsending kommer det en link til Statistikkbankens GUI for Ã¥ fÃ¸lge med pÃ¥ om innsendingen gikk bra hos dem. Om det var det du Ã¸nsket, sÃ¥ er du nÃ¥ ferdigâ€¦ Men det finnes mer funksjonalitet herâ€¦\n\n\n\nFor Ã¥ hente filbeskrivelsen til en hovedtabell bruker du denne metoden under klienten:\n\n\nnotebook\n\nfilbeskrivelse = statcli.get_description(tableid=\"07495\")\nprint(filbeskrivelse)\n\nMed filbeskrivelsen kan du lett fÃ¥ en mal pÃ¥ dictionaryet du mÃ¥ plassere dataene i:\n\n\nnotebook\n\nfilbeskrivelse.transferdata_template()\n\nDu kan ogsÃ¥ validere dataene dine mot filbeskrivelsen:\n\n\nnotebook\n\nfilbeskrivelse.validate(data_07495, tableid=\"07495\")\n\n\n\n\n\nDet tas noe ekstra ansvar i pakken rundt avrunding av desimaltall, da filbeskrivelsen inneholder informasjon om hvor mange desimaler som blir lagret per kolonne, kan vi runde av til rett antall samtidig som vi tar ett bevisst valg rundt â€œhvilken vei vi skal runde avâ€. PÃ¥ barneskolen lÃ¦rte vi at ved 2,5 avrundet til 0 desimaler, sÃ¥ runder vi opp til 3. Det samme skjer i utgangspunktet i SAS og Excel. Python og R runder derimot â€œmot nÃ¦rmeste partallâ€, sÃ¥ fra 2,5 blir det rundet til 2, men fra 1,5 blir det ogsÃ¥ rundet til 2. Dette er for Ã¥ forhindre bias i en retning. Dvs. om alle tall rundes opp, vil en hel kolonne med tall â€œdras oppoverâ€, ved Ã¥ gjÃ¸re annenhver opp og ned, vil ikke helheten bli â€œdratt en spesifikk veiâ€. Siden â€œround to evenâ€ ikke er det folk er vandte til, gjÃ¸r vi derfor noe annet i denne pakken, enn det som er vanlig oppfÃ¸rsel i Python. Vi runder opp. Om du bruker fÃ¸lgende metoden under filbeskrivelsen pÃ¥ dataene, sÃ¥ vil denne runde oppover, samtidig som den konverterer til en streng for Ã¥ bevare formateringen.\n\n\nnotebook\n\ndata_07495 = filbeskrivelse.round_data(data_07495)  # For Ã¥ ta vare pÃ¥ endringene, sÃ¥ mÃ¥ du skrive tilbake over variabelen\n\n\n\n\n\nEn date-widget for Ã¥ visuelt endre til en valid dato.\nLagring av overfÃ¸ring og filbeskrivelses-objekter til json-filer\nUthenting av logg fra klienten\nHenting av publisert data fra statbanken\nRotering av tabeller fra statbanken",
    "crumbs": [
      "Statistikere",
      "Datatjenester",
      "dapla-statbank-client"
    ]
  },
  {
    "objectID": "statistikkere/statistikkbanken.html#testoverfÃ¸ring-fra-staging---faktisk-oppdatering-fra-prod",
    "href": "statistikkere/statistikkbanken.html#testoverfÃ¸ring-fra-staging---faktisk-oppdatering-fra-prod",
    "title": "dapla-statbank-client",
    "section": "",
    "text": "Statistikkbanken opererer med ulike databaser for testing og produksjonssetting. Om du vil at tallene faktisk skal oppdateres pÃ¥ nettsidene sÃ¥ mÃ¥ du sende til Statistikkbankens â€œPRODâ€-database. Om du kun vil teste innsending skal du sende til databasen â€œTESTâ€. Disse databasene har de samme lastebrukerne, men passordene er ulike. Om du vil teste innsending mÃ¥ du derfor skaffe deg â€œtest-passordetâ€ til den lastebrukeren som du har tilgjengelig. For Ã¥ gjÃ¸re tester via pakken mÃ¥ du vÃ¦re i staging pÃ¥ dapla: https://jupyter.dapla-staging.ssb.no/ eller staging i prodosonen: https://sl-jupyter-t.ssb.no/ Om du faktisk vil sende inn data til publisering i Statistikkbanken mÃ¥ du vÃ¦re i dapla-prod (den vanlige): https://jupyter.dapla.ssb.no/ eller i prodsonen pÃ¥: https://sl-jupyter-p.ssb.no/ For Ã¥ teste er det fint Ã¥ skaffe seg noe data fra fjorÃ¥rets publisering pÃ¥ et produksjonslÃ¸p man kjenner fra fÃ¸r. Evt. kan man hente data fra Statistikkbanken og sende disse tilbake snudd rett, og med riktig antall prikke-kolonner lagt til.",
    "crumbs": [
      "Statistikere",
      "Datatjenester",
      "dapla-statbank-client"
    ]
  },
  {
    "objectID": "statistikkere/statistikkbanken.html#kode-eksempler",
    "href": "statistikkere/statistikkbanken.html#kode-eksempler",
    "title": "dapla-statbank-client",
    "section": "",
    "text": "Se mer detaljer i readme-en pÃ¥ prosjektets kodebase.\n\n\nFor Ã¥ kunne bruke pakken mÃ¥ du importere klienten:\n\n\nnotebook\n\nfrom statbank import StatbankClient\n\nSÃ¥ initialiserer du klienten med de innstillingene som oftest er faste pÃ¥ tvers av alle innsendingene fra ett produksjonslÃ¸p:\n\n\nnotebook\n\nstatcli = StatbankClient(loaduser=\"LAST360\", date=\"2050-01-01\", overwrite=True, approve=2)\n\nHer vil du bli bedt om Ã¥ skrive inn passordet til lastebrukeren.\n\n\n\nOm du har dataene klare er det bare Ã¥ overfÃ¸re, men du mÃ¥ vite navnet pÃ¥ deltabell-dat-filene. (Statistikkbanken lagrer disse ned igjen som dat-filer.)\n\n\nnotebook\n\ndata_07495 = {\"kargrs01fylker1.dat\" : df_07495_fylker,\n              \"kargrs01landet1.dat\" : df_07495_landet,}\nstatcli.transfer(data_07495, tableid=\"07495\")\n\nEtter innsending kommer det en link til Statistikkbankens GUI for Ã¥ fÃ¸lge med pÃ¥ om innsendingen gikk bra hos dem. Om det var det du Ã¸nsket, sÃ¥ er du nÃ¥ ferdigâ€¦ Men det finnes mer funksjonalitet herâ€¦\n\n\n\nFor Ã¥ hente filbeskrivelsen til en hovedtabell bruker du denne metoden under klienten:\n\n\nnotebook\n\nfilbeskrivelse = statcli.get_description(tableid=\"07495\")\nprint(filbeskrivelse)\n\nMed filbeskrivelsen kan du lett fÃ¥ en mal pÃ¥ dictionaryet du mÃ¥ plassere dataene i:\n\n\nnotebook\n\nfilbeskrivelse.transferdata_template()\n\nDu kan ogsÃ¥ validere dataene dine mot filbeskrivelsen:\n\n\nnotebook\n\nfilbeskrivelse.validate(data_07495, tableid=\"07495\")",
    "crumbs": [
      "Statistikere",
      "Datatjenester",
      "dapla-statbank-client"
    ]
  },
  {
    "objectID": "statistikkere/statistikkbanken.html#problematikk-knyttet-til-avrunding",
    "href": "statistikkere/statistikkbanken.html#problematikk-knyttet-til-avrunding",
    "title": "dapla-statbank-client",
    "section": "",
    "text": "Det tas noe ekstra ansvar i pakken rundt avrunding av desimaltall, da filbeskrivelsen inneholder informasjon om hvor mange desimaler som blir lagret per kolonne, kan vi runde av til rett antall samtidig som vi tar ett bevisst valg rundt â€œhvilken vei vi skal runde avâ€. PÃ¥ barneskolen lÃ¦rte vi at ved 2,5 avrundet til 0 desimaler, sÃ¥ runder vi opp til 3. Det samme skjer i utgangspunktet i SAS og Excel. Python og R runder derimot â€œmot nÃ¦rmeste partallâ€, sÃ¥ fra 2,5 blir det rundet til 2, men fra 1,5 blir det ogsÃ¥ rundet til 2. Dette er for Ã¥ forhindre bias i en retning. Dvs. om alle tall rundes opp, vil en hel kolonne med tall â€œdras oppoverâ€, ved Ã¥ gjÃ¸re annenhver opp og ned, vil ikke helheten bli â€œdratt en spesifikk veiâ€. Siden â€œround to evenâ€ ikke er det folk er vandte til, gjÃ¸r vi derfor noe annet i denne pakken, enn det som er vanlig oppfÃ¸rsel i Python. Vi runder opp. Om du bruker fÃ¸lgende metoden under filbeskrivelsen pÃ¥ dataene, sÃ¥ vil denne runde oppover, samtidig som den konverterer til en streng for Ã¥ bevare formateringen.\n\n\nnotebook\n\ndata_07495 = filbeskrivelse.round_data(data_07495)  # For Ã¥ ta vare pÃ¥ endringene, sÃ¥ mÃ¥ du skrive tilbake over variabelen",
    "crumbs": [
      "Statistikere",
      "Datatjenester",
      "dapla-statbank-client"
    ]
  },
  {
    "objectID": "statistikkere/statistikkbanken.html#annen-funksjonalitet-du-kan-lese-om-i-readme-en-er",
    "href": "statistikkere/statistikkbanken.html#annen-funksjonalitet-du-kan-lese-om-i-readme-en-er",
    "title": "dapla-statbank-client",
    "section": "",
    "text": "En date-widget for Ã¥ visuelt endre til en valid dato.\nLagring av overfÃ¸ring og filbeskrivelses-objekter til json-filer\nUthenting av logg fra klienten\nHenting av publisert data fra statbanken\nRotering av tabeller fra statbanken",
    "crumbs": [
      "Statistikere",
      "Datatjenester",
      "dapla-statbank-client"
    ]
  },
  {
    "objectID": "statistikkere/contribution.html",
    "href": "statistikkere/contribution.html",
    "title": "Bidra til Dapla-manualen",
    "section": "",
    "text": "Alle i SSB kan bidra til denne manualen. Endringer mÃ¥ godkjennes av noen i Team Statistikktjenester, si gjerne i fra at det ligger en PR Ã¥ se pÃ¥.\n\n\n\n\n\n\nWarning\n\n\n\nDenne nettsiden er Ã¥pen og hvem som helst kan lese det som er skrevet her. Hold det i tankene nÃ¥r du skriver.\n\n\n\n\n\nMan trenger basis git kompetanse, det ligger en fin beskrivelse av det pÃ¥ Beste Praksis siden fra KVAKK.\nMan trenger en konto pÃ¥ Github, det kan man opprette ved Ã¥ fÃ¸lge instruksjonene her.\nMan kan lÃ¦re seg Markdown (enkel tekst formatering), en fin oversikt finnes her.\nVerktÃ¸yet Quarto burde installeres for Ã¥ kunne se endringene slik som de ser ut pÃ¥ nettsiden. Installasjon instruksjoner finnes her.\n\n\n\n\n\nKlone repoet\n\n\n\nterminal\n\ngit clone https://github.com/statisticsnorway/dapla-manual.git\n\n\nLage en ny gren\nGjÃ¸re endringen\nKjÃ¸r fÃ¸lgende og fÃ¸lge lenken for Ã¥ sjekke at alt ser bra ut pÃ¥ nettsiden:\n\n\n\nterminal\n\nquarto preview dapla-manual\n\n\nÃ…pne en PR\nBe noen Ã¥ gjennomgÃ¥ endringen og eventuelt godkjenne den\nMerge PRen\nEtter et par minutter skal din endring vÃ¦re synlig!\n\n\n\nQuarto tilbyr Ã¥ legge ved (embed) notebooks inn i nettsiden. Dette er en fin mÃ¥te Ã¥ dele kode og output pÃ¥. Men det krever at vi tenker gjennom hvor output`en genereres. Siden Dapla-manualen renderes med GitHub-action, sÃ¥ Ã¸nsker vi ikke Ã¥ introdusere kompleksiteten det innebÃ¦rer Ã¥ generere output fra kode her. I tillegg er det mange miljÃ¸-spesifikke konfigurasjoner som bestemmer hvordan output kan genereres. Derfor anbefaler vi fÃ¸lgende tilnÃ¦rming nÃ¥r man skal legge ved en notebook til et kapittel:\n\nLogg deg inn i miljÃ¸et du Ã¸nsker Ã¥ bruke.\nKlone ned repoet til Dapla-manualen.\nOpprett en notebook i mappen ./dapla-manual/notebooks/notebook-navn.ipynb\nSkriv kode og generer output som du Ã¸nsker. Husk Ã¥ bare bruke data som alle har tilgang til eller de kan generere selv.\nEmbed output i kapitlet du Ã¸nsker iht til denne beskrivelsen\nPÃ¥ toppen av notebooken lager du en raw-celle der du skriver:\n\n\n\nnotebook\n\n---\nfreeze: true\n---\n\n\nKjÃ¸r denne notebooken fra terminalen med kommandoen:\n\n\n\nterminal\n\nquarto render ./notebooks/&lt;notebook-navn.ipynb&gt; --execute\n\nDette sikrer at vi kan se output etter at boken bygges i GitHub Actions. Husk at notebooks ikke blir strippet for output pÃ¥ vanlig mÃ¥te, slik at kun Ã¥pne data skal benyttes.\nSpÃ¸r Team Statistikktjenester om du lurer pÃ¥ noe."
  },
  {
    "objectID": "statistikkere/contribution.html#forutsetninger",
    "href": "statistikkere/contribution.html#forutsetninger",
    "title": "Bidra til Dapla-manualen",
    "section": "",
    "text": "Man trenger basis git kompetanse, det ligger en fin beskrivelse av det pÃ¥ Beste Praksis siden fra KVAKK.\nMan trenger en konto pÃ¥ Github, det kan man opprette ved Ã¥ fÃ¸lge instruksjonene her.\nMan kan lÃ¦re seg Markdown (enkel tekst formatering), en fin oversikt finnes her.\nVerktÃ¸yet Quarto burde installeres for Ã¥ kunne se endringene slik som de ser ut pÃ¥ nettsiden. Installasjon instruksjoner finnes her."
  },
  {
    "objectID": "statistikkere/contribution.html#fremgangsmÃ¥ten",
    "href": "statistikkere/contribution.html#fremgangsmÃ¥ten",
    "title": "Bidra til Dapla-manualen",
    "section": "",
    "text": "Klone repoet\n\n\n\nterminal\n\ngit clone https://github.com/statisticsnorway/dapla-manual.git\n\n\nLage en ny gren\nGjÃ¸re endringen\nKjÃ¸r fÃ¸lgende og fÃ¸lge lenken for Ã¥ sjekke at alt ser bra ut pÃ¥ nettsiden:\n\n\n\nterminal\n\nquarto preview dapla-manual\n\n\nÃ…pne en PR\nBe noen Ã¥ gjennomgÃ¥ endringen og eventuelt godkjenne den\nMerge PRen\nEtter et par minutter skal din endring vÃ¦re synlig!\n\n\n\nQuarto tilbyr Ã¥ legge ved (embed) notebooks inn i nettsiden. Dette er en fin mÃ¥te Ã¥ dele kode og output pÃ¥. Men det krever at vi tenker gjennom hvor output`en genereres. Siden Dapla-manualen renderes med GitHub-action, sÃ¥ Ã¸nsker vi ikke Ã¥ introdusere kompleksiteten det innebÃ¦rer Ã¥ generere output fra kode her. I tillegg er det mange miljÃ¸-spesifikke konfigurasjoner som bestemmer hvordan output kan genereres. Derfor anbefaler vi fÃ¸lgende tilnÃ¦rming nÃ¥r man skal legge ved en notebook til et kapittel:\n\nLogg deg inn i miljÃ¸et du Ã¸nsker Ã¥ bruke.\nKlone ned repoet til Dapla-manualen.\nOpprett en notebook i mappen ./dapla-manual/notebooks/notebook-navn.ipynb\nSkriv kode og generer output som du Ã¸nsker. Husk Ã¥ bare bruke data som alle har tilgang til eller de kan generere selv.\nEmbed output i kapitlet du Ã¸nsker iht til denne beskrivelsen\nPÃ¥ toppen av notebooken lager du en raw-celle der du skriver:\n\n\n\nnotebook\n\n---\nfreeze: true\n---\n\n\nKjÃ¸r denne notebooken fra terminalen med kommandoen:\n\n\n\nterminal\n\nquarto render ./notebooks/&lt;notebook-navn.ipynb&gt; --execute\n\nDette sikrer at vi kan se output etter at boken bygges i GitHub Actions. Husk at notebooks ikke blir strippet for output pÃ¥ vanlig mÃ¥te, slik at kun Ã¥pne data skal benyttes.\nSpÃ¸r Team Statistikktjenester om du lurer pÃ¥ noe."
  },
  {
    "objectID": "statistikkere/kildomaten.html",
    "href": "statistikkere/kildomaten.html",
    "title": "Kildomaten",
    "section": "",
    "text": "Kildomaten er en tjeneste for Ã¥ automatisere overgangen fra kildedata til inndata. Tjenesten lar statistikkere kjÃ¸re sine egne skript automatisk pÃ¥ alle nye filer i kildedatabÃ¸tta og skrive resultatet til produktbÃ¸tta. FormÃ¥let med tjenesten er minimere behovet for tilgang til kildedata samtidig som teamet selv bestemmer hvordan transformasjonn til inndata skal foregÃ¥. Statistikkproduksjon kan da starte i en tilstand der dataminimering og pseudonymisering allerede er gjennomfÃ¸rt.\nProsessering som skal skje i overgangen fra kildedata til inndata har SSB definert til Ã¥ vÃ¦re:\n\nDataminimering:\nFjerne alle felter som ikke er strengt nÃ¸dvendig for Ã¥ produsere statistikk.\nPseudonymisering:\nPseudonymisering av personidentifiserende data.\nKodeverk:\nLegge pÃ¥ standard kodeverk fra for eksempel Klass.\n\nStandardisering:\nTegnsett, datoformat, etc. endres til SSBs standardformat.\n\nUnder forklarer vi nÃ¦rmere hvordan man bruker tjenesten. Da forutsetter vi at du har et Dapla-team med tjenesten er aktivert. les mer om hvordan du aktiverer tjenester her (lenker her).\n\n\nFÃ¸r et Dapla-team kan ta i bruk Kildomaten mÃ¥ man tjenesten aktivert for teamet. Som standard fÃ¥r alle statistikkteam dette skrudd pÃ¥ i prod-miljÃ¸et som opprettes for teamet. Ã˜nsker du Ã¥ aktivere Kildomaten i test-miljÃ¸et kan dette gjÃ¸res selvbetjent som en feature. Alternativt kan man opprette en Kundeservice-sak og be om Ã¥ hjelp til dette.\n\n\n\nI denne delen bryter vi ned prosessen med Ã¥ sette opp Kildomaten i de stegene vi mener det er hensiktsmessig Ã¥ gjÃ¸re det nÃ¥r den settes opp for fÃ¸rste gang pÃ¥ en enkeltkilde. Senere i kapitlet forklarer vi hvordan man setter den opp flere kilder og hvordan man vedlikeholder tjenesten.\nHusk at alle pÃ¥ teamet kan gjÃ¸re det meste av arbeidet her, men det er data-admins som mÃ¥ godkjenne at tjenesten rulles ut1.\n\n\nOppsett av Kildomaten gjÃ¸res i teamets IaC-repo2. NÃ¥r vi skal sette opp Kildomaten-kilde mÃ¥ vi gjÃ¸re gjÃ¸re endringer i teamets IaC-repo. Man finner teamets IaC-repo ved gÃ¥ inn pÃ¥ SSBs GitHub-organisasjon og sÃ¸ke etter repoet som heter &lt;teamnavn&gt;-iac. NÃ¥r du har funnet repoet sÃ¥ kan du gjÃ¸re fÃ¸lgende:\n\nKlon ned ditt teams Iac-repo git clone &lt;repo-url&gt;\nOpprett en ny branch: git checkout -b add-kildomaten-source\n\n\n\n\n\n\n\n\n\n\nAutomatisk oppretting kommer snart\n\n\n\nPlattformteamene jobber med Ã¥ automatisere opprettelsen av mappestruktur i prod-miljÃ¸et for alle statistikkteam ved opprettelse av teamet. Av den grunn vil innholdet her endre seg snart.\n\n\nFor at Kildomaten skal fungere sÃ¥ mÃ¥ vi fÃ¸lge en bestemt mappestruktur i IaC-repoet. Denne strukturen er nÃ¸dvendig for at tjenesten skal vite hvor den skal hente kildedata fra, og hvor den skal legge inndata. Denne strukturen blir ikke laget ved opprettelsen av teamet, siden ikke alle team kommer til Ã¥ bruke tjenesten.\nAnta at det er Dapla-team som heter dapla-example, og de har et IaC-repo som heter dapla-example-iac. De Ã¸nsker Ã¥ sette opp Kildomaten for en kilde som heter altinn i prod-miljÃ¸et til teamet. FÃ¸lgende mappestruktur mÃ¥ da opprettes i IaC-repoet til teamet:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\nâ”œâ”€â”€ automation/\nâ”‚   â””â”€â”€ source-data/\nâ”‚       â”œâ”€â”€ dapla-example-prod/\nâ”‚           â””â”€â”€ altinn/\nâ”‚ \nâ”‚...           \n\nI eksempelet over sÃ¥ har vi laget mappen dapla-example-prod for Ã¥ legge til kilder som skal kjÃ¸re i prod-miljÃ¸et. Strukturen pÃ¥ denne mappen mÃ¥ alltid vÃ¦re teamnavn etterfulgt av -miljÃ¸.\n\n\n\n\n\n\nAlle kilder pÃ¥ samme nivÃ¥\n\n\n\nEt team kan sette opp mange kilder som skal prosesseres med ulike skript. Men Kildomaten tillater bare et nivÃ¥ under automation/source-data-&lt;teamnavn&gt;-&lt;miljÃ¸&gt;/. Det vil si at du ikke kan ha undermapper under en kilde. Det er ogsÃ¥ slik at man alltid mÃ¥ opprette en mappe for en kilde, selv om du kun har en kilde. F.eks. kan man ikke droppe mappen altinn i eksempelet over.\n\n\n\n\n\nNÃ¥r mappestrukturen er opprettet kan man legge til filene som beskriver hvilke filer som skal prosesseres, og python-skriptet med koden som skal prosessere filene. Det er to filer som mÃ¥ eksistere for at tjenesten skal fungere:\n\nEn konfigurasjonsfil\nEt Python-skript\n\nNÃ¥r du har opprettet de skal de ligge pÃ¥ denne mÃ¥ten i IaC-repoet:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\nâ”œâ”€â”€ automation/\nâ”‚   â””â”€â”€ source-data/\nâ”‚       â”œâ”€â”€ dapla-example-prod/\nâ”‚           â””â”€â”€ altinn/\nâ”‚               â”œâ”€â”€ config.yaml\nâ”‚               â””â”€â”€ process_source_data.py\nâ”‚...           \n\nUnder forklares hvordan skriver innholdet i de to filene.\n\n\nKildomaten trigges ved at det oppstÃ¥r nye filer i kildebÃ¸tta til teamet. Hvorvidt den skal trigges pÃ¥ alle filer, eller kun filer i en gitt undermappe, bestemmer brukeren ved Ã¥ konfigurere tjenesten i config.yaml. Her kan du ogsÃ¥ angi hvor mye ressurser prosesseringen skal fÃ¥.\nHvis vi fortsetter eksempelet vÃ¥rt fra tidligere med dapla-example, sÃ¥ kan vi tenkes oss at teamet Ã¸nsker Ã¥ Kildomaten skal trigges pÃ¥ alle filer som oppstÃ¥r i kildebÃ¸tta under filstien:\nssb-dapla-example-data-kilde-prod/ledstill/altinn/.\nFor Ã¥ konfigurere tjenesten i Kildomaten mÃ¥ vi legge til en fil som heter config.yaml under automation/source-data/dapla-example-prod/altinn/, slik som vist i forrige kapitel. I dette tilfellet vil den ha innholdet som vist til venstre under:\n\n\n\n\nconfig.yaml\n\nfolder_prefix: ledstill/altinn\nmemory_size: 1  # GiB\n\n\n\n\n\n\nssb-dapla-example-data-kilde-prod/\n\nâ”œâ”€â”€ ledstill/\nâ”‚   â””â”€â”€ altinn/\nâ”‚   â””â”€â”€ aordningen/\nâ”œâ”€â”€ sykefra/\nâ”‚   â””â”€â”€ altinn/\nâ”‚   â””â”€â”€ freg/\nâ”‚...\n       \n\n\n\nMappestrukturen til hÃ¸yre over viser hvordan vi mappestrukturen ser ut i kildebÃ¸tta, mens config.yaml-fila til venstre viser hvordan vi angir at Kildomaten kun skal trigges pÃ¥ nye filer som oppstÃ¥r i undermappen ssb-dapla-example-data-kilde-prod/ledstill/altinn/. Vi bruker nÃ¸kkelen folder_prefix for Ã¥ angi hvilken sti i kildebÃ¸tta som tjenesten skal trigges pÃ¥. NÃ¸kkelen memory_size lar brukeren angi hvor mye minne og CPU hver prosessering skal fÃ¥.\n\n\n\n\n\n\nHvor mye minne og CPU er mulig?\n\n\n\nSom standard sÃ¥ fÃ¥r hver prosessering med Kildomaten 1 CPU-kjerne og 512MB minne/RAM. Hvis man skal gjÃ¸re mye prossesering, eller filene som prosesseres er veldig store, kan man sette memory_size opp til max 32GB minne. Antall CPU-kjerner blir satt implisitt ut i fra valg av memory_size iht til begrensningene som Google setter for Cloud Run. Se de nÃ¸yaktige verdiene som blir satt her.\n\n\n\n\n\n\n\n\n\n\n\nHusk dette nÃ¥r du skriver skriptet ditt\n\n\n\nNÃ¥r du skal skrive et Python-skript for Kildomaten er det spesielt viktig Ã¥ huske pÃ¥ 2 ting:\n\nSkriptet ditt kommer til Ã¥ bli kjÃ¸rt pÃ¥ en-og-en fil.\nSkriptet ditt mÃ¥ skrive ut et unikt navn pÃ¥ filen som skal skrives til produktbÃ¸tta, ellers risikerer du at filer blir overskrevet.\nPython-pakkene som kan benyttes er definert her. Ved behov for andre pakker, ta kontakt med Kundeservice.\n\n\n\n\nPython-skriptet er der teamet kan angi hva slags kode som skal kjÃ¸re pÃ¥ hver fil som dukker opp i den angitte mappen i kildebÃ¸tta. For at dette skal vÃ¦re mulig mÃ¥ koden fÃ¸lge disse reglene:\n\nKoden mÃ¥ ligge i en fil som heter process_source_data.py.\nKoden mÃ¥ pakkes inn i en funksjon som heter main().\n\nmain() er en funksjon med ett parameter som heter source_file som du alltid fÃ¥r av Kildomaten nÃ¥r en fil blir prosessert. NÃ¥r du skriver koden kan man derfor anta at dette parameteret blir gitt til funksjonen, og du kan benytte det inne i main-funksjonen.\nsource_file-parameteret gir main() en ferdig definert filsti til filen som skal prosesseres. For eksempel sÃ¥ kan source_file ha verdien\ngs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv hvis filen test20231010.csv dukker opp i ssb-dapla-example-data-kilde-prod/ledstill/altinn/.\nHvis vi fortsetter eksempelet fra tidligere sÃ¥ ser mappen i IaC-repoet vÃ¥rt slik ut nÃ¥:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\nâ”œâ”€â”€ automation/\nâ”‚   â””â”€â”€ source-data/\nâ”‚       â”œâ”€â”€ dapla-example-prod/\nâ”‚           â””â”€â”€ altinn/\nâ”‚               â”œâ”€â”€ config.yaml\nâ”‚               â””â”€â”€ process_source_data.py\nâ”‚\nâ”‚...         \n\nVi ser nÃ¥ at filene config.yaml og process_source_data.py ligger i mappen\nautomation/source-data/dapla-example-prod/ledstill/altinn/. Senere, nÃ¥r vi har rullet ut tjenesten, vil en ny fil i gs://ssb-dapla-exmaple-data-kilde-prod/ledstill/altinn/ trigge Kildomaten og kjÃ¸re koden i process_source_data.py pÃ¥ filen.\nUnder ser du et eksempel pÃ¥ hvordan en vanlig kodesnutt kan konverteres til Ã¥ kjÃ¸re i Kildomaten:\n\n\n\n\nVanlig kode\n\nimport dapla as dp\n\n# Stien til filen\nsource_file = \"gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv\"\n\ndf = dp.read_pandas(source_file, file_type=\"csv\")\n\n# Dataminimerer ved Ã¥ velge kolonner\ndf2 = df[['col1', 'col2', 'col3']]\n\n# Skriver ut fil til produktbÃ¸tta\ndp.write_pandas(df2, \n            \"gs://ssb-dapla-example-data-produkt-prod/test20231010.parquet\")\n\n\n\n\n\n\nKode i Kildomaten\n\nimport dapla as dp\n\ndef main(source_file):\n    df = dp.read_pandas(source_file, file_type=\"csv\")\n\n# Dataminimerer ved Ã¥ velge kolonner\n    df2 = df[['col1', 'col2', 'col3']]\n\n# Bytter ut \"kilde\" med \"produkt\" i filstien\n    new_path = source_file.replace(\"kilde\", \"produkt\")\n\n# Bytter ut \".csv\" med \".parquet\" i filstien\n    new_path2 = new_path.replace(\".csv\", \".parquet\")\n\n# Skriver ut fil til produktbÃ¸tta\n    dp.write_pandas(df2, new_path2)\n\n\n\nDe to kodesnuttene over gir det samme resultatet, bare at koden til venstre kjÃ¸res som vanlig python-kode, mens koden til hÃ¸yre kjÃ¸res i Kildomaten. Som vi ser av koden til hÃ¸yre sÃ¥ trenger vi aldri Ã¥ hardkode inn filstier i Kildomaten. Tjenesten gir oss filstien, og vi kan bruke den til Ã¥ skrive ut filen til produktbÃ¸tta.\nStrukturen pÃ¥ filene som skrives bÃ¸r tenkes nÃ¸ye gjennom nÃ¥r man automatiseres prosessering av data. Hvis vi ikke har unike filnavn eller stier sÃ¥ kan du risikere at filer blir overskrevet. Typisk er dette allerede tenkt pÃ¥ nÃ¥r filer skrives til kildebÃ¸tta, sÃ¥ hvis man kopierer den strukturen, slik vi gjorde i eksempelet over, sÃ¥ vil det ikke vÃ¦re noe problem.\n\n\n\n\n\n\nHvilke Python-biblioteker kan jeg bruke?\n\n\n\nDu kan kun bruke biblioteker som er forhÃ¥ndsinstallert i Kildomaten. Du kan se en liste over disse bibliotekene her. Ã˜nsker du andre biblioteker sÃ¥ mÃ¥ du ta kontakt med Kunderservice.\n\n\n\n\n\n\n\n\n\n\n\n\nTilgang til kildedata\n\n\n\nTilgang til kildedata i prod-miljÃ¸et er det kun gruppen data-admins som kan aktivere ved Ã¥ bruke tilgangsstyringslÃ¸sningen Just-in-Time Access (JIT). Les mer om hvordan JIT-lÃ¸sningen fungerer her. Ã˜nsker man Ã¥ kunne liste ut innhold fra bÃ¸tta mÃ¥ man aktivere rollen ssb.buckets.list. Ã˜nsker man i tillegg Ã¥ lese/skrive til bÃ¸tta mÃ¥ man ogsÃ¥ aktivere ssb.bucket.write. Tilgang til kildebÃ¸tta i test-miljÃ¸et krever ikke JIT-tilgang.\n\n\nFÃ¸r man ruller ut koden i tjenesten er det greit Ã¥ teste at alt fungerer som det skal i Jupyterlab. Hvis vi tar utgangspunkt i eksempelet over sÃ¥ kan vi teste koden ved Ã¥ kjÃ¸re fÃ¸lgende nederst i skriptet:\n\n\nprocess_source_data.py\n\nmain(\"gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv\")\n\nNÃ¥r tjenesten er rullet ut sÃ¥ vil det vÃ¦re dette som kjÃ¸res nÃ¥r en fil dukker opp i gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/. Ved Ã¥ kjÃ¸re det manuelt pÃ¥ denne mÃ¥ten fÃ¥r vi sett at ting fungerer som det skal.\nHusk Ã¥ fjerne kjÃ¸ringen av koden fÃ¸r du ruller ut tjenesten.\n\n\n\n\n\n\nPseudonymisering kan ikke kjÃ¸res fra en IDE i prod-miljÃ¸et\n\n\n\nDu kan teste kode fra Jupyter og andre IDE-er i prod-miljÃ¸et pÃ¥ Dapla. Men hvis prosesseringen innebÃ¦rer bruk av pseudonymisering, sÃ¥ vil den ikke kunne kalles fra programmeringsmiljÃ¸er som Jupyter. Grunnen til dette er at det ikke er Ã¸nskelig Ã¥ gjÃ¸re det lett Ã¥ se upseudonymisert og pseudonymisert data samtidig. Hvis du Ã¸nsker Ã¥ teste prosesseringen av pseudo-tjenesten, sÃ¥ kan du gjÃ¸re med testdata i test-miljÃ¸et.\n\n\n\n\n\nFor Ã¥ rulle ut tjenesten gjÃ¸r du fÃ¸lgende;\n\nPush branchen til GitHub og opprette en pull request.\nPull request mÃ¥ godkjennes av en data-admins pÃ¥ teamet.\n\n\n\n\nNÃ¥r pull request er godkjent sÃ¥ sjekker du om alle tester, planer og utrullinger var vellykket, slik som vist i FigurÂ 1.\nHvis alt er vellykket sÃ¥ kan du merge branchen inn i main-branchen.\n\n\n\n\n\n\n\n\n\nFigurÂ 1: Suksessfulle tester pÃ¥ GitHub\n\n\n\n\n\nEtter at du har merget inn i main kan du fÃ¸lge med pÃ¥ utrullingen under Actions-fanen i repoet. NÃ¥r den siste jobben lyser grÃ¸nt er Kildomaten rullet ut og klar til bruk.\n\n\n\n\n\n\nVanlige feil ved utrulling\n\n\n\nFor Ã¥ gi raskt tilbakemelding pÃ¥ noen mulige feilsituasjoner, sÃ¥ kjÃ¸res det enkel validering pÃ¥ config.yaml og process_source_data.py nÃ¥r en Pull request er opprettet. FÃ¸lgende validering gjennomfÃ¸res:\n\nEr det et python-script kalt process_source_data.py?\nEr det en config.yaml med en gyldig folder_prefix: definert?\nBruker process_source_data.py biblioteker som ikke er installert i Kildomaten?\nHar koden i process_source_data.py feil iht Pyflakes?\n\nHvis utrullingen feiler kan du trykke pÃ¥ Details i sjekkene som feilet, og se om noen feilene over har forekommet. Hvis ikke kan Kundeservice kontaktes for hjelp.\n\n\n\n\n\nNÃ¥r du har rullet ut tjenesten kan teste tjenesten ved at data-admins flytter en fil til en den filstien Kildomaten er satt opp for. I eksempelet vi har brukt tidligere, gjÃ¸r du fÃ¸lgende: 1. Aktiverer JIT-tilgangen til kildedata (som beskrevet over). 2. Kopier en fil over til filstien:\ngs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/ 3. Sjekk om filen ble skrevet til Ã¸nsket mappe i produktbÃ¸tta.\nDu kan ogsÃ¥ sjekke logger og monitorere Kildomaten. Les mer i neste avsnitt.\n\n\n\nNÃ¥r en kilde i Kildomaten er satt opp og ruller ut, kan du monitere tjenesten i Google Cloud Console (GCC) ved Ã¥ gjÃ¸re fÃ¸lgende:\n\nLogg deg inn med SSB-bruker pÃ¥ GCC.\nVelg standardprosjektet i prosjektvelgeren3.\nSÃ¸k opp Cloud Run i sÃ¸kefeltet pÃ¥ toppen av siden og gÃ¥ inn pÃ¥ siden.\n\nPÃ¥ siden til Cloud Run vil du se en oversikt over alle kilder teamet har kjÃ¸rende i Kildomaten. De har formen source-&lt;kildenavn&gt;-processor. I eksempelet med team dapla-example tidligere, vil man da se en kilde som heter source-altinn-processor, siden mappen de opprettet under\nautomation/source-data-dapla-example-prod/ i IaC-repoet heter altinn.\nTrykker man seg inn pÃ¥ Kilden vil man kunne monitorere aktiviteten i kilden, og man kan trykke seg videre for Ã¥ se loggene.\n\n\n\nKildomaten er satt opp for Ã¥ kunne prosessere hver kilde i 5 parallelle intanser samtidig. F.eks. hvis det oppstÃ¥r 10 nye filer i en mappe som trigger en Kildomaten-kilde, sÃ¥ kan det prosesseres 5 filer samtidig. Ta kontakt med Kundeservice hvis man har behov for ytterlige skalering.\n\n\n\nKildomaten tilbyr e-postvarsling til teamet nÃ¥r tjenesten feiler. Opprett en Kundeservice-sak for Ã¥ fÃ¥ satt opp e-postvarsling for teamet ditt.\n\n\n\nMan kan sette opp sÃ¥ mange kilder man Ã¸nsker. Men nÃ¥r man setter det opp er det viktig Ã¥ huske at alle kildene mÃ¥ spesifiseres rett under automation/source-data/&lt;teamnavn&gt;-&lt;miljÃ¸&gt;/. Her er et eksempel pÃ¥ hvordan team dapla-example har to kilder i Kildomaten:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\nâ”œâ”€â”€ automation/\nâ”‚   â””â”€â”€ source-data/\nâ”‚       â”œâ”€â”€ dapla-example-prod/\nâ”‚           â””â”€â”€ altinn/\nâ”‚               â”œâ”€â”€ config.yaml\nâ”‚               â””â”€â”€ process_source_data.py\nâ”‚           â””â”€â”€ ledstill/\nâ”‚               â”œâ”€â”€ config.yaml\nâ”‚               â””â”€â”€ process_source_data.py\nâ”‚...           \n\nI eksempelet over ser vi det er to kilder: altinn og ledstill. Hver kilde trigges pÃ¥ ulike filstier i kildebÃ¸tta, og python-koden som kjÃ¸res kan vÃ¦re ulik mellom kilder.\n\n\n\nI eksempelet som er brukt i dette kapitlet er Kildomaten satt opp i prodmiljÃ¸et til teamet. Mens egentlig burde man teste ut nye kilder i teamets test-miljÃ¸. Kildomaten er ikke satt opp i test-miljÃ¸et som standard, og derfor mÃ¥ det skrus pÃ¥ fÃ¸r man kan anvende det. Teamet kan gjÃ¸re det selv ved Ã¥ fÃ¸lge denne beskrivelsen, eller de kan ta kontakt med Kundeservice og fÃ¥ hjelp til dette.\nEn av de store fordelene med Ã¥ sette opp Kildomaten-kilder i test-miljÃ¸et fÃ¸r man gjÃ¸r det i prod-miljÃ¸et, er at tilgangsstyringen til data er mye mindre streng. Det gjÃ¸r det lettere for alle i teamet Ã¥ utvikle koden som skal benyttes.\nNÃ¥r man skal sette opp Kildomaten i test-miljÃ¸et sÃ¥ fÃ¸lger det samme oppskrift som vi har vist for prod-miljÃ¸et over. Den store forskjellen er at test-kilder skal legges under en egen mappe under automation/source-data/ i teamets IaC-repo. Eksempelet under med team dapla-example viser hvordan man kan sette opp kildene altinn og ledstill for bÃ¥de prod- og test-testmiljÃ¸et:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\nâ”œâ”€â”€ automation/\nâ”‚   â””â”€â”€ source-data/\nâ”‚       â”œâ”€â”€ dapla-example-prod/\nâ”‚       â”‚       â”œâ”€â”€ altinn/\nâ”‚       â”‚       â”‚       â”œâ”€â”€ config.yaml\nâ”‚       â”‚       â”‚       â””â”€â”€ process_source_data.py\nâ”‚       â”‚       â””â”€â”€ ledstill/\nâ”‚       â”‚               â”œâ”€â”€ config.yaml\nâ”‚       â”‚               â””â”€â”€ process_source_data.py\nâ”‚       â”œâ”€â”€ dapla-example-test/\nâ”‚               â”œâ”€â”€ altinn/\nâ”‚               â”‚       â”œâ”€â”€ config.yaml\nâ”‚               â”‚       â””â”€â”€ process_source_data.py\nâ”‚               â””â”€â”€ ledstill/\nâ”‚                       â”œâ”€â”€ config.yaml\nâ”‚                       â””â”€â”€ process_source_data.py\nâ”‚...           \n\nSom vi ser av mappestrukturen over sÃ¥ er det to mapper under\nautomation/source-data:\n\ndapla-example-prod\ndapla-example-test\n\nDisse to mappene angir om det er henholdsvis prod- eller test-miljÃ¸et vi setter opp kilder for.\n\n\n\nKildomaten er bygget for Ã¥ trigge pÃ¥ nye filer som oppstÃ¥r i en gitt filsti. Men noen ganger er det nÃ¸dvendig Ã¥ trigge kjÃ¸ring av alle filer pÃ¥ nytt. Noen ganger Ã¸nsker man kanskje Ã¥ kun trigge noen filer for en gitt kilde. Dette kan gjÃ¸res med en funksjon i Python-pakken dapla-toolbelt.\nFÃ¸r du kan gjÃ¸re dette trenger du fÃ¸lgende informasjon:\n\nproject-id for kildeprosjektet. Slik finner du det.\nfolder_prefix som du Ã¸nsker at koden skal trigges pÃ¥. Dette fungerer likt som tidligere forklart for config.yaml, men her har du ogsÃ¥ mulighet til Ã¥ kunne trigge prosesseringen pÃ¥ et undermappe av stien som var satt i kildens config.yaml.\nsource_name er navnet pÃ¥ kilden. Navnet pÃ¥ kilden i eksempelet med team dapla-example var altinn.\n\nHer er et kodeeksempel som viser hvordan man kan trigge prossesering av alle filer for kilde\n\n\nnotebook\n\nfrom dapla import trigger_source_data_processing\n\nproject_id = \"dapla-example-kilde-p-xx\"\nsource_name = \"altinn\"\nfolder_prefix = \"ledstill/altinn/ra0678\"\n\ntrigger_source_data_processing(project_id, source_name, folder_prefix)          \n\nI eksempelet over trigger vi scriptet som er satt opp for kilden altinn til Ã¥ kjÃ¸re pÃ¥ alle undermapper av\nssb-dapla-example-data-kilde-prod/ledstill/altinn/ra0678/.\n\n\n\n\nNÃ¥r tjenesten er rullet ut sÃ¥ vil den kjÃ¸re automatisk pÃ¥ alle filer som dukker opp i filsti i kildebÃ¸tta. Etter hvert vil det vÃ¦re behov for Ã¥ endre pÃ¥ skript, endre filstier som skal trigge tjenesten, eller prosessere alle filer pÃ¥ nytt. I denne delen forklarer vi hvordan du gÃ¥r frem for Ã¥ gjÃ¸re dette.\n\n\nAlle pÃ¥ team kan endre pÃ¥ skriptet, men det er data-admins som mÃ¥ godkjenne endringene fÃ¸r de blir rullet ut. For Ã¥ endre skriptet gjÃ¸r du fÃ¸lgende:\n\nKlon repoet.\nGjÃ¸r endringene du Ã¸nsker i en branch.\nPush opp endringene til GitHub og opprett en pull request.\nFÃ¥ en data-admins pÃ¥ teamet til Ã¥ godkjenne endringene.\nNÃ¥r endringene er godkjent sÃ¥ kan du merge inn i main-branchen.\n\nEn endring i Python-skriptet krever ikke at tjenesten rulles ut pÃ¥ nytt. Derfor er det ikke like mange tester og kjÃ¸ringer som gjÃ¸res som nÃ¥r man oppretter en helt ny kilde.\n\n\n\nAlle pÃ¥ teamet kan gjÃ¸re endringer i config.yaml, men det er data-admins som mÃ¥ godkjenne endringene fÃ¸r de blir rullet ut. For Ã¥ endre config.yaml gjÃ¸r du fÃ¸lgende:\n\nKlon repoet.\nGjÃ¸r endringene du Ã¸nsker i en branch.\nPush opp endringene til GitHub og opprett en pull request.\nFÃ¥ en data-admins pÃ¥ teamet til Ã¥ godkjenne endringene.\nNÃ¥r endringene er godkjent sÃ¥ kan du merge inn i main-branchen.\n\nEn endring i config.yaml krever at tjenesten rulles ut pÃ¥ nytt og tar derfor litt mer tid enn en endring i Python-skriptet.",
    "crumbs": [
      "Statistikere",
      "Datatjenester"
    ]
  },
  {
    "objectID": "statistikkere/kildomaten.html#forberedelser",
    "href": "statistikkere/kildomaten.html#forberedelser",
    "title": "Kildomaten",
    "section": "",
    "text": "FÃ¸r et Dapla-team kan ta i bruk Kildomaten mÃ¥ man tjenesten aktivert for teamet. Som standard fÃ¥r alle statistikkteam dette skrudd pÃ¥ i prod-miljÃ¸et som opprettes for teamet. Ã˜nsker du Ã¥ aktivere Kildomaten i test-miljÃ¸et kan dette gjÃ¸res selvbetjent som en feature. Alternativt kan man opprette en Kundeservice-sak og be om Ã¥ hjelp til dette.",
    "crumbs": [
      "Statistikere",
      "Datatjenester"
    ]
  },
  {
    "objectID": "statistikkere/kildomaten.html#sette-opp-tjenesten",
    "href": "statistikkere/kildomaten.html#sette-opp-tjenesten",
    "title": "Kildomaten",
    "section": "",
    "text": "I denne delen bryter vi ned prosessen med Ã¥ sette opp Kildomaten i de stegene vi mener det er hensiktsmessig Ã¥ gjÃ¸re det nÃ¥r den settes opp for fÃ¸rste gang pÃ¥ en enkeltkilde. Senere i kapitlet forklarer vi hvordan man setter den opp flere kilder og hvordan man vedlikeholder tjenesten.\nHusk at alle pÃ¥ teamet kan gjÃ¸re det meste av arbeidet her, men det er data-admins som mÃ¥ godkjenne at tjenesten rulles ut1.\n\n\nOppsett av Kildomaten gjÃ¸res i teamets IaC-repo2. NÃ¥r vi skal sette opp Kildomaten-kilde mÃ¥ vi gjÃ¸re gjÃ¸re endringer i teamets IaC-repo. Man finner teamets IaC-repo ved gÃ¥ inn pÃ¥ SSBs GitHub-organisasjon og sÃ¸ke etter repoet som heter &lt;teamnavn&gt;-iac. NÃ¥r du har funnet repoet sÃ¥ kan du gjÃ¸re fÃ¸lgende:\n\nKlon ned ditt teams Iac-repo git clone &lt;repo-url&gt;\nOpprett en ny branch: git checkout -b add-kildomaten-source\n\n\n\n\n\n\n\n\n\n\nAutomatisk oppretting kommer snart\n\n\n\nPlattformteamene jobber med Ã¥ automatisere opprettelsen av mappestruktur i prod-miljÃ¸et for alle statistikkteam ved opprettelse av teamet. Av den grunn vil innholdet her endre seg snart.\n\n\nFor at Kildomaten skal fungere sÃ¥ mÃ¥ vi fÃ¸lge en bestemt mappestruktur i IaC-repoet. Denne strukturen er nÃ¸dvendig for at tjenesten skal vite hvor den skal hente kildedata fra, og hvor den skal legge inndata. Denne strukturen blir ikke laget ved opprettelsen av teamet, siden ikke alle team kommer til Ã¥ bruke tjenesten.\nAnta at det er Dapla-team som heter dapla-example, og de har et IaC-repo som heter dapla-example-iac. De Ã¸nsker Ã¥ sette opp Kildomaten for en kilde som heter altinn i prod-miljÃ¸et til teamet. FÃ¸lgende mappestruktur mÃ¥ da opprettes i IaC-repoet til teamet:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\nâ”œâ”€â”€ automation/\nâ”‚   â””â”€â”€ source-data/\nâ”‚       â”œâ”€â”€ dapla-example-prod/\nâ”‚           â””â”€â”€ altinn/\nâ”‚ \nâ”‚...           \n\nI eksempelet over sÃ¥ har vi laget mappen dapla-example-prod for Ã¥ legge til kilder som skal kjÃ¸re i prod-miljÃ¸et. Strukturen pÃ¥ denne mappen mÃ¥ alltid vÃ¦re teamnavn etterfulgt av -miljÃ¸.\n\n\n\n\n\n\nAlle kilder pÃ¥ samme nivÃ¥\n\n\n\nEt team kan sette opp mange kilder som skal prosesseres med ulike skript. Men Kildomaten tillater bare et nivÃ¥ under automation/source-data-&lt;teamnavn&gt;-&lt;miljÃ¸&gt;/. Det vil si at du ikke kan ha undermapper under en kilde. Det er ogsÃ¥ slik at man alltid mÃ¥ opprette en mappe for en kilde, selv om du kun har en kilde. F.eks. kan man ikke droppe mappen altinn i eksempelet over.\n\n\n\n\n\nNÃ¥r mappestrukturen er opprettet kan man legge til filene som beskriver hvilke filer som skal prosesseres, og python-skriptet med koden som skal prosessere filene. Det er to filer som mÃ¥ eksistere for at tjenesten skal fungere:\n\nEn konfigurasjonsfil\nEt Python-skript\n\nNÃ¥r du har opprettet de skal de ligge pÃ¥ denne mÃ¥ten i IaC-repoet:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\nâ”œâ”€â”€ automation/\nâ”‚   â””â”€â”€ source-data/\nâ”‚       â”œâ”€â”€ dapla-example-prod/\nâ”‚           â””â”€â”€ altinn/\nâ”‚               â”œâ”€â”€ config.yaml\nâ”‚               â””â”€â”€ process_source_data.py\nâ”‚...           \n\nUnder forklares hvordan skriver innholdet i de to filene.\n\n\nKildomaten trigges ved at det oppstÃ¥r nye filer i kildebÃ¸tta til teamet. Hvorvidt den skal trigges pÃ¥ alle filer, eller kun filer i en gitt undermappe, bestemmer brukeren ved Ã¥ konfigurere tjenesten i config.yaml. Her kan du ogsÃ¥ angi hvor mye ressurser prosesseringen skal fÃ¥.\nHvis vi fortsetter eksempelet vÃ¥rt fra tidligere med dapla-example, sÃ¥ kan vi tenkes oss at teamet Ã¸nsker Ã¥ Kildomaten skal trigges pÃ¥ alle filer som oppstÃ¥r i kildebÃ¸tta under filstien:\nssb-dapla-example-data-kilde-prod/ledstill/altinn/.\nFor Ã¥ konfigurere tjenesten i Kildomaten mÃ¥ vi legge til en fil som heter config.yaml under automation/source-data/dapla-example-prod/altinn/, slik som vist i forrige kapitel. I dette tilfellet vil den ha innholdet som vist til venstre under:\n\n\n\n\nconfig.yaml\n\nfolder_prefix: ledstill/altinn\nmemory_size: 1  # GiB\n\n\n\n\n\n\nssb-dapla-example-data-kilde-prod/\n\nâ”œâ”€â”€ ledstill/\nâ”‚   â””â”€â”€ altinn/\nâ”‚   â””â”€â”€ aordningen/\nâ”œâ”€â”€ sykefra/\nâ”‚   â””â”€â”€ altinn/\nâ”‚   â””â”€â”€ freg/\nâ”‚...\n       \n\n\n\nMappestrukturen til hÃ¸yre over viser hvordan vi mappestrukturen ser ut i kildebÃ¸tta, mens config.yaml-fila til venstre viser hvordan vi angir at Kildomaten kun skal trigges pÃ¥ nye filer som oppstÃ¥r i undermappen ssb-dapla-example-data-kilde-prod/ledstill/altinn/. Vi bruker nÃ¸kkelen folder_prefix for Ã¥ angi hvilken sti i kildebÃ¸tta som tjenesten skal trigges pÃ¥. NÃ¸kkelen memory_size lar brukeren angi hvor mye minne og CPU hver prosessering skal fÃ¥.\n\n\n\n\n\n\nHvor mye minne og CPU er mulig?\n\n\n\nSom standard sÃ¥ fÃ¥r hver prosessering med Kildomaten 1 CPU-kjerne og 512MB minne/RAM. Hvis man skal gjÃ¸re mye prossesering, eller filene som prosesseres er veldig store, kan man sette memory_size opp til max 32GB minne. Antall CPU-kjerner blir satt implisitt ut i fra valg av memory_size iht til begrensningene som Google setter for Cloud Run. Se de nÃ¸yaktige verdiene som blir satt her.\n\n\n\n\n\n\n\n\n\n\n\nHusk dette nÃ¥r du skriver skriptet ditt\n\n\n\nNÃ¥r du skal skrive et Python-skript for Kildomaten er det spesielt viktig Ã¥ huske pÃ¥ 2 ting:\n\nSkriptet ditt kommer til Ã¥ bli kjÃ¸rt pÃ¥ en-og-en fil.\nSkriptet ditt mÃ¥ skrive ut et unikt navn pÃ¥ filen som skal skrives til produktbÃ¸tta, ellers risikerer du at filer blir overskrevet.\nPython-pakkene som kan benyttes er definert her. Ved behov for andre pakker, ta kontakt med Kundeservice.\n\n\n\n\nPython-skriptet er der teamet kan angi hva slags kode som skal kjÃ¸re pÃ¥ hver fil som dukker opp i den angitte mappen i kildebÃ¸tta. For at dette skal vÃ¦re mulig mÃ¥ koden fÃ¸lge disse reglene:\n\nKoden mÃ¥ ligge i en fil som heter process_source_data.py.\nKoden mÃ¥ pakkes inn i en funksjon som heter main().\n\nmain() er en funksjon med ett parameter som heter source_file som du alltid fÃ¥r av Kildomaten nÃ¥r en fil blir prosessert. NÃ¥r du skriver koden kan man derfor anta at dette parameteret blir gitt til funksjonen, og du kan benytte det inne i main-funksjonen.\nsource_file-parameteret gir main() en ferdig definert filsti til filen som skal prosesseres. For eksempel sÃ¥ kan source_file ha verdien\ngs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv hvis filen test20231010.csv dukker opp i ssb-dapla-example-data-kilde-prod/ledstill/altinn/.\nHvis vi fortsetter eksempelet fra tidligere sÃ¥ ser mappen i IaC-repoet vÃ¥rt slik ut nÃ¥:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\nâ”œâ”€â”€ automation/\nâ”‚   â””â”€â”€ source-data/\nâ”‚       â”œâ”€â”€ dapla-example-prod/\nâ”‚           â””â”€â”€ altinn/\nâ”‚               â”œâ”€â”€ config.yaml\nâ”‚               â””â”€â”€ process_source_data.py\nâ”‚\nâ”‚...         \n\nVi ser nÃ¥ at filene config.yaml og process_source_data.py ligger i mappen\nautomation/source-data/dapla-example-prod/ledstill/altinn/. Senere, nÃ¥r vi har rullet ut tjenesten, vil en ny fil i gs://ssb-dapla-exmaple-data-kilde-prod/ledstill/altinn/ trigge Kildomaten og kjÃ¸re koden i process_source_data.py pÃ¥ filen.\nUnder ser du et eksempel pÃ¥ hvordan en vanlig kodesnutt kan konverteres til Ã¥ kjÃ¸re i Kildomaten:\n\n\n\n\nVanlig kode\n\nimport dapla as dp\n\n# Stien til filen\nsource_file = \"gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv\"\n\ndf = dp.read_pandas(source_file, file_type=\"csv\")\n\n# Dataminimerer ved Ã¥ velge kolonner\ndf2 = df[['col1', 'col2', 'col3']]\n\n# Skriver ut fil til produktbÃ¸tta\ndp.write_pandas(df2, \n            \"gs://ssb-dapla-example-data-produkt-prod/test20231010.parquet\")\n\n\n\n\n\n\nKode i Kildomaten\n\nimport dapla as dp\n\ndef main(source_file):\n    df = dp.read_pandas(source_file, file_type=\"csv\")\n\n# Dataminimerer ved Ã¥ velge kolonner\n    df2 = df[['col1', 'col2', 'col3']]\n\n# Bytter ut \"kilde\" med \"produkt\" i filstien\n    new_path = source_file.replace(\"kilde\", \"produkt\")\n\n# Bytter ut \".csv\" med \".parquet\" i filstien\n    new_path2 = new_path.replace(\".csv\", \".parquet\")\n\n# Skriver ut fil til produktbÃ¸tta\n    dp.write_pandas(df2, new_path2)\n\n\n\nDe to kodesnuttene over gir det samme resultatet, bare at koden til venstre kjÃ¸res som vanlig python-kode, mens koden til hÃ¸yre kjÃ¸res i Kildomaten. Som vi ser av koden til hÃ¸yre sÃ¥ trenger vi aldri Ã¥ hardkode inn filstier i Kildomaten. Tjenesten gir oss filstien, og vi kan bruke den til Ã¥ skrive ut filen til produktbÃ¸tta.\nStrukturen pÃ¥ filene som skrives bÃ¸r tenkes nÃ¸ye gjennom nÃ¥r man automatiseres prosessering av data. Hvis vi ikke har unike filnavn eller stier sÃ¥ kan du risikere at filer blir overskrevet. Typisk er dette allerede tenkt pÃ¥ nÃ¥r filer skrives til kildebÃ¸tta, sÃ¥ hvis man kopierer den strukturen, slik vi gjorde i eksempelet over, sÃ¥ vil det ikke vÃ¦re noe problem.\n\n\n\n\n\n\nHvilke Python-biblioteker kan jeg bruke?\n\n\n\nDu kan kun bruke biblioteker som er forhÃ¥ndsinstallert i Kildomaten. Du kan se en liste over disse bibliotekene her. Ã˜nsker du andre biblioteker sÃ¥ mÃ¥ du ta kontakt med Kunderservice.\n\n\n\n\n\n\n\n\n\n\n\n\nTilgang til kildedata\n\n\n\nTilgang til kildedata i prod-miljÃ¸et er det kun gruppen data-admins som kan aktivere ved Ã¥ bruke tilgangsstyringslÃ¸sningen Just-in-Time Access (JIT). Les mer om hvordan JIT-lÃ¸sningen fungerer her. Ã˜nsker man Ã¥ kunne liste ut innhold fra bÃ¸tta mÃ¥ man aktivere rollen ssb.buckets.list. Ã˜nsker man i tillegg Ã¥ lese/skrive til bÃ¸tta mÃ¥ man ogsÃ¥ aktivere ssb.bucket.write. Tilgang til kildebÃ¸tta i test-miljÃ¸et krever ikke JIT-tilgang.\n\n\nFÃ¸r man ruller ut koden i tjenesten er det greit Ã¥ teste at alt fungerer som det skal i Jupyterlab. Hvis vi tar utgangspunkt i eksempelet over sÃ¥ kan vi teste koden ved Ã¥ kjÃ¸re fÃ¸lgende nederst i skriptet:\n\n\nprocess_source_data.py\n\nmain(\"gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/test20231010.csv\")\n\nNÃ¥r tjenesten er rullet ut sÃ¥ vil det vÃ¦re dette som kjÃ¸res nÃ¥r en fil dukker opp i gs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/. Ved Ã¥ kjÃ¸re det manuelt pÃ¥ denne mÃ¥ten fÃ¥r vi sett at ting fungerer som det skal.\nHusk Ã¥ fjerne kjÃ¸ringen av koden fÃ¸r du ruller ut tjenesten.\n\n\n\n\n\n\nPseudonymisering kan ikke kjÃ¸res fra en IDE i prod-miljÃ¸et\n\n\n\nDu kan teste kode fra Jupyter og andre IDE-er i prod-miljÃ¸et pÃ¥ Dapla. Men hvis prosesseringen innebÃ¦rer bruk av pseudonymisering, sÃ¥ vil den ikke kunne kalles fra programmeringsmiljÃ¸er som Jupyter. Grunnen til dette er at det ikke er Ã¸nskelig Ã¥ gjÃ¸re det lett Ã¥ se upseudonymisert og pseudonymisert data samtidig. Hvis du Ã¸nsker Ã¥ teste prosesseringen av pseudo-tjenesten, sÃ¥ kan du gjÃ¸re med testdata i test-miljÃ¸et.\n\n\n\n\n\nFor Ã¥ rulle ut tjenesten gjÃ¸r du fÃ¸lgende;\n\nPush branchen til GitHub og opprette en pull request.\nPull request mÃ¥ godkjennes av en data-admins pÃ¥ teamet.\n\n\n\n\nNÃ¥r pull request er godkjent sÃ¥ sjekker du om alle tester, planer og utrullinger var vellykket, slik som vist i FigurÂ 1.\nHvis alt er vellykket sÃ¥ kan du merge branchen inn i main-branchen.\n\n\n\n\n\n\n\n\n\nFigurÂ 1: Suksessfulle tester pÃ¥ GitHub\n\n\n\n\n\nEtter at du har merget inn i main kan du fÃ¸lge med pÃ¥ utrullingen under Actions-fanen i repoet. NÃ¥r den siste jobben lyser grÃ¸nt er Kildomaten rullet ut og klar til bruk.\n\n\n\n\n\n\nVanlige feil ved utrulling\n\n\n\nFor Ã¥ gi raskt tilbakemelding pÃ¥ noen mulige feilsituasjoner, sÃ¥ kjÃ¸res det enkel validering pÃ¥ config.yaml og process_source_data.py nÃ¥r en Pull request er opprettet. FÃ¸lgende validering gjennomfÃ¸res:\n\nEr det et python-script kalt process_source_data.py?\nEr det en config.yaml med en gyldig folder_prefix: definert?\nBruker process_source_data.py biblioteker som ikke er installert i Kildomaten?\nHar koden i process_source_data.py feil iht Pyflakes?\n\nHvis utrullingen feiler kan du trykke pÃ¥ Details i sjekkene som feilet, og se om noen feilene over har forekommet. Hvis ikke kan Kundeservice kontaktes for hjelp.\n\n\n\n\n\nNÃ¥r du har rullet ut tjenesten kan teste tjenesten ved at data-admins flytter en fil til en den filstien Kildomaten er satt opp for. I eksempelet vi har brukt tidligere, gjÃ¸r du fÃ¸lgende: 1. Aktiverer JIT-tilgangen til kildedata (som beskrevet over). 2. Kopier en fil over til filstien:\ngs://ssb-dapla-example-data-kilde-prod/ledstill/altinn/ 3. Sjekk om filen ble skrevet til Ã¸nsket mappe i produktbÃ¸tta.\nDu kan ogsÃ¥ sjekke logger og monitorere Kildomaten. Les mer i neste avsnitt.\n\n\n\nNÃ¥r en kilde i Kildomaten er satt opp og ruller ut, kan du monitere tjenesten i Google Cloud Console (GCC) ved Ã¥ gjÃ¸re fÃ¸lgende:\n\nLogg deg inn med SSB-bruker pÃ¥ GCC.\nVelg standardprosjektet i prosjektvelgeren3.\nSÃ¸k opp Cloud Run i sÃ¸kefeltet pÃ¥ toppen av siden og gÃ¥ inn pÃ¥ siden.\n\nPÃ¥ siden til Cloud Run vil du se en oversikt over alle kilder teamet har kjÃ¸rende i Kildomaten. De har formen source-&lt;kildenavn&gt;-processor. I eksempelet med team dapla-example tidligere, vil man da se en kilde som heter source-altinn-processor, siden mappen de opprettet under\nautomation/source-data-dapla-example-prod/ i IaC-repoet heter altinn.\nTrykker man seg inn pÃ¥ Kilden vil man kunne monitorere aktiviteten i kilden, og man kan trykke seg videre for Ã¥ se loggene.\n\n\n\nKildomaten er satt opp for Ã¥ kunne prosessere hver kilde i 5 parallelle intanser samtidig. F.eks. hvis det oppstÃ¥r 10 nye filer i en mappe som trigger en Kildomaten-kilde, sÃ¥ kan det prosesseres 5 filer samtidig. Ta kontakt med Kundeservice hvis man har behov for ytterlige skalering.\n\n\n\nKildomaten tilbyr e-postvarsling til teamet nÃ¥r tjenesten feiler. Opprett en Kundeservice-sak for Ã¥ fÃ¥ satt opp e-postvarsling for teamet ditt.\n\n\n\nMan kan sette opp sÃ¥ mange kilder man Ã¸nsker. Men nÃ¥r man setter det opp er det viktig Ã¥ huske at alle kildene mÃ¥ spesifiseres rett under automation/source-data/&lt;teamnavn&gt;-&lt;miljÃ¸&gt;/. Her er et eksempel pÃ¥ hvordan team dapla-example har to kilder i Kildomaten:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\nâ”œâ”€â”€ automation/\nâ”‚   â””â”€â”€ source-data/\nâ”‚       â”œâ”€â”€ dapla-example-prod/\nâ”‚           â””â”€â”€ altinn/\nâ”‚               â”œâ”€â”€ config.yaml\nâ”‚               â””â”€â”€ process_source_data.py\nâ”‚           â””â”€â”€ ledstill/\nâ”‚               â”œâ”€â”€ config.yaml\nâ”‚               â””â”€â”€ process_source_data.py\nâ”‚...           \n\nI eksempelet over ser vi det er to kilder: altinn og ledstill. Hver kilde trigges pÃ¥ ulike filstier i kildebÃ¸tta, og python-koden som kjÃ¸res kan vÃ¦re ulik mellom kilder.\n\n\n\nI eksempelet som er brukt i dette kapitlet er Kildomaten satt opp i prodmiljÃ¸et til teamet. Mens egentlig burde man teste ut nye kilder i teamets test-miljÃ¸. Kildomaten er ikke satt opp i test-miljÃ¸et som standard, og derfor mÃ¥ det skrus pÃ¥ fÃ¸r man kan anvende det. Teamet kan gjÃ¸re det selv ved Ã¥ fÃ¸lge denne beskrivelsen, eller de kan ta kontakt med Kundeservice og fÃ¥ hjelp til dette.\nEn av de store fordelene med Ã¥ sette opp Kildomaten-kilder i test-miljÃ¸et fÃ¸r man gjÃ¸r det i prod-miljÃ¸et, er at tilgangsstyringen til data er mye mindre streng. Det gjÃ¸r det lettere for alle i teamet Ã¥ utvikle koden som skal benyttes.\nNÃ¥r man skal sette opp Kildomaten i test-miljÃ¸et sÃ¥ fÃ¸lger det samme oppskrift som vi har vist for prod-miljÃ¸et over. Den store forskjellen er at test-kilder skal legges under en egen mappe under automation/source-data/ i teamets IaC-repo. Eksempelet under med team dapla-example viser hvordan man kan sette opp kildene altinn og ledstill for bÃ¥de prod- og test-testmiljÃ¸et:\n\n\ngithub.com/statisticsnorway/dapla-example-iac\n\ndapla-example-iac\nâ”œâ”€â”€ automation/\nâ”‚   â””â”€â”€ source-data/\nâ”‚       â”œâ”€â”€ dapla-example-prod/\nâ”‚       â”‚       â”œâ”€â”€ altinn/\nâ”‚       â”‚       â”‚       â”œâ”€â”€ config.yaml\nâ”‚       â”‚       â”‚       â””â”€â”€ process_source_data.py\nâ”‚       â”‚       â””â”€â”€ ledstill/\nâ”‚       â”‚               â”œâ”€â”€ config.yaml\nâ”‚       â”‚               â””â”€â”€ process_source_data.py\nâ”‚       â”œâ”€â”€ dapla-example-test/\nâ”‚               â”œâ”€â”€ altinn/\nâ”‚               â”‚       â”œâ”€â”€ config.yaml\nâ”‚               â”‚       â””â”€â”€ process_source_data.py\nâ”‚               â””â”€â”€ ledstill/\nâ”‚                       â”œâ”€â”€ config.yaml\nâ”‚                       â””â”€â”€ process_source_data.py\nâ”‚...           \n\nSom vi ser av mappestrukturen over sÃ¥ er det to mapper under\nautomation/source-data:\n\ndapla-example-prod\ndapla-example-test\n\nDisse to mappene angir om det er henholdsvis prod- eller test-miljÃ¸et vi setter opp kilder for.\n\n\n\nKildomaten er bygget for Ã¥ trigge pÃ¥ nye filer som oppstÃ¥r i en gitt filsti. Men noen ganger er det nÃ¸dvendig Ã¥ trigge kjÃ¸ring av alle filer pÃ¥ nytt. Noen ganger Ã¸nsker man kanskje Ã¥ kun trigge noen filer for en gitt kilde. Dette kan gjÃ¸res med en funksjon i Python-pakken dapla-toolbelt.\nFÃ¸r du kan gjÃ¸re dette trenger du fÃ¸lgende informasjon:\n\nproject-id for kildeprosjektet. Slik finner du det.\nfolder_prefix som du Ã¸nsker at koden skal trigges pÃ¥. Dette fungerer likt som tidligere forklart for config.yaml, men her har du ogsÃ¥ mulighet til Ã¥ kunne trigge prosesseringen pÃ¥ et undermappe av stien som var satt i kildens config.yaml.\nsource_name er navnet pÃ¥ kilden. Navnet pÃ¥ kilden i eksempelet med team dapla-example var altinn.\n\nHer er et kodeeksempel som viser hvordan man kan trigge prossesering av alle filer for kilde\n\n\nnotebook\n\nfrom dapla import trigger_source_data_processing\n\nproject_id = \"dapla-example-kilde-p-xx\"\nsource_name = \"altinn\"\nfolder_prefix = \"ledstill/altinn/ra0678\"\n\ntrigger_source_data_processing(project_id, source_name, folder_prefix)          \n\nI eksempelet over trigger vi scriptet som er satt opp for kilden altinn til Ã¥ kjÃ¸re pÃ¥ alle undermapper av\nssb-dapla-example-data-kilde-prod/ledstill/altinn/ra0678/.",
    "crumbs": [
      "Statistikere",
      "Datatjenester"
    ]
  },
  {
    "objectID": "statistikkere/kildomaten.html#vedlikehold",
    "href": "statistikkere/kildomaten.html#vedlikehold",
    "title": "Kildomaten",
    "section": "",
    "text": "NÃ¥r tjenesten er rullet ut sÃ¥ vil den kjÃ¸re automatisk pÃ¥ alle filer som dukker opp i filsti i kildebÃ¸tta. Etter hvert vil det vÃ¦re behov for Ã¥ endre pÃ¥ skript, endre filstier som skal trigge tjenesten, eller prosessere alle filer pÃ¥ nytt. I denne delen forklarer vi hvordan du gÃ¥r frem for Ã¥ gjÃ¸re dette.\n\n\nAlle pÃ¥ team kan endre pÃ¥ skriptet, men det er data-admins som mÃ¥ godkjenne endringene fÃ¸r de blir rullet ut. For Ã¥ endre skriptet gjÃ¸r du fÃ¸lgende:\n\nKlon repoet.\nGjÃ¸r endringene du Ã¸nsker i en branch.\nPush opp endringene til GitHub og opprett en pull request.\nFÃ¥ en data-admins pÃ¥ teamet til Ã¥ godkjenne endringene.\nNÃ¥r endringene er godkjent sÃ¥ kan du merge inn i main-branchen.\n\nEn endring i Python-skriptet krever ikke at tjenesten rulles ut pÃ¥ nytt. Derfor er det ikke like mange tester og kjÃ¸ringer som gjÃ¸res som nÃ¥r man oppretter en helt ny kilde.\n\n\n\nAlle pÃ¥ teamet kan gjÃ¸re endringer i config.yaml, men det er data-admins som mÃ¥ godkjenne endringene fÃ¸r de blir rullet ut. For Ã¥ endre config.yaml gjÃ¸r du fÃ¸lgende:\n\nKlon repoet.\nGjÃ¸r endringene du Ã¸nsker i en branch.\nPush opp endringene til GitHub og opprett en pull request.\nFÃ¥ en data-admins pÃ¥ teamet til Ã¥ godkjenne endringene.\nNÃ¥r endringene er godkjent sÃ¥ kan du merge inn i main-branchen.\n\nEn endring i config.yaml krever at tjenesten rulles ut pÃ¥ nytt og tar derfor litt mer tid enn en endring i Python-skriptet.",
    "crumbs": [
      "Statistikere",
      "Datatjenester"
    ]
  },
  {
    "objectID": "statistikkere/kildomaten.html#footnotes",
    "href": "statistikkere/kildomaten.html#footnotes",
    "title": "Kildomaten",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI tillegg er det data-admins som mÃ¥ teste tjenesten manuelt hvis det gjÃ¸res pÃ¥ skarpe data, siden det kun er data-admins som kan fÃ¥ tilgang til de dataene.â†©ï¸\nInfrastructure-as-Code (IaC) er repo som definerer alle ressursene til teamet pÃ¥ Dapla. Alle Dapla-team har et eget IaC-repo pÃ¥ GiHub og du finner det ved Ã¥ sÃ¸ke etter repoet -iac under statisticsnorway.â†©ï¸\nStandardprosjektet har navnestrukturen &lt;teamnavn&gt;-pâ†©ï¸",
    "crumbs": [
      "Statistikere",
      "Datatjenester"
    ]
  },
  {
    "objectID": "statistikkere/hvorfor-dapla.html",
    "href": "statistikkere/hvorfor-dapla.html",
    "title": "Hvorfor Dapla?",
    "section": "",
    "text": "Hvorfor Dapla?\nSom dataplattform skal Dapla stimulerere til Ã¸kt kvalitet pÃ¥ statistikk og forskning, samtidig som den gjÃ¸r organisasjonen mer tilpasningsdyktig i mÃ¸te med fremtiden.\n\n\nDen nye skybaserte dataplattformen (Dapla) skal bli viktig for Ã¥ effektivisere arbeids-og produksjonsprosesser, den skal sikre effektiv lagring og gjenfinning av data og metadata, og stÃ¸tte opp under deling av data pÃ¥ tvers av statistikkomrÃ¥der.\n\nKilde: Langtidsplan for SSB (2022-2024)\n\n\n\nMÃ¥let med Dapla er Ã¥ tilby tjenester og verktÃ¸y som lar statistikkprodusenter og forskere produsere resultater pÃ¥ en sikker og effektiv mÃ¥te."
  },
  {
    "objectID": "statistikkere/statistikkproduksjon.html",
    "href": "statistikkere/statistikkproduksjon.html",
    "title": "Statistikkproduksjon",
    "section": "",
    "text": "Statistikkproduksjon\nI forrige del beskrev vi noen grunnleggende ting rundt Dapla. I denne delen tar vi for oss hvordan du kan begynne Ã¥ jobbe med skarpe data pÃ¥ plattformen.\nKapittelet som beskriver hvordan man logger seg inn pÃ¥ Dapla vil fungere uten at du mÃ¥ gjÃ¸re noen forberedelser. Er man koblet pÃ¥ SSB sitt nettverk sÃ¥ vil alle SSB-ansatte kunne gÃ¥ inn pÃ¥ plattformen og kode i Python og R. Men du fÃ¥r ikke tilgang til SSBs omrÃ¥de for datalagring pÃ¥ plattformen. I praksis vil det si at man kan generere data med kode, men man kan ikke jobbe med skarpe data.\nFor Ã¥ fÃ¥ muligheten til Ã¥ jobbe med skarpe data MÃ… du fÃ¸rst opprette et dapla-team. Dette er det fÃ¸rste naturlige steget Ã¥ ta nÃ¥r man skal begynne Ã¥ jobbe med statistikkproduksjon pÃ¥ Dapla. I dette kapittelet vil vi forklare det du trenger Ã¥ vite om det Ã¥ opprette og jobbe innenfor et team."
  },
  {
    "objectID": "statistikkere/datadoc.html",
    "href": "statistikkere/datadoc.html",
    "title": "DataDoc",
    "section": "",
    "text": "For Ã¥ kunne gjenfinne data i SSB er man helt avhengig av at det finnes et enhetlig system for metadata knyttet til dataene. DataDoc er SSBs system for Ã¥ dokumentere datasett pÃ¥ den nye dataplattformen Dapla.\nDet er bygget et grensesnitt i Python for Ã¥ gjÃ¸re det enklest mulig Ã¥ dokumentere et datasett. ForelÃ¸pig stÃ¸tter lÃ¸sningen fÃ¸lgende filformater:\n\nparquet\nsas7bdat\n\nUnder finner du beskrivelse av hvordan du kan begynne Ã¥ bruke lÃ¸sningen til Ã¥ dokumentere datasett.\n\n\n\n\n\n\nWarning\n\n\n\nVi Ã¸nsker at du skal teste DataDoc-applikasjonen. Den viktigste funksjonaliteten skal vÃ¦re tilgjengelig, og det er fullt mulig Ã¥ benytte DataDoc i SSBs Jupyter-miljÃ¸er. Det er imidlertid viktig Ã¥ vÃ¦re klar over at applikasjonen fortsatt er i en utviklings- og testfase (beta-lÃ¸sning) og kan inneholde feil og mangler.\nHar du spÃ¸rsmÃ¥l, eventuelt vil rapporterer om feil og mangler, sÃ¥ setter vi pris pÃ¥ om du gjÃ¸r dette i Yammer-gruppa Dapla.\n\n\n\n\nFÃ¸r du tar i bruk DataDoc-applikasjonen er det viktig Ã¥ forstÃ¥ hvilken informasjon som skal til for Ã¥ dokumentere et datasett. I DataDoc-applikasjonen skal du fylle ut flere felter om bÃ¥de datasettet og variablene som inngÃ¥r i datasettet, eksempelvis\n\nkortnavn\nnavn\ndatatilstand\npopulasjonsbeskrivelse\n++\n\nDet er utarbeidet en detaljert beskrivelse hva hvert felt betyr, og hvordan de skal fylles ut bÃ¥de for datasett og variabler: - DataDoc - hvordan dokumentere et datasett - DataDoc - hvordan dokumentere variablene (variabelforekomstene) som inngÃ¥r i datasettet\nDataDoc skal vÃ¦re installert i alle Jupyter-miljÃ¸ene i SSB, sÃ¥ du trenger ikke installere pakken selv.\n\n\n\n\n\n\nViktig informasjon\n\n\n\nDataDoc kan forelÃ¸pig ikke kjÃ¸res i Jupyter notebook med virtuelle miljÃ¸er (f.eks. et ssb-project), men mÃ¥ startes i den vanlige kernelen i en notebook.\n\n\n\n\n\nLa oss lage et test-datasett slik at vi kan leke oss litt med DataDoc:\n\n\nnotebook\n\nimport pandas as pd\nfrom datadoc import main\n  \n# Create fake data\ndata = {'id': ['9999999999', '8888888888', '7777777777', '6666666666'],\n        'fylke': [\"01\", \"02\", \"03\", \"03\"],\n        'inntekt': [500000, 250000, 400000, 440000],\n        'rente': [3.2, 4.1, 3.3, 3.4]}\n  \n# Creates a Pandas dataframe\ndf = pd.DataFrame(data)\n\n# Write a Parquet-file to current folder\ndf.to_parquet(\"./test.parquet\")\n\nNÃ¥ har vi en fil som heter test.parquet i mappen vi stÃ¥r. Da kan vi Ã¥pne DataDoc-grensesnittet for Ã¥ legge inn metadataene:\n\n\nnotebook\n\nmain(\"./test.parquet\")\n\nFigurÂ 1 viser hvordan DataDoc-grensesnittet ser ut.\n\n\n\n\n\n\nFigurÂ 1: Gif som viser hvordan DataDoc-grensesnittet ser ut.\n\n\n\n\n\n\nNÃ¥r du trykker pÃ¥ Lagre-knappen i DataDoc sÃ¥ skrives alle metadata til en fil i samme mappe (katalog) som datafilen. Dette er en JSON-fil med nesten samme navn som datafilen. Navnekonvensjonen for metadatafilen er\n&lt;navn pÃ¥ datafil uten endelse&gt;__DOC.json\nEksempelvis hvis datafilen har navnet skattedata_p2022_v1.parquet, sÃ¥ vil DataDoc lagre metadata i filen skattedata_p2022_v1__DOC.json.\nFordelen med Ã¥ benytte en JSON-fil til Ã¥ lagre metadata er at denne filen kan kopieres og flyttes like enkelt som selve datafilen. JSON-filer er strukturerte tekstfiler som kan leses av bÃ¥de maskiner (Python/R) og av mennesker (Ã¥pnes i en tekst-editor).\nSe et eksempel pÃ¥ JSON metadata-fil lagret av DataDoc.\n\n\n\n\n\n\nInformasjon\n\n\n\nI Dapla skal det bygges en felles datakatalog for SSB. Tanken er at alle metadata, eksempelvis datasett-dokumentasjon fra DataDoc (JSON-filene), skal inngÃ¥ i SSBs datakatalog. Datakatalogen gjÃ¸r det mulig Ã¥ finne (sÃ¸ke etter), forstÃ¥r og gjenbruke data bÃ¥de internt og ekstern.",
    "crumbs": [
      "Statistikere",
      "Metadata"
    ]
  },
  {
    "objectID": "statistikkere/datadoc.html#hvordan-dokumentere-datasett-og-variabler-med-datadoc",
    "href": "statistikkere/datadoc.html#hvordan-dokumentere-datasett-og-variabler-med-datadoc",
    "title": "DataDoc",
    "section": "",
    "text": "FÃ¸r du tar i bruk DataDoc-applikasjonen er det viktig Ã¥ forstÃ¥ hvilken informasjon som skal til for Ã¥ dokumentere et datasett. I DataDoc-applikasjonen skal du fylle ut flere felter om bÃ¥de datasettet og variablene som inngÃ¥r i datasettet, eksempelvis\n\nkortnavn\nnavn\ndatatilstand\npopulasjonsbeskrivelse\n++\n\nDet er utarbeidet en detaljert beskrivelse hva hvert felt betyr, og hvordan de skal fylles ut bÃ¥de for datasett og variabler: - DataDoc - hvordan dokumentere et datasett - DataDoc - hvordan dokumentere variablene (variabelforekomstene) som inngÃ¥r i datasettet\nDataDoc skal vÃ¦re installert i alle Jupyter-miljÃ¸ene i SSB, sÃ¥ du trenger ikke installere pakken selv.\n\n\n\n\n\n\nViktig informasjon\n\n\n\nDataDoc kan forelÃ¸pig ikke kjÃ¸res i Jupyter notebook med virtuelle miljÃ¸er (f.eks. et ssb-project), men mÃ¥ startes i den vanlige kernelen i en notebook.",
    "crumbs": [
      "Statistikere",
      "Metadata"
    ]
  },
  {
    "objectID": "statistikkere/datadoc.html#prÃ¸ve-datadoc",
    "href": "statistikkere/datadoc.html#prÃ¸ve-datadoc",
    "title": "DataDoc",
    "section": "",
    "text": "La oss lage et test-datasett slik at vi kan leke oss litt med DataDoc:\n\n\nnotebook\n\nimport pandas as pd\nfrom datadoc import main\n  \n# Create fake data\ndata = {'id': ['9999999999', '8888888888', '7777777777', '6666666666'],\n        'fylke': [\"01\", \"02\", \"03\", \"03\"],\n        'inntekt': [500000, 250000, 400000, 440000],\n        'rente': [3.2, 4.1, 3.3, 3.4]}\n  \n# Creates a Pandas dataframe\ndf = pd.DataFrame(data)\n\n# Write a Parquet-file to current folder\ndf.to_parquet(\"./test.parquet\")\n\nNÃ¥ har vi en fil som heter test.parquet i mappen vi stÃ¥r. Da kan vi Ã¥pne DataDoc-grensesnittet for Ã¥ legge inn metadataene:\n\n\nnotebook\n\nmain(\"./test.parquet\")\n\nFigurÂ 1 viser hvordan DataDoc-grensesnittet ser ut.\n\n\n\n\n\n\nFigurÂ 1: Gif som viser hvordan DataDoc-grensesnittet ser ut.",
    "crumbs": [
      "Statistikere",
      "Metadata"
    ]
  },
  {
    "objectID": "statistikkere/datadoc.html#hvor-lagres-datadoc-dokumentasjonen-metadata",
    "href": "statistikkere/datadoc.html#hvor-lagres-datadoc-dokumentasjonen-metadata",
    "title": "DataDoc",
    "section": "",
    "text": "NÃ¥r du trykker pÃ¥ Lagre-knappen i DataDoc sÃ¥ skrives alle metadata til en fil i samme mappe (katalog) som datafilen. Dette er en JSON-fil med nesten samme navn som datafilen. Navnekonvensjonen for metadatafilen er\n&lt;navn pÃ¥ datafil uten endelse&gt;__DOC.json\nEksempelvis hvis datafilen har navnet skattedata_p2022_v1.parquet, sÃ¥ vil DataDoc lagre metadata i filen skattedata_p2022_v1__DOC.json.\nFordelen med Ã¥ benytte en JSON-fil til Ã¥ lagre metadata er at denne filen kan kopieres og flyttes like enkelt som selve datafilen. JSON-filer er strukturerte tekstfiler som kan leses av bÃ¥de maskiner (Python/R) og av mennesker (Ã¥pnes i en tekst-editor).\nSe et eksempel pÃ¥ JSON metadata-fil lagret av DataDoc.\n\n\n\n\n\n\nInformasjon\n\n\n\nI Dapla skal det bygges en felles datakatalog for SSB. Tanken er at alle metadata, eksempelvis datasett-dokumentasjon fra DataDoc (JSON-filene), skal inngÃ¥ i SSBs datakatalog. Datakatalogen gjÃ¸r det mulig Ã¥ finne (sÃ¸ke etter), forstÃ¥r og gjenbruke data bÃ¥de internt og ekstern.",
    "crumbs": [
      "Statistikere",
      "Metadata"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard-datalagring.html",
    "href": "statistikkere/navnestandard-datalagring.html",
    "title": "Navnestandard datalagring",
    "section": "",
    "text": "Krav til dokumentasjon av datasett\n\n\n\nI SSB er det et krav at datasett i datatilstandene1 (inndata), klargjorte data, statistikk og utdata dokumenteres. Det er spesielt viktig at klargjorte data, som er grunnlag bÃ¥de for statistikk, arkiv og datadeling, dokumenteres godt. Dokumentasjon er nÃ¸dvendig for at vi skal finne, forstÃ¥ og (gjen)bruke SSBs data. De fysiske datasettene og variablene som inngÃ¥r i disse skal dokumenteres med bruk av metadatalÃ¸sningene DataDoc (datasettdokumentasjon), VarDef 2 (variabeldefinisjoner) og KLASS (kodelister og klassifikasjoner).\nSe ogsÃ¥ detaljerte krav til datasett-metadata i dokumentene DataDoc - Krav til dokumentasjon av datasett og for variablene som inngÃ¥r i datasettet DataDoc - Variabelforekomst\nEn viktig del av datadokumentasjonsarbeidet er hvordan datasett versjoneres og organiseres i en standardisert mappestruktur (katalogstruktur). Dette dokumentet beskriver en navnestandard for mappestruktur og datasett (datafilene), i tillegg til regler for versjonering av datasett (datafilene).\n1 Datatilstanden â€œkildedataâ€ omfattes ikke av denne navnestandarden i og med at kildedata mottas av SSB i mange former/strukturer, og at kildedata i liten grad skal deles internt og eksternt (begrenset tilgang). Det mÃ¥ eventuelt vurderes om det skal utarbeides egne retningslinjer for lagring av kildedata. Datatilstanden â€œinndataâ€ er nevnt i parentes i og med at denne datatilstanden ikke er obligatorisk.\n2 Utvikling av nye VarDef er i skrivende stund ikke pÃ¥begynt. SSBs eksisterende variabeldefinisjonslÃ¸sning Vardok skal derfor benyttes fram til ny lÃ¸sning er klar.\n\n\n\n\n\n\n\n\nKrav til lagringsformater for datasett\n\n\n\nDokumentet â€œStandardformater for datalagring i SSBâ€ beskriver krav til filformater og dataformater ved lagring av klargjorte data som skal benyttes til utarbeiding av statistikk, datadeling og arkivering.",
    "crumbs": [
      "Statistikere",
      "Standarder",
      "Navnestandard datalagring"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard-datalagring.html#mapper-kataloger",
    "href": "statistikkere/navnestandard-datalagring.html#mapper-kataloger",
    "title": "Navnestandard datalagring",
    "section": "Mapper (kataloger)",
    "text": "Mapper (kataloger)\n\nStandard lagringsomrÃ¥der (bÃ¸tter) som opprettes for alle statistikkprodukt-team i SSB\nFÃ¸lgende lagringsomrÃ¥der (bÃ¸tter) operettes for alle team:\n\nssb-prod-&lt;teamnavn&gt;-data-kilde : Inneholder ubehandlede rÃ¥data fra datakildene.\nssb-prod-&lt;teamnavn&gt;-data-produkt : Inneholder data knyttet til statistikkproduktet.\nssb-prod-&lt;teamnavn&gt;-data-delt : Inneholder data knyttet til statistikkproduktet som kan deles med andre statistikkteam.\n\nDenne navnestandarden gjelder primÃ¦rt for lagringsomrÃ¥dene data-produkt og data-delt, men er ogsÃ¥ anbefalt brukt for data-kilde*.\n\n\nDatatilstander, prosesser, permanente data og temporÃ¦re data\nI en statistikkproduksjon skal det for hver datatilstand lagres permanente datasett (datafiler). Disse datasettene skal fÃ¸lge denne navnestandarden inkludert kravet til versjonering og dokumentasjon (metadata). Det er imidlertid viktig Ã¥ skille mellom behovet for permanente data og temporÃ¦re data. I prosessene som kjÃ¸res mellom hver datatilstand, f.eks. klargjÃ¸ringsprosessene mellom inndata og klargjorte data, vil det vÃ¦re behov for temporÃ¦r datalagring. TemporÃ¦re data skal aldri deles, og det stilles derfor ingen krav til verken navnestandard, versjonering eller dokumentasjon (metadata) av disse. Det er helt opp til hvert produkt-team hvordan de vil organisere temporÃ¦re data. Ved Ã¥ skille mellom permanent datalagring og temporÃ¦re datalagring oppnÃ¥r vi en optimal lÃ¸sning bÃ¥de for dataprodusenter (statistikkseksjonene) og data-konsumentene (interne og eksterne brukere). Produsentene fÃ¥r nÃ¸dvendig fleksibilitet til Ã¥ prosessere data i temporÃ¦re omrÃ¥der, mens konsumentene fÃ¥r godt dokumenterte og versjonerte data i en standardisert mappe-struktur og tilgjengelig for gjenfinning i en sÃ¸kbar datakatalog.\n\n\n\nFigur 1: Datatilstander, permanente og temporÃ¦re data\n\n\n\n\nStatistikkprodukter og dataprodukter\n\nStatistikkprodukter\nAlle SSBs tidligere og nÃ¥vÃ¦rende statistikkprodukter inngÃ¥r Statistikkregisteret. FÃ¸r publisering pÃ¥ ssb.no mÃ¥ alle statistikkprodukter vÃ¦re registrert i Statistikkregisteret med informasjon om bl.a. statistikkens navn, emne-omrÃ¥de, eierseksjon og publiseringstidspunkt. I tillegg fÃ¥r statistikkene tildelt et kortnavn. Eksempler pÃ¥ statistikk-kortnavn er:\n\n\"kpi\" for konsumprisindeksen\n\"reise\" for reiseundersÃ¸kelsen\n\"ftot\" for nÃ¦ringslivstjenester, omsetning etter tjenestetype\n\nKortnavnene er unike og stabilt over tid (uforanderlige). De er derfor valgt som grunnlag for kategorisering/inndeling av datasett i Dapla, dvs. benyttes som grunnlag for navn pÃ¥ mappene i lagringsomrÃ¥dene (bÃ¸ttene).\nStatistikkregisteret har ogsÃ¥ et API for Ã¥ hente informasjon om alle SSBs statistikker i json- format.\n\n\nDataprodukter\nDet er imidlertid ikke slik at alle data i SSB kan knyttes direkte til en statistikk i Statistikkregisteret. Flere statistikkseksjoner i SSB bearbeider ogsÃ¥ data til andre bruksomrÃ¥der og formÃ¥l, eksempelvis klargjÃ¸ring av data til forskning og utlÃ¥n, bearbeiding av data som skal inngÃ¥ som en del av andre statistikker, og data som inngÃ¥r i populasjonsregistre. Denne typen data omtales i dette dokumentet som â€œdataprodukterâ€, og i navnestandarden skiller vi mellom â€œdataprodukterâ€ og â€œstatistikkprodukterâ€. Det eksisterer ikke et register med â€œkortnavnâ€ for data-produktene i SSB, men hvert team mÃ¥ lage kortnavn ogsÃ¥ for dataproduktene. Eksempler pÃ¥ dataprodukt-kortnavn er â€œnudbâ€ (utdanningsdatabasen) og â€œfd_trygdâ€ (forlÃ¸psdatabasen for trygdedata).\n\n\n\n\n\n\nâ€œ_dataâ€ - navnekonvensjon for dataprodukter\n\n\n\nFor Ã¥ skille mellom dataprodukter og statistikkprodukter skal navnet pÃ¥ alle mapper som representerer dataprodukter ha endelsen â€œ_dataâ€, f.eks. â€œnudb_dataâ€ og â€œfd_trygd_dataâ€.\n\n\n\n\n\nMappestruktur for organisering av datasett i Dapla-teamenes lagringsomrÃ¥der (bÃ¸tter)\nMed utgangspunkt i standard bÃ¸tter som opprettes for alle statistikk-team i DAPLA, Statistikkregisteret og datatilstander, er det utarbeidet fÃ¸lgende regler for mappestrukturen og navngiving av mappene i bÃ¸ttene (gjelder for data-produkt-bÃ¸tte og data-delt-bÃ¸tte, men anbefalt ogsÃ¥ for data-kilde- bÃ¸tte) :\nssb-prod-&lt;team-name&gt;-data-produkt/  \nâ””â”€ &lt;statistikk-kortnavn&gt; | &lt;dataprodukt&gt;_data/  \n   â””â”€â”€ &lt;datatilstand&gt;/  \n       â”œâ”€â”€ [datasett-1]  \n       â”œâ”€â”€ [datasett-2]  \n       â””â”€â”€ [datasett-NN]\n\n\n\n\n\n\nForklaring av mappe-nivÃ¥er i navenstandarden\n\n\n\n\nFÃ¸rste nivÃ¥ er lagringsomrÃ¥det (bÃ¸tte)\nAndre nivÃ¥ er:\n-&gt; enten kortnavn fra Statistikkregisteret\n-&gt; eller et dataprodukt-navn (â€œkortnavnâ€)\nTredje nivÃ¥ er datatilstand\nFjerde nivÃ¥ er datasett (datafiler)\n\n\n\n\nStÃ¸tte for â€œegendefinerte under-mapperâ€ ved behov for organisering av datasett i flere nivÃ¥er\nVed behov er det tillatt Ã¥ utvide mappestrukturen med med flere egendefinerte nivÃ¥er (nivÃ¥ 4, 5, 6, .., N). Dette kan vÃ¦re nyttig for team som har veldig mange datasett og har behov for Ã¥ gruppere disse i flere undermapper:\nssb-prod-&lt;team-name&gt;-data-produkt/  \nâ””â”€ &lt;statistikk-kortnavn&gt; | &lt;dataprodukt&gt;_data/  \n   â””â”€â”€ &lt;datatilstand&gt;/  \n       â””â”€â”€ &lt;egen under-mappe&gt;/  \n           â””â”€â”€ &lt;egen under-under-mappe&gt;/  \n               â””â”€â”€ &lt;.. osv.&gt;/  \n                   â”œâ”€â”€ [datasett-1]  \n                   â”œâ”€â”€ [datasett-2]  \n                   â””â”€â”€ [datasett-NN]\n\n\n\n\n\n\nEventuell bruk av egendefinerte nivÃ¥er i mappestrukturen\n\n\n\n\nFÃ¸rste nivÃ¥ er lagringsomrÃ¥det (bÃ¸tte)\nAndre nivÃ¥ er:\n-&gt; enten kortnavn fra Statistikkregisteret\n-&gt; eller et dataprodukt-navn (â€œkortnavnâ€)\nTredje nivÃ¥ er datatilstand\nFjerde nivÃ¥ er datasett (datafiler)\nNivÃ¥ 4, 5, osv. er egendefinerte under-mapper\nNederste nivÃ¥ er datasett (datafiler)\n\n\n\n\n\nStÃ¸tte for temporÃ¦re data (temp-mappe) og oppdragsdata (oppdrag-mappe)\nVed behov for lagring av temporÃ¦re data (tilsvarende wk-katalogene pÃ¥ Linux pÃ¥ bakken) er det stÃ¸tte for Ã¥ opprette en temp -mappe. TemporÃ¦re data er kun tillatt i data-produkt-bÃ¸tten, bÃ¸r fjernes etter en viss tid, og skal ikke deles med andre (kun tilgjengelige innenfor eget team).\nDet er ogsÃ¥ anbefalt Ã¥ opprette en oppdrag -mappe for team som jobber med oppdragsvirksomhet. Egne regler gjelder for behandling og oppbevaring av oppdragsdata. Det er derfor Ã¸nskelig at disse organiseres i en egen mappe. Utover dette er det anbefalt Ã¥ ha med WebSak-saksnummer til oppdraget enten som en undermappe eller som en del av datasett-navnet.\nssb-prod-&lt;team-name&gt;-data-produkt/  \nâ””â”€ &lt;statistikk-kortnavn&gt; | &lt;dataprodukt&gt;_data/  \n   â””â”€â”€ &lt;datatilstand&gt;/  \n   â””â”€â”€ &lt;datatilstand&gt;/  \n       â”œâ”€â”€ [datasett-1]  \n       â”œâ”€â”€ [datasett-2]  \n       â””â”€â”€ [datasett-NN]\nâ””â”€ temp/  \n   â”œâ”€â”€ [temp-datasett-A]  \n   â””â”€â”€ [temp-datasett-X]  \nâ””â”€ oppdrag/  \n   â””â”€â”€ &lt;WebSak-saksnummer&gt;/  \n       â”œâ”€â”€ [oppdrag-datasett-Y]  \n       â””â”€â”€ [oppdrag-datasett-Z]\n\n\n\n\n\n\nBruk av temp-mappe og oppdrag-mappe\n\n\n\n\nFÃ¸rste nivÃ¥ er lagringsomrÃ¥det (bÃ¸tte)\nAndre nivÃ¥ er:\n-&gt; enten kortnavn fra Statistikkregisteret\n-&gt; eller et dataprodukt-navn (â€œkortnavnâ€)\nTredje nivÃ¥ er datatilstand\nFjerde nivÃ¥ er datasett (datafiler)\n\nHer vises ogsÃ¥:\n\ntemp-mappe for temporÃ¦re data\noppdrag-mappe for oppdragsdata\n\n\n\n\n\n\nEksempel pÃ¥ mappestruktur i data-produkt-bÃ¸tte\nEksempel â€œTeam overnaturligâ€\nNedenfor vises et eksempel pÃ¥ hvordan et tenkt team â€œ Team overnaturlig â€ kan organisere sine tenkte statistikkprodukter â€œufoâ€ og â€œsuperheltâ€ i en mappestruktur:\nssb-prod-team-overnaturlig-data-produkt/  \nâ””â”€â”€ ufo/  \n    â”œâ”€â”€ inndata/  \n    â”œâ”€â”€ klargjorte-data/  \n    â”œâ”€â”€ statistikk/  \n    â””â”€â”€ utdata/  \nâ””â”€â”€ superhelt/  \n    â”œâ”€â”€ inndata/  \n    â”œâ”€â”€ klargjorte-data/  \n    â”œâ”€â”€ statistikk/  \n    â””â”€â”€ utdata/  \nâ””â”€â”€ temp/  \nâ””â”€â”€ oppdrag/  \nEksempel â€œTeam reiselivâ€ - Seksjon for nÃ¦ringslivets konjunkturer (S422)\nTeamet har ansvar for 3 statistikk-produkter (kortnavn â€œovernattingâ€, â€œreiseâ€ og â€œgrensehandelâ€)\n\nEtt ALTINN-skjema og 2 utvalgsinnsamlinger med intervjuer pÃ¥ telefon/CATI\nProduksjonslÃ¸pet har fokus pÃ¥ statistikkprodukter fra kildedata til utdata\n\nssb-prod-reiseliv-data-produkt/  \nâ””â”€â”€ overnatting/  \n     â”œâ”€â”€ inndata/  \n     â”œâ”€â”€ klargjorte-data/  \n     â”œâ”€â”€ statistikk/  \n     â””â”€â”€ utdata/  \nâ””â”€â”€ reise/  \n     â”œâ”€â”€ inndata/  \n     â”œâ”€â”€ klargjorte-data/  \n     â”œâ”€â”€ statistikk/  \n     â””â”€â”€ utdata/  \nâ””â”€â”€ grensehandel/  \n     â”œâ”€â”€ inndata/  \n     â”œâ”€â”€ klargjorte-data/  \n     â”œâ”€â”€ statistikk/  \n     â””â”€â”€ utdata/  \nâ””â”€â”€ temp/\nEksempel â€œTeam trygdâ€ - Seksjon for inntekts- og levekÃ¥rsstatistikk (S350)\nTeamet har datainnsamling fra flere av NAV sine register\n\nData klargjÃ¸res og brukes til flere formÃ¥l, bl.a. utlÃ¥n av data til forskere (FD-Trygd) og levering til microdata.no (S380)\n\n\n\n\n\n\n\nEksempel pÃ¥ dataprodukt\n\n\n\nTeam trygd klargjÃ¸r dataprodukter, ikke statistikkprodukter. Alle dataprodukt-kortnavn har derfor endelsen â€œ_dataâ€ i eksempel-mappestrukturen nedenfor.\n\n\nssb-prod-trygd-data-produkt/  \nâ””â”€â”€ barnetrygd_data/  \n    â”œâ”€â”€ inndata/  \n    â”œâ”€â”€ klargjorte-data/  \n    â”œâ”€â”€ statistikk/  \n    â””â”€â”€ utdata/  \nâ””â”€â”€ foedsykp_data/  \n    â”œâ”€â”€ inndata/  \n    â”œâ”€â”€ klargjorte-data/  \n    â”œâ”€â”€ statistikk/  \n    â””â”€â”€ utdata/  \nâ””â”€â”€ pensj_data/  \n    â”œâ”€â”€ inndata/  \n    â”œâ”€â”€ klargjorte-data/  \n    â”œâ”€â”€ statistikk/  \n    â””â”€â”€ utdata/  \nâ””â”€â”€` â€¦ osv.  \nâ””â”€â”€ temp/  \nâ””â”€â”€ oppdrag/",
    "crumbs": [
      "Statistikere",
      "Standarder",
      "Navnestandard datalagring"
    ]
  },
  {
    "objectID": "statistikkere/navnestandard-datalagring.html#filnavn-for-datasett",
    "href": "statistikkere/navnestandard-datalagring.html#filnavn-for-datasett",
    "title": "Navnestandard datalagring",
    "section": "Filnavn for datasett",
    "text": "Filnavn for datasett\nFilnavnet til datasettet skal bygges opp av fÃ¸lgende elementer:\n\n\n\n\nElement\nForklaring\n\n\n\n\n1\nKort beskrivelse\nKort tekst som forklarer datasettets innhold, f.eks. â€œvarehandel, â€œpersoninntektâ€, â€œgrensehandel_imputertâ€œ eller â€œframskrevne-befolkningsendringerâ€œ\n\n\n2\nPeriode - inneholder data f.o.m. dato\nDatasettet inneholder data fra og med dato/tidspunkt. I filnavnet mÃ¥ perioden prefikses med â€œ_pâ€, eksempel â€œ_p2022â€ eller â€œ_p2022-01-01â€. â€œ_pâ€ er en forkortelse for â€œperiodeâ€. Se ogsÃ¥ gyldige formater for periode (dato/tidspunkt) \n\n\n3\nPeriode - inneholder data t.o.m. dato\nDatasettet inneholder data til og med dato/tidspunkt. Denne brukes ved behov, eksempelvis for datasett som inneholder forlÃ¸psdata eller datasett med flere perioder/Ã¥rganger.\n\n\n4\nVersjon\nVersjon av datasettet. I filnavnet mÃ¥ versjonsnummeret prefikses med â€œ_v, eksempel â€œv1â€, â€œv2â€ eller â€œv3â€. Se ogsÃ¥ eget kapittel om regler for versjonering av datasett.\n\n\n5\nFiltype\nFilendelse som sier noen om filtypen, f.eks. â€œ.jsonâ€, â€œ.csvâ€, â€œ.xmlâ€ eller â€œ.parquetâ€.\n\n\n\n\nFormat for filnavn (datasettnavn)\nFilnavnet skal bygges opp pÃ¥ fÃ¸lgende mÃ¥te:\n&lt;kort-beskrivelse&gt;_p&lt;periode-fra-og-med&gt;_p&lt;perode-til-og- med&gt;_v&lt;versjon&gt;.&lt;filtype&gt;\n\nNoen eksempler pÃ¥ gyldige filnavn :\nflygende_objekter_p2019_v1.parquet (inneholder en Ã¥rgang med data)\nufo_observasjoner_p2019_p2020_v1.parquet (inneholder 2 Ã¥rganger med data)\nframskrevne-befolkningsendringer_p2019_p2050_v1.parquet (inneholder data fra 2019 til 2050)\nsykepenger_p2022-01-01_p2022-12-31_v1.parquet (inneholder data fra 01.01.2022 til 31.12.2022)\nutanningsnivaa_p2022-10-01_v1.parquet (inneholder tverrsnittsdata (status) per 01.10.2022)\ngrensehandel_imputert_p2022-10_p2022-12_v1.parquet (inneholder data for okt., nov. og des. 2022)\nomsetning_p2020W15_v1.parquet (inneholder data for uke-nummer 15 (week))\nskipsanloep_p2022B1_v1.parquet (inneholder data for fÃ¸rste 2 mÃ¥neders-periode i 2022 (bimester))\npensjon_p2018Q1_v1.parquet (inneholder data for fÃ¸rste kvartal (3-mÃ¥neders-periode) i 2018 (quarter))\nnybilreg_p2022T1_v1.parquet (inneholder data for fÃ¸rste tertial (4 mÃ¥neders-periode) i 2022)\npersoninntekt_p2022H1_v1.parquet (inneholder data for fÃ¸rste halvÃ¥r (6-mÃ¥neders-periode) i 2022)\nvarehandel_p2018Q1_p2018Q4_v1.parquet (inneholder data for kvartalene 1, 2,3 og 4 i 2018)\n\n\nEksempel pÃ¥ datasett-filer i en mappestruktur\nNedenfor vises et eksempel pÃ¥ hvordan â€œ Team overnaturlig â€ har organisert sine datasett-filer i en mappe-struktur for sin â€œ ufo-statistikk â€:\nssb-prod-team-overnaturlig-data-produkt/  \nâ””â”€â”€ ufo/  \n    â””â”€â”€ inndata/  \n         â”œâ”€â”€ lysfenomen_p2019_v1.parquet  \n         â”œâ”€â”€ lysfenomen_p2020_v1.parquet  \n         â”œâ”€â”€ flygende_objekter_p2019_v1.parquet  \n         â””â”€â”€ flygende_objekter_p2020_v1.parquet  \n    â””â”€â”€ klargjorte-data/  \n         â”œâ”€â”€ ufo_observasjoner_samlet_p2019_v1.parquet  \n         â””â”€â”€ ufo_observasjoner_samlet_p2020_v1.parquet  \n    â””â”€â”€ statistikk/  \n         â””â”€â”€ ufo_statistikk_p2019_p2020_v1.parquet  \n    â””â”€â”€ utdata/  \n         â”œâ”€â”€ ufo_statistikk_fylke_p2019_p2020_v1.csv  \n         â””â”€â”€ ufo_statistikk_landet_p2019_p2020_v1.csv  \nâ””â”€â”€ â€¦ osv.\n\n\n\nTillatte tegn for bruk i filnavn og mappe-navn\nDet er kun tillatt Ã¥ bruke alfanumerisk tegn begrenset til:\n\na-z og A-Z\n0-9\n- (bindestrek)\n_ (understrek)\n\n\n\n\n\n\n\nAndre krav til filnavn og mappe-navn\n\n\n\nIkke bruk bokstavene â€œÃ¦â€, â€œÃ¸â€ og â€œÃ¥â€ i filnavn eller i mappe-navn.\nAnbefalingen er at disse erstattes med â€œaeâ€, â€œoeâ€ og â€œaaâ€.\n\nEksempel: â€œnaeringâ€, â€œoekonomiâ€ og â€œlevekaarâ€\n\nMellomrom/ordskiller (space) erstattes med bindestrek (â€œ-â€) eller understrek (â€œ_â€).\n\nEksempel: â€œskatt_for_personerâ€ og â€œvann-og-avloepâ€\n\nPunktum (â€œ.â€) er kun tillatt brukt for Ã¥ skille filnavnet fra filendelsen (filtypen).\n\nEksempel: â€œpersondata.parquetâ€ og â€œpersondata.csvâ€\n\nIngen andre spesialtegn er tillatt brukt i filnavn eller mappe-navn.\n\n\n\n\nâ€œDatapartisjoneringâ€ - alternativ organisering av datasett med flere mapper og filer\nDatatjenester/program-bibliotek som PySpark, PyArrow, Pandas og Dask har funksjonalitet for datapartisjonering. Dette er en teknikk som benyttes for Ã¥ splitte opp veldig store datasett til flere smÃ¥ datasett og deretter plassere disse filene i en mappe-struktur. En av fordelene med dette er at konsumenter (brukere) kan jobbe med mindre deler av store datasett. En vanlig praksis er Ã¥ dele opp (partisjonere) et stort datasett med mange Ã¥rganger/perioder i flere smÃ¥ Ã¥rgangsfiler. Da vil navnet pÃ¥ root-mappen tilsvare navnet pÃ¥ filen (hvis vi kun hadde Ã©n stor fil) , og underkatalogene vil vÃ¦re periodeinndeling, f.eks. â€œ/aargang2019â€ og â€œ/aargang2020â€. Det er ogsÃ¥ mulig Ã¥ â€œpartisjonereâ€ data pÃ¥ andre mÃ¥ter for Ã¥ stÃ¸tte parallell-prosessering av store datasett i f.eks. Spark.\n\nEksempel pÃ¥ partisjonering av stort datasett\nâ€œTeam overnaturligâ€ har et stort datasett med mange Ã¥rganger med observasjoner av â€œflygende objekterâ€.\nssb-prod-team-overnaturlig-data-produkt/  \nâ””â”€â”€ ufo/  \n    â””â”€â”€ inndata/  \n        â””â”€â”€ flygende_objekter_p1980_p2020_v1.parquet\nTeamet Ã¸nsker nÃ¥ Ã¥ â€œpartisjonereâ€ denne store datafilen i flere smÃ¥ Ã¥rgangsfiler (med filtype parquet).\nDet finnes flere bibliotek og verktÃ¸y som stÃ¸tter partisjonering av store datafiler, eksempelvis Pandas hvor det er mulig Ã¥ partisjonere en dataframe pÃ¥ fÃ¸lgende mÃ¥te nÃ¥r den skrives til en parquet-fil(er):\ndf.to_parquet('./flygende_objekter_p1980_p2020_v1', partition_cols=['aar'])\nSe mer informasjon om datapartisjonering med Pandas: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html?highlight=example\nNedenfor vises et eksempel pÃ¥ hvordan en mappestruktur med datasett-filer kan se ut etter utfÃ¸rt data-partisjonering.\nssb-prod-team-overnaturlig-data-produkt/  \nâ””â”€â”€ ufo/  \n    â””â”€â”€ inndata/  \n        â””â”€â”€ flygende_objekter_p1980_p2020_v1/  \n            â””â”€â”€ aar=1980/  \n                â””â”€â”€ data.parquet  \n            â””â”€â”€ aar=1981/  \n                â””â”€â”€ data.parquet  \n            â””â”€â”€ aar=1982/  \n                â””â”€â”€ data.parquet  \n            â€¦ osv.  \n            â””â”€â”€ aar=2020/  \n                â””â”€â”€ data.parquet",
    "crumbs": [
      "Statistikere",
      "Standarder",
      "Navnestandard datalagring"
    ]
  },
  {
    "objectID": "statistikkere/pakke-install-bakken.html",
    "href": "statistikkere/pakke-install-bakken.html",
    "title": "Installere pakker",
    "section": "",
    "text": "Installering av pakker i Jupyter miljÃ¸er pÃ¥ bakken (f.eks https://sl-jupyter-p.ssb.no) foregÃ¥r stort sett helt lik som pÃ¥ Dapla. Det er Ã©n viktig forskjell, og det er at installasjon skjer via en proxy som heter Nexus.\n\n\nPip er ferdig konfigurert for bruk av Nexus og kan kjÃ¸res som beskrevet for Dapla\n\n\n\nHvis man bruker Poetry for hÃ¥ndtering av pakker i et prosjekt, sÃ¥ mÃ¥ man kjÃ¸re fÃ¸lgende kommando i prosjekt-mappe etter prosjektet er opprettet.\n\n\nterminal\n\npoetry source add --default nexus `echo $PIP_INDEX_URL`\n\nDa fÃ¥r man installere pakker som vanlig f.eks\n\n\nterminal\n\npoetry add matplotlib\n\n\n\n\n\n\n\nHvis man forsÃ¸ker Ã¥ installere prosjektet i et annet miljÃ¸ (f.eks Dapla), sÃ¥ mÃ¥ man fÃ¸rst fjerne nexus som kilde ved Ã¥ kjÃ¸re:\n\n\nterminal\n\npoetry source remove nexus\n\n\n\n\n\n\n\n\nProsessen med Ã¥ installere pakker for R pÃ¥ bakken er det samme som pÃ¥ Dapla. Noen pakker (for eksempel devtools) kan forelÃ¸pig ikke installeres pÃ¥ bakken pÃ¥ egenhÃ¥nd pga 3. parti avhengigheter. Vi jobber med Ã¥ finne en lÃ¸sning til dette.\nFor Ã¥ installere arrow, kopier og kjÃ¸r fÃ¸lgende kommando i R:\n\n\nnotebook\n\ninstall.packages(\"/ssb/bruker/felles/R_pakker/arrow_11.0.0.2.tar.gz\",\n                repos = NULL,\n                type = \"source\")"
  },
  {
    "objectID": "statistikkere/pakke-install-bakken.html#python",
    "href": "statistikkere/pakke-install-bakken.html#python",
    "title": "Installere pakker",
    "section": "",
    "text": "Installering av pakker i Jupyter miljÃ¸er pÃ¥ bakken (f.eks https://sl-jupyter-p.ssb.no) foregÃ¥r stort sett helt lik som pÃ¥ Dapla. Det er Ã©n viktig forskjell, og det er at installasjon skjer via en proxy som heter Nexus.\n\n\nPip er ferdig konfigurert for bruk av Nexus og kan kjÃ¸res som beskrevet for Dapla\n\n\n\nHvis man bruker Poetry for hÃ¥ndtering av pakker i et prosjekt, sÃ¥ mÃ¥ man kjÃ¸re fÃ¸lgende kommando i prosjekt-mappe etter prosjektet er opprettet.\n\n\nterminal\n\npoetry source add --default nexus `echo $PIP_INDEX_URL`\n\nDa fÃ¥r man installere pakker som vanlig f.eks\n\n\nterminal\n\npoetry add matplotlib\n\n\n\n\n\n\n\nHvis man forsÃ¸ker Ã¥ installere prosjektet i et annet miljÃ¸ (f.eks Dapla), sÃ¥ mÃ¥ man fÃ¸rst fjerne nexus som kilde ved Ã¥ kjÃ¸re:\n\n\nterminal\n\npoetry source remove nexus"
  },
  {
    "objectID": "statistikkere/pakke-install-bakken.html#r",
    "href": "statistikkere/pakke-install-bakken.html#r",
    "title": "Installere pakker",
    "section": "",
    "text": "Prosessen med Ã¥ installere pakker for R pÃ¥ bakken er det samme som pÃ¥ Dapla. Noen pakker (for eksempel devtools) kan forelÃ¸pig ikke installeres pÃ¥ bakken pÃ¥ egenhÃ¥nd pga 3. parti avhengigheter. Vi jobber med Ã¥ finne en lÃ¸sning til dette.\nFor Ã¥ installere arrow, kopier og kjÃ¸r fÃ¸lgende kommando i R:\n\n\nnotebook\n\ninstall.packages(\"/ssb/bruker/felles/R_pakker/arrow_11.0.0.2.tar.gz\",\n                repos = NULL,\n                type = \"source\")"
  },
  {
    "objectID": "statistikkere/dashboard.html",
    "href": "statistikkere/dashboard.html",
    "title": "Dash og dashboard",
    "section": "",
    "text": "Hvis en Ã¸nsker Ã¥ lage ett dashbord som et brukergrensesnitt, sÃ¥ kan pakken Dash vÃ¦re et godt alternativ. Dash er ett rammeverk hvor man selv kan bygge opp applikasjoner i form av dashbord pÃ¥ en enklere mÃ¥te, og det bygges oppÃ¥ javascript pakker som plotly.js og react.js. Det er et produkt ved siden av og helintegrert med plotly, som ogsÃ¥ er en annen pakke i Python som gir oss interaktive grafer. Dash er et godt verktÃ¸y hvis en Ã¸nsker et dashbord som brukergrensesnitt for interaktiv visualisering av data. Dash kan kodes i Python og R, men ogsÃ¥ Julia og F#.\n\n\nI SSB kan man lage dashbord i virtuelle miljÃ¸er, gjerne satt opp med ssb-project, men man trenger helt spesifikke pakker for Ã¥ fÃ¥ det oppe Ã¥ gÃ¥. Mer info om Ã¥ sette opp et eget miljÃ¸ med ssb-project finner du her. Tabell under viser navn pÃ¥ pakkene, og gjeldende versjoner som er benyttet i skrivende stund av eksempel i neste avsnitt. Vel og merke kan nyere versjoner ogsÃ¥ fungere fint, noe man mÃ¥ prÃ¸ve ut selv, men fÃ¸lgende versjoner fungerer fint.\n\n\n\nPakke\nVersjon (i skrivende stund)\n\n\n\n\ndash\n2.8.1\n\n\njupyter-dash\n0.4.2\n\n\njupyter-server-proxy\n3.2.2\n\n\njupyterlab-dash\n0.1.0a3\n\n\nipykernel\n6.21.3\n\n\ndash-bootstrap-components\n1.3.0 (hvis Ã¸nskelig)\n\n\n\nFor mer om hÃ¥ndtering av pakker i ett virtuelt miljÃ¸ satt opp med ssb-project kan man se nÃ¦rmere her. For Ã¥ legge til disse pakkene kan man gjÃ¸re fÃ¸lgende i terminalen:\n\n\nterminal\n\npoetry add dash jupyter-dash jupyter-server-proxy jupyterlab-dash ipykernel\n\nOg hvis en Ã¸nsker Dash-Bootstrap-Components:\n\n\nterminal\n\npoetry add dash-bootstrap-components\n\nVel og merke sÃ¥ vil ikke denne pakken fungere uten at tilhÃ¸rende filer er med i selve repoet. Dette fordi i hovedsak leter denne etter filer den trenger pÃ¥ internett. Pakken i seg selv har en fordel i at det er lettere Ã¥ bygge opp utseende(layout) i dashbordet ditt, samt andre komponenter som ikke ligger i standard dash pakken.\n\n\n\nNoen ting er viktig Ã¥ huske pÃ¥ at kommer i korrekt rekkefÃ¸lge nÃ¥r en koder opp i JupyterLab. Her kommer ett veldig enkelt eksempel.\nFÃ¸rste celle importerer vi alle nÃ¸dvendige pakker\n\n\nnotebook\n\nfrom dash import html\nfrom jupyter_dash import JupyterDash\nfrom jupyter_dash.comms import _send_jupyter_config_comm_request\n_send_jupyter_config_comm_request()\n\nI Andre celle mÃ¥ fÃ¸lgende kjÃ¸res, men her er det VELDIG VIKTIG at man avventer 2-3 sekunder etter at forrige celle ble kjÃ¸rt.\n\n\nnotebook\n\nJupyterDash.infer_jupyter_proxy_config()\n\nDeretter sÃ¥ er vi klare for Ã¥ bygge opp selve dashbordet. sÃ¥ i Tredje celle kan en enkel kode for eksempel se slik ut:\n\n\nnotebook\n\napp = JupyterDash(__name__)\nporten = 8642 # Valgfritt fire sifret nummer\n\napp.layout = html.Div([\n    html.H1(\"Eget dashbord med Dash i SSB\")\n])\n\nif __name__ == \"__main__\":\n    app.run_server(debug=True, port=porten, mode=\"external\")\n\nDenne koden vil starte opp dashbordet i eget vindu i browseren ettersom mode=â€œexternalâ€. Dersom man vil ha et eget vindu i JupyterLab kan man sette denne til â€œjupyterlabâ€, eller hvis du vil at dashbordet skal dukke opp under cellen i notebooken din, sÃ¥ kan man sette denne til â€œinlineâ€.\n\n\n\nDiverse som er verdt Ã¥ se nÃ¦rmere pÃ¥ nÃ¥r en bygger dashbord applikasjon med Dash. Det fÃ¸lger med mange gode eksempler for bruk av diverse komponenter i dokumentasjonene under her.\n\nStartsiden til Dash\nLage interaktive grafer i Python med Plotly\nDash Core Components\nDash HTML Components\nDash Bootstrap Components\n\nVel og merke henter denne diverse materialer fra internett, og vil ikke fungere uten tilgang. Det er heller anbefalt Ã¥ ha de nÃ¸dvendige filene lagret lokalt for bruk av denne pakken.",
    "crumbs": [
      "Statistikere",
      "Appendix"
    ]
  },
  {
    "objectID": "statistikkere/dashboard.html#anbefalte-nÃ¸dvendige-pakker",
    "href": "statistikkere/dashboard.html#anbefalte-nÃ¸dvendige-pakker",
    "title": "Dash og dashboard",
    "section": "",
    "text": "I SSB kan man lage dashbord i virtuelle miljÃ¸er, gjerne satt opp med ssb-project, men man trenger helt spesifikke pakker for Ã¥ fÃ¥ det oppe Ã¥ gÃ¥. Mer info om Ã¥ sette opp et eget miljÃ¸ med ssb-project finner du her. Tabell under viser navn pÃ¥ pakkene, og gjeldende versjoner som er benyttet i skrivende stund av eksempel i neste avsnitt. Vel og merke kan nyere versjoner ogsÃ¥ fungere fint, noe man mÃ¥ prÃ¸ve ut selv, men fÃ¸lgende versjoner fungerer fint.\n\n\n\nPakke\nVersjon (i skrivende stund)\n\n\n\n\ndash\n2.8.1\n\n\njupyter-dash\n0.4.2\n\n\njupyter-server-proxy\n3.2.2\n\n\njupyterlab-dash\n0.1.0a3\n\n\nipykernel\n6.21.3\n\n\ndash-bootstrap-components\n1.3.0 (hvis Ã¸nskelig)\n\n\n\nFor mer om hÃ¥ndtering av pakker i ett virtuelt miljÃ¸ satt opp med ssb-project kan man se nÃ¦rmere her. For Ã¥ legge til disse pakkene kan man gjÃ¸re fÃ¸lgende i terminalen:\n\n\nterminal\n\npoetry add dash jupyter-dash jupyter-server-proxy jupyterlab-dash ipykernel\n\nOg hvis en Ã¸nsker Dash-Bootstrap-Components:\n\n\nterminal\n\npoetry add dash-bootstrap-components\n\nVel og merke sÃ¥ vil ikke denne pakken fungere uten at tilhÃ¸rende filer er med i selve repoet. Dette fordi i hovedsak leter denne etter filer den trenger pÃ¥ internett. Pakken i seg selv har en fordel i at det er lettere Ã¥ bygge opp utseende(layout) i dashbordet ditt, samt andre komponenter som ikke ligger i standard dash pakken.",
    "crumbs": [
      "Statistikere",
      "Appendix"
    ]
  },
  {
    "objectID": "statistikkere/dashboard.html#eksempel-kode-i-jupyterlab",
    "href": "statistikkere/dashboard.html#eksempel-kode-i-jupyterlab",
    "title": "Dash og dashboard",
    "section": "",
    "text": "Noen ting er viktig Ã¥ huske pÃ¥ at kommer i korrekt rekkefÃ¸lge nÃ¥r en koder opp i JupyterLab. Her kommer ett veldig enkelt eksempel.\nFÃ¸rste celle importerer vi alle nÃ¸dvendige pakker\n\n\nnotebook\n\nfrom dash import html\nfrom jupyter_dash import JupyterDash\nfrom jupyter_dash.comms import _send_jupyter_config_comm_request\n_send_jupyter_config_comm_request()\n\nI Andre celle mÃ¥ fÃ¸lgende kjÃ¸res, men her er det VELDIG VIKTIG at man avventer 2-3 sekunder etter at forrige celle ble kjÃ¸rt.\n\n\nnotebook\n\nJupyterDash.infer_jupyter_proxy_config()\n\nDeretter sÃ¥ er vi klare for Ã¥ bygge opp selve dashbordet. sÃ¥ i Tredje celle kan en enkel kode for eksempel se slik ut:\n\n\nnotebook\n\napp = JupyterDash(__name__)\nporten = 8642 # Valgfritt fire sifret nummer\n\napp.layout = html.Div([\n    html.H1(\"Eget dashbord med Dash i SSB\")\n])\n\nif __name__ == \"__main__\":\n    app.run_server(debug=True, port=porten, mode=\"external\")\n\nDenne koden vil starte opp dashbordet i eget vindu i browseren ettersom mode=â€œexternalâ€. Dersom man vil ha et eget vindu i JupyterLab kan man sette denne til â€œjupyterlabâ€, eller hvis du vil at dashbordet skal dukke opp under cellen i notebooken din, sÃ¥ kan man sette denne til â€œinlineâ€.",
    "crumbs": [
      "Statistikere",
      "Appendix"
    ]
  },
  {
    "objectID": "statistikkere/dashboard.html#aktuell-dokumentasjon",
    "href": "statistikkere/dashboard.html#aktuell-dokumentasjon",
    "title": "Dash og dashboard",
    "section": "",
    "text": "Diverse som er verdt Ã¥ se nÃ¦rmere pÃ¥ nÃ¥r en bygger dashbord applikasjon med Dash. Det fÃ¸lger med mange gode eksempler for bruk av diverse komponenter i dokumentasjonene under her.\n\nStartsiden til Dash\nLage interaktive grafer i Python med Plotly\nDash Core Components\nDash HTML Components\nDash Bootstrap Components\n\nVel og merke henter denne diverse materialer fra internett, og vil ikke fungere uten tilgang. Det er heller anbefalt Ã¥ ha de nÃ¸dvendige filene lagret lokalt for bruk av denne pakken.",
    "crumbs": [
      "Statistikere",
      "Appendix"
    ]
  },
  {
    "objectID": "statistikkere/produksjonslÃ¸p.html",
    "href": "statistikkere/produksjonslÃ¸p.html",
    "title": "ProduksjonslÃ¸p",
    "section": "",
    "text": "ProduksjonslÃ¸p\nI forrige del beskrev vi noen grunnleggende ting rundt Dapla. I denne delen tar vi for oss hvordan du kan begynne Ã¥ jobbe med skarpe data pÃ¥ plattformen.\nKapittelet som beskriver hvordan man logger seg inn pÃ¥ Dapla vil fungere uten at du mÃ¥ gjÃ¸re noen forberedelser. Er man koblet pÃ¥ SSB sitt nettverk sÃ¥ vil alle SSB-ansatte kunne gÃ¥ inn pÃ¥ plattformen og kode i Python og R. Men du fÃ¥r ikke tilgang til SSBs omrÃ¥de for datalagring pÃ¥ plattformen. I praksis vil det si at man kan generere data med kode, men man kan ikke jobbe med skarpe data.\nFor Ã¥ fÃ¥ muligheten til Ã¥ jobbe med skarpe data MÃ… du fÃ¸rst opprette et dapla-team. Dette er det fÃ¸rste naturlige steget Ã¥ ta nÃ¥r man skal begynne Ã¥ jobbe med statistikkproduksjon pÃ¥ Dapla. I dette kapittelet vil vi forklare det du trenger Ã¥ vite om det Ã¥ opprette og jobbe innenfor et team."
  },
  {
    "objectID": "statistikkere/versjonering-av-datasett.html",
    "href": "statistikkere/versjonering-av-datasett.html",
    "title": "Versjonering av datasett",
    "section": "",
    "text": "Versjonering av data(sett) er viktig for Ã¥ dekke kravet om uforanderlighet og etterprÃ¸vbarhet. Hovedpoenget med versjonering er at data-konsumenter (menneske eller maskin) skal ha kontroll pÃ¥ endringer, dvs. tilgang bÃ¥de til en stabil versjon (uforanderlighet), men ogsÃ¥ tilgang til eventuelle nye data- versjoner som oppstÃ¥r. Se mer om dette i SSBs produksjonsarkitektur og prinsippet om â€œuforandelighet av dataâ€\n\n\n\n\n\n\nNote\n\n\n\nDet finnes i dag ingen internasjonale standarder eller spesifikasjoner for hvordan endringer av data skal versjoneres. Dette pÃ¥pekes ogsÃ¥ av w3.org (https://www.w3.org/TR/dwbp/#dataVersioning) :\nDatasets published on the Web may change over time. Some datasets are updated on a scheduled basis, and other datasets are changed as improvements in collecting the data make updates worthwhile. In order to deal with these changes, new versions of a dataset may be created. Unfortunately, there is no consensus about when changes to a dataset should cause it to be considered a different dataset altogether rather than a new version. In the following, we present some scenarios where most publishers would agree that the revision should be considered a new version of the existing dataset.\n\nScenario 1: a new bus stop is created and it should be added to the dataset;\nScenario 2: an existing bus stop is removed and it should be deleted from the dataset;\nScenario 3: an error was identified in one of the existing bus stops stored in the dataset and this error must be corrected.\n\nAustralian National Data Service (ANDS) beskriver ogsÃ¥ i sitt dokument https://www.ands.org.au/working-with-data/data-management/data-versioning behovet for versjonering, men ogsÃ¥ utfordringene med Ã¥ implementere data- versjonering i praksis.\nErfaringer fra versjonering i microdata.no viser at data- versjoneringen i SSB bÃ¸r baseres pÃ¥ prinsippet om â€œbreaking changesâ€ ( major changes ) fra Semantic Versioning (SemVer), dvs. at alle endringer som gjÃ¸r at vi ikke kan gjenskape samme resultat vil medfÃ¸re at det opprettes en ny versjon av datasettet (en ny versjon i tillegg til gammel/forrige versjon av datasettet).\n\n\n\n\nFÃ¸lgende hendelser skaper en ny versjon av et datasett:\n\nReberegninger av data med nye metoder.\nKorrigeringer av verdier for eksisterende observasjoner/enheter i datasettet.\n\nSelv manuell endring av bare Ã©n data-verdi (celle) i et stort datasett skaper en ny versjon!\n\nLagt til nye og/eller fjernet observasjoner/enheter i datasettet.\nOmkodinger, dvs. oppdatert/erstattet kodeverk.\nLagt til nye variabler. Dette skaper en ny versjon i og med at dette kan pÃ¥virke prosesser (programkode).\n\nHvis det gjÃ¸res vesentlige endringer (mange nye variabler) sÃ¥ bÃ¸r det vurderes om dette er et annet (et helt nytt) datasett, ikke en ny versjon av et eksisterende datasett.\n\nFjernet variabler.\n\nVed fjerning av variabler fra et datasett bÃ¸r det vurderes om dette egentlig er et annet (et helt nytt) datasett, ikke en ny versjon av et eksisterende datasett!\n\nAndre strukturendringer, f.eks. bytte av datatyper eller formater.\n\n\n\n\n\nData som er under utarbeiding skal ikke deles/publiseres, og det er derfor ikke behov for Ã¥ versjonere slike data/datasett. Disse mÃ¥ betraktes som temporÃ¦re og â€œversjonslÃ¸seâ€. Bearbeiding av data bÃ¸r derfor foregÃ¥ i egne temporÃ¦re dataomrÃ¥der1. Det er fÃ¸rst pÃ¥ det tidspunktet et datasett er ferdig bearbeidet og klart for deling/publisering at det skal versjoneres og dokumenteres.\n1 Med temporÃ¦re dataomrÃ¥der menes f.eks. egne mapper med temporÃ¦re datafiler i bÃ¸tter (noe tilsvarende â€œwk-katalogeneâ€ i SSBs eksisterende stammekataloger pÃ¥ Linux). For en del statistikkomrÃ¥der vil databearbeidingen ogsÃ¥ foregÃ¥ i temporÃ¦re datasett i datatjenester som Google BigQuery og CloudSQL.\n\n\n\nVed fÃ¸rste gangs deling/publisering av et ferdig bearbeidet datasett oppstÃ¥r â€œversjon 1â€. Dette er datasett som mÃ¥ bevares uforandret for ettertiden for Ã¥ dekke kravet til etterprÃ¸vbarhet av SSBs statistikk. I tillegg til selve versjonsnummeret er det viktig Ã¥ dokumentere versjonstidspunktet (metadata om tidspunktet versjonen ble frigitt for bruk) samt en beskrivelse av hvorfor det er utarbeidet en ny versjon. Dette er informasjon data-konsumentene trenger for Ã¥ kunne reprodusere statistikk med gamle versjoner av data.\nVersjonsnummer skal legges pÃ¥ som et eget element i filnavnet. For versjon 1 vil dette vÃ¦re â€œ_v1â€, eksempelvis â€œframskrevne-befolkningsendringer_p2019_p2050_v1.parquetâ€\nEksempel pÃ¥ versjon 1 av et datasett i en mappe-struktur:\nssb-prod-team-personstatistikk-data-produkt/  \nâ””â”€â”€ befolkningsframskrivinger/  \n    â””â”€â”€ klargjorte_data/  \n        â””â”€â”€ framskrevne-befolkningsendringer_p2019_p2050_v1.parquet\n\n\n\nFor hver versjon som oppstÃ¥r av datasettet opprettes det en ny fysisk fil hvor versjonsnummeret Ã¸kes med 1. Alle gamle versjoner av et datasett skal ogsÃ¥ eksistere i mappen.\n\n\n\n\n\n\nPermanente data skal ikke endres eller slettes\n\n\n\nTidligere delte/publiserte data skal ikke slettes eller overskrives!\nDet mÃ¥ derfor lagres fysiske filer for hver versjon av datasettet. Dette er viktig for at SSB skal oppfylle krav om etterprÃ¸vbarhet av statistikkene.\n\n\nEksempel pÃ¥ versjon 1, 2 og 3 av et datasett i en mappe-struktur:\nssb-prod-team-personstatistikk-data-produkt/  \nâ””â”€â”€ befolkningsframskrivinger/  \n    â””â”€â”€ klargjorte_data/  \n        â”œâ”€â”€ framskrevne-befolkningsendringer_p2019_p2050_v1.parquet  \n        â”œâ”€â”€ framskrevne-befolkningsendringer_p2019_p2050_v2.parquet  \n        â””â”€â”€ framskrevne-befolkningsendringer_p2019_p2050_v3.parquet\n\n\n\n\n\n\nEksempel pÃ¥ versjonsinformasjon og bearbeidingshistorikk for et datasett:\n\n\n\nVersjonsinformasjon:\nDatasett: framskrevne-befolkningsendringer_p2019_p2050\nVersjon: 1\nVersjonstidspunkt: 2019-01-01T10:00:00\nVersjonsbeskrivelse: FÃ¸rste deling/publisering\nFilnavn: framskrevne-befolkningsendringer_p2019_p2050_v1.parquet\nVersjon: 2\nVersjonstidspunkt: 2020-02-15T08:00:00\nVersjonsbeskrivelse: Reberegning med nytt datagrunnlag\nFilnavn: framskrevne-befolkningsendringer_p2019_p2050_v2.parquet\nVersjon: 3\nVersjonstidspunkt: 2021-05-31T10:00:00\nVersjonsbeskrivelse: Revisjon og reberegning med nye framskrivingsmetoder\nFilnavn: framskrevne-befolkningsendringer_p2019_p2050_v3.parquet\n\n\n\n\n\nHvis det er behov for Ã¥ dele data som er som er i â€œfartâ€, dvs. data som fortsatt er under innsamling eller pÃ¥gÃ¥ende klargjÃ¸ring, gjÃ¸res dette ved Ã¥ bruke versjonsnummer 0 i filnavnet. Versjonsnummer 0 skal kun brukes midlertidig fram til datasettet oppnÃ¥r stabil tilstand (ved stabil tilstand byttes versjonsnummer for datasettet til 1 eller hÃ¸yere).\nEksempel pÃ¥ deling av â€œversjon 0â€ av et datasett:\nssb-prod-team-personstatistikk-data-produkt/  \nâ””â”€â”€ befolkningsframskrivinger/  \n    â””â”€â”€ klargjorte_data/  \n        â””â”€â”€ framskrevne-befolkningsendringer_p2019_p2050_v0.parquet\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nw3.org anbefaler at gjeldende versjon av et datasett alltid skal vÃ¦re tilgjengelig ogsÃ¥ uten versjonsnummer. Dette vil ogsÃ¥ vÃ¦re svÃ¦rt nyttig for statistikkproduksjonen i SSB, i og med at det i de aller fleste tilfeller er siste versjon (den mest oppdaterte) av de klargjorte datasettene som skal benyttes. Den samme programkoden (Python/R) kan da kjÃ¸res ved hver statistikkproduksjon uten at filnavnet mÃ¥ endres i programkoden. Det er kun i de fÃ¥ tilfellene hvor gamle versjoner skal benyttes at programkoden mÃ¥ endres, f.eks. ved â€œreproduseringâ€ av gamle tall.\nEksempel fra w3.org:\n\n/data_example/transport/dataset/bus/stops is the â€œgeneric URIâ€ at which the current version of a dataset is always available\n/data_example/transport/dataset/bus/stops_v2 is the versioned URI for the current dataset\n/data_example/transport/dataset/bus/stops_v1 is the versioned URI of the prior version of the dataset\n\n\n\n\n\nEksempel pÃ¥ â€œversjon 1â€ (hvor versjonen ogsÃ¥ er tilgjengelig uten versjonsnummer i datasettnavnet):\nssb-prod-personstatistikk-data-produkt/  \nâ””â”€â”€ befolkningsframskrivinger/  \n    â””â”€â”€ klargjorte-data/  \n        â”œâ”€â”€ framskrevne-befolkningsendringer_p2019_p2050_v1.parquet  \n        â””â”€â”€ framskrevne-befolkningsendringer_p2019_p2050.parquet\n\n\n\n\n\n\nNote\n\n\n\nw3.org anbefaling er at siste versjon ogsÃ¥ er tilgjengelig uten versjonselement i navnet. I eksempelet over er derfor filen framskrevne-befolkningsendringer_p2019_p2050.parquet helt identisk med framskrevne-befolkningsendringer_p2019_p2050_v1.parquet (versjon 1).\n\n\nEksempel med versjon 1, 2, 3 og 4 (hvor versjon 4 ogsÃ¥ er tilgjengelig uten versjonsnummer i datasettnavnet)\nssb-prod-personstatistikk-data-produkt/  \nâ””â”€â”€ befolkningsframskrivinger/  \n    â””â”€â”€ klargjorte-data/  \n        â”œâ”€â”€ framskrevne-befolkningsendringer_p2019_p2050_v1.parquet  \n        â”œâ”€â”€ framskrevne-befolkningsendringer_p2019_p2050_v2.parquet  \n        â”œâ”€â”€ framskrevne-befolkningsendringer_p2019_p2050_v3.parquet  \n        â”œâ”€â”€ framskrevne-befolkningsendringer_p2019_p2050_v4.parquet  \n        â””â”€â”€ framskrevne-befolkningsendringer_p2019_p2050.parquet\n\n\n\n\n\n\nNote\n\n\n\nw3.org anbefaling er at siste versjon ogsÃ¥ er tilgjengelig uten versjonselement i navnet. I eksempelet over er derfor filen framskrevne-befolkningsendringer_p2019_p2050.parquet helt identisk med framskrevne-befolkningsendringer_p2019_p2050_v4.parquet (versjon 4).\n\n\n\n\n\nFor Ã¥ unngÃ¥ dobbeltlagring av data i form av at siste versjon av et datasett skal lagres som en fysisk datafil bÃ¥de med og uten versjonsnummer (som vist i kapittelet over), anbefales det at det utvikles felles SSB-programbibliotek i Python og R for utlede denne informasjonen. Da vil da kun vÃ¦re nÃ¸dvendig Ã¥ lagre filer med fullt versjonsnummer, men statistikkprodusentene kan bruke en funksjon for Ã¥ finne siste versjon - eksempelvis gi meg siste versjon av datasettet â€œframskrevne-befolkningsendringer_p2019_p2050â€œ.\nNedenfor vises en enkel Python-funksjon for hvordan dette kan fungere i praksis. Denne funksjonaliteten er imidlertid ikke tilgjengelig i Dapla i skrivende stund, sÃ¥ dette er bare et forslag til lÃ¸sning.\n# Eksempel pÃ¥ en felles SSB-funksjon for Ã¥ hente riktig fysisk filnavn til siste versjon\n# av et datasettt i en mappe (katalog) i en bÃ¸tte hvor det eksisterer flere versjoner\n# (flere filversjoner) av datasettet.\n# NB! Dette er kun ment som et eksempel (konsept), og er ikke en produksjonsklar lÃ¸sning!\n\nimport operator\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\ndef get_current_dataset_version(path:str,\n                                dataset_name_without_version:str,\n                                file_type:str = \"parquet\"\n                               ) -&gt; str:    \n    gcs_dataset_files = fs.glob(path=path + dataset_name_without_version + \"*.\" + file_type)\n\n    file_list = []\n    for file in gcs_dataset_files:\n        file_version = file.split(\"_v\")[-1].split(\".\")[0]\n        if file_version.isnumeric():\n            file_list.append({\"file_path\": str(file), \"file_version\": int(file_version) })\n\n    file_list.sort(key=operator.itemgetter('file_version'))\n    if len(file_list) &gt; 0:\n        return file_list[-1][\"file_path\"]\n    else:\n        return None\n\n\n### Eksempel pÃ¥ bruk ###\n# Hent sti og filnavn til siste versjon av datasettet \"framskrevne-befolkningsendringer_p2019_p2050\"\n# i mappen gs://ssb-prod-befolkning-data-produkt/befolkningsendringer/klargjorte-data/\n# som inneholder 4 versjoner (\"framskrevne-befolkningsendringer_p2019_p2050_v1\" \n# til \"framskrevne-befolkningsendringer_p2019_p2050_v4\")\n\nget_current_dataset_version(\n    path=\"gs://ssb-prod-befolkning-data-produkt/befolkningsendringer/klargjorte-data/\",\n    dataset_name_without_version=\"framskrevne-befolkningsendringer_p2019_p2050\",\n    file_type=\"parquet\")\n\n\n### Eksempel pÃ¥ resultat (sti og filnavn til versjon 4 av datasettet) ###\ngs://ssb-prod-befolkning-data-produkt/befolkningsendringer/klargjorte-data/framskrevne-befolkningsendringer_p2019_p2050_v4",
    "crumbs": [
      "Statistikere",
      "Standarder",
      "Versjonering av datasett"
    ]
  },
  {
    "objectID": "statistikkere/versjonering-av-datasett.html#regler-for-hva-som-skaper-en-ny-versjon-av-et-datasett-breaking-changes",
    "href": "statistikkere/versjonering-av-datasett.html#regler-for-hva-som-skaper-en-ny-versjon-av-et-datasett-breaking-changes",
    "title": "Versjonering av datasett",
    "section": "",
    "text": "FÃ¸lgende hendelser skaper en ny versjon av et datasett:\n\nReberegninger av data med nye metoder.\nKorrigeringer av verdier for eksisterende observasjoner/enheter i datasettet.\n\nSelv manuell endring av bare Ã©n data-verdi (celle) i et stort datasett skaper en ny versjon!\n\nLagt til nye og/eller fjernet observasjoner/enheter i datasettet.\nOmkodinger, dvs. oppdatert/erstattet kodeverk.\nLagt til nye variabler. Dette skaper en ny versjon i og med at dette kan pÃ¥virke prosesser (programkode).\n\nHvis det gjÃ¸res vesentlige endringer (mange nye variabler) sÃ¥ bÃ¸r det vurderes om dette er et annet (et helt nytt) datasett, ikke en ny versjon av et eksisterende datasett.\n\nFjernet variabler.\n\nVed fjerning av variabler fra et datasett bÃ¸r det vurderes om dette egentlig er et annet (et helt nytt) datasett, ikke en ny versjon av et eksisterende datasett!\n\nAndre strukturendringer, f.eks. bytte av datatyper eller formater.\n\n\n\n\n\nData som er under utarbeiding skal ikke deles/publiseres, og det er derfor ikke behov for Ã¥ versjonere slike data/datasett. Disse mÃ¥ betraktes som temporÃ¦re og â€œversjonslÃ¸seâ€. Bearbeiding av data bÃ¸r derfor foregÃ¥ i egne temporÃ¦re dataomrÃ¥der1. Det er fÃ¸rst pÃ¥ det tidspunktet et datasett er ferdig bearbeidet og klart for deling/publisering at det skal versjoneres og dokumenteres.\n1 Med temporÃ¦re dataomrÃ¥der menes f.eks. egne mapper med temporÃ¦re datafiler i bÃ¸tter (noe tilsvarende â€œwk-katalogeneâ€ i SSBs eksisterende stammekataloger pÃ¥ Linux). For en del statistikkomrÃ¥der vil databearbeidingen ogsÃ¥ foregÃ¥ i temporÃ¦re datasett i datatjenester som Google BigQuery og CloudSQL.\n\n\n\nVed fÃ¸rste gangs deling/publisering av et ferdig bearbeidet datasett oppstÃ¥r â€œversjon 1â€. Dette er datasett som mÃ¥ bevares uforandret for ettertiden for Ã¥ dekke kravet til etterprÃ¸vbarhet av SSBs statistikk. I tillegg til selve versjonsnummeret er det viktig Ã¥ dokumentere versjonstidspunktet (metadata om tidspunktet versjonen ble frigitt for bruk) samt en beskrivelse av hvorfor det er utarbeidet en ny versjon. Dette er informasjon data-konsumentene trenger for Ã¥ kunne reprodusere statistikk med gamle versjoner av data.\nVersjonsnummer skal legges pÃ¥ som et eget element i filnavnet. For versjon 1 vil dette vÃ¦re â€œ_v1â€, eksempelvis â€œframskrevne-befolkningsendringer_p2019_p2050_v1.parquetâ€\nEksempel pÃ¥ versjon 1 av et datasett i en mappe-struktur:\nssb-prod-team-personstatistikk-data-produkt/  \nâ””â”€â”€ befolkningsframskrivinger/  \n    â””â”€â”€ klargjorte_data/  \n        â””â”€â”€ framskrevne-befolkningsendringer_p2019_p2050_v1.parquet\n\n\n\nFor hver versjon som oppstÃ¥r av datasettet opprettes det en ny fysisk fil hvor versjonsnummeret Ã¸kes med 1. Alle gamle versjoner av et datasett skal ogsÃ¥ eksistere i mappen.\n\n\n\n\n\n\nPermanente data skal ikke endres eller slettes\n\n\n\nTidligere delte/publiserte data skal ikke slettes eller overskrives!\nDet mÃ¥ derfor lagres fysiske filer for hver versjon av datasettet. Dette er viktig for at SSB skal oppfylle krav om etterprÃ¸vbarhet av statistikkene.\n\n\nEksempel pÃ¥ versjon 1, 2 og 3 av et datasett i en mappe-struktur:\nssb-prod-team-personstatistikk-data-produkt/  \nâ””â”€â”€ befolkningsframskrivinger/  \n    â””â”€â”€ klargjorte_data/  \n        â”œâ”€â”€ framskrevne-befolkningsendringer_p2019_p2050_v1.parquet  \n        â”œâ”€â”€ framskrevne-befolkningsendringer_p2019_p2050_v2.parquet  \n        â””â”€â”€ framskrevne-befolkningsendringer_p2019_p2050_v3.parquet\n\n\n\n\n\n\nEksempel pÃ¥ versjonsinformasjon og bearbeidingshistorikk for et datasett:\n\n\n\nVersjonsinformasjon:\nDatasett: framskrevne-befolkningsendringer_p2019_p2050\nVersjon: 1\nVersjonstidspunkt: 2019-01-01T10:00:00\nVersjonsbeskrivelse: FÃ¸rste deling/publisering\nFilnavn: framskrevne-befolkningsendringer_p2019_p2050_v1.parquet\nVersjon: 2\nVersjonstidspunkt: 2020-02-15T08:00:00\nVersjonsbeskrivelse: Reberegning med nytt datagrunnlag\nFilnavn: framskrevne-befolkningsendringer_p2019_p2050_v2.parquet\nVersjon: 3\nVersjonstidspunkt: 2021-05-31T10:00:00\nVersjonsbeskrivelse: Revisjon og reberegning med nye framskrivingsmetoder\nFilnavn: framskrevne-befolkningsendringer_p2019_p2050_v3.parquet\n\n\n\n\n\nHvis det er behov for Ã¥ dele data som er som er i â€œfartâ€, dvs. data som fortsatt er under innsamling eller pÃ¥gÃ¥ende klargjÃ¸ring, gjÃ¸res dette ved Ã¥ bruke versjonsnummer 0 i filnavnet. Versjonsnummer 0 skal kun brukes midlertidig fram til datasettet oppnÃ¥r stabil tilstand (ved stabil tilstand byttes versjonsnummer for datasettet til 1 eller hÃ¸yere).\nEksempel pÃ¥ deling av â€œversjon 0â€ av et datasett:\nssb-prod-team-personstatistikk-data-produkt/  \nâ””â”€â”€ befolkningsframskrivinger/  \n    â””â”€â”€ klargjorte_data/  \n        â””â”€â”€ framskrevne-befolkningsendringer_p2019_p2050_v0.parquet",
    "crumbs": [
      "Statistikere",
      "Standarder",
      "Versjonering av datasett"
    ]
  },
  {
    "objectID": "statistikkere/versjonering-av-datasett.html#funksjonalitet-for-Ã¥-hente-siste-gjeldende-versjon-av-et-datasett",
    "href": "statistikkere/versjonering-av-datasett.html#funksjonalitet-for-Ã¥-hente-siste-gjeldende-versjon-av-et-datasett",
    "title": "Versjonering av datasett",
    "section": "",
    "text": "Note\n\n\n\nw3.org anbefaler at gjeldende versjon av et datasett alltid skal vÃ¦re tilgjengelig ogsÃ¥ uten versjonsnummer. Dette vil ogsÃ¥ vÃ¦re svÃ¦rt nyttig for statistikkproduksjonen i SSB, i og med at det i de aller fleste tilfeller er siste versjon (den mest oppdaterte) av de klargjorte datasettene som skal benyttes. Den samme programkoden (Python/R) kan da kjÃ¸res ved hver statistikkproduksjon uten at filnavnet mÃ¥ endres i programkoden. Det er kun i de fÃ¥ tilfellene hvor gamle versjoner skal benyttes at programkoden mÃ¥ endres, f.eks. ved â€œreproduseringâ€ av gamle tall.\nEksempel fra w3.org:\n\n/data_example/transport/dataset/bus/stops is the â€œgeneric URIâ€ at which the current version of a dataset is always available\n/data_example/transport/dataset/bus/stops_v2 is the versioned URI for the current dataset\n/data_example/transport/dataset/bus/stops_v1 is the versioned URI of the prior version of the dataset\n\n\n\n\n\nEksempel pÃ¥ â€œversjon 1â€ (hvor versjonen ogsÃ¥ er tilgjengelig uten versjonsnummer i datasettnavnet):\nssb-prod-personstatistikk-data-produkt/  \nâ””â”€â”€ befolkningsframskrivinger/  \n    â””â”€â”€ klargjorte-data/  \n        â”œâ”€â”€ framskrevne-befolkningsendringer_p2019_p2050_v1.parquet  \n        â””â”€â”€ framskrevne-befolkningsendringer_p2019_p2050.parquet\n\n\n\n\n\n\nNote\n\n\n\nw3.org anbefaling er at siste versjon ogsÃ¥ er tilgjengelig uten versjonselement i navnet. I eksempelet over er derfor filen framskrevne-befolkningsendringer_p2019_p2050.parquet helt identisk med framskrevne-befolkningsendringer_p2019_p2050_v1.parquet (versjon 1).\n\n\nEksempel med versjon 1, 2, 3 og 4 (hvor versjon 4 ogsÃ¥ er tilgjengelig uten versjonsnummer i datasettnavnet)\nssb-prod-personstatistikk-data-produkt/  \nâ””â”€â”€ befolkningsframskrivinger/  \n    â””â”€â”€ klargjorte-data/  \n        â”œâ”€â”€ framskrevne-befolkningsendringer_p2019_p2050_v1.parquet  \n        â”œâ”€â”€ framskrevne-befolkningsendringer_p2019_p2050_v2.parquet  \n        â”œâ”€â”€ framskrevne-befolkningsendringer_p2019_p2050_v3.parquet  \n        â”œâ”€â”€ framskrevne-befolkningsendringer_p2019_p2050_v4.parquet  \n        â””â”€â”€ framskrevne-befolkningsendringer_p2019_p2050.parquet\n\n\n\n\n\n\nNote\n\n\n\nw3.org anbefaling er at siste versjon ogsÃ¥ er tilgjengelig uten versjonselement i navnet. I eksempelet over er derfor filen framskrevne-befolkningsendringer_p2019_p2050.parquet helt identisk med framskrevne-befolkningsendringer_p2019_p2050_v4.parquet (versjon 4).\n\n\n\n\n\nFor Ã¥ unngÃ¥ dobbeltlagring av data i form av at siste versjon av et datasett skal lagres som en fysisk datafil bÃ¥de med og uten versjonsnummer (som vist i kapittelet over), anbefales det at det utvikles felles SSB-programbibliotek i Python og R for utlede denne informasjonen. Da vil da kun vÃ¦re nÃ¸dvendig Ã¥ lagre filer med fullt versjonsnummer, men statistikkprodusentene kan bruke en funksjon for Ã¥ finne siste versjon - eksempelvis gi meg siste versjon av datasettet â€œframskrevne-befolkningsendringer_p2019_p2050â€œ.\nNedenfor vises en enkel Python-funksjon for hvordan dette kan fungere i praksis. Denne funksjonaliteten er imidlertid ikke tilgjengelig i Dapla i skrivende stund, sÃ¥ dette er bare et forslag til lÃ¸sning.\n# Eksempel pÃ¥ en felles SSB-funksjon for Ã¥ hente riktig fysisk filnavn til siste versjon\n# av et datasettt i en mappe (katalog) i en bÃ¸tte hvor det eksisterer flere versjoner\n# (flere filversjoner) av datasettet.\n# NB! Dette er kun ment som et eksempel (konsept), og er ikke en produksjonsklar lÃ¸sning!\n\nimport operator\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\ndef get_current_dataset_version(path:str,\n                                dataset_name_without_version:str,\n                                file_type:str = \"parquet\"\n                               ) -&gt; str:    \n    gcs_dataset_files = fs.glob(path=path + dataset_name_without_version + \"*.\" + file_type)\n\n    file_list = []\n    for file in gcs_dataset_files:\n        file_version = file.split(\"_v\")[-1].split(\".\")[0]\n        if file_version.isnumeric():\n            file_list.append({\"file_path\": str(file), \"file_version\": int(file_version) })\n\n    file_list.sort(key=operator.itemgetter('file_version'))\n    if len(file_list) &gt; 0:\n        return file_list[-1][\"file_path\"]\n    else:\n        return None\n\n\n### Eksempel pÃ¥ bruk ###\n# Hent sti og filnavn til siste versjon av datasettet \"framskrevne-befolkningsendringer_p2019_p2050\"\n# i mappen gs://ssb-prod-befolkning-data-produkt/befolkningsendringer/klargjorte-data/\n# som inneholder 4 versjoner (\"framskrevne-befolkningsendringer_p2019_p2050_v1\" \n# til \"framskrevne-befolkningsendringer_p2019_p2050_v4\")\n\nget_current_dataset_version(\n    path=\"gs://ssb-prod-befolkning-data-produkt/befolkningsendringer/klargjorte-data/\",\n    dataset_name_without_version=\"framskrevne-befolkningsendringer_p2019_p2050\",\n    file_type=\"parquet\")\n\n\n### Eksempel pÃ¥ resultat (sti og filnavn til versjon 4 av datasettet) ###\ngs://ssb-prod-befolkning-data-produkt/befolkningsendringer/klargjorte-data/framskrevne-befolkningsendringer_p2019_p2050_v4",
    "crumbs": [
      "Statistikere",
      "Standarder",
      "Versjonering av datasett"
    ]
  },
  {
    "objectID": "statistikkere/transfer-service.html",
    "href": "statistikkere/transfer-service.html",
    "title": "Transfer Service",
    "section": "",
    "text": "Storage Transfer Service1 er en Google-tjeneste for Ã¥ flytte data mellom lagringsomrÃ¥der. I SSB bruker vi hovedsakelig tjenesten til Ã¥:\nTjenesten stÃ¸tter bÃ¥de automatiserte og ad-hoc overfÃ¸ringer, og den inkluderer et brukergrensesnitt for Ã¥ sette opp og administrere overfÃ¸ringene i Google Cloud Console (GCC).",
    "crumbs": [
      "Statistikere",
      "Datatjenester",
      "Transfer Service"
    ]
  },
  {
    "objectID": "statistikkere/transfer-service.html#tilgangsstyring",
    "href": "statistikkere/transfer-service.html#tilgangsstyring",
    "title": "Transfer Service",
    "section": "Tilgangsstyring",
    "text": "Tilgangsstyring\nTilgangsstyringen til data gjelder ogsÃ¥ for overfÃ¸ringer av data med Transfer Service. Det betyr at du mÃ¥ ha tilgang til dataene du skal sette opp overfÃ¸ringsjobber for. Ved bruk av Transfer Service for overfÃ¸ring av data mellom bakke og sky sÃ¥ er det satt opp en dedikerte mapper for dette i prodsonen. OgsÃ¥ her fÃ¸lges tilgangsstyringen til dataene, med unntak av at data-admins har permanent tilgang til kildedata som er synkronisert ned til bakken, mens man pÃ¥ Dapla mÃ¥ de gi seg selv korte, begrunnede tilganger ved behov.\n\n\nPÃ¥ Dapla sÃ¥ er det opprettet dedikerte bÃ¸tter for overfÃ¸ring av data mellom bakke og sky. Disse heter tilsky og frasky. Tanken med disse â€œmellomstasjoneneâ€ for overfÃ¸ring av data er at de skal beskytte Dapla-team fra Ã¥ overskrive data ved en feil. Ved Ã¥ ha egne bÃ¸tter som data blir synkronisert gjennom, sÃ¥ legges det opp til at man deretter manuelt3 flytter dataene til riktig bÃ¸tte.\nMen det er ikke lagt noen sperrer for synkronisere direkte til en annen bÃ¸tte man har tilgang til. Systembrukeren (se forklaringsboks) som kjÃ¸rer Transfer Service har tilgang til alle bÃ¸ttene i prosjektet. Det betyr at en data-admin kan velge Ã¥ synkronisere data direkte inn i kildebÃ¸tta hvis man mener at det er hensiktsmessig. Det samme gjelder for developers som setter opp dataoverfÃ¸ringer i standardprosjektet. Men da er det som sagt viktig Ã¥ vÃ¦re bevisst pÃ¥ hvordan man setter opp reglene for overskriving av data hvis filene har like navn. Disse opsjonene forklares nÃ¦rmere senere i kapitlet.\n\n\n\n\n\n\n\n\n\nPersonlig bruker vs systembruker\n\n\n\nNÃ¥r du setter opp en overfÃ¸ringsjobb med Transfer Service sÃ¥ setter du opp en jobb som kjÃ¸res av en systembruker4 og ikke din egen personlige bruker. Dette er spesielt viktig Ã¥ vÃ¦re klar over nÃ¥r man setter opp automatiserte overfÃ¸ringsjobber. En konsekves av dette er at automatiske overfÃ¸ringsjobber vil fortsette Ã¥ kjÃ¸re selv om din tilgang til dataene er midlertidig, siden det er en systembruker som faktisk kjÃ¸rer jobben.",
    "crumbs": [
      "Statistikere",
      "Datatjenester",
      "Transfer Service"
    ]
  },
  {
    "objectID": "statistikkere/transfer-service.html#forberedelser",
    "href": "statistikkere/transfer-service.html#forberedelser",
    "title": "Transfer Service",
    "section": "Forberedelser",
    "text": "Forberedelser\nFÃ¸rste gang du bruker Transfer Service mÃ¥ du sjekke at tjenesten er aktivert for teamet. Transfer Service er en sÃ¥kalt feature som teamet kan skru av og pÃ¥ selv. For Ã¥ sjekke om den er skrudd pÃ¥ gÃ¥r du inn i teamets IaC-repo5 og sjekker filen ./infra/projects.yaml.\n\n\ndapla-example-iac/infra/projects.yaml\n\nteam_uniform_name: dapla-example\n\nprojects:\n  - project_name: dapla-example\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service\n\nI filen over ser du at teamet har skrudd pÃ¥ tjenesten i prod-miljÃ¸et, siden den transfer-service er listet under features. Hvis tjenesten ikke er skrudd pÃ¥ kan du lese om hvordan du skrur den pÃ¥ i feature-dokumentasjonen.",
    "crumbs": [
      "Statistikere",
      "Datatjenester",
      "Transfer Service"
    ]
  },
  {
    "objectID": "statistikkere/transfer-service.html#overfÃ¸ring-av-data",
    "href": "statistikkere/transfer-service.html#overfÃ¸ring-av-data",
    "title": "Transfer Service",
    "section": "OverfÃ¸ring av data",
    "text": "OverfÃ¸ring av data\n\n\n\n\n\n\nOverfÃ¸ring av kildedata\n\n\n\nOverfÃ¸ring av kildedata mÃ¥ gjÃ¸res av en data-admin i teamet som har aktivert sin forhÃ¥ndsgodkjente tilgang til kildedata. Tilgangen aktiveres ved Ã¥ gÃ¥ inn i JIT-applikasjonen og velge prosjekt-id. Deretter velger du rollene ssb.bucket.write, ssb.buckets.list og storagetransfer.admin, og hvor lenge du Ã¸nsker tilgangen. Til slutt oppgir du en begrunnelse for hvorfor du trenger tilgangentilgangen og trykker Request access. NÃ¥r du har gjort dette vil du fÃ¥ en bekreftelse pÃ¥ at tilgangen er aktivert, og det tar ca 1 minutt fÃ¸r den aktiverte tilgangen er synlig i GCC.\n\n\nGrensesnittet for Ã¥ sette opp overfÃ¸ringsjobber i Transfer Service er tilgjengelig i Google Cloud Console (GCC).\n\n\n\nGÃ¥ inn pÃ¥ Google Cloud Console i en nettleser.\nSjekk, Ã¸verst i hÃ¸yre hjÃ¸rne, at du er logget inn med din SSB-konto (xxx@ssb.no).\nVelg prosjektet6 som overfÃ¸ringen skal settes opp under.\nEtter at du har valgt prosjekt kan du sÃ¸ke etter Storage Transfer i sÃ¸kefeltet Ã¸verst pÃ¥ siden, og gÃ¥ inn pÃ¥ siden.\n\n\n\n\n\n\n\n\n\n\nHva er mitt prosjektnavn?\n\n\n\nNÃ¥r det opprettes et Dapla-team, sÃ¥ opprettes det flere Google-prosjekter for teamet. NÃ¥r du skal velge hvilket prosjekt du skal jobbe pÃ¥ i GCC, sÃ¥ fÃ¸lger de en fast navnestruktur. For eksempel sÃ¥ vil et team med navnet dapla-example fÃ¥ et standardprosjekt som heter dapla-example-p. Det blir ogsÃ¥ opprettet et kildeprosjekt som heter dapla-example-kilde-p.\n\n\n\n\nFÃ¸rste gang du bruker Storage Transfer mÃ¥ man gjÃ¸re en engangsjobb for Ã¥ bruke tjenesten. Dette gjÃ¸res kun fÃ¸rste gang din bruker setter opp en jobb, og deretter trenger du ikke Ã¥ gjÃ¸re det flere ganger.\nNÃ¥r du kommer inn pÃ¥ siden til Storage Transfer sÃ¥ trykker du pÃ¥ Set Up Connection. NÃ¥r du trykker pÃ¥ denne vil det dukke opp et nytt felt hvor du fÃ¥r valget Create Pub-Sub Resources. Trykk pÃ¥ den blÃ¥ Create-knappen, og deretter trykk pÃ¥ Close lenger nede. Da er engangsjobben gjort, og du kan begynne Ã¥ sette opp overfÃ¸ringsjobber.\n\nI navigasjonsmenyen til venstre trykk Transfer jobs, og deretter trykk pÃ¥ + Create transfer job Ã¸verst pÃ¥ siden for Ã¥ opprette en ny overfÃ¸ringsjobb. Da fÃ¥r du opp bildet som vist i FigurÂ 1.\n\n\n\n\n\n\n\nFigurÂ 1: Opprett overfÃ¸ringsjobb i Google Cloud Console.\n\n\n\nVidere vil det variere om man skal overfÃ¸re data mellom bÃ¸tter eller mellom Dapla og prodsonen. Under forklarer vi begge fremgangsmÃ¥tene.\n\nProdsonen og Dapla\nOverfÃ¸ring mellom bakke og sky er en overfÃ¸ring av data mellom en bÃ¸tte pÃ¥ Dapla og en mappe i prodsonen. Siden tilgangsstyring til kildedata er strengere enn tilgangsstyring til annen data, sÃ¥ er det det to litt fremgangsmÃ¥ter for Ã¥ sette opp overfÃ¸ringsjobber for disse.\nSiden stegene er litt forskjellig avhengig av om man skal flytte kildedata eller annen data, sÃ¥ deler vi denne delen i to. FigurÂ 2 viser hvordan dette er satt opp. Kildeprosjektet pÃ¥ Dapla har en tilsky-bÃ¸tte for Ã¥ flytting av data fra prodsonen til Dapla, og den har en frasky-bÃ¸tte for Ã¥ flytte data fra Dapla til prodsonen. Standardprosjektet pÃ¥ Dapla har ogsÃ¥ en tilsky-bÃ¸tte for Ã¥ flytte data fra prodsonen til Dapla, og den har en frasky-bÃ¸tte for Ã¥ flytte data fra Dapla til prodsonen.\n\n\n\n\n\n\nFigurÂ 2: OverfÃ¸ring av data mellom prodsonen og Dapla.\n\n\n\nVidere viser vi hvordan man overfÃ¸rer fra Dapla til prodsonen. OverfÃ¸ring motsatt vei innebÃ¦rer bare at man bytter om pÃ¥ Source type og Destination type.\n\nI fanen Get started velger du:\n\nSource type: Google Cloud Storage\nDestination type: POSIX filesystem\n\nI fanen Choose a source trykker du pÃ¥ Browse, velger hvilken bÃ¸tte eller â€œundermappeâ€ i en bÃ¸tte du skal overfÃ¸re fra, og trykker Select7.\nI fanen Choose a destination velger du transfer_service_default under Agent pool. Under Destination directory path velger du hvilken undermappe av som filen skal overfÃ¸res til. Tjenesten vet allerede om du er i kilde- eller standardprosjektet, sÃ¥ du trenger kun Ã¥ skrive inn frasky/ eller tilsky/ her, og evt. undermappenavn hvis det er aktuelt (f.eks. frasky/data/8). Trykk Next step.\nI fanen Choose when to run job velger du hvor ofte og hvordan jobber skal kjÃ¸re. TabellÂ 1 viser hvilke valg du kan ta. Trykk Next step.\n\n\n\n\nTabellÂ 1: Valg under Choose when to run job\n\n\n\n\n\n\n\n\n\nValg\nFrekvens\n\n\n\n\nRun once\nEngangoverfÃ¸ringer\n\n\nRun every day\nSynkroniser hver dag\n\n\nRun every week\nSynkroniser hver uke\n\n\nRun with custom frequency\nSynkroniser inntill hver time\n\n\nRun on demand\nSynkroniserer nÃ¥r du manuelt trigger jobben\n\n\n\n\n\n\n\nI fanen Choose settings kan du velge hvordan detaljer knyttet til overfÃ¸ringen skal hÃ¥ndteres. TabellÂ 2 viser hvilke valg du kan ta.\n\n\n\n\nTabellÂ 2: Valg under Choose settings\n\n\n\n\n\n\n\n\n\n\nValg\nUndervalg\nHandling\n\n\n\n\nIdentify your job\n\nBeskriv jobben kort.\n\n\nManifest file\n\nIkke relevant. Bruk default valg.\n\n\nChoose how to handle your data\nMetadata options\nIkke relevant. Bruk default valg.\n\n\n\nWhen to overwrite\nTenk nÃ¸ye gjennom hva du velger her.\n\n\n\nWhen to delete\nTenk nÃ¸ye gjennom hva du velger her.\n\n\nChoose how to keep track of transfer progress\nLogging options\nSkru pÃ¥ logging.\n\n\n\n\n\n\nValgene When to overwrite og When to delete er det viktig at tenkes nÃ¸ye gjennom, spesielt ved automatiske synkroniseringer. When to overwrite er spesielt siden det kan fÃ¸re til data blir overskrevet eller tapt.\n\nTrykk pÃ¥ den blÃ¥ Create-knappen for Ã¥ opprette overfÃ¸ringsjobben. Du vil kunne se kjÃ¸rende jobber under menyen Transfer jobs.\n\n\nMappestrukturen i prodsonen\nMappestrukturen for overfÃ¸ringer med Transfer Service mellom bakke og sky har en fast struktur som er likt for alle team. Hvis du logger deg inn i terminalen pÃ¥ en av Linux-serverne i prodsonen, Ã¥pner du mappen ved Ã¥ skrive cd /ssb/cloud_sync. Under denne mappen finner du en mappe for hvert team som har aktivert Transfer Service. Hvis et team for eksempel heter dapla-example sÃ¥ vil det vÃ¦re en mappe som heter dapla-example. Her kan teamet hente og levere data som skal synkroniseres mellom bakke og sky. Videre er det undermapper for kilde- og standardprosjektet til teamet. Det er kun data-admins som har tilgang til kildeprosjektet, og det er kun developers som har tilgang til standardprosjektet. Under finner du en oversikt over hvordan mappene ser ut for et team som heter dapla-example.\n\n\n/ssb/cloud_sync/dapla-example/\n\ndapla-example\nâ”‚\nâ”œâ”€â”€ kilde\nâ”‚   â”‚\nâ”‚   â”‚â”€â”€ tilsky\nâ”‚   â”‚\nâ”‚   â””â”€â”€ frasky\nâ”‚\nâ””â”€â”€ standard\n    â”‚\n    â”‚â”€â”€ tilsky\n    â”‚\n    â””â”€â”€ frasky\n\n\n\n\nBÃ¸tte til bÃ¸tte\nOverfÃ¸ring mellom bÃ¸tter er en overfÃ¸ring av data mellom to bÃ¸tter pÃ¥ Dapla. FremgangsmÃ¥ten er helt likt som beskrevet tidligere, men at du nÃ¥ velger Google Cloud Storage som bÃ¥de kilde og destinasjon. Igjen er vi avhengig av at systembrukeren som utfÃ¸rer jobben har tilgang til begge bÃ¸ttene som er involvert i overfÃ¸ringen. Default er at et team kan overfÃ¸re mellom bÃ¸tter i kildeprosjektet, og at de kan overfÃ¸re mellom bÃ¸tter i standardprosjektet, men aldri mellom de to. Hvis du Ã¸nsker Ã¥ overfÃ¸re mellom bÃ¸tter i ditt prosjekt og et annet teams prosjekt, sÃ¥ mÃ¥ du be det andre teamet om Ã¥ gi din systembruker tilgang til dette.",
    "crumbs": [
      "Statistikere",
      "Datatjenester",
      "Transfer Service"
    ]
  },
  {
    "objectID": "statistikkere/transfer-service.html#footnotes",
    "href": "statistikkere/transfer-service.html#footnotes",
    "title": "Transfer Service",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI SSB kaller vi tjenesten for Transfer Service, men du kan oppleve at Google kaller den litt forskjellige ting. Den blir omtalt som Storage Transfer Service noen steder, mens i Google Cloud Console blir den omtalt som Data Transfer eller Storage Transferâ†©ï¸\nFlytting av data mellom bÃ¸tter krever at prosjektets Transfer Service har tilgang til begge bÃ¸ttene.â†©ï¸\nMed manuelt menes her at man gÃ¥r inn og flytter filer fra en bÃ¸tte til en annen. Men det kan ogsÃ¥ bety at man flytter data til riktig bÃ¸tte som en del produksjonskoden sin, som igjen kan kjÃ¸res automatisk.â†©ï¸\nSystembrukere heter Service Accounts pÃ¥ engelsk og blir ofte referert til som SA-er i dagligtale.â†©ï¸\nDu finner teamets IaC-repo ved Ã¥ gÃ¥ inn pÃ¥ https://github.com/orgs/statisticsnorway/repositories og sÃ¸ke etter ditt teamnavn og Ã¥pne den som har navnestrukturen teamnavn-iac. For eksempel vil et team som heter dapla-example har et IaC-repo som heter dapla-example-iac.â†©ï¸\nDu kan velge prosjekt Ã¸verst pÃ¥ siden, til hÃ¸yre for teksten Google Cloud. I bildet under ser du at hvordan det ser ut nÃ¥r prosjektet dapla-felles-p er valgt.â†©ï¸\nNÃ¥r du skal velge en undermappe i en bÃ¸tte sÃ¥ er grensesnittet litt lite intuitivt. Du kan ikke trykke pÃ¥ navnet, men du pÃ¥ trykke pÃ¥ -tegnet for Ã¥ se undermappene.â†©ï¸\nNÃ¥r du skal synkronisere fra Dapla til en undermappe i prodsonen, sÃ¥ mÃ¥ mappen i prodsonen allerede eksisterere. Hvis den ikke gjÃ¸r det vil jobben feile. Ved synkronsiering fra prodsonen til Dapla trenger ikke undermappen eksistere, siden bÃ¸tter egentlig ikke har undermapper og filstien fra prodsonen bare blir til filnavnet i bÃ¸tta.â†©ï¸",
    "crumbs": [
      "Statistikere",
      "Datatjenester",
      "Transfer Service"
    ]
  },
  {
    "objectID": "statistikkere/tilgangsstyring.html",
    "href": "statistikkere/tilgangsstyring.html",
    "title": "Tilgangsstyring",
    "section": "",
    "text": "Hvert Dapla-team har sine egne lagringsomrÃ¥der for data som ingen andre har tilgang til, med mindre teamet eksplisitt velger Ã¥ dele data med andre team. I tillegg har teamet tilgang til egne ressurser for Ã¥ behandle dataene.\nDet er tilgangsgruppen managers som bestemmer hvilke personer som skal ha hvilke roller i et team, og dermed hvilke data de ulike team-medlemmene fÃ¥r tilgang til. Den som jobber med data kan bli plassert i tilgangsgruppene data-admins eller developers. Sistnevnte fÃ¥r tilgang til alle datatilstander utenom kildedata, mens data-admins er forhÃ¥ndsgodkjent til Ã¥ ogsÃ¥ Ã¥ aksessere kildedata ved behov. Dermed er data-admins en priveligert rolle pÃ¥ teamet som er forbeholdt noen fÃ¥ personer.\n\n\n\n\n\n\nFigurÂ 1: Datatilstander som et team sitt medlemmer har ilgang til.\n\n\n\nFigurÂ 1 viser hvem som har tilgang til hvilke datatilstander. Som nevnt er data-admins ansett som forhÃ¥ndsgodkjent til Ã¥ aksessere kildedata ved behov. MÃ¥ten dette er implementert pÃ¥ er at data-admins mÃ¥ aktivere denne tilgangen selv, ved Ã¥ bruke et JIT-grensesnitt (Just-In-Time Access). Tilgangen krever en begrunnelse og bruken kan lÃ¸pende monitoreres av managers for teamet.",
    "crumbs": [
      "Statistikere",
      "Dapla-team",
      "Tilgangsstyring"
    ]
  },
  {
    "objectID": "statistikkere/ssbproject.html",
    "href": "statistikkere/ssbproject.html",
    "title": "SSB-project",
    "section": "",
    "text": "SSB-project\nI forrige del forklarte vi hvordan man jobber med skarpe data pÃ¥ Dapla. Det neste steget vil ofte vÃ¦re Ã¥ begynne Ã¥ utvikle kode i Python og/eller R. Dette innebÃ¦rer at man helst skal:\n\nversjonshÃ¥ndtere med Git\nopprette et GitHub-repo\nopprette et virtuelt miljÃ¸ som husker hvilke versjoner av pakker og programmeringssprÃ¥k du brukte\n\nI tillegg mÃ¥ alt dette konfigureres for hvordan SSB sine systemer er satt opp. Dette har vist seg Ã¥ vÃ¦re unÃ¸dvendig krevende for mange. Team Statistikktjenester har derfor utviklet et program som gjÃ¸r alt dette for deg pÃ¥ en enkel mÃ¥te som heter ssb-project.\nVi mener at ssb-project er et naturlig sted Ã¥ starte nÃ¥r man skal bygge opp koden i Python eller R. Det gjelder bÃ¥de pÃ¥ bakken og pÃ¥ sky. I denne delen av boken forklarer vi fÃ¸rst hvordan du bruker ssb-project i det fÃ¸rste kapittelet. Siden programmet skjuler mye av kompleksiteten rundt dette, sÃ¥ bruker vi de andre kapitlene til Ã¥ forklare hvordan man ville satt opp dette uten hjelp av programmet. Dermed vil det vÃ¦re lett for SSB-ansatte Ã¥ skjÃ¸nne hva som gjÃ¸res og hvorfor det er nÃ¸dvendig.\nHer kan du kan lese mer om hvordan et SSB-project kan opprettes."
  },
  {
    "objectID": "statistikkere/administrasjon-av-team.html",
    "href": "statistikkere/administrasjon-av-team.html",
    "title": "Administrasjon av team",
    "section": "",
    "text": "I dette kapitlet viser vi hvordan du kan opprette et nytt team eller gjÃ¸re endringer i et eksisterende team. Typiske endringer er Ã¥:\n\nLegge til eller fjerne medlemmer i et team\nListe ut medlemmer og tilgangsgrupper i et team\n\n\n\nFor Ã¥ komme i gang med Ã¥ opprette et Dapla-team trengs det en oversikt over teamets medlemmer og hvilke tilgangsgrupper medlemmene skal vÃ¦re med i. Det trengs ogsÃ¥ informasjon om hvilke Dapla-tjenester som er aktuelle for teamet Ã¥ ta i bruk. Derfor har det blitt opprettet en egen veileder for dette kalt Dapla Start.\n\n\n\n\n\n\nGÃ¥ til Dapla Start for starte bestilling av et nytt Dapla-team.\n\n\n\nNÃ¥r teamet er opprettet fÃ¥r alle medlemmene tilgang til sitt eget prosjekt i Google Cloud Platform (GCP), som er SSBs leverandÃ¸r av skytjenester. Videre fÃ¥r hvert prosjekt et sett med tjenester og tilganger som knyttes til teamet. Det opprettes ogsÃ¥ datalagringsomrÃ¥der (ofte kalt bÃ¸tter) som bare kan aksesseres av brukere som er med i teamets tilgangsgrupper.\nDapla-teamet vil ogsÃ¥ fÃ¥ sin egen gruppe i SSBs Active Directory slik at medlemskapet i gruppen kan administreres av Kundeservice.\n\n\n\nFor Ã¥ legge til eller fjerne medlemmer i et team mÃ¥ du forelÃ¸pig opprette en Kundeservice-sak. Send en e-post til Kundeservice eller registrer en sak i portalen deres med fÃ¸lgende innhold:\n\nNavnet pÃ¥ Dapla-teamet du Ã¸nsker tilgang\nHvilken tilgang du sÃ¸ker om. Typisk er det enten data-admins, developers eller consumers). Les mer om hva de ulike tilgangene innebÃ¦rer her.\nBeskrivelse av formÃ¥let med tilgangen.\n\nEndringer i team mÃ¥ godkjennes av sÃ¸ker seksjonsleder fÃ¸r de blir effektuert.\nFor fjerning av tilganger kan man sende en tilsvarende henvendelse, men man trenger ikke inkludere forklaring av formÃ¥l.\n\n\n\n\n\n\nMidlertidig lÃ¸sning\n\n\n\nAt endringer i team mÃ¥ gjÃ¸res via Kundeservice er midlertidig. Det jobbes med Ã¥ lage et eget verktÃ¸y for dette.\n\n\n\n\n\nFor Ã¥ se hvem som har de ulike tilgangsrollene i et team, sÃ¥ kan man bruke pakken dapla-team-cli fra Jupyter. Pakken er installert for alle og du kan liste ut medlemmer av team ved Ã¥ gjÃ¸re fÃ¸lgende:\n\nLogg deg inn pÃ¥ Dapla.\nÃ…pne en ny terminal\nSkriv inn dpteam groups list-members og trykk Enter\nI prompten som dukker opp skriver du inn team-navn og trykker Enter.\n\nHvis du ikke har lagret Personal Access Token i Jupyter sÃ¥ blir du spurt om GitHub-bruker og passord etter punkt 4. Da oppgir du du bare din GitHub-bruker og token som er autentisert mot statisticsnorway.\nFor de som ikke har mulighet til Ã¥ bruke Jupyter sÃ¥ kan man ogsÃ¥ sende inn en forespÃ¸rsel til Kundeservice om Ã¥ fÃ¥ en oversikt.",
    "crumbs": [
      "Statistikere",
      "Dapla-team",
      "Administrasjon av team"
    ]
  },
  {
    "objectID": "statistikkere/administrasjon-av-team.html#opprette-dapla-team",
    "href": "statistikkere/administrasjon-av-team.html#opprette-dapla-team",
    "title": "Administrasjon av team",
    "section": "",
    "text": "For Ã¥ komme i gang med Ã¥ opprette et Dapla-team trengs det en oversikt over teamets medlemmer og hvilke tilgangsgrupper medlemmene skal vÃ¦re med i. Det trengs ogsÃ¥ informasjon om hvilke Dapla-tjenester som er aktuelle for teamet Ã¥ ta i bruk. Derfor har det blitt opprettet en egen veileder for dette kalt Dapla Start.\n\n\n\n\n\n\nGÃ¥ til Dapla Start for starte bestilling av et nytt Dapla-team.\n\n\n\nNÃ¥r teamet er opprettet fÃ¥r alle medlemmene tilgang til sitt eget prosjekt i Google Cloud Platform (GCP), som er SSBs leverandÃ¸r av skytjenester. Videre fÃ¥r hvert prosjekt et sett med tjenester og tilganger som knyttes til teamet. Det opprettes ogsÃ¥ datalagringsomrÃ¥der (ofte kalt bÃ¸tter) som bare kan aksesseres av brukere som er med i teamets tilgangsgrupper.\nDapla-teamet vil ogsÃ¥ fÃ¥ sin egen gruppe i SSBs Active Directory slik at medlemskapet i gruppen kan administreres av Kundeservice.",
    "crumbs": [
      "Statistikere",
      "Dapla-team",
      "Administrasjon av team"
    ]
  },
  {
    "objectID": "statistikkere/administrasjon-av-team.html#legge-til-eller-fjerne-medlemmer-i-et-team",
    "href": "statistikkere/administrasjon-av-team.html#legge-til-eller-fjerne-medlemmer-i-et-team",
    "title": "Administrasjon av team",
    "section": "",
    "text": "For Ã¥ legge til eller fjerne medlemmer i et team mÃ¥ du forelÃ¸pig opprette en Kundeservice-sak. Send en e-post til Kundeservice eller registrer en sak i portalen deres med fÃ¸lgende innhold:\n\nNavnet pÃ¥ Dapla-teamet du Ã¸nsker tilgang\nHvilken tilgang du sÃ¸ker om. Typisk er det enten data-admins, developers eller consumers). Les mer om hva de ulike tilgangene innebÃ¦rer her.\nBeskrivelse av formÃ¥let med tilgangen.\n\nEndringer i team mÃ¥ godkjennes av sÃ¸ker seksjonsleder fÃ¸r de blir effektuert.\nFor fjerning av tilganger kan man sende en tilsvarende henvendelse, men man trenger ikke inkludere forklaring av formÃ¥l.\n\n\n\n\n\n\nMidlertidig lÃ¸sning\n\n\n\nAt endringer i team mÃ¥ gjÃ¸res via Kundeservice er midlertidig. Det jobbes med Ã¥ lage et eget verktÃ¸y for dette.",
    "crumbs": [
      "Statistikere",
      "Dapla-team",
      "Administrasjon av team"
    ]
  },
  {
    "objectID": "statistikkere/administrasjon-av-team.html#se-medlemmer-og-roller-i-et-team",
    "href": "statistikkere/administrasjon-av-team.html#se-medlemmer-og-roller-i-et-team",
    "title": "Administrasjon av team",
    "section": "",
    "text": "For Ã¥ se hvem som har de ulike tilgangsrollene i et team, sÃ¥ kan man bruke pakken dapla-team-cli fra Jupyter. Pakken er installert for alle og du kan liste ut medlemmer av team ved Ã¥ gjÃ¸re fÃ¸lgende:\n\nLogg deg inn pÃ¥ Dapla.\nÃ…pne en ny terminal\nSkriv inn dpteam groups list-members og trykk Enter\nI prompten som dukker opp skriver du inn team-navn og trykker Enter.\n\nHvis du ikke har lagret Personal Access Token i Jupyter sÃ¥ blir du spurt om GitHub-bruker og passord etter punkt 4. Da oppgir du du bare din GitHub-bruker og token som er autentisert mot statisticsnorway.\nFor de som ikke har mulighet til Ã¥ bruke Jupyter sÃ¥ kan man ogsÃ¥ sende inn en forespÃ¸rsel til Kundeservice om Ã¥ fÃ¥ en oversikt.",
    "crumbs": [
      "Statistikere",
      "Dapla-team",
      "Administrasjon av team"
    ]
  },
  {
    "objectID": "statistikkere/pseudonymisering.html",
    "href": "statistikkere/pseudonymisering.html",
    "title": "Pseudonymisering",
    "section": "",
    "text": "PÃ¥ Dapla er det flere tjenester som til sammen gir team muligheten til Ã¥ pseudonymisere, de-pseudonymisere eller re-pseudonymisere data1. Disse tjenestene er satt opp pÃ¥ en slik mÃ¥te at statistikkteam kan vÃ¦re selvbetjent i bruken av funksjonaliteten, samtidig som de sikrer at direkte identifiserende opplysninger hÃ¥ndteres i henhold til lovverk og SSBs tolkninger av disse.\nI dette kapitlet forteller vi hvordan man gÃ¥r frem for Ã¥ pseudonymisere data i et statistikkteam pÃ¥ Dapla. FÃ¸r man leser videre er det viktig at leseren har en klar forstÃ¥else av hvordan Dapla-team og tilgangsstyring fungerer pÃ¥ Dapla, samt er kjent med tjenesten Kildomaten. Kildomaten er tjenesten som automatiserer overgangen fra kildedata til inndata, som blant annet inkluderer pseudonymisering.\n\n\nFor Ã¥ pseudonymisere pÃ¥ Dapla mÃ¥ man vÃ¦re en del av Dapla-team og Kildomaten mÃ¥ vÃ¦re tilgjengeliggjort for teamet. Hvis du er usikker pÃ¥ om ditt team har tilgang til Kildomaten, sÃ¥ kan du sjekke dette selv i teamets IaC-repo. For hjelp til Ã¥ aktivere Kildomaten kan man kontakte Kundeservice.\n\n\n\nHvert team er selv ansvarlig for at deres sensitiv data hÃ¥ndteres i henhold til lover og regler. I kontekst av pseudonymisering vil dette bety at teamet mÃ¥ finne ut av hvilken informasjon som skal pseudomymiseres i overgangen fra kildedata til inndata. All kildedata i SSB er klassifisert som sensitivt, og hvis man har kildedata som skal pseudonymiseres, sÃ¥ skal det skje i overgangen fra kildedata til inndata. Er man usikker pÃ¥ om man skal pseudonymisere eller ikke, eller hvilke lover og regler som gjelder for teamets kildedata, sÃ¥ kan man diskutere med personvernombudet og/eller ta kontakt med juristene i SSB.\n\n\n\n\n\n\nIkke glem dataminimering!\n\n\n\nEt av de viktigste tiltakene for Ã¥ verne om personvern i data er Ã¥ fjerne informasjon som ikke er strengt nÃ¸dvendig for formÃ¥let. Dataminimering bÃ¸r gjÃ¸res nÃ¥r data samles inn til SSB. I de tilfellene der det ikke er mulig, sÃ¥ skal det dataminimeres i overgangen fra kildedata til inndata. Etter at man har dataminimert sÃ¥ kan man vurdere hvilken gjenvÃ¦rende informasjon som skal pseudonymiseres.\n\n\n\n\nPersonidentifiserende informasjon (PII) er alle opplysninger som kan knyttes til en enkeltperson. Det fÃ¸lger av bÃ¥de personopplysningsloven og statistikkloven at personidentifiserende informasjon som samles inn for statistikkformÃ¥l skal pseudonymiseres. I SSB sÃ¥ er det gjennomfÃ¸rt personvernkonsekvensutredninger2 (PVK) for to omrÃ¥der som benytter seg av PII:\n\nPersonstatistikk\nNÃ¦ringsstatistikk\n\nUnder gÃ¥r vi gjennom hvordan PII skal behandels i de to tilfellene, og hvilke konsekvenser det har for samarbeid mellom team pÃ¥ Dapla.\n\n\n\n\nI SSB er det gjennomfÃ¸rt en PVK for personstatistikk som presiserer at all PII innen personstatistikk skal pseudonymiseres. Hvert team som hÃ¥ndterer personopplysninger mÃ¥ derfor vurdere sine kildedata og identifisere alt av PII. All data som innholder PII, og som ikke kan dataminimeres bort, skal pseudonymiseres.\nTabellÂ 1 viser en ikke-uttÃ¸mmende liste over PII som har vÃ¦rt identifisert i SSB tidligere. Merk at dette ikke er en fullstendig liste over PII, men hvis et team har denne informasjonen i sine data sÃ¥ skal de pseudonymiseres i overgangen fra kildedata til inndata.\nFor mange er fÃ¸dselsnummer en viktig variabel i kraft at den fungerer som en koblingsnÃ¸kkel for ulike typer persondata. Men som vist i TabellÂ 1 sÃ¥ er det mye annen informasjon som ogsÃ¥ er Ã¥ regne som PII. For eksempel er ogsÃ¥ adresse og bankkontonummer Ã¥ regne som PII.\n\n\nI TabellÂ 1 ser vi at organisasjonsnummeret til et enkeltpersonforetak (ENK) er Ã¥ regne som PII. Grunnen til det er at en ENK eies av en fysisk person og all informasjon knyttet til ENK er Ã¥ regne som personopplysninger. PVK for personstatistikk begrenser sine vurderinger til Ã¥ ikke inkludere informasjon om ENK.\nSiden personstatistikk skal pseudonymisere PII sÃ¥ fÃ¸lger det at ogsÃ¥ PII knyttet til ENK ogsÃ¥ bÃ¸r pseudonymiseres. Grunnen til det er at det finnes mye Ã¥pen informasjon om ENK, og derfor kan man lett knytte pseudonym til en person basert lett tilgjengelig informasjon. Kan man knytte et pseudonym til en person sÃ¥ kan pseudonymet anses Ã¥ vÃ¦re avslÃ¸rt. Selv om det samme ikke gjelder for andre organisasjonsformer (AS, ansvarlig selskap, etc.) sÃ¥ vil det vÃ¦re naturlig at disse ogsÃ¥ pseudonymiseres, blant annet for Ã¥ sikre ENKâ€™er ikke lett kan skilles ut.\n\n\n\nI SSB er det gjennomfÃ¸rt en egen PVK for nÃ¦ringsstatistikk. Grunnen til det er at det ogsÃ¥ her behandles personopplysninger, men at behovene i nÃ¦ringstatistikken er vurdert som sÃ¥pass annerledes sammenlignet med personstatistikk, at de gjennomfÃ¸rte en egen PVK. Et viktig bakteppe for denne vurderingen var at organisasjonsidentifiserende informasjon (OII) tidligere hadde blitt vurdert som nÃ¸dvendig Ã¥ se klarttekst. Mer om OII i neste avsnitt.\n\n\n\n\n\n\n\nTabellÂ 1: Ikke-uttÃ¸mmende liste over PII som har vÃ¦rt identifisert i SSB tidligere.\n\n\n\n\n\nPII\n\n\n\n\nFÃ¸dselsnummer\n\n\nD-nummer\n\n\nS-nummer\n\n\nEktefellenummer\n\n\nFamilienummer\n\n\nHusholdningsnummer\n\n\nDufnummer\n\n\nBankkontonummer\n\n\nAdresse (tekstlig)\n\n\nEiendomsidentifikator\n\n\nVeiadresse (numerisk)\n\n\nMatrikkeladresse (numerisk)\n\n\nRegistreringsnummer (kjÃ¸retÃ¸y)\n\n\nNavn\n\n\nKontaktinformasjon\n\n\nHelsepersonellnummer\n\n\nOrganisasjonsnummer enkeltpersonsforetak\n\n\nForetakets navn\n\n\n\n\n\n\n\n\nSiden informasjon om ENK, en viktig del av nÃ¦ringstatistikken, er regnet som personopplysninger, var dette et viktig omrÃ¥de som mÃ¥tte avklares i PVK for nÃ¦ringsstatistikk. I tillegg kobles det andre personopplysninger, blant annet fra team som er definert under PVK for personstatistikk, som ytterligere kompliserer behandlingen av personopplysninger i statistikkproduksjon.\nKonklusjonen i PVK for nÃ¦ringsstatistikk er et klart skille mellom personopplysninger knyttet til populasjonsforvaltning og statistikkproduksjon. Det blir vurdert som nÃ¸dvendig at ansatte som jobber med populasjonsforvaltning alltid har tilgang til all informasjon i klartekst, dvs. at ingen personopplysninger skal pseudonymiseres. NÃ¥r det gjelder personopplysninger i statistikkproduksjons sÃ¥ er bildet noe mer sammensatt. FÃ¸lgende PII blir identifisert som tilstede i produksjon av nÃ¦ringsstatistikk:\n\nEnkeltpersonforetak (ENK).\nFysiske personer som innehar roller i foretak eller bedrifter.\nFysiske personer som er kontaktpersoner for virksomheter i rapporteringer til SSB.\nPersonopplysninger fra administrative registre og skjemaundersÃ¸kelser om fysiske personer som har roller i foretak eller virksomheter.\n\nDe tre fÃ¸rste punktene over blir vurdert som ikke nÃ¸dvendig Ã¥ pseudonymisere. Det siste punktet, som ofte innebÃ¦rer Ã¥ koble pÃ¥ informasjon fra team som er definert under PVK for personstatistikk, skal som hovedregel pseudonymiseres. I disse tilfellene skal man da som hovedregel pseudonymisere bÃ¥de PII og OII. Men det Ã¥pnes for at unntak kan gjÃ¸res i fÃ¸lgende scenarioer:\n\nDersom opplysninger om foretak/virksomhet (OII) mÃ¥ behandles upseudonymisert, sÃ¥ mÃ¥ bÃ¥de PII og OII behandles upseudonymisert3.\nHvis det er opplysninger om foretak/virksomhet som gjÃ¸r det enkelt Ã¥ identifisere enheten ut fra f.eks. geografisk tilhÃ¸righet, nÃ¦ringskode eller dominans innen nÃ¦ringen som utÃ¸ves, bÃ¸r bÃ¥de PII og OII behandles upseudonymisert.\n\nI sum betyr dette at PII i nÃ¦ringstatistikk kun skal pseudonymiseres der det kobles pÃ¥ personoppplysninger fra registre eller skjemaundersÃ¸kelser, og det ikke er nÃ¸dvendig med OII for Ã¥ produsere statistikken.\n\n\n\n\nOrganisasjonsidentifiserende informasjon (OII) er alle opplysninger som kan knyttes til en konkret virksomhet, foretak, selskap eller annen organisasjonsenhet. I SSB har det vÃ¦rt en lang tradisjon for at OII mÃ¥ kunne ses i klartekst, og at kravene er mindre strenge enn for personopplysninger. Men i statistikkloven av 2019 ble kravene til informasjonssikkerhet skjerpet, og Â§ 9 (2) inneholder en mer generell bestemmelse om at direkte identifiserende opplysninger skal behandles og lagres adskilt fra Ã¸vrige opplysninger. Statisikklovutvalget presiserer ogsÃ¥ fÃ¸lgende om intensjonen med bak bestemmelsen:\n\nByrÃ¥et behandler imidlertid ogsÃ¥ opplysninger som faller utenfor personopplysningsloven, inkludert markedssensitiv informasjon om foretak. Det bÃ¸r derfor tas inn en generell bestemmelse om informasjonssikkerhet i statistikkloven, som omfatter alle opplysninger. Det kan vÃ¦re grunn til Ã¥ behandle ulike opplysninger ulikt, og derfor bÃ¸r bestemmelsen vÃ¦re av noksÃ¥ generell karakter.\n\nI 2022 ble satt ned en gruppe som skulle evaluere hvordan SSB skulle behandle organisasjonsidentifiserende informasjon (OII) etter den nye statistikkloven fra 2019. Gruppa leverte en rapport med konklusjonen om at pseudonymisering av OII er uforenlig med formÃ¥let med behandlingen.\nKonklusjonen er at OII ikke skal pseudonymiseres.\n\n\n\nPÃ¥ Dapla jobber man i team som organisasjonen selv utformer. Team som definerer seg under PVK for nÃ¦ringsstatistikk mÃ¥ derfor ta hensyn til hvilke data ulike medarbeidere fÃ¥r tilgang til nÃ¥r de oppretter et team. F.eks. kan ikke de samme ansatte vÃ¦re med i team arbeider med PII i klartekst, samtidig som de er med i team der PII behandles i pseudonymisert form. I disse tilfellene mÃ¥ managers (som er ansvarlig for teamet) sikre at tilgang til data ikke avslÃ¸rer pseudonymer eller bryter regelverket.\n\n\n\nSiden team som definerer seg under PVK for personstatistikk pseudonymiserer bÃ¥de PII og OII, skaper dette hindringer for deling av data med team som definerer seg under PVK for nÃ¦ringstatistikk. F.eks. hvis fÃ¸rstnevnte deler et datasett med pseudonymer med sistnevnte, vil det vÃ¦re stor risiko for at pseudonymet blir avslÃ¸rt. I tillegg vil det vÃ¦re praktisk vanskelig Ã¥ koble data pÃ¥ tvers nÃ¥r koblingsnÃ¸kkelen er representert forskjellig. PÃ¥ nÃ¥vÃ¦rende tidspunkt finnes det ingen tjenester pÃ¥ Dapla som lar et team koble data der koblingsnÃ¸kkel er behandlet forskjellig.\n\n\n\n\nPseudonymisering skjer med python-biblioteket dapla-toolbelt-pseudo, og skal skje i overgangen fra kildedata til inndata. Brukeren kan sende inn skriptet som skal kjÃ¸res pÃ¥ dataene, men selve kjÃ¸ringen skjer automatisk i Kildomaten.\n\n\n\n\n\n\nIkke mulig Ã¥ pseudonymisere â€œmanueltâ€\n\n\n\nDet er ikke mulig Ã¥ pseudonymisere â€œmanueltâ€ fra en Jupyter Notebook eller lignende med ekte data i prod-miljÃ¸et. Grunnen til det er at det ville gitt brukeren/data-admins mulighet til Ã¥ printe ut bÃ¥de data i klartekst og pseudonymisert form, og pÃ¥ den mÃ¥ten avslÃ¸re pseudonymet. Av den grunn er det bare systembrukeren i Kildomaten som kan prosessere skarpe data. Hvis man Ã¸nsker Ã¥ teste koden sin manuelt fÃ¸r man produksjonssetter det i Kildomaten, sÃ¥ kan man benytte testdata i teamets test-miljÃ¸.\n\n\n\n\nI dette avsnittet viser vi et en eksempel pÃ¥ en typisk arbeidsflyt for et team skal pseudonymisere data. La oss anta at det finnes et team som heter dapla-example, som Ã¸nsker Ã¥ dataminimere og pseudonymisere sine kildedata i overgangen til inndata. Kort fortalt mÃ¥ teamet gjÃ¸re fÃ¸lgende:\n\nSÃ¸rg for at Kildomaten er aktivert i prod-miljÃ¸et.\nSkriv et script som skal kjÃ¸res pÃ¥ alle nye filer som kommer inn pÃ¥ en gitt filsti i kildebÃ¸tta.\nAutomatiser scriptet som en kilde i Kildomaten.\n\nPunkt 1 og 3 er nÃ¸ye forklart i kapitlet oom Kildomaten. Derfor fokuserer vi pÃ¥ skriving av selve sciptet under, deriblant bruken av **dapla-toolbelt-pseudo.\n\n\nAnta at team dapla-example fortlÃ¸pende fÃ¥r inn nye kildedatafiler4 i sin kildebÃ¸tte som vist i TabellÂ 2.\n\n\n\nTabellÂ 2: Kildedata for team dapla-example\n\n\n\n\n\nfÃ¸dselsnummer\nfornavn\netternavn\nadresse\ninntekt\n\n\n\n\n11111122222\nDonald\nDuck\nAndeby 15\n500000\n\n\n\n\n\n\nTeamet er definert under PVK for personstatistikk og Ã¸nsker dataminimere bort kolonnene fornavn, etternavn og adresse, mens de Ã¸nsker Ã¥ pseudonymisere fÃ¸dselsnummer og beholde inntekt som den er. Teamet skriver derfor fÃ¸lgende script:\n\n\nprocess_source_data.py\n\nimport dapla as dp\nfrom dapla_pseudo import Pseudonymize\n\n# Filsti til en kildedatafil\nsource_file = \"gs://ssb-dapla-example-data-kilde-prod/donald/andeby_inntekt_p2024_v1.parquet\"\n\n# Leser inn parquet-filen til en Pandas dataframe\ndf = dp.read_pandas(source_file)\n\n# Dataminimerer ved Ã¥ beholde to kolonner\ndf2 = df[['fÃ¸dselsnummer', 'inntekt']]\n\n# Pseudonymiserer fÃ¸dselsnummer\ndf3 = (\n    Pseudonymize.from_pandas(df2)                  \n    .on_fields(\"fÃ¸dselsnummer\")                    \n    .with_stable_id()                              \n    .run()                                     \n    .to_pandas()\n)\n\n# Skriv inndata til produktbÃ¸tte ved Ã¥ bytte ut et ord i bÃ¸ttenavnet\ninndata_file = source_file.replace(\"kilde\", \"produkt\")\ndp.write_pandas(inndata_file)\n\nI process_source_data.py-eksempelet over sÃ¥ leses det inn en konkret fil fra kildebÃ¸tta, deretter dataminimeres det ved Ã¥ kun beholde de to kolonne av interesse. SÃ¥ pseudonymiseres fÃ¸dselsnummer, og til slutt skrives en parquet-fil til produktbÃ¸tta.\nPseudonymize()-metoden fÃ¸lger et sÃ¥kalt builder-pattern der vi kan spesifisere hva som skal gjÃ¸res i hvilken rekkefÃ¸lge. I from_pandas() sier vi at dataene som skal brukes er en Pandas dataframe, i on_fields() spesifiseres hvilke kolonner som skal pseudonymiseres, og with_stable_id spesifiserer at vi Ã¸nsker Ã¥ oversette fnr til stabil ID og bruke samme krypteringsalgoritme som i Papis-prosjektet5. Til slutt ber vi om at alt blir kjÃ¸rt med run() og output skal vÃ¦re en Pandas dataframe med to_pandas().\nResultatet blir en dataframe som vist i TabellÂ 3.\n\n\n\nTabellÂ 3: Dataminimert og pseudonymisert inndata fra TabellÂ 2\n\n\n\n\n\nfÃ¸dselsnummer\ninntekt\n\n\n\n\n1a45x88\n500000\n\n\n\n\n\n\nSiden with_stable_id() bytter ut fÃ¸dselsnummer med stabil ID fÃ¸r den pseudonymiseres med en formatbevarende algoritme, og stabil ID alltid er syv karakterer lang, sÃ¥ fÃ¥r vi tilbake et pseudonym som er syv karakterer lang. Legg ogsÃ¥ merke til at pseudonymiseringen aldri endrer navn pÃ¥ kolonner eller datatyper.\nFor Ã¥ teste denne koden pÃ¥ noe data fra et verktÃ¸y som Jupyterlab, sÃ¥ mÃ¥ vi gjÃ¸re det i test-miljÃ¸et til teamet. Til det trenger vi noe testdata. I tilfellet med stabil ID, sÃ¥ mÃ¥ man da generere testdata med fÃ¸dselsnummer som finnes i test-versjonen av stabilID-katalogen (mer informasjon rundt dette kommer snart).\nFÃ¸r vi kan sende inn dette scriptet for automatisk prosessering i Kildomaten, sÃ¥ mÃ¥ vi tilpasse koden litt. Helt konkret mÃ¥ vi gjÃ¸re fÃ¸lgende:\n\nPakke koden inn i en main()-funksjon\nKvitte oss med hardkoding av stier i koden.\n\nI eksempelet over kan gjÃ¸re fÃ¸lgende for Ã¥ tilpasse koden til Kildomaten:\n\n\nprocess_source_data.py\n\ndef main(source_file):\n    import dapla as dp\n    from dapla_pseudo import Pseudonymize\n\n    # Leser inn parquet-filen til en Pandas dataframe\n    df = dp.read_pandas(source_file)\n\n    # Dataminimerer ved Ã¥ beholde to kolonner\n    df2 = df[['fÃ¸dselsnummer', 'inntekt']]\n\n    # Pseudonymiserer fÃ¸dselsnummer\n    df3 = (\n        Pseudonymize.from_pandas(df2)                  \n        .on_fields(\"fÃ¸dselsnummer\")                    \n        .with_stable_id()                              \n        .run()                                     \n        .to_pandas()\n    )\n\n    # Skriv inndata til produktbÃ¸tte ved Ã¥ bytte ut et ord i bÃ¸ttenavnet\n    inndata_file = source_file.replace(\"kilde\", \"produkt\")\n    dp.write_pandas(inndata_file)\n\nSiden Kildomaten trigges hver gang en ny fil dukker opp, sÃ¥ fÃ¥r main()-funksjonen filstien injisert hver gang en fil prosesseres. Du trenger derfor ikke Ã¥ definere denne selv, og derfor er denne delen av koden slettet. Og med de tilpassingene kan koden produksjonssettes i IaC-repoet til dapla-example, slik som vist her.\n\n\n\n\ndapla-toolbelt-pseudo er ferdig installert i Kildomaten. Men Ã¸nsker du Ã¥ bruke det i test-miljÃ¸et til teamet sÃ¥ kan du installere det i et ssb-project fra PyPI med denne kommandoen:\n\n\nTerminal\n\npoetry add dapla-toolbelt-pseudo\n\n\n\n\nPseudonymisering tilbys via Pseudonymize-metoden i dapla-toolbelt-pseudo. Den fÃ¸lger et builder-pattern der man spesifiserer hva og i hvilken rekkefÃ¸lge operasjonene skal gjÃ¸res. Anta at det finnes i en dataframe i minnet som heter df hvor kolonnen fnr skal pseudonymiseres. Da vil koden se slik ut:\nfrom dapla_pseudo import Pseudonymize\n\nresult_df = (\n    Pseudonymize.from_polars(df) \n    .on_fields(\"fnr\")\n    .with_default_encryption()                     \n    .run()                                \n    .to_polars()                                   \n)\nI koden over sÃ¥ angir from_polars(df) at kolonnen vi Ã¸nsker Ã¥ pseudonymisere ligger i en Polars dataframe i minnet og heter df. Deretter spesifiserer vi at kolonnen fnr er den som skal behandles med funksjonen on_fields(\"fnr\"). Videre angir with_default_encryption() at fnr skal pseudonymiseres med dapla-toolbelt-speudo sin standard-algoritme6. Til slutt ber vi om at det ovennevnte blir kjÃ¸rt med funksjonen run(), og at dataene skal returneres som en Polars dataframe med funksjonen to_polars().\nSe flere eksempler i dokumentasjonen.\n\n\n\nPseudonymisering tilbys via Depseudonymize-metoden i dapla-toolbelt-pseudo. Den fÃ¸lger et builder-pattern der man spesifiserer hva og i hvilken rekkefÃ¸lge operasjonene skal gjÃ¸res. Anta at det finnes i en Polars dataframe i minnet som heter df hvor kolonnen fnr skal de-pseudonymiseres. Da vil koden se slik ut:\nresult_df = (\n    Depseudonymize.from_polars(df)            \n    .on_fields(\"fnr\")                          \n    .with_default_encryption()                     \n    .run()                                         \n    .to_polars()                                   \n)\nOppbygningen av koden med Depseudonymize() er helt lik som for Pseudonomize(). Les beskrivelsen der for Ã¥ se hva de ulike funksjonskallene gjÃ¸r.\nDepseudonymize() fungerer ikke for data som er pseudponymisert med with_stable_id() enda. Kommer snart.\nSe flere eksempler i dokumentasjonen.\n\n\n\nIkke tilgjengelig enda.\n\n\n\nI statistikkproduksjon og forskning er det viktig Ã¥ kunne fÃ¸lge de samme personene over tid. Derfor har fÃ¸dselsnummer ofte blitt oversatt til en mer stabilt identifikator, ofte kalt SNR eller stabil ID7. Funksjonene with_stable_id() og validator() bruker stabilID-katalogen til Ã¥ henholdsvis bytte ut fÃ¸dselsnummer med stabil ID, og for Ã¥ validere om fÃ¸dselsnummer er gyldige(se under).\nDu kan selv spesifisere hvilken versjon av SNR-katalogen du Ã¸nsker Ã¥ bruke. Det gjÃ¸r du ved Ã¥ oppgi en gyldighetsdato og sÃ¥ finner dapla-toolbelt-pseudo hvilken versjon av katalogen som ligger nÃ¦rmest i tid.\n\n\n\nValidator-metoden kan benyttes til Ã¥ sjekke om fÃ¸dselsnummer finnes i SNR-katalogen (se over). Her kan man ogsÃ¥ spesifisere hvilken versjon av SNR-katalogen man Ã¸nsker Ã¥ bruke. Standard, hvis ingenting velges, er at siste tilgjengelige versjon velges. Under er et eksempel pÃ¥ hvordan man validerer fÃ¸dselsnummer for en gitt gyldighetsdato:\nfrom dapla_pseudo import Validator\nfrom dapla_pseudo.utils import convert_to_date\n\nresult = (\n    Validator.from_polars(df)\n    .on_field(\"fnr\")\n    .validate_map_to_stable_id(\n        sid_snapshot_date=convert_to_date(\"2023-08-29\")\n    )\n)\n# Vis hvilken versjon av SNR-katalogen som er benyttet\nresult.metadata\n# Vis fÃ¸dselsnummer som ikke fikk treff i SNR-katalogen som en Polars dataframe\nresult.to_polars()\n\n\n\ndapla-toolbelt-pseudo stÃ¸tter fÃ¸lgende dataformater:\n\ncsv (filformat)\njson (filformat)\nPolars dataframe (minnet)\nPandas dataframe (minnet)\n\nOver har vi vist hvordan vi leser data fra minnet, men det stÃ¸ttes ogsÃ¥ Ã¥ lese direkte fra filformatene csv og json. Under er et eksempel med en csv-fil:\nfrom dapla_pseudo import Pseudonymize\nfrom dapla import AuthClient\n\n\nfile_path=\"data/personer.csv\"\n\noptions = {\n    \"dtypes\": {\"fnr\": pl.Utf8, \"fornavn\": pl.Utf8, \"etternavn\": pl.Utf8, \"kjonn\": pl.Categorical, \"fodselsdato\": pl.Utf8}\n}\n\nresult_df = (\n    Pseudonymize.from_file(file_path)\n    .on_fields(\"fnr\")\n    .with_default_encryption()\n    .run()\n    .to_polars(**options)\n)\nSe flere eksempler i dokumentasjonen.\n\n\n\ndapla-toolbelt-pseudo stÃ¸tter mange forskjellige krypteringsalgoritmer. Les mer i dokumentasjonen.\n\n\n\nKommer snart.\n\n\n\n\nKommer snart."
  },
  {
    "objectID": "statistikkere/pseudonymisering.html#forberedelser",
    "href": "statistikkere/pseudonymisering.html#forberedelser",
    "title": "Pseudonymisering",
    "section": "",
    "text": "For Ã¥ pseudonymisere pÃ¥ Dapla mÃ¥ man vÃ¦re en del av Dapla-team og Kildomaten mÃ¥ vÃ¦re tilgjengeliggjort for teamet. Hvis du er usikker pÃ¥ om ditt team har tilgang til Kildomaten, sÃ¥ kan du sjekke dette selv i teamets IaC-repo. For hjelp til Ã¥ aktivere Kildomaten kan man kontakte Kundeservice."
  },
  {
    "objectID": "statistikkere/pseudonymisering.html#hva-skal-pseudonymiseres",
    "href": "statistikkere/pseudonymisering.html#hva-skal-pseudonymiseres",
    "title": "Pseudonymisering",
    "section": "",
    "text": "Hvert team er selv ansvarlig for at deres sensitiv data hÃ¥ndteres i henhold til lover og regler. I kontekst av pseudonymisering vil dette bety at teamet mÃ¥ finne ut av hvilken informasjon som skal pseudomymiseres i overgangen fra kildedata til inndata. All kildedata i SSB er klassifisert som sensitivt, og hvis man har kildedata som skal pseudonymiseres, sÃ¥ skal det skje i overgangen fra kildedata til inndata. Er man usikker pÃ¥ om man skal pseudonymisere eller ikke, eller hvilke lover og regler som gjelder for teamets kildedata, sÃ¥ kan man diskutere med personvernombudet og/eller ta kontakt med juristene i SSB.\n\n\n\n\n\n\nIkke glem dataminimering!\n\n\n\nEt av de viktigste tiltakene for Ã¥ verne om personvern i data er Ã¥ fjerne informasjon som ikke er strengt nÃ¸dvendig for formÃ¥let. Dataminimering bÃ¸r gjÃ¸res nÃ¥r data samles inn til SSB. I de tilfellene der det ikke er mulig, sÃ¥ skal det dataminimeres i overgangen fra kildedata til inndata. Etter at man har dataminimert sÃ¥ kan man vurdere hvilken gjenvÃ¦rende informasjon som skal pseudonymiseres.\n\n\n\n\nPersonidentifiserende informasjon (PII) er alle opplysninger som kan knyttes til en enkeltperson. Det fÃ¸lger av bÃ¥de personopplysningsloven og statistikkloven at personidentifiserende informasjon som samles inn for statistikkformÃ¥l skal pseudonymiseres. I SSB sÃ¥ er det gjennomfÃ¸rt personvernkonsekvensutredninger2 (PVK) for to omrÃ¥der som benytter seg av PII:\n\nPersonstatistikk\nNÃ¦ringsstatistikk\n\nUnder gÃ¥r vi gjennom hvordan PII skal behandels i de to tilfellene, og hvilke konsekvenser det har for samarbeid mellom team pÃ¥ Dapla.\n\n\n\n\nI SSB er det gjennomfÃ¸rt en PVK for personstatistikk som presiserer at all PII innen personstatistikk skal pseudonymiseres. Hvert team som hÃ¥ndterer personopplysninger mÃ¥ derfor vurdere sine kildedata og identifisere alt av PII. All data som innholder PII, og som ikke kan dataminimeres bort, skal pseudonymiseres.\nTabellÂ 1 viser en ikke-uttÃ¸mmende liste over PII som har vÃ¦rt identifisert i SSB tidligere. Merk at dette ikke er en fullstendig liste over PII, men hvis et team har denne informasjonen i sine data sÃ¥ skal de pseudonymiseres i overgangen fra kildedata til inndata.\nFor mange er fÃ¸dselsnummer en viktig variabel i kraft at den fungerer som en koblingsnÃ¸kkel for ulike typer persondata. Men som vist i TabellÂ 1 sÃ¥ er det mye annen informasjon som ogsÃ¥ er Ã¥ regne som PII. For eksempel er ogsÃ¥ adresse og bankkontonummer Ã¥ regne som PII.\n\n\nI TabellÂ 1 ser vi at organisasjonsnummeret til et enkeltpersonforetak (ENK) er Ã¥ regne som PII. Grunnen til det er at en ENK eies av en fysisk person og all informasjon knyttet til ENK er Ã¥ regne som personopplysninger. PVK for personstatistikk begrenser sine vurderinger til Ã¥ ikke inkludere informasjon om ENK.\nSiden personstatistikk skal pseudonymisere PII sÃ¥ fÃ¸lger det at ogsÃ¥ PII knyttet til ENK ogsÃ¥ bÃ¸r pseudonymiseres. Grunnen til det er at det finnes mye Ã¥pen informasjon om ENK, og derfor kan man lett knytte pseudonym til en person basert lett tilgjengelig informasjon. Kan man knytte et pseudonym til en person sÃ¥ kan pseudonymet anses Ã¥ vÃ¦re avslÃ¸rt. Selv om det samme ikke gjelder for andre organisasjonsformer (AS, ansvarlig selskap, etc.) sÃ¥ vil det vÃ¦re naturlig at disse ogsÃ¥ pseudonymiseres, blant annet for Ã¥ sikre ENKâ€™er ikke lett kan skilles ut.\n\n\n\nI SSB er det gjennomfÃ¸rt en egen PVK for nÃ¦ringsstatistikk. Grunnen til det er at det ogsÃ¥ her behandles personopplysninger, men at behovene i nÃ¦ringstatistikken er vurdert som sÃ¥pass annerledes sammenlignet med personstatistikk, at de gjennomfÃ¸rte en egen PVK. Et viktig bakteppe for denne vurderingen var at organisasjonsidentifiserende informasjon (OII) tidligere hadde blitt vurdert som nÃ¸dvendig Ã¥ se klarttekst. Mer om OII i neste avsnitt.\n\n\n\n\n\n\n\nTabellÂ 1: Ikke-uttÃ¸mmende liste over PII som har vÃ¦rt identifisert i SSB tidligere.\n\n\n\n\n\nPII\n\n\n\n\nFÃ¸dselsnummer\n\n\nD-nummer\n\n\nS-nummer\n\n\nEktefellenummer\n\n\nFamilienummer\n\n\nHusholdningsnummer\n\n\nDufnummer\n\n\nBankkontonummer\n\n\nAdresse (tekstlig)\n\n\nEiendomsidentifikator\n\n\nVeiadresse (numerisk)\n\n\nMatrikkeladresse (numerisk)\n\n\nRegistreringsnummer (kjÃ¸retÃ¸y)\n\n\nNavn\n\n\nKontaktinformasjon\n\n\nHelsepersonellnummer\n\n\nOrganisasjonsnummer enkeltpersonsforetak\n\n\nForetakets navn\n\n\n\n\n\n\n\n\nSiden informasjon om ENK, en viktig del av nÃ¦ringstatistikken, er regnet som personopplysninger, var dette et viktig omrÃ¥de som mÃ¥tte avklares i PVK for nÃ¦ringsstatistikk. I tillegg kobles det andre personopplysninger, blant annet fra team som er definert under PVK for personstatistikk, som ytterligere kompliserer behandlingen av personopplysninger i statistikkproduksjon.\nKonklusjonen i PVK for nÃ¦ringsstatistikk er et klart skille mellom personopplysninger knyttet til populasjonsforvaltning og statistikkproduksjon. Det blir vurdert som nÃ¸dvendig at ansatte som jobber med populasjonsforvaltning alltid har tilgang til all informasjon i klartekst, dvs. at ingen personopplysninger skal pseudonymiseres. NÃ¥r det gjelder personopplysninger i statistikkproduksjons sÃ¥ er bildet noe mer sammensatt. FÃ¸lgende PII blir identifisert som tilstede i produksjon av nÃ¦ringsstatistikk:\n\nEnkeltpersonforetak (ENK).\nFysiske personer som innehar roller i foretak eller bedrifter.\nFysiske personer som er kontaktpersoner for virksomheter i rapporteringer til SSB.\nPersonopplysninger fra administrative registre og skjemaundersÃ¸kelser om fysiske personer som har roller i foretak eller virksomheter.\n\nDe tre fÃ¸rste punktene over blir vurdert som ikke nÃ¸dvendig Ã¥ pseudonymisere. Det siste punktet, som ofte innebÃ¦rer Ã¥ koble pÃ¥ informasjon fra team som er definert under PVK for personstatistikk, skal som hovedregel pseudonymiseres. I disse tilfellene skal man da som hovedregel pseudonymisere bÃ¥de PII og OII. Men det Ã¥pnes for at unntak kan gjÃ¸res i fÃ¸lgende scenarioer:\n\nDersom opplysninger om foretak/virksomhet (OII) mÃ¥ behandles upseudonymisert, sÃ¥ mÃ¥ bÃ¥de PII og OII behandles upseudonymisert3.\nHvis det er opplysninger om foretak/virksomhet som gjÃ¸r det enkelt Ã¥ identifisere enheten ut fra f.eks. geografisk tilhÃ¸righet, nÃ¦ringskode eller dominans innen nÃ¦ringen som utÃ¸ves, bÃ¸r bÃ¥de PII og OII behandles upseudonymisert.\n\nI sum betyr dette at PII i nÃ¦ringstatistikk kun skal pseudonymiseres der det kobles pÃ¥ personoppplysninger fra registre eller skjemaundersÃ¸kelser, og det ikke er nÃ¸dvendig med OII for Ã¥ produsere statistikken.\n\n\n\n\nOrganisasjonsidentifiserende informasjon (OII) er alle opplysninger som kan knyttes til en konkret virksomhet, foretak, selskap eller annen organisasjonsenhet. I SSB har det vÃ¦rt en lang tradisjon for at OII mÃ¥ kunne ses i klartekst, og at kravene er mindre strenge enn for personopplysninger. Men i statistikkloven av 2019 ble kravene til informasjonssikkerhet skjerpet, og Â§ 9 (2) inneholder en mer generell bestemmelse om at direkte identifiserende opplysninger skal behandles og lagres adskilt fra Ã¸vrige opplysninger. Statisikklovutvalget presiserer ogsÃ¥ fÃ¸lgende om intensjonen med bak bestemmelsen:\n\nByrÃ¥et behandler imidlertid ogsÃ¥ opplysninger som faller utenfor personopplysningsloven, inkludert markedssensitiv informasjon om foretak. Det bÃ¸r derfor tas inn en generell bestemmelse om informasjonssikkerhet i statistikkloven, som omfatter alle opplysninger. Det kan vÃ¦re grunn til Ã¥ behandle ulike opplysninger ulikt, og derfor bÃ¸r bestemmelsen vÃ¦re av noksÃ¥ generell karakter.\n\nI 2022 ble satt ned en gruppe som skulle evaluere hvordan SSB skulle behandle organisasjonsidentifiserende informasjon (OII) etter den nye statistikkloven fra 2019. Gruppa leverte en rapport med konklusjonen om at pseudonymisering av OII er uforenlig med formÃ¥let med behandlingen.\nKonklusjonen er at OII ikke skal pseudonymiseres.\n\n\n\nPÃ¥ Dapla jobber man i team som organisasjonen selv utformer. Team som definerer seg under PVK for nÃ¦ringsstatistikk mÃ¥ derfor ta hensyn til hvilke data ulike medarbeidere fÃ¥r tilgang til nÃ¥r de oppretter et team. F.eks. kan ikke de samme ansatte vÃ¦re med i team arbeider med PII i klartekst, samtidig som de er med i team der PII behandles i pseudonymisert form. I disse tilfellene mÃ¥ managers (som er ansvarlig for teamet) sikre at tilgang til data ikke avslÃ¸rer pseudonymer eller bryter regelverket.\n\n\n\nSiden team som definerer seg under PVK for personstatistikk pseudonymiserer bÃ¥de PII og OII, skaper dette hindringer for deling av data med team som definerer seg under PVK for nÃ¦ringstatistikk. F.eks. hvis fÃ¸rstnevnte deler et datasett med pseudonymer med sistnevnte, vil det vÃ¦re stor risiko for at pseudonymet blir avslÃ¸rt. I tillegg vil det vÃ¦re praktisk vanskelig Ã¥ koble data pÃ¥ tvers nÃ¥r koblingsnÃ¸kkelen er representert forskjellig. PÃ¥ nÃ¥vÃ¦rende tidspunkt finnes det ingen tjenester pÃ¥ Dapla som lar et team koble data der koblingsnÃ¸kkel er behandlet forskjellig."
  },
  {
    "objectID": "statistikkere/pseudonymisering.html#funksjonalitet",
    "href": "statistikkere/pseudonymisering.html#funksjonalitet",
    "title": "Pseudonymisering",
    "section": "",
    "text": "Pseudonymisering skjer med python-biblioteket dapla-toolbelt-pseudo, og skal skje i overgangen fra kildedata til inndata. Brukeren kan sende inn skriptet som skal kjÃ¸res pÃ¥ dataene, men selve kjÃ¸ringen skjer automatisk i Kildomaten.\n\n\n\n\n\n\nIkke mulig Ã¥ pseudonymisere â€œmanueltâ€\n\n\n\nDet er ikke mulig Ã¥ pseudonymisere â€œmanueltâ€ fra en Jupyter Notebook eller lignende med ekte data i prod-miljÃ¸et. Grunnen til det er at det ville gitt brukeren/data-admins mulighet til Ã¥ printe ut bÃ¥de data i klartekst og pseudonymisert form, og pÃ¥ den mÃ¥ten avslÃ¸re pseudonymet. Av den grunn er det bare systembrukeren i Kildomaten som kan prosessere skarpe data. Hvis man Ã¸nsker Ã¥ teste koden sin manuelt fÃ¸r man produksjonssetter det i Kildomaten, sÃ¥ kan man benytte testdata i teamets test-miljÃ¸.\n\n\n\n\nI dette avsnittet viser vi et en eksempel pÃ¥ en typisk arbeidsflyt for et team skal pseudonymisere data. La oss anta at det finnes et team som heter dapla-example, som Ã¸nsker Ã¥ dataminimere og pseudonymisere sine kildedata i overgangen til inndata. Kort fortalt mÃ¥ teamet gjÃ¸re fÃ¸lgende:\n\nSÃ¸rg for at Kildomaten er aktivert i prod-miljÃ¸et.\nSkriv et script som skal kjÃ¸res pÃ¥ alle nye filer som kommer inn pÃ¥ en gitt filsti i kildebÃ¸tta.\nAutomatiser scriptet som en kilde i Kildomaten.\n\nPunkt 1 og 3 er nÃ¸ye forklart i kapitlet oom Kildomaten. Derfor fokuserer vi pÃ¥ skriving av selve sciptet under, deriblant bruken av **dapla-toolbelt-pseudo.\n\n\nAnta at team dapla-example fortlÃ¸pende fÃ¥r inn nye kildedatafiler4 i sin kildebÃ¸tte som vist i TabellÂ 2.\n\n\n\nTabellÂ 2: Kildedata for team dapla-example\n\n\n\n\n\nfÃ¸dselsnummer\nfornavn\netternavn\nadresse\ninntekt\n\n\n\n\n11111122222\nDonald\nDuck\nAndeby 15\n500000\n\n\n\n\n\n\nTeamet er definert under PVK for personstatistikk og Ã¸nsker dataminimere bort kolonnene fornavn, etternavn og adresse, mens de Ã¸nsker Ã¥ pseudonymisere fÃ¸dselsnummer og beholde inntekt som den er. Teamet skriver derfor fÃ¸lgende script:\n\n\nprocess_source_data.py\n\nimport dapla as dp\nfrom dapla_pseudo import Pseudonymize\n\n# Filsti til en kildedatafil\nsource_file = \"gs://ssb-dapla-example-data-kilde-prod/donald/andeby_inntekt_p2024_v1.parquet\"\n\n# Leser inn parquet-filen til en Pandas dataframe\ndf = dp.read_pandas(source_file)\n\n# Dataminimerer ved Ã¥ beholde to kolonner\ndf2 = df[['fÃ¸dselsnummer', 'inntekt']]\n\n# Pseudonymiserer fÃ¸dselsnummer\ndf3 = (\n    Pseudonymize.from_pandas(df2)                  \n    .on_fields(\"fÃ¸dselsnummer\")                    \n    .with_stable_id()                              \n    .run()                                     \n    .to_pandas()\n)\n\n# Skriv inndata til produktbÃ¸tte ved Ã¥ bytte ut et ord i bÃ¸ttenavnet\ninndata_file = source_file.replace(\"kilde\", \"produkt\")\ndp.write_pandas(inndata_file)\n\nI process_source_data.py-eksempelet over sÃ¥ leses det inn en konkret fil fra kildebÃ¸tta, deretter dataminimeres det ved Ã¥ kun beholde de to kolonne av interesse. SÃ¥ pseudonymiseres fÃ¸dselsnummer, og til slutt skrives en parquet-fil til produktbÃ¸tta.\nPseudonymize()-metoden fÃ¸lger et sÃ¥kalt builder-pattern der vi kan spesifisere hva som skal gjÃ¸res i hvilken rekkefÃ¸lge. I from_pandas() sier vi at dataene som skal brukes er en Pandas dataframe, i on_fields() spesifiseres hvilke kolonner som skal pseudonymiseres, og with_stable_id spesifiserer at vi Ã¸nsker Ã¥ oversette fnr til stabil ID og bruke samme krypteringsalgoritme som i Papis-prosjektet5. Til slutt ber vi om at alt blir kjÃ¸rt med run() og output skal vÃ¦re en Pandas dataframe med to_pandas().\nResultatet blir en dataframe som vist i TabellÂ 3.\n\n\n\nTabellÂ 3: Dataminimert og pseudonymisert inndata fra TabellÂ 2\n\n\n\n\n\nfÃ¸dselsnummer\ninntekt\n\n\n\n\n1a45x88\n500000\n\n\n\n\n\n\nSiden with_stable_id() bytter ut fÃ¸dselsnummer med stabil ID fÃ¸r den pseudonymiseres med en formatbevarende algoritme, og stabil ID alltid er syv karakterer lang, sÃ¥ fÃ¥r vi tilbake et pseudonym som er syv karakterer lang. Legg ogsÃ¥ merke til at pseudonymiseringen aldri endrer navn pÃ¥ kolonner eller datatyper.\nFor Ã¥ teste denne koden pÃ¥ noe data fra et verktÃ¸y som Jupyterlab, sÃ¥ mÃ¥ vi gjÃ¸re det i test-miljÃ¸et til teamet. Til det trenger vi noe testdata. I tilfellet med stabil ID, sÃ¥ mÃ¥ man da generere testdata med fÃ¸dselsnummer som finnes i test-versjonen av stabilID-katalogen (mer informasjon rundt dette kommer snart).\nFÃ¸r vi kan sende inn dette scriptet for automatisk prosessering i Kildomaten, sÃ¥ mÃ¥ vi tilpasse koden litt. Helt konkret mÃ¥ vi gjÃ¸re fÃ¸lgende:\n\nPakke koden inn i en main()-funksjon\nKvitte oss med hardkoding av stier i koden.\n\nI eksempelet over kan gjÃ¸re fÃ¸lgende for Ã¥ tilpasse koden til Kildomaten:\n\n\nprocess_source_data.py\n\ndef main(source_file):\n    import dapla as dp\n    from dapla_pseudo import Pseudonymize\n\n    # Leser inn parquet-filen til en Pandas dataframe\n    df = dp.read_pandas(source_file)\n\n    # Dataminimerer ved Ã¥ beholde to kolonner\n    df2 = df[['fÃ¸dselsnummer', 'inntekt']]\n\n    # Pseudonymiserer fÃ¸dselsnummer\n    df3 = (\n        Pseudonymize.from_pandas(df2)                  \n        .on_fields(\"fÃ¸dselsnummer\")                    \n        .with_stable_id()                              \n        .run()                                     \n        .to_pandas()\n    )\n\n    # Skriv inndata til produktbÃ¸tte ved Ã¥ bytte ut et ord i bÃ¸ttenavnet\n    inndata_file = source_file.replace(\"kilde\", \"produkt\")\n    dp.write_pandas(inndata_file)\n\nSiden Kildomaten trigges hver gang en ny fil dukker opp, sÃ¥ fÃ¥r main()-funksjonen filstien injisert hver gang en fil prosesseres. Du trenger derfor ikke Ã¥ definere denne selv, og derfor er denne delen av koden slettet. Og med de tilpassingene kan koden produksjonssettes i IaC-repoet til dapla-example, slik som vist her.\n\n\n\n\ndapla-toolbelt-pseudo er ferdig installert i Kildomaten. Men Ã¸nsker du Ã¥ bruke det i test-miljÃ¸et til teamet sÃ¥ kan du installere det i et ssb-project fra PyPI med denne kommandoen:\n\n\nTerminal\n\npoetry add dapla-toolbelt-pseudo\n\n\n\n\nPseudonymisering tilbys via Pseudonymize-metoden i dapla-toolbelt-pseudo. Den fÃ¸lger et builder-pattern der man spesifiserer hva og i hvilken rekkefÃ¸lge operasjonene skal gjÃ¸res. Anta at det finnes i en dataframe i minnet som heter df hvor kolonnen fnr skal pseudonymiseres. Da vil koden se slik ut:\nfrom dapla_pseudo import Pseudonymize\n\nresult_df = (\n    Pseudonymize.from_polars(df) \n    .on_fields(\"fnr\")\n    .with_default_encryption()                     \n    .run()                                \n    .to_polars()                                   \n)\nI koden over sÃ¥ angir from_polars(df) at kolonnen vi Ã¸nsker Ã¥ pseudonymisere ligger i en Polars dataframe i minnet og heter df. Deretter spesifiserer vi at kolonnen fnr er den som skal behandles med funksjonen on_fields(\"fnr\"). Videre angir with_default_encryption() at fnr skal pseudonymiseres med dapla-toolbelt-speudo sin standard-algoritme6. Til slutt ber vi om at det ovennevnte blir kjÃ¸rt med funksjonen run(), og at dataene skal returneres som en Polars dataframe med funksjonen to_polars().\nSe flere eksempler i dokumentasjonen.\n\n\n\nPseudonymisering tilbys via Depseudonymize-metoden i dapla-toolbelt-pseudo. Den fÃ¸lger et builder-pattern der man spesifiserer hva og i hvilken rekkefÃ¸lge operasjonene skal gjÃ¸res. Anta at det finnes i en Polars dataframe i minnet som heter df hvor kolonnen fnr skal de-pseudonymiseres. Da vil koden se slik ut:\nresult_df = (\n    Depseudonymize.from_polars(df)            \n    .on_fields(\"fnr\")                          \n    .with_default_encryption()                     \n    .run()                                         \n    .to_polars()                                   \n)\nOppbygningen av koden med Depseudonymize() er helt lik som for Pseudonomize(). Les beskrivelsen der for Ã¥ se hva de ulike funksjonskallene gjÃ¸r.\nDepseudonymize() fungerer ikke for data som er pseudponymisert med with_stable_id() enda. Kommer snart.\nSe flere eksempler i dokumentasjonen.\n\n\n\nIkke tilgjengelig enda.\n\n\n\nI statistikkproduksjon og forskning er det viktig Ã¥ kunne fÃ¸lge de samme personene over tid. Derfor har fÃ¸dselsnummer ofte blitt oversatt til en mer stabilt identifikator, ofte kalt SNR eller stabil ID7. Funksjonene with_stable_id() og validator() bruker stabilID-katalogen til Ã¥ henholdsvis bytte ut fÃ¸dselsnummer med stabil ID, og for Ã¥ validere om fÃ¸dselsnummer er gyldige(se under).\nDu kan selv spesifisere hvilken versjon av SNR-katalogen du Ã¸nsker Ã¥ bruke. Det gjÃ¸r du ved Ã¥ oppgi en gyldighetsdato og sÃ¥ finner dapla-toolbelt-pseudo hvilken versjon av katalogen som ligger nÃ¦rmest i tid.\n\n\n\nValidator-metoden kan benyttes til Ã¥ sjekke om fÃ¸dselsnummer finnes i SNR-katalogen (se over). Her kan man ogsÃ¥ spesifisere hvilken versjon av SNR-katalogen man Ã¸nsker Ã¥ bruke. Standard, hvis ingenting velges, er at siste tilgjengelige versjon velges. Under er et eksempel pÃ¥ hvordan man validerer fÃ¸dselsnummer for en gitt gyldighetsdato:\nfrom dapla_pseudo import Validator\nfrom dapla_pseudo.utils import convert_to_date\n\nresult = (\n    Validator.from_polars(df)\n    .on_field(\"fnr\")\n    .validate_map_to_stable_id(\n        sid_snapshot_date=convert_to_date(\"2023-08-29\")\n    )\n)\n# Vis hvilken versjon av SNR-katalogen som er benyttet\nresult.metadata\n# Vis fÃ¸dselsnummer som ikke fikk treff i SNR-katalogen som en Polars dataframe\nresult.to_polars()\n\n\n\ndapla-toolbelt-pseudo stÃ¸tter fÃ¸lgende dataformater:\n\ncsv (filformat)\njson (filformat)\nPolars dataframe (minnet)\nPandas dataframe (minnet)\n\nOver har vi vist hvordan vi leser data fra minnet, men det stÃ¸ttes ogsÃ¥ Ã¥ lese direkte fra filformatene csv og json. Under er et eksempel med en csv-fil:\nfrom dapla_pseudo import Pseudonymize\nfrom dapla import AuthClient\n\n\nfile_path=\"data/personer.csv\"\n\noptions = {\n    \"dtypes\": {\"fnr\": pl.Utf8, \"fornavn\": pl.Utf8, \"etternavn\": pl.Utf8, \"kjonn\": pl.Categorical, \"fodselsdato\": pl.Utf8}\n}\n\nresult_df = (\n    Pseudonymize.from_file(file_path)\n    .on_fields(\"fnr\")\n    .with_default_encryption()\n    .run()\n    .to_polars(**options)\n)\nSe flere eksempler i dokumentasjonen.\n\n\n\ndapla-toolbelt-pseudo stÃ¸tter mange forskjellige krypteringsalgoritmer. Les mer i dokumentasjonen.\n\n\n\nKommer snart."
  },
  {
    "objectID": "statistikkere/pseudonymisering.html#ytelse",
    "href": "statistikkere/pseudonymisering.html#ytelse",
    "title": "Pseudonymisering",
    "section": "",
    "text": "Kommer snart."
  },
  {
    "objectID": "statistikkere/pseudonymisering.html#footnotes",
    "href": "statistikkere/pseudonymisering.html#footnotes",
    "title": "Pseudonymisering",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPseudonymisering defineres her som Ã¥ erstatte spesifikke data med kunstige data (pseudonymer), mens de-pseudonymisering gjÃ¸r det samme bare motsatt vei. Re-pseudonymisering defineres som Ã¥ bytte ut et pseudonym med et annet, uten at brukeren nÃ¸dvendigvis fÃ¥r tilgang til den opprinnelige verdien. â†©ï¸\nPersonvernkonsekvensutredningen (PVK) er det norske ordet for den engelske betegnelsen Data Protection Impact Assessment (DPIA). Datatilsynet definerer en DPIA som en systematisk prosess, som identifiserer og evaluerer potensielle personvernkonsekvenser fra alle interessenters synsvinkel i et prosjekt, initiativ, foreslÃ¥tt system eller prosess. Les mer om DPIA i SSBâ†©ï¸\nGrunnen til dette er at hvis hvis PII er pseudonymisert og OII ikke, sÃ¥ vil det vÃ¦re lett Ã¥ avslÃ¸re pseudonymet.â†©ï¸\nFor Ã¥ holde eksempelet enkelt sÃ¥ kan vi anta at filene kommer i parquet-formatetâ†©ï¸\nPapis-prosjektet pseudonymiserte alle PII pÃ¥ bakken med et spesifikt formatbevarende krypteringsalgoritme. Dette kunne gjÃ¸res direkte pÃ¥ fÃ¸dselsnummer eller pÃ¥ stabil ID, ogsÃ¥ kjent som snr-nummer. For Ã¥ gjÃ¸re det lett Ã¥ migrere data til Dapla sÃ¥ stÃ¸tter ogsÃ¥ pseudonymisering pÃ¥ Dapla denne tilnÃ¦rmingen.â†©ï¸\nStandardalgoritmen i dapla-toolbelt-pseudo er den deterministiske krypteringsalgoritmen Deterministic Authenticated Encryption with Associated Data, eller DAEAD-algoritmen.â†©ï¸\nSNR-katalogen eies og tilbys av Team Register pÃ¥ Dapla.â†©ï¸"
  },
  {
    "objectID": "statistikkere/virtual-env.html",
    "href": "statistikkere/virtual-env.html",
    "title": "Virtuelle miljÃ¸er",
    "section": "",
    "text": "Et python virtuelt miljÃ¸ inneholder en spesifikk versjon av python og et sett med pakker. Pakkene er kun tilgjengelige nÃ¥r det virtuelle miljÃ¸et er aktivert. Dette gjÃ¸r at man ungÃ¥r avhengighetskonflikter pÃ¥ tvers av prosjekter.\nSe her for mer informasjon om virtuelle miljÃ¸er.\n\n\nDet er anbefalt Ã¥ benytte verktÃ¸yet poetry for Ã¥ administrere prosjekter og deres virtuelle miljÃ¸.\nPoetry setter opp virtuelt miljÃ¸, gjÃ¸r det enkelt Ã¥ oppdatere avhengigheter, sette versjonsbegrensninger og reprodusere prosjektet.\nPoetry gjÃ¸r dette ved Ã¥ lagre avhengigheters eksakte versjon i prosjektets â€œpoetry.lockâ€. Og eventuelle begrensninger i â€œpyproject.tomlâ€. Dette gjÃ¸r det enkelt for andre Ã¥ bygge prosjektet med akkurat de samme pakkene og begrensningene."
  },
  {
    "objectID": "statistikkere/virtual-env.html#python",
    "href": "statistikkere/virtual-env.html#python",
    "title": "Virtuelle miljÃ¸er",
    "section": "",
    "text": "Et python virtuelt miljÃ¸ inneholder en spesifikk versjon av python og et sett med pakker. Pakkene er kun tilgjengelige nÃ¥r det virtuelle miljÃ¸et er aktivert. Dette gjÃ¸r at man ungÃ¥r avhengighetskonflikter pÃ¥ tvers av prosjekter.\nSe her for mer informasjon om virtuelle miljÃ¸er.\n\n\nDet er anbefalt Ã¥ benytte verktÃ¸yet poetry for Ã¥ administrere prosjekter og deres virtuelle miljÃ¸.\nPoetry setter opp virtuelt miljÃ¸, gjÃ¸r det enkelt Ã¥ oppdatere avhengigheter, sette versjonsbegrensninger og reprodusere prosjektet.\nPoetry gjÃ¸r dette ved Ã¥ lagre avhengigheters eksakte versjon i prosjektets â€œpoetry.lockâ€. Og eventuelle begrensninger i â€œpyproject.tomlâ€. Dette gjÃ¸r det enkelt for andre Ã¥ bygge prosjektet med akkurat de samme pakkene og begrensningene."
  },
  {
    "objectID": "statistikkere/git-og-github.html",
    "href": "statistikkere/git-og-github.html",
    "title": "Git og Github",
    "section": "",
    "text": "I SSB anbefales det man versjonhÃ¥ndterer koden sin med Git og deler koden via GitHub. For Ã¥ lÃ¦re seg Ã¥ bruke disse verktÃ¸yene pÃ¥ en god mÃ¥te er det derfor viktig Ã¥ forstÃ¥ forskjellen mellom Git og Github. Helt overordnet er forskjellen fÃ¸lgende:\n\nGit er programvare som er installert pÃ¥ maskinen du jobber pÃ¥ og som sporer endringer i koden din.\nGitHub er et slags felles mappesystem pÃ¥ internett som lar deg dele og samarbeide med andre om kode.\n\nAv definisjonene over sÃ¥ skjÃ¸nner vi at det er Git som gir oss all funksjonalitet for Ã¥ lagre versjoner av koden vÃ¥r. GitHub er mer som et valg av mappesystem. Men mÃ¥ten kodemiljÃ¸ene vÃ¥re er satt opp pÃ¥ Dapla sÃ¥ har vi ingen fellesmappe som alle kan kjÃ¸re koden fra. Man utvikler kode i sin egen hjemmemappe, som bare du har tilgang til, og nÃ¥r du skal samarbeide med andre, sÃ¥ mÃ¥ du sende koden til GitHub. De du samarbeider med mÃ¥ deretter hente ned denne koden fÃ¸r de kan kjÃ¸re den.\nI dette kapittelet ser vi nÃ¦rmere pÃ¥ Git og Github og hvordan de er implementert i SSB. Selv om SSB har laget programmet ssb-project for Ã¥ gjÃ¸re det lettere Ã¥ bl.a. forholde seg til Git og GitHub, sÃ¥ vil vi dette kapittelet forklare nÃ¦rmere hvordan det funker uten dette hjelpemiddelet. ForhÃ¥pentligvis vil det gjÃ¸re det lettere Ã¥ hÃ¥ndtere mer kompliserte situasjoner som oppstÃ¥r i arbeidshverdagen som statistikker.\n\n\nGit er terminalprogram som installert pÃ¥ maskinen du jobber. Hvis man ikke liker Ã¥ bruke terminalen finnes det mange pek-og-klikk versjoner av Git, blant annet i Jupyterlab, SAS EG og RStudio. Men typisk vil det en eller annen gang oppstÃ¥ situasjoner der det ikke finnes lÃ¸sninger i pek-og-klikk versjonen, og man mÃ¥ ordne opp i terminalen. Av den grunn velger vi her Ã¥ fokusere pÃ¥ hvordan Git fungerer fra terminalen. Vi vil ogsÃ¥ fokusere pÃ¥ hvordan Git fungerer fra terminalen i Jupyterlab pÃ¥ Dapla.\n\n\nGit er en programvare for distribuert versjonshÃ¥ndtering av filer. Det vil si at den tar vare pÃ¥ historien til koden din. At Git er distribuert betyr at alle som jobber med koden har en kopi av koden hos seg selv. NÃ¥r man Ã¸nsker Ã¥ dele koden med andre, sÃ¥ laster man opp koden til et felles kodelager som GitHub. Typisk vil Git brukes til Ã¥ passe pÃ¥ historien til rene tekstfiler, f.eks. kode-script, hvor det gir mening Ã¥ se pÃ¥ forskjeller mellom filen pÃ¥ ulike tidspunkter. Men nÃ¥r det er sagt, sÃ¥ kan Git ogsÃ¥ brukes til Ã¥ fÃ¸lge med pÃ¥ endringer i andre filtyper, f.eks. binÃ¦re filer som bilder, PDF-filer, etc.. Men binÃ¦re filer er ikke sÃ¥ vanlig inkludere i Git-prosjekter siden forskjellene i historikken til filene er vanskelig Ã¥ forstÃ¥ for mennesker.\nMan aktiverer Git pÃ¥ en mappe i filsystemet sitt med kommandoen git init nÃ¥r man stÃ¥r i mappen som skal versjonshÃ¥nderes. Da vil Git versjonshÃ¥ndtere alle filer som er i den mappen og i eventuelle undermapper. NÃ¥r du sÃ¥ gjÃ¸r endringer pÃ¥ en fil i mappen, sÃ¥ vil Git registrere endringer. Ã˜nsker du at endringen skal bli et punkt i historikken til prosjektet, sÃ¥ mÃ¥ du fÃ¸rst legge til filen i Git med kommandoen git add filnavn. NÃ¥r du har gjort dette, sÃ¥ kan du lagre endringen med kommandoen git commit -m \"Din melding her\". NÃ¥r du har gjort dette, sÃ¥ vil endringen vÃ¦re lagret i Git. NÃ¥r du har gjort mange endringer, sÃ¥ kan du sende endringene til GitHub med kommandoen git push. NÃ¥r du har gjort dette, sÃ¥ vil endringene vÃ¦re synlige for alle som har tilgang til GitHub-prosjektet.\nI SSB anbefaler vi at du starter et nytt Git-prosjekt ved Ã¥ benytte ssb-project. Da vil du ikke bare aktivere Git, men du kan ogsÃ¥ fÃ¥ implementert en del andre gode praksiser for Ã¥ holde koden din ryddig, oversiktlig og sikker.\nMen fÃ¸r vi kan begynne Ã¥ bruke Git mÃ¥ vi konfigurere vÃ¥r egen bruker for Git, som er forklart i neste seksjon.\nLes mer om Git pÃ¥ https://git-scm.com/.\n\n\n\n\n\n\n\n\n\nBruk SSB sin konfigurasjon!\n\n\n\nAlle som bruker GitHub i SSB bÃ¸r bruke SSB sin Git-konfigurasjon. Hvis ikke kan man bl.a. risikere at output fra Notebooks blir pushet til GitHub. Du kan bruke SSBs konfigurasjon ved Ã¥ kjÃ¸re ssb-gitconfig.py i terminalen og svare pÃ¥ spÃ¸rsmÃ¥lene som dukker opp.\n\n\nFor Ã¥ jobbe med Git sÃ¥ mÃ¥ man konfigurere brukeren sin slik at Git vet hvem som gjÃ¸r endringer i koden. I praksis betyr det at du mÃ¥ ha filen .gitconfig pÃ¥ hjemmeomrÃ¥det ditt (f.eks. /home/jovyan/.gitconfig pÃ¥ Dapla) med noe grunnleggende informasjon:\n\n\n.gitconfig\n\n# /home/jovyan/.gitconfig pÃ¥ Dapla\n[user]\n        name = Ola Nordmann\n        email = &lt;ini&gt;@ssb.no\n\nMed denne konfigurasjonen sÃ¥ kan man bruke Git lokalt. Men skal man ogsÃ¥ bruke GitHub i SSB, dvs. dele kode med andre, sÃ¥ mÃ¥ man ogsÃ¥ legge til mer informasjon i .gitconfig. Heldigvis er det skrevet et script som gjÃ¸r dette for deg. For Ã¥ fÃ¥ anbefalt konfigurasjon for Git sÃ¥ kan du kjÃ¸re fÃ¸lgende kommando i terminalen:\n\n\nterminal\n\nssb-gitconfig.py\n\nDette scriptet vil spÃ¸rre deg om ditt brukernavn i SSB, og sÃ¥ vil det opprette en fil som heter .gitconfig i hjemmeomrÃ¥det ditt. Denne filen vil inneholde informasjon om brukernavn, e-post og tilgang til GitHub. I tillegg vil den sÃ¸rge for at ditt ssb-project ikke sender output fra notebooks til GitHub.\n\n\n\nPÃ¥ Dapla er det Jupyterlab som er utviklingsmiljÃ¸et for kode. I Jupyterlab vil man ofte jobbe i notebooks, og det har noen konsekvenser for versjonshÃ¥ndtering. En notebook er en ipynb-fil som inneholder bÃ¥de tekst og kode. Ã…pner vi disse filene i Jupyterlab sÃ¥ ser vi det kjente notebook-grensesnittet med kode- og markdown-celler. Men egentlig er det bare et grensesnitt bygd over en json-fil1. Denne json-filen inneholder metadata og masse annet som gjÃ¸r det vanskelig Ã¥ se forskjellen pÃ¥ en fil over tid. Dette er derfor noe som Ã¥ fikses fÃ¸r Git blir et nyttig verktÃ¸y for disse filene.\nI SSB sin anbefalte konfigurasjon setter opp nbdime for Ã¥ fÃ¥ leselige differ mellom notebooks. Har du satt opp Git med ssb-gitconfig.py sÃ¥ vil dette vÃ¦re automatisk satt opp for deg.\nDet finnes ogsÃ¥ alternativer til Ã¥ bruke nbdime. PÃ¥ Dapla er Jupytext installert for de som ikke Ã¸nsker Ã¥ versjonshÃ¥ndtere ipynb-filer, men heller vil bruke rene script-filer som .py- eller .R-filer. MÃ¥ten Jupytext gjÃ¸r dette pÃ¥ er konvertere f.eks. en ipynb-fil py-fil for deg, Den kan gjÃ¸re det automatisk nÃ¥r du lagrer, eller du kan gjÃ¸re det manuelt. Med denne tilnÃ¦rmingen sÃ¥ kan du be Git ignorere alle ipynb-filer og bare versjonshÃ¥ndtere de rene script.filene. Oppsett er Jupytext er ikke en del av SSB sin standard-konfigurasjon og er noe du mÃ¥ sett opp selv.\n\n\n\nGit er veldig sterkt verktÃ¸y med mange muligheter. Det er ikke tenkt at alt skal adresseres i dette kapitlet. Men det er noen vanlig operasjoner som er sÃ¥ vanlige at alle som jobber med kode i SSB bÃ¸r kjenne dem.\nVi har tidligere nevnt at kommandoen for Ã¥ aktivere versjonshÃ¥ndtering med Git pÃ¥ en mappe, er git init. Dette gjÃ¸res ogsÃ¥ automatisk nÃ¥r man oppretter et nytt ssb-project.\nHva skjer hvis man gjÃ¸r en endring i en fil i mappa? For det fÃ¸rste kan du kjÃ¸re kommandoen git status for Ã¥ se hvilke endringer Git har oppdaget. Men det betyr ikke at endringene er under versjonskontroll enda. For Ã¥ fortelle Git om at disse endringene skal registreres sÃ¥ mÃ¥ du kjÃ¸re git add &lt;filnavn&gt;. Men fortsatt er ikke endringene et punkt i historien til koden din. For Ã¥ gjÃ¸re det mÃ¥ du kjÃ¸re git commit -m \"Din melding her\". Ved Ã¥ gjÃ¸re det sÃ¥ har du laget et unikt punkt i historien til koden din, med en egen ID, som du kan gÃ¥ tilbake til senere hvis du Ã¸nsker.\nNÃ¥r man utvikler kode sÃ¥ gjÃ¸r man det fra sÃ¥kalte branches2. Hvis vi tenker oss at din eksisterende kodebase er stammen pÃ¥ et tre (ofte kalt master eller main), sÃ¥ legger Git opp til at man gjÃ¸r endringer pÃ¥ denne koden via branches eller grener av treet. Med andre ord holder vi stammen/master urÃ¸rt helt til vi vet at endringen fungerer som den skal. Til slutt merger vi den inn i master. Vi kan opprette en ny branch og gÃ¥ inn i den ved Ã¥ skrive git checkout -b &lt;branch navn&gt;. Da stÃ¥r du i en branch og kan bruke kommandoer som git add og git commit som vist tidligere.\nVi kan merge endringene fra vÃ¥r branch inn i main ved Ã¥ fÃ¸rst gÃ¥ inn i main, git switch main, og deretter skrive git merge &lt;branch navn&gt;. Men typisk vil ikke dette vÃ¦re fremgangsmÃ¥ten i SSB. NÃ¥r man er fornÃ¸yd med endringene i en branch, sÃ¥ vil man pushe den opp til GitHub, slik at en kollega kan vurdere om den skal merges inn i main. Dermed gjÃ¸res selve mergen i GitHub-grensenittet. Vi skal se nÃ¦rmere pÃ¥ GitHub i neste kapittel.\n\n\n\n\nGitHub er et nettsted som bl.a. fungerer som vÃ¥rt felles mappesystem for deling av kode. SSB har sin egen organisasjonskonto med navn statisticsnorway. Men selv om SSB har en organisasjonskonto pÃ¥ GitHub mÃ¥ alle ansatte opprette sin egen brukerprofil, og knytte den mot SSB sin organisasjonskonto. Under forklarer vi hvordan du gjÃ¸r dette.\n\n\nDette kapittelet er bare relevant hvis man ikke har en GitHub-brukerkonto fra fÃ¸r. For Ã¥ bruke ssb-project-programmet til Ã¥ generere et remote repo pÃ¥ GitHub mÃ¥ du ha en konto. Derfor starter vi med Ã¥ gjÃ¸re dette. Det er en engangsjobb og du trenger aldri gjÃ¸re det igjen.\n\n\n\n\n\n\nSSB har valgt Ã¥ ikke sette opp SSB-brukerne til de ansatte som GitHub-brukere. En viktig Ã¥rsak er at er en GitHub-konto ofte regnes som en del av den ansattes CV. For de som aldri har brukt GitHub fÃ¸r kan det virke fremmed, men det er nok en fordel pÃ¥ sikt nÃ¥r alle blir godt kjent med denne arbeidsformen.\n\n\n\nSlik gjÃ¸r du det:\n\nGÃ¥ til https://github.com/\nTrykk Sign up Ã¸verst i hÃ¸yre hjÃ¸rne\nI dialogboksen som Ã¥pnes, se FigurÂ 1, skriver du inn din e-postkonto og lager et passord. Dette trenger ikke vÃ¦re din SSB-bruker og e-post. Vi anbefaler at du bruker en personlig e-postkonto og velger ditt eget passord. Det samme gjelder brukernavn ogsÃ¥.\n\n\n\n\n\n\n\nFigurÂ 1: Dialogboks for opprettelse av GitHub-bruker.\n\n\n\nDu har nÃ¥ laget en egen GitHub-bruker. I neste steg skal vi knytte denne kontoen til din SSB-bruker.\n\n\n\nHvis du har fullfÃ¸rt forrige steg sÃ¥ har du nÃ¥ en GitHub-konto. Hvis du stÃ¥r pÃ¥ din profil-side sÃ¥ ser den ut som i FigurÂ 2.\n\n\n\n\n\n\nFigurÂ 2: Et eksempel pÃ¥ hjemmeomrÃ¥det til en GitHub-bruker\n\n\n\nDet neste vi mÃ¥ gjÃ¸re er Ã¥ aktivere 2-faktor autentisering, siden det er dette som benyttes i SSB. Hvis du stÃ¥r pÃ¥ siden i bildet over, sÃ¥ gjÃ¸r du fÃ¸lgende for Ã¥ aktivere 2-faktor autentisering mot GitHub:\n\n\n\nTrykk pÃ¥ den lille pilen Ã¸verst til hÃ¸yre og velg Settings(se FigurÂ 3).\nDeretter velger du Password and authentification i menyen til venstre.\nUnder Two-factor authentication trykker du pÃ¥ Enable.\n\n\n\n\n\n\n\n\n\nFigurÂ 3: Ã…pne settings for din GitHub-bruker.\n\n\n\n\n\n\nFigurÂ 4 viser dialogboksen som vises. Velg Enable two-factor authentification.\n\n\n\n\n\n\n\nFigurÂ 4: Dialogboks som Ã¥pnes nÃ¥r 2FA skrus pÃ¥ fÃ¸rste gang.\n\n\n\n\nFigurÂ 5 viser dialogboksen som vises for Ã¥ velge hvordan man skal autentisere seg. Her anbefales det Ã¥ velge Set up using an app, slik at du kan bruke Microsoft Authenticator-appen pÃ¥ din mobil.\n\n\n\n\n\n\n\nFigurÂ 5: Dialogboks for Ã¥ velge hvordan man skal autentisere seg med 2FA.\n\n\n\nFigurÂ 6 viser QR-koden som vises. Denne skal vi bruke i neste steg.\n\n\n\n\n\n\nFigurÂ 6: QR-kode som skannes av Microsoft Authenticator.\n\n\n\n\n\n\nStrekkoden over skal skannes i din Microsoft Authenticator-app pÃ¥ mobilen, som vist i FigurÂ 7. Ã…pne appen, trykk pÃ¥ Bekreftede ID-er, og til slutt trykk pÃ¥ Skann QR-kode. Deretter skanner du QR-koden fra punkt 5.\nNÃ¥r koden er skannet har du fÃ¥tt opp fÃ¸lgende bilde pÃ¥ appens hovedside (se bilde til hÃ¸yre). Skriv inn den 6-siffer koden pÃ¥ GitHub-siden med QR-koden.\nTil slutt lagrer du Recovery-codes et trygt sted som bare du har tilgang til.\n\n\n\n\n\n\n\n\n\nFigurÂ 7: Mobilappen Microsoft authenticator\n\n\n\n\n\nNÃ¥ har vi aktivert 2-faktor autentisering for GitHub og er klare til Ã¥ knytte vÃ¥r personlige konto til vÃ¥r SSB-bruker pÃ¥ SSBs â€œGithub organisationâ€ statisticsnorway.\n\n\n\nI forrige steg aktiverte vi 2-faktor autentisering for GitHub. Det neste vi mÃ¥ gjÃ¸re er Ã¥ koble oss til Single Sign On (SSO) for SSB sin organisasjon pÃ¥ GitHub:\n\nTrykk pÃ¥ lenken https://github.com/orgs/statisticsnorway/sso\nI dialogboksen som dukker opp trykker du pÃ¥ Continue, slik som vist i FigurÂ 8.\n\n\n\n\n\n\n\nFigurÂ 8: Single Sign on (SSO) for SSB sin organisasjon pÃ¥ GitHub\n\n\n\nNÃ¥r du har gjennomfÃ¸rt dette sÃ¥ har du tilgang til statisticsnorway pÃ¥ GitHub. GÃ¥r du inn pÃ¥ denne lenken sÃ¥ skal du nÃ¥ kunne lese bÃ¥de Public, Private og Internal repoer, slik som vist i FigurÂ 9.\n\n\n\n\n\n\nFigurÂ 9: Medlemsvisning for SSB sin GitHub-organisasjon.\n\n\n\n\n\n\nNÃ¥r vi skal jobbe med SSB-kode som ligger lagret hos statistcsnorway pÃ¥ GitHub, sÃ¥ mÃ¥ vi autentisere oss. MÃ¥ten vi gjÃ¸re det pÃ¥ er ved Ã¥ generere et Personal Access Token (ofte forkortet PAT) som vi oppgir nÃ¥r vi vil hente eller oppdatere kode pÃ¥ GitHub. Da sender vi med PAT for Ã¥ autentisere oss for GitHub.\n\n\nFor Ã¥ lage en PAT som er godkjent mot statisticsnorway sÃ¥ gjÃ¸r man fÃ¸lgende:\n\nGÃ¥ til din profilside pÃ¥ GitHub og Ã¥pne Settings slik som ble vist SeksjonÂ 1.2.2.\nVelg Developer Settings i menyen til venstre.\nI menyen til venstre velger du Personal Access Token, og deretter Tokens (classic).\nVelg Generate new token og deretter Generate new token (classic).\nUnder Note kan du gi PATâ€™en et navn. Velg et navn som er intuitivt for deg. Hvis du skal bruke PAT til Ã¥ jobbe mot Dapla, sÃ¥ ville jeg ganske enkelt kalt den dapla. Hvis du skal bruke den mot bakkemiljÃ¸et ville jeg kalt den prodsone eller noe annet som gjÃ¸r det lett for det skjÃ¸nne innholdet i ettertid.\nUnder Expiration velger du hvor lang tid som skal gÃ¥ fÃ¸r PAT blir ugyldig. Dette er en avvening mellom sikkerhet og hva som er praktisk. Det anbefales at du velger 365 dager. NÃ¥r PAT utlÃ¸per mÃ¥ du gjenta stegene i dette kapittelet.\nUnder Select scopes velger du repo og workflow slik som vist i FigurÂ 10.\n\n\n\n\n\n\n\nFigurÂ 10: Gi token et kort og beskrivende navn\n\n\n\n\nTrykk pÃ¥ Generate token nederst pÃ¥ siden og du fÃ¥r noe lignende det du ser i FigurÂ 11.\n\n\n\n\n\n\n\nFigurÂ 11: Token som ble generert.\n\n\n\n\nKopier deretter PAT til en midlertidig fil. Grunnen er at du aldri vil se det igjen her etter at vi har gjennomfÃ¸rt neste steg.\nDeretter trykker du pÃ¥ Configure SSO og velger Authorize ved siden statisticsnorway, slik som vist i FigurÂ 12. Svar deretter pÃ¥ spÃ¸rsmÃ¥lene som dukker opp.\n\n\n\n\n\n\n\nFigurÂ 12: Autorisering av Token mot SSBs GiHub-organisasjon.\n\n\n\nVi har nÃ¥ opprettet en PAT som er godkjent for bruk mot SSB sin kode pÃ¥ GitHub. Det betyr at hvis vi vil jobbe med Git pÃ¥ SSB sine maskiner i sky eller pÃ¥ bakken, sÃ¥ mÃ¥ vi sendte med dette tokenet for Ã¥ fÃ¥ lov til Ã¥ jobbe med koden som ligger pÃ¥ statisticsnorway pÃ¥ GitHub.\n\n\n\nDet er ganske upraktisk Ã¥ mÃ¥tte sende med tokenet hver gang vi skal jobbe med GitHub. Vi bÃ¸r derfor lagre det lokalt der vi jobber, slik at Git automatisk finner det. Det finnes mange mÃ¥ter Ã¥ gjÃ¸re dette pÃ¥ og det er ikke bestemt hva som skal vÃ¦re beste-praksis i SSB. Men en mÃ¥te Ã¥ gjÃ¸re det er via en .netrc-fil. Vi oppretter da en fil som heter .netrc pÃ¥ vÃ¥rt hjemmeomrÃ¥de, og legger fÃ¸lgende informasjon pÃ¥ en (hvilken som helst) linje i filen:\n\n\n~/.netrc\n\nmachine github.com login &lt;github-bruker&gt; password &lt;Personal Access Token&gt;\n\nGitHub-bruker er da din personlige bruker og IKKE brukernavnet ditt i SSB. Personal Access Token er det vi lagde i forrige kapittelet.\nEn veldig enkel mÃ¥te Ã¥ lagre dette er som fÃ¸lger. Anta at min personlige GitHub-bruker er SSB-Chad og at min Personal Access Token er blablabla. Da kan jeg gjÃ¸re fÃ¸lgende for Ã¥ lagre det i .netrc:\n\nGÃ¥ inn i Jupyterlab og Ã¥pne en Python-notebook.\nI den fÃ¸rste kodecellen skriver du:\n\n\n\nterminal\n\n!echo \"machine github.com login SSB-Chad password blablabla\" &gt;&gt; ~/.netrc\n\nAlternativt kan du droppe det utropstegnet og kjÃ¸re det direkte i en terminal. Det vil gi samme resultat. Koden over legger til en linje med teksten\nmachine github.com login SSB-Chad password blablabla\ni en .netrc-fil pÃ¥ din hjemmeomrÃ¥det, uanvhengig av om du har en fra fÃ¸r eller ikke. Hvis du har en fil fra fÃ¸r som allerede har et token fra GitHub, ville jeg nok slettet det fÃ¸r jeg legger en et nytt token.\nHver gang du jobber mot GitHub vil Git sjekke om informasjon om autentisering ligger i denne filen, og bruke den hvis den ligger der.\n\n\n\nI eksempelet over lagde vi en PAT som var gyldig i 90 dager. Dermed vil du ikke kunne jobbe mot GitHub med dette tokenet etter 90 dager. For Ã¥ oppdatere tokenet gjÃ¸r du fÃ¸lgende:\n\nLag et nytt PAT ved Ã¥ repetere SeksjonÂ 1.2.4.1.\nI miljÃ¸et der du skal jobbe med Git og GitHub gÃ¥r du inn i din .netrc og bytter ut token med det nye.\n\nOg med det er du klar til Ã¥ jobbe mot statisticsnorway pÃ¥ GitHub.",
    "crumbs": [
      "Statistikere",
      "Programmering",
      "Git og Github"
    ]
  },
  {
    "objectID": "statistikkere/git-og-github.html#git",
    "href": "statistikkere/git-og-github.html#git",
    "title": "Git og Github",
    "section": "",
    "text": "Git er terminalprogram som installert pÃ¥ maskinen du jobber. Hvis man ikke liker Ã¥ bruke terminalen finnes det mange pek-og-klikk versjoner av Git, blant annet i Jupyterlab, SAS EG og RStudio. Men typisk vil det en eller annen gang oppstÃ¥ situasjoner der det ikke finnes lÃ¸sninger i pek-og-klikk versjonen, og man mÃ¥ ordne opp i terminalen. Av den grunn velger vi her Ã¥ fokusere pÃ¥ hvordan Git fungerer fra terminalen. Vi vil ogsÃ¥ fokusere pÃ¥ hvordan Git fungerer fra terminalen i Jupyterlab pÃ¥ Dapla.\n\n\nGit er en programvare for distribuert versjonshÃ¥ndtering av filer. Det vil si at den tar vare pÃ¥ historien til koden din. At Git er distribuert betyr at alle som jobber med koden har en kopi av koden hos seg selv. NÃ¥r man Ã¸nsker Ã¥ dele koden med andre, sÃ¥ laster man opp koden til et felles kodelager som GitHub. Typisk vil Git brukes til Ã¥ passe pÃ¥ historien til rene tekstfiler, f.eks. kode-script, hvor det gir mening Ã¥ se pÃ¥ forskjeller mellom filen pÃ¥ ulike tidspunkter. Men nÃ¥r det er sagt, sÃ¥ kan Git ogsÃ¥ brukes til Ã¥ fÃ¸lge med pÃ¥ endringer i andre filtyper, f.eks. binÃ¦re filer som bilder, PDF-filer, etc.. Men binÃ¦re filer er ikke sÃ¥ vanlig inkludere i Git-prosjekter siden forskjellene i historikken til filene er vanskelig Ã¥ forstÃ¥ for mennesker.\nMan aktiverer Git pÃ¥ en mappe i filsystemet sitt med kommandoen git init nÃ¥r man stÃ¥r i mappen som skal versjonshÃ¥nderes. Da vil Git versjonshÃ¥ndtere alle filer som er i den mappen og i eventuelle undermapper. NÃ¥r du sÃ¥ gjÃ¸r endringer pÃ¥ en fil i mappen, sÃ¥ vil Git registrere endringer. Ã˜nsker du at endringen skal bli et punkt i historikken til prosjektet, sÃ¥ mÃ¥ du fÃ¸rst legge til filen i Git med kommandoen git add filnavn. NÃ¥r du har gjort dette, sÃ¥ kan du lagre endringen med kommandoen git commit -m \"Din melding her\". NÃ¥r du har gjort dette, sÃ¥ vil endringen vÃ¦re lagret i Git. NÃ¥r du har gjort mange endringer, sÃ¥ kan du sende endringene til GitHub med kommandoen git push. NÃ¥r du har gjort dette, sÃ¥ vil endringene vÃ¦re synlige for alle som har tilgang til GitHub-prosjektet.\nI SSB anbefaler vi at du starter et nytt Git-prosjekt ved Ã¥ benytte ssb-project. Da vil du ikke bare aktivere Git, men du kan ogsÃ¥ fÃ¥ implementert en del andre gode praksiser for Ã¥ holde koden din ryddig, oversiktlig og sikker.\nMen fÃ¸r vi kan begynne Ã¥ bruke Git mÃ¥ vi konfigurere vÃ¥r egen bruker for Git, som er forklart i neste seksjon.\nLes mer om Git pÃ¥ https://git-scm.com/.\n\n\n\n\n\n\n\n\n\nBruk SSB sin konfigurasjon!\n\n\n\nAlle som bruker GitHub i SSB bÃ¸r bruke SSB sin Git-konfigurasjon. Hvis ikke kan man bl.a. risikere at output fra Notebooks blir pushet til GitHub. Du kan bruke SSBs konfigurasjon ved Ã¥ kjÃ¸re ssb-gitconfig.py i terminalen og svare pÃ¥ spÃ¸rsmÃ¥lene som dukker opp.\n\n\nFor Ã¥ jobbe med Git sÃ¥ mÃ¥ man konfigurere brukeren sin slik at Git vet hvem som gjÃ¸r endringer i koden. I praksis betyr det at du mÃ¥ ha filen .gitconfig pÃ¥ hjemmeomrÃ¥det ditt (f.eks. /home/jovyan/.gitconfig pÃ¥ Dapla) med noe grunnleggende informasjon:\n\n\n.gitconfig\n\n# /home/jovyan/.gitconfig pÃ¥ Dapla\n[user]\n        name = Ola Nordmann\n        email = &lt;ini&gt;@ssb.no\n\nMed denne konfigurasjonen sÃ¥ kan man bruke Git lokalt. Men skal man ogsÃ¥ bruke GitHub i SSB, dvs. dele kode med andre, sÃ¥ mÃ¥ man ogsÃ¥ legge til mer informasjon i .gitconfig. Heldigvis er det skrevet et script som gjÃ¸r dette for deg. For Ã¥ fÃ¥ anbefalt konfigurasjon for Git sÃ¥ kan du kjÃ¸re fÃ¸lgende kommando i terminalen:\n\n\nterminal\n\nssb-gitconfig.py\n\nDette scriptet vil spÃ¸rre deg om ditt brukernavn i SSB, og sÃ¥ vil det opprette en fil som heter .gitconfig i hjemmeomrÃ¥det ditt. Denne filen vil inneholde informasjon om brukernavn, e-post og tilgang til GitHub. I tillegg vil den sÃ¸rge for at ditt ssb-project ikke sender output fra notebooks til GitHub.\n\n\n\nPÃ¥ Dapla er det Jupyterlab som er utviklingsmiljÃ¸et for kode. I Jupyterlab vil man ofte jobbe i notebooks, og det har noen konsekvenser for versjonshÃ¥ndtering. En notebook er en ipynb-fil som inneholder bÃ¥de tekst og kode. Ã…pner vi disse filene i Jupyterlab sÃ¥ ser vi det kjente notebook-grensesnittet med kode- og markdown-celler. Men egentlig er det bare et grensesnitt bygd over en json-fil1. Denne json-filen inneholder metadata og masse annet som gjÃ¸r det vanskelig Ã¥ se forskjellen pÃ¥ en fil over tid. Dette er derfor noe som Ã¥ fikses fÃ¸r Git blir et nyttig verktÃ¸y for disse filene.\nI SSB sin anbefalte konfigurasjon setter opp nbdime for Ã¥ fÃ¥ leselige differ mellom notebooks. Har du satt opp Git med ssb-gitconfig.py sÃ¥ vil dette vÃ¦re automatisk satt opp for deg.\nDet finnes ogsÃ¥ alternativer til Ã¥ bruke nbdime. PÃ¥ Dapla er Jupytext installert for de som ikke Ã¸nsker Ã¥ versjonshÃ¥ndtere ipynb-filer, men heller vil bruke rene script-filer som .py- eller .R-filer. MÃ¥ten Jupytext gjÃ¸r dette pÃ¥ er konvertere f.eks. en ipynb-fil py-fil for deg, Den kan gjÃ¸re det automatisk nÃ¥r du lagrer, eller du kan gjÃ¸re det manuelt. Med denne tilnÃ¦rmingen sÃ¥ kan du be Git ignorere alle ipynb-filer og bare versjonshÃ¥ndtere de rene script.filene. Oppsett er Jupytext er ikke en del av SSB sin standard-konfigurasjon og er noe du mÃ¥ sett opp selv.\n\n\n\nGit er veldig sterkt verktÃ¸y med mange muligheter. Det er ikke tenkt at alt skal adresseres i dette kapitlet. Men det er noen vanlig operasjoner som er sÃ¥ vanlige at alle som jobber med kode i SSB bÃ¸r kjenne dem.\nVi har tidligere nevnt at kommandoen for Ã¥ aktivere versjonshÃ¥ndtering med Git pÃ¥ en mappe, er git init. Dette gjÃ¸res ogsÃ¥ automatisk nÃ¥r man oppretter et nytt ssb-project.\nHva skjer hvis man gjÃ¸r en endring i en fil i mappa? For det fÃ¸rste kan du kjÃ¸re kommandoen git status for Ã¥ se hvilke endringer Git har oppdaget. Men det betyr ikke at endringene er under versjonskontroll enda. For Ã¥ fortelle Git om at disse endringene skal registreres sÃ¥ mÃ¥ du kjÃ¸re git add &lt;filnavn&gt;. Men fortsatt er ikke endringene et punkt i historien til koden din. For Ã¥ gjÃ¸re det mÃ¥ du kjÃ¸re git commit -m \"Din melding her\". Ved Ã¥ gjÃ¸re det sÃ¥ har du laget et unikt punkt i historien til koden din, med en egen ID, som du kan gÃ¥ tilbake til senere hvis du Ã¸nsker.\nNÃ¥r man utvikler kode sÃ¥ gjÃ¸r man det fra sÃ¥kalte branches2. Hvis vi tenker oss at din eksisterende kodebase er stammen pÃ¥ et tre (ofte kalt master eller main), sÃ¥ legger Git opp til at man gjÃ¸r endringer pÃ¥ denne koden via branches eller grener av treet. Med andre ord holder vi stammen/master urÃ¸rt helt til vi vet at endringen fungerer som den skal. Til slutt merger vi den inn i master. Vi kan opprette en ny branch og gÃ¥ inn i den ved Ã¥ skrive git checkout -b &lt;branch navn&gt;. Da stÃ¥r du i en branch og kan bruke kommandoer som git add og git commit som vist tidligere.\nVi kan merge endringene fra vÃ¥r branch inn i main ved Ã¥ fÃ¸rst gÃ¥ inn i main, git switch main, og deretter skrive git merge &lt;branch navn&gt;. Men typisk vil ikke dette vÃ¦re fremgangsmÃ¥ten i SSB. NÃ¥r man er fornÃ¸yd med endringene i en branch, sÃ¥ vil man pushe den opp til GitHub, slik at en kollega kan vurdere om den skal merges inn i main. Dermed gjÃ¸res selve mergen i GitHub-grensenittet. Vi skal se nÃ¦rmere pÃ¥ GitHub i neste kapittel.",
    "crumbs": [
      "Statistikere",
      "Programmering",
      "Git og Github"
    ]
  },
  {
    "objectID": "statistikkere/git-og-github.html#github",
    "href": "statistikkere/git-og-github.html#github",
    "title": "Git og Github",
    "section": "",
    "text": "GitHub er et nettsted som bl.a. fungerer som vÃ¥rt felles mappesystem for deling av kode. SSB har sin egen organisasjonskonto med navn statisticsnorway. Men selv om SSB har en organisasjonskonto pÃ¥ GitHub mÃ¥ alle ansatte opprette sin egen brukerprofil, og knytte den mot SSB sin organisasjonskonto. Under forklarer vi hvordan du gjÃ¸r dette.\n\n\nDette kapittelet er bare relevant hvis man ikke har en GitHub-brukerkonto fra fÃ¸r. For Ã¥ bruke ssb-project-programmet til Ã¥ generere et remote repo pÃ¥ GitHub mÃ¥ du ha en konto. Derfor starter vi med Ã¥ gjÃ¸re dette. Det er en engangsjobb og du trenger aldri gjÃ¸re det igjen.\n\n\n\n\n\n\nSSB har valgt Ã¥ ikke sette opp SSB-brukerne til de ansatte som GitHub-brukere. En viktig Ã¥rsak er at er en GitHub-konto ofte regnes som en del av den ansattes CV. For de som aldri har brukt GitHub fÃ¸r kan det virke fremmed, men det er nok en fordel pÃ¥ sikt nÃ¥r alle blir godt kjent med denne arbeidsformen.\n\n\n\nSlik gjÃ¸r du det:\n\nGÃ¥ til https://github.com/\nTrykk Sign up Ã¸verst i hÃ¸yre hjÃ¸rne\nI dialogboksen som Ã¥pnes, se FigurÂ 1, skriver du inn din e-postkonto og lager et passord. Dette trenger ikke vÃ¦re din SSB-bruker og e-post. Vi anbefaler at du bruker en personlig e-postkonto og velger ditt eget passord. Det samme gjelder brukernavn ogsÃ¥.\n\n\n\n\n\n\n\nFigurÂ 1: Dialogboks for opprettelse av GitHub-bruker.\n\n\n\nDu har nÃ¥ laget en egen GitHub-bruker. I neste steg skal vi knytte denne kontoen til din SSB-bruker.\n\n\n\nHvis du har fullfÃ¸rt forrige steg sÃ¥ har du nÃ¥ en GitHub-konto. Hvis du stÃ¥r pÃ¥ din profil-side sÃ¥ ser den ut som i FigurÂ 2.\n\n\n\n\n\n\nFigurÂ 2: Et eksempel pÃ¥ hjemmeomrÃ¥det til en GitHub-bruker\n\n\n\nDet neste vi mÃ¥ gjÃ¸re er Ã¥ aktivere 2-faktor autentisering, siden det er dette som benyttes i SSB. Hvis du stÃ¥r pÃ¥ siden i bildet over, sÃ¥ gjÃ¸r du fÃ¸lgende for Ã¥ aktivere 2-faktor autentisering mot GitHub:\n\n\n\nTrykk pÃ¥ den lille pilen Ã¸verst til hÃ¸yre og velg Settings(se FigurÂ 3).\nDeretter velger du Password and authentification i menyen til venstre.\nUnder Two-factor authentication trykker du pÃ¥ Enable.\n\n\n\n\n\n\n\n\n\nFigurÂ 3: Ã…pne settings for din GitHub-bruker.\n\n\n\n\n\n\nFigurÂ 4 viser dialogboksen som vises. Velg Enable two-factor authentification.\n\n\n\n\n\n\n\nFigurÂ 4: Dialogboks som Ã¥pnes nÃ¥r 2FA skrus pÃ¥ fÃ¸rste gang.\n\n\n\n\nFigurÂ 5 viser dialogboksen som vises for Ã¥ velge hvordan man skal autentisere seg. Her anbefales det Ã¥ velge Set up using an app, slik at du kan bruke Microsoft Authenticator-appen pÃ¥ din mobil.\n\n\n\n\n\n\n\nFigurÂ 5: Dialogboks for Ã¥ velge hvordan man skal autentisere seg med 2FA.\n\n\n\nFigurÂ 6 viser QR-koden som vises. Denne skal vi bruke i neste steg.\n\n\n\n\n\n\nFigurÂ 6: QR-kode som skannes av Microsoft Authenticator.\n\n\n\n\n\n\nStrekkoden over skal skannes i din Microsoft Authenticator-app pÃ¥ mobilen, som vist i FigurÂ 7. Ã…pne appen, trykk pÃ¥ Bekreftede ID-er, og til slutt trykk pÃ¥ Skann QR-kode. Deretter skanner du QR-koden fra punkt 5.\nNÃ¥r koden er skannet har du fÃ¥tt opp fÃ¸lgende bilde pÃ¥ appens hovedside (se bilde til hÃ¸yre). Skriv inn den 6-siffer koden pÃ¥ GitHub-siden med QR-koden.\nTil slutt lagrer du Recovery-codes et trygt sted som bare du har tilgang til.\n\n\n\n\n\n\n\n\n\nFigurÂ 7: Mobilappen Microsoft authenticator\n\n\n\n\n\nNÃ¥ har vi aktivert 2-faktor autentisering for GitHub og er klare til Ã¥ knytte vÃ¥r personlige konto til vÃ¥r SSB-bruker pÃ¥ SSBs â€œGithub organisationâ€ statisticsnorway.\n\n\n\nI forrige steg aktiverte vi 2-faktor autentisering for GitHub. Det neste vi mÃ¥ gjÃ¸re er Ã¥ koble oss til Single Sign On (SSO) for SSB sin organisasjon pÃ¥ GitHub:\n\nTrykk pÃ¥ lenken https://github.com/orgs/statisticsnorway/sso\nI dialogboksen som dukker opp trykker du pÃ¥ Continue, slik som vist i FigurÂ 8.\n\n\n\n\n\n\n\nFigurÂ 8: Single Sign on (SSO) for SSB sin organisasjon pÃ¥ GitHub\n\n\n\nNÃ¥r du har gjennomfÃ¸rt dette sÃ¥ har du tilgang til statisticsnorway pÃ¥ GitHub. GÃ¥r du inn pÃ¥ denne lenken sÃ¥ skal du nÃ¥ kunne lese bÃ¥de Public, Private og Internal repoer, slik som vist i FigurÂ 9.\n\n\n\n\n\n\nFigurÂ 9: Medlemsvisning for SSB sin GitHub-organisasjon.\n\n\n\n\n\n\nNÃ¥r vi skal jobbe med SSB-kode som ligger lagret hos statistcsnorway pÃ¥ GitHub, sÃ¥ mÃ¥ vi autentisere oss. MÃ¥ten vi gjÃ¸re det pÃ¥ er ved Ã¥ generere et Personal Access Token (ofte forkortet PAT) som vi oppgir nÃ¥r vi vil hente eller oppdatere kode pÃ¥ GitHub. Da sender vi med PAT for Ã¥ autentisere oss for GitHub.\n\n\nFor Ã¥ lage en PAT som er godkjent mot statisticsnorway sÃ¥ gjÃ¸r man fÃ¸lgende:\n\nGÃ¥ til din profilside pÃ¥ GitHub og Ã¥pne Settings slik som ble vist SeksjonÂ 1.2.2.\nVelg Developer Settings i menyen til venstre.\nI menyen til venstre velger du Personal Access Token, og deretter Tokens (classic).\nVelg Generate new token og deretter Generate new token (classic).\nUnder Note kan du gi PATâ€™en et navn. Velg et navn som er intuitivt for deg. Hvis du skal bruke PAT til Ã¥ jobbe mot Dapla, sÃ¥ ville jeg ganske enkelt kalt den dapla. Hvis du skal bruke den mot bakkemiljÃ¸et ville jeg kalt den prodsone eller noe annet som gjÃ¸r det lett for det skjÃ¸nne innholdet i ettertid.\nUnder Expiration velger du hvor lang tid som skal gÃ¥ fÃ¸r PAT blir ugyldig. Dette er en avvening mellom sikkerhet og hva som er praktisk. Det anbefales at du velger 365 dager. NÃ¥r PAT utlÃ¸per mÃ¥ du gjenta stegene i dette kapittelet.\nUnder Select scopes velger du repo og workflow slik som vist i FigurÂ 10.\n\n\n\n\n\n\n\nFigurÂ 10: Gi token et kort og beskrivende navn\n\n\n\n\nTrykk pÃ¥ Generate token nederst pÃ¥ siden og du fÃ¥r noe lignende det du ser i FigurÂ 11.\n\n\n\n\n\n\n\nFigurÂ 11: Token som ble generert.\n\n\n\n\nKopier deretter PAT til en midlertidig fil. Grunnen er at du aldri vil se det igjen her etter at vi har gjennomfÃ¸rt neste steg.\nDeretter trykker du pÃ¥ Configure SSO og velger Authorize ved siden statisticsnorway, slik som vist i FigurÂ 12. Svar deretter pÃ¥ spÃ¸rsmÃ¥lene som dukker opp.\n\n\n\n\n\n\n\nFigurÂ 12: Autorisering av Token mot SSBs GiHub-organisasjon.\n\n\n\nVi har nÃ¥ opprettet en PAT som er godkjent for bruk mot SSB sin kode pÃ¥ GitHub. Det betyr at hvis vi vil jobbe med Git pÃ¥ SSB sine maskiner i sky eller pÃ¥ bakken, sÃ¥ mÃ¥ vi sendte med dette tokenet for Ã¥ fÃ¥ lov til Ã¥ jobbe med koden som ligger pÃ¥ statisticsnorway pÃ¥ GitHub.\n\n\n\nDet er ganske upraktisk Ã¥ mÃ¥tte sende med tokenet hver gang vi skal jobbe med GitHub. Vi bÃ¸r derfor lagre det lokalt der vi jobber, slik at Git automatisk finner det. Det finnes mange mÃ¥ter Ã¥ gjÃ¸re dette pÃ¥ og det er ikke bestemt hva som skal vÃ¦re beste-praksis i SSB. Men en mÃ¥te Ã¥ gjÃ¸re det er via en .netrc-fil. Vi oppretter da en fil som heter .netrc pÃ¥ vÃ¥rt hjemmeomrÃ¥de, og legger fÃ¸lgende informasjon pÃ¥ en (hvilken som helst) linje i filen:\n\n\n~/.netrc\n\nmachine github.com login &lt;github-bruker&gt; password &lt;Personal Access Token&gt;\n\nGitHub-bruker er da din personlige bruker og IKKE brukernavnet ditt i SSB. Personal Access Token er det vi lagde i forrige kapittelet.\nEn veldig enkel mÃ¥te Ã¥ lagre dette er som fÃ¸lger. Anta at min personlige GitHub-bruker er SSB-Chad og at min Personal Access Token er blablabla. Da kan jeg gjÃ¸re fÃ¸lgende for Ã¥ lagre det i .netrc:\n\nGÃ¥ inn i Jupyterlab og Ã¥pne en Python-notebook.\nI den fÃ¸rste kodecellen skriver du:\n\n\n\nterminal\n\n!echo \"machine github.com login SSB-Chad password blablabla\" &gt;&gt; ~/.netrc\n\nAlternativt kan du droppe det utropstegnet og kjÃ¸re det direkte i en terminal. Det vil gi samme resultat. Koden over legger til en linje med teksten\nmachine github.com login SSB-Chad password blablabla\ni en .netrc-fil pÃ¥ din hjemmeomrÃ¥det, uanvhengig av om du har en fra fÃ¸r eller ikke. Hvis du har en fil fra fÃ¸r som allerede har et token fra GitHub, ville jeg nok slettet det fÃ¸r jeg legger en et nytt token.\nHver gang du jobber mot GitHub vil Git sjekke om informasjon om autentisering ligger i denne filen, og bruke den hvis den ligger der.\n\n\n\nI eksempelet over lagde vi en PAT som var gyldig i 90 dager. Dermed vil du ikke kunne jobbe mot GitHub med dette tokenet etter 90 dager. For Ã¥ oppdatere tokenet gjÃ¸r du fÃ¸lgende:\n\nLag et nytt PAT ved Ã¥ repetere SeksjonÂ 1.2.4.1.\nI miljÃ¸et der du skal jobbe med Git og GitHub gÃ¥r du inn i din .netrc og bytter ut token med det nye.\n\nOg med det er du klar til Ã¥ jobbe mot statisticsnorway pÃ¥ GitHub.",
    "crumbs": [
      "Statistikere",
      "Programmering",
      "Git og Github"
    ]
  },
  {
    "objectID": "statistikkere/git-og-github.html#footnotes",
    "href": "statistikkere/git-og-github.html#footnotes",
    "title": "Git og Github",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPrÃ¸v selv Ã¥ Ã¥pne en ipynb som json ved hÃ¸reklikke pÃ¥ fila i Jupyterlab, velge Open with, og velg json. Da vil du se den underliggende json-filenâ†©ï¸\nBranches kan oversettes til grener pÃ¥ norsk. Men i denne boken velger vi Ã¥ bruke det engelske ordet branches. Grunnen er at det erfaringsmessig er lettere forholde seg til det engelske ordet nÃ¥r man skal sÃ¸ke etter informasjon i annen dokumentasjonâ†©ï¸",
    "crumbs": [
      "Statistikere",
      "Programmering",
      "Git og Github"
    ]
  },
  {
    "objectID": "statistikkere/arkivering.html",
    "href": "statistikkere/arkivering.html",
    "title": "Arkivering",
    "section": "",
    "text": "Alle som flytter produksjon til Dapla mÃ¥ fortsatt arkivere dataene i bakkemiljÃ¸et. Grunnen til dette er at det enda ikke er bestemt hvordan arkivering skal foregÃ¥ pÃ¥ Dapla. Inntill videre mÃ¥ derfor statistikkteam arkivere i de gamle systemene.\n\n\nFÃ¸r man kan arkivere data mÃ¥ det skrives ut slik det er definert i Datadok. Mens man tidligere gjorde dette fra SAS, skal man pÃ¥ Dapla gjÃ¸re det fra R eller Python.\n\n\n\nEtter at filen er skrevet mÃ¥ den flyttes fra Dapla til bakkemiljÃ¸et, og til slutt inn i riktig arkiv-mappe. OverfÃ¸ring av filer mellom bakke og sky gjÃ¸res med Transfer Service. NÃ¥r filen er flyttet til bakkemiljÃ¸et, mÃ¥ brukeren selv flytte filen til arkiv-mappen. Ã˜nsker man Ã¥ automatisere flyttingen, sÃ¥ kan man sende en forespÃ¸rsel til Kundeservice.",
    "crumbs": [
      "Statistikere",
      "Appendix",
      "Arkivering"
    ]
  },
  {
    "objectID": "statistikkere/arkivering.html#skrive-fil",
    "href": "statistikkere/arkivering.html#skrive-fil",
    "title": "Arkivering",
    "section": "",
    "text": "FÃ¸r man kan arkivere data mÃ¥ det skrives ut slik det er definert i Datadok. Mens man tidligere gjorde dette fra SAS, skal man pÃ¥ Dapla gjÃ¸re det fra R eller Python.",
    "crumbs": [
      "Statistikere",
      "Appendix",
      "Arkivering"
    ]
  },
  {
    "objectID": "statistikkere/arkivering.html#overfÃ¸re-fil",
    "href": "statistikkere/arkivering.html#overfÃ¸re-fil",
    "title": "Arkivering",
    "section": "",
    "text": "Etter at filen er skrevet mÃ¥ den flyttes fra Dapla til bakkemiljÃ¸et, og til slutt inn i riktig arkiv-mappe. OverfÃ¸ring av filer mellom bakke og sky gjÃ¸res med Transfer Service. NÃ¥r filen er flyttet til bakkemiljÃ¸et, mÃ¥ brukeren selv flytte filen til arkiv-mappen. Ã˜nsker man Ã¥ automatisere flyttingen, sÃ¥ kan man sende en forespÃ¸rsel til Kundeservice.",
    "crumbs": [
      "Statistikere",
      "Appendix",
      "Arkivering"
    ]
  },
  {
    "objectID": "statistikkere/kildedata-prosessering.html",
    "href": "statistikkere/kildedata-prosessering.html",
    "title": "Kildedata prosessering",
    "section": "",
    "text": "Denne tjenesten er under utvikling og kan ikke anses som klar for produksjon.\n\n\n\nFor Ã¥ minske aksessering av PII1, oppfordres alle team pÃ¥ Dapla Ã¥ benytte seg av automatisering av kildedata prosessering. Automatisering av kildedata er en tjeneste som er tilgjengelig for team Ã¥ ta i bruk 100% selv-betjent. Kildedata (Standardutvalget 2021, 5) prosesseres til inndata gjennom et bestemt utvalg av operasjoner. Kildedata prosesseres som individuelle filer for Ã¥ holde oppsettet enkelt og mÃ¥lrettet mot de definerte operasjoner. Mer kompleks operasjoner som gÃ¥r pÃ¥ tvers av flere filer burde utfÃ¸res pÃ¥ inndata eller senere datatilstander.\n\n\n\n\n\n\nDet er kun teamets kildedataansvarlige som skal aksessere kildedata.\n\n\n\n\n\n\n\n\n\nTeamets kildedataansvarlige tar ansvar for Ã¥ prosessere kildedata til inndata pÃ¥ en forsvarlig mÃ¥te.\n\n\n\n\n\n\n\n\n\n\n\nFigurÂ 1: Operasjoner som inngÃ¥r i kildedata prosessering\n\n\n\n\nInndata er kildedata som er transformert til SSBs standard lagringsformat. Variabelnavn og -innhold er uendret bortsett fra at\n\ndirekte identifiserende variabler (f.eks. fÃ¸dselsnummer) er pseudonymisert\ntegnsett, datoformat, adresse mm er endret til SSBs standardformat\ndet benyttes standard kodeverk (Klass) der det er mulig (f.eks. kjÃ¸nn)\ndataene er minimert slik at kun variablene som er nÃ¸dvendige i den videre produksjonsprosessen, inngÃ¥r.\n\n\n(Standardutvalget 2021, 8)\nDet er ikke anbefalt Ã¥ gjennomfÃ¸re operasjoner som:\n\nGÃ¥r pÃ¥ tvers av flere filer\nLegge til nye felt\nEndre navn pÃ¥ felt\nAggregerer data\nosv.\n\n\n\n\n\n\nFÃ¸lg instruksjonene her for Ã¥ koble prosjektet ditt til Github.\n\n\n\n\n\nKilder konfigureres i et teams Infrastructure as Code (IaC) repo pÃ¥ Github. Det kan finnes basert pÃ¥ fÃ¸lgende formulering: github.com/statisticsnorway/&lt;teamnavn&gt;-iac. Kilder konfigureres under stien automation/source_data pÃ¥ repoet.\n\n\n\nHver kilde konfigureres ved hjelp av to filer:\n\nconfig.yaml som blant annet konfigurer hvilke stier i teamets kildedatabÃ¸tte prosesseres.\nprocess_source_data.py som kjÃ¸res nÃ¥r en kildedatafil prosesseres. Her mÃ¥ man skrive en python funksjon pÃ¥ en viss format.\n\nDisse filene er lagt til en mappe per kilde, under automation/source_data i IaC repoet. Se eksemplet under for en detaljert forklaring.\n\n\n\n\n\n\nMappenavnet for kilder i IaC repoene er brukt som navn for ressurser. Dette i praksis betyr at det enesete tillatte tegnene i mappenavnet er bokstaver, tall og bindestrek. Det er ikke tillatt med mellomrom eller andre spesialtegn. Mappenavnet kan bestÃ¥ av opptil 20 tegn.\n\n\n\n\n\n\nDette gÃ¥r ut pÃ¥ om prosesseringsscriptet kan enkelt hÃ¥ndtere variasjonen i filene som samles inn.\nGrunn til Ã¥ opprette en ny kilde kan vÃ¦re: - Kildedatafilen har en annen format (f.eks xml eller json) - Kildedataen har ulike felter - Kildedataen inneholder PII2 eller ikke\n\n\n\n\n\n\n\n\n\nDisse instruksjoner forutsetter at prosjektet ditt er koblet til Github allerede.\n\n\n\n\nSkrive skriptet process_source_data.py som prosesserer kildedatafilen til inndata. Dette kan testes av kildedataansvarlige manuelt pÃ¥ Jupyter for Ã¥ verifisere at dataene blir prosessert som Ã¸nsket.\nI en branch i teamets IaC repo, legge til config.yaml og process_source_data.py i en mappe under automation/source_data. Se eksemplet under for en detaljert forklaring av formatet.\nLag en PR pÃ¥ grenen og fÃ¥ den godkjent av kildedataansvarlige.\nVent til alle tester er ferdige. Det skal stÃ¥ â€œAll checks have passedâ€ fÃ¸r man gÃ¥r videre, hvis testene feiler fÃ¸lg stegene her. \nSkrive atlantis apply i en kommentar pÃ¥ PRen for Ã¥ opprette det nÃ¸dvendige infrastruktur for Ã¥ prosessere kilden.\nMerge PRen.\nSjekk resultatet av det automatiske bygget.\nVerifisere at nye filer lagt i kildedatabÃ¸tten blir prosessert til inndata som forventet.\n\n\n\n\n\nLa oss si at et team (smÃ¥bakst) har to datakilder levert av ulik dataeiere pÃ¥ ulik formater. Den ene er om boller og er pÃ¥ csv format og den andre er om rundstykker og er pÃ¥ json format. Kildedataansvarlige i teamet bestemmer seg for at filene i boller/ er like nok Ã¥ prosesseres som en kilde, og at filene i rundstykker/ kan prosesseres som en annen kilde.\n\n\nssb-prod-smaabakst-data-kilde\nâ”œâ”€â”€ boller\nâ”‚Â Â  â”œâ”€â”€ hvetebolle\nâ”‚Â Â  â”‚Â Â  â”œâ”€â”€ 2018-salg.csv\nâ”‚Â Â  â”‚Â Â  â”œâ”€â”€ 2019-salg.csv\nâ”‚Â Â  â”‚Â Â  â”œâ”€â”€ ...\nâ”‚Â Â  â”œâ”€â”€ kanelbolle\nâ”‚Â Â  â”‚Â Â  â”œâ”€â”€ 2018-salg.csv\nâ”‚Â Â  â”‚Â Â  â”œâ”€â”€ 2019-salg.csv\nâ”‚Â Â  â”‚Â Â  â”œâ”€â”€ ...\nâ”‚Â Â  â””â”€â”€ skolebolle\nâ”‚Â Â      â”œâ”€â”€ 2018-salg.csv\nâ”‚Â Â      â”œâ”€â”€ 2019-salg.csv\nâ”‚Â Â      â”œâ”€â”€ ...\nâ””â”€â”€ rundstykker\n    â”œâ”€â”€ haandverker\n    â”‚Â Â  â”œâ”€â”€ apr-2022-resultater.json\n    â”‚Â Â  â”œâ”€â”€ aug-2022-resultater.json\n    â”‚Â Â  â”œâ”€â”€ ...\n    â””â”€â”€ havre\n        â”œâ”€â”€ apr-2022-resultater.json\n        â”œâ”€â”€ aug-2022-resultater.json\n        â”œâ”€â”€ ...\n\n\n\nsmaabakst-iac\nâ””â”€â”€ automation\n    â””â”€â”€ source_data\n        â”œâ”€â”€ boller\n        â”‚Â Â  â”œâ”€â”€ config.yaml\n        â”‚Â Â  â””â”€â”€ process_source_data.py\n        â””â”€â”€ rundstykker\n            â”œâ”€â”€ config.yaml\n            â””â”€â”€ process_source_data.py\n\n\n\n\n\nconfig.yaml\n\nfolder_prefix: boller\n\nVerdien for folder_prefix tilsvarer en â€œfil stiâ€ i kildedatabÃ¸tte. I dette tilfellet vil nye filer lagt til under boller/ trigge en prosessering.\n\n\n\n\n\nconfig.yaml\n\nfolder_prefix: rundstykker\n\n\n\n\n\n\nMed prosesseringsscriptet mener vi filen process_source_data.py som ligger i en mappe per kilde under automation/source_data. Dette scriptet vil bli kalt hver gang det blir lagt til en ny fil i kildedatabÃ¸tten. Metodesignaturen ser slik ut:\n\n\nprocess_source_data.py\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n\nDette betyr at hvis f.eks. 10 filer blir lagt til i kildedatabÃ¸tten samtidig sÃ¥ vil det startes opp 10 individuelle Python-prosesser som kaller denne main-metoden med forskjellig file_path. Parameteren file_path vil inneholde hele filstien inkl. filnavn. SÃ¥ en enkel flytteoperasjon fra kildedatabÃ¸tten til inndatebÃ¸tten (uten noen form for konvertering) vil kunne uttrykkes slik:\n\n\nprocess_source_data.py\n\nimport dapla as dp\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n    source_bucket_name = \"ssb-prod-my-project-data-kilde\"\n    destination_bucket_name = \"ssb-prod-my-project-data-produkt\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    df = dp.read_pandas(file_path)\n    dp.write_pandas(df, destination_bucket_path)\n\nAlternativtâ€¦\n\n\nprocess_source_data.py\n\nfrom dapla import FileClient\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n    source_bucket_name = \"ssb-prod-my-project-data-kilde\"\n    destination_bucket_name = \"ssb-prod-my-project-data-produkt\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    fs = FileClient.get_gcs_file_system()\n    fs.copy(file_path, destination_path)\n\n\n\nDet anbefales Ã¥ bruke Pythons logging modul for Ã¥ logge, og ikke bruke print eller skrive til stdout/stderr. Det er satt opp en standard logger-konfigurasjon som skriver informasjonsmeldinger (log level info) til stdout og feilmeldinger (log level warning eller error) til stderr. Feil som ikke blir hÃ¥ndtert blir automatisk fanget opp og logget av automatiseringslÃ¸sningen. Eksemplet nedenfor logger en informasjonsmelding, en advarsel og en feilmelding:\n\n\nprocess_source_data.py\n\nimport logging\nlogging.info('Til info')\nlogging.warning('Advarsel!')\nlogging.error('En feil oppstod!')"
  },
  {
    "objectID": "statistikkere/kildedata-prosessering.html#operasjoner-som-inngÃ¥r-i-kildedata-prosessering",
    "href": "statistikkere/kildedata-prosessering.html#operasjoner-som-inngÃ¥r-i-kildedata-prosessering",
    "title": "Kildedata prosessering",
    "section": "",
    "text": "FigurÂ 1: Operasjoner som inngÃ¥r i kildedata prosessering\n\n\n\n\nInndata er kildedata som er transformert til SSBs standard lagringsformat. Variabelnavn og -innhold er uendret bortsett fra at\n\ndirekte identifiserende variabler (f.eks. fÃ¸dselsnummer) er pseudonymisert\ntegnsett, datoformat, adresse mm er endret til SSBs standardformat\ndet benyttes standard kodeverk (Klass) der det er mulig (f.eks. kjÃ¸nn)\ndataene er minimert slik at kun variablene som er nÃ¸dvendige i den videre produksjonsprosessen, inngÃ¥r.\n\n\n(Standardutvalget 2021, 8)\nDet er ikke anbefalt Ã¥ gjennomfÃ¸re operasjoner som:\n\nGÃ¥r pÃ¥ tvers av flere filer\nLegge til nye felt\nEndre navn pÃ¥ felt\nAggregerer data\nosv."
  },
  {
    "objectID": "statistikkere/kildedata-prosessering.html#ta-tjenesten-i-bruk",
    "href": "statistikkere/kildedata-prosessering.html#ta-tjenesten-i-bruk",
    "title": "Kildedata prosessering",
    "section": "",
    "text": "FÃ¸lg instruksjonene her for Ã¥ koble prosjektet ditt til Github.\n\n\n\n\n\nKilder konfigureres i et teams Infrastructure as Code (IaC) repo pÃ¥ Github. Det kan finnes basert pÃ¥ fÃ¸lgende formulering: github.com/statisticsnorway/&lt;teamnavn&gt;-iac. Kilder konfigureres under stien automation/source_data pÃ¥ repoet.\n\n\n\nHver kilde konfigureres ved hjelp av to filer:\n\nconfig.yaml som blant annet konfigurer hvilke stier i teamets kildedatabÃ¸tte prosesseres.\nprocess_source_data.py som kjÃ¸res nÃ¥r en kildedatafil prosesseres. Her mÃ¥ man skrive en python funksjon pÃ¥ en viss format.\n\nDisse filene er lagt til en mappe per kilde, under automation/source_data i IaC repoet. Se eksemplet under for en detaljert forklaring.\n\n\n\n\n\n\nMappenavnet for kilder i IaC repoene er brukt som navn for ressurser. Dette i praksis betyr at det enesete tillatte tegnene i mappenavnet er bokstaver, tall og bindestrek. Det er ikke tillatt med mellomrom eller andre spesialtegn. Mappenavnet kan bestÃ¥ av opptil 20 tegn.\n\n\n\n\n\n\nDette gÃ¥r ut pÃ¥ om prosesseringsscriptet kan enkelt hÃ¥ndtere variasjonen i filene som samles inn.\nGrunn til Ã¥ opprette en ny kilde kan vÃ¦re: - Kildedatafilen har en annen format (f.eks xml eller json) - Kildedataen har ulike felter - Kildedataen inneholder PII2 eller ikke\n\n\n\n\n\n\n\n\n\nDisse instruksjoner forutsetter at prosjektet ditt er koblet til Github allerede.\n\n\n\n\nSkrive skriptet process_source_data.py som prosesserer kildedatafilen til inndata. Dette kan testes av kildedataansvarlige manuelt pÃ¥ Jupyter for Ã¥ verifisere at dataene blir prosessert som Ã¸nsket.\nI en branch i teamets IaC repo, legge til config.yaml og process_source_data.py i en mappe under automation/source_data. Se eksemplet under for en detaljert forklaring av formatet.\nLag en PR pÃ¥ grenen og fÃ¥ den godkjent av kildedataansvarlige.\nVent til alle tester er ferdige. Det skal stÃ¥ â€œAll checks have passedâ€ fÃ¸r man gÃ¥r videre, hvis testene feiler fÃ¸lg stegene her. \nSkrive atlantis apply i en kommentar pÃ¥ PRen for Ã¥ opprette det nÃ¸dvendige infrastruktur for Ã¥ prosessere kilden.\nMerge PRen.\nSjekk resultatet av det automatiske bygget.\nVerifisere at nye filer lagt i kildedatabÃ¸tten blir prosessert til inndata som forventet.\n\n\n\n\n\nLa oss si at et team (smÃ¥bakst) har to datakilder levert av ulik dataeiere pÃ¥ ulik formater. Den ene er om boller og er pÃ¥ csv format og den andre er om rundstykker og er pÃ¥ json format. Kildedataansvarlige i teamet bestemmer seg for at filene i boller/ er like nok Ã¥ prosesseres som en kilde, og at filene i rundstykker/ kan prosesseres som en annen kilde.\n\n\nssb-prod-smaabakst-data-kilde\nâ”œâ”€â”€ boller\nâ”‚Â Â  â”œâ”€â”€ hvetebolle\nâ”‚Â Â  â”‚Â Â  â”œâ”€â”€ 2018-salg.csv\nâ”‚Â Â  â”‚Â Â  â”œâ”€â”€ 2019-salg.csv\nâ”‚Â Â  â”‚Â Â  â”œâ”€â”€ ...\nâ”‚Â Â  â”œâ”€â”€ kanelbolle\nâ”‚Â Â  â”‚Â Â  â”œâ”€â”€ 2018-salg.csv\nâ”‚Â Â  â”‚Â Â  â”œâ”€â”€ 2019-salg.csv\nâ”‚Â Â  â”‚Â Â  â”œâ”€â”€ ...\nâ”‚Â Â  â””â”€â”€ skolebolle\nâ”‚Â Â      â”œâ”€â”€ 2018-salg.csv\nâ”‚Â Â      â”œâ”€â”€ 2019-salg.csv\nâ”‚Â Â      â”œâ”€â”€ ...\nâ””â”€â”€ rundstykker\n    â”œâ”€â”€ haandverker\n    â”‚Â Â  â”œâ”€â”€ apr-2022-resultater.json\n    â”‚Â Â  â”œâ”€â”€ aug-2022-resultater.json\n    â”‚Â Â  â”œâ”€â”€ ...\n    â””â”€â”€ havre\n        â”œâ”€â”€ apr-2022-resultater.json\n        â”œâ”€â”€ aug-2022-resultater.json\n        â”œâ”€â”€ ...\n\n\n\nsmaabakst-iac\nâ””â”€â”€ automation\n    â””â”€â”€ source_data\n        â”œâ”€â”€ boller\n        â”‚Â Â  â”œâ”€â”€ config.yaml\n        â”‚Â Â  â””â”€â”€ process_source_data.py\n        â””â”€â”€ rundstykker\n            â”œâ”€â”€ config.yaml\n            â””â”€â”€ process_source_data.py\n\n\n\n\n\nconfig.yaml\n\nfolder_prefix: boller\n\nVerdien for folder_prefix tilsvarer en â€œfil stiâ€ i kildedatabÃ¸tte. I dette tilfellet vil nye filer lagt til under boller/ trigge en prosessering.\n\n\n\n\n\nconfig.yaml\n\nfolder_prefix: rundstykker"
  },
  {
    "objectID": "statistikkere/kildedata-prosessering.html#skrive-prosesseringsscriptet",
    "href": "statistikkere/kildedata-prosessering.html#skrive-prosesseringsscriptet",
    "title": "Kildedata prosessering",
    "section": "",
    "text": "Med prosesseringsscriptet mener vi filen process_source_data.py som ligger i en mappe per kilde under automation/source_data. Dette scriptet vil bli kalt hver gang det blir lagt til en ny fil i kildedatabÃ¸tten. Metodesignaturen ser slik ut:\n\n\nprocess_source_data.py\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n\nDette betyr at hvis f.eks. 10 filer blir lagt til i kildedatabÃ¸tten samtidig sÃ¥ vil det startes opp 10 individuelle Python-prosesser som kaller denne main-metoden med forskjellig file_path. Parameteren file_path vil inneholde hele filstien inkl. filnavn. SÃ¥ en enkel flytteoperasjon fra kildedatabÃ¸tten til inndatebÃ¸tten (uten noen form for konvertering) vil kunne uttrykkes slik:\n\n\nprocess_source_data.py\n\nimport dapla as dp\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n    source_bucket_name = \"ssb-prod-my-project-data-kilde\"\n    destination_bucket_name = \"ssb-prod-my-project-data-produkt\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    df = dp.read_pandas(file_path)\n    dp.write_pandas(df, destination_bucket_path)\n\nAlternativtâ€¦\n\n\nprocess_source_data.py\n\nfrom dapla import FileClient\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n    source_bucket_name = \"ssb-prod-my-project-data-kilde\"\n    destination_bucket_name = \"ssb-prod-my-project-data-produkt\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    fs = FileClient.get_gcs_file_system()\n    fs.copy(file_path, destination_path)\n\n\n\nDet anbefales Ã¥ bruke Pythons logging modul for Ã¥ logge, og ikke bruke print eller skrive til stdout/stderr. Det er satt opp en standard logger-konfigurasjon som skriver informasjonsmeldinger (log level info) til stdout og feilmeldinger (log level warning eller error) til stderr. Feil som ikke blir hÃ¥ndtert blir automatisk fanget opp og logget av automatiseringslÃ¸sningen. Eksemplet nedenfor logger en informasjonsmelding, en advarsel og en feilmelding:\n\n\nprocess_source_data.py\n\nimport logging\nlogging.info('Til info')\nlogging.warning('Advarsel!')\nlogging.error('En feil oppstod!')"
  },
  {
    "objectID": "statistikkere/kildedata-prosessering.html#footnotes",
    "href": "statistikkere/kildedata-prosessering.html#footnotes",
    "title": "Kildedata prosessering",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPersonidentifiserende Informasjonâ†©ï¸\nPersonidentifiserende Informasjonâ†©ï¸"
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html",
    "href": "statistikkere/jobbe-med-kode.html",
    "title": "Jobbe med kode",
    "section": "",
    "text": "PÃ¥ Dapla jobber vi med utvikling av Python- og R-kode i et Jupyter-miljÃ¸. For de som Ã¸nsker det, er det mulig Ã¥ enkelt Ã¥pne en notebook med en av vÃ¥re forhÃ¥ndskonfigurerte kernels1. Man kan umiddelbart begynne Ã¥ skrive kode og deretter lagre den i det lokale filsystemet. Dette er ideelt for enkel datautforskning eller for pedagogiske formÃ¥l.\nNÃ¥r koden skal settes i produksjon, er det essensielt Ã¥ ta hensyn til fÃ¸lgende:\n\nResultater bÃ¸r vÃ¦re reproduserbare.\nKoden mÃ¥ kunne deles med andre.\nKoden bÃ¸r vÃ¦re organisert slik at den er gjenkjennelig for kollegaer.\n\nFor Ã¥ lette etterlevelsen av beste praksis for kodeutvikling pÃ¥ Dapla, har vi utviklet et verktÃ¸y kalt ssb-project. Dette er et CLI-verktÃ¸y2 som enkelt lar deg opprette et prosjekt med en standard mappestruktur, et virtuelt miljÃ¸ og integrasjon med Git for versjonshÃ¥ndtering. Som en bonus kan det ogsÃ¥ opprette et GitHub-repositorium for deg ved behov.\nI dette kapitlet vil vi veilede deg gjennom bruken av ssb-project. Du vil lÃ¦re Ã¥ opprette et nytt prosjekt, installere pakker, hÃ¥ndtere versjoner med Git, bygge et eksisterende prosjekt og vedlikeholde prosjektet over tid.\n\n\n\n\n\n\nSSB-project stÃ¸tter ikke R enda\n\n\n\nPer nÃ¥ stÃ¸tter SSB-project kun prosjekter skrevet i Python. Dette skyldes begrensninger ved det populÃ¦re virtuelle miljÃ¸-verktÃ¸yet for R, renv. Mens renv effektivt hÃ¥ndterer versjoner av R-pakker, har det ikke kapasitet til Ã¥ ta vare pÃ¥ spesifikke R-installasjonsversjoner. Dette kan potensielt gjÃ¸re det mer utfordrende Ã¥ reprodusere tidligere publiserte resultater ved bruk av ssb-project. Vi arbeider mot en lÃ¸sning for Ã¥ inkludere stÃ¸tte for R i fremtiden.",
    "crumbs": [
      "Statistikere",
      "Programmering"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#forberedelser",
    "href": "statistikkere/jobbe-med-kode.html#forberedelser",
    "title": "Jobbe med kode",
    "section": "Forberedelser",
    "text": "Forberedelser\nFÃ¸r du kan ta i bruk ssb-project sÃ¥ er det et par ting som mÃ¥ vÃ¦re pÃ¥ plass:\n\nDu mÃ¥ ha konfigurert Git etter SSB sin standard (les mer om hvordan her).\nHvis du Ã¸nsker at ssb-project ogsÃ¥ skal opprette et GitHub-repo for deg mÃ¥ du ogsÃ¥ fÃ¸lgende vÃ¦re pÃ¥ plass:\n\nDu mÃ¥ ha en GitHub-bruker (les hvordan her)\nSkru pÃ¥ 2-faktor autentifisering for GitHub-brukeren din (les hvordan her)\nVÃ¦re koblet mot SSBs organisasjon statisticsnorway pÃ¥ GitHub (les hvordan her)\nOpprette Personal Access Token (PAT) og godkjenne det for bruk mot statisticsnorway (les hvordan her)\n\n\nDet er ogsÃ¥ Ã¥ anbefale at du lagrer PAT lokalt slik at du ikke trenger Ã¥ forholde deg til det nÃ¥r jobber med Git og GitHub. Hvis du har alt dette pÃ¥ plass sÃ¥ kan du bare fortsette Ã¥ fÃ¸lge de neste kapitlene.",
    "crumbs": [
      "Statistikere",
      "Programmering"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#opprett-ssb-project",
    "href": "statistikkere/jobbe-med-kode.html#opprett-ssb-project",
    "title": "Jobbe med kode",
    "section": "Opprett ssb-project",
    "text": "Opprett ssb-project\n\n\n\n\n\n\nHar du Github bruker? Noe funksjonalitet i ssb-project krever det. Finn ut hvordan ved Ã¥ lese forrige kapittel.\n\n\n\nssb-project lar deg opprette en prosjekt-mappe med og uten GitHub-repo. La oss ta for oss hver av alternativene.\n\nUten GitHub-repo\nFor Ã¥ opprette et nytt ssb-project uten GitHub-repo gjÃ¸r du fÃ¸lgende:\n\nÃ…pne en terminal. De fleste vil gjÃ¸re dette i Jupyterlab pÃ¥ bakke eller sky og da kan de bare trykke pÃ¥ det blÃ¥ â•-tegnet i Jupyterlab og velge Terminal.\nFÃ¸r vi kjÃ¸rer programmet mÃ¥ vi vÃ¦re obs pÃ¥ at ssb-project vil opprette en ny mappe der vi stÃ¥r. GÃ¥ derfor til den mappen du Ã¸nsker Ã¥ ha den nye prosjektmappen. For Ã¥ opprette et prosjekt som heter stat-testprod sÃ¥ skriver du fÃ¸lgende i terminalen:\n\n\n\nterminal\n\nssb-project create stat-testprod\n\n\n\nHvis du stod i hjemmemappen din pÃ¥ nÃ¥r du skrev inn kommandoen over i terminalen, sÃ¥ har du fÃ¥tt mappestrukturen som vises i FigurÂ 1. 3. Den inneholder fÃ¸lgende :\n\n.git-mappe som blir opprettet for Ã¥ versjonshÃ¥ndtere med Git.\nsrc-mappe som skal inneholde all koden som utgjÃ¸r produksjonslÃ¸pet.\ntests-mappe som inneholder tester du skriver for koden din.\nLICENCE-fil som skal benyttes for public-repos i SSB.\npoetry.lock-fil som inneholder alle versjoner av Python-pakker som blir brukt.\npyproject.toml-fil som inneholder informasjon om prosjektet og hvilke pakker som blir brukt.\nREADME.md-fil som brukes for tekstlig innhold pÃ¥ GitHub-siden for prosjektet.\n\n\n\n\n\n\n\n\n\nFigurÂ 1: Mappen som ble opprettet av ssb-project.\n\n\n\n\n\n\n\nMed Github-repo\nOver sÃ¥ opprettet vi et ssb-project uten Ã¥ opprette et GitHub-repo. Hvis du Ã¸nsker Ã¥ opprette et GitHub-repo ogsÃ¥ mÃ¥ du endre kommandoen over til:\n\n\nterminal\n\nssb-project create stat-testprod --github --github-token='blablabla'\n\nKommandoen over oppretter en mappestruktur slik vi sÃ¥ tidligere, men ogsÃ¥ et ssb-project som heter stat-testprod med et GitHub-repo med samme navn. Som du ser sÃ¥ mÃ¥ vi da sende med opsjonen --github og PAT med opsjonen --github-token='blablabla'. Repoet i GitHub ser da ut som i FigurÂ 2. Hvis du Ã¸nsker Ã¥ slippe mÃ¥tte forholde deg til PAT hver gang interagerer med GitHub, kan du fÃ¸lge denne beskrivelsen for Ã¥ lagre den lokalt. Da kan droppe --github-token='blablabla' fra kommandoen over.\n\n\n\n\n\n\nFigurÂ 2: GitHub-repo som er opprettet av ssb-project\n\n\n\n\n\n\n\n\n\nNÃ¥r du har opprettet et nytt ssb-project, eller bygget et eksisterende prosjekt, sÃ¥ kan det ta rundt 30 sekunder fÃ¸r kernelen viser seg i Jupterlab-launcher. VÃ¦r tÃ¥lmodig!",
    "crumbs": [
      "Statistikere",
      "Programmering"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#installere-pakker",
    "href": "statistikkere/jobbe-med-kode.html#installere-pakker",
    "title": "Jobbe med kode",
    "section": "Installere pakker",
    "text": "Installere pakker\nNÃ¥r du har opprettet et ssb-project sÃ¥ kan du installere de python-pakkene du trenger fra PyPI. Men fÃ¸r du installerer en pakke bÃ¸r gjÃ¸re fÃ¸lgende for Ã¥ sikre deg at du ikke installerer en pakke med skadelig kode:\n\nSÃ¸k opp pakken pÃ¥ PyPI.\nSjekk om pakken er et populÃ¦rt/velkjent prosjekt ved Ã¥ besÃ¸ke repoet der koden ligger. Antall Stars og Forks pÃ¥ gitHub er en grei indikasjon pÃ¥ dette.\nHvis du er i tvil om pakken er trygg Ã¥ installere, sÃ¥ kan du spÃ¸rre kollegaer om de har erfaring med den, eller spÃ¸rre pÃ¥ en egnet Yammer-kanal i SSB.\nHvis du fortsatt Ã¸nsker Ã¥ installere pakken sÃ¥ anbefaler vi Ã¥ copy-paste navnet fra PyPi, ikke skrive det inn manuelt nÃ¥r du installerer.\n\nSelve installeringen av pakken gjÃ¸res enkelt pÃ¥ fÃ¸lgende mÃ¥te:\n\nÃ…pne en terminal i Jupyterlab.\nGÃ¥ inn i prosjektmappen din ved Ã¥ skrive:\n\n\n\nterminal\n\ncd &lt;sti til prosjektmappe&gt;\n\n\nLag en branch/utviklingsbranch som f.eks. heter install-pandas:\n\n\n\nterminal\n\ngit checkout -b install-pandas\n\n\nInstaller, f.eks. Pandas, ved Ã¥ skrive fÃ¸lgende:\n\n\n\nterminal\n\npoetry add pandas\n\n\n\n\n\n\n\nFigurÂ 3: Installasjon av Pandas med ssb-project\n\n\n\nFigurÂ 3 viser hvordan dette vil se ut i en Jupyterlab-terminal. Kommandoen for Ã¥ installere noe er poetry add etterfulgt av pakkenavnet. Vi ser ogsÃ¥ at den automatisk legger til Pandas-versjonen i filen poetry.lock.\nDu kan ogsÃ¥ spesifisere en konkret versjon av pakken som skal installeres med fÃ¸lgende kommando:\n\n\nterminal\n\npoetry add pandas@1.2.3\n\n\nAvinstallere pakker\nOfte eksperimenterer man med nye pakker, og alle blir ikke med videre i produksjonskoden. Det er god praksis Ã¥ fjerne pakker som ikke brukes, blant annet for Ã¥ unngÃ¥ at de blir en sikkerhetsrisiko. Det gjÃ¸r du enkelt ved Ã¥ skrive fÃ¸lgende i terminalen:\n\n\nterminal\n\npoetry remove pandas\n\n\n\nOppdatere pakker\nHvis det kommer en ny versjon av en pakke du bruker, sÃ¥ kan du oppdatere den med fÃ¸lgende kommando:\n\n\nterminal\n\npoetry update pandas\n\nhvis du kjÃ¸rer poetry update uten noe pakkenavn, sÃ¥ vil alle pakkene dine oppdateres til siste versjon, med mindre du har spesifisert versjonsbegrensninger i pyproject.toml-filen.\n\n\nUndersÃ¸k avhengigheter\nHvis du lurer pÃ¥ hvilke pakker som har hvilke avhengigheter, sÃ¥ kan du lett liste ut dette i terminalen med fÃ¸lgende kommando:\n\n\nterminal\n\npoetry show --tree\n\nDet vil gi en grafisk fremstilling av avhengighetene som vist i FigurÂ 4.\n\n\n\n\n\n\nFigurÂ 4: Visning av pakke-avhengigheter i ssb-project",
    "crumbs": [
      "Statistikere",
      "Programmering"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#push-til-github",
    "href": "statistikkere/jobbe-med-kode.html#push-til-github",
    "title": "Jobbe med kode",
    "section": "Push til GitHub",
    "text": "Push til GitHub\nNÃ¥r du nÃ¥ har installert en pakke sÃ¥ har filen poetry.lock endret seg. For at dine samarbeidspartnere skal fÃ¥ tilgang til denne endringen i et SSB-project, sÃ¥ mÃ¥ du pushe en ny versjon av poetry.lock-filen opp Github, og kollegaene mÃ¥ pulle ned og bygge prosjektet pÃ¥ nytt. Du kan gjÃ¸re dette pÃ¥ fÃ¸lgende mÃ¥te etter at du har installert en ny pakke:\n\nVi kan stage alle endringer med fÃ¸lgende kommando i terminalen nÃ¥r vi stÃ¥r i prosjektmappen:\n\n\n\nterminal\n\ngit add -A\n\n\nDeretter commite en endring, dvs. ta et stillbilde av koden i dette Ã¸yeblikket, ved Ã¥ skrive fÃ¸lgende:\n\n\n\nterminal\n\ngit commit -m \"Installert pandas\"\n\n\nPush det opp til GitHub4. Anta at vi gjorde dette i branchen install-pandas som ble opprettet tidligere. Da kan vi skrive fÃ¸lgende :\n\n\n\nterminal\n\ngit push --set-upstream origin install-pandas\n\nDeretter kan kollegaene dine pulle ned endringene og bygge prosjektet pÃ¥ nytt. Vi forklarer hvordan man kan bygge prosjektet pÃ¥ nytt senere i kapitlet.",
    "crumbs": [
      "Statistikere",
      "Programmering"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#dependabot",
    "href": "statistikkere/jobbe-med-kode.html#dependabot",
    "title": "Jobbe med kode",
    "section": "Dependabot",
    "text": "Dependabot\nNÃ¥r man installerer pakker sÃ¥ vil det etter hvert utvikle seg et sett av potensielt kompliserte avhengigheter mellom disse pakkene. Dette skyldes at en pakke kan benytte seg av funksjonalitet i andre pakker, som igjen benytter seg av funksjonalitet i andre pakker, osv.. Hvis noen finner en sikkerhetssÃ¥rbarhet i en pakke sÃ¥ kan det fikses ved at en ny versjon av den pakken slippes, som igjen kan fÃ¥ konsekvenser for pakker som er avhengig av denne.\nI SSB er det tilrettelagt for at alle som versjonshÃ¥ndterer koden sin pÃ¥ GitHub kan skanne pakkene sine for sÃ¥rbarheter og nye versjoner av pakker med Dependabot. Dependabot hjelper oss med Ã¥ finne og fikse sÃ¥rbarheter og gamle pakkeversjoner. Dette er spesielt viktig nÃ¥r man installerer sine egne pakker.\nDependabot sjekker med jevne mellomrom om det finnes oppdateringer i pakkene som er listet i din pyproject.toml-fil, og den tilhÃ¸rende poetry.lock. Hvis det finnes oppdateringer sÃ¥ vil den lage en pull request som du kan godkjenne. NÃ¥r du godkjenner den sÃ¥ vil den oppdatere poetry.lock-filen og lage en ny commit som du kan pushe til GitHub. Dependabot gir ogsÃ¥ en sikkerhetsvarslinger hvis det finnes kjente sÃ¥rbarheter i pakkene du bruker.\nDet er anbefalt at alle som installerer sine egne pakker i SSB skrur pÃ¥ Dependabot i sine GitHub-repoer.\n\nAktivere Dependabot\nDu kan aktivere Dependabot ved Ã¥ gi inn i GitHub-repoet ditt og gjÃ¸re fÃ¸lgende:\n\nGÃ¥ inn repoet\nTrykk pÃ¥ Settings for det repoet som vist pÃ¥ FigurÂ 5.\n\n\n\n\n\n\n\nFigurÂ 5: Ã…pne Settings for et GitHub-repo.\n\n\n\n\nI menyen til venstre velger du Code security and analysis\nUnder seksjonen Dependabot velger Enable pÃ¥ minst Dependabot alerts og Dependabot security updates, slik som vist i FigurÂ 6.\n\n\n\n\n\n\n\nFigurÂ 6: Skru pÃ¥ Dependabot i GitHub.\n\n\n\nNÃ¥r du har gjort dette vil GitHub varsle deg hvis det finnes en kjent sÃ¥rbarhet i pakkene som benyttes.\n\n\nOppdatere pakker\nHvis en av pakkene du bruker kommer med en oppdatering, sÃ¥ vil Dependabot lage en pull request (PR) i GitHub som du kan godkjenne. Dependabot sjekker ogsÃ¥ om oppdateringen er i konflikt med andre pakker du bruker. Hvis det er tilfellet sÃ¥ vil den lage en pull request som oppdaterer alle pakkene som er i konflikt.\nHvis en av pakkene du bruker har en kjent sikkerhetssÃ¥rbarhet, sÃ¥ vil Dependabot varsle deg om dette under Security-fanen i GitHub-repoet ditt. Hvis du trykker pÃ¥ View Dependabot alerts sÃ¥ vil du fÃ¥ en oversikt over alle sÃ¥rbarhetene som er funnet, og hvilken alvorlighetsgrad den har. Hvis du trykker pÃ¥ en av sÃ¥rbarhetene sÃ¥ vil du fÃ¥ mer informasjon om den, og du kan trykke pÃ¥ Create pull request for Ã¥ oppdatere pakken.\nSom nevnt over kan du enkelt oppdatere pakker fra GitHub ved hjelp av Dependabot. Men det finnes tilfeller der du vil teste om en oppdatering gjÃ¸r at deler av koden din ikke fungerer lenger. Anta at du bruker en Python-pakken Pandas i koden din, og at du fÃ¥r en pull request fra Dependabot om Ã¥ oppdatere den fra versjon 1.5 til 2.0. Hvis du Ã¸nsker Ã¥ teste om koden din fortsatt fungerer med den nye versjonen av Pandas, sÃ¥ kan du gjÃ¸re dette i Jupyterlab ved Ã¥ fÃ¸lge ved Ã¥ lage en branch som f.eks. heter update-pandas. Deretter kan du installere den nye versjonen av Pandas med fÃ¸lgende kommando fra terminalen:\n\n\nterminal\n\npoetry update pandas@2.0\n\nHvis du nÃ¥ kjÃ¸rer koden din kan du teste om den fortsatt fungerer som forventet. GjÃ¸r den ikke det kan du tilpasse koden din og pushe endringene til Github. Deretter kan du godkjenne pull requesten fra Dependabot og merge den. Etter dette kan du slette den PR-en som Dependabot lagde for deg.",
    "crumbs": [
      "Statistikere",
      "Programmering"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#bygg-eksisterende-ssb-project",
    "href": "statistikkere/jobbe-med-kode.html#bygg-eksisterende-ssb-project",
    "title": "Jobbe med kode",
    "section": "Bygg eksisterende ssb-project",
    "text": "Bygg eksisterende ssb-project\nNÃ¥r vi skal samarbeide med andre om kode sÃ¥ gjÃ¸r vi dette via GitHub. NÃ¥r du pusher koden din til GitHub, sÃ¥ kan samarbeidspartnere pulle ned koden og jobbe videre med den. Men nÃ¥r de henter ned koden sÃ¥ vil de bare hente ned selve koden, ikke pakker og Python-versjonen som ble brukt. De mÃ¥ installere alt som du hadde installert. I tillegg trenger de en kernel hvis de skal jobbe i Jupyterlab. ssb-project gjÃ¸r det svÃ¦rt enkelt Ã¥ bygge opp det du trenger, siden det virtuelle miljÃ¸et har all informasjon om hva som trengs.\nFor at samarbeidspartneren din skal kunne bygge miljÃ¸et pÃ¥ nytt, mÃ¥ de ha gjort en minimal konfigurering av Git. Les mer om hvordan du frem for Ã¥ gjÃ¸re dette her.\nFor Ã¥ bygge opp et eksisterende miljÃ¸ gjÃ¸r du fÃ¸lgende:\n\nFÃ¸rst mÃ¥ du kopiere prosjektet ned lokalt, eller klone repoet med git-terminologi\n\n\n\nterminal\n\ngit clone https://github.com/statisticsnorway/&lt;prosjektnavn&gt;\n\n\nGÃ¥ inn i mappen du klonet\n\n\n\nterminal\n\ncd &lt;prosjektnavn&gt;\n\n\nSkape et virtuelt miljÃ¸ og installere en tilsvarende Jupyter kernel med\n\n\n\nterminal\n\nssb-project build",
    "crumbs": [
      "Statistikere",
      "Programmering"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#slette-ssb-project",
    "href": "statistikkere/jobbe-med-kode.html#slette-ssb-project",
    "title": "Jobbe med kode",
    "section": "Slette ssb-project",
    "text": "Slette ssb-project\nDet vil vÃ¦re tilfeller hvor man Ã¸nsker Ã¥ slette et ssb-project, enten fordi man ikke trenger koden lenger eller fordi man bare testet litt.\n\nLokalt\nHvis man jobber med flere prosjekter sÃ¥ kan det fort bli mange Jupyter kerneler hengende igjen. Derfor er det ogsÃ¥ mulighet Ã¥ kjÃ¸re\n\n\nterminal\n\nssb-project clean stat-testprod\n\nsom sletter Jupyter-kernelen og de installerte pakkene i prosjektet. Hvis du ogsÃ¥ Ã¸nsker Ã¥ slette selve mappen med kode mÃ¥ du gjÃ¸re det manuelt5:\n\n\nterminal\n\nrm -rf ~/stat-testprod/\n\nProsjektmappen over lÃ¥ direkte i hjemmemappen min og hjemmemappen pÃ¥ Linux kan alltid referes til med et tilda-tegn ~.\n\n\nArkiver GitHub-repo\nGitHub-repoer som er opprettet under SSB sin organinasjons statisticsnorway pÃ¥ GitHub kan ikke slettes, bare arkiveres. Grunnen er at hvis man oppdager en sÃ¥rbarhet senere sÃ¥ er det viktig Ã¥ kunne se repoet for Ã¥ forstÃ¥ hva som har skjedd.\nHvis du ikke trenger et GitHub-repo lenger kan man arkivere repoet. Det gjÃ¸r du pÃ¥ fÃ¸lgende mÃ¥te:\n\nGi inn i repoet Settings slik som vist med rÃ¸d pil i FigurÂ 7.\n\n\n\n\n\n\n\nFigurÂ 7: Settings for repoet.\n\n\n\n\nUnder General scroller du deg ned til delen som heter Danger Zone og velger Archive this repository, slik som vist pÃ¥ FigurÂ 8.\n\n\n\n\n\n\n\nFigurÂ 8: Arkivering av et repo.\n\n\n\n\nI dialogboksen som dukker opp fyller du inn reponavnet som beskrevet og trykker pÃ¥ I understand the consequences, archive this repository.\n\nNÃ¥r det er gjort sÃ¥ er repoet lesbart, men man kan ikke jobbe med det. Men som vi ser av @#fig-github-repo-settings-archive-warning kan man omgjÃ¸re arkiveringen senere hvis det skulle vÃ¦re Ã¸nskelig.",
    "crumbs": [
      "Statistikere",
      "Programmering"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#spark-i-ssb-project",
    "href": "statistikkere/jobbe-med-kode.html#spark-i-ssb-project",
    "title": "Jobbe med kode",
    "section": "Spark i ssb-project",
    "text": "Spark i ssb-project\nFor Ã¥ kunne bruke Spark i et ssb-project mÃ¥ man fÃ¸rst installere pyspark. Det gjÃ¸r du ved Ã¥ skrive fÃ¸lgende i en terminal:\n\n\nterminal\n\npoetry add pyspark==$(pip show pyspark | grep Version | egrep -o \"([0-9]{1,}\\.)+[0-9]{1,}\") --no-dev\n\nHer installerer du samme versjon av pyspark som pÃ¥ Jupyterlab.\nVidere kan vi konfigurere Spark til Ã¥ enten kjÃ¸re pÃ¥ lokal maskin eller pÃ¥ flere maskiner (sÃ¥kalte clusters). Under beskriver vi begge variantene.\n\nLokal maskin\nOppsettet for Pyspark pÃ¥ lokal maskin er det enkleste Ã¥ sette opp siden Pyspark vil ha direkte tilgang til det lokale filsystemet. Man kan bruke miljÃ¸variabelen PYSPARK_PYTHON til Ã¥ peke pÃ¥ det virtuelle miljÃ¸et, og dermed vil Pyspark ogsÃ¥ ha tilgang til alle pakkene som er installert der. I en notebook vil dette kunne settes opp slik:\n\n\nnotebook\n\nimport os\nimport subprocess\n\n# Finner filstien til det virtuelle miljÃ¸et\npython_path = subprocess.run(['poetry', 'run', 'which', 'python'],\n                             capture_output=True, text=True).stdout.rstrip('\\n')\n\nos.environ[\"PYSPARK_PYTHON\"] = python_path\nos.environ[\"PYSPARK_SUBMIT_ARGS\"] = os.environ[\"PYSPARK_LOCAL_SUBMIT_ARGS\"]\n\nNÃ¥r du oppretter en Notebook og bruker den kernelen du har laget sÃ¥ mÃ¥ du alltid ha denne pÃ¥ toppen av notebooken. Det initialiserer Spark slik at det kan brukes i notebooken.\n\n\nnotebook\n\n%run /usr/local/share/jupyter/kernels/pyspark_local/init.py\n\nDette scriptet vil sette et spark objekt som brukes for Ã¥ kalle APIâ€™et til pyspark.\n\n\nCluster\nHvis man vil kjÃ¸re Pyspark i et cluster (dvs. pÃ¥ flere maskiner) sÃ¥ vil databehandlingen foregÃ¥ pÃ¥ andre maskiner som ikke har tilgang til det lokale filsystemet. Man mÃ¥ dermed lage en â€œpakkeâ€ av det virtuelle miljÃ¸et pÃ¥ lokal maskin og tilgjengeliggjÃ¸re dette for alle maskinene i clusteret. For Ã¥ lage en slik â€œpakkeâ€ kan man bruke et bibliotek som heter venv-pack. Dette kan kjÃ¸res fra et terminalvindu slik:\n\n\nterminal\n\nvenv-pack -p .venv -o pyspark_venv.tar.gz\n\nMerk at kommandoen over mÃ¥ kjÃ¸res fra rot-mappen i prosjektet ditt. Her er pyspark_venv.tar.gz et tilfeldig valgt filnavn, men dette filnavnet skal brukes videre i notebooken.\n\n\nnotebook\n\nimport os\nimport subprocess\n\n# MiljÃ¸variabel som peker pÃ¥ en utpakket versjon av det virtuelle miljÃ¸et\nos.environ[\"PYSPARK_PYTHON\"] = \"./environment/bin/python\"\n\n# Legg til et flagg, --archives, som peker pÃ¥ \"pakken\" med det virtuelle miljÃ¸et\nconf = os.environ[\"PYSPARK_K8S_SUBMIT_ARGS\"].split(' ')\nlast_index = conf.index('pyspark-shell')\nconf[last_index:last_index] = ['--archives', 'pyspark_venv.tar.gz#environment']\nos.environ[\"PYSPARK_SUBMIT_ARGS\"] = ' '.join(conf)\n\nNÃ¥r du oppretter en Notebook og bruker den kernelen du har laget sÃ¥ mÃ¥ du alltid ha denne pÃ¥ toppen av notebooken. Det initialiserer Spark slik at det kan brukes i notebooken.\n\n\nnotebook\n\n%run /usr/local/share/jupyter/kernels/pyspark_k8s/init.py\n\nDette scriptet vil sette et spark objekt som brukes for Ã¥ kalle APIâ€™et til pyspark.",
    "crumbs": [
      "Statistikere",
      "Programmering"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#tips-og-triks",
    "href": "statistikkere/jobbe-med-kode.html#tips-og-triks",
    "title": "Jobbe med kode",
    "section": "Tips og triks",
    "text": "Tips og triks\nI denne delen av kapitlet vil vi gi deg noen tips og triks som kan vÃ¦re nyttige nÃ¥r du jobber med ssb-project.\n\nPoetry\nssb-project bruker Poetry for Ã¥ hÃ¥ndtere virtuelle miljÃ¸er. Poetry er et verktÃ¸y som gjÃ¸r det enkelt Ã¥ installere pakker og hÃ¥ndtere versjoner av disse. Det er ogsÃ¥ Poetry som hÃ¥ndterer Jupyter-kernelen for deg.\nHvis du etterlyser funksjonalitet i et ssb-project sÃ¥ kan det vÃ¦re nyttig Ã¥ lese dokumentasjonen til Poetry for Ã¥ se om det er mulig Ã¥ fÃ¥ til det du Ã¸nsker.\n\n\nFull disk pÃ¥ Dapla\nDet â€œlokaleâ€ filsystemet pÃ¥ Dapla har kun 10GB diskplass. Har du mange virtuelle miljÃ¸er pÃ¥ denne disken kan det fort bli fullt, siden alle pakker blir installert her. Vanligvis er det 2 grunner til at disken blir full:\n\nFor mange virtuelle miljÃ¸er (ssb-projects) lagret lokalt.\nDette vil ofte kunne lÃ¸ses ved Ã¥ slette virtuelle miljÃ¸er som ikke lenger er i bruk. Hvis du har 5 virtuelle miljÃ¸er som hver bruker 1GB, og du kun jobber pÃ¥ en av de nÃ¥, sÃ¥ vil du frigjÃ¸re 40% av disken ved Ã¥ slette 4 av dem. Husk at det permanente lagringsstedet for kode er pÃ¥ GitHub, og du kan alltid klone ned et prosjekt senere og bygge det hvis det trengs.\n/home/jovyan/.cache/ har blitt for stort.\nDette er en mappe som brukes av applikasjoner til Ã¥ lagre midlertidig data slik at de kan kjÃ¸re raskere. Denne kan bli ganske stor etter hvert. Ofte kan man frigjÃ¸re flere GB ved Ã¥ slette denne. Du sletter denne mappen ved Ã¥ skrive fÃ¸lgende i en terminal:\n\n\n\nterminal\n\nrm -rf /home/jovyan/.cache/\n\nHvis du opplever at disken er full, sÃ¥ kan det anbefales Ã¥ undersÃ¸ke hvilke mapper som tar stÃ¸rst plass med fÃ¸lgende kommando i terminalen:\n\n\nterminal\n\ncd ~ && du -h --max-depth=5 | sort -rh | head -n 10 && cd -\n\nKommandoen over sjekker, fra hjemmemappen din, hvilke mapper og undermapper som tar mest plass. Den viser de 10 stÃ¸rste mappene. Hvis du Ã¸nsker Ã¥ se flere mapper sÃ¥ kan du endre tallet etter head -n. Hvis du Ã¸nsker Ã¥ se alle mapper sÃ¥ kan du fjerne head -n. --max-depth=5 betyr at den kun sjekker mapper som er 5 mapper dype fra hjemmemappen din.\nNÃ¥r du har gjort det kan selv vurdere hvilke som kan slettes for Ã¥ frigjÃ¸re plass.\n\n\nHold prosjektet oppdatert\nHvis du sitter med en lokal kopi av prosjektet ditt, og flere andre jobber med den samme kodebasen, sÃ¥ er det viktig at du holder din lokale kopi oppdatert. Hvis du jobber i en branch pÃ¥ en lokal kopi, bÃ¸r du holde denne oppdatert med main-branchen pÃ¥ GitHub. Det er vanlig Git-praksis. NÃ¥r man ogsÃ¥ bruker ssb-project, sÃ¥ man huske Ã¥ ogsÃ¥ bygge prosjektet pÃ¥ nytt hver gang det er endringer som er gjort av andre i poetry.lock.-filen.",
    "crumbs": [
      "Statistikere",
      "Programmering"
    ]
  },
  {
    "objectID": "statistikkere/jobbe-med-kode.html#footnotes",
    "href": "statistikkere/jobbe-med-kode.html#footnotes",
    "title": "Jobbe med kode",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nEn kernel refererer til en Python- eller R-installasjon som er optimalisert for bruk med Jupyterlab Notebooks.â†©ï¸\nCLI = Command-Line-Interface, som betyr et program designet for bruk i terminalen med kommandoer.â†©ï¸\nFiler og mapper som starter med punktum er skjulte med mindre man ber om Ã¥ se dem. I Jupyterlab kan disse vises i filutforskeren ved Ã¥ velge View fra menylinjen, og deretter velge Show hidden files. I en terminal skriver man ls -a for Ã¥ se de.â†©ï¸\nÃ… pushe til GitHub uten Ã¥ sende ved Personal Access Token fordrer at du har lagret det lokalt sÃ¥ Git kan finne det. Her et eksempel pÃ¥ hvordan det kan gjÃ¸res.â†©ï¸\nDette kan ogsÃ¥ gjÃ¸res ved Ã¥ hÃ¸yreklikke pÃ¥ mappen i Jupyterlab sin filutforsker og velge Delete.â†©ï¸",
    "crumbs": [
      "Statistikere",
      "Programmering"
    ]
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "SpÃ¸rsmÃ¥l og svar",
    "section": "",
    "text": "Hvordan finner jeg et Google-prosjekt sin prosjekt-ID?\nProsjekt-ID-en til et Google-prosjekt er en unik identifikator som brukes til Ã¥ identifisere prosjektet i Google Cloud Platform. Prosjekt-ID-en er en streng som bestÃ¥r av smÃ¥ bokstaver, tall og bindestrek. Prosjekt-ID-en er ikke det samme som prosjektnavnet, som kan inneholde store bokstaver og mellomrom.\nDu finner prosjekt-ID ved logge deg inn pÃ¥ GCC, Ã¥pne prosjektvelgeren, sÃ¸k opp ditt prosjekt, og sÃ¥ ser du det i hÃ¸yre kolonne, slik som vist i denne sladdete kolonnen i FigurÂ 1.\n\n\n\n\n\n\nFigurÂ 1: Prosjektvelgeren i Google Cloud Console\n\n\n\n\n\nHvordan fÃ¥r jeg slettet et GitHub-repo under statisticsnorway?\nHovedregelen er at vi arkiverer repoer istedenfor Ã¥ slette. Det skyldes at vi kan trenge Ã¥ ettergÃ¥ historikken i repoer ved et senere tidspunkt. Arkivering av repoer kan du gjÃ¸re selv under Settings i repoet.\nI de tilfellene der du mener at det gir mest mening Ã¥ slette repoet, sÃ¥ mÃ¥ dette gjÃ¸res av en Github-administrator. Da sender du en henvendelse til Kundeservice og ber om at repoet slettes. Husk Ã¥ oppgi navnet pÃ¥ repoet du Ã¸nsker Ã¥ fÃ¥ slettet.\n\n\nHvordan lÃ¸ser jeg feilmeldinger knyttet til at data rate exceeded i Jupyter?\nNÃ¥r du mottar fÃ¸lgende melding i Jupyter:\n\nFeilmelding:\nIOPub data rate exceeded.\nThe Jupyter server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--ServerApp.iopub_data_rate_limit`.\n\nCurrent values:\nServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\nServerApp.rate_limit_window=3.0 (secs)\n\nbetyr det at mengden data som sendes fra jupyter-kernelen til jupyterlab-frontend overskrider den tillatte grensen. Selv om det er mulig Ã¥ justere ServerApp.iopub_data_rate_limit og ServerApp.rate_limit_window for Ã¥ endre denne grensen, Ã¸nsker vi ikke dette. Ã… endre disse verdiene kan ha en negativ pÃ¥virkning pÃ¥ Jupyterlab sin ytelse.\n\nHer er noen lÃ¸sningsforslag:\n\nReduser datamengden: PrÃ¸v Ã¥ redusere datamengden du prÃ¸ver Ã¥ vise. Hvis du for eksempel viser en stor pandas dataframe, kan du vise kun toppradene med df.head() eller et tilfeldig utvalg med df.sample(10).\nLegg til forsinkelse: Bruk time.sleep()-funksjonen i Python for Ã¥ legge til en pause mellom hver utskrift. Dette kan spre utdataene over en lengre tidsperiode, noe som kan hjelpe med Ã¥ unngÃ¥ Ã¥ overskride datagrensen.\nSkriv til en fil: I stedet for Ã¥ skrive utdata direkte i Jupyter, kan du vurdere Ã¥ skrive dataene til en fil. Dette omgÃ¥r IOPub-datahastighetsgrensen, og du kan se gjennom dataene i ettertid.\nUnngÃ¥ utskrift: Hvis du kun trenger Ã¥ utfÃ¸re beregninger eller operasjoner pÃ¥ dataene, vurder Ã¥ gjÃ¸re det uten Ã¥ skrive ut resultatene i Jupyter.\n\n\n\n\nHvordan kan jeg gjennopprette data fra bÃ¸tter?\nAlle bÃ¸tter har automatisk versjonering. Dette gjÃ¸r det mulig Ã¥ tilbakefÃ¸re filer til en tidligere versjon eller gjenopprette filer som er slettet ved et uhell. Logg inn pÃ¥ Google Cloud Console og sÃ¸k opp â€œCloud Storageâ€ i sÃ¸kefeltet. Klikk pÃ¥ den bÃ¸tten hvor filen er lagret under â€œBucketsâ€.\n\nGjenopprette en slettet fil\nFra Cloud Storage skjermbildet kan man navigere seg frem til den mappen hvor filen tidligere er lagret og skru pÃ¥ radioknappen â€œShow deleted dataâ€ (FigurÂ 2)\n\n\n\n\n\n\nFigurÂ 2: Skru pÃ¥ visning av slettede filer\n\n\n\nNÃ¥ vil man kunne se slettede filer i kursiv med teksten (Deleted) pÃ¥ slutten. Kolonnen â€œVersion historyâ€ vil ogsÃ¥ vise hvor mange tidligere versjoner som finnes av denne filen. Trykk pÃ¥ filnavnet du Ã¸nsker Ã¥ gjenopprette og velg deretter fanen â€œVersion historyâ€. I listen av versjoner til denne filen har man mulighet til Ã¥ gjenopprette til en tidligere versjon ved Ã¥ klikke pÃ¥ â€œRestoreâ€ (FigurÂ 3).\n\n\n\n\n\n\nFigurÂ 3: Gjenoppretting av en slettet fil\n\n\n\n\n\nGjenopprette en fil til en tidligere versjon\nFra Cloud Storage skjermbildet kan man navigere seg frem til den mappen hvor filen er lagret, og trykke pÃ¥ filnavnet. Velg deretter fanen â€œVersion historyâ€. I listen av versjoner til denne filen har man mulighet til Ã¥ gjenopprette til en tidligere versjon ved Ã¥ klikke pÃ¥ â€œRestoreâ€ (FigurÂ 4).\n\n\n\n\n\n\nFigurÂ 4: Versjonshistorikk til en fil\n\n\n\n\n\n\nHvordan sletter jeg data fra bÃ¸tter?\nSletting av filer og mapper fra bÃ¸tter kan gjÃ¸res fra Google Cloud Console. SÃ¸k opp â€œCloud Storageâ€ i sÃ¸kefeltet og klikk pÃ¥ den bÃ¸tten hvor filen er lagret under â€œBucketsâ€.\nKryss av filen/katalogen som du Ã¸nsker Ã¥ slette og trykk â€œDeleteâ€ (FigurÂ 5)\n\n\n\n\n\n\nFigurÂ 5: Sletting av en fil\n\n\n\nSiden bÃ¸tter pÃ¥ Dapla har versjonering fÃ¥r man opp en dialogboks som informerer om at objektet (dvs. filen) er versjonert (FigurÂ 6). Trykk pÃ¥ â€œDeleteâ€.\n\n\n\n\n\n\nFigurÂ 6: Bekreft sletting av fil\n\n\n\nSlettingen kan ta noe tid. NÃ¥r denne er ferdig vil filen vÃ¦re slettet, men den kan fortsatt gjenopprettes. Hvis du Ã¸nsker at filen skal slettes permanent, gjÃ¸r fÃ¸lgende:\n\nSkru pÃ¥ visning av slettede filer med Ã¥ bruke radioknappen â€œShow deleted dataâ€ (FigurÂ 7)\n\n\n\n\n\n\n\nFigurÂ 7: Skru pÃ¥ visning av slettede filer\n\n\n\n\nFinn frem til den slettede filen og trykk pÃ¥ linken â€œ1 noncurrent versionâ€ eller tilsvarende (FigurÂ 8). Dette vil ta deg direkte til en side som viser filens versjonshistorikk.\n\n\n\n\n\n\n\nFigurÂ 8: Velg versjonshistorikk\n\n\n\n\nVelg alle versjoner som vist pÃ¥ FigurÂ 9 og trykk â€œDeleteâ€\n\n\n\n\n\n\n\nFigurÂ 9: Slett alle versioner\n\n\n\n\nTil slutt mÃ¥ man bekrefte at man Ã¸nsker Ã¥ slette alle versioner (FigurÂ 10) med Ã¥ skrive inn DELETE og trykke pÃ¥ den blÃ¥ â€œDeleteâ€-knappen:\n\n\n\n\n\n\n\nFigurÂ 10: Bekreft sletting av alle versjoner\n\n\n\n\n\nHvordan sjekker jeg om Kildomaten er tilgjengelig for mitt team?\n\n\nDu kan sjekke om teamet ditt har tilgang til Kildomaten ved Ã¥ gÃ¥ inn i teamets IaC-repo. Du finner IaC-repoet ved Ã¥ sÃ¸ke etter team-navnet pÃ¥ statisticsnorway pÃ¥ Github. Repoet er det som heter &lt;teamnavn&gt;-iac. Inne i repoet gÃ¥r du inn filen infra/projects.yaml.\nI eksemplet til hÃ¸yre ser vi hvordan filen ser ut for det fiktive teamet dapla-example. For dette teamet ser vi at Kildomaten er skrudd pÃ¥ i prod-miljÃ¸et til teamet, men ikke test-miljÃ¸et.\n\n\n\n\n\ndapla-example-iac/infra/projects.yaml\n\nteam_uniform_name: dapla-example\n\nprojects:\n  - project_name: dapla-example\n    env: test\n    features:\n      - dapla-buckets\n\n  - project_name: dapla-example\n    env: prod\n    features:\n      - dapla-buckets\n      - transfer-service\n      - kildomaten\n\n\n\n\n\nHvordan kan jeg se innholdet i mitt team sine bÃ¸tter?\nFor Ã¥ se innholdet i ditt teams sine bÃ¸tter kan du enten gjÃ¸re dette med kode fra Jupyter, eller via Google Cloud Console (GCC). Les mer om hvordan man kan benytte GCC i SSB.\nHusk at det er kun data-admins som skal kunne se innhold i kildeprosjektet til teamet. For Ã¥ gjÃ¸re det mÃ¥ de aktivere en tilgang i JIT-applikasjonen."
  },
  {
    "objectID": "utviklere/tldr/kvalitet.html#mÃ¥-iverksette-four-eyes-prinsippet-gjennom-kodegjennomgang-ogeller-parprogrammering.",
    "href": "utviklere/tldr/kvalitet.html#mÃ¥-iverksette-four-eyes-prinsippet-gjennom-kodegjennomgang-ogeller-parprogrammering.",
    "title": "Kvalitet",
    "section": "MÃ… iverksette â€œFour Eyesâ€ prinsippet gjennom kodegjennomgang og/eller parprogrammering.",
    "text": "MÃ… iverksette â€œFour Eyesâ€ prinsippet gjennom kodegjennomgang og/eller parprogrammering.",
    "crumbs": [
      "Utviklere",
      "TL;DR",
      "Kvalitet"
    ]
  },
  {
    "objectID": "utviklere/tldr/kvalitet.html#mÃ¥-ha-pÃ¥-plass-prosesser-for-Ã¥-fange-opp-brukerbehov-og-eventuelle-feil-som-pÃ¥virker-sluttbrukere.",
    "href": "utviklere/tldr/kvalitet.html#mÃ¥-ha-pÃ¥-plass-prosesser-for-Ã¥-fange-opp-brukerbehov-og-eventuelle-feil-som-pÃ¥virker-sluttbrukere.",
    "title": "Kvalitet",
    "section": "MÃ… ha pÃ¥ plass prosesser for Ã¥ fange opp brukerbehov og eventuelle feil som pÃ¥virker sluttbrukere.",
    "text": "MÃ… ha pÃ¥ plass prosesser for Ã¥ fange opp brukerbehov og eventuelle feil som pÃ¥virker sluttbrukere.",
    "crumbs": [
      "Utviklere",
      "TL;DR",
      "Kvalitet"
    ]
  },
  {
    "objectID": "utviklere/tldr/kvalitet.html#bÃ¸r-dekke-viktige-brukerflyt-med-ende-til-ende-tester.",
    "href": "utviklere/tldr/kvalitet.html#bÃ¸r-dekke-viktige-brukerflyt-med-ende-til-ende-tester.",
    "title": "Kvalitet",
    "section": "BÃ˜R dekke viktige brukerflyt med ende-til-ende tester.",
    "text": "BÃ˜R dekke viktige brukerflyt med ende-til-ende tester.",
    "crumbs": [
      "Utviklere",
      "TL;DR",
      "Kvalitet"
    ]
  },
  {
    "objectID": "utviklere/tldr/kvalitet.html#bÃ¸r-kjÃ¸re-tester-og-kvalitetsverktÃ¸y-sÃ¥-tidlig-som-mulig-i-utviklingsprosessen.",
    "href": "utviklere/tldr/kvalitet.html#bÃ¸r-kjÃ¸re-tester-og-kvalitetsverktÃ¸y-sÃ¥-tidlig-som-mulig-i-utviklingsprosessen.",
    "title": "Kvalitet",
    "section": "BÃ˜R kjÃ¸re tester og kvalitetsverktÃ¸y sÃ¥ tidlig som mulig i utviklingsprosessen.",
    "text": "BÃ˜R kjÃ¸re tester og kvalitetsverktÃ¸y sÃ¥ tidlig som mulig i utviklingsprosessen.\nOgsÃ¥ kjent som â€œShift Leftâ€ prinsippet, dette fanger opp sÃ¥ mange potensielle bugs som mulig tidlig, hvor de er lettere Ã¥ fikse.\nEksempler av dette er:\n\nVerktÃ¸y som kjÃ¸rer direkte i IDEen.\nVerktÃ¸y kjÃ¸rende i â€œpre-commit hooksâ€ som kjÃ¸rer nÃ¥r man fÃ¸rst commiter kode.\nVerktÃ¸y som kjÃ¸rer nÃ¥r man fÃ¸rst pusher kode til et repo.",
    "crumbs": [
      "Utviklere",
      "TL;DR",
      "Kvalitet"
    ]
  },
  {
    "objectID": "utviklere/tldr/kvalitet.html#bÃ¸r-anvende-kodeformatteringsverktÃ¸y-slik-at-kodebaser-er-formatert-jevnt.",
    "href": "utviklere/tldr/kvalitet.html#bÃ¸r-anvende-kodeformatteringsverktÃ¸y-slik-at-kodebaser-er-formatert-jevnt.",
    "title": "Kvalitet",
    "section": "BÃ˜R anvende kodeformatteringsverktÃ¸y slik at kodebaser er formatert jevnt.",
    "text": "BÃ˜R anvende kodeformatteringsverktÃ¸y slik at kodebaser er formatert jevnt.\nIdeelt sett skal disse kjÃ¸res automatisk.\nEksempler av kodeformatteringsverktÃ¸y er:\n\nEditorConfig\nPrettier (for Javascript bl.a.)\nBlack (for Python)\nterraform fmt (for Terraform)",
    "crumbs": [
      "Utviklere",
      "TL;DR",
      "Kvalitet"
    ]
  },
  {
    "objectID": "utviklere/tldr/kvalitet.html#bÃ¸r-anvende-statiskkodeanalyseverktÃ¸y-lint-verktÃ¸y.",
    "href": "utviklere/tldr/kvalitet.html#bÃ¸r-anvende-statiskkodeanalyseverktÃ¸y-lint-verktÃ¸y.",
    "title": "Kvalitet",
    "section": "BÃ˜R anvende statiskkodeanalyseverktÃ¸y (â€œLintâ€-verktÃ¸y).",
    "text": "BÃ˜R anvende statiskkodeanalyseverktÃ¸y (â€œLintâ€-verktÃ¸y).",
    "crumbs": [
      "Utviklere",
      "TL;DR",
      "Kvalitet"
    ]
  },
  {
    "objectID": "utviklere/tldr/tldr.html",
    "href": "utviklere/tldr/tldr.html",
    "title": "TL;DR",
    "section": "",
    "text": "Her finnes det ikke lange beskrivelser, bare akkurat det du trenger Ã¥ vite som utvikler i SSB.\n\n\nHver TL;DR:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBetydningen defineres av RFC 2119, med tilhÃ¸rende oversettelse til norsk.",
    "crumbs": [
      "Utviklere",
      "TL;DR"
    ]
  },
  {
    "objectID": "utviklere/tldr/tldr.html#meta",
    "href": "utviklere/tldr/tldr.html#meta",
    "title": "TL;DR",
    "section": "",
    "text": "Hver TL;DR:",
    "crumbs": [
      "Utviklere",
      "TL;DR"
    ]
  },
  {
    "objectID": "utviklere/tldr/tldr.html#begrepsforklaring",
    "href": "utviklere/tldr/tldr.html#begrepsforklaring",
    "title": "TL;DR",
    "section": "",
    "text": "Betydningen defineres av RFC 2119, med tilhÃ¸rende oversettelse til norsk.",
    "crumbs": [
      "Utviklere",
      "TL;DR"
    ]
  },
  {
    "objectID": "utviklere/tldr/kjoeremiljoe.html#mÃ¥-bruke-en-service-account-for-autentisering-fra-en-tjeneste.",
    "href": "utviklere/tldr/kjoeremiljoe.html#mÃ¥-bruke-en-service-account-for-autentisering-fra-en-tjeneste.",
    "title": "KjÃ¸remiljÃ¸",
    "section": "MÃ… bruke en Service Account for autentisering fra en tjeneste.",
    "text": "MÃ… bruke en Service Account for autentisering fra en tjeneste.",
    "crumbs": [
      "Utviklere",
      "TL;DR",
      "KjÃ¸remiljÃ¸"
    ]
  },
  {
    "objectID": "utviklere/deploy.html",
    "href": "utviklere/deploy.html",
    "title": "Deploy applikasjon",
    "section": "",
    "text": "TL;DR:\n\nApplikasjoner deployes til BiP fra teamet sitt eget git repo (flux2-tenant-&lt;teamnavn&gt; eller &lt;teamnavn&gt;-iac)\nAutomatisk oppdatering av images krever et par ekstra manifester for at flux skal fange opp endringene\n\n\n\nFor utrulling av applikasjoner, og manifester/filer pÃ¥ kubernetes bruker vi Flux. Flux er et verktÃ¸y for Ã¥ synce filer i et git repositoriet med kubernetes. Denne tilnÃ¦rmingen for Ã¥ ha kontroll pÃ¥ manifestene kalles gitops, og gjÃ¸r at du som utvikler slipper Ã¥ forholde deg til kubernetes-kommandoer eller andre for Ã¥ fÃ¥ ut endringene du Ã¸nsker.\nFlux tilbyr ogsÃ¥ en rekke funksjonalitet, blant annet automatisk utrulling av nye versjoner nÃ¥r en ny versjon av et image er tilgjengelig.\n\n\n\nFor BiP har vi to KubernetesmiljÃ¸er dit applikasjoner kan bli deployet til:\n\nstaging-bip-app: Staging/test\nprod-bip-app: Produksjon\n\nMiljÃ¸ene er i utgangspunktet sÃ¥Â like som mulig.\n\n\n\nForutsetninger:\n\nDu er medlem av et team som er etablert pÃ¥ kubernetes (og dermed har tilgang til github repositoriet hvor manifestene ligger)\nImaget for applikasjonen bygges til teamet sitt artifakt repository (eller container registry om teamet ikke er pÃ¥ ny teamstruktur)\n\nNÃ¸dvendig infrastruktur provisjoneres fortrinnsvins via teamets iac-repo med terraform og atlantis.\nFor Ã¥ legge til en ny applikasjon i clusteret (med ssb-chart) er planen grovt sett:\n\nOpprett en fil appnavn.yaml i korrekt mappe (&lt;cluster-navn&gt;/&lt;evt mappe&gt;).\nFilen skal inneholde fÃ¸lgende (kan skilles ut i tre enkeltfiler), separert med ---. Det er mulig Ã¥ ta inspirasjon av tidligere manfiester.\n\nEt manifest for ImageRepository, som peker pÃ¥ hvor imaget ligger\nEt manifest for ImagePolicy som sier hvordan automatisk oppdatering av imaget skal foregÃ¥r\nEt manifest for HelmRelease, der ssb-chart benyttes. Dobbeltsjekk at kommentarene etter image og tag peker til korrekt ImagePolicy (feks # {\"$imagepolicy\": \"&lt;namespace&gt;:&lt;navn pÃ¥ image policy-ressurs&gt;:name\"}). Det er denne kommentaren som gjÃ¸r at flux fÃ¸lger med pÃ¥ om ny versjon av applikasjonen skal rulles ut. Hvis man ikke Ã¸nsker autoopgradering av imaget sitt kan imagepolicy-kommentaren, ImageRepository og ImagePolicy fjernes.\n\nLegg til filen i kustomization.yaml-filen\nDu kan dobbeltsjekke at konfigurasjonen har blitt korrekt ved Ã¥ kjÃ¸re kustomize build .. (krever at du har kustomize cli installert).\n\nEtter dette er commitet og merget til main-branchen vil Flux plukke opp endringen.\nNotifikasjoner blir sendt til teamet sin cd-slack-kanel (#cd-&lt;team-navn&gt;).\n\n\n\nFlux vil periodisk sjekke git repoet til teamet for endringer. Ã˜nsker man likevel Ã¥ trigge flux til Ã¥ gjÃ¸re dette umiddelbart kan det gjÃ¸res ved Ã¥ kjÃ¸re kommandoen: flux reconcile kustomization &lt;namespace/team-navn&gt; -n &lt;namespace/team-navn&gt; --with-source\n\n\n\nflux get all -n &lt;namespace&gt;\n\n\n\nI forbindelse med oppgradering av flux fra v1 til v2 var det nÃ¸dvendig Ã¥ gjÃ¸re en rekke endringer.\n\n\nFiler som tidligere lÃ¥ i platform-dev ligger nÃ¥ i teamet sitt eget git repo (flux2-tenant-&lt;teamnavn&gt; eller &lt;teamnavn&gt;-iac under /apps/bip). Strukturen matcher det som tidligere lÃ¥ i platform-dev, der man legger manifestene inn i en mappe som bestemmer hvilket cluster man gÃ¥r mot.\nKustomize blir benyttet for Ã¥ styre hvilke filer og konfigurasjon som blir inkludert, gjennom filen kustomization.yaml (som kan forekomme i undermapper ogsÃ¥). Det skal ikke vÃ¦re behov for Ã¥ endre denne filen med tanke pÃ¥ at dette ble gjort under migrering.\n\n\n\nNoen brukte tidligere et bash-skript liggende i platform-dev for Ã¥ generere grafana dashboard config maps utfra json-eksporten i grafana. Dette skriptet er fortsatt mulig Ã¥ benytte ( https://github.com/statisticsnorway/platform-dev/blob/master/bin/dashboard.sh ), og husk at korrekt output mappe mÃ¥ settes. Etter filen er generert mÃ¥ dashboardet inkluderes i kustomization.yaml i den aktuelle mappen for at filen blir plukket opp.\n\n\n\n\nShould not be copy pasted without considering each configuration\n# See documentation for the ImageRepository fields here: https://fluxcd.io/flux/components/image/imagerepositories/\napiVersion: image.toolkit.fluxcd.io/v1beta2\nkind: ImageRepository\nmetadata:\n  name: &lt;app-name&gt;\n  namespace: &lt;team-name&gt;\nspec:\n  # If team is on new structure they have their own repo on artifact repository\n  image: europe-north1-docker.pkg.dev/artifact-registry-5n/&lt;team-navn&gt;-docker/&lt;image-name&gt;\n  interval: 10m0s\n  provider: gcp\n---\n# See documentation for the ImagePolicy fields here: https://fluxcd.io/flux/components/image/imagepolicies/\napiVersion: image.toolkit.fluxcd.io/v1beta2\nkind: ImagePolicy\nmetadata:\n  name: &lt;app-name&gt;\n  namespace: &lt;team-name&gt;\nspec:\n  imageRepositoryRef:\n    name: &lt;app-name&gt;\n  filterTags:\n    # Flux uses naming to decide which tag is newest, hence the need to specify a pattern if semver or other versoning is not used\n    # Some common patterns:\n    # 1. A incrementing number (e.g. global build id)\n    pattern: 'main-[a-z0-9]+-az(?P&lt;buildid&gt;[0-9]+)'\n    extract: '$buildid'\n    # 2. Timestamp of build (e.g. yyyyMMssâ€¦)\n    pattern: '^main-[a-f0-9]+-(?P&lt;ts&gt;[0-9]+)'\n    extract: '$ts'\n  policy:\n    # Semver is also possible. See docs\n    numerical:\n      order: asc\n---\napiVersion: helm.toolkit.fluxcd.io/v2beta2\nkind: HelmRelease\nmetadata:\n  name: &lt;app-name&gt;\n  namespace: &lt;team-name&gt;\n\n# See docs ( https://github.com/statisticsnorway/helm-charts/tree/main/source/ssb-chart ) for description of each field\nspec:\n  interval: 10m\n  chart:\n    spec:\n      chart: ssb-chart\n      version: 4.0.1\n      sourceRef:\n        kind: HelmRepository\n        name: ssb-helm-charts\n      interval: 1h\n  valuesFrom:\n    - kind: ConfigMap\n      name: ssb-chart-common-values\n      valuesKey: values.yaml\n      optional: false\n  releaseName: \"&lt;app-name&gt;\"\n  values:\n    name: \"&lt;app-name&gt;\"\n    appType: \"backend\"\n    cluster: \"&lt;env&gt;-bip-app\"\n    billingProject: \"ssb-&lt;team-navn&gt;\"\n    image:\n      # The comment after repository and tag maks Flux refer to the ImagePolicy defined above. Because of this comments, flux will update and commit back changes to this fields when the image updates\n      repository: europe-north1-docker.pkg.dev/artifact-â€¦ # {\"$imagepolicy\": \"&lt;namespace&gt;:&lt;app-name&gt;:name\"}\n      tag: 'main-â€¦' # {\"$imagepolicy\": \"&lt;namespace&gt;:&lt;app-name&gt;:tag\"}\n    port:\n      name: \"http\"\n      containerport: 8080\n    replicaCount: 1\n    access:\n      ingress:\n        internal:\n          # Allow traffic from jupyterhub to this application if jwt is correct. Use keycloak as jwt issuer.\n          - application: jupyterhub\n            namespace: jupyterhub\n            allow:\n              - jwt:\n                  issuer: https://keycloak.staging-bip-app.ssb.no/auth/realms/ssb\n                  audiences:\n                    - jupyterhub\n        external:\n          gateways:\n            # Allow traffic from internet where jwt has audience httpbin-fe and\n            - type: public\n              allow:\n                - jwt:\n                    issuer: https://keycloak.staging-bip-app.ssb.no/auth/realms/ssb\n                    audiences:\n                      - httpbin-fe\n                  when:\n                    - key: request.headers[Authorization]\n                      values: [\"Bearer *\"]\n      egress:\n        external:\n          # Allow traffic out of pod to the following hosts to the given port\n          - hosts:\n              - \"api.github.com\"\n              - \"pubsub.googleapis.com\"\n              - \"secretmanager.googleapis.com\"\n            ports:\n              - name: https\n                port: 443\n                protocol: HTTPS\n    resources:\n      # Set quotas for how much cpu and memory the application requests (i.e. garuenteed resources) and limits (i.e. when the applicaiton will get killed/restarted for going over it's limit).\n      enabled: true\n      limits:\n        memory: 4Gi\n      requests:\n        cpu: 200m\n        memory: 512Mi\n    metrics:\n      enabled: true\n      port: 8081\n      path: /actuator/prometheus\n    probes:\n      # Probes are used to indicate to kubernetes when the application is ready\n      # Liveness can be used to check for deadlocks (i.e. a running applications stops)\n      # Readiness is used to indicate when the application is ready to receive traffic\n      liveness:\n        enabled: true\n        livenessProbe:\n          httpGet:\n            port: 8081\n            path: /actuator/health/liveness\n      readiness:\n        enabled: true\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /actuator/health/readiness\n    # serviceAccount is used to couple a google SA to this application (workload identity).\n    serviceAccount:\n      annotations:\n        iam.gke.io/gcp-service-account: \"&lt;app-name&gt;-wi-sa@&lt;project-id&gt;.iam.gserviceaccount.com\"\n    environmentVariables:\n      - name: \"FOO\"\n        value: \"BAR\"\n    configs:\n      - name: my-config.properties\n        mountPath: /config\n        config: |\n          example.of.custom=config\n          server.port=8080",
    "crumbs": [
      "Utviklere",
      "Deploy"
    ]
  },
  {
    "objectID": "utviklere/deploy.html#introduksjon",
    "href": "utviklere/deploy.html#introduksjon",
    "title": "Deploy applikasjon",
    "section": "",
    "text": "For utrulling av applikasjoner, og manifester/filer pÃ¥ kubernetes bruker vi Flux. Flux er et verktÃ¸y for Ã¥ synce filer i et git repositoriet med kubernetes. Denne tilnÃ¦rmingen for Ã¥ ha kontroll pÃ¥ manifestene kalles gitops, og gjÃ¸r at du som utvikler slipper Ã¥ forholde deg til kubernetes-kommandoer eller andre for Ã¥ fÃ¥ ut endringene du Ã¸nsker.\nFlux tilbyr ogsÃ¥ en rekke funksjonalitet, blant annet automatisk utrulling av nye versjoner nÃ¥r en ny versjon av et image er tilgjengelig.",
    "crumbs": [
      "Utviklere",
      "Deploy"
    ]
  },
  {
    "objectID": "utviklere/deploy.html#miljÃ¸",
    "href": "utviklere/deploy.html#miljÃ¸",
    "title": "Deploy applikasjon",
    "section": "",
    "text": "For BiP har vi to KubernetesmiljÃ¸er dit applikasjoner kan bli deployet til:\n\nstaging-bip-app: Staging/test\nprod-bip-app: Produksjon\n\nMiljÃ¸ene er i utgangspunktet sÃ¥Â like som mulig.",
    "crumbs": [
      "Utviklere",
      "Deploy"
    ]
  },
  {
    "objectID": "utviklere/deploy.html#legge-til-en-ny-applikasjon",
    "href": "utviklere/deploy.html#legge-til-en-ny-applikasjon",
    "title": "Deploy applikasjon",
    "section": "",
    "text": "Forutsetninger:\n\nDu er medlem av et team som er etablert pÃ¥ kubernetes (og dermed har tilgang til github repositoriet hvor manifestene ligger)\nImaget for applikasjonen bygges til teamet sitt artifakt repository (eller container registry om teamet ikke er pÃ¥ ny teamstruktur)\n\nNÃ¸dvendig infrastruktur provisjoneres fortrinnsvins via teamets iac-repo med terraform og atlantis.\nFor Ã¥ legge til en ny applikasjon i clusteret (med ssb-chart) er planen grovt sett:\n\nOpprett en fil appnavn.yaml i korrekt mappe (&lt;cluster-navn&gt;/&lt;evt mappe&gt;).\nFilen skal inneholde fÃ¸lgende (kan skilles ut i tre enkeltfiler), separert med ---. Det er mulig Ã¥ ta inspirasjon av tidligere manfiester.\n\nEt manifest for ImageRepository, som peker pÃ¥ hvor imaget ligger\nEt manifest for ImagePolicy som sier hvordan automatisk oppdatering av imaget skal foregÃ¥r\nEt manifest for HelmRelease, der ssb-chart benyttes. Dobbeltsjekk at kommentarene etter image og tag peker til korrekt ImagePolicy (feks # {\"$imagepolicy\": \"&lt;namespace&gt;:&lt;navn pÃ¥ image policy-ressurs&gt;:name\"}). Det er denne kommentaren som gjÃ¸r at flux fÃ¸lger med pÃ¥ om ny versjon av applikasjonen skal rulles ut. Hvis man ikke Ã¸nsker autoopgradering av imaget sitt kan imagepolicy-kommentaren, ImageRepository og ImagePolicy fjernes.\n\nLegg til filen i kustomization.yaml-filen\nDu kan dobbeltsjekke at konfigurasjonen har blitt korrekt ved Ã¥ kjÃ¸re kustomize build .. (krever at du har kustomize cli installert).\n\nEtter dette er commitet og merget til main-branchen vil Flux plukke opp endringen.\nNotifikasjoner blir sendt til teamet sin cd-slack-kanel (#cd-&lt;team-navn&gt;).",
    "crumbs": [
      "Utviklere",
      "Deploy"
    ]
  },
  {
    "objectID": "utviklere/deploy.html#trigge-flux-til-Ã¥-hente-en-endring",
    "href": "utviklere/deploy.html#trigge-flux-til-Ã¥-hente-en-endring",
    "title": "Deploy applikasjon",
    "section": "",
    "text": "Flux vil periodisk sjekke git repoet til teamet for endringer. Ã˜nsker man likevel Ã¥ trigge flux til Ã¥ gjÃ¸re dette umiddelbart kan det gjÃ¸res ved Ã¥ kjÃ¸re kommandoen: flux reconcile kustomization &lt;namespace/team-navn&gt; -n &lt;namespace/team-navn&gt; --with-source",
    "crumbs": [
      "Utviklere",
      "Deploy"
    ]
  },
  {
    "objectID": "utviklere/deploy.html#se-status-for-flux",
    "href": "utviklere/deploy.html#se-status-for-flux",
    "title": "Deploy applikasjon",
    "section": "",
    "text": "flux get all -n &lt;namespace&gt;",
    "crumbs": [
      "Utviklere",
      "Deploy"
    ]
  },
  {
    "objectID": "utviklere/deploy.html#migrering-fra-platform-dev",
    "href": "utviklere/deploy.html#migrering-fra-platform-dev",
    "title": "Deploy applikasjon",
    "section": "",
    "text": "I forbindelse med oppgradering av flux fra v1 til v2 var det nÃ¸dvendig Ã¥ gjÃ¸re en rekke endringer.\n\n\nFiler som tidligere lÃ¥ i platform-dev ligger nÃ¥ i teamet sitt eget git repo (flux2-tenant-&lt;teamnavn&gt; eller &lt;teamnavn&gt;-iac under /apps/bip). Strukturen matcher det som tidligere lÃ¥ i platform-dev, der man legger manifestene inn i en mappe som bestemmer hvilket cluster man gÃ¥r mot.\nKustomize blir benyttet for Ã¥ styre hvilke filer og konfigurasjon som blir inkludert, gjennom filen kustomization.yaml (som kan forekomme i undermapper ogsÃ¥). Det skal ikke vÃ¦re behov for Ã¥ endre denne filen med tanke pÃ¥ at dette ble gjort under migrering.\n\n\n\nNoen brukte tidligere et bash-skript liggende i platform-dev for Ã¥ generere grafana dashboard config maps utfra json-eksporten i grafana. Dette skriptet er fortsatt mulig Ã¥ benytte ( https://github.com/statisticsnorway/platform-dev/blob/master/bin/dashboard.sh ), og husk at korrekt output mappe mÃ¥ settes. Etter filen er generert mÃ¥ dashboardet inkluderes i kustomization.yaml i den aktuelle mappen for at filen blir plukket opp.",
    "crumbs": [
      "Utviklere",
      "Deploy"
    ]
  },
  {
    "objectID": "utviklere/deploy.html#eksempel-pÃ¥-manifest",
    "href": "utviklere/deploy.html#eksempel-pÃ¥-manifest",
    "title": "Deploy applikasjon",
    "section": "",
    "text": "Should not be copy pasted without considering each configuration\n# See documentation for the ImageRepository fields here: https://fluxcd.io/flux/components/image/imagerepositories/\napiVersion: image.toolkit.fluxcd.io/v1beta2\nkind: ImageRepository\nmetadata:\n  name: &lt;app-name&gt;\n  namespace: &lt;team-name&gt;\nspec:\n  # If team is on new structure they have their own repo on artifact repository\n  image: europe-north1-docker.pkg.dev/artifact-registry-5n/&lt;team-navn&gt;-docker/&lt;image-name&gt;\n  interval: 10m0s\n  provider: gcp\n---\n# See documentation for the ImagePolicy fields here: https://fluxcd.io/flux/components/image/imagepolicies/\napiVersion: image.toolkit.fluxcd.io/v1beta2\nkind: ImagePolicy\nmetadata:\n  name: &lt;app-name&gt;\n  namespace: &lt;team-name&gt;\nspec:\n  imageRepositoryRef:\n    name: &lt;app-name&gt;\n  filterTags:\n    # Flux uses naming to decide which tag is newest, hence the need to specify a pattern if semver or other versoning is not used\n    # Some common patterns:\n    # 1. A incrementing number (e.g. global build id)\n    pattern: 'main-[a-z0-9]+-az(?P&lt;buildid&gt;[0-9]+)'\n    extract: '$buildid'\n    # 2. Timestamp of build (e.g. yyyyMMssâ€¦)\n    pattern: '^main-[a-f0-9]+-(?P&lt;ts&gt;[0-9]+)'\n    extract: '$ts'\n  policy:\n    # Semver is also possible. See docs\n    numerical:\n      order: asc\n---\napiVersion: helm.toolkit.fluxcd.io/v2beta2\nkind: HelmRelease\nmetadata:\n  name: &lt;app-name&gt;\n  namespace: &lt;team-name&gt;\n\n# See docs ( https://github.com/statisticsnorway/helm-charts/tree/main/source/ssb-chart ) for description of each field\nspec:\n  interval: 10m\n  chart:\n    spec:\n      chart: ssb-chart\n      version: 4.0.1\n      sourceRef:\n        kind: HelmRepository\n        name: ssb-helm-charts\n      interval: 1h\n  valuesFrom:\n    - kind: ConfigMap\n      name: ssb-chart-common-values\n      valuesKey: values.yaml\n      optional: false\n  releaseName: \"&lt;app-name&gt;\"\n  values:\n    name: \"&lt;app-name&gt;\"\n    appType: \"backend\"\n    cluster: \"&lt;env&gt;-bip-app\"\n    billingProject: \"ssb-&lt;team-navn&gt;\"\n    image:\n      # The comment after repository and tag maks Flux refer to the ImagePolicy defined above. Because of this comments, flux will update and commit back changes to this fields when the image updates\n      repository: europe-north1-docker.pkg.dev/artifact-â€¦ # {\"$imagepolicy\": \"&lt;namespace&gt;:&lt;app-name&gt;:name\"}\n      tag: 'main-â€¦' # {\"$imagepolicy\": \"&lt;namespace&gt;:&lt;app-name&gt;:tag\"}\n    port:\n      name: \"http\"\n      containerport: 8080\n    replicaCount: 1\n    access:\n      ingress:\n        internal:\n          # Allow traffic from jupyterhub to this application if jwt is correct. Use keycloak as jwt issuer.\n          - application: jupyterhub\n            namespace: jupyterhub\n            allow:\n              - jwt:\n                  issuer: https://keycloak.staging-bip-app.ssb.no/auth/realms/ssb\n                  audiences:\n                    - jupyterhub\n        external:\n          gateways:\n            # Allow traffic from internet where jwt has audience httpbin-fe and\n            - type: public\n              allow:\n                - jwt:\n                    issuer: https://keycloak.staging-bip-app.ssb.no/auth/realms/ssb\n                    audiences:\n                      - httpbin-fe\n                  when:\n                    - key: request.headers[Authorization]\n                      values: [\"Bearer *\"]\n      egress:\n        external:\n          # Allow traffic out of pod to the following hosts to the given port\n          - hosts:\n              - \"api.github.com\"\n              - \"pubsub.googleapis.com\"\n              - \"secretmanager.googleapis.com\"\n            ports:\n              - name: https\n                port: 443\n                protocol: HTTPS\n    resources:\n      # Set quotas for how much cpu and memory the application requests (i.e. garuenteed resources) and limits (i.e. when the applicaiton will get killed/restarted for going over it's limit).\n      enabled: true\n      limits:\n        memory: 4Gi\n      requests:\n        cpu: 200m\n        memory: 512Mi\n    metrics:\n      enabled: true\n      port: 8081\n      path: /actuator/prometheus\n    probes:\n      # Probes are used to indicate to kubernetes when the application is ready\n      # Liveness can be used to check for deadlocks (i.e. a running applications stops)\n      # Readiness is used to indicate when the application is ready to receive traffic\n      liveness:\n        enabled: true\n        livenessProbe:\n          httpGet:\n            port: 8081\n            path: /actuator/health/liveness\n      readiness:\n        enabled: true\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /actuator/health/readiness\n    # serviceAccount is used to couple a google SA to this application (workload identity).\n    serviceAccount:\n      annotations:\n        iam.gke.io/gcp-service-account: \"&lt;app-name&gt;-wi-sa@&lt;project-id&gt;.iam.gserviceaccount.com\"\n    environmentVariables:\n      - name: \"FOO\"\n        value: \"BAR\"\n    configs:\n      - name: my-config.properties\n        mountPath: /config\n        config: |\n          example.of.custom=config\n          server.port=8080",
    "crumbs": [
      "Utviklere",
      "Deploy"
    ]
  },
  {
    "objectID": "utviklere/index.html",
    "href": "utviklere/index.html",
    "title": "Velkommen",
    "section": "",
    "text": "Velkommen\nDenne delen er rettet mot IT-utviklere som bygger applikasjoner pÃ¥ Dapla. Informasjonen du finner her skal gjÃ¸re det lettere Ã¥ vÃ¦re ny en utvikler pÃ¥ plattformen.",
    "crumbs": [
      "Utviklere",
      "Velkommen"
    ]
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html",
    "title": "Fra Fame til Python",
    "section": "",
    "text": "Mange i SSB har data lagret i Fame som de Ã¸nsker Ã¥ bearbeide med Python og R. Dette er spesielt relevant nÃ¥r man skal flytte statistikkproduksjon til Dapla. fython er en Python-pakke som gjÃ¸r dette pÃ¥ en enkel mÃ¥te for deg. Den lar deg eksportere data fra Fame med en enkel funksjon, og kan returnere dataene som enten CSV eller Pandas DataFrame.\nPakken finner du pÃ¥ GitHub."
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#installasjon",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#installasjon",
    "title": "Fra Fame til Python",
    "section": "Installasjon",
    "text": "Installasjon\nPakken er avhengig av at Fame er installert miljÃ¸et der den benyttes. Siden den er installert pÃ¥ sl-fame-1.ssb.no1 sÃ¥ vil de fÃ¦rreste har behov for Ã¥ installere den selv.\nSkulle du likevel Ã¸nske Ã¥ installere pakken selv kan det gjÃ¸res med Poetry pÃ¥ fÃ¸lgende mÃ¥te:\n\n\nterminal\n\npoetry add git+https://github.com/statisticsnorway/ssb-fame-to-python.git"
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#bruk-av-funksjonene",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#bruk-av-funksjonene",
    "title": "Fra Fame til Python",
    "section": "Bruk av funksjonene",
    "text": "Bruk av funksjonene\nfython har to funksjoner: fame_to_csv og fame_to_df. Begge disse funksjonene tar inn de samme argumentene og de er listet opp i TabellÂ 1.\n\n\n\nTabellÂ 1: Forklaring av argumentene i funksjonene til fython\n\n\n\n\n\n\n\n\n\n\n\nArgument\nForklaring\nfame_to_csv()\nfame_to_pandas()\n\n\n\n\ndatabases\nList of Fame databases to access (with full path).\nâœ“\nâœ“\n\n\nfrequency\nFrequency of the data (â€˜aâ€™, â€˜qâ€™, â€˜mâ€™).\nâœ“\nâœ“\n\n\ndate_from\nStart date for the data in Fame syntax (e.g., â€˜2023:1â€™ for quarterly, â€˜2023â€™ for annual).\nâœ“\nâœ“\n\n\ndate_to\nEnd date for the data in Fame syntax (e.g., â€˜2023:1â€™ for quarterly, â€˜2023â€™ for annual).\nâœ“\nâœ“\n\n\nsearch_string\nQuery string for fetching specific data. The search is not case sensitive, and â€œ^â€ and â€œ?â€ are wildcards (for exactly one and any number of characters, respectively)\nâœ“\nâœ“\n\n\ndecimals\nNumber of decimal places in the fetched data (default is 10).\nâœ“\nâœ“\n\n\npath\nPath to write the csv-file.\nâœ“\n\n\n\n\n\n\n\nLa se pÃ¥ noen eksempler.\n\nEksempler\nDersom vi Ã¸nsker Ã¥ hente alt i database1.db og database2.db fra januar 2012 til desember 2022, og fÃ¥ det returnert i en DataFrame, kan vi skrive fÃ¸lgende kode:\n\n\npython\n\nfrom fython import fame_to_pandas\n\ndf = fame_to_pandas(\n  ['sti/til/database1.db', 'sti/til/database2.db', 'm', '2012:1', '2022:12', '?']\n  )\n\nDersom vi i stedet Ã¸nsker Ã¥ hente alle serier som begynner pÃ¥ abc, slutter pÃ¥ d etterfulgt av ett vilkÃ¥rlig tegn, kan vi skrive fÃ¸lgende kode:\n\n\npython\n\nfrom fython import fame_to_pandas\n\ndf = fame_to_pandas(\n  ['sti/til/database1.db', 'sti/til/database2.db', 'm', '2012:1', '2022:12', 'abc?d^']\n  )\n\n? og ^ er altsÃ¥ jokertegn/wildcards som representerer henholdvis et vilkÃ¥rlig antall tegn og nÃ¸yaktig ett tegn.\nDersom vi i stedet vil lagre dataene til en csv-fil kan vi skrive\n\n\npython\n\nfrom fython import fame_to_csv\n\nfame_to_csv(\n  ['sti/til/database1.db', 'sti/til/database2.db', 'm', '2012:1', '2022:12', 'abc?d^', 'sti/til/csv-fil.csv']\n  )\n\n\n\n\n\n\n\nWarning\n\n\n\nDet er viktig Ã¥ pÃ¥peke at enhver serie kun skrives Ã©n gang, og da fra den fÃ¸rste databasen den finnes i (kronologisk iht. til listen med databaser)."
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#kjÃ¸ringer-pÃ¥-serveren",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#kjÃ¸ringer-pÃ¥-serveren",
    "title": "Fra Fame til Python",
    "section": "KjÃ¸ringer pÃ¥ serveren",
    "text": "KjÃ¸ringer pÃ¥ serveren\nNÃ¥r du skal bruke fython sÃ¥ mÃ¥ du ta hensyn til hvilken server Fame er installert pÃ¥, og hvilken server du har tenkt til Ã¥ jobbe pÃ¥. Fame er som sagt installert pÃ¥ sl-fame-1.ssb.no, mens Jupyterlab er installert pÃ¥ sl-jupyter-p.ssb.no. Dvs. at hvis du Ã¸nsker Ã¥ bruke fython i en notebook i Jupyterlab, sÃ¥ mÃ¥ du bruke ssh til Ã¥ koble deg til sl-fame-1.ssb.no, og sÃ¥ kjÃ¸re koden derfra. Koden din kan skrive en fil til Ã¸nsket stammeomrÃ¥det, som du igjen kan lese inn direkte i Jupyterlab."
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#automatiserte-uttrekk",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#automatiserte-uttrekk",
    "title": "Fra Fame til Python",
    "section": "Automatiserte uttrekk",
    "text": "Automatiserte uttrekk\nHvis man Ã¸nsker at utrekk fra Fame skal skje automatisk pÃ¥ gitte tidspunkter eller intervaller, sÃ¥ kan man ta kontakt med Kundeservice. Fordelen med dette er at man ikke trenger Ã¥ bruke ssh slik som beskrevet over. Man kan lese inn direkte fra stammeomrÃ¥det."
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#overfÃ¸re-data-til-dapla",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#overfÃ¸re-data-til-dapla",
    "title": "Fra Fame til Python",
    "section": "OverfÃ¸re data til Dapla",
    "text": "OverfÃ¸re data til Dapla\nHvis man Ã¸nsker Ã¥ overfÃ¸re data fra Fame til Dapla, sÃ¥ kan dette settes opp som en MoveIt-operasjon. For Ã¥ sette opp en MoveIt-jobb mÃ¥ ma kontakte Kundeservice. OverfÃ¸ring til Dapla forutsetter at man har et Dapla-team, og at man setter opp en synkroniseringjobb med Transfer Service."
  },
  {
    "objectID": "blog/posts/2023-01-12-fame-extractor-python/index.html#footnotes",
    "href": "blog/posts/2023-01-12-fame-extractor-python/index.html#footnotes",
    "title": "Fra Fame til Python",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPakken er installert i Python-versjon 3.6 pÃ¥ serveren. Du kan Ã¥pne et Python-shell i terminalen pÃ¥ sl-fame-1.ssb.no ved Ã¥ skrive: python3.6.â†©ï¸"
  }
]